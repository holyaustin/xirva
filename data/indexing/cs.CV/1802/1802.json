[{"id": "1802.00036", "submitter": "Jason Ku", "authors": "Jason Ku, Ali Harakeh, Steven L. Waslander", "title": "In Defense of Classical Image Processing: Fast Depth Completion on the\n  CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the rise of data driven deep neural networks as a realization of\nuniversal function approximators, most research on computer vision problems has\nmoved away from hand crafted classical image processing algorithms. This paper\nshows that with a well designed algorithm, we are capable of outperforming\nneural network based methods on the task of depth completion. The proposed\nalgorithm is simple and fast, runs on the CPU, and relies only on basic image\nprocessing operations to perform depth completion of sparse LIDAR depth data.\nWe evaluate our algorithm on the challenging KITTI depth completion benchmark,\nand at the time of submission, our method ranks first on the KITTI test server\namong all published methods. Furthermore, our algorithm is data independent,\nrequiring no training data to perform the task at hand. The code written in\nPython will be made publicly available at https://github.com/kujason/ip_basic.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:46:11 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Ku", "Jason", ""], ["Harakeh", "Ali", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1802.00066", "submitter": "Mohan Trivedi", "authors": "Sujitha Martin, Sourabh Vora, Kevan Yuen and Mohan M. Trivedi", "title": "Dynamics of Driver's Gaze: Explorations in Behavior Modeling & Maneuver\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study and modeling of driver's gaze dynamics is important because, if and\nhow the driver is monitoring the driving environment is vital for driver\nassistance in manual mode, for take-over requests in highly automated mode and\nfor semantic perception of the surround in fully autonomous mode. We developed\na machine vision based framework to classify driver's gaze into context rich\nzones of interest and model driver's gaze behavior by representing gaze\ndynamics over a time period using gaze accumulation, glance duration and glance\nfrequencies. As a use case, we explore the driver's gaze dynamic patterns\nduring maneuvers executed in freeway driving, namely, left lane change\nmaneuver, right lane change maneuver and lane keeping. It is shown that\ncondensing gaze dynamics into durations and frequencies leads to recurring\npatterns based on driver activities. Furthermore, modeling these patterns show\npredictive powers in maneuver detection up to a few hundred milliseconds a\npriori.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 21:09:01 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Martin", "Sujitha", ""], ["Vora", "Sourabh", ""], ["Yuen", "Kevan", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1802.00088", "submitter": "Marc Bosch", "authors": "Marc Bosch and Christopher M. Gifford and Austin G. Dress and Clare W.\n  Lau and Jeffrey G. Skibo and Gordon A. Christie", "title": "Improved Image Segmentation via Cost Minimization of Multiple Hypotheses", "comments": "Accepted BMVC 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is an important component of many image understanding\nsystems. It aims to group pixels in a spatially and perceptually coherent\nmanner. Typically, these algorithms have a collection of parameters that\ncontrol the degree of over-segmentation produced. It still remains a challenge\nto properly select such parameters for human-like perceptual grouping. In this\nwork, we exploit the diversity of segments produced by different choices of\nparameters. We scan the segmentation parameter space and generate a collection\nof image segmentation hypotheses (from highly over-segmented to\nunder-segmented). These are fed into a cost minimization framework that\nproduces the final segmentation by selecting segments that: (1) better describe\nthe natural contours of the image, and (2) are more stable and persistent among\nall the segmentation hypotheses. We compare our algorithm's performance with\nstate-of-the-art algorithms, showing that we can achieve improved results. We\nalso show that our framework is robust to the choice of segmentation kernel\nthat produces the initial set of hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 22:37:46 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Bosch", "Marc", ""], ["Gifford", "Christopher M.", ""], ["Dress", "Austin G.", ""], ["Lau", "Clare W.", ""], ["Skibo", "Jeffrey G.", ""], ["Christie", "Gordon A.", ""]]}, {"id": "1802.00093", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Sungmin Eum, Heesung Kwon", "title": "Cross-domain CNN for Hyperspectral Image Classification", "comments": "IGARSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the dataset scarcity issue with the hyperspectral\nimage classification. As only a few thousands of pixels are available for\ntraining, it is difficult to effectively learn high-capacity Convolutional\nNeural Networks (CNNs). To cope with this problem, we propose a novel\ncross-domain CNN containing the shared parameters which can co-learn across\nmultiple hyperspectral datasets. The network also contains the non-shared\nportions designed to handle the dataset specific spectral characteristics and\nthe associated classification tasks. Our approach is the first attempt to learn\na CNN for multiple hyperspectral datasets, in an end-to-end fashion. Moreover,\nwe have experimentally shown that the proposed network trained on three of the\nwidely used datasets outperform all the baseline networks which are trained on\nsingle dataset.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 23:02:08 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 19:43:20 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Lee", "Hyungtae", ""], ["Eum", "Sungmin", ""], ["Kwon", "Heesung", ""]]}, {"id": "1802.00094", "submitter": "Zhixiang Chi", "authors": "Zhixiang Chi, Xiaolin Wu, Xiao Shu, Jinjin Gu", "title": "Single Image Reflection Removal Using Deep Encoder-Decoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image of a scene captured through a piece of transparent and reflective\nmaterial, such as glass, is often spoiled by a superimposed layer of reflection\nimage. While separating the reflection from a familiar object in an image is\nmentally not difficult for humans, it is a challenging, ill-posed problem in\ncomputer vision. In this paper, we propose a novel deep convolutional\nencoder-decoder method to remove the objectionable reflection by learning a map\nbetween image pairs with and without reflection. For training the neural\nnetwork, we model the physical formation of reflections in images and\nsynthesize a large number of photo-realistic reflection-tainted images from\nreflection-free images collected online. Extensive experimental results show\nthat, although the neural network learns only from synthetic data, the proposed\nmethod is effective on real-world images, and it significantly outperforms the\nother tested state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 23:05:44 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Chi", "Zhixiang", ""], ["Wu", "Xiaolin", ""], ["Shu", "Xiao", ""], ["Gu", "Jinjin", ""]]}, {"id": "1802.00121", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu", "title": "Interpreting CNNs via Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to quantitatively explain rationales of each prediction that\nis made by a pre-trained convolutional neural network (CNN). We propose to\nlearn a decision tree, which clarifies the specific reason for each prediction\nmade by the CNN at the semantic level. I.e., the decision tree decomposes\nfeature representations in high conv-layers of the CNN into elementary concepts\nof object parts. In this way, the decision tree tells people which object parts\nactivate which filters for the prediction and how much they contribute to the\nprediction score. Such semantic and quantitative explanations for CNN\npredictions have specific values beyond the traditional pixel-level analysis of\nCNNs. More specifically, our method mines all potential decision modes of the\nCNN, where each mode represents a common case of how the CNN uses object parts\nfor prediction. The decision tree organizes all potential decision modes in a\ncoarse-to-fine manner to explain CNN predictions at different fine-grained\nlevels. Experiments have demonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 01:47:15 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 10:12:11 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Zhang", "Quanshi", ""], ["Yang", "Yu", ""], ["Ma", "Haotian", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1802.00153", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi", "title": "Semantic White Balance: Semantic Color Constancy Using Convolutional\n  Neural Network", "comments": "Deep Learning and Reinforcement Learning Summer School (DLR), CIFAR -\n  Vector Institute, (Poster sessions), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of computational color constancy is to preserve the perceptive\ncolors of objects under different lighting conditions by removing the effect of\ncolor casts caused by the scene's illumination. With the rapid development of\ndeep learning based techniques, significant progress has been made in image\nsemantic segmentation. In this work, we exploit the semantic information\ntogether with the color and spatial information of the input image in order to\nremove color casts. We train a convolutional neural network (CNN) model that\nlearns to estimate the illuminant color and gamma correction parameters based\non the semantic information of the given image. Experimental results show that\nfeeding the CNN with the semantic information leads to a significant\nimprovement in the results by reducing the error by more than 40%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 04:29:56 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 07:49:55 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 15:41:57 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 16:51:34 GMT"}, {"version": "v5", "created": "Fri, 31 May 2019 04:23:06 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Afifi", "Mahmoud", ""]]}, {"id": "1802.00168", "submitter": "Bao Wang", "authors": "Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi and Stanley J.\n  Osher", "title": "Deep Neural Nets with Interpolating Function as Output Activation", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We replace the output layer of deep neural nets, typically the softmax\nfunction, by a novel interpolating function. And we propose end-to-end training\nand testing algorithms for this new architecture. Compared to classical neural\nnets with softmax function as output activation, the surrogate with\ninterpolating function as output activation combines advantages of both deep\nand manifold learning. The new framework demonstrates the following major\nadvantages: First, it is better applicable to the case with insufficient\ntraining data. Second, it significantly improves the generalization accuracy on\na wide variety of networks. The algorithm is implemented in PyTorch, and code\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 06:25:39 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 17:38:08 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 00:56:50 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Wang", "Bao", ""], ["Luo", "Xiyang", ""], ["Li", "Zhen", ""], ["Zhu", "Wei", ""], ["Shi", "Zuoqiang", ""], ["Osher", "Stanley J.", ""]]}, {"id": "1802.00176", "submitter": "Xuemei Xie", "authors": "Jiang Du, Xuemei Xie, Chenye Wang, Guangming Shi", "title": "Perceptual Compressive Sensing", "comments": "Accepted by The First Chinese Conference on Pattern Recognition and\n  Computer Vision (PRCV 2018). This is a pre-print version (not final version)", "journal-ref": "Chinese Conference on Pattern Recognition and Computer Vision\n  (PRCV) 2018", "doi": "10.1007/978-3-030-03338-5_23", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) works to acquire measurements at sub-Nyquist rate\nand recover the scene images. Existing CS methods always recover the scene\nimages in pixel level. This causes the smoothness of recovered images and lack\nof structure information, especially at a low measurement rate. To overcome\nthis drawback, in this paper, we propose perceptual CS to obtain high-level\nstructured recovery. Our task no longer focuses on pixel level. Instead, we\nwork to make a better visual effect. In detail, we employ perceptual loss,\ndefined on feature level, to enhance the structure information of the recovered\nimages. Experiments show that our method achieves better visual results with\nstronger structure information than existing CS methods at the same measurement\nrate.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 07:19:18 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 03:08:39 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Du", "Jiang", ""], ["Xie", "Xuemei", ""], ["Wang", "Chenye", ""], ["Shi", "Guangming", ""]]}, {"id": "1802.00179", "submitter": "Xuemei Xie", "authors": "Xuemei Xie, Chenye Wang, Jiang Du, Guangming Shi", "title": "Full Image Recover for Block-Based Compressive Sensing", "comments": "ICME 2018 submission # 1536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years, compressive sensing (CS) has improved greatly for the\napplication of deep learning technology. For convenience, the input image is\nusually measured and reconstructed block by block. This usually causes block\neffect in reconstructed images. In this paper, we present a novel CNN-based\nnetwork to solve this problem. In measurement part, the input image is\nadaptively measured block by block to acquire a group of measurements. While in\nreconstruction part, all the measurements from one image are used to\nreconstruct the full image at the same time. Different from previous method\nrecovering block by block, the structure information destroyed in measurement\npart is recovered in our framework. Block effect is removed accordingly. We\ntrain the proposed framework by mean square error (MSE) loss function.\nExperiments show that there is no block effect at all in the proposed method.\nAnd our results outperform 1.8 dB compared with existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 07:23:34 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Xie", "Xuemei", ""], ["Wang", "Chenye", ""], ["Du", "Jiang", ""], ["Shi", "Guangming", ""]]}, {"id": "1802.00209", "submitter": "Ahmed Osman", "authors": "Ahmed Osman and Wojciech Samek", "title": "Dual Recurrent Attention Units for Visual Question Answering", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) requires AI models to comprehend data in two\ndomains, vision and text. Current state-of-the-art models use learned attention\nmechanisms to extract relevant information from the input domains to answer a\ncertain question. Thus, robust attention mechanisms are essential for powerful\nVQA models. In this paper, we propose a recurrent attention mechanism and show\nits benefits compared to the traditional convolutional approach. We perform two\nablation studies to evaluate recurrent attention. First, we introduce a\nbaseline VQA model with visual attention and test the performance difference\nbetween convolutional and recurrent attention on the VQA 2.0 dataset. Secondly,\nwe design an architecture for VQA which utilizes dual (textual and visual)\nRecurrent Attention Units (RAUs). Using this model, we show the effect of all\npossible combinations of recurrent and convolutional dual attention. Our single\nmodel outperforms the first place winner on the VQA 2016 challenge and to the\nbest of our knowledge, it is the second best performing single model on the VQA\n1.0 dataset. Furthermore, our model noticeably improves upon the winner of the\nVQA 2017 challenge. Moreover, we experiment replacing attention mechanisms in\nstate-of-the-art models with our RAUs and show increased performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 09:35:33 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 16:27:26 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 13:41:21 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Osman", "Ahmed", ""], ["Samek", "Wojciech", ""]]}, {"id": "1802.00237", "submitter": "Defa Zhu", "authors": "Si Liu, Yao Sun, Defa Zhu, Renda Bao, Wei Wang, Xiangbo Shu, Shuicheng\n  Yan", "title": "Face Aging with Contextual Generative Adversarial Nets", "comments": "accepted at ACM Multimedia 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face aging, which renders aging faces for an input face, has attracted\nextensive attention in the multimedia research. Recently, several conditional\nGenerative Adversarial Nets (GANs) based methods have achieved great success.\nThey can generate images fitting the real face distributions conditioned on\neach individual age group. However, these methods fail to capture the\ntransition patterns, e.g., the gradual shape and texture changes between\nadjacent age groups. In this paper, we propose a novel Contextual Generative\nAdversarial Nets (C-GANs) to specifically take it into consideration. The\nC-GANs consists of a conditional transformation network and two discriminative\nnetworks. The conditional transformation network imitates the aging procedure\nwith several specially designed residual blocks. The age discriminative network\nguides the synthesized face to fit the real conditional distribution. The\ntransition pattern discriminative network is novel, aiming to distinguish the\nreal transition patterns with the fake ones. It serves as an extra\nregularization term for the conditional transformation network, ensuring the\ngenerated image pairs to fit the corresponding real transition pattern\ndistribution. Experimental results demonstrate the proposed framework produces\nappealing results by comparing with the state-of-the-art and ground truth. We\nalso observe performance gain for cross-age face verification.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 10:52:21 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Liu", "Si", ""], ["Sun", "Yao", ""], ["Zhu", "Defa", ""], ["Bao", "Renda", ""], ["Wang", "Wei", ""], ["Shu", "Xiangbo", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1802.00265", "submitter": "Lei Tai", "authors": "Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka\n  Boedecker, Wolfram Burgard", "title": "VR-Goggles for Robots: Real-to-sim Domain Adaptation for Visual Control", "comments": "IEEE RA-L 2019 to appear. The first two authors contributed equally.\n  Video and supplement file are available on the project\n  page(https://goo.gl/KcvmRm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the reality gap from a novel perspective,\ntargeting transferring Deep Reinforcement Learning (DRL) policies learned in\nsimulated environments to the real-world domain for visual control tasks.\nInstead of adopting the common solutions to the problem by increasing the\nvisual fidelity of synthetic images output from simulators during the training\nphase, we seek to tackle the problem by translating the real-world image\nstreams back to the synthetic domain during the deployment phase, to make the\nrobot feel at home. We propose this as a lightweight, flexible, and efficient\nsolution for visual control, as 1) no extra transfer steps are required during\nthe expensive training of DRL agents in simulation; 2) the trained DRL agents\nwill not be constrained to being deployable in only one specific real-world\nenvironment; 3) the policy training and the transfer operations are decoupled,\nand can be conducted in parallel. Besides this, we propose a simple yet\neffective shift loss that is agnostic to the downstream task, to constrain the\nconsistency between subsequent frames which is important for consistent policy\noutputs. We validate the shift loss for artistic style transfer for videos and\ndomain adaptation, and validate our visual control approach in indoor and\noutdoor robotics experiments.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:42:02 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 04:26:35 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 02:22:47 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 09:04:06 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Zhang", "Jingwei", ""], ["Tai", "Lei", ""], ["Yun", "Peng", ""], ["Xiong", "Yufeng", ""], ["Liu", "Ming", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1802.00278", "submitter": "Marek Kowalski", "authors": "Marek Kowalski, Zbigniew Nasarzewski, Grzegorz Galinski, Piotr Garbat", "title": "HoloFace: Augmenting Human-to-Human Interactions on HoloLens", "comments": "9 pages, 7 figures, 2018 IEEE Winter Conference on Applications of\n  Computer Vision (WACV 2018), YouTube video:\n  https://www.youtube.com/watch?v=Zexjx9VWkSU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HoloFace, an open-source framework for face alignment, head pose\nestimation and facial attribute retrieval for Microsoft HoloLens. HoloFace\nimplements two state-of-the-art face alignment methods which can be used\ninterchangeably: one running locally and one running on a remote backend. Head\npose estimation is accomplished by fitting a deformable 3D model to the\nlandmarks localized using face alignment. The head pose provides both the\nrotation of the head and a position in the world space. The parameters of the\nfitted 3D face model provide estimates of facial attributes such as mouth\nopening or smile. Together the above information can be used to augment the\nfaces of people seen by the HoloLens user, and thus their interaction.\nPotential usage scenarios include facial recognition, emotion recognition, eye\ngaze tracking and many others. We demonstrate the capabilities of our framework\nby augmenting the faces of people seen through the HoloLens with various\nobjects and animations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 13:33:04 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Kowalski", "Marek", ""], ["Nasarzewski", "Zbigniew", ""], ["Galinski", "Grzegorz", ""], ["Garbat", "Piotr", ""]]}, {"id": "1802.00285", "submitter": "Zhang-Wei Hong", "authors": "Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang\n  Chang, Hsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang,\n  Tsu-Ching Hsiao, Hsin-Wei Hsiao, Sih-Pin Lai, Chun-Yi Lee", "title": "Virtual-to-Real: Learning to Control in Visual Semantic Segmentation", "comments": "7 pages, accepted by IJCAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting training data from the physical world is usually time-consuming\nand even dangerous for fragile robots, and thus, recent advances in robot\nlearning advocate the use of simulators as the training platform.\nUnfortunately, the reality gap between synthetic and real visual data prohibits\ndirect migration of the models trained in virtual worlds to the real world.\nThis paper proposes a modular architecture for tackling the virtual-to-real\nproblem. The proposed architecture separates the learning model into a\nperception module and a control policy module, and uses semantic image\nsegmentation as the meta representation for relating these two modules. The\nperception module translates the perceived RGB image to semantic image\nsegmentation. The control policy module is implemented as a deep reinforcement\nlearning agent, which performs actions based on the translated image\nsegmentation. Our architecture is evaluated in an obstacle avoidance task and a\ntarget following task. Experimental results show that our architecture\nsignificantly outperforms all of the baseline methods in both virtual and real\nenvironments, and demonstrates a faster learning curve than them. We also\npresent a detailed analysis for a variety of variant configurations, and\nvalidate the transferability of our modular architecture.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 13:52:38 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 08:50:22 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 07:11:44 GMT"}, {"version": "v4", "created": "Sun, 28 Oct 2018 04:56:58 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Hong", "Zhang-Wei", ""], ["Yu-Ming", "Chen", ""], ["Su", "Shih-Yang", ""], ["Shann", "Tzu-Yun", ""], ["Chang", "Yi-Hsiang", ""], ["Yang", "Hsuan-Kung", ""], ["Ho", "Brian Hsi-Lin", ""], ["Tu", "Chih-Chieh", ""], ["Chang", "Yueh-Chuan", ""], ["Hsiao", "Tsu-Ching", ""], ["Hsiao", "Hsin-Wei", ""], ["Lai", "Sih-Pin", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1802.00383", "submitter": "Zheng Wu", "authors": "Zheng Wu, Ruiheng Chang, Jiaxu Ma, Cewu Lu, Chi-Keung Tang", "title": "Annotation-Free and One-Shot Learning for Instance Segmentation of\n  Homogeneous Object Clusters", "comments": "7 pages, 8 figures, submission to IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for instance segmen- tation given an image of\nhomogeneous object clus- ter (HOC). Our learning approach is one-shot be- cause\na single video of an object instance is cap- tured and it requires no human\nannotation. Our in- tuition is that images of homogeneous objects can be\neffectively synthesized based on structure and illumination priors derived from\nreal images. A novel solver is proposed that iteratively maximizes our\nstructured likelihood to generate realistic im- ages of HOC. Illumination\ntransformation scheme is applied to make the real and synthetic images share\nthe same illumination condition. Extensive experiments and comparisons are\nperformed to ver- ify our method. We build a dataset consisting of pixel-level\nannotated images of HOC. The dataset and code will be published with the paper.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 16:46:49 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 07:17:20 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Wu", "Zheng", ""], ["Chang", "Ruiheng", ""], ["Ma", "Jiaxu", ""], ["Lu", "Cewu", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1802.00411", "submitter": "Bo Yang", "authors": "Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen", "title": "Dense 3D Object Reconstruction from a Single Depth View", "comments": "TPAMI 2018. Code and data are available at:\n  https://github.com/Yang7879/3D-RecGAN-extended. This article extends from\n  arXiv:1708.07969", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2868195", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs\nthe complete 3D structure of a given object from a single arbitrary depth view\nusing generative adversarial networks. Unlike existing work which typically\nrequires multiple views of the same object or class labels to recover the full\n3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation\nof a depth view of the object as input, and is able to generate the complete 3D\noccupancy grid with a high resolution of 256^3 by recovering the\noccluded/missing regions. The key idea is to combine the generative\ncapabilities of autoencoders and the conditional Generative Adversarial\nNetworks (GAN) framework, to infer accurate and fine-grained 3D structures of\nobjects in high-dimensional voxel space. Extensive experiments on large\nsynthetic datasets and real-world Kinect datasets show that the proposed\n3D-RecGAN++ significantly outperforms the state of the art in single view 3D\nobject reconstruction, and is able to reconstruct unseen types of objects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 17:39:15 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 23:54:05 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Yang", "Bo", ""], ["Rosa", "Stefano", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""], ["Wen", "Hongkai", ""]]}, {"id": "1802.00421", "submitter": "Srijan Das", "authors": "Srijan Das, Michal Koperski, Francois Bremond, Gianpiero Francesca", "title": "Deep-Temporal LSTM for Daily Living Action Recognition", "comments": "Submitted in conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to improve the traditional use of RNNs by employing\na many to many model for video classification. We analyze the importance of\nmodeling spatial layout and temporal encoding for daily living action\nrecognition. Many RGB methods focus only on short term temporal information\nobtained from optical flow. Skeleton based methods on the other hand show that\nmodeling long term skeleton evolution improves action recognition accuracy. In\nthis work, we propose a deep-temporal LSTM architecture which extends standard\nLSTM and allows better encoding of temporal information. In addition, we\npropose to fuse 3D skeleton geometry with deep static appearance. We validate\nour approach on public available CAD60, MSRDailyActivity3D and NTU-RGB+D,\nachieving competitive performance as compared to the state-of-the art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 18:25:59 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 13:42:27 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Das", "Srijan", ""], ["Koperski", "Michal", ""], ["Bremond", "Francois", ""], ["Francesca", "Gianpiero", ""]]}, {"id": "1802.00434", "submitter": "R{\\i}za Alp Guler", "authors": "R{\\i}za Alp G\\\"uler, Natalia Neverova, Iasonas Kokkinos", "title": "DensePose: Dense Human Pose Estimation In The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we establish dense correspondences between RGB image and a\nsurface-based representation of the human body, a task we refer to as dense\nhuman pose estimation. We first gather dense correspondences for 50K persons\nappearing in the COCO dataset by introducing an efficient annotation pipeline.\nWe then use our dataset to train CNN-based systems that deliver dense\ncorrespondence 'in the wild', namely in the presence of background, occlusions\nand scale variations. We improve our training set's effectiveness by training\nan 'inpainting' network that can fill in missing groundtruth values and report\nclear improvements with respect to the best results that would be achievable in\nthe past. We experiment with fully-convolutional networks and region-based\nmodels and observe a superiority of the latter; we further improve accuracy\nthrough cascading, obtaining a system that delivers highly0accurate results in\nreal time. Supplementary materials and videos are provided on the project page\nhttp://densepose.org\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 18:53:26 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["G\u00fcler", "R\u0131za Alp", ""], ["Neverova", "Natalia", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1802.00469", "submitter": "Ayelet Heimowitz", "authors": "Ayelet Heimowitz, Joakim And\\'en, Amit Singer", "title": "APPLE Picker: Automatic Particle Picking, a Low-Effort Cryo-EM Framework", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle picking is a crucial first step in the computational pipeline of\nsingle-particle cryo-electron microscopy (cryo-EM). Selecting particles from\nthe micrographs is difficult especially for small particles with low contrast.\nAs high-resolution reconstruction typically requires hundreds of thousands of\nparticles, manually picking that many particles is often too time-consuming.\nWhile semi-automated particle picking is currently a popular approach, it may\nsuffer from introducing manual bias into the selection process. In addition,\nsemi-automated particle picking is still somewhat time-consuming. This paper\npresents the APPLE (Automatic Particle Picking with Low user Effort) picker, a\nsimple and novel approach for fast, accurate, and fully automatic particle\npicking. While our approach was inspired by template matching, it is completely\ntemplate-free. This approach is evaluated on publicly available datasets\ncontaining micrographs of $\\beta$-galactosidase and keyhole limpet hemocyanin\nprojections.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 19:37:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 13:51:12 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Heimowitz", "Ayelet", ""], ["And\u00e9n", "Joakim", ""], ["Singer", "Amit", ""]]}, {"id": "1802.00470", "submitter": "Paul Vernaza", "authors": "Paul Vernaza and Manmohan Chandraker", "title": "Learning random-walk label propagation for weakly-supervised semantic\n  segmentation", "comments": "This is a revised version of a paper presented at CVPR 2017 that\n  corrects some equations. See footnotes", "journal-ref": "CVPR 2017", "doi": "10.1109/CVPR.2017.315", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale training for semantic segmentation is challenging due to the\nexpense of obtaining training data for this task relative to other vision\ntasks. We propose a novel training approach to address this difficulty. Given\ncheaply-obtained sparse image labelings, we propagate the sparse labels to\nproduce guessed dense labelings. A standard CNN-based segmentation network is\ntrained to mimic these labelings. The label-propagation process is defined via\nrandom-walk hitting probabilities, which leads to a differentiable\nparameterization with uncertainty estimates that are incorporated into our\nloss. We show that by learning the label-propagator jointly with the\nsegmentation predictor, we are able to effectively learn semantic edges given\nno direct edge supervision. Experiments also show that training a segmentation\nnetwork in this way outperforms the naive approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 19:44:45 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Vernaza", "Paul", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1802.00491", "submitter": "Peixian Liang", "authors": "Peixian Liang, Jianxu Chen, Pavel A. Brodskiy, Qinfeng Wu, Yejia\n  Zhang, Yizhe Zhang, Lin Yang, Jeremiah J.Zartman, Danny Z. Chen", "title": "A New Registration Approach for Dynamic Analysis of Calcium Signals in\n  Organs", "comments": "Accepted at ISBI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wing disc pouches of fruit flies are a powerful genetic model for studying\nphysiological intercellular calcium ($Ca^{2+}$) signals for dynamic analysis of\ncell signaling in organ development and disease studies. A key to analyzing\nspatial-temporal patterns of $Ca^{2+}$ signal waves is to accurately align the\npouches across image sequences. However, pouches in different image frames may\nexhibit extensive intensity oscillations due to $Ca^{2+}$ signaling dynamics,\nand commonly used multimodal non-rigid registration methods may fail to achieve\nsatisfactory results. In this paper, we develop a new two-phase non-rigid\nregistration approach to register pouches in image sequences. First, we conduct\nsegmentation of the region of interest. (i.e., pouches) using a deep neural\nnetwork model. Second, we obtain an optimal transformation and align pouches\nacross the image sequences. Evaluated using both synthetic data and real pouch\ndata, our method considerably outperforms the state-of-the-art non-rigid\nregistration methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 21:24:47 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Liang", "Peixian", ""], ["Chen", "Jianxu", ""], ["Brodskiy", "Pavel A.", ""], ["Wu", "Qinfeng", ""], ["Zhang", "Yejia", ""], ["Zhang", "Yizhe", ""], ["Yang", "Lin", ""], ["Zartman", "Jeremiah J.", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1802.00509", "submitter": "Linwei Ye", "authors": "Linwei Ye, Zhi Liu, Yang Wang", "title": "Learning Semantic Segmentation with Diverse Supervision", "comments": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models based on deep convolutional neural networks (CNN) have significantly\nimproved the performance of semantic segmentation. However, learning these\nmodels requires a large amount of training images with pixel-level labels,\nwhich are very costly and time-consuming to collect. In this paper, we propose\na method for learning CNN-based semantic segmentation models from images with\nseveral types of annotations that are available for various computer vision\ntasks, including image-level labels for classification, box-level labels for\nobject detection and pixel-level labels for semantic segmentation. The proposed\nmethod is flexible and can be used together with any existing CNN-based\nsemantic segmentation networks. Experimental evaluation on the challenging\nPASCAL VOC 2012 and SIFT-flow benchmarks demonstrate that the proposed method\ncan effectively make use of diverse training data to improve the performance of\nthe learned models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 22:26:24 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Ye", "Linwei", ""], ["Liu", "Zhi", ""], ["Wang", "Yang", ""]]}, {"id": "1802.00539", "submitter": "Ruyue Xin", "authors": "Ruyue Xin and Jiang Zhang and Yitong Shao", "title": "Complex Network Classification with Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying large scale networks into several categories and distinguishing\nthem according to their fine structures is of great importance with several\napplications in real life. However, most studies of complex networks focus on\nproperties of a single network but seldom on classification, clustering, and\ncomparison between different networks, in which the network is treated as a\nwhole. Due to the non-Euclidean properties of the data, conventional methods\ncan hardly be applied on networks directly. In this paper, we propose a novel\nframework of complex network classifier (CNC) by integrating network embedding\nand convolutional neural network to tackle the problem of network\nclassification. By training the classifiers on synthetic complex network data\nand real international trade network data, we show CNC can not only classify\nnetworks in a high accuracy and robustness, it can also extract the features of\nthe networks automatically.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 02:12:33 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 13:31:41 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Xin", "Ruyue", ""], ["Zhang", "Jiang", ""], ["Shao", "Yitong", ""]]}, {"id": "1802.00542", "submitter": "Tal Hassner", "authors": "Feng-Ju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia,\n  Gerard Medioni", "title": "ExpNet: Landmark-Free, Deep, 3D Facial Expressions", "comments": "Accepted to the IEEE International Conference on Automatic Face and\n  Gesture Recognition, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a deep learning based method for estimating 3D facial expression\ncoefficients. Unlike previous work, our process does not relay on facial\nlandmark detection methods as a proxy step. Recent methods have shown that a\nCNN can be trained to regress accurate and discriminative 3D morphable model\n(3DMM) representations, directly from image intensities. By foregoing facial\nlandmark detection, these methods were able to estimate shapes for occluded\nfaces appearing in unprecedented in-the-wild viewing conditions. We build on\nthose methods by showing that facial expressions can also be estimated by a\nrobust, deep, landmark-free approach. Our ExpNet CNN is applied directly to the\nintensities of a face image and regresses a 29D vector of 3D expression\ncoefficients. We propose a unique method for collecting data to train this\nnetwork, leveraging on the robustness of deep networks to training label noise.\nWe further offer a novel means of evaluating the accuracy of estimated\nexpression coefficients: by measuring how well they capture facial emotions on\nthe CK+ and EmotiW-17 emotion recognition benchmarks. We show that our ExpNet\nproduces expression coefficients which better discriminate between facial\nemotions than those obtained using state of the art, facial landmark detection\ntechniques. Moreover, this advantage grows as image scales drop, demonstrating\nthat our ExpNet is more robust to scale changes than landmark detection\nmethods. Finally, at the same level of accuracy, our ExpNet is orders of\nmagnitude faster than its alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 02:42:44 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Chang", "Feng-Ju", ""], ["Tran", "Anh Tuan", ""], ["Hassner", "Tal", ""], ["Masi", "Iacopo", ""], ["Nevatia", "Ram", ""], ["Medioni", "Gerard", ""]]}, {"id": "1802.00565", "submitter": "Abel Ag Rb Guimaraes M.Sc", "authors": "Abel Ag Rb Guimaraes, Ghassem Tofighi", "title": "Detecting Zones and Threat on 3D Body for Security in Airports using\n  Deep Machine Learning", "comments": "7 pages, 17 figures, This article was accepted from the Star\n  Conference, Data Science and Big Data Analyses MAY 24-25, 2018 | Toronto,\n  Canada", "journal-ref": null, "doi": "10.5281/zenodo.1189345", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research, it was used a segmentation and classification method to\nidentify threat recognition in human scanner images of airport security. The\nDepartment of Homeland Security's (DHS) in USA has a higher false alarm,\nproduced from theirs algorithms using today's scanners at the airports. To\nrepair this problem they started a new competition at Kaggle site asking the\nscience community to improve their detection with new algorithms. The dataset\nused in this research comes from DHS at\nhttps://www.kaggle.com/c/passenger-screening-algorithm-challenge/data According\nto DHS: \"This dataset contains a large number of body scans acquired by a new\ngeneration of millimeter wave scanner called the High Definition-Advanced\nImaging Technology (HD-AIT) system. They are comprised of volunteers wearing\ndifferent clothing types (from light summer clothes to heavy winter clothes),\ndifferent body mass indices, different genders, different numbers of threats,\nand different types of threats\". Using Python as a principal language, the\npreprocessed of the dataset images extracted features from 200 bodies using:\nintensity, intensity differences and local neighbourhood to detect, to produce\nsegmentation regions and label those regions to be used as a truth in a\ntraining and test dataset. The regions are subsequently give to a CNN deep\nlearning classifier to predict 17 classes (that represents the body zones):\nzone1, zone2, ... zone17 and zones with threat in a total of 34 zones. The\nanalysis showed the results of the classifier an accuracy of 98.2863% and a\nloss of 0.091319, as well as an average of 100% for recall and precision.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 05:45:21 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 17:14:09 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Guimaraes", "Abel Ag Rb", ""], ["Tofighi", "Ghassem", ""]]}, {"id": "1802.00614", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Song-Chun Zhu", "title": "Visual Interpretability for Deep Learning: a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent studies in understanding neural-network\nrepresentations and learning neural networks with interpretable/disentangled\nmiddle-layer representations. Although deep neural networks have exhibited\nsuperior performance in various tasks, the interpretability is always the\nAchilles' heel of deep neural networks. At present, deep neural networks obtain\nhigh discrimination power at the cost of low interpretability of their\nblack-box representations. We believe that high model interpretability may help\npeople to break several bottlenecks of deep learning, e.g., learning from very\nfew annotations, learning via human-computer communications at the semantic\nlevel, and semantically debugging network representations. We focus on\nconvolutional neural networks (CNNs), and we revisit the visualization of CNN\nrepresentations, methods of diagnosing representations of pre-trained CNNs,\napproaches for disentangling pre-trained CNN representations, learning of CNNs\nwith disentangled representations, and middle-to-end learning based on model\ninterpretability. Finally, we discuss prospective trends in explainable\nartificial intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 09:39:40 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 08:02:59 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Quanshi", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1802.00624", "submitter": "Filip Malmberg", "authors": "Filip Malmberg and Robin Strand", "title": "When can $l_p$-norm objective functions be minimized via graph cuts?", "comments": "In proceedings of the 19th international workshop on combinatorial\n  image analysis (IWCIA), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques based on minimal graph cuts have become a standard tool for\nsolving combinatorial optimization problems arising in image processing and\ncomputer vision applications. These techniques can be used to minimize\nobjective functions written as the sum of a set of unary and pairwise terms,\nprovided that the objective function is submodular. This can be interpreted as\nminimizing the $l_1$-norm of the vector containing all pairwise and unary\nterms. By raising each term to a power $p$, the same technique can also be used\nto minimize the $l_p$-norm of the vector. Unfortunately, the submodularity of\nan $l_1$-norm objective function does not guarantee the submodularity of the\ncorresponding $l_p$-norm objective function. The contribution of this paper is\nto provide useful conditions under which an $l_p$-norm objective function is\nsubmodular for all $p\\geq 1$, thereby identifying a large class of $l_p$-norm\nobjective functions that can be minimized via minimal graph cuts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 10:09:22 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 08:10:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Malmberg", "Filip", ""], ["Strand", "Robin", ""]]}, {"id": "1802.00634", "submitter": "Moritz Einfalt", "authors": "Moritz Einfalt, Dan Zecha, Rainer Lienhart", "title": "Activity-conditioned continuous human pose estimation for performance\n  analysis of athletes using the example of swimming", "comments": "10 pages, 9 figures, accepted at WACV 2018", "journal-ref": null, "doi": "10.1109/wacv.2018.00055", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of human pose estimation in real-world\nvideos of swimmers. Swimming channels allow filming swimmers simultaneously\nabove and below the water surface with a single stationary camera. These\nrecordings can be used to quantitatively assess the athletes' performance. The\nquantitative evaluation, so far, requires manual annotations of body parts in\neach video frame. We therefore apply the concept of CNNs in order to\nautomatically infer the required pose information. Starting with an\noff-the-shelf architecture, we develop extensions to leverage activity\ninformation - in our case the swimming style of an athlete - and the continuous\nnature of the video recordings. Our main contributions are threefold: (a) We\napply and evaluate a fine-tuned Convolutional Pose Machine architecture as a\nbaseline in our very challenging aquatic environment and discuss its error\nmodes, (b) we propose an extension to input swimming style information into the\nfully convolutional architecture and (c) modify the architecture for continuous\npose estimation in videos. With these additions we achieve reliable pose\nestimates with up to +16% more correct body joint detections compared to the\nbaseline architecture.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 10:56:41 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Einfalt", "Moritz", ""], ["Zecha", "Dan", ""], ["Lienhart", "Rainer", ""]]}, {"id": "1802.00664", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Takashi Kakue, Tomoyoshi Ito", "title": "Convolutional neural network-based regression for depth prediction in\n  digital holography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital holography enables us to reconstruct objects in three-dimensional\nspace from holograms captured by an imaging device. For the reconstruction, we\nneed to know the depth position of the recoded object in advance. In this\nstudy, we propose depth prediction using convolutional neural network\n(CNN)-based regression. In the previous researches, the depth of an object was\nestimated through reconstructed images at different depth positions from a\nhologram using a certain metric that indicates the most focused depth position;\nhowever, such a depth search is time-consuming. The CNN of the proposed method\ncan directly predict the depth position with millimeter precision from\nholograms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 12:41:08 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1802.00671", "submitter": "Saikat Roy", "authors": "Saikat Roy, Nibaran Das, Mahantapas Kundu, Mita Nasipuri", "title": "Handwritten Isolated Bangla Compound Character Recognition: a new\n  benchmark using a novel deep learning approach", "comments": null, "journal-ref": "Pattern Recognition Letters, Elsevier, Vol. 90, Pages 15-21, 2017", "doi": "10.1016/j.patrec.2017.03.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel deep learning technique for the recognition of\nhandwritten Bangla isolated compound character is presented and a new benchmark\nof recognition accuracy on the CMATERdb 3.1.3.3 dataset is reported. Greedy\nlayer wise training of Deep Neural Network has helped to make significant\nstrides in various pattern recognition problems. We employ layerwise training\nto Deep Convolutional Neural Networks (DCNN) in a supervised fashion and\naugment the training process with the RMSProp algorithm to achieve faster\nconvergence. We compare results with those obtained from standard shallow\nlearning methods with predefined features, as well as standard DCNNs.\nSupervised layerwise trained DCNNs are found to outperform standard shallow\nlearning models such as Support Vector Machines as well as regular DCNNs of\nsimilar architecture by achieving error rate of 9.67% thereby setting a new\nbenchmark on the CMATERdb 3.1.3.3 with recognition accuracy of 90.33%,\nrepresenting an improvement of nearly 10%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 13:06:43 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Roy", "Saikat", ""], ["Das", "Nibaran", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1802.00745", "submitter": "Hugo Jair  Escalante", "authors": "Hugo Jair Escalante, Heysem Kaya, Albert Ali Salah, Sergio Escalera,\n  Yagmur Gucluturk, Umut Guclu, Xavier Baro, Isabelle Guyon, Julio Jacques\n  Junior, Meysam Madadi, Stephane Ayache, Evelyne Viegas, Furkan Gurpinar,\n  Achmadnoer Sukma Wicaksana, Cynthia C. S. Liem, Marcel A. J. van Gerven, Rob\n  van Lier", "title": "Explaining First Impressions: Modeling, Recognizing, and Explaining\n  Apparent Personality from Videos", "comments": "Preprint submitted to TAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability and interpretability are two critical aspects of decision\nsupport systems. Within computer vision, they are critical in certain tasks\nrelated to human behavior analysis such as in health care applications. Despite\ntheir importance, it is only recently that researchers are starting to explore\nthese aspects. This paper provides an introduction to explainability and\ninterpretability in the context of computer vision with an emphasis on looking\nat people tasks. Specifically, we review and study those mechanisms in the\ncontext of first impressions analysis. To the best of our knowledge, this is\nthe first effort in this direction. Additionally, we describe a challenge we\norganized on explainability in first impressions analysis from video. We\nanalyze in detail the newly introduced data set, the evaluation protocol, and\nsummarize the results of the challenge. Finally, derived from our study, we\noutline research opportunities that we foresee will be decisive in the near\nfuture for the development of the explainable computer vision field.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 16:02:55 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 14:48:26 GMT"}, {"version": "v3", "created": "Sun, 29 Sep 2019 03:24:32 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Escalante", "Hugo Jair", ""], ["Kaya", "Heysem", ""], ["Salah", "Albert Ali", ""], ["Escalera", "Sergio", ""], ["Gucluturk", "Yagmur", ""], ["Guclu", "Umut", ""], ["Baro", "Xavier", ""], ["Guyon", "Isabelle", ""], ["Junior", "Julio Jacques", ""], ["Madadi", "Meysam", ""], ["Ayache", "Stephane", ""], ["Viegas", "Evelyne", ""], ["Gurpinar", "Furkan", ""], ["Wicaksana", "Achmadnoer Sukma", ""], ["Liem", "Cynthia C. S.", ""], ["van Gerven", "Marcel A. J.", ""], ["van Lier", "Rob", ""]]}, {"id": "1802.00752", "submitter": "Alexey Shvets", "authors": "Alexander Rakhlin, Alexey Shvets, Vladimir Iglovikov and Alexandr A.\n  Kalinin", "title": "Deep Convolutional Neural Networks for Breast Cancer Histology Image\n  Analysis", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-319-93000-8_83", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the main causes of cancer death worldwide. Early\ndiagnostics significantly increases the chances of correct treatment and\nsurvival, but this process is tedious and often leads to a disagreement between\npathologists. Computer-aided diagnosis systems showed potential for improving\nthe diagnostic accuracy. In this work, we develop the computational approach\nbased on deep convolution neural networks for breast cancer histology image\nclassification. Hematoxylin and eosin stained breast histology microscopy image\ndataset is provided as a part of the ICIAR 2018 Grand Challenge on Breast\nCancer Histology Images. Our approach utilizes several deep neural network\narchitectures and gradient boosted trees classifier. For 4-class classification\ntask, we report 87.2% accuracy. For 2-class classification task to detect\ncarcinomas we report 93.8% accuracy, AUC 97.3%, and sensitivity/specificity\n96.5/88.0% at the high-sensitivity operating point. To our knowledge, this\napproach outperforms other common methods in automated histopathological image\nclassification. The source code for our approach is made publicly available at\nhttps://github.com/alexander-rakhlin/ICIAR2018\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 16:20:58 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 13:59:59 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Shvets", "Alexey", ""], ["Iglovikov", "Vladimir", ""], ["Kalinin", "Alexandr A.", ""]]}, {"id": "1802.00761", "submitter": "Fernando Moya Rueda", "authors": "Fernando Moya Rueda, Gernot A. Fink", "title": "Learning Attribute Representation for Human Activity Recognition", "comments": "6 pages, submitted to ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute representations became relevant in image recognition and word\nspotting, providing support under the presence of unbalance and disjoint\ndatasets. However, for human activity recognition using sequential data from\non-body sensors, human-labeled attributes are lacking. This paper introduces a\nsearch for attributes that represent favorably signal segments for recognizing\nhuman activities. It presents three deep architectures, including\ntemporal-convolutions and an IMU centered design, for predicting attributes. An\nempiric evaluation of random and learned attribute representations, and as well\nas the networks is carried out on two datasets, outperforming the state-of-the\nart.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 16:37:30 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Rueda", "Fernando Moya", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1802.00771", "submitter": "Vinay Namboodiri", "authors": "Shashank Sharma and Vinay P. Namboodiri", "title": "No Modes left behind: Capturing the data distribution effectively using\n  GANs", "comments": "accepted to AAAI 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) while being very versatile in\nrealistic image synthesis, still are sensitive to the input distribution. Given\na set of data that has an imbalance in the distribution, the networks are\nsusceptible to missing modes and not capturing the data distribution. While\nvarious methods have been tried to improve training of GANs, these have not\naddressed the challenges of covering the full data distribution. Specifically,\na generator is not penalized for missing a mode. We show that these are\ntherefore still susceptible to not capturing the full data distribution.\n  In this paper, we propose a simple approach that combines an encoder based\nobjective with novel loss functions for generator and discriminator that\nimproves the solution in terms of capturing missing modes. We validate that the\nproposed method results in substantial improvements through its detailed\nanalysis on toy and real datasets. The quantitative and qualitative results\ndemonstrate that the proposed method improves the solution for the problem of\nmissing modes and improves training of GANs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 17:10:55 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Sharma", "Shashank", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1802.00776", "submitter": "Nikola Bani\\'c", "authors": "Nikola Bani\\'c and Sven Lon\\v{c}ari\\'c", "title": "Green Stability Assumption: Unsupervised Learning for Statistics-Based\n  Illumination Estimation", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the image processing pipeline of almost every digital camera there is a\npart dedicated to computational color constancy i.e. to removing the influence\nof illumination on the colors of the image scene. Some of the best known\nillumination estimation methods are the so called statistics-based methods.\nThey are less accurate than the learning-based illumination estimation methods,\nbut they are faster and simpler to implement in embedded systems, which is one\nof the reasons for their widespread usage. Although in the relevant literature\nit often appears as if they require no training, this is not true because they\nhave parameter values that need to be fine-tuned in order to be more accurate.\nIn this paper it is first shown that the accuracy of statistics-based methods\nreported in most papers was not obtained by means of the necessary\ncross-validation, but by using the whole benchmark datasets for both training\nand testing. After that the corrected results are given for the best known\nbenchmark datasets. Finally, the so called green stability assumption is\nproposed that can be used to fine-tune the values of the parameters of the\nstatistics-based methods by using only non-calibrated images without known\nground-truth illumination. The obtained accuracy is practically the same as\nwhen using calibrated training images, but the whole process is much faster.\nThe experimental results are presented and discussed. The source code is\navailable at http://www.fer.unizg.hr/ipg/resources/color_constancy/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 17:19:40 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Bani\u0107", "Nikola", ""], ["Lon\u010dari\u0107", "Sven", ""]]}, {"id": "1802.00844", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, John K. Tsotsos", "title": "Intriguing Properties of Randomly Weighted Networks: Generalizing While\n  Learning Next to Nothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks results in strong learned representations that\nshow good generalization capabilities. In most cases, training involves\niterative modification of all weights inside the network via back-propagation.\nIn Extreme Learning Machines, it has been suggested to set the first layer of a\nnetwork to fixed random values instead of learning it. In this paper, we\npropose to take this approach a step further and fix almost all layers of a\ndeep convolutional neural network, allowing only a small portion of the weights\nto be learned. As our experiments show, fixing even the majority of the\nparameters of the network often results in performance which is on par with the\nperformance of learning all of them. The implications of this intriguing\nproperty of deep neural networks are discussed and we suggest ways to harness\nit to create more robust representations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 20:53:31 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1802.00853", "submitter": "Yue Wu", "authors": "Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong\n  Guo, Zhengyou Zhang, Yun Fu", "title": "Incremental Classifier Learning with Generative Adversarial Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the incremental classifier learning problem, which\nsuffers from catastrophic forgetting. The main reason for catastrophic\nforgetting is that the past data are not available during learning. Typical\napproaches keep some exemplars for the past classes and use distillation\nregularization to retain the classification capability on the past classes and\nbalance the past and new classes. However, there are four main problems with\nthese approaches. First, the loss function is not efficient for classification.\nSecond, there is unbalance problem between the past and new classes. Third, the\nsize of pre-decided exemplars is usually limited and they might not be\ndistinguishable from unseen new classes. Forth, the exemplars may not be\nallowed to be kept for a long time due to privacy regulations. To address these\nproblems, we propose (a) a new loss function to combine the cross-entropy loss\nand distillation loss, (b) a simple way to estimate and remove the unbalance\nbetween the old and new classes , and (c) using Generative Adversarial Networks\n(GANs) to generate historical data and select representative exemplars during\ngeneration. We believe that the data generated by GANs have much less privacy\nissues than real images because GANs do not directly copy any real image\npatches. We evaluate the proposed method on CIFAR-100, Flower-102, and\nMS-Celeb-1M-Base datasets and extensive experiments demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 21:35:45 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wu", "Yue", ""], ["Chen", "Yinpeng", ""], ["Wang", "Lijuan", ""], ["Ye", "Yuancheng", ""], ["Liu", "Zicheng", ""], ["Guo", "Yandong", ""], ["Zhang", "Zhengyou", ""], ["Fu", "Yun", ""]]}, {"id": "1802.00904", "submitter": "Yixing Li", "authors": "Yixing Li and Fengbo Ren", "title": "Build a Compact Binary Neural Network through Bit-level Sensitivity and\n  Data Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) has been widely used for vision-based\ntasks. Due to the high computational complexity and memory storage requirement,\nit is hard to directly deploy a full-precision CNN on embedded devices. The\nhardware-friendly designs are needed for re-source-limited and\nenergy-constrained embed-ded devices. Emerging solutions are adopted for the\nneural network compression, e.g., bina-ry/ternary weight network, pruned\nnetwork and quantized network. Among them, Binarized Neural Network (BNN) is\nbelieved to be the most hardware-friendly framework due to its small network\nsize and low computational com-plexity. No existing work has further shrunk the\nsize of BNN. In this work, we explore the redun-dancy in BNN and build a\ncompact BNN (CBNN) based on the bit-level sensitivity analy-sis and bit-level\ndata pruning. The input data is converted to a high dimensional bit-sliced\nfor-mat. In post-training stage, we analyze the im-pact of different bit slices\nto the accuracy. By pruning the redundant input bit slices and shrinking the\nnetwork size, we are able to build a more compact BNN. Our result shows that we\ncan further scale down the network size of the BNN up to 3.9x with no more than\n1% accuracy drop. The actual runtime can be reduced up to 2x and 9.9x compared\nwith the baseline BNN and its full-precision counterpart, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 03:50:41 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Li", "Yixing", ""], ["Ren", "Fengbo", ""]]}, {"id": "1802.00912", "submitter": "Zongwei Zhou", "authors": "Zongwei Zhou, Jae Y. Shin, Suryakanth R. Gurudu, Michael B. Gotway,\n  Jianming Liang", "title": "Active, Continual Fine Tuning of Convolutional Neural Networks for\n  Reducing Annotation Efforts", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2021.101997", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The splendid success of convolutional neural networks (CNNs) in computer\nvision is largely attributable to the availability of massive annotated\ndatasets, such as ImageNet and Places. However, in medical imaging, it is\nchallenging to create such large annotated datasets, as annotating medical\nimages is not only tedious, laborious, and time consuming, but it also demands\ncostly, specialty-oriented skills, which are not easily accessible. To\ndramatically reduce annotation cost, this paper presents a novel method to\nnaturally integrate active learning and transfer learning (fine-tuning) into a\nsingle framework, which starts directly with a pre-trained CNN to seek \"worthy\"\nsamples for annotation and gradually enhances the (fine-tuned) CNN via\ncontinual fine-tuning. We have evaluated our method using three distinct\nmedical imaging applications, demonstrating that it can reduce annotation\nefforts by at least half compared with random selection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 05:01:17 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 02:13:28 GMT"}, {"version": "v3", "created": "Sat, 23 May 2020 18:03:48 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 00:19:51 GMT"}, {"version": "v5", "created": "Sat, 10 Apr 2021 22:38:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Zongwei", ""], ["Shin", "Jae Y.", ""], ["Gurudu", "Suryakanth R.", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1802.00931", "submitter": "Yeeleng Vang", "authors": "Yeeleng S. Vang, Zhen Chen, and Xiaohui Xie", "title": "Deep Learning Framework for Multi-class Breast Cancer Histology Image\n  Classification", "comments": "8 pages, 2 figures, 3 tables, ICIAR2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a deep learning framework for multi-class breast\ncancer image classification as our submission to the International Conference\non Image Analysis and Recognition (ICIAR) 2018 Grand Challenge on BreAst Cancer\nHistology images (BACH). As these histology images are too large to fit into\nGPU memory, we first propose using Inception V3 to perform patch level\nclassification. The patch level predictions are then passed through an ensemble\nfusion framework involving majority voting, gradient boosting machine (GBM),\nand logistic regression to obtain the image level prediction. We improve the\nsensitivity of the Normal and Benign predicted classes by designing a Dual Path\nNetwork (DPN) to be used as a feature extractor where these extracted features\nare further sent to a second layer of ensemble prediction fusion using GBM,\nlogistic regression, and support vector machine (SVM) to refine predictions.\nExperimental results demonstrate our framework shows a 12.5$\\%$ improvement\nover the state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 07:13:02 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Vang", "Yeeleng S.", ""], ["Chen", "Zhen", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1802.00939", "submitter": "Peisong Wang", "authors": "Jian Cheng, Peisong Wang, Gang Li, Qinghao Hu, Hanqing Lu", "title": "Recent Advances in Efficient Computation of Deep Convolutional Neural\n  Networks", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have evolved remarkably over the past few years and they\nare currently the fundamental tools of many intelligent systems. At the same\ntime, the computational complexity and resource consumption of these networks\nalso continue to increase. This will pose a significant challenge to the\ndeployment of such networks, especially in real-time applications or on\nresource-limited devices. Thus, network acceleration has become a hot topic\nwithin the deep learning community. As for hardware implementation of deep\nneural networks, a batch of accelerators based on FPGA/ASIC have been proposed\nin recent years. In this paper, we provide a comprehensive survey of recent\nadvances in network acceleration, compression and accelerator design from both\nalgorithm and hardware points of view. Specifically, we provide a thorough\nanalysis of each of the following topics: network pruning, low-rank\napproximation, network quantization, teacher-student networks, compact network\ndesign and hardware accelerators. Finally, we will introduce and discuss a few\npossible future directions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 08:52:12 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 10:22:38 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Cheng", "Jian", ""], ["Wang", "Peisong", ""], ["Li", "Gang", ""], ["Hu", "Qinghao", ""], ["Lu", "Hanqing", ""]]}, {"id": "1802.00941", "submitter": "Gui-Song Xia", "authors": "Feng Yang, Gui-Song Xia, Dengxin Dai, Liangpei Zhang", "title": "Learning the Synthesizability of Dynamic Texture Samples", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2886807", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic texture (DT) refers to a sequence of images that exhibit temporal\nregularities and has many applications in computer vision and graphics. Given\nan exemplar of dynamic texture, it is a dynamic but challenging task to\ngenerate new samples with high quality that are perceptually similar to the\ninput exemplar, which is known to be {\\em example-based dynamic texture\nsynthesis (EDTS)}. Numerous approaches have been devoted to this problem, in\nthe past decades, but none them are able to tackle all kinds of dynamic\ntextures equally well. In this paper, we investigate the synthesizability of\ndynamic texture samples: {\\em given a dynamic texture sample, how synthesizable\nit is by using EDTS, and which EDTS method is the most suitable to synthesize\nit?} To this end, we propose to learn regression models to connect dynamic\ntexture samples with synthesizability scores, with the help of a compiled\ndynamic texture dataset annotated in terms of synthesizability. More precisely,\nwe first define the synthesizability of DT samples and characterize them by a\nset of spatiotemporal features. Based on these features and an annotated\ndynamic texture dataset, we then train regression models to predict the\nsynthesizability scores of texture samples and learn classifiers to select the\nmost suitable EDTS methods. We further complete the selection, partition and\nsynthesizability prediction of dynamic texture samples in a hierarchical\nscheme. We finally apply the learned synthesizability to detecting\nsynthesizable regions in videos. The experiments demonstrate that our method\ncan effectively learn and predict the synthesizability of DT samples.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 09:22:29 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Yang", "Feng", ""], ["Xia", "Gui-Song", ""], ["Dai", "Dengxin", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1802.00947", "submitter": "Mikhail Belyaev", "authors": "Gleb Makarchuk and Vladimir Kondratenko and Maxim Pisov and Artem\n  Pimkin and Egor Krivov and Mikhail Belyaev", "title": "Ensembling Neural Networks for Digital Pathology Images Classification\n  and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, neural networks have proven to be a powerful framework for\nvarious image analysis problems. However, some application domains have\nspecific limitations. Notably, digital pathology is an example of such fields\ndue to tremendous image sizes and quite limited number of training examples\navailable. In this paper, we adopt state-of-the-art convolutional neural\nnetworks (CNN) architectures for digital pathology images analysis. We propose\nto classify image patches to increase effective sample size and then to apply\nan ensembling technique to build prediction for the original images. To\nvalidate the developed approaches, we conducted experiments with \\textit{Breast\nCancer Histology Challenge} dataset and obtained 90\\% accuracy for the 4-class\ntissue classification task.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 10:05:28 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Makarchuk", "Gleb", ""], ["Kondratenko", "Vladimir", ""], ["Pisov", "Maxim", ""], ["Pimkin", "Artem", ""], ["Krivov", "Egor", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1802.00977", "submitter": "Cewu Lu", "authors": "Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, Cewu Lu", "title": "Pose Flow: Efficient Online Pose Tracking", "comments": "Our source codes and models are made publicly available at\n  https://github.com/YuliangXiu/PoseFlow and\n  https://github.com/MVIG-SJTU/AlphaPose", "journal-ref": "British Machine Vision Conference (BMVC), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person articulated pose tracking in unconstrained videos is an\nimportant while challenging problem. In this paper, going along the road of\ntop-down approaches, we propose a decent and efficient pose tracker based on\npose flows. First, we design an online optimization framework to build the\nassociation of cross-frame poses and form pose flows (PF-Builder). Second, a\nnovel pose flow non-maximum suppression (PF-NMS) is designed to robustly reduce\nredundant pose flows and re-link temporal disjoint ones. Extensive experiments\nshow that our method significantly outperforms best-reported results on two\nstandard Pose Tracking datasets by 13 mAP 25 MOTA and 6 mAP 3 MOTA\nrespectively. Moreover, in the case of working on detected poses in individual\nframes, the extra computation of pose tracker is very minor, guaranteeing\nonline 10FPS tracking. Our source codes are made publicly\navailable(https://github.com/YuliangXiu/PoseFlow).\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 14:08:36 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 18:40:46 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Xiu", "Yuliang", ""], ["Li", "Jiefeng", ""], ["Wang", "Haoyu", ""], ["Fang", "Yinghong", ""], ["Lu", "Cewu", ""]]}, {"id": "1802.01009", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi", "title": "Image Posterization Using Fuzzy Logic and Bilateral Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image posterization is converting images with a large number of tones into\nsynthetic images with distinct flat areas and a fewer number of tones. In this\ntechnical report, we present the implementation and results of using fuzzy\nlogic in order to generate a posterized image in a simple and fast way. The\nimage filter is based on fuzzy logic and bilateral filtering; where, the given\nimage is blurred to remove small details. Then, the fuzzy logic is used to\nclassify each pixel into one of three specific categories in order to reduce\nthe number of colors. This filter was developed during building the Specs on\nFace dataset in order to add a new level of difficulty to the original face\nimages in the dataset. This filter does not hurt the human detection\nperformance; however, it is considered a hindrance evading the face detection\nprocess. This filter can be used generally for posterizing images, especially\nthose have a high contrast to get images with vivid colors.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 17:57:22 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Afifi", "Mahmoud", ""]]}, {"id": "1802.01093", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz, Yusuf Tas, Hongguang Zhang, Mehrtash Harandi, Fatih\n  Porikli, Rui Zhang", "title": "Museum Exhibit Identification Challenge for Domain Adaptation and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we approach an open problem of artwork identification and\npropose a new dataset dubbed Open Museum Identification Challenge (Open MIC).\nIt contains photos of exhibits captured in 10 distinct exhibition spaces of\nseveral museums which showcase paintings, timepieces, sculptures, glassware,\nrelics, science exhibits, natural history pieces, ceramics, pottery, tools and\nindigenous crafts. The goal of Open MIC is to stimulate research in domain\nadaptation, egocentric recognition and few-shot learning by providing a testbed\ncomplementary to the famous Office dataset which reaches 90% accuracy. To form\nour dataset, we captured a number of images per art piece with a mobile phone\nand wearable cameras to form the source and target data splits, respectively.\nTo achieve robust baselines, we build on a recent approach that aligns\nper-class scatter matrices of the source and target CNN streams [15]. Moreover,\nwe exploit the positive definite nature of such representations by using\nend-to-end Bregman divergences and the Riemannian metric. We present baselines\nsuch as training/evaluation per exhibition and training/evaluation on the\ncombined set covering 866 exhibit identities. As each exhibition poses distinct\nchallenges e.g., quality of lighting, motion blur, occlusions, clutter,\nviewpoint and scale variations, rotations, glares, transparency, non-planarity,\nclipping, we break down results w.r.t. these factors.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 09:16:44 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Koniusz", "Piotr", ""], ["Tas", "Yusuf", ""], ["Zhang", "Hongguang", ""], ["Harandi", "Mehrtash", ""], ["Porikli", "Fatih", ""], ["Zhang", "Rui", ""]]}, {"id": "1802.01115", "submitter": "Panagiotis Tzirakis", "authors": "Panagiotis Tzirakis, Stefanos Zafeiriou, Bjorn W. Schuller", "title": "End2You -- The Imperial Toolkit for Multimodal Profiling by End-to-End\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce End2You -- the Imperial College London toolkit for multimodal\nprofiling by end-to-end deep learning. End2You is an open-source toolkit\nimplemented in Python and is based on Tensorflow. It provides capabilities to\ntrain and evaluate models in an end-to-end manner, i.e., using raw input. It\nsupports input from raw audio, visual, physiological or other types of\ninformation or combination of those, and the output can be of an arbitrary\nrepresentation, for either classification or regression tasks. To our\nknowledge, this is the first toolkit that provides generic end-to-end learning\nfor profiling capabilities in either unimodal or multimodal cases. To test our\ntoolkit, we utilise the RECOLA database as was used in the AVEC 2016 challenge.\nExperimental results indicate that End2You can provide comparable results to\nstate-of-the-art methods despite no need of expert-alike feature\nrepresentations, but self-learning these from the data \"end to end\".\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 12:30:00 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Tzirakis", "Panagiotis", ""], ["Zafeiriou", "Stefanos", ""], ["Schuller", "Bjorn W.", ""]]}, {"id": "1802.01116", "submitter": "Wenji Li", "authors": "Zhun Fan, Zhongxing Li, Benzhang Qiu, Wenji Li, Jianye Hu, Alex Noel\n  Josephraj and Heping Chen", "title": "Object Sorting Using a Global Texture-Shape 3D Feature Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition and grasping plays a key role in robotic systems,\nespecially for the autonomous robots to implement object sorting tasks in a\nwarehouse. In this paper, we present a global texture-shape 3D feature\ndescriptor which can be utilized in a system of object recognition and\ngrasping, and can perform object sorting tasks well. Our proposed descriptor\nstems from the clustered viewpoint feature histogram (CVFH), which relies on\nthe geometrical information of the whole 3D object surface only, and can not\nperform well in recognizing the objects with similar geometrical information.\nTherefore, we extend the CVFH descriptor with texture and color information to\ngenerate a new global 3D feature descriptor. The proposed descriptor is\nevaluated in tasks of recognizing and classifying 3D objects by applying\nmulti-class support vector machines (SVM) in both public 3D image dataset and\nreal scenes. The results of evaluation show that the proposed descriptor\nachieves a significant better performance for object recognition compared with\nthe original CVFH. Then, the proposed descriptor is applied in our object\nrecognition and grasping system, showing that the proposed descriptor helps the\nsystem implement the object recognition, object grasping and object sorting\ntasks well.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 12:35:49 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 14:56:49 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 06:59:07 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Fan", "Zhun", ""], ["Li", "Zhongxing", ""], ["Qiu", "Benzhang", ""], ["Li", "Wenji", ""], ["Hu", "Jianye", ""], ["Josephraj", "Alex Noel", ""], ["Chen", "Heping", ""]]}, {"id": "1802.01129", "submitter": "Xiao Guobao", "authors": "Hanzi Wang and Guobao Xiao and Yan Yan and David Suter", "title": "Searching for Representative Modes on Hypergraphs for Robust Geometric\n  Model Fitting", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine\n  Intelligence,2018", "doi": "10.1109/TPAMI.2018.2803173", "report-no": "1802.01129", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple and effective {geometric} model fitting\nmethod to fit and segment multi-structure data even in the presence of severe\noutliers. We cast the task of geometric model fitting as a representative\nmode-seeking problem on hypergraphs. Specifically, a hypergraph is firstly\nconstructed, where the vertices represent model hypotheses and the hyperedges\ndenote data points. The hypergraph involves higher-order similarities (instead\nof pairwise similarities used on a simple graph), and it can characterize\ncomplex relationships between model hypotheses and data points. {In addition,\nwe develop a hypergraph reduction technique to remove \"insignificant\" vertices\nwhile retaining as many \"significant\" vertices as possible in the hypergraph}.\nBased on the {simplified hypergraph, we then propose a novel mode-seeking\nalgorithm to search for representative modes within reasonable time. Finally,\nthe} proposed mode-seeking algorithm detects modes according to two key\nelements, i.e., the weighting scores of vertices and the similarity analysis\nbetween vertices. Overall, the proposed fitting method is able to efficiently\nand effectively estimate the number and the parameters of model instances in\nthe data simultaneously. Experimental results demonstrate that the proposed\nmethod achieves significant superiority over {several} state-of-the-art model\nfitting methods on both synthetic data and real images.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 14:18:19 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Wang", "Hanzi", ""], ["Xiao", "Guobao", ""], ["Yan", "Yan", ""], ["Suter", "David", ""]]}, {"id": "1802.01144", "submitter": "Cewu Lu", "authors": "Bo Pang, Kaiwen Zha, Cewu Lu", "title": "Human Action Adverb Recognition: ADHA Dataset and A Three-Stream Hybrid\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first benchmark for a new problem --- recognizing human\naction adverbs (HAA): \"Adverbs Describing Human Actions\" (ADHA). This is the\nfirst step for computer vision to change over from pattern recognition to real\nAI. We demonstrate some key features of ADHA: a semantically complete set of\nadverbs describing human actions, a set of common, describable human actions,\nand an exhaustive labeling of simultaneously emerging actions in each video. We\ncommit an in-depth analysis on the implementation of current effective models\nin action recognition and image captioning on adverb recognition, and the\nresults show that such methods are unsatisfactory. Moreover, we propose a novel\nthree-stream hybrid model to deal the HAA problem, which achieves a better\nresult.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 15:25:52 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 06:49:38 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Pang", "Bo", ""], ["Zha", "Kaiwen", ""], ["Lu", "Cewu", ""]]}, {"id": "1802.01186", "submitter": "Oggi Rudovic", "authors": "Ognjen Rudovic, Jaeryoung Lee, Miles Dai, Bjorn Schuller and Rosalind\n  Picard", "title": "Personalized Machine Learning for Robot Perception of Affect and\n  Engagement in Autism Therapy", "comments": "The paper has undergone a major revision and its content is outdated", "journal-ref": null, "doi": "10.1126/scirobotics.aao6760", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots have great potential to facilitate future therapies for children on\nthe autism spectrum. However, existing robots lack the ability to automatically\nperceive and respond to human affect, which is necessary for establishing and\nmaintaining engaging interactions. Moreover, their inference challenge is made\nharder by the fact that many individuals with autism have atypical and\nunusually diverse styles of expressing their affective-cognitive states. To\ntackle the heterogeneity in behavioral cues of children with autism, we use the\nlatest advances in deep learning to formulate a personalized machine learning\n(ML) framework for automatic perception of the childrens affective states and\nengagement during robot-assisted autism therapy. The key to our approach is a\nnovel shift from the traditional ML paradigm - instead of using\n'one-size-fits-all' ML models, our personalized ML framework is optimized for\neach child by leveraging relevant contextual information (demographics and\nbehavioral assessment scores) and individual characteristics of each child. We\ndesigned and evaluated this framework using a dataset of multi-modal audio,\nvideo and autonomic physiology data of 35 children with autism (age 3-13) and\nfrom 2 cultures (Asia and Europe), participating in a 25-minute child-robot\ninteraction (~500k datapoints). Our experiments confirm the feasibility of the\nrobot perception of affect and engagement, showing clear improvements due to\nthe model personalization. The proposed approach has potential to improve\nexisting therapies for autism by offering more efficient monitoring and\nsummarization of the therapy progress.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 20:05:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 01:21:12 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Lee", "Jaeryoung", ""], ["Dai", "Miles", ""], ["Schuller", "Bjorn", ""], ["Picard", "Rosalind", ""]]}, {"id": "1802.01218", "submitter": "Linjie Yang", "authors": "Linjie Yang and Yanran Wang and Xuehan Xiong and Jianchao Yang and\n  Aggelos K. Katsaggelos", "title": "Efficient Video Object Segmentation via Network Modulation", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation targets at segmenting a specific object throughout\na video sequence, given only an annotated first frame. Recent deep learning\nbased approaches find it effective by fine-tuning a general-purpose\nsegmentation model on the annotated frame using hundreds of iterations of\ngradient descent. Despite the high accuracy these methods achieve, the\nfine-tuning process is inefficient and fail to meet the requirements of real\nworld applications. We propose a novel approach that uses a single forward pass\nto adapt the segmentation model to the appearance of a specific object.\nSpecifically, a second meta neural network named modulator is learned to\nmanipulate the intermediate layers of the segmentation network given limited\nvisual and spatial information of the target object. The experiments show that\nour approach is 70times faster than fine-tuning approaches while achieving\nsimilar accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 23:53:58 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yang", "Linjie", ""], ["Wang", "Yanran", ""], ["Xiong", "Xuehan", ""], ["Yang", "Jianchao", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1802.01221", "submitter": "Salman Ul Hassan Dar", "authors": "Salman Ul Hassan Dar, Mahmut Yurt, Levent Karacan, Aykut Erdem, Erkut\n  Erdem, Tolga \\c{C}ukur", "title": "Image Synthesis in Multi-Contrast MRI with Conditional Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring images of the same anatomy with multiple different contrasts\nincreases the diversity of diagnostic information available in an MR exam. Yet,\nscan time limitations may prohibit acquisition of certain contrasts, and images\nfor some contrast may be corrupted by noise and artifacts. In such cases, the\nability to synthesize unacquired or corrupted contrasts from remaining\ncontrasts can improve diagnostic utility. For multi-contrast synthesis, current\nmethods learn a nonlinear intensity transformation between the source and\ntarget images, either via nonlinear regression or deterministic neural\nnetworks. These methods can in turn suffer from loss of high-spatial-frequency\ninformation in synthesized images. Here we propose a new approach for\nmulti-contrast MRI synthesis based on conditional generative adversarial\nnetworks. The proposed approach preserves high-frequency details via an\nadversarial loss; and it offers enhanced synthesis performance via a pixel-wise\nloss for registered multi-contrast images and a cycle-consistency loss for\nunregistered images. Information from neighboring cross-sections are utilized\nto further improved synthesis quality. Demonstrations on T1- and T2-weighted\nimages from healthy subjects and patients clearly indicate the superior\nperformance of the proposed approach compared to previous state-of-the-art\nmethods. Our synthesis approach can help improve quality and versatility of\nmulti-contrast MRI exams without the need for prolonged examinations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 00:10:12 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Dar", "Salman Ul Hassan", ""], ["Yurt", "Mahmut", ""], ["Karacan", "Levent", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "1802.01235", "submitter": "Xi Chen", "authors": "Xi Chen, Xiao Wang, Jianhua Xuan", "title": "Tracking Multiple Moving Objects Using Unscented Kalman Filtering\n  Techniques", "comments": "2012 International Conference on Engineering and Applied Science\n  (ICEAS 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an important task to reliably detect and track multiple moving objects\nfor video surveillance and monitoring. However, when occlusion occurs in\nnonlinear motion scenarios, many existing methods often fail to continuously\ntrack multiple moving objects of interest. In this paper we propose an\neffective approach for detection and tracking of multiple moving objects with\nocclusion. Moving targets are initially detected using a simple yet efficient\nblock matching technique, providing rough location information for multiple\nobject tracking. More accurate location information is then estimated for each\nmoving object by a nonlinear tracking algorithm. Considering the ambiguity\ncaused by the occlusion among multiple moving objects, we apply an unscented\nKalman filtering (UKF) technique for reliable object detection and tracking.\nDifferent from conventional Kalman filtering (KF), which cannot achieve the\noptimal estimation in nonlinear tracking scenarios, UKF can be used to track\nboth linear and nonlinear motions due to the unscented transform. Further, it\nestimates the velocity information for each object to assist to the object\ndetection algorithm, effectively delineating multiple moving objects of\nocclusion. The experimental results demonstrate that the proposed method can\ncorrectly detect and track multiple moving objects with nonlinear motion\npatterns and occlusions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:27:56 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chen", "Xi", ""], ["Wang", "Xiao", ""], ["Xuan", "Jianhua", ""]]}, {"id": "1802.01237", "submitter": "Fatemeh Shiri", "authors": "Fatemeh Shiri, Xin Yu, Fatih Porikli, Piotr Koniusz", "title": "Face Destylization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous style transfer methods which produce artistic styles of portraits\nhave been proposed to date. However, the inverse problem of converting the\nstylized portraits back into realistic faces is yet to be investigated\nthoroughly. Reverting an artistic portrait to its original photo-realistic face\nimage has potential to facilitate human perception and identity analysis. In\nthis paper, we propose a novel Face Destylization Neural Network (FDNN) to\nrestore the latent photo-realistic faces from the stylized ones. We develop a\nStyle Removal Network composed of convolutional, fully-connected and\ndeconvolutional layers. The convolutional layers are designed to extract facial\ncomponents from stylized face images. Consecutively, the fully-connected layer\ntransfers the extracted feature maps of stylized images into the corresponding\nfeature maps of real faces and the deconvolutional layers generate real faces\nfrom the transferred feature maps. To enforce the destylized faces to be\nsimilar to authentic face images, we employ a discriminative network, which\nconsists of convolutional and fully connected layers. We demonstrate the\neffectiveness of our network by conducting experiments on an extensive set of\nsynthetic images. Furthermore, we illustrate our network can recover faces from\nstylized portraits and real paintings for which the stylized data was\nunavailable during the training phase.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:30:46 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Shiri", "Fatemeh", ""], ["Yu", "Xin", ""], ["Porikli", "Fatih", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1802.01240", "submitter": "Rakesh Katuwal", "authors": "Rakesh Katuwal and P.N. Suganthan", "title": "Enhancing Multi-Class Classification of Random Forest using Random\n  Vector Functional Neural Network and Oblique Decision Surfaces", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both neural networks and decision trees are popular machine learning methods\nand are widely used to solve problems from diverse domains. These two\nclassifiers are commonly used base classifiers in an ensemble framework. In\nthis paper, we first present a new variant of oblique decision tree based on a\nlinear classifier, then construct an ensemble classifier based on the fusion of\na fast neural network, random vector functional link network and oblique\ndecision trees. Random Vector Functional Link Network has an elegant closed\nform solution with extremely short training time. The neural network partitions\neach training bag (obtained using bagging) at the root level into C subsets\nwhere C is the number of classes in the dataset and subsequently, C oblique\ndecision trees are trained on such partitions. The proposed method provides a\nrich insight into the data by grouping the confusing or hard to classify\nsamples for each class and thus, provides an opportunity to employ fine-grained\nclassification rule over the data. The performance of the ensemble classifier\nis evaluated on several multi-class datasets where it demonstrates a superior\nperformance compared to other state-of- the-art classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:42:39 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Katuwal", "Rakesh", ""], ["Suganthan", "P. N.", ""]]}, {"id": "1802.01267", "submitter": "Yohei Kikuta", "authors": "Kazuma Arino, Yohei Kikuta", "title": "ClassSim: Similarity between Classes Defined by Misclassification Ratios\n  of Trained Classifiers", "comments": "15 pages, 2 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved exceptional performances in many\ntasks, particularly, in supervised classification tasks. However, achievements\nwith supervised classification tasks are based on large datasets with\nwell-separated classes. Typically, real-world applications involve wild\ndatasets that include similar classes; thus, evaluating similarities between\nclasses and understanding relations among classes are important. To address\nthis issue, a similarity metric, ClassSim, based on the misclassification\nratios of trained DNNs is proposed herein. We conducted image recognition\nexperiments to demonstrate that the proposed method provides better\nsimilarities compared with existing methods and is useful for classification\nproblems. Source code including all experimental results is available at\nhttps://github.com/karino2/ClassSim/.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 05:00:35 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Arino", "Kazuma", ""], ["Kikuta", "Yohei", ""]]}, {"id": "1802.01268", "submitter": "Mai Thanh Nhat Truong", "authors": "Duy H. M. Nguyen, Duy M. Nguyen, Mai T. N. Truong, Thu Nguyen, Khanh\n  T. Tran, Nguyen A. Triet, Pham T. Bao, Binh T. Nguyen", "title": "ASMCNN: An Efficient Brain Extraction Using Active Shape Model and\n  Convolutional Neural Networks", "comments": "43 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain extraction (skull stripping) is a challenging problem in neuroimaging.\nIt is due to the variability in conditions from data acquisition or\nabnormalities in images, making brain morphology and intensity characteristics\nchangeable and complicated. In this paper, we propose an algorithm for skull\nstripping in Magnetic Resonance Imaging (MRI) scans, namely ASMCNN, by\ncombining the Active Shape Model (ASM) and Convolutional Neural Network (CNN)\nfor taking full of their advantages to achieve remarkable results. Instead of\nworking with 3D structures, we process 2D image sequences in the sagittal\nplane. First, we divide images into different groups such that, in each group,\nshapes and structures of brain boundaries have similar appearances. Second, a\nmodified version of ASM is used to detect brain boundaries by utilizing prior\nknowledge of each group. Finally, CNN and post-processing methods, including\nConditional Random Field (CRF), Gaussian processes, and several special rules\nare applied to refine the segmentation contours. Experimental results show that\nour proposed method outperforms current state-of-the-art algorithms by a\nsignificant margin in all experiments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 05:01:25 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 07:01:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nguyen", "Duy H. M.", ""], ["Nguyen", "Duy M.", ""], ["Truong", "Mai T. N.", ""], ["Nguyen", "Thu", ""], ["Tran", "Khanh T.", ""], ["Triet", "Nguyen A.", ""], ["Bao", "Pham T.", ""], ["Nguyen", "Binh T.", ""]]}, {"id": "1802.01273", "submitter": "S Ritika", "authors": "S Ritika, Dattaraj Rao", "title": "Face recognition for monitoring operator shift in railways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Train Pilot is a very tedious and stressful job. Pilots must be vigilant at\nall times and its easy for them to lose track of time of shift. In countries\nlike USA the pilots are mandated by law to adhere to 8 hour shifts. If they\nexceed 8 hours of shift the railroads may be penalized for over-tiring their\ndrivers. The problem happens when the 8 hour shift may end in middle of a\njourney. In such case, the new drivers must be moved to the location locomotive\nis operating for shift change. Hence accurate monitoring of drivers during\ntheir shift and making sure the shifts are scheduled correctly is very\nimportant for railroads. Here we propose an automated camera system that uses\ncamera mounted inside Locomotive cabs to continuously record video feeds. These\nfeeds are analyzed in real time to detect the face of driver and recognize the\ndriver using state of the art deep Learning techniques. The outcome is an\nincreased safety of train pilots. Cameras continuously capture video from\ninside the cab which is stored on an on board data acquisition device. Using\nadvanced computer vision and deep learning techniques the videos are analyzed\nat regular intervals to detect presence of the pilot and identify the pilot.\nUsing a time based analysis, it is identified for how long that shift has been\nactive. If this time exceeds allocated shift time an alert is sent to the\ndispatch to adjust shift hours.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 05:52:51 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 04:31:05 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Ritika", "S", ""], ["Rao", "Dattaraj", ""]]}, {"id": "1802.01274", "submitter": "Emily Spratt L", "authors": "Emily L. Spratt", "title": "Dream Formulations and Deep Neural Networks: Humanistic Themes in the\n  Iconology of the Machine-Learned Image", "comments": "29 pages, 8 Figures, This paper was originally presented as Dream\n  Formulations and Image Recognition: Algorithms for the Study of Renaissance\n  Art, at Critical Approaches to Digital Art History, The Villa I Tatti, The\n  Harvard University Center for Italian Renaissance Studies and The Newberry\n  Center for Renaissance Studies, Renaissance Society of America Annual\n  Meeting, Chicago, 31 March 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the interpretability of deep learning-enabled image\nrecognition processes in computer vision science in relation to theories in art\nhistory and cognitive psychology on the vision-related perceptual capabilities\nof humans. Examination of what is determinable about the machine-learned image\nin comparison to humanistic theories of visual perception, particularly in\nregard to art historian Erwin Panofsky's methodology for image analysis and\npsychologist Eleanor Rosch's theory of graded categorization according to\nprototypes, finds that there are surprising similarities between the two that\nsuggest that researchers in the arts and the sciences would have much to\nbenefit from closer collaborations. Utilizing the examples of Google's\nDeepDream and the Machine Learning and Perception Lab at Georgia Tech's\nGrad-CAM: Gradient-weighted Class Activation Mapping programs, this study\nsuggests that a revival of art historical research in iconography and formalism\nin the age of AI is essential for shaping the future navigation and\ninterpretation of all machine-learned images, given the rapid developments in\nimage recognition technologies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 05:57:40 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Spratt", "Emily L.", ""]]}, {"id": "1802.01279", "submitter": "Piotr Koniusz", "authors": "Hongguang Zhang, Piotr Koniusz", "title": "Zero-Shot Kernel Learning", "comments": "IEEE Conference on Computer Vision and Pattern Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address an open problem of zero-shot learning. Its\nprinciple is based on learning a mapping that associates feature vectors\nextracted from i.e. images and attribute vectors that describe objects and/or\nscenes of interest. In turns, this allows classifying unseen object classes\nand/or scenes by matching feature vectors via mapping to a newly defined\nattribute vector describing a new class. Due to importance of such a learning\ntask, there exist many methods that learn semantic, probabilistic, linear or\npiece-wise linear mappings. In contrast, we apply well-established kernel\nmethods to learn a non-linear mapping between the feature and attribute spaces.\nWe propose an easy learning objective inspired by the Linear Discriminant\nAnalysis, Kernel-Target Alignment and Kernel Polarization methods that promotes\nincoherence. We evaluate performance of our algorithm on the Polynomial as well\nas shift-invariant Gaussian and Cauchy kernels. Despite simplicity of our\napproach, we obtain state-of-the-art results on several zero-shot learning\ndatasets and benchmarks including a recent AWA2 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 06:30:44 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 03:05:25 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Hongguang", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1802.01286", "submitter": "Ritika S", "authors": "S Ritika, Dattaraj Rao", "title": "Data Augmentation of Railway Images for Track Inspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular maintenance of all the assets is pivotal for proper functioning of\nrailway. Manual maintenance can be very cumbersome and leave room for errors.\nTrack anomalies like vegetation overgrowth, sun kinks affect the track\nconstruct and result in unequal load transfer, imbalanced lateral forces on\ntracks which causes further deterioration of tracks and can ultimately result\nin derailment of locomotive. Hence there is a need to continuously monitor rail\ntrack health. Track anomalies are rare with the skew as high as one anomaly in\nmillions of good images. We propose a method to build training data that will\nmake our algorithms more robust and help us detect real world track issues. The\ndata augmentation will have a direct effect in making us detect better\nanomalies and hence improve time for railroads that is spent in manual\ninspection. This paper talks about a real world use case of detecting railway\ntrack defects from a camera mounted on a moving locomotive and tracking their\nlocations. The camera is engineered to withstand the environment factors on a\nmoving train and provide a consistent steady image at around 30 frames per\nsecond. An image simulation pipeline of track detection, region of interest\nselection, augmenting image for anomalies is implemented. Training images are\nsimulated for sun kink and vegetation overgrowth. Inception V3 model pretrained\non Imagenet dataset is finetuned for a 2 class classification. For the case of\nvegetation overgrowth, the model generalizes well on actual vegetation images,\nthough it was trained and validated solely on simulated images which might have\ndifferent distribution than the actual vegetation. Sun kink classifier can\nclassify professionally simulated sun kink videos with a precision of 97.5%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 07:07:14 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Ritika", "S", ""], ["Rao", "Dattaraj", ""]]}, {"id": "1802.01421", "submitter": "Carl-Johann Simon-Gabriel", "authors": "Carl-Johann Simon-Gabriel, Yann Ollivier, L\\'eon Bottou, Bernhard\n  Sch\\\"olkopf, David Lopez-Paz", "title": "First-order Adversarial Vulnerability of Neural Networks and Input\n  Dimension", "comments": "Paper previously called: \"Adversarial Vulnerability of Neural\n  Networks Increases with Input Dimension\". 9 pages main text and references,\n  11 pages appendix, 14 figures", "journal-ref": "Proceedings of ICML 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:36:44 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 13:26:43 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 19:01:10 GMT"}, {"version": "v4", "created": "Sun, 16 Jun 2019 20:55:06 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["Ollivier", "Yann", ""], ["Bottou", "L\u00e9on", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Lopez-Paz", "David", ""]]}, {"id": "1802.01435", "submitter": "Joshua Chacksfield", "authors": "Alexey Chaplygin and Joshua Chacksfield", "title": "A Method for Restoring the Training Set Distribution in an Image\n  Classifier", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are a well-known staple of modern image\nclassification. However, it can be difficult to assess the quality and\nrobustness of such models. Deep models are known to perform well on a given\ntraining and estimation set, but can easily be fooled by data that is\nspecifically generated for the purpose. It has been shown that one can produce\nan artificial example that does not represent the desired class, but activates\nthe network in the desired way. This paper describes a new way of\nreconstructing a sample from the training set distribution of an image\nclassifier without deep knowledge about the underlying distribution. This\nenables access to the elements of images that most influence the decision of a\nconvolutional network and to extract meaningful information about the training\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:49:06 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chaplygin", "Alexey", ""], ["Chacksfield", "Joshua", ""]]}, {"id": "1802.01445", "submitter": "Corentin Henry", "authors": "Corentin Henry, Seyed Majid Azimi, Nina Merkle", "title": "Road Segmentation in SAR Satellite Images with Deep Fully-Convolutional\n  Neural Networks", "comments": "5 pages, accepted for publication in IEEE Geoscience and Remote\n  Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2018.2864342", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing is extensively used in cartography. As transportation networks\ngrow and change, extracting roads automatically from satellite images is\ncrucial to keep maps up-to-date. Synthetic Aperture Radar satellites can\nprovide high resolution topographical maps. However roads are difficult to\nidentify in these data as they look visually similar to targets such as rivers\nand railways. Most road extraction methods on Synthetic Aperture Radar images\nstill rely on a prior segmentation performed by classical computer vision\nalgorithms. Few works study the potential of deep learning techniques, despite\ntheir successful applications to optical imagery. This letter presents an\nevaluation of Fully-Convolutional Neural Networks for road segmentation in SAR\nimages. We study the relative performance of early and state-of-the-art\nnetworks after carefully enhancing their sensitivity towards thin objects by\nadding spatial tolerance rules. Our models shows promising results,\nsuccessfully extracting most of the roads in our test dataset. This shows that,\nalthough Fully-Convolutional Neural Networks natively lack efficiency for road\nsegmentation, they are capable of good results if properly tuned. As the\nsegmentation quality does not scale well with the increasing depth of the\nnetworks, the design of specialized architectures for roads extraction should\nyield better performances.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:59:39 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 09:45:37 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Henry", "Corentin", ""], ["Azimi", "Seyed Majid", ""], ["Merkle", "Nina", ""]]}, {"id": "1802.01447", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Feng Li, Anhong Wang and Yao Zhao", "title": "Mixed-Resolution Image Representation and Compression with Convolutional\n  Neural Networks", "comments": "5 pages, and 2 figures. arXiv admin note: substantial text overlap\n  with arXiv:1712.05969", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end mixed-resolution image compression\nframework with convolutional neural networks. Firstly, given one input image,\nfeature description neural network (FDNN) is used to generate a new\nrepresentation of this image, so that this image representation can be more\nefficiently compressed by standard codec, as compared to the input image.\nFurthermore, we use post-processing neural network (PPNN) to remove the coding\nartifacts caused by quantization of codec. Secondly, low-resolution image\nrepresentation is adopted for high efficiency compression in terms of most of\nbit spent by image's structures under low bit-rate. However, more bits should\nbe assigned to image details in the high-resolution, when most of structures\nhave been kept after compression at the high bit-rate. This comes from a fact\nthat the low-resolution image representation can't burden more information than\nhigh-resolution representation beyond a certain bit-rate. Finally, to resolve\nthe problem of error back-propagation from the PPNN network to the FDNN\nnetwork, we introduce to learn a virtual codec neural network to imitate two\ncontinuous procedures of standard compression and post-processing. The\nobjective experimental results have demonstrated the proposed method has a\nlarge margin improvement, when comparing with several state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 08:57:44 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 03:26:44 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Li", "Feng", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1802.01458", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB, UCSD), Shibin Parameswaran (UCSD),\n  Truong Q. Nguyen (UCSD)", "title": "Image denoising with generalized Gaussian mixture model patch priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch priors have become an important component of image restoration. A\npowerful approach in this category of restoration algorithms is the popular\nExpected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture\nmodel (GMM) prior learned on clean image patches as a way to regularize\ndegraded patches. In this paper, we show that a generalized Gaussian mixture\nmodel (GGMM) captures the underlying distribution of patches better than a GMM.\nEven though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity\nof its components presents major challenges to be applied to a computationally\nintensive process of image restoration. Specifically, each patch has to undergo\na patch classification step and a shrinkage step. These two steps can be\nefficiently solved with a GMM prior but are computationally impractical when\nusing a GGMM prior. In this paper, we provide approximations and computational\nrecipes for fast evaluation of these two steps, so that EPLL can embed a GGMM\nprior on an image with more than tens of thousands of patches. Our main\ncontribution is to analyze the accuracy of our approximations based on thorough\ntheoretical analysis. Our evaluations indicate that the GGMM prior is\nconsistently a better fit formodeling image patch distribution and performs\nbetter on average in image denoising task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 15:18:21 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:00:13 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB, UCSD"], ["Parameswaran", "Shibin", "", "UCSD"], ["Nguyen", "Truong Q.", "", "UCSD"]]}, {"id": "1802.01500", "submitter": "Francis Engelmann", "authors": "Francis Engelmann and Theodora Kontogianni and Alexander Hermans and\n  Bastian Leibe", "title": "Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds", "comments": null, "journal-ref": null, "doi": "10.1109/ICCVW.2017.90", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning approaches have made tremendous progress in the field of\nsemantic segmentation over the past few years. However, most current approaches\noperate in the 2D image space. Direct semantic segmentation of unstructured 3D\npoint clouds is still an open research problem. The recently proposed PointNet\narchitecture presents an interesting step ahead in that it can operate on\nunstructured point clouds, achieving encouraging segmentation results. However,\nit subdivides the input points into a grid of blocks and processes each such\nblock individually. In this paper, we investigate the question how such an\narchitecture can be extended to incorporate larger-scale spatial context. We\nbuild upon PointNet and propose two extensions that enlarge the receptive field\nover the 3D scene. We evaluate the proposed strategies on challenging indoor\nand outdoor datasets and show improved results in both scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 16:17:58 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 22:28:53 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Engelmann", "Francis", ""], ["Kontogianni", "Theodora", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "1802.01516", "submitter": "Marcelo Saval Calvo", "authors": "Marcelo Saval-Calvo, Jorge Azorin-Lopez, Andres Fuster-Guillo, Victor\n  Villena-Martinez, Robert B. Fisher", "title": "3D non-rigid registration using color: Color Coherent Point Drift", "comments": "Published in Computer Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2018.01.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into object deformations using computer vision techniques has been\nunder intense study in recent years. A widely used technique is 3D non-rigid\nregistration to estimate the transformation between two instances of a\ndeforming structure. Despite many previous developments on this topic, it\nremains a challenging problem. In this paper we propose a novel approach to\nnon-rigid registration combining two data spaces in order to robustly calculate\nthe correspondences and transformation between two data sets. In particular, we\nuse point color as well as 3D location as these are the common outputs of RGB-D\ncameras. We have propose the Color Coherent Point Drift (CCPD) algorithm (an\nextension of the CPD method [1]). Evaluation is performed using synthetic and\nreal data. The synthetic data includes easy shapes that allow evaluation of the\neffect of noise, outliers and missing data. Moreover, an evaluation of\nrealistic figures obtained using Blensor is carried out. Real data acquired\nusing a general purpose Primesense Carmine sensor is used to validate the CCPD\nfor real shapes. For all tests, the proposed method is compared to the original\nCPD showing better results in registration accuracy in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 17:14:09 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Saval-Calvo", "Marcelo", ""], ["Azorin-Lopez", "Jorge", ""], ["Fuster-Guillo", "Andres", ""], ["Villena-Martinez", "Victor", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1802.01522", "submitter": "Soonam Lee", "authors": "Soonam Lee and Daekeun Kim", "title": "Background subtraction using the factored 3-way restricted Boltzmann\n  machines", "comments": "EECS545 (2011 Winter) class project report at the University of\n  Michigan. This is for archiving purpose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a method for reconstructing the 3D model based on\ncontinuous sensory input. The robot can draw on extremely large data from the\nreal world using various sensors. However, the sensory inputs are usually too\nnoisy and high-dimensional data. It is very difficult and time consuming for\nrobot to process using such raw data when the robot tries to construct 3D\nmodel. Hence, there needs to be a method that can extract useful information\nfrom such sensory inputs. To address this problem our method utilizes the\nconcept of Object Semantic Hierarchy (OSH). Different from the previous work\nthat used this hierarchy framework, we extract the motion information using the\nDeep Belief Network technique instead of applying classical computer vision\napproaches. We have trained on two large sets of random dot images (10,000)\nwhich are translated and rotated, respectively, and have successfully extracted\nseveral bases that explain the translation and rotation motion. Based on this\ntranslation and rotation bases, background subtraction have become possible\nusing Object Semantic Hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 17:29:40 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lee", "Soonam", ""], ["Kim", "Daekeun", ""]]}, {"id": "1802.01532", "submitter": "Blake Wulfe", "authors": "Blake Wulfe, Sunil Chintakindi, Sou-Cheng T. Choi, Rory\n  Hartong-Redden, Anuradha Kodali, Mykel J. Kochenderfer", "title": "Real-time Prediction of Intermediate-Horizon Automotive Collision Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced collision avoidance and driver hand-off systems can benefit from the\nability to accurately predict, in real time, the probability a vehicle will be\ninvolved in a collision within an intermediate horizon of 10 to 20 seconds. The\nrarity of collisions in real-world data poses a significant challenge to\ndeveloping this capability because, as we demonstrate empirically,\nintermediate-horizon risk prediction depends heavily on high-dimensional driver\nbehavioral features. As a result, a large amount of data is required to fit an\neffective predictive model. In this paper, we assess whether simulated data can\nhelp alleviate this issue. Focusing on highway driving, we present a three-step\napproach for generating data and fitting a predictive model capable of\nreal-time prediction. First, high-risk automotive scenes are generated using\nimportance sampling on a learned Bayesian network scene model. Second,\ncollision risk is estimated through Monte Carlo simulation. Third, a neural\nnetwork domain adaptation model is trained on real and simulated data to\naddress discrepancies between the two domains. Experiments indicate that\nsimulated data can mitigate issues resulting from collision rarity, thereby\nimproving risk prediction in real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 17:47:58 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wulfe", "Blake", ""], ["Chintakindi", "Sunil", ""], ["Choi", "Sou-Cheng T.", ""], ["Hartong-Redden", "Rory", ""], ["Kodali", "Anuradha", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1802.01548", "submitter": "Esteban Real", "authors": "Esteban Real, Alok Aggarwal, Yanping Huang and Quoc V Le", "title": "Regularized Evolution for Image Classifier Architecture Search", "comments": "Accepted for publication at AAAI 2019, the Thirty-Third AAAI\n  Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effort devoted to hand-crafting neural network image classifiers has\nmotivated the use of architecture search to discover them automatically.\nAlthough evolutionary algorithms have been repeatedly applied to neural network\ntopologies, the image classifiers thus discovered have remained inferior to\nhuman-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that\nsurpasses hand-designs for the first time. To do this, we modify the tournament\nselection evolutionary algorithm by introducing an age property to favor the\nyounger genotypes. Matching size, AmoebaNet-A has comparable accuracy to\ncurrent state-of-the-art ImageNet models discovered with more complex\narchitecture-search methods. Scaled to larger size, AmoebaNet-A sets a new\nstate-of-the-art 83.9% / 96.6% top-5 ImageNet accuracy. In a controlled\ncomparison against a well known reinforcement learning algorithm, we give\nevidence that evolution can obtain results faster with the same hardware,\nespecially at the earlier stages of the search. This is relevant when fewer\ncompute resources are available. Evolution is, thus, a simple method to\neffectively discover high-quality architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 18:20:52 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:24:29 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 00:10:00 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 06:21:47 GMT"}, {"version": "v5", "created": "Thu, 4 Oct 2018 00:11:37 GMT"}, {"version": "v6", "created": "Fri, 26 Oct 2018 05:56:00 GMT"}, {"version": "v7", "created": "Sat, 16 Feb 2019 23:28:16 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Real", "Esteban", ""], ["Aggarwal", "Alok", ""], ["Huang", "Yanping", ""], ["Le", "Quoc V", ""]]}, {"id": "1802.01557", "submitter": "Chelsea Finn", "authors": "Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang,\n  Pieter Abbeel, Sergey Levine", "title": "One-Shot Imitation from Observing Humans via Domain-Adaptive\n  Meta-Learning", "comments": "First two authors contributed equally. Video available at\n  https://sites.google.com/view/daml", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals are capable of learning a new behavior by observing others\nperform the skill just once. We consider the problem of allowing a robot to do\nthe same -- learning from a raw video pixels of a human, even when there is\nsubstantial domain shift in the perspective, environment, and embodiment\nbetween the robot and the observed human. Prior approaches to this problem have\nhand-specified how human and robot actions correspond and often relied on\nexplicit human pose detection systems. In this work, we present an approach for\none-shot learning from a video of a human by using human and robot\ndemonstration data from a variety of previous tasks to build up prior knowledge\nthrough meta-learning. Then, combining this prior knowledge and only a single\nvideo demonstration from a human, the robot can perform the task that the human\ndemonstrated. We show experiments on both a PR2 arm and a Sawyer arm,\ndemonstrating that after meta-learning, the robot can learn to place, push, and\npick-and-place new objects using just one video of a human performing the\nmanipulation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 18:36:19 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yu", "Tianhe", ""], ["Finn", "Chelsea", ""], ["Xie", "Annie", ""], ["Dasari", "Sudeep", ""], ["Zhang", "Tianhao", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1802.01666", "submitter": "Mohamed El Banani", "authors": "Mohamed El Banani and Jason J. Corso", "title": "Adviser Networks: Learning What Question to Ask for Human-In-The-Loop\n  Viewpoint Estimation", "comments": "15 pages, 3 figures. Updated Acknowledgment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have an unparalleled visual intelligence and can overcome visual\nambiguities that machines currently cannot. Recent works have shown that\nincorporating guidance from humans during inference for monocular\nviewpoint-estimation can help overcome difficult cases in which the\ncomputer-alone would have otherwise failed. These hybrid intelligence\napproaches are hence gaining traction. However, deciding what question to ask\nthe human at inference time remains an unknown for these problems.\n  We address this question by formulating it as an Adviser Problem: can we\nlearn a mapping from the input to a specific question to ask the human to\nmaximize the expected positive impact to the overall task? We formulate a\nsolution to the adviser problem for viewpoint estimation using a deep network\nwhere the question asks for the location of a keypoint in the input image. We\nshow that by using the Adviser Network's recommendations, the model and the\nhuman outperforms the previous hybrid-intelligence state-of-the-art by 3.7%,\nand the computer-only state-of-the-art by 5.28% absolute.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 21:01:10 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 16:18:20 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 14:30:13 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Banani", "Mohamed El", ""], ["Corso", "Jason J.", ""]]}, {"id": "1802.01722", "submitter": "Kuldeep S Kulkarni Mr.", "authors": "Mayank Gupta, Arjun Jauhari, Kuldeep Kulkarni, Suren Jayasuriya,\n  Alyosha Molnar, Pavan Turaga", "title": "Compressive Light Field Reconstructions using Deep Learning", "comments": "Published at CCD 2017 workshop held in conjunction with CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging is limited in its computational processing demands of\nhigh sampling for both spatial and angular dimensions. Single-shot light field\ncameras sacrifice spatial resolution to sample angular viewpoints, typically by\nmultiplexing incoming rays onto a 2D sensor array. While this resolution can be\nrecovered using compressive sensing, these iterative solutions are slow in\nprocessing a light field. We present a deep learning approach using a new, two\nbranch network architecture, consisting jointly of an autoencoder and a 4D CNN,\nto recover a high resolution 4D light field from a single coded 2D image. This\nnetwork decreases reconstruction time significantly while achieving average\nPSNR values of 26-32 dB on a variety of light fields. In particular,\nreconstruction time is decreased from 35 minutes to 6.7 minutes as compared to\nthe dictionary method for equivalent visual quality. These reconstructions are\nperformed at small sampling/compression ratios as low as 8%, allowing for\ncheaper coded light field cameras. We test our network reconstructions on\nsynthetic light fields, simulated coded measurements of real light fields\ncaptured from a Lytro Illum camera, and real coded images from a custom CMOS\ndiffractive light field camera. The combination of compressive light field\ncapture with deep learning allows the potential for real-time light field video\nacquisition systems in the future.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 23:00:47 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Gupta", "Mayank", ""], ["Jauhari", "Arjun", ""], ["Kulkarni", "Kuldeep", ""], ["Jayasuriya", "Suren", ""], ["Molnar", "Alyosha", ""], ["Turaga", "Pavan", ""]]}, {"id": "1802.01741", "submitter": "Xi Peng", "authors": "Rahil Mehrizi and Xi Peng and Zhiqiang Tang and Xu Xu and Dimitris\n  Metaxas and Kang Li", "title": "Toward Marker-free 3D Pose Estimation in Lifting: A Deep Multi-view\n  Solution", "comments": "FG2018, accepted as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifting is a common manual material handling task performed in the\nworkplaces. It is considered as one of the main risk factors for Work-related\nMusculoskeletal Disorders. To improve work place safety, it is necessary to\nassess musculoskeletal and biomechanical risk exposures associated with these\ntasks, which requires very accurate 3D pose. Existing approaches mainly utilize\nmarker-based sensors to collect 3D information. However, these methods are\nusually expensive to setup, time-consuming in process, and sensitive to the\nsurrounding environment. In this study, we propose a multi-view based deep\nperceptron approach to address aforementioned limitations. Our approach\nconsists of two modules: a \"view-specific perceptron\" network extracts rich\ninformation independently from the image of view, which includes both 2D shape\nand hierarchical texture information; while a \"multi-view integration\" network\nsynthesizes information from all available views to predict accurate 3D pose.\nTo fully evaluate our approach, we carried out comprehensive experiments to\ncompare different variants of our design. The results prove that our approach\nachieves comparable performance with former marker-based methods, i.e. an\naverage error of $14.72 \\pm 2.96$ mm on the lifting dataset. The results are\nalso compared with state-of-the-art methods on HumanEva-I dataset, which\ndemonstrates the superior performance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 00:28:44 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Mehrizi", "Rahil", ""], ["Peng", "Xi", ""], ["Tang", "Zhiqiang", ""], ["Xu", "Xu", ""], ["Metaxas", "Dimitris", ""], ["Li", "Kang", ""]]}, {"id": "1802.01756", "submitter": "Xiuzhen Huang Dr.", "authors": "Jason Causey, Junyu Zhang, Shiqian Ma, Bo Jiang, Jake Qualls, David G.\n  Politte, Fred Prior, Shuzhong Zhang and Xiuzhen Huang", "title": "Highly accurate model for prediction of lung nodule malignancy with CT\n  scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) examinations are commonly used to predict lung\nnodule malignancy in patients, which are shown to improve noninvasive early\ndiagnosis of lung cancer. It remains challenging for computational approaches\nto achieve performance comparable to experienced radiologists. Here we present\nNoduleX, a systematic approach to predict lung nodule malignancy from CT data,\nbased on deep learning convolutional neural networks (CNN). For training and\nvalidation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.\nAll nodules were identified and classified by four experienced thoracic\nradiologists who participated in the LIDC project. NoduleX achieves high\naccuracy for nodule malignancy classification, with an AUC of ~0.99. This is\ncommensurate with the analysis of the dataset by experienced radiologists. Our\napproach, NoduleX, provides an effective framework for highly accurate nodule\nmalignancy prediction with the model trained on a large patient population. Our\nresults are replicable with software available at\nhttp://bioinformatics.astate.edu/NoduleX.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 01:42:21 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Causey", "Jason", ""], ["Zhang", "Junyu", ""], ["Ma", "Shiqian", ""], ["Jiang", "Bo", ""], ["Qualls", "Jake", ""], ["Politte", "David G.", ""], ["Prior", "Fred", ""], ["Zhang", "Shuzhong", ""], ["Huang", "Xiuzhen", ""]]}, {"id": "1802.01770", "submitter": "Xin Tao", "authors": "Xin Tao, Hongyun Gao, Yi Wang, Xiaoyong Shen, Jue Wang, Jiaya Jia", "title": "Scale-recurrent Network for Deep Image Deblurring", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single image deblurring, the \"coarse-to-fine\" scheme, i.e. gradually\nrestoring the sharp image on different resolutions in a pyramid, is very\nsuccessful in both traditional optimization-based methods and recent\nneural-network-based approaches. In this paper, we investigate this strategy\nand propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task.\nCompared with the many recent learning-based approaches in [25], it has a\nsimpler network structure, a smaller number of parameters and is easier to\ntrain. We evaluate our method on large-scale deblurring datasets with complex\nmotion. Results show that our method can produce better quality results than\nstate-of-the-arts, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 03:00:40 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Tao", "Xin", ""], ["Gao", "Hongyun", ""], ["Wang", "Yi", ""], ["Shen", "Xiaoyong", ""], ["Wang", "Jue", ""], ["Jia", "Jiaya", ""]]}, {"id": "1802.01777", "submitter": "Mengtian Li", "authors": "Mengtian Li, Laszlo Jeni and Deva Ramanan", "title": "Brute-Force Facial Landmark Analysis With A 140,000-Way Classifier", "comments": "In AAAI 2018, code can be find at https://github.com/mtli/BFFL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple approach to visual alignment, focusing on the\nillustrative task of facial landmark estimation. While most prior work treats\nthis as a regression problem, we instead formulate it as a discrete $K$-way\nclassification task, where a classifier is trained to return one of $K$\ndiscrete alignments. One crucial benefit of a classifier is the ability to\nreport back a (softmax) distribution over putative alignments. We demonstrate\nthat this distribution is a rich representation that can be marginalized (to\ngenerate uncertainty estimates over groups of landmarks) and conditioned on (to\nincorporate top-down context, provided by temporal constraints in a video\nstream or an interactive human user). Such capabilities are difficult to\nintegrate into classic regression-based approaches. We study performance as a\nfunction of the number of classes $K$, including the extreme \"exemplar class\"\nsetting where $K$ is equal to the number of training examples (140K in our\nsetting). Perhaps surprisingly, we show that classifiers can still be learned\nin this setting. When compared to prior work in classification, our $K$ is\nunprecedentedly large, including many \"fine-grained\" classes that are very\nsimilar. We address these issues by using a multi-label loss function that\nallows for training examples to be non-uniformly shared across discrete\nclasses. We perform a comprehensive experimental analysis of our method on\nstandard benchmarks, demonstrating state-of-the-art results for facial\nalignment in videos.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 03:20:41 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 18:26:16 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Li", "Mengtian", ""], ["Jeni", "Laszlo", ""], ["Ramanan", "Deva", ""]]}, {"id": "1802.01821", "submitter": "Kazutoshi Sagi", "authors": "Kazutoshi Sagi, Takahiro Toizumi, and Yuzo Senda", "title": "Rollable Latent Space for Azimuth Invariant SAR Target Recognition", "comments": "4 pages, Accepted in International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes rollable latent space (RLS) for an azimuth invariant\nsynthetic aperture radar (SAR) target recognition. Scarce labeled data and\nlimited viewing direction are critical issues in SAR target recognition.The RLS\nis a designed space in which rolling of latent features corresponds to 3D\nrotation of an object. Thus latent features of an arbitrary view can be\ninferred using those of different views. This characteristic further enables us\nto augment data from limited viewing in RLS. RLS-based classifiers with and\nwithout data augmentation and a conventional classifier trained with target\nfront shots are evaluated over untrained target back shots. Results show that\nthe RLS-based classifier with augmentation improves an accuracy by 30% compared\nto the conventional classifier.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 06:59:05 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 01:17:20 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Sagi", "Kazutoshi", ""], ["Toizumi", "Takahiro", ""], ["Senda", "Yuzo", ""]]}, {"id": "1802.01822", "submitter": "Fengchun Qiao", "authors": "Fengchun Qiao, Naiming Yao, Zirui Jiao, Zhihao Li, Hui Chen, Hongan\n  Wang", "title": "Geometry-Contrastive GAN for Facial Expression Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Geometry-Contrastive Generative Adversarial\nNetwork (GC-GAN) for transferring continuous emotions across different\nsubjects. Given an input face with certain emotion and a target facial\nexpression from another subject, GC-GAN can generate an identity-preserving\nface with the target expression. Geometry information is introduced into cGANs\nas continuous conditions to guide the generation of facial expressions. In\norder to handle the misalignment across different subjects or emotions,\ncontrastive learning is used to transform geometry manifold into an embedded\nsemantic manifold of facial expressions. Therefore, the embedded geometry is\ninjected into the latent space of GANs and control the emotion generation\neffectively. Experimental results demonstrate that our proposed method can be\napplied in facial expression transfer even there exist big differences in\nfacial shapes and expressions between different subjects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 07:09:13 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 08:56:09 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Qiao", "Fengchun", ""], ["Yao", "Naiming", ""], ["Jiao", "Zirui", ""], ["Li", "Zhihao", ""], ["Chen", "Hui", ""], ["Wang", "Hongan", ""]]}, {"id": "1802.01872", "submitter": "Denis Fortun", "authors": "Denis Fortun, Martin Storath, Dennis Rickert, Andreas Weinmann,\n  Michael Unser", "title": "Fast Piecewise-Affine Motion Estimation Without Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current algorithmic approaches for piecewise affine motion estimation are\nbased on alternating motion segmentation and estimation. We propose a new\nmethod to estimate piecewise affine motion fields directly without intermediate\nsegmentation. To this end, we reformulate the problem by imposing piecewise\nconstancy of the parameter field, and derive a specific proximal splitting\noptimization scheme. A key component of our framework is an efficient\none-dimensional piecewise-affine estimator for vector-valued signals. The first\nadvantage of our approach over segmentation-based methods is its absence of\ninitialization. The second advantage is its lower computational cost which is\nindependent of the complexity of the motion field. In addition to these\nfeatures, we demonstrate competitive accuracy with other piecewise-parametric\nmethods on standard evaluation benchmarks. Our new regularization scheme also\noutperforms the more standard use of total variation and total generalized\nvariation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 10:15:30 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Fortun", "Denis", ""], ["Storath", "Martin", ""], ["Rickert", "Dennis", ""], ["Weinmann", "Andreas", ""], ["Unser", "Michael", ""]]}, {"id": "1802.01873", "submitter": "Wei Wang", "authors": "Wei Wang, Xavier Alameda-Pineda, Dan Xu, Pascal Fua, Elisa Ricci and\n  Nicu Sebe", "title": "Every Smile is Unique: Landmark-Guided Diverse Smile Generation", "comments": "Accepted as a poster in Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each smile is unique: one person surely smiles in different ways (e.g.,\nclosing/opening the eyes or mouth). Given one input image of a neutral face,\ncan we generate multiple smile videos with distinctive characteristics? To\ntackle this one-to-many video generation problem, we propose a novel deep\nlearning architecture named Conditional Multi-Mode Network (CMM-Net). To better\nencode the dynamics of facial expressions, CMM-Net explicitly exploits facial\nlandmarks for generating smile sequences. Specifically, a variational\nauto-encoder is used to learn a facial landmark embedding. This single\nembedding is then exploited by a conditional recurrent network which generates\na landmark embedding sequence conditioned on a specific expression (e.g.,\nspontaneous smile). Next, the generated landmark embeddings are fed into a\nmulti-mode recurrent landmark generator, producing a set of landmark sequences\nstill associated to the given smile class but clearly distinct from each other.\nFinally, these landmark sequences are translated into face videos. Our\nexperimental results demonstrate the effectiveness of our CMM-Net in generating\nrealistic videos of multiple smile expressions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 10:15:39 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 16:26:03 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 11:44:33 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Wang", "Wei", ""], ["Alameda-Pineda", "Xavier", ""], ["Xu", "Dan", ""], ["Fua", "Pascal", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1802.01880", "submitter": "Dahun Kim", "authors": "Dahun Kim, Donghyeon Cho, Donggeun Yoo, In So Kweon", "title": "Learning Image Representations by Completing Damaged Jigsaw Puzzles", "comments": "accepted at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore methods of complicating self-supervised tasks for\nrepresentation learning. That is, we do severe damage to data and encourage a\nnetwork to recover them. First, we complicate each of three powerful\nself-supervised task candidates: jigsaw puzzle, inpainting, and colorization.\nIn addition, we introduce a novel complicated self-supervised task called\n\"Completing damaged jigsaw puzzles\" which is puzzles with one piece missing and\nthe other pieces without color. We train a convolutional neural network not\nonly to solve the puzzles, but also generate the missing content and colorize\nthe puzzles. The recovery of the aforementioned damage pushes the network to\nobtain robust and general-purpose representations. We demonstrate that\ncomplicating the self-supervised tasks improves their original versions and\nthat our final task learns more robust and transferable representations\ncompared to the previous methods, as well as the simple combination of our\ncandidate tasks. Our approach achieves state-of-the-art performance in transfer\nlearning on PASCAL classification and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 10:42:28 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kim", "Dahun", ""], ["Cho", "Donghyeon", ""], ["Yoo", "Donggeun", ""], ["Kweon", "In So", ""]]}, {"id": "1802.01894", "submitter": "Boris Landa", "authors": "Boris Landa and Yoel Shkolnisky", "title": "The steerable graph Laplacian and its application to filtering image\n  data-sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, improvements in various image acquisition techniques gave\nrise to the need for adaptive processing methods, aimed particularly for large\ndatasets corrupted by noise and deformations. In this work, we consider\ndatasets of images sampled from a low-dimensional manifold (i.e. an\nimage-valued manifold), where the images can assume arbitrary planar rotations.\nTo derive an adaptive and rotation-invariant framework for processing such\ndatasets, we introduce a graph Laplacian (GL)-like operator over the dataset,\ntermed ${\\textit{steerable graph Laplacian}}$. Essentially, the steerable GL\nextends the standard GL by accounting for all (infinitely-many) planar\nrotations of all images. As it turns out, similarly to the standard GL, a\nproperly normalized steerable GL converges to the Laplace-Beltrami operator on\nthe low-dimensional manifold. However, the steerable GL admits an improved\nconvergence rate compared to the GL, where the improved convergence behaves as\nif the intrinsic dimension of the underlying manifold is lower by one.\nMoreover, it is shown that the steerable GL admits eigenfunctions of the form\nof Fourier modes (along the orbits of the images' rotations) multiplied by\neigenvectors of certain matrices, which can be computed efficiently by the FFT.\nFor image datasets corrupted by noise, we employ a subset of these\neigenfunctions to \"filter\" the dataset via a Fourier-like filtering scheme,\nessentially using all images and their rotations simultaneously. We demonstrate\nour filtering framework by de-noising simulated single-particle cryo-EM image\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 11:56:25 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 19:00:04 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1802.01943", "submitter": "Zhong Ji", "authors": "Zhong Ji, Yuxin Sun, Yunlong Yu, Yanwei Pang, Jungong Han", "title": "Attribute-Guided Network for Cross-Modal Zero-Shot Hashing", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Hashing aims at learning a hashing model that is trained only by\ninstances from seen categories but can generate well to those of unseen\ncategories. Typically, it is achieved by utilizing a semantic embedding space\nto transfer knowledge from seen domain to unseen domain. Existing efforts\nmainly focus on single-modal retrieval task, especially Image-Based Image\nRetrieval (IBIR). However, as a highlighted research topic in the field of\nhashing, cross-modal retrieval is more common in real world applications. To\naddress the Cross-Modal Zero-Shot Hashing (CMZSH) retrieval task, we propose a\nnovel Attribute-Guided Network (AgNet), which can perform not only IBIR, but\nalso Text-Based Image Retrieval (TBIR). In particular, AgNet aligns different\nmodal data into a semantically rich attribute space, which bridges the gap\ncaused by modality heterogeneity and zero-shot setting. We also design an\neffective strategy that exploits the attribute to guide the generation of hash\ncodes for image and text within the same network. Extensive experimental\nresults on three benchmark datasets (AwA, SUN, and ImageNet) demonstrate the\nsuperiority of AgNet on both cross-modal and single-modal zero-shot image\nretrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 13:43:06 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Ji", "Zhong", ""], ["Sun", "Yuxin", ""], ["Yu", "Yunlong", ""], ["Pang", "Yanwei", ""], ["Han", "Jungong", ""]]}, {"id": "1802.01958", "submitter": "Philipp Harzig", "authors": "Philipp Harzig, Stephan Brehm, Rainer Lienhart, Carolin Kaiser, Ren\\'e\n  Schallner", "title": "Multimodal Image Captioning for Marketing Analysis", "comments": "4 pages, 1 figure, accepted at MIPR2018", "journal-ref": null, "doi": "10.1109/MIPR.2018.00035", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically captioning images with natural language sentences is an\nimportant research topic. State of the art models are able to produce\nhuman-like sentences. These models typically describe the depicted scene as a\nwhole and do not target specific objects of interest or emotional relationships\nbetween these objects in the image. However, marketing companies require to\ndescribe these important attributes of a given scene. In our case, objects of\ninterest are consumer goods, which are usually identifiable by a product logo\nand are associated with certain brands. From a marketing point of view, it is\ndesirable to also evaluate the emotional context of a trademarked product,\ni.e., whether it appears in a positive or a negative connotation. We address\nthe problem of finding brands in images and deriving corresponding captions by\nintroducing a modified image captioning network. We also add a third output\nmodality, which simultaneously produces real-valued image ratings. Our network\nis trained using a classification-aware loss function in order to stimulate the\ngeneration of sentences with an emphasis on words identifying the brand of a\nproduct. We evaluate our model on a dataset of images depicting interactions\nbetween humans and branded products. The introduced network improves mean class\naccuracy by 24.5 percent. Thanks to adding the third output modality, it also\nconsiderably improves the quality of generated captions for images depicting\nbranded products.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 14:23:32 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 10:35:53 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Harzig", "Philipp", ""], ["Brehm", "Stephan", ""], ["Lienhart", "Rainer", ""], ["Kaiser", "Carolin", ""], ["Schallner", "Ren\u00e9", ""]]}, {"id": "1802.02018", "submitter": "Tiantong Guo", "authors": "Tiantong Guo, Hojjat S. Mousavi, and Vishal Monga", "title": "Orthogonally Regularized Deep Networks For Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning methods, in particular trained Convolutional Neural Networks\n(CNNs) have recently been shown to produce compelling state-of-the-art results\nfor single image Super-Resolution (SR). Invariably, a CNN is learned to map the\nlow resolution (LR) image to its corresponding high resolution (HR) version in\nthe spatial domain. Aiming for faster inference and more efficient solutions\nthan solving the SR problem in the spatial domain, we propose a novel network\nstructure for learning the SR mapping function in an image transform domain,\nspecifically the Discrete Cosine Transform (DCT). As a first contribution, we\nshow that DCT can be integrated into the network structure as a Convolutional\nDCT (CDCT) layer. We further extend the network to allow the CDCT layer to\nbecome trainable (i.e. optimizable). Because this layer represents an image\ntransform, we enforce pairwise orthogonality constraints on the individual\nbasis functions/filters. This Orthogonally Regularized Deep SR network (ORDSR)\nsimplifies the SR task by taking advantage of image transform domain while\nadapting the design of transform basis to the training image set.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 15:57:49 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Guo", "Tiantong", ""], ["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""]]}, {"id": "1802.02040", "submitter": "K\\'evin Degraux", "authors": "K\\'evin Degraux, Valerio Cambareri, Bert Geelen, Laurent Jacques,\n  Gauthier Lafruit", "title": "Multispectral Compressive Imaging Strategies using Fabry-P\\'erot\n  Filtered Sensors", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two acquisition device architectures for multispectral\ncompressive imaging. Unlike most existing methods, the proposed computational\nimaging techniques do not include any dispersive element, as they use a\ndedicated sensor which integrates narrowband Fabry-P\\'erot spectral filters at\nthe pixel level. The first scheme leverages joint inpainting and\nsuper-resolution to fill in those voxels that are missing due to the device's\nlimited pixel count. The second scheme, in link with compressed sensing,\nintroduces spatial random convolutions, but is more complex and may be affected\nby diffraction. In both cases we solve the associated inverse problems by using\nthe same signal prior. Specifically, we propose a redundant analysis signal\nprior in a convex formulation. Through numerical simulations, we explore\ndifferent realistic setups. Our objective is also to highlight some practical\nguidelines and discuss their complexity trade-offs to integrate these schemes\ninto actual computational imaging systems. Our conclusion is that the second\ntechnique performs best at high compression levels, in a properly sized and\ncalibrated setup. Otherwise, the first, simpler technique should be favored.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 16:23:42 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Degraux", "K\u00e9vin", ""], ["Cambareri", "Valerio", ""], ["Geelen", "Bert", ""], ["Jacques", "Laurent", ""], ["Lafruit", "Gauthier", ""]]}, {"id": "1802.02080", "submitter": "Marc Ru{\\ss}wurm", "authors": "Marc Ru{\\ss}wurm and Marco K\\\"orner", "title": "Multi-Temporal Land Cover Classification with Sequential Recurrent\n  Encoders", "comments": null, "journal-ref": "ISPRS Int. J. Geo-Inf. 2018, 7, 129", "doi": "10.3390/ijgi7040129", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Earth observation (EO) sensors deliver data with daily or weekly temporal\nresolution. Most land use and land cover (LULC) approaches, however, expect\ncloud-free and mono-temporal observations. The increasing temporal capabilities\nof today's sensors enables the use of temporal, along with spectral and spatial\nfeatures. Domains, such as speech recognition or neural machine translation,\nwork with inherently temporal data and, today, achieve impressive results using\nsequential encoder-decoder structures. Inspired by these sequence-to-sequence\nmodels, we adapt an encoder structure with convolutional recurrent layers in\norder to approximate a phenological model for vegetation classes based on a\ntemporal sequence of Sentinel 2 (S2) images. In our experiments, we visualize\ninternal activations over a sequence of cloudy and non-cloudy images and find\nseveral recurrent cells, which reduce the input activity for cloudy\nobservations. Hence, we assume that our network has learned cloud-filtering\nschemes solely from input data, which could alleviate the need for tedious\ncloud-filtering as a preprocessing step for many EO approaches. Moreover, using\nunfiltered temporal series of top-of-atmosphere (TOA) reflectance data, we\nachieved in our experiments state-of-the-art classification accuracies on a\nlarge number of crop classes with minimal preprocessing compared to other\nclassification approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 17:13:05 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 10:22:47 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 16:05:46 GMT"}, {"version": "v4", "created": "Sat, 7 Apr 2018 18:20:04 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ru\u00dfwurm", "Marc", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1802.02088", "submitter": "Tom Vercauteren", "authors": "Jyotirmoy Banerjee, Premal A. Patel, Fred Ushakov, Donald Peebles, Jan\n  Deprest, Sebastien Ourselin, David Hawkes, Tom Vercauteren", "title": "A Log-Euclidean and Total Variation based Variational Framework for\n  Computational Sonography", "comments": "SPIE Medical Imaging 2018", "journal-ref": null, "doi": "10.1117/12.2292501", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a spatial compounding technique and variational framework to\nimprove 3D ultrasound image quality by compositing multiple ultrasound volumes\nacquired from different probe orientations. In the composite volume, instead of\nintensity values, we estimate a tensor at every voxel. The resultant tensor\nimage encapsulates the directional information of the underlying imaging data\nand can be used to generate ultrasound volumes from arbitrary, potentially\nunseen, probe positions. Extending the work of Hennersperger et al., we\nintroduce a log-Euclidean framework to ensure that the tensors are\npositive-definite, eventually ensuring non-negative images. Additionally, we\nregularise the underpinning ill-posed variational problem while preserving edge\ninformation by relying on a total variation penalisation of the tensor field in\nthe log domain. We present results on in vivo human data to show the efficacy\nof the approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 17:42:07 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Banerjee", "Jyotirmoy", ""], ["Patel", "Premal A.", ""], ["Ushakov", "Fred", ""], ["Peebles", "Donald", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Hawkes", "David", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1802.02091", "submitter": "Sovan Biswas", "authors": "Sovan Biswas and Juergen Gall", "title": "Structural Recurrent Neural Network (SRNN) for Group Activity Analysis", "comments": "Accepted in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A group of persons can be analyzed at various semantic levels such as\nindividual actions, their interactions, and the activity of the entire group.\nIn this paper, we propose a structural recurrent neural network (SRNN) that\nuses a series of interconnected RNNs to jointly capture the actions of\nindividuals, their interactions, as well as the group activity. While previous\nstructural recurrent neural networks assumed that the number of nodes and edges\nis constant, we use a grid pooling layer to address the fact that the number of\nindividuals in a group can vary. We evaluate two variants of the structural\nrecurrent neural network on the Volleyball Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 17:46:32 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Biswas", "Sovan", ""], ["Gall", "Juergen", ""]]}, {"id": "1802.02137", "submitter": "Kevan Yuen", "authors": "Kevan Yuen and Mohan M. Trivedi", "title": "An Occluded Stacked Hourglass Approach to Facial Landmark Localization\n  and Occlusion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key step to driver safety is to observe the driver's activities with the\nface being a key step in this process to extracting information such as head\npose, blink rate, yawns, talking to passenger which can then help derive higher\nlevel information such as distraction, drowsiness, intent, and where they are\nlooking. In the context of driving safety, it is important for the system\nperform robust estimation under harsh lighting and occlusion but also be able\nto detect when the occlusion occurs so that information predicted from occluded\nparts of the face can be taken into account properly. This paper introduces the\nOccluded Stacked Hourglass, based on the work of original Stacked Hourglass\nnetwork for body pose joint estimation, which is retrained to process a\ndetected face window and output 68 occlusion heat maps, each corresponding to a\nfacial landmark. Landmark location, occlusion levels and a refined face\ndetection score, to reject false positives, are extracted from these heat maps.\nUsing the facial landmark locations, features such as head pose and eye/mouth\nopenness can be extracted to derive driver attention and activity. The system\nis evaluated for face detection, head pose, and occlusion estimation on various\ndatasets in the wild, both quantitatively and qualitatively, and shows\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:17:52 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Yuen", "Kevan", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1802.02138", "submitter": "Ramyad Hadidi", "authors": "Ramyad Hadidi, Jiashen Cao, Matthew Woodward, Michael S. Ryoo, Hyesoon\n  Kim", "title": "Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of Internet of things (IoT) devices and abundance of sensor\ndata has created an increase in real-time data processing such as recognition\nof speech, image, and video. While currently such processes are offloaded to\nthe computationally powerful cloud system, a localized and distributed approach\nis desirable because (i) it preserves the privacy of users and (ii) it omits\nthe dependency on cloud services. However, IoT networks are usually composed of\nresource-constrained devices, and a single device is not powerful enough to\nprocess real-time data. To overcome this challenge, we examine data and model\nparallelism for such devices in the context of deep neural networks. We propose\nMusical Chair to enable efficient, localized, and dynamic real-time recognition\nby harvesting the aggregated computational power from the resource-constrained\ndevices in the same IoT network as input sensors. Musical chair adapts to the\navailability of computing devices at runtime and adjusts to the inherit\ndynamics of IoT networks. To demonstrate Musical Chair, on a network of\nRaspberry PIs (up to 12) each connected to a camera, we implement a\nstate-of-the-art action recognition model for videos and two recognition models\nfor images. Compared to the Tegra TX2, an embedded low-power platform with a\nsix-core CPU and a GPU, our distributed action recognition system achieves not\nonly similar energy consumption but also twice the performance of the TX2.\nFurthermore, in image recognition, Musical Chair achieves similar performance\nand saves dynamic energy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:32:35 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 23:45:14 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 16:21:24 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Hadidi", "Ramyad", ""], ["Cao", "Jiashen", ""], ["Woodward", "Matthew", ""], ["Ryoo", "Michael S.", ""], ["Kim", "Hyesoon", ""]]}, {"id": "1802.02139", "submitter": "Karim Said Barsim", "authors": "Karim Said Barsim and Bin Yang", "title": "On the Feasibility of Generic Deep Disaggregation for Single-Load\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, and with the growing development of big energy datasets,\ndata-driven learning techniques began to represent a potential solution to the\nenergy disaggregation problem outperforming engineered and hand-crafted models.\nHowever, most proposed deep disaggregation models are load-dependent in the\nsense that either expert knowledge or a hyper-parameter optimization stage is\nrequired prior to training and deployment (normally for each load category)\neven upon acquisition and cleansing of aggregate and sub-metered data. In this\npaper, we present a feasibility study on the development of a generic\ndisaggregation model based on data-driven learning. Specifically, we present a\ngeneric deep disaggregation model capable of achieving state-of-art performance\nin load monitoring for a variety of load categories. The developed model is\nevaluated on the publicly available UK-DALE dataset with a moderately low\nsampling frequency and various domestic loads.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 20:59:36 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Barsim", "Karim Said", ""], ["Yang", "Bin", ""]]}, {"id": "1802.02142", "submitter": "Zhang Zheng", "authors": "Changzheng Zhang, Xiang Xu, Dandan Tu", "title": "Face Detection Using Improved Faster RCNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faster RCNN has achieved great success for generic object detection including\nPASCAL object detection and MS COCO object detection. In this report, we\npropose a detailed designed Faster RCNN method named FDNet1.0 for face\ndetection. Several techniques were employed including multi-scale training,\nmulti-scale testing, light-designed RCNN, some tricks for inference and a\nvote-based ensemble method. Our method achieves two 1th places and one 2nd\nplace in three tasks over WIDER FACE validation dataset (easy set, medium set,\nhard set).\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 05:25:02 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Changzheng", ""], ["Xu", "Xiang", ""], ["Tu", "Dandan", ""]]}, {"id": "1802.02147", "submitter": "Hanyuan Zhang", "authors": "Hanyuan Zhang, Hao Wu, Weiwei Sun, Baihua Zheng", "title": "DeepTravel: a Neural Network Based Travel Time Estimation Model with\n  Auxiliary Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the travel time of a path is of great importance to smart urban\nmobility. Existing approaches are either based on estimating the time cost of\neach road segment which are not able to capture many cross-segment complex\nfactors, or designed heuristically in a non-learning-based way which fail to\nutilize the existing abundant temporal labels of the data, i.e., the time stamp\nof each trajectory point. In this paper, we leverage on new development of deep\nneural networks and propose a novel auxiliary supervision model, namely\nDeepTravel, that can automatically and effectively extract different features,\nas well as make full use of the temporal labels of the trajectory data. We have\nconducted comprehensive experiments on real datasets to demonstrate the\nout-performance of DeepTravel over existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 16:08:06 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Hanyuan", ""], ["Wu", "Hao", ""], ["Sun", "Weiwei", ""], ["Zheng", "Baihua", ""]]}, {"id": "1802.02179", "submitter": "Hui Wu", "authors": "Hui Wu, Matrix Yao, Albert Hu, Gaofeng Sun, Xiaokun Yu, Jian Tang", "title": "A Systematic Analysis for State-of-the-Art 3D Lung Nodule Proposals\n  Generation", "comments": "7 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung nodule proposals generation is the primary step of lung nodule detection\nand has received much attention in recent years . In this paper, we first\nconstruct a model of 3-dimension Convolutional Neural Network (3D CNN) to\ngenerate lung nodule proposals, which can achieve the state-of-the-art\nperformance. Then, we analyze a series of key problems concerning the training\nperformance and efficiency. Firstly, we train the 3D CNN model with data in\ndifferent resolutions and find out that models trained by high resolution input\ndata achieve better lung nodule proposals generation performances especially\nfor nodules in too small sizes, while consumes much more memory at the same\ntime. Then, we analyze the memory consumptions on different platforms and the\nexperimental results indicate that CPU architecture can provide us with larger\nmemory and enables us to explore more possibilities of 3D applications. We\nimplement the 3D CNN model on CPU platform and propose an Intel Extended-Caffe\nframework which supports many highly-efficient 3D computations, which is opened\nsource at https://github.com/extendedcaffe/extended-caffe.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 01:13:33 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Wu", "Hui", ""], ["Yao", "Matrix", ""], ["Hu", "Albert", ""], ["Sun", "Gaofeng", ""], ["Yu", "Xiaokun", ""], ["Tang", "Jian", ""]]}, {"id": "1802.02181", "submitter": "Yonatan Tariku Tesfaye", "authors": "Yonatan Tariku Tesfaye", "title": "Applications of a Graph Theoretic Based Clustering Framework in Computer\n  Vision and Pattern Recognition", "comments": "doctoral dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, several clustering algorithms have been used to solve variety of\nproblems from different discipline. This dissertation aims to address different\nchallenging tasks in computer vision and pattern recognition by casting the\nproblems as a clustering problem. We proposed novel approaches to solve\nmulti-target tracking, visual geo-localization and outlier detection problems\nusing a unified underlining clustering framework, i.e., dominant set clustering\nand its extensions, and presented a superior result over several\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 09:35:12 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Tesfaye", "Yonatan Tariku", ""]]}, {"id": "1802.02182", "submitter": "Ganapathy Krishnamurthi", "authors": "Krishna Chaitanya Kaluva, Mahendra Khened, Avinash Kori and Ganapathy\n  Krishnamurthi", "title": "2D-Densely Connected Convolution Neural Networks for automatic Liver and\n  Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a fully automatic 2-stage cascaded approach for\nsegmentation of liver and its tumors in CT (Computed Tomography) images using\ndensely connected fully convolutional neural network (DenseNet). We\nindependently train liver and tumor segmentation models and cascade them for a\ncombined segmentation of the liver and its tumor. The first stage involves\nsegmentation of liver and the second stage uses the first stage's segmentation\nresults for localization of liver and henceforth tumor segmentations inside\nliver region. The liver model was trained on the down-sampled axial slices\n$(256 \\times 256)$, whereas for the tumor model no down-sampling of slices was\ndone, but instead it was trained on the CT axial slices windowed at three\ndifferent Hounsfield (HU) levels. On the test set our model achieved a global\ndice score of 0.923 and 0.625 on liver and tumor respectively. The computed\ntumor burden had an rmse of 0.044.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 12:30:53 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kaluva", "Krishna Chaitanya", ""], ["Khened", "Mahendra", ""], ["Kori", "Avinash", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "1802.02183", "submitter": "Ganapathy Krishnamurthi", "authors": "Avinash Kori, Ganapathy Krishnamurthi, Balaji Srinivasan", "title": "Enhanced Image Classification With Data Augmentation Using Position\n  Coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the use of image pixel position coordinate system to\nimprove image classification accuracy in various applications. Specifically, we\nhypothesize that the use of pixel coordinates will lead to (a) Resolution\ninvariant performance. Here, by resolution we mean the spacing between the\npixels rather than the size of the image matrix. (b) Overall improvement in\nclassification accuracy in comparison with network models trained without local\npixel coordinates. This is due to position coordinates enabling the network to\nlearn relationship between parts of objects, mimicking the human vision system.\nWe demonstrate our hypothesis using empirical results and intuitive\nexplanations of the feature maps learnt by deep neural networks. Specifically,\nour approach showed improvements in MNIST digit classification and beats state\nof the results on the SVHN database. We also show that the performance of our\nnetworks is unaffected despite training the same using blurred images of the\nMNIST database and predicting on the high resolution database.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 12:32:43 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kori", "Avinash", ""], ["Krishnamurthi", "Ganapathy", ""], ["Srinivasan", "Balaji", ""]]}, {"id": "1802.02185", "submitter": "Xin Guo", "authors": "Xin Guo and Luisa F. Polan\\'ia and Kenneth E. Barner", "title": "Smile detection in the wild based on transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smile detection from unconstrained facial images is a specialized and\nchallenging problem. As one of the most informative expressions, smiles convey\nbasic underlying emotions, such as happiness and satisfaction, which lead to\nmultiple applications, e.g., human behavior analysis and interactive\ncontrolling. Compared to the size of databases for face recognition, far less\nlabeled data is available for training smile detection systems. To leverage the\nlarge amount of labeled data from face recognition datasets and to alleviate\noverfitting on smile detection, an efficient transfer learning-based smile\ndetection approach is proposed in this paper. Unlike previous works which use\neither hand-engineered features or train deep convolutional networks from\nscratch, a well-trained deep face recognition model is explored and fine-tuned\nfor smile detection in the wild. Three different models are built as a result\nof fine-tuning the face recognition model with different inputs, including\naligned, unaligned and grayscale images generated from the GENKI-4K dataset.\nExperiments show that the proposed approach achieves improved state-of-the-art\nperformance. Robustness of the model to noise and blur artifacts is also\nevaluated in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 19:58:26 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Guo", "Xin", ""], ["Polan\u00eda", "Luisa F.", ""], ["Barner", "Kenneth E.", ""]]}, {"id": "1802.02186", "submitter": "John Olafenwa", "authors": "John Olafenwa, Moses Olafenwa", "title": "FastNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inception and the Resnet family of Convolutional Neural Network\narchi-tectures have broken records in the past few years, but recent state of\nthe art models have also incurred very high computational cost in terms of\ntraining, inference and model size. Making the deployment of these models on\nEdge devices, impractical. In light of this, we present a new novel\narchitecture that is designed for high computational efficiency on both GPUs\nand CPUs, and is highly suited for deployment on Mobile Applications, Smart\nCameras, Iot devices and controllers as well as low cost drones. Our\narchitecture boasts competitive accuracies on standard Datasets even\nout-performing the original Resnet. We present below the motivation for this\nresearch, the architecture of the network, single test accuracies on CIFAR 10\nand CIFAR 100 , a detailed comparison with other well-known architectures and\nlink to an implementation in Keras.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 10:37:58 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Olafenwa", "John", ""], ["Olafenwa", "Moses", ""]]}, {"id": "1802.02187", "submitter": "Vinh Ngo", "authors": "Vinh Ngo, Arnau Casadevall, Marc Codina, David Castells-Rufas, Jordi\n  Carrabina", "title": "A High-Performance HOG Extractor on FPGA", "comments": "Presented at HIP3ES, 2018", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2018/5", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is one of the key problems in emerging self-driving car\nindustry. And HOG algorithm has proven to provide good accuracy for pedestrian\ndetection. There are plenty of research works have been done in accelerating\nHOG algorithm on FPGA because of its low-power and high-throughput\ncharacteristics. In this paper, we present a high-performance HOG architecture\nfor pedestrian detection on a low-cost FPGA platform. It achieves a maximum\nthroughput of 526 FPS with 640x480 input images, which is 3.25 times faster\nthan the state of the art design. The accelerator is integrated with SVM-based\nprediction in realizing a pedestrian detection system. And the power\nconsumption of the whole system is comparable with the best existing\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 18:12:43 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Ngo", "Vinh", ""], ["Casadevall", "Arnau", ""], ["Codina", "Marc", ""], ["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1802.02202", "submitter": "Stefan Hoermann", "authors": "Stefan Hoermann, Philipp Henzler, Martin Bach and Klaus Dietmayer", "title": "Object Detection on Dynamic Occupancy Grid Maps Using Deep Learning and\n  Automatic Label Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of object detection and pose estimation in a shared\nspace downtown environment. For perception multiple laser scanners with\n360{\\deg} coverage were fused in a dynamic occupancy grid map (DOGMa). A\nsingle-stage deep convolutional neural network is trained to provide object\nhypotheses comprising of shape, position, orientation and an existence score\nfrom a single input DOGMa. Furthermore, an algorithm for offline object\nextraction was developed to automatically label several hours of training data.\nThe algorithm is based on a two-pass trajectory extraction, forward and\nbackward in time. Typical for engineered algorithms, the automatic label\ngeneration suffers from misdetections, which makes hard negative mining\nimpractical. Therefore, we propose a loss function counteracting the high\nimbalance between mostly static background and extremely rare dynamic grid\ncells. Experiments indicate, that the trained network has good generalization\ncapabilities since it detects objects occasionally lost by the label algorithm.\nEvaluation reaches an average precision (AP) of 75.9%\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 08:18:31 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Hoermann", "Stefan", ""], ["Henzler", "Philipp", ""], ["Bach", "Martin", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1802.02203", "submitter": "Yang Hu Dr.", "authors": "Yang Hu, Guihua Wen, Huiqiang Liao, Changjun Wang, Dan Dai, Zhiwen Yu", "title": "Automatic construction of Chinese herbal prescription from tongue image\n  via CNNs and auxiliary latent therapy topics", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The tongue image provides important physical information of humans. It is of\ngreat importance for diagnoses and treatments in clinical medicine. Herbal\nprescriptions are simple, noninvasive and have low side effects. Thus, they are\nwidely applied in China. Studies on the automatic construction technology of\nherbal prescriptions based on tongue images have great significance for deep\nlearning to explore the relevance of tongue images for herbal prescriptions, it\ncan be applied to healthcare services in mobile medical systems. In order to\nadapt to the tongue image in a variety of photographic environments and\nconstruct herbal prescriptions, a neural network framework for prescription\nconstruction is designed. It includes single/double convolution channels and\nfully connected layers. Furthermore, it proposes the auxiliary therapy topic\nloss mechanism to model the therapy of Chinese doctors and alleviate the\ninterference of sparse output labels on the diversity of results. The\nexperiment use the real world tongue images and the corresponding prescriptions\nand the results can generate prescriptions that are close to the real samples,\nwhich verifies the feasibility of the proposed method for the automatic\nconstruction of herbal prescriptions from tongue images. Also, it provides a\nreference for automatic herbal prescription construction from more physical\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 08:21:04 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 05:43:28 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 08:39:35 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 09:37:56 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Hu", "Yang", ""], ["Wen", "Guihua", ""], ["Liao", "Huiqiang", ""], ["Wang", "Changjun", ""], ["Dai", "Dan", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1802.02204", "submitter": "Pawel Cyrta", "authors": "Tomasz Trzcinski, Adam Bielski, Pawe{\\l} Cyrta, Matthew Zak", "title": "SocialML: machine learning for social media video creators", "comments": "2pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, social media have become one of the main places where\ncreative content is being published and consumed by billions of users. Contrary\nto traditional media, social media allow the publishers to receive almost\ninstantaneous feedback regarding their creative work at an unprecedented scale.\nThis is a perfect use case for machine learning methods that can use these\nmassive amounts of data to provide content creators with inspirational ideas\nand constructive criticism of their work. In this work, we present a\ncomprehensive overview of machine learning-empowered tools we developed for\nvideo creators at Group Nine Media - one of the major social media companies\nthat creates short-form videos with over three billion views per month. Our\nmain contribution is a set of tools that allow the creators to leverage massive\namounts of data to improve their creation process, evaluate their videos before\nthe publication and improve content quality. These applications include an\ninteractive conversational bot that allows access to material archives, a\nWeb-based application for automatic selection of optimal video thumbnail, as\nwell as deep learning methods for optimizing headline and predicting video\npopularity. Our A/B tests show that deployment of our tools leads to\nsignificant increase of average video view count by 12.9%. Our additional\ncontribution is a set of considerations collected during the deployment of\nthose tools that can hel\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 08:15:54 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Trzcinski", "Tomasz", ""], ["Bielski", "Adam", ""], ["Cyrta", "Pawe\u0142", ""], ["Zak", "Matthew", ""]]}, {"id": "1802.02207", "submitter": "Jaro Zink", "authors": "Jaro Milan Zink", "title": "Automated dataset generation for image recognition using the example of\n  taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This master thesis addresses the subject of automatically generating a\ndataset for image recognition, which takes a lot of time when being done\nmanually. As the thesis was written with motivation from the context of the\nbiodiversity workgroup at the City University of Applied Sciences Bremen, the\nclassification of taxonomic entries was chosen as an exemplary use case. In\norder to automate the dataset creation, a prototype was conceptualized and\nimplemented after working out knowledge basics and analyzing requirements for\nit. It makes use of an pre-trained abstract artificial intelligence which is\nable to sort out images that do not contain the desired content. Subsequent to\nthe implementation and the automated dataset creation resulting from it, an\nevaluation was performed. Other, manually collected datasets were compared to\nthe one the prototype produced in means of specifications and accuracy. The\nresults were more than satisfactory and showed that automatically generating a\ndataset for image recognition is not only possible, but also might be a decent\nalternative to spending time and money in doing this task manually. At the very\nend of this work, an idea of how to use the principle of employing abstract\nartificial intelligences for step-by-step classification of deeper taxonomic\nlayers in a productive system is presented and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 19:30:04 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zink", "Jaro Milan", ""]]}, {"id": "1802.02208", "submitter": "Wenji Li", "authors": "Zhun Fan, Yuming Wu, Jiewei Lu and Wenji Li", "title": "Automatic Pavement Crack Detection Based on Structured Prediction with\n  the Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pavement crack detection is a challenging task that has been\nresearched for decades due to the complicated pavement conditions in real\nworld. In this paper, a supervised method based on deep learning is proposed,\nwhich has the capability of dealing with different pavement conditions.\nSpecifically, a convolutional neural network (CNN) is used to learn the\nstructure of the cracks from raw images, without any preprocessing. Small\npatches are extracted from crack images as inputs to generate a large training\ndatabase, a CNN is trained and crack detection is modeled as a multi-label\nclassification problem. Typically, crack pixels are much fewer than non-crack\npixels. To deal with the problem with severely imbalanced data, a strategy with\nmodifying the ratio of positive to negative samples is proposed. The method is\ntested on two public databases and compared with five existing methods.\nExperimental results show that it outperforms the other methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 09:07:55 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Fan", "Zhun", ""], ["Wu", "Yuming", ""], ["Lu", "Jiewei", ""], ["Li", "Wenji", ""]]}, {"id": "1802.02209", "submitter": "Changhao Chen", "authors": "Changhao Chen, Xiaoxuan Lu, Andrew Markham, Niki Trigoni", "title": "IONet: Learning to Cure the Curse of Drift in Inertial Odometry", "comments": "To appear in AAAI18 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inertial sensors play a pivotal role in indoor localization, which in turn\nlays the foundation for pervasive personal applications. However, low-cost\ninertial sensors, as commonly found in smartphones, are plagued by bias and\nnoise, which leads to unbounded growth in error when accelerations are double\nintegrated to obtain displacement. Small errors in state estimation propagate\nto make odometry virtually unusable in a matter of seconds. We propose to break\nthe cycle of continuous integration, and instead segment inertial data into\nindependent windows. The challenge becomes estimating the latent states of each\nwindow, such as velocity and orientation, as these are not directly observable\nfrom sensor data. We demonstrate how to formulate this as an optimization\nproblem, and show how deep recurrent neural networks can yield highly accurate\ntrajectories, outperforming state-of-the-art shallow techniques, on a wide\nrange of tests and attachments. In particular, we demonstrate that IONet can\ngeneralize to estimate odometry for non-periodic motion, such as a shopping\ntrolley or baby-stroller, an extremely challenging task for existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 18:29:02 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Chen", "Changhao", ""], ["Lu", "Xiaoxuan", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1802.02210", "submitter": "Eri Matsuo", "authors": "Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida,\n  Hideki Asoh", "title": "Describing Semantic Representations of Brain Activity Evoked by Visual\n  Stimuli", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative modeling of human brain activity based on language\nrepresentations has been actively studied in systems neuroscience. However,\nprevious studies examined word-level representation, and little is known about\nwhether we could recover structured sentences from brain activity. This study\nattempts to generate natural language descriptions of semantic contents from\nhuman brain activity evoked by visual stimuli. To effectively use a small\namount of available brain activity data, our proposed method employs a\npre-trained image-captioning network model using a deep learning framework. To\napply brain activity to the image-captioning network, we train regression\nmodels that learn the relationship between brain activity and deep-layer image\nfeatures. The results demonstrate that the proposed model can decode brain\nactivity and generate descriptions using natural language sentences. We also\nconducted several experiments with data from different subsets of brain regions\nknown to process visual stimuli. The results suggest that semantic information\nfor sentence generations is widespread across the entire cortex.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:12:59 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Matsuo", "Eri", ""], ["Kobayashi", "Ichiro", ""], ["Nishimoto", "Shinji", ""], ["Nishida", "Satoshi", ""], ["Asoh", "Hideki", ""]]}, {"id": "1802.02212", "submitter": "Eric Tramel", "authors": "Pierre Courtiol, Eric W. Tramel, Marc Sanselme, Gilles Wainrib", "title": "Classification and Disease Localization in Histopathology Using Only\n  Global Labels: A Weakly-Supervised Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of histopathology slides is a critical step for many diagnoses, and\nin particular in oncology where it defines the gold standard. In the case of\ndigital histopathological analysis, highly trained pathologists must review\nvast whole-slide-images of extreme digital resolution ($100,000^2$ pixels)\nacross multiple zoom levels in order to locate abnormal regions of cells, or in\nsome cases single cells, out of millions. The application of deep learning to\nthis problem is hampered not only by small sample sizes, as typical datasets\ncontain only a few hundred samples, but also by the generation of ground-truth\nlocalized annotations for training interpretable classification and\nsegmentation models. We propose a method for disease localization in the\ncontext of weakly supervised learning, where only image-level labels are\navailable during training. Even without pixel-level annotations, we are able to\ndemonstrate performance comparable with models trained with strong annotations\non the Camelyon-16 lymph node metastases detection challenge. We accomplish\nthis through the use of pre-trained deep convolutional networks, feature\nembedding, as well as learning via top instances and negative evidence, a\nmultiple instance learning technique from the field of semantic segmentation\nand object detection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 15:21:14 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:25:47 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Courtiol", "Pierre", ""], ["Tramel", "Eric W.", ""], ["Sanselme", "Marc", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1802.02213", "submitter": "Kivanc Kose", "authors": "Alican Bozkurt, Kivanc Kose, Christi Alessi-Fox, Melissa Gill, Dana H.\n  Brooks, Jennifer G. Dy, Milind Rajadhyaksha", "title": "A Multiresolution Convolutional Neural Network with Partial Label\n  Training for Annotating Reflectance Confocal Microscopy Images of Skin", "comments": "This paper is accepted to MICCAI'18 conference. This is an extended\n  version of the abstract presented at to \"The Optical Society Biophotonics\n  Congress: Biomedical Optics 2018\" conference (c.f. previous ARXIV version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new multiresolution \"nested encoder-decoder\" convolutional\nnetwork architecture and use it to annotate morphological patterns in\nreflectance confocal microscopy (RCM) images of human skin for aiding cancer\ndiagnosis. Skin cancers are the most common types of cancers, melanoma being\nthe deadliest among them. RCM is an effective, non-invasive pre-screening tool\nfor skin cancer diagnosis, with the required cellular resolution. However,\nimages are complex, low-contrast, and highly variable, so that clinicians\nrequire months to years of expert-level training to be able to make accurate\nassessments. In this paper, we address classifying 4 key clinically important\nstructural/textural patterns in RCM images. The occurrence and morphology of\nthese patterns are used by clinicians for diagnosis of melanomas. The large\nsize of RCM images, the large variance of pattern size, the large-scale range\nover which patterns appear, the class imbalance in collected images, and the\nlack of fully-labeled images all make this a challenging problem to address,\neven with automated machine learning tools. We designed a novel nested U-net\narchitecture to cope with these challenges, and a selective loss function to\nhandle partial labeling. Trained and tested on 56 melanoma-suspicious,\npartially labeled, 12k x 12k pixel images, our network automatically annotated\ndiagnostic patterns with high sensitivity and specificity, providing consistent\nlabels for unlabeled sections of the test images. Providing such annotation\nwill aid clinicians in achieving diagnostic accuracy, and perhaps more\nimportant, dramatically facilitate clinical training, thus enabling much more\nrapid adoption of RCM into widespread clinical use process. In addition, our\nadaptation of U-net architecture provides an intrinsically multiresolution deep\nnetwork that may be useful in other challenging biomedical image analysis\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 04:20:45 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 01:09:52 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Bozkurt", "Alican", ""], ["Kose", "Kivanc", ""], ["Alessi-Fox", "Christi", ""], ["Gill", "Melissa", ""], ["Brooks", "Dana H.", ""], ["Dy", "Jennifer G.", ""], ["Rajadhyaksha", "Milind", ""]]}, {"id": "1802.02216", "submitter": "Jonito Aerts Arguelles", "authors": "Jonito Aerts Arguelles", "title": "The Heart of an Image: Quantum Superposition and Entanglement in Visual\n  Perception", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the way in which the principle that 'the whole is greater than the\nsum of its parts' manifests itself with phenomena of visual perception. For\nthis investigation we use insights and techniques coming from quantum\ncognition, and more specifically we are inspired by the correspondence of this\nprinciple with the phenomenon of the conjunction effect in human cognition. We\nidentify entities of meaning within artefacts of visual perception and rely on\nhow such entities are modelled for corpuses of texts such as the webpages of\nthe World-Wide Web for our study of how they appear in phenomena of visual\nperception. We identify concretely the conjunction effect in visual artefacts\nand analyse its structure in the example of a photograph. We also analyse\nquantum entanglement between different aspects of meaning in artefacts of\nvisual perception. We confirm its presence by showing that well elected\nexperiments on images retrieved accordingly by Google Images give rise to\nprobabilities and expectation values violating the Clauser Horne Shimony Holt\nversion of Bell's inequalities. We point out how this approach can lead to a\nmathematical description of the meaning content of a visual artefact such as a\nphotograph.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:41:57 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Arguelles", "Jonito Aerts", ""]]}, {"id": "1802.02223", "submitter": "Song-Hwa Kwon", "authors": "Song-Hwa Kwon and Hyeong In Choi and Sung Jin Lee and Nam-Sook Wee", "title": "Seeded Ising Model and Statistical Natures of Human Iris Templates", "comments": "7 pages", "journal-ref": "Phys. Rev. E 98, 032115 (2018)", "doi": "10.1103/PhysRevE.98.032115", "report-no": null, "categories": "stat.AP cs.CV cs.HC physics.data-an q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variant of Ising model, called the Seeded Ising Model, to model\nprobabilistic nature of human iris templates. This model is an Ising model in\nwhich the values at certain lattice points are held fixed throughout Ising\nmodel evolution. Using this we show how to reconstruct the full iris template\nfrom partial information, and we show that about 1/6 of the given template is\nneeded to recover almost all information content of the original one in the\nsense that the resulting Hamming distance is well within the range to assert\ncorrectly the identity of the subject. This leads us to propose the concept of\neffective statistical degree of freedom of iris templates and show it is about\n1/6 of the total number of bits. In particular, for a template of $2048$ bits,\nits effective statistical degree of freedom is about $342$ bits, which\ncoincides very well with the degree of freedom computed by the completely\ndifferent method proposed by Daugman.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 02:32:18 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kwon", "Song-Hwa", ""], ["Choi", "Hyeong In", ""], ["Lee", "Sung Jin", ""], ["Wee", "Nam-Sook", ""]]}, {"id": "1802.02226", "submitter": "Nhat Nguyen", "authors": "Nhat M. Nguyen, Nilanjan Ray", "title": "Generative Adversarial Networks using Adaptive Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing GANs architectures that generate images use transposed\nconvolution or resize-convolution as their upsampling algorithm from lower to\nhigher resolution feature maps in the generator. We argue that this kind of\nfixed operation is problematic for GANs to model objects that have very\ndifferent visual appearances. We propose a novel adaptive convolution method\nthat learns the upsampling algorithm based on the local context at each\nlocation to address this problem. We modify a baseline GANs architecture by\nreplacing normal convolutions with adaptive convolutions in the generator.\nExperiments on CIFAR-10 dataset show that our modified models improve the\nbaseline model by a large margin. Furthermore, our models achieve\nstate-of-the-art performance on CIFAR-10 and STL-10 datasets in the\nunsupervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 23:49:19 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Nguyen", "Nhat M.", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1802.02232", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi, Mahdi Alizadeh", "title": "Feature Based Framework to Detect Diseases, Tumor, and Bleeding in\n  Wireless Capsule Endoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying animal locomotion improves our understanding of motor control and\naids in the treatment of motor impairment. Mice are a premier model of human\ndisease and are the model system of choice for much of basic neuroscience. High\nframe rates (250 Hz) are needed to quantify the kinematics of these running\nrodents. Manual tracking, especially for multiple markers, becomes\ntime-consuming and impossible. Therefore, an automated method is necessary. We\npropose a method to track the paws of the animal in the following manner:\nfirst, segmenting all the possible paws based on color; second, classifying the\nsegmented objects using a support vector machine (SVM) and neural network (NN);\nthird, classifying the objects using the kinematic features of the running\nanimal, coupled with texture features from earlier frames; and finally,\ndetecting and handling collisions to assure the correctness of labelled paws.\nThe proposed method is validated in sixty 1,000 frame video sequences (4\nseconds) captured by four cameras from five mice. The total sensitivity for\ntracking of the front and hind paw is 99.70% using the SVM classifier and\n99.76% using the NN classifier. In addition, we show the feasibility of 3D\nreconstruction using the four camera system.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 16:17:04 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Maghsoudi", "Omid Haji", ""], ["Alizadeh", "Mahdi", ""]]}, {"id": "1802.02240", "submitter": "Avinash Malik", "authors": "Avinash Malik, Tommy Peng, Mark Trew", "title": "A machine learning approach to reconstruction of heart surface\n  potentials from body surface potentials", "comments": "4 pages, 9 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive cardiac catheterisation is a common procedure that is carried out\nbefore surgical intervention. Yet, invasive cardiac diagnostics are full of\nrisks, especially for young children. Decades of research has been conducted on\nthe so called inverse problem of electrocardiography, which can be used to\nreconstruct Heart Surface Potentials (HSPs) from Body Surface Potentials\n(BSPs), for non-invasive diagnostics. State of the art solutions to the inverse\nproblem are unsatisfactory, since the inverse problem is known to be ill-posed.\nIn this paper we propose a novel approach to reconstructing HSPs from BSPs\nusing a Time-Delay Artificial Neural Network (TDANN). We first design the TDANN\narchitecture, and then develop an iterative search space algorithm to find the\nparameters of the TDANN, which results in the best overall HSP prediction. We\nuse real-world recorded BSPs and HSPs from individuals suffering from serious\ncardiac conditions to validate our TDANN. The results are encouraging, in that\ncoefficients obtained by correlating the predicted HSP with the recorded\npatient' HSP approach ideal values.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 22:36:46 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Malik", "Avinash", ""], ["Peng", "Tommy", ""], ["Trew", "Mark", ""]]}, {"id": "1802.02241", "submitter": "Yue Wu", "authors": "Yue Wu, Youzuo Lin, Zheng Zhou, Andrew Delorey", "title": "Seismic-Net: A Deep Densely Connected Neural Network to Detect Seismic\n  Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the risks of large-scale geologic carbon sequestration is the\npotential migration of fluids out of the storage formations. Accurate and fast\ndetection of this fluids migration is not only important but also challenging,\ndue to the large subsurface uncertainty and complex governing physics.\nTraditional leakage detection and monitoring techniques rely on geophysical\nobservations including seismic. However, the resulting accuracy of these\nmethods is limited because of indirect information they provide requiring\nexpert interpretation, therefore yielding in-accurate estimates of leakage\nrates and locations. In this work, we develop a novel machine-learning\ndetection package, named \"Seismic-Net\", which is based on the deep densely\nconnected neural network. To validate the performance of our proposed leakage\ndetection method, we employ our method to a natural analog site at Chimay\\'o,\nNew Mexico. The seismic events in the data sets are generated because of the\neruptions of geysers, which is due to the leakage of $\\mathrm{CO}_\\mathrm{2}$.\nIn particular, we demonstrate the efficacy of our Seismic-Net by formulating\nour detection problem as an event detection problem with time series data. A\nfixed-length window is slid throughout the time series data and we build a deep\ndensely connected network to classify each window to determine if a geyser\nevent is included. Through our numerical tests, we show that our model achieves\nprecision/recall as high as 0.889/0.923. Therefore, our Seismic-Net has a great\npotential for detection of $\\mathrm{CO}_\\mathrm{2}$ leakage.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 17:06:04 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Wu", "Yue", ""], ["Lin", "Youzuo", ""], ["Zhou", "Zheng", ""], ["Delorey", "Andrew", ""]]}, {"id": "1802.02271", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Universal Deep Neural Network Compression", "comments": "NeurIPS 2018 Workshop on Compact Deep Neural Network Representation\n  with Industrial Applications (CDNNRIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate lossy compression of deep neural networks\n(DNNs) by weight quantization and lossless source coding for memory-efficient\ndeployment. Whereas the previous work addressed non-universal scalar\nquantization and entropy coding of DNN weights, we for the first time introduce\nuniversal DNN compression by universal vector quantization and universal source\ncoding. In particular, we examine universal randomized lattice quantization of\nDNNs, which randomizes DNN weights by uniform random dithering before lattice\nquantization and can perform near-optimally on any source without relying on\nknowledge of its probability distribution. Moreover, we present a method of\nfine-tuning vector quantized DNNs to recover the performance loss after\nquantization. Our experimental results show that the proposed universal DNN\ncompression scheme compresses the 32-layer ResNet (trained on CIFAR-10) and the\nAlexNet (trained on ImageNet) with compression ratios of $47.1$ and $42.5$,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 00:39:56 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 01:33:44 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1802.02290", "submitter": "Siyu Chen", "authors": "Siyu Chen and Danping Liao and Yuntao Qian", "title": "Spectral Image Visualization Using Generative Adversarial Networks", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral images captured by satellites and radio-telescopes are analyzed to\nobtain information about geological compositions distributions, distant asters\nas well as undersea terrain. Spectral images usually contain tens to hundreds\nof continuous narrow spectral bands and are widely used in various fields. But\nthe vast majority of those image signals are beyond the visible range, which\ncalls for special visualization technique. The visualizations of spectral\nimages shall convey as much information as possible from the original signal\nand facilitate image interpretation. However, most of the existing visualizatio\nmethods display spectral images in false colors, which contradict with human's\nexperience and expectation. In this paper, we present a novel visualization\ngenerative adversarial network (GAN) to display spectral images in natural\ncolors. To achieve our goal, we propose a loss function which consists of an\nadversarial loss and a structure loss. The adversarial loss pushes our solution\nto the natural image distribution using a discriminator network that is trained\nto differentiate between false-color images and natural-color images. We also\nuse a cycle loss as the structure constraint to guarantee structure\nconsistency. Experimental results show that our method is able to generate\nstructure-preserved and natural-looking visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 02:23:47 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Chen", "Siyu", ""], ["Liao", "Danping", ""], ["Qian", "Yuntao", ""]]}, {"id": "1802.02297", "submitter": "Xian-Feng Han", "authors": "Xian-Feng Han, Shi-Jie Sun, Xiang-Yu Song, Guo-Qiang Xiao", "title": "3D Point Cloud Descriptors in Hand-crafted and Deep Learning Age:\n  State-of-the-Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of inexpensive 3D data acquisition devices has promisingly\nfacilitated the wide availability and popularity of 3D point cloud, which\nattracts more attention to the effective extraction of novel 3D point cloud\ndescriptors for accuracy of the efficiency of 3D computer vision tasks in\nrecent years. However, how to develop discriminative and robust feature\ndescriptors from 3D point cloud remains a challenging task due to their\nintrinsic characteristics. In this paper, we give a comprehensively insightful\ninvestigation of the existing 3D point cloud descriptors. These methods can\nprincipally be divided into two categories according to the advancement of\ndescriptors: hand-crafted based and deep learning-based apporaches, which will\nbe further discussed from the perspective of elaborate classification, their\nadvantages, and limitations. Finally, we present the future research direction\nof the extraction of 3D point cloud descriptors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 03:39:08 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 00:42:27 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["Sun", "Shi-Jie", ""], ["Song", "Xiang-Yu", ""], ["Xiao", "Guo-Qiang", ""]]}, {"id": "1802.02305", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song, Hanwang Zhang, Xiangpeng Li, Lianli Gao, Meng Wang and\n  Richang Hong", "title": "Self-Supervised Video Hashing with Hierarchical Binary Auto-encoder", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2814344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video hash functions are built on three isolated stages: frame\npooling, relaxed learning, and binarization, which have not adequately explored\nthe temporal order of video frames in a joint binary optimization model,\nresulting in severe information loss. In this paper, we propose a novel\nunsupervised video hashing framework dubbed Self-Supervised Video Hashing\n(SSVH), that is able to capture the temporal nature of videos in an end-to-end\nlearning-to-hash fashion. We specifically address two central problems: 1) how\nto design an encoder-decoder architecture to generate binary codes for videos;\nand 2) how to equip the binary codes with the ability of accurate video\nretrieval. We design a hierarchical binary autoencoder to model the temporal\ndependencies in videos with multiple granularities, and embed the videos into\nbinary codes with less computations than the stacked architecture. Then, we\nencourage the binary codes to simultaneously reconstruct the visual content and\nneighborhood structure of the videos. Experiments on two real-world datasets\n(FCVID and YFCC) show that our SSVH method can significantly outperform the\nstate-of-the-art methods and achieve the currently best performance on the task\nof unsupervised video retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 04:31:19 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Song", "Jingkuan", ""], ["Zhang", "Hanwang", ""], ["Li", "Xiangpeng", ""], ["Gao", "Lianli", ""], ["Wang", "Meng", ""], ["Hong", "Richang", ""]]}, {"id": "1802.02312", "submitter": "Kevin Moran P", "authors": "Kevin Moran and Carlos Bernal-C\\'ardenas and Michael Curcio and\n  Richard Bonett and Denys Poshyvanyk", "title": "Machine Learning-Based Prototyping of Graphical User Interfaces for\n  Mobile Apps", "comments": "Accepted to IEEE Transactions on Software Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice for developers of user-facing software to transform a\nmock-up of a graphical user interface (GUI) into code. This process takes place\nboth at an application's inception and in an evolutionary context as GUI\nchanges keep pace with evolving features. Unfortunately, this practice is\nchallenging and time-consuming. In this paper, we present an approach that\nautomates this process by enabling accurate prototyping of GUIs via three\ntasks: detection, classification, and assembly. First, logical components of a\nGUI are detected from a mock-up artifact using either computer vision\ntechniques or mock-up metadata. Then, software repository mining, automated\ndynamic analysis, and deep convolutional neural networks are utilized to\naccurately classify GUI-components into domain-specific types (e.g.,\ntoggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates\na suitable hierarchical GUI structure from which a prototype application can be\nautomatically assembled. We implemented this approach for Android in a system\ncalled ReDraw. Our evaluation illustrates that ReDraw achieves an average\nGUI-component classification accuracy of 91% and assembles prototype\napplications that closely mirror target mock-ups in terms of visual affinity\nwhile exhibiting reasonable code structure. Interviews with industrial\npractitioners illustrate ReDraw's potential to improve real development\nworkflows.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 05:32:59 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 03:12:06 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Moran", "Kevin", ""], ["Bernal-C\u00e1rdenas", "Carlos", ""], ["Curcio", "Michael", ""], ["Bonett", "Richard", ""], ["Poshyvanyk", "Denys", ""]]}, {"id": "1802.02326", "submitter": "Xin Chen", "authors": "Xin Chen and Hua Zhou and Yuxiang Gao and Yu Zhu", "title": "A Novel Co-design Peta-scale Heterogeneous Cluster for Deep Learning\n  Training", "comments": "23 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale deep Convolution Neural Networks (CNNs) increasingly demands the\ncomputing power. It is key for researchers to own a great powerful computing\nplatform to leverage deep learning (DL) advancing.On the other hand, as the\ncommonly-used accelerator, the commodity GPUs cards of new generations are more\nand more expensive. Consequently, it is of importance to design an affordable\ndistributed heterogeneous system that provides powerful computational capacity\nand develop a well-suited software that efficiently utilizes its computational\ncapacity. In this paper, we present our co-design distributed system including\na peta-scale GPU cluster, called \"Manoa\". Based on properties and topology of\nManoa, we first propose job server framework and implement it, named\n\"MiMatrix\". The central node of MiMatrix, referred to as the job server,\nundertakes all of controlling, scheduling and monitoring, and I/O tasks without\nweight data transfer for AllReduce processing in each iteration. Therefore,\nMiMatrix intrinsically solves the bandwidth bottleneck of central node in\nparameter server framework that is widely used in distributed DL tasks.\nMeanwhile, we also propose a new AllReduce algorithm, GPUDirect RDMA-Aware\nAllReduce~(GDRAA), in which both computation and handshake message are O(1) and\nthe number of synchronization is two in each iteration that is a theoretical\nminimum number. Owe to the dedicated co-design distributed system, MiMatrix\nefficiently makes use of the Manoa's computational capacity and bandwidth. We\nbenchmark Manoa Resnet50 and Resenet101 on Imagenet-1K dataset. Some of results\nhave demonstrated state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 07:20:42 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 05:25:06 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 23:01:23 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chen", "Xin", ""], ["Zhou", "Hua", ""], ["Gao", "Yuxiang", ""], ["Zhu", "Yu", ""]]}, {"id": "1802.02341", "submitter": "Leonid Blouvshtein", "authors": "Leonid Blouvshtein, Daniel Cohen-Or", "title": "Outlier Detection for Robust Multi-dimensional Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional scaling (MDS) plays a central role in data-exploration,\ndimensionality reduction and visualization. State-of-the-art MDS algorithms are\nnot robust to outliers, yielding significant errors in the embedding even when\nonly a handful of outliers are present. In this paper, we introduce a technique\nto detect and filter outliers based on geometric reasoning. We test the\nvalidity of triangles formed by three points, and mark a triangle as broken if\nits triangle inequality does not hold. The premise of our work is that unlike\ninliers, outlier distances tend to break many triangles. Our method is tested\nand its performance is evaluated on various datasets and distributions of\noutliers. We demonstrate that for a reasonable amount of outliers, e.g., under\n$20\\%$, our method is effective, and leads to a high embedding quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 08:16:56 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Blouvshtein", "Leonid", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1802.02347", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof Bertram, Robert Klopfleisch, Andreas Maier", "title": "SlideRunner - A Tool for Massive Cell Annotations in Whole Slide Images", "comments": "6 pages, submitted to Bildverarbeitung in der Medizin 2018", "journal-ref": "Bildverarbeitung f\\\"ur die Medizin 2018", "doi": "10.1007/978-3-662-56537-7_81", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale image data such as digital whole-slide histology images pose a\nchallenging task at annotation software solutions. Today, a number of good\nsolutions with varying scopes exist. For cell annotation, however, we find that\nmany do not match the prerequisites for fast annotations. Especially in the\nfield of mitosis detection, it is assumed that detection accuracy could\nsignificantly benefit from larger annotation databases that are currently\nhowever very troublesome to produce. Further, multiple independent (blind)\nexpert labels are a big asset for such databases, yet there is currently no\ntool for this kind of annotation available.\n  To ease this tedious process of expert annotation and grading, we introduce\nSlideRunner, an open source annotation and visualization tool for digital\nhistopathology, developed in close cooperation with two pathologists.\nSlideRunner is capable of setting annotations like object centers (for e.g.\ncells) as well as object boundaries (e.g. for tumor outlines). It provides\nsingle-click annotations as well as a blind mode for multi-annotations, where\nthe expert is directly shown the microscopy image containing the cells that he\nhas not yet rated.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 08:29:17 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1802.02375", "submitter": "Yoshihiro Yamada", "authors": "Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba and Koichi Kise", "title": "ShakeDrop Regularization for Deep Residual Learning", "comments": null, "journal-ref": "IEEE Access, 7, 1, pp.186126-186136 (2019)", "doi": "10.1109/ACCESS.2019.2960566", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting is a crucial problem in deep neural networks, even in the latest\nnetwork architectures. In this paper, to relieve the overfitting effect of\nResNet and its improvements (i.e., Wide ResNet, PyramidNet, and ResNeXt), we\npropose a new regularization method called ShakeDrop regularization. ShakeDrop\nis inspired by Shake-Shake, which is an effective regularization method, but\ncan be applied to ResNeXt only. ShakeDrop is more effective than Shake-Shake\nand can be applied not only to ResNeXt but also ResNet, Wide ResNet, and\nPyramidNet. An important key is to achieve stability of training. Because\neffective regularization often causes unstable training, we introduce a\ntraining stabilizer, which is an unusual use of an existing regularizer.\nThrough experiments under various conditions, we demonstrate the conditions\nunder which ShakeDrop works well.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 10:23:54 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 15:04:55 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 07:14:51 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yamada", "Yoshihiro", ""], ["Iwamura", "Masakazu", ""], ["Akiba", "Takuya", ""], ["Kise", "Koichi", ""]]}, {"id": "1802.02398", "submitter": "Hongmin Li", "authors": "Hongmin Li, Guoqi Li, Hanchao Liu, Luping Shi", "title": "Super-resolution of spatiotemporal event-stream image captured by the\n  asynchronous temporal contrast vision sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) is a useful technology to generate a high-resolution\n(HR) visual output from the low-resolution (LR) visual inputs overcoming the\nphysical limitations of the cameras. However, SR has not been applied to\nenhance the resolution of spatiotemporal event-stream images captured by the\nframe-free dynamic vision sensors (DVSs). SR of event-stream image is\nfundamentally different from existing frame-based schemes since basically each\npixel value of DVS images is an event sequence. In this work, a two-stage\nscheme is proposed to solve the SR problem of the spatiotemporal event-stream\nimage. We use a nonhomogeneous Poisson point process to model the event\nsequence, and sample the events of each pixel by simulating a nonhomogeneous\nPoisson process according to the specified event number and rate function.\nFirstly, the event number of each pixel of the HR DVS image is determined with\na sparse signal representation based method to obtain the HR event-count map\nfrom that of the LR DVS recording. The rate function over time line of the\npoint process of each HR pixel is computed using a spatiotemporal filter on the\ncorresponding LR neighbor pixels. Secondly, the event sequence of each new\npixel is generated with a thinning based event sampling algorithm. Two metrics\nare proposed to assess the event-stream SR results. The proposed method is\ndemonstrated through obtaining HR event-stream images from a series of DVS\nrecordings with the proposed method. Results show that the upscaled HR event\nstreams has perceptually higher spatial texture detail than the LR DVS images.\nBesides, the temporal properties of the upscaled HR event streams match that of\nthe original input very well. This work enables many potential applications of\nevent-based vision.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 12:14:56 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 10:39:03 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Li", "Hongmin", ""], ["Li", "Guoqi", ""], ["Liu", "Hanchao", ""], ["Shi", "Luping", ""]]}, {"id": "1802.02422", "submitter": "Dmitry Baranchuk", "authors": "Dmitry Baranchuk, Artem Babenko, Yury Malkov", "title": "Revisiting the Inverted Indices for Billion-Scale Approximate Nearest\n  Neighbors", "comments": "Paper accepted to ECCV 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of billion-scale nearest neighbor search. The\nstate-of-the-art retrieval systems for billion-scale databases are currently\nbased on the inverted multi-index, the recently proposed generalization of the\ninverted index structure. The multi-index provides a very fine-grained\npartition of the feature space that allows extracting concise and accurate\nshort-lists of candidates for the search queries. In this paper, we argue that\nthe potential of the simple inverted index was not fully exploited in previous\nworks and advocate its usage both for the highly-entangled deep descriptors and\nrelatively disentangled SIFT descriptors. We introduce a new retrieval system\nthat is based on the inverted index and outperforms the multi-index by a large\nmargin for the same memory consumption and construction complexity. For\nexample, our system achieves the state-of-the-art recall rates several times\nfaster on the dataset of one billion deep descriptors compared to the efficient\nimplementation of the inverted multi-index from the FAISS library.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 14:01:04 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 11:52:39 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Baranchuk", "Dmitry", ""], ["Babenko", "Artem", ""], ["Malkov", "Yury", ""]]}, {"id": "1802.02423", "submitter": "Ethan C Jackson", "authors": "Ethan C. Jackson, James Alexander Hughes, Mark Daley", "title": "On the Generalizability of Linear and Non-Linear Region of\n  Interest-Based Multivariate Regression Models for fMRI Data", "comments": "Pre-print of paper submitted for review to 2018 IEEE CIBCB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to conventional, univariate analysis, various types of\nmultivariate analysis have been applied to functional magnetic resonance\nimaging (fMRI) data. In this paper, we compare two contemporary approaches for\nmultivariate regression on task-based fMRI data: linear regression with ridge\nregularization and non-linear symbolic regression using genetic programming.\nThe data for this project is representative of a contemporary fMRI experimental\ndesign for visual stimuli. Linear and non-linear models were generated for 10\nsubjects, with another 4 withheld for validation. Model quality is evaluated by\ncomparing $R$ scores (Pearson product-moment correlation) in various contexts,\nincluding single run self-fit, within-subject generalization, and\nbetween-subject generalization. Propensity for modelling strategies to overfit\nis estimated using a separate resting state scan. Results suggest that neither\nmethod is objectively or inherently better than the other.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 17:39:17 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Jackson", "Ethan C.", ""], ["Hughes", "James Alexander", ""], ["Daley", "Mark", ""]]}, {"id": "1802.02427", "submitter": "Yue Wu", "authors": "Lele Chen, Yue Wu, Adora M. DSouza, Anas Z. Abidin, Axel Wismuller,\n  Chenliang Xu", "title": "MRI Tumor Segmentation with Densely Connected 3D CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioma is one of the most common and aggressive types of primary brain\ntumors. The accurate segmentation of subcortical brain structures is crucial to\nthe study of gliomas in that it helps the monitoring of the progression of\ngliomas and aids the evaluation of treatment outcomes. However, the large\namount of required human labor makes it difficult to obtain the manually\nsegmented Magnetic Resonance Imaging (MRI) data, limiting the use of precise\nquantitative measurements in the clinical practice. In this work, we try to\naddress this problem by developing a 3D Convolutional Neural Network~(3D CNN)\nbased model to automatically segment gliomas. The major difficulty of our\nsegmentation model comes with the fact that the location, structure, and shape\nof gliomas vary significantly among different patients. In order to accurately\nclassify each voxel, our model captures multi-scale contextual information by\nextracting features from two scales of receptive fields. To fully exploit the\ntumor structure, we propose a novel architecture that hierarchically segments\ndifferent lesion regions of the necrotic and non-enhancing tumor~(NCR/NET),\nperitumoral edema~(ED) and GD-enhancing tumor~(ET). Additionally, we utilize\ndensely connected convolutional blocks to further boost the performance. We\ntrain our model with a patch-wise training schema to mitigate the class\nimbalance problem. The proposed method is validated on the BraTS 2017 dataset\nand it achieves Dice scores of 0.72, 0.83 and 0.81 for the complete tumor,\ntumor core and enhancing tumor, respectively. These results are comparable to\nthe reported state-of-the-art results, and our method is better than existing\n3D-based methods in terms of compactness, time and space efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 05:16:26 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 15:59:15 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Chen", "Lele", ""], ["Wu", "Yue", ""], ["DSouza", "Adora M.", ""], ["Abidin", "Anas Z.", ""], ["Wismuller", "Axel", ""], ["Xu", "Chenliang", ""]]}, {"id": "1802.02436", "submitter": "Joshua Chacksfield", "authors": "Alexey Chaplygin and Joshua Chacksfield", "title": "Stochastic Deconvolutional Neural Network Ensemble Training on\n  Generative Pseudo-Adversarial Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of Generative Adversarial Networks is a difficult task mainly\ndue to the nature of the networks. One such issue is when the generator and\ndiscriminator start oscillating, rather than converging to a fixed point.\nAnother case can be when one agent becomes more adept than the other which\nresults in the decrease of the other agent's ability to learn, reducing the\nlearning capacity of the system as a whole. Additionally, there exists the\nproblem of Mode Collapse which involves the generators output collapsing to a\nsingle sample or a small set of similar samples. To train GANs a careful\nselection of the architecture that is used along with a variety of other\nmethods to improve training. Even when applying these methods there is low\nstability of training in relation to the parameters that are chosen. Stochastic\nensembling is suggested as a method for improving the stability while training\nGANs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 14:36:15 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Chaplygin", "Alexey", ""], ["Chacksfield", "Joshua", ""]]}, {"id": "1802.02438", "submitter": "Benyamin Ghojogh", "authors": "Hoda Mohammadzade, Amirhossein Sayyafan, Benyamin Ghojogh", "title": "Pixel-Level Alignment of Facial Images for High Accuracy Recognition\n  Using Ensemble of Patches", "comments": "11 pages, 16 figures, 1 table, key-words: face recognition, pixel\n  alignment, geometrical transformation, pose and expression variation,\n  ensemble of patches, fusion of texture and geometry", "journal-ref": null, "doi": "10.1364/JOSAA.35.001149", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variation of pose, illumination and expression makes face recognition\nstill a challenging problem. As a pre-processing in holistic approaches, faces\nare usually aligned by eyes. The proposed method tries to perform a pixel\nalignment rather than eye-alignment by mapping the geometry of faces to a\nreference face while keeping their own textures. The proposed geometry\nalignment not only creates a meaningful correspondence among every pixel of all\nfaces, but also removes expression and pose variations effectively. The\ngeometry alignment is performed pixel-wise, i.e., every pixel of the face is\ncorresponded to a pixel of the reference face. In the proposed method, the\ninformation of intensity and geometry of faces are separated properly, trained\nby separate classifiers, and finally fused together to recognize human faces.\nExperimental results show a great improvement using the proposed method in\ncomparison to eye-aligned recognition. For instance, at the false acceptance\nrate of 0.001, the recognition rates are respectively improved by 24% and 33%\nin Yale and AT&T datasets. In LFW dataset, which is a challenging big dataset,\nimprovement is 20% at FAR of 0.1.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 14:38:37 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mohammadzade", "Hoda", ""], ["Sayyafan", "Amirhossein", ""], ["Ghojogh", "Benyamin", ""]]}, {"id": "1802.02488", "submitter": "Yuxin Peng", "authors": "Jian Zhang, Yuxin Peng and Mingkuan Yuan", "title": "SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial\n  Network", "comments": "12 pages, submitted to IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Supervised cross-modal hashing methods have achieved considerable\nprogress by incorporating semantic side information. However, they mainly have\ntwo limitations: (1) Heavily rely on large-scale labeled cross-modal training\ndata which are labor intensive and hard to obtain. (2) Ignore the rich\ninformation contained in the large amount of unlabeled data across different\nmodalities, especially the margin examples that are easily to be incorrectly\nretrieved, which can help to model the correlations. To address these problems,\nin this paper we propose a novel Semi-supervised Cross-Modal Hashing approach\nby Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's\nability for modeling data distributions to promote cross-modal hashing learning\nin an adversarial way. The main contributions can be summarized as follows: (1)\nWe propose a novel generative adversarial network for cross-modal hashing. In\nour proposed SCH-GAN, the generative model tries to select margin examples of\none modality from unlabeled data when giving a query of another modality. While\nthe discriminative model tries to distinguish the selected examples and true\npositive examples of the query. These two models play a minimax game so that\nthe generative model can promote the hashing performance of discriminative\nmodel. (2) We propose a reinforcement learning based algorithm to drive the\ntraining of proposed SCH-GAN. The generative model takes the correlation score\npredicted by discriminative model as a reward, and tries to select the examples\nclose to the margin to promote discriminative model by maximizing the margin\nbetween positive and negative data. Experiments on 3 widely-used datasets\nverify the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 15:45:12 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Jian", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1802.02522", "submitter": "Amir Rasouli", "authors": "Amir Rasouli and John K. Tsotsos", "title": "Joint Attention in Driver-Pedestrian Interaction: from Theory to\n  Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, one of the major challenges that autonomous vehicles are facing is the\nability to drive in urban environments. Such a task requires communication\nbetween autonomous vehicles and other road users in order to resolve various\ntraffic ambiguities. The interaction between road users is a form of\nnegotiation in which the parties involved have to share their attention\nregarding a common objective or a goal (e.g. crossing an intersection), and\ncoordinate their actions in order to accomplish it. In this literature review\nwe aim to address the interaction problem between pedestrians and drivers (or\nvehicles) from joint attention point of view. More specifically, we will\ndiscuss the theoretical background behind joint attention, its application to\ntraffic interaction and practical approaches to implementing joint attention\nfor autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:03:11 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:39:37 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Rasouli", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1802.02531", "submitter": "Loris Nanni", "authors": "Alessandra Lumini, Loris Nanni", "title": "Fair comparison of skin detection approaches on publicly available\n  datasets", "comments": null, "journal-ref": "Expert Systems with Applications Volume 160, 1 December 2020,\n  113677", "doi": "10.1016/j.eswa.2020.113677", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin detection is the process of discriminating skin and non-skin regions in\na digital image and it is widely used in several applications ranging from hand\ngesture analysis to track body parts and face detection. Skin detection is a\nchallenging problem which has drawn extensive attention from the research\ncommunity, nevertheless a fair comparison among approaches is very difficult\ndue to the lack of a common benchmark and a unified testing protocol. In this\nwork, we investigate the most recent researches in this field and we propose a\nfair comparison among approaches using several different datasets. The major\ncontributions of this work are an exhaustive literature review of skin color\ndetection approaches, a framework to evaluate and combine different skin\ndetector approaches, whose source code is made freely available for future\nresearch, and an extensive experimental comparison among several recent methods\nwhich have also been used to define an ensemble that works well in many\ndifferent problems. Experiments are carried out in 10 different datasets\nincluding more than 10000 labelled images: experimental results confirm that\nthe best method here proposed obtains a very good performance with respect to\nother stand-alone approaches, without requiring ad hoc parameter tuning. A\nMATLAB version of the framework for testing and of the methods proposed in this\npaper will be freely available from https://github.com/LorisNanni\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:16:46 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 14:58:57 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 22:09:40 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 17:54:18 GMT"}, {"version": "v5", "created": "Wed, 3 Jun 2020 20:35:36 GMT"}, {"version": "v6", "created": "Fri, 10 Jul 2020 21:31:26 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lumini", "Alessandra", ""], ["Nanni", "Loris", ""]]}, {"id": "1802.02532", "submitter": "Thomas Corcoran", "authors": "Thomas Corcoran, Rafael Zamora-Resendiz, Xinlian Liu, Silvia Crivelli", "title": "A Spatial Mapping Algorithm with Applications in Deep Learning-Based\n  Structure Classification", "comments": "17 Pages, 4 figures, 7 tables. The first two authors contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN)-based machine learning systems have made\nbreakthroughs in feature extraction and image recognition tasks in two\ndimensions (2D). Although there is significant ongoing work to apply CNN\ntechnology to domains involving complex 3D data, the success of such efforts\nhas been constrained, in part, by limitations in data representation\ntechniques. Most current approaches rely upon low-resolution 3D models,\nstrategic limitation of scope in the 3D space, or the application of lossy\nprojection techniques to allow for the use of 2D CNNs. To address this issue,\nwe present a mapping algorithm that converts 3D structures to 2D and 1D data\ngrids by mapping a traversal of a 3D space-filling curve to the traversal of\ncorresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs\ntrained on data encoded with our method versus comparable volumetric CNNs\noperating upon raw 3D data from a popular benchmarking dataset. Our experiments\ndemonstrate that both 2D and 1D representations of 3D data generated via our\nmethod preserve a significant proportion of the 3D data's features in forms\nlearnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D\ndata into lower-dimensional representations allows for decreased CNN training\ntime cost, increased original 3D model rendering resolutions, and supports\nincreased numbers of data channels when compared to purely volumetric\napproaches. This demonstration is accomplished in the context of a structural\nbiology classification task wherein we train 3D, 2D, and 1D CNNs on examples of\ntwo homologous branches within the Ras protein family. The essential\ncontribution of this paper is the introduction of a dimensionality-reduction\nmethod that may ease the application of powerful deep learning tools to domains\ncharacterized by complex structural data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:18:33 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 20:38:14 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Corcoran", "Thomas", ""], ["Zamora-Resendiz", "Rafael", ""], ["Liu", "Xinlian", ""], ["Crivelli", "Silvia", ""]]}, {"id": "1802.02534", "submitter": "Dario Zanca", "authors": "Dario Zanca, Valeria Serchi, Pietro Piu, Francesca Rosini and\n  Alessandra Rufa", "title": "FixaTons: A collection of Human Fixations Datasets and Metrics for\n  Scanpath Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last three decades, human visual attention has been a topic of great\ninterest in various disciplines. In computer vision, many models have been\nproposed to predict the distribution of human fixations on a visual stimulus.\nRecently, thanks to the creation of large collections of data, machine learning\nalgorithms have obtained state-of-the-art performance on the task of saliency\nmap estimation. On the other hand, computational models of scanpath are much\nless studied. Works are often only descriptive or task specific. This is due to\nthe fact that the scanpath is harder to model because it must include the\ndescription of a dynamic. General purpose computational models are present in\nthe literature, but are then evaluated in tasks of saliency prediction, losing\ntherefore information about the dynamics and the behaviour. In addition, two\ntechnical reasons have limited the research. The first reason is the lack of\nrobust and uniformly used set of metrics to compare the similarity between\nscanpath. The second reason is the lack of sufficiently large and varied\nscanpath datasets. In this report we want to help in both directions. We\npresent FixaTons, a large collection of datasets human scanpaths (temporally\nordered sequences of fixations) and saliency maps. It comes along with a\nsoftware library for easy data usage, statistics calculation and implementation\nof metrics for scanpath and saliency prediction evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:20:31 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 09:48:42 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 15:20:59 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Zanca", "Dario", ""], ["Serchi", "Valeria", ""], ["Piu", "Pietro", ""], ["Rosini", "Francesca", ""], ["Rufa", "Alessandra", ""]]}, {"id": "1802.02568", "submitter": "Hamid Izadinia", "authors": "Hamid Izadinia, Pierre Garrigues", "title": "VISER: Visual Self-Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the use of large set of unlabeled images as a source\nof regularization data for learning robust visual representation. Given a\nvisual model trained by a labeled dataset in a supervised fashion, we augment\nour training samples by incorporating large number of unlabeled data and train\na semi-supervised model. We demonstrate that our proposed learning approach\nleverages an abundance of unlabeled images and boosts the visual recognition\nperformance which alleviates the need to rely on large labeled datasets for\nlearning robust representation. To increment the number of image instances\nneeded to learn robust visual models in our approach, each labeled image\npropagates its label to its nearest unlabeled image instances. These retrieved\nunlabeled images serve as local perturbations of each labeled image to perform\nVisual Self-Regularization (VISER). To retrieve such visual self regularizers,\nwe compute the cosine similarity in a semantic space defined by the penultimate\nlayer in a fully convolutional neural network. We use the publicly available\nYahoo Flickr Creative Commons 100M dataset as the source of our unlabeled image\nset and propose a distributed approximate nearest neighbor algorithm to make\nretrieval practical at that scale. Using the labeled instances and their\nregularizer samples we show that we significantly improve object categorization\nand localization performance on the MS COCO and Visual Genome datasets where\nobjects appear in context.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:55:01 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Izadinia", "Hamid", ""], ["Garrigues", "Pierre", ""]]}, {"id": "1802.02571", "submitter": "Ning Tan", "authors": "Jiang Yun and Tan Ning and Zhang Hai and Peng Tingting", "title": "Bitewing Radiography Semantic Segmentation Base on Conditional\n  Generative Adversarial Nets", "comments": "12pages, in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Segmentation of bitewing radiograpy images is a very challenging\ntask. The focus of the study is to segment it into caries, enamel, dentin,\npulp, crowns, restoration and root canal treatments. The main method of\nsemantic segmentation of bitewing radiograpy images at this stage is the\nU-shaped deep convolution neural network, but its accuracy is low. in order to\nimprove the accuracy of semantic segmentation of bitewing radiograpy images,\nthis paper proposes the use of Conditional Generative Adversarial network\n(cGAN) combined with U-shaped network structure (U-Net) approach to semantic\nsegmentation of bitewing radiograpy images. The experimental results show that\nthe accuracy of cGAN combined with U-Net is 69.7%, which is 13.3% higher than\nthe accuracy of u-shaped deep convolution neural network of 56.4%.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 05:44:48 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Yun", "Jiang", ""], ["Ning", "Tan", ""], ["Hai", "Zhang", ""], ["Tingting", "Peng", ""]]}, {"id": "1802.02595", "submitter": "Hanfei Sun", "authors": "Hanfei Sun, Yiming Luo, Ziang Lu", "title": "Unsupervised Typography Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods in Chinese typography synthesis view characters as an\nassembly of radicals and strokes, but they rely on manual definition of the key\npoints, which is still time-costing. Some recent work on computer vision\nproposes a brand new approach: to treat every Chinese character as an\nindependent and inseparable image, so the pre-processing and post-processing of\neach character can be avoided. Then with a combination of a transfer network\nand a discriminating network, one typography can be well transferred to\nanother. Despite the quite satisfying performance of the model, the training\nprocess requires to be supervised, which means in the training data each\ncharacter in the source domain and the target domain needs to be perfectly\npaired. Sometimes the pairing is time-costing, and sometimes there is no\nperfect pairing, such as the pairing between traditional Chinese and simplified\nChinese characters. In this paper, we proposed an unsupervised typography\ntransfer method which doesn't need pairing.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:03:42 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Sun", "Hanfei", ""], ["Luo", "Yiming", ""], ["Lu", "Ziang", ""]]}, {"id": "1802.02598", "submitter": "Matthew Klawonn", "authors": "Matthew Klawonn and Eric Heim", "title": "Generating Triples with Adversarial Networks for Scene Graph\n  Construction", "comments": "Accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by successes in deep learning, computer vision research has begun to\nmove beyond object detection and image classification to more sophisticated\ntasks like image captioning or visual question answering. Motivating such\nendeavors is the desire for models to capture not only objects present in an\nimage, but more fine-grained aspects of a scene such as relationships between\nobjects and their attributes. Scene graphs provide a formal construct for\ncapturing these aspects of an image. Despite this, there have been only a few\nrecent efforts to generate scene graphs from imagery. Previous works limit\nthemselves to settings where bounding box information is available at train\ntime and do not attempt to generate scene graphs with attributes. In this paper\nwe propose a method, based on recent advancements in Generative Adversarial\nNetworks, to overcome these deficiencies. We take the approach of first\ngenerating small subgraphs, each describing a single statement about a scene\nfrom a specific region of the input image chosen using an attention mechanism.\nBy doing so, our method is able to produce portions of the scene graphs with\nattribute information without the need for bounding box labels. Then, the\ncomplete scene graph is constructed from these subgraphs. We show that our\nmodel improves upon prior work in scene graph generation on state-of-the-art\ndata sets and accepted metrics. Further, we demonstrate that our model is\ncapable of handling a larger vocabulary size than prior work has attempted.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:12:31 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Klawonn", "Matthew", ""], ["Heim", "Eric", ""]]}, {"id": "1802.02601", "submitter": "Yusuke Uchida", "authors": "Yuki Nagai, Yusuke Uchida, Shigeyuki Sakazawa, Shin'ichi Satoh", "title": "Digital Watermarking for Deep Neural Networks", "comments": "This is a pre-print of an article published in International Journal\n  of Multimedia Information Retrieval. The final authenticated version is\n  available online at: https://doi.org/10.1007/s13735-018-0147-1 . arXiv admin\n  note: substantial text overlap with arXiv:1701.04082", "journal-ref": null, "doi": "10.1007/s13735-018-0147-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks have made tremendous progress in the area of\nmultimedia representation, training neural models requires a large amount of\ndata and time. It is well-known that utilizing trained models as initial\nweights often achieves lower training error than neural networks that are not\npre-trained. A fine-tuning step helps to reduce both the computational cost and\nimprove performance. Therefore, sharing trained models has been very important\nfor the rapid progress of research and development. In addition, trained models\ncould be important assets for the owner(s) who trained them, hence we regard\ntrained models as intellectual property. In this paper, we propose a digital\nwatermarking technology for ownership authorization of deep neural networks.\nFirst, we formulate a new problem: embedding watermarks into deep neural\nnetworks. We also define requirements, embedding situations, and attack types\non watermarking in deep neural networks. Second, we propose a general framework\nfor embedding a watermark in model parameters, using a parameter regularizer.\nOur approach does not impair the performance of networks into which a watermark\nis placed because the watermark is embedded while training the host network.\nFinally, we perform comprehensive experiments to reveal the potential of\nwatermarking deep neural networks as the basis of this new research effort. We\nshow that our framework can embed a watermark during the training of a deep\nneural network from scratch, and during fine-tuning and distilling, without\nimpairing its performance. The embedded watermark does not disappear even after\nfine-tuning or parameter pruning; the watermark remains complete even after 65%\nof parameters are pruned.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 05:32:36 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Nagai", "Yuki", ""], ["Uchida", "Yusuke", ""], ["Sakazawa", "Shigeyuki", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1802.02604", "submitter": "Guha Balakrishnan", "authors": "Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V.\n  Dalca", "title": "An Unsupervised Learning Model for Deformable Medical Image Registration", "comments": "9 pages, in CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00964", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast learning-based algorithm for deformable, pairwise 3D\nmedical image registration. Current registration methods optimize an objective\nfunction independently for each pair of images, which can be time-consuming for\nlarge data. We define registration as a parametric function, and optimize its\nparameters given a set of images from a collection of interest. Given a new\npair of scans, we can quickly compute a registration field by directly\nevaluating the function using the learned parameters. We model this function\nusing a convolutional neural network (CNN), and use a spatial transform layer\nto reconstruct one image from another while imposing smoothness constraints on\nthe registration field. The proposed method does not require supervised\ninformation such as ground truth registration fields or anatomical landmarks.\nWe demonstrate registration accuracy comparable to state-of-the-art 3D image\nregistration, while operating orders of magnitude faster in practice. Our\nmethod promises to significantly speed up medical image analysis and processing\npipelines, while facilitating novel directions in learning-based registration\nand its applications. Our code is available at\nhttps://github.com/balakg/voxelmorph .\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:23:57 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 17:43:35 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 21:31:35 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Balakrishnan", "Guha", ""], ["Zhao", "Amy", ""], ["Sabuncu", "Mert R.", ""], ["Guttag", "John", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "1802.02608", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Theodore Josue, Md Nayim Rahman, Will Mitchell,\n  Chris Yakopcic, and Tarek M. Taha", "title": "Deep Versus Wide Convolutional Neural Networks for Object Recognition on\n  Neuromorphic System", "comments": "8 pages, 14 figures. Submitted to International Joint Conference on\n  Neural Networks (IJCNN) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decade, special purpose computing systems, such as Neuromorphic\ncomputing, have become very popular in the field of computer vision and machine\nlearning for classification tasks. In 2015, IBM's released the TrueNorth\nNeuromorphic system, kick-starting a new era of Neuromorphic computing.\nAlternatively, Deep Learning approaches such as Deep Convolutional Neural\nNetworks (DCNN) show almost human-level accuracies for detection and\nclassification tasks. IBM's 2016 release of a deep learning framework for\nDCNNs, called Energy Efficient Deep Neuromorphic Networks (Eedn). Eedn shows\npromise for delivering high accuracies across a number of different benchmarks,\nwhile consuming very low power, using IBM's TrueNorth chip. However, there are\nmany things that remained undiscovered using the Eedn framework for\nclassification tasks on a Neuromorphic system. In this paper, we have\nempirically evaluated the performance of different DCNN architectures\nimplemented within the Eedn framework. The goal of this work was discover the\nmost efficient way to implement DCNN models for object classification tasks\nusing the TrueNorth system. We performed our experiments using benchmark data\nsets such as MNIST, COIL 20, and COIL 100. The experimental results show very\npromising classification accuracies with very low power consumption on IBM's\nNS1e Neurosynaptic system. The results show that for datasets with large\nnumbers of classes, wider networks perform better when compared to deep\nnetworks comprised of nearly the same core complexity on IBM's TrueNorth\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:32:40 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Josue", "Theodore", ""], ["Rahman", "Md Nayim", ""], ["Mitchell", "Will", ""], ["Yakopcic", "Chris", ""], ["Taha", "Tarek M.", ""]]}, {"id": "1802.02611", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff,\n  Hartwig Adam", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image\n  Segmentation", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial pyramid pooling module or encode-decoder structure are used in deep\nneural networks for semantic segmentation task. The former networks are able to\nencode multi-scale contextual information by probing the incoming features with\nfilters or pooling operations at multiple rates and multiple effective\nfields-of-view, while the latter networks can capture sharper object boundaries\nby gradually recovering the spatial information. In this work, we propose to\ncombine the advantages from both methods. Specifically, our proposed model,\nDeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module\nto refine the segmentation results especially along object boundaries. We\nfurther explore the Xception model and apply the depthwise separable\nconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,\nresulting in a faster and stronger encoder-decoder network. We demonstrate the\neffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,\nachieving the test set performance of 89.0\\% and 82.1\\% without any\npost-processing. Our paper is accompanied with a publicly available reference\nimplementation of the proposed models in Tensorflow at\n\\url{https://github.com/tensorflow/models/tree/master/research/deeplab}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:37:11 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 22:11:04 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2018 20:41:10 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Zhu", "Yukun", ""], ["Papandreou", "George", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""]]}, {"id": "1802.02615", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Adam T Moody, Naoya Maruyama, Brian C Van Essen, and\n  Tarek M. Taha", "title": "Effective Quantization Approaches for Recurrent Neural Networks", "comments": "8 pages, 23 figures,Submitted to International Joint Conference on\n  Neural Networks (IJCNN) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning, and in particular Recurrent Neural Networks (RNN) have shown\nsuperior accuracy in a large variety of tasks including machine translation,\nlanguage understanding, and movie frame generation. However, these deep\nlearning approaches are very expensive in terms of computation. In most cases,\nGraphic Processing Units (GPUs) are in used for large scale implementations.\nMeanwhile, energy efficient RNN approaches are proposed for deploying solutions\non special purpose hardware including Field Programming Gate Arrays (FPGAs) and\nmobile platforms. In this paper, we propose an effective quantization approach\nfor Recurrent Neural Networks (RNN) techniques including Long Short Term Memory\n(LSTM), Gated Recurrent Units (GRU), and Convolutional Long Short Term Memory\n(ConvLSTM). We have implemented different quantization methods including Binary\nConnect {-1, 1}, Ternary Connect {-1, 0, 1}, and Quaternary Connect {-1, -0.5,\n0.5, 1}. These proposed approaches are evaluated on different datasets for\nsentiment analysis on IMDB and video frame predictions on the moving MNIST\ndataset. The experimental results are compared against the full precision\nversions of the LSTM, GRU, and ConvLSTM. They show promising results for both\nsentiment analysis and video frame prediction.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:43:01 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Moody", "Adam T", ""], ["Maruyama", "Naoya", ""], ["Van Essen", "Brian C", ""], ["Taha", "Tarek M.", ""]]}, {"id": "1802.02627", "submitter": "Abhronil Sengupta", "authors": "Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, Kaushik Roy", "title": "Going Deeper in Spiking Neural Networks: VGG and Residual Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, Spiking Neural Networks (SNNs) have become popular\nas a possible pathway to enable low-power event-driven neuromorphic hardware.\nHowever, their application in machine learning have largely been limited to\nvery shallow neural network architectures for simple problems. In this paper,\nwe propose a novel algorithmic technique for generating an SNN with a deep\narchitecture, and demonstrate its effectiveness on complex visual recognition\nproblems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and\nResidual network architectures, with significantly better accuracy than the\nstate-of-the-art. Finally, we present analysis of the sparse event-driven\ncomputations to demonstrate reduced hardware overhead when operating in the\nspiking domain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 20:58:10 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 22:44:53 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 00:22:39 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2019 18:31:03 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Sengupta", "Abhronil", ""], ["Ye", "Yuting", ""], ["Wang", "Robert", ""], ["Liu", "Chiao", ""], ["Roy", "Kaushik", ""]]}, {"id": "1802.02629", "submitter": "David Minnen", "authors": "David Minnen, George Toderici, Michele Covell, Troy Chinen, Nick\n  Johnston, Joel Shor, Sung Jin Hwang, Damien Vincent, Saurabh Singh", "title": "Spatially adaptive image compression using a tiled deep network", "comments": null, "journal-ref": "International Conference on Image Processing 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks represent a powerful class of function approximators\nthat can learn to compress and reconstruct images. Existing image compression\nalgorithms based on neural networks learn quantized representations with a\nconstant spatial bit rate across each image. While entropy coding introduces\nsome spatial variation, traditional codecs have benefited significantly by\nexplicitly adapting the bit rate based on local image complexity and visual\nsaliency. This paper introduces an algorithm that combines deep neural networks\nwith quality-sensitive bit rate adaptation using a tiled network. We\ndemonstrate the importance of spatial context prediction and show improved\nquantitative (PSNR) and qualitative (subjective rater assessment) results\ncompared to a non-adaptive baseline and a recently published image compression\nmodel based on fully-convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 20:59:39 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Minnen", "David", ""], ["Toderici", "George", ""], ["Covell", "Michele", ""], ["Chinen", "Troy", ""], ["Johnston", "Nick", ""], ["Shor", "Joel", ""], ["Hwang", "Sung Jin", ""], ["Vincent", "Damien", ""], ["Singh", "Saurabh", ""]]}, {"id": "1802.02647", "submitter": "Thanh Hong Phuoc", "authors": "Thanh Hong-Phuoc, Yifeng He, Ling Guan", "title": "SCK: A sparse coding based key-point detector", "comments": "Manuscript accepted for presentation at 2018 IEEE International\n  Conference on Image Processing, October 7-10, 2018, Athens, Greece. Patent\n  applied. If you use any techniques, claims, images in this manuscript, please\n  cite the corresponding paper", "journal-ref": "2018 25th IEEE International Conference on Image Processing\n  (ICIP), Athens, Greece, 2018, pp. 3768-3772", "doi": "10.1109/ICIP.2018.8451829", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All current popular hand-crafted key-point detectors such as Harris corner,\nMSER, SIFT, SURF... rely on some specific pre-designed structures for the\ndetection of corners, blobs, or junctions in an image. In this paper, a novel\nsparse coding based key-point detector which requires no particular\npre-designed structures is presented. The key-point detector is based on\nmeasuring the complexity level of each block in an image to decide where a\nkey-point should be. The complexity level of a block is defined as the total\nnumber of non-zero components of a sparse representation of that block.\nGenerally, a block constructed with more components is more complex and has\ngreater potential to be a good key-point. Experimental results on Webcam and EF\ndatasets [1, 2] show that the proposed detector achieves significantly high\nrepeatability compared to hand-crafted features, and even outperforms the\nmatching scores of the state-of-the-art learning based detector.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 21:39:22 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 16:13:55 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 15:54:18 GMT"}, {"version": "v4", "created": "Wed, 26 Sep 2018 19:19:17 GMT"}, {"version": "v5", "created": "Fri, 7 Dec 2018 22:30:41 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hong-Phuoc", "Thanh", ""], ["He", "Yifeng", ""], ["Guan", "Ling", ""]]}, {"id": "1802.02668", "submitter": "Yi Zhu", "authors": "Yi Zhu and Xueqing Deng and Shawn Newsam", "title": "Fine-Grained Land Use Classification at the City Scale Using\n  Ground-Level Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform fine-grained land use mapping at the city scale using ground-level\nimages. Mapping land use is considerably more difficult than mapping land cover\nand is generally not possible using overhead imagery as it requires close-up\nviews and seeing inside buildings. We postulate that the growing collections of\ngeoreferenced, ground-level images suggest an alternate approach to this\ngeographic knowledge discovery problem. We develop a general framework that\nuses Flickr images to map 45 different land-use classes for the City of San\nFrancisco. Individual images are classified using a novel convolutional neural\nnetwork containing two streams, one for recognizing objects and another for\nrecognizing scenes. This network is trained in an end-to-end manner directly on\nthe labeled training images. We propose several strategies to overcome the\nnoisiness of our user-generated data including search-based training set\naugmentation and online adaptive training. We derive a ground truth map of San\nFrancisco in order to evaluate our method. We demonstrate the effectiveness of\nour approach through geo-visualization and quantitative analysis. Our framework\nachieves over 29% recall at the individual land parcel level which represents a\nstrong baseline for the challenging 45-way land use classification problem\nespecially given the noisiness of the image data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 23:01:13 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Zhu", "Yi", ""], ["Deng", "Xueqing", ""], ["Newsam", "Shawn", ""]]}, {"id": "1802.02669", "submitter": "Tolga Birdal", "authors": "Haowen Deng, Tolga Birdal and Slobodan Ilic", "title": "PPFNet: Global Context Aware Local Features for Robust 3D Point Matching", "comments": "Accepted for publication at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PPFNet - Point Pair Feature NETwork for deeply learning a globally\ninformed 3D local feature descriptor to find correspondences in unorganized\npoint clouds. PPFNet learns local descriptors on pure geometry and is highly\naware of the global context, an important cue in deep learning. Our 3D\nrepresentation is computed as a collection of point-pair-features combined with\nthe points and normals within a local vicinity. Our permutation invariant\nnetwork design is inspired by PointNet and sets PPFNet to be ordering-free. As\nopposed to voxelization, our method is able to consume raw point clouds to\nexploit the full sparsity. PPFNet uses a novel $\\textit{N-tuple}$ loss and\narchitecture injecting the global information naturally into the local\ndescriptor. It shows that context awareness also boosts the local feature\nrepresentation. Qualitative and quantitative evaluations of our network suggest\nincreased recall, improved robustness and invariance as well as a vital step in\nthe 3D descriptor extraction performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 23:01:52 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 20:26:25 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Deng", "Haowen", ""], ["Birdal", "Tolga", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1802.02679", "submitter": "Yifan Ding", "authors": "Yifan Ding, Liqiang Wang, Deliang Fan, Boqing Gong", "title": "A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels", "comments": null, "journal-ref": "IEEE Winter Conf. on Applications of Computer Vision 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of deep neural networks is powered in part by large-scale\nwell-labeled training data. However, it is a daunting task to laboriously\nannotate an ImageNet-like dateset. On the contrary, it is fairly convenient,\nfast, and cheap to collect training images from the Web along with their noisy\nlabels. This signifies the need of alternative approaches to training deep\nneural networks using such noisy labels. Existing methods tackling this problem\neither try to identify and correct the wrong labels or reweigh the data terms\nin the loss function according to the inferred noisy rates. Both strategies\ninevitably incur errors for some of the data points. In this paper, we contend\nthat it is actually better to ignore the labels of some of the data points than\nto keep them if the labels are incorrect, especially when the noisy rate is\nhigh. After all, the wrong labels could mislead a neural network to a bad local\noptimum. We suggest a two-stage framework for the learning from noisy labels.\nIn the first stage, we identify a small portion of images from the noisy\ntraining set of which the labels are correct with a high probability. The noisy\nlabels of the other images are ignored. In the second stage, we train a deep\nneural network in a semi-supervised manner. This framework effectively takes\nadvantage of the whole training set and yet only a portion of its labels that\nare most likely correct. Experiments on three datasets verify the effectiveness\nof our approach especially when the noisy rate is high.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 00:35:42 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 00:04:02 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 22:50:33 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Ding", "Yifan", ""], ["Wang", "Liqiang", ""], ["Fan", "Deliang", ""], ["Gong", "Boqing", ""]]}, {"id": "1802.02690", "submitter": "Akshay Rangesh", "authors": "Sourabh Vora, Akshay Rangesh, and Mohan M. Trivedi", "title": "Driver Gaze Zone Estimation using Convolutional Neural Networks: A\n  General Framework and Ablative Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver gaze has been shown to be an excellent surrogate for driver attention\nin intelligent vehicles. With the recent surge of highly autonomous vehicles,\ndriver gaze can be useful for determining the handoff time to a human driver.\nWhile there has been significant improvement in personalized driver gaze zone\nestimation systems, a generalized system which is invariant to different\nsubjects, perspectives and scales is still lacking. We take a step towards this\ngeneralized system using Convolutional Neural Networks (CNNs). We finetune 4\npopular CNN architectures for this task, and provide extensive comparisons of\ntheir outputs. We additionally experiment with different input image patches,\nand also examine how image size affects performance. For training and testing\nthe networks, we collect a large naturalistic driving dataset comprising of 11\nlong drives, driven by 10 subjects in two different cars. Our best performing\nmodel achieves an accuracy of 95.18% during cross-subject testing,\noutperforming current state of the art techniques for this task. Finally, we\nevaluate our best performing model on the publicly available Columbia Gaze\nDataset comprising of images from 56 subjects with varying head pose and gaze\ndirections. Without any training, our model successfully encodes the different\ngaze directions on this diverse dataset, demonstrating good generalization\ncapabilities.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 02:13:50 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 00:45:58 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Vora", "Sourabh", ""], ["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1802.02721", "submitter": "Hojjat Seyed Mousavi", "authors": "Hojjat S. Mousavi, Tiantong Guo, Vishal Monga", "title": "Deep Image Super Resolution via Natural Image Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SR) via deep learning has recently gained\nsignificant attention in the literature. Convolutional neural networks (CNNs)\nare typically learned to represent the mapping between low-resolution (LR) and\nhigh-resolution (HR) images/patches with the help of training examples. Most\nexisting deep networks for SR produce high quality results when training data\nis abundant. However, their performance degrades sharply when training is\nlimited. We propose to regularize deep structures with prior knowledge about\nthe images so that they can capture more structural information from the same\nlimited data. In particular, we incorporate in a tractable fashion within the\nCNN framework, natural image priors which have shown to have much recent\nsuccess in imaging and vision inverse problems. Experimental results show that\nthe proposed deep network with natural image priors is particularly effective\nin training starved regimes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 05:34:01 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Mousavi", "Hojjat S.", ""], ["Guo", "Tiantong", ""], ["Monga", "Vishal", ""]]}, {"id": "1802.02731", "submitter": "Julien Tierny", "authors": "Maxime Soler and Melanie Plainchault and Bruno Conche and Julien\n  Tierny", "title": "Topologically Controlled Lossy Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for the lossy compression of scalar data\ndefined on 2D or 3D regular grids, with topological control. Certain techniques\nallow users to control the pointwise error induced by the compression. However,\nin many scenarios it is desirable to control in a similar way the preservation\nof higher-level notions, such as topological features , in order to provide\nguarantees on the outcome of post-hoc data analyses. This paper presents the\nfirst compression technique for scalar data which supports a strictly\ncontrolled loss of topological features. It provides users with specific\nguarantees both on the preservation of the important features and on the size\nof the smaller features destroyed during compression. In particular, we present\na simple compression strategy based on a topologically adaptive quantization of\nthe range. Our algorithm provides strong guarantees on the bottleneck distance\nbetween persistence diagrams of the input and decompressed data, specifically\nthose associated with extrema. A simple extension of our strategy additionally\nenables a control on the pointwise error. We also show how to combine our\napproach with state-of-the-art compressors, to further improve the geometrical\nreconstruction. Extensive experiments, for comparable compression rates,\ndemonstrate the superiority of our algorithm in terms of the preservation of\ntopological features. We show the utility of our approach by illustrating the\ncompatibility between the output of post-hoc topological data analysis\npipelines, executed on the input and decompressed data, for simulated or\nacquired data sets. We also provide a lightweight VTK-based C++ implementation\nof our approach for reproduction purposes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 07:35:43 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Soler", "Maxime", ""], ["Plainchault", "Melanie", ""], ["Conche", "Bruno", ""], ["Tierny", "Julien", ""]]}, {"id": "1802.02733", "submitter": "Qinghao Hu", "authors": "Qinghao Hu, Peisong Wang and Jian Cheng", "title": "From Hashing to CNNs: Training BinaryWeight Networks via Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown appealing performance on\nvarious computer vision tasks in recent years. This motivates people to deploy\nCNNs to realworld applications. However, most of state-of-art CNNs require\nlarge memory and computational resources, which hinders the deployment on\nmobile devices. Recent studies show that low-bit weight representation can\nreduce much storage and memory demand, and also can achieve efficient network\ninference. To achieve this goal, we propose a novel approach named BWNH to\ntrain Binary Weight Networks via Hashing. In this paper, we first reveal the\nstrong connection between inner-product preserving hashing and binary weight\nnetworks, and show that training binary weight networks can be intrinsically\nregarded as a hashing problem. Based on this perspective, we propose an\nalternating optimization method to learn the hash codes instead of directly\nlearning binary weights. Extensive experiments on CIFAR10, CIFAR100 and\nImageNet demonstrate that our proposed BWNH outperforms current state-of-art by\na large margin.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 07:51:41 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Hu", "Qinghao", ""], ["Wang", "Peisong", ""], ["Cheng", "Jian", ""]]}, {"id": "1802.02745", "submitter": "Reuben Feinman", "authors": "Reuben Feinman, Brenden M. Lake", "title": "Learning Inductive Biases with Simple Neural Networks", "comments": "Published in Proceedings of the 40th Annual Meeting of the Cognitive\n  Science Society, July 2018", "journal-ref": "Feinman, R. and Lake, B.M. (2018). Learning inductive biases with\n  simple neural networks. In Proceedings of the 40th Annual Meeting of the\n  Cognitive Science Society", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People use rich prior knowledge about the world in order to efficiently learn\nnew concepts. These priors - also known as \"inductive biases\" - pertain to the\nspace of internal models considered by a learner, and they help the learner\nmake inferences that go beyond the observed data. A recent study found that\ndeep neural networks optimized for object recognition develop the shape bias\n(Ritter et al., 2017), an inductive bias possessed by children that plays an\nimportant role in early word learning. However, these networks use\nunrealistically large quantities of training data, and the conditions required\nfor these biases to develop are not well understood. Moreover, it is unclear\nhow the learning dynamics of these networks relate to developmental processes\nin childhood. We investigate the development and influence of the shape bias in\nneural networks using controlled datasets of abstract patterns and synthetic\nimages, allowing us to systematically vary the quantity and form of the\nexperience provided to the learning algorithms. We find that simple neural\nnetworks develop a shape bias after seeing as few as 3 examples of 4 object\ncategories. The development of these biases predicts the onset of vocabulary\nacceleration in our networks, consistent with the developmental process in\nchildren.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 08:25:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 18:01:21 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Feinman", "Reuben", ""], ["Lake", "Brenden M.", ""]]}, {"id": "1802.02774", "submitter": "Chengming Xu", "authors": "Chengming Xu, Yanwei Fu, Bing Zhang, Zitian Chen, Yu-Gang Jiang,\n  Xiangyang Xue", "title": "Learning to score the figure skating sports videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets at learning to score the figure skating sports videos. To\naddress this task, we propose a deep architecture that includes two\ncomplementary components, i.e., Self-Attentive LSTM and Multi-scale\nConvolutional Skip LSTM. These two components can efficiently learn the local\nand global sequential information in each video. Furthermore, we present a\nlarge-scale figure skating sports video dataset -- FisV dataset. This dataset\nincludes 500 figure skating videos with the average length of 2 minutes and 50\nseconds. Each video is annotated by two scores of nine different referees,\ni.e., Total Element Score(TES) and Total Program Component Score (PCS). Our\nproposed model is validated on FisV and MIT-skate datasets. The experimental\nresults show the effectiveness of our models in learning to score the figure\nskating videos.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 09:53:56 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 03:08:51 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 08:31:18 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Xu", "Chengming", ""], ["Fu", "Yanwei", ""], ["Zhang", "Bing", ""], ["Chen", "Zitian", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1802.02783", "submitter": "\\c{C}a\\u{g}lar Aytekin", "authors": "Caglar Aytekin, Francesco Cricri and Emre Aksu", "title": "Saliency-Enhanced Robust Visual Tracking", "comments": "Submitted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete correlation filter (DCF) based trackers have shown considerable\nsuccess in visual object tracking. These trackers often make use of low to mid\nlevel features such as histogram of gradients (HoG) and mid-layer activations\nfrom convolution neural networks (CNNs). We argue that including semantically\nhigher level information to the tracked features may provide further robustness\nto challenging cases such as viewpoint changes. Deep salient object detection\nis one example of such high level features, as it make use of semantic\ninformation to highlight the important regions in the given scene. In this\nwork, we propose an improvement over DCF based trackers by combining saliency\nbased and other features based filter responses. This combination is performed\nwith an adaptive weight on the saliency based filter responses, which is\nautomatically selected according to the temporal consistency of visual\nsaliency. We show that our method consistently improves a baseline DCF based\ntracker especially in challenging cases and performs superior to the\nstate-of-the-art. Our improved tracker operates at 9.3 fps, introducing a small\ncomputational burden over the baseline which operates at 11 fps.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 10:21:50 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Aytekin", "Caglar", ""], ["Cricri", "Francesco", ""], ["Aksu", "Emre", ""]]}, {"id": "1802.02796", "submitter": "Angelica I. Aviles-Rivero", "authors": "Georg Maierhofer, Daniel Heydecker, Angelica I. Aviles-Rivero, Samar\n  M. Alsaleh and Carola-Bibiane Sch\\\"onlieb", "title": "Peekaboo - Where are the Objects? Structure Adjusting Superpixels", "comments": "5 pages, 5 figures, IEEE International Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the search for a fast and meaningful image segmentation\nin the context of $k$-means clustering. The proposed method builds on a\nwidely-used local version of Lloyd's algorithm, called Simple Linear Iterative\nClustering (SLIC). We propose an algorithm which extends SLIC to dynamically\nadjust the local search, adopting superpixel resolution dynamically to\nstructure existent in the image, and thus provides for more meaningful\nsuperpixels in the same linear runtime as standard SLIC. The proposed method is\nevaluated against state-of-the-art techniques and improved boundary adherence\nand undersegmentation error are observed, whilst still remaining among the\nfastest algorithms which are tested.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 10:38:56 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 09:34:26 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Maierhofer", "Georg", ""], ["Heydecker", "Daniel", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Alsaleh", "Samar M.", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1802.02813", "submitter": "Ribana Roscher", "authors": "Lukas Drees, Ribana Roscher, Susanne Wenzel", "title": "Archetypal Analysis for Sparse Representation-based Hyperspectral\n  Sub-pixel Quantification", "comments": null, "journal-ref": "Photogrammetric Engineering \\& Remote Sensing, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of land cover fractions from remote sensing images is a\nfrequently used indicator of the environmental quality. This paper focuses on\nthe quantification of land cover fractions in an urban area of Berlin, Germany,\nusing simulated hyperspectral EnMAP data with a spatial resolution of\n30m$\\times$30m. We use constrained sparse representation, where each pixel with\nunknown surface characteristics is expressed by a weighted linear combination\nof elementary spectra with known land cover class. We automatically determine\nthe elementary spectra from image reference data using archetypal analysis by\nsimplex volume maximization, and combine it with reversible jump Markov chain\nMonte Carlo method. In our experiments, the estimation of the automatically\nderived elementary spectra is compared to the estimation obtained by a manually\ndesigned spectral library by means of reconstruction error, mean absolute error\nof the fraction estimates, sum of fractions, $R^2$, and the number of used\nelementary spectra. The experiments show that a collection of archetypes can be\nan adequate and efficient alternative to the manually designed spectral library\nwith respect to the mentioned criteria.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 11:47:19 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Drees", "Lukas", ""], ["Roscher", "Ribana", ""], ["Wenzel", "Susanne", ""]]}, {"id": "1802.02892", "submitter": "Douwe Kiela", "authors": "D. Kiela, E. Grave, A. Joulin, T. Mikolov", "title": "Efficient Large-Scale Multi-Modal Classification", "comments": "Published at AAAI-18, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the incipient internet was largely text-based, the modern digital world\nis becoming increasingly multi-modal. Here, we examine multi-modal\nclassification where one modality is discrete, e.g. text, and the other is\ncontinuous, e.g. visual representations transferred from a convolutional neural\nnetwork. In particular, we focus on scenarios where we have to be able to\nclassify large quantities of data quickly. We investigate various methods for\nperforming multi-modal fusion and analyze their trade-offs in terms of\nclassification accuracy and computational efficiency. Our findings indicate\nthat the inclusion of continuous information improves performance over\ntext-only on a range of multi-modal classification tasks, even with simple\nfusion methods. In addition, we experiment with discretizing the continuous\nfeatures in order to speed up and simplify the fusion process even further. Our\nresults show that fusion with discretized features outperforms text-only\nclassification, at a fraction of the computational cost of full multi-modal\nfusion, with the additional benefit of improved interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 20:30:59 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Kiela", "D.", ""], ["Grave", "E.", ""], ["Joulin", "A.", ""], ["Mikolov", "T.", ""]]}, {"id": "1802.02899", "submitter": "Tuan N.A. Hoang", "authors": "Thanh-Toan Do, Tuan Hoang, Dang-Khoa Le Tan, Huu Le, Tam V. Nguyen,\n  Ngai-Man Cheung", "title": "From Selective Deep Convolutional Features to Compact Binary\n  Representations for Image Retrieval", "comments": "Accepted to Transactions on Multimedia Computing Communications and\n  Applications (TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the large-scale image retrieval task, the two most important requirements\nare the discriminability of image representations and the efficiency in\ncomputation and storage of representations. Regarding the former requirement,\nConvolutional Neural Network (CNN) is proven to be a very powerful tool to\nextract highly discriminative local descriptors for effective image search.\nAdditionally, in order to further improve the discriminative power of the\ndescriptors, recent works adopt fine-tuned strategies. In this paper, taking a\ndifferent approach, we propose a novel, computationally efficient, and\ncompetitive framework. Specifically, we firstly propose various strategies to\ncompute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and eliminate redundant\nfeatures. Our in-depth analyses demonstrate that proposed masking schemes are\neffective to address the burstiness drawback and improve retrieval accuracy.\nSecondly, we propose to employ recent embedding and aggregating methods which\ncan significantly boost the feature discriminability. Regarding the computation\nand storage efficiency, we include a hashing module to produce very compact\nbinary image representations. Extensive experiments on six image retrieval\nbenchmarks demonstrate that our proposed framework achieves the\nstate-of-the-art retrieval performances.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 10:45:14 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 02:06:31 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 23:24:22 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Hoang", "Tuan", ""], ["Tan", "Dang-Khoa Le", ""], ["Le", "Huu", ""], ["Nguyen", "Tam V.", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1802.02904", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Jian Zhang and Zhaoda Ye", "title": "Deep Reinforcement Learning for Image Hashing", "comments": "12 pages, submitted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions' error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN's hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 15:50:48 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 12:53:12 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Peng", "Yuxin", ""], ["Zhang", "Jian", ""], ["Ye", "Zhaoda", ""]]}, {"id": "1802.02925", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yao Wang, Anna Choromanska, Sohae Chung, Xiuyuan Wang,\n  Els Fieremans, Steven Flanagan, Joseph Rath, Yvonne W Lui", "title": "A Deep Unsupervised Learning Approach Toward MTBI Identification Using\n  Diffusion MRI", "comments": "arXiv admin note: text overlap with arXiv:1710.06824", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild traumatic brain injury is a growing public health problem with an\nestimated incidence of over 1.7 million people annually in US. Diagnosis is\nbased on clinical history and symptoms, and accurate, concrete measures of\ninjury are lacking. This work aims to directly use diffusion MR images obtained\nwithin one month of trauma to detect injury, by incorporating deep learning\ntechniques. To overcome the challenge due to limited training data, we describe\neach brain region using the bag of word representation, which specifies the\ndistribution of representative patch patterns. We apply a convolutional\nauto-encoder to learn the patch-level features, from overlapping image patches\nextracted from the MR images, to learn features from diffusion MR images of\nbrain using an unsupervised approach. Our experimental results show that the\nbag of word representation using patch level features learnt by the auto\nencoder provides similar performance as that using the raw patch patterns, both\nsignificantly outperform earlier work relying on the mean values of MR metrics\nin selected brain regions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 15:37:10 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 22:01:13 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""], ["Choromanska", "Anna", ""], ["Chung", "Sohae", ""], ["Wang", "Xiuyuan", ""], ["Fieremans", "Els", ""], ["Flanagan", "Steven", ""], ["Rath", "Joseph", ""], ["Lui", "Yvonne W", ""]]}, {"id": "1802.02950", "submitter": "Xialei Liu", "authors": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M.\n  Lopez and Andrew D. Bagdanov", "title": "Rotate your Networks: Better Weight Consolidation and Less Catastrophic\n  Forgetting", "comments": "Accepted at ICPR'18. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose an approach to avoiding catastrophic forgetting in\nsequential task learning scenarios. Our technique is based on a network\nreparameterization that approximately diagonalizes the Fisher Information\nMatrix of the network parameters. This reparameterization takes the form of a\nfactorized rotation of parameter space which, when used in conjunction with\nElastic Weight Consolidation (which assumes a diagonal Fisher Information\nMatrix), leads to significantly better performance on lifelong learning of\nsequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and\nStanford-40 datasets demonstrate that we significantly improve the results of\nstandard elastic weight consolidation, and that we obtain competitive results\nwhen compared to other state-of-the-art in lifelong learning without\nforgetting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 16:25:29 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 11:35:24 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 13:14:36 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 16:17:25 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Liu", "Xialei", ""], ["Masana", "Marc", ""], ["Herranz", "Luis", ""], ["Van de Weijer", "Joost", ""], ["Lopez", "Antonio M.", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1802.02975", "submitter": "Yao Hengshuai", "authors": "Donglai Zhu, Hao Chen, Hengshuai Yao, Masoud Nosrati, Peyman\n  Yadmellat, Yunfei Zhang", "title": "Practical Issues of Action-conditioned Next Image Prediction", "comments": "12 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of action-conditioned image prediction is to predict the expected\nnext frame given the current camera frame the robot observes and an action\nselected by the robot. We provide the first comparison of two recent popular\nmodels, especially for image prediction on cars. Our major finding is that\naction tiling encoding is the most important factor leading to the remarkable\nperformance of the CDNA model. We present a light-weight model by action tiling\nencoding which has a single-decoder feedforward architecture same as\n[action_video_prediction_honglak]. On a real driving dataset, the CDNA model\nachieves ${0.3986} \\times 10^{-3}$ MSE and ${0.9846}$ Structure SIMilarity\n(SSIM) with a network size of about {\\bfseries ${12.6}$ million} parameters.\nWith a small network of fewer than {\\bfseries ${1}$ million} parameters, our\nnew model achieves a comparable performance to CDNA at ${0.3613} \\times\n10^{-3}$ MSE and ${0.9633}$ SSIM. Our model requires less memory, is more\ncomputationally efficient and is advantageous to be used inside self-driving\nvehicles.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 17:38:26 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Zhu", "Donglai", ""], ["Chen", "Hao", ""], ["Yao", "Hengshuai", ""], ["Nosrati", "Masoud", ""], ["Yadmellat", "Peyman", ""], ["Zhang", "Yunfei", ""]]}, {"id": "1802.02992", "submitter": "Chichen Fu", "authors": "Chichen Fu, Di Chen, Edward J. Delp, Zoe Liu, Fengqing Zhu", "title": "Texture Segmentation Based Video Compression Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in using different approaches to improve\nthe coding efficiency of modern video codec in recent years as demand for\nweb-based video consumption increases. In this paper, we propose a model-based\napproach that uses texture analysis/synthesis to reconstruct blocks in texture\nregions of a video to achieve potential coding gains using the AV1 codec\ndeveloped by the Alliance for Open Media (AOM). The proposed method uses\nconvolutional neural networks to extract texture regions in a frame, which are\nthen reconstructed using a global motion model. Our preliminary results show an\nincrease in coding efficiency while maintaining satisfactory visual quality.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 18:09:49 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Fu", "Chichen", ""], ["Chen", "Di", ""], ["Delp", "Edward J.", ""], ["Liu", "Zoe", ""], ["Zhu", "Fengqing", ""]]}, {"id": "1802.03079", "submitter": "Shuai Li", "authors": "Shuai Li, Ce Zhu, Ming-Ting Sun", "title": "Hole Filling with Multiple Reference Views in DIBR View Synthesis", "comments": "12 pages, accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2018.2791810", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth-image-based rendering (DIBR) oriented view synthesis has been widely\nemployed in the current depth-based 3D video systems by synthesizing a virtual\nview from an arbitrary viewpoint. However, holes may appear in the synthesized\nview due to disocclusion, thus significantly degrading the quality.\nConsequently, efforts have been made on developing effective and efficient hole\nfilling algorithms. Current hole filling techniques generally\nextrapolate/interpolate the hole regions with the neighboring information based\non an assumption that the texture pattern in the holes is similar to that of\nthe neighboring background information. However, in many scenarios especially\nof complex texture, the assumption may not hold. In other words, hole filling\ntechniques can only provide an estimation for a hole which may not be good\nenough or may even be erroneous considering a wide variety of complex scene of\nimages. In this paper, we first examine the view interpolation with multiple\nreference views, demonstrating that the problem of emerging holes in a target\nvirtual view can be greatly alleviated by making good use of other neighboring\ncomplementary views in addition to its two (commonly used) most neighboring\nprimary views. The effects of using multiple views for view extrapolation in\nreducing holes are also investigated in this paper. In view of the 3D Video and\nongoing free-viewpoint TV standardization, we propose a new view synthesis\nframework which employs multiple views to synthesize output virtual views.\nFurthermore, a scheme of selective warping of complementary views is developed\nby efficiently locating a small number of useful pixels in the complementary\nviews for hole reduction, to avoid a full warping of additional complementary\nviews thus lowering greatly the warping complexity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 23:38:12 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Li", "Shuai", ""], ["Zhu", "Ce", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1802.03086", "submitter": "Luciano Oliveira", "authors": "Gil Jader, Luciano Oliveira, Matheus Pithon", "title": "Automatic segmenting teeth in X-ray images: Trends, a novel data set,\n  benchmarking and future perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This review presents an in-depth study of the literature on segmentation\nmethods applied in dental imaging. Ten segmentation methods were studied and\ncategorized according to the type of the segmentation method (region-based,\nthreshold-based, cluster-based, boundary-based or watershed-based), type of\nX-ray images used (intra-oral or extra-oral) and characteristics of the dataset\nused to evaluate the methods in the state-of-the-art works. We found that the\nliterature has primarily focused on threshold-based segmentation methods (54%).\n80% of the reviewed papers have used intra-oral X-ray images in their\nexperiments, demonstrating preference to perform segmentation on images of\nalready isolated parts of the teeth, rather than using extra-oral X-rays, which\nshow tooth structure of the mouth and bones of the face. To fill a scientific\ngap in the field, a novel data set based on extra-oral X-ray images are\nproposed here. A statistical comparison of the results found with the 10 image\nsegmentation methods over our proposed data set comprised of 1,500 images is\nalso carried out, providing a more comprehensive source of performance\nassessment. Discussion on limitations of the methods conceived over the past\nyear as well as future perspectives on exploiting learning-based segmentation\nmethods to improve performance are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 00:31:06 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Jader", "Gil", ""], ["Oliveira", "Luciano", ""], ["Pithon", "Matheus", ""]]}, {"id": "1802.03098", "submitter": "Mustansar Fiaz", "authors": "Mustansar Fiaz, Arif Mahmood and Soon Ki Jung", "title": "Tracking Noisy Targets: A Review of Recent Object Tracking Approaches", "comments": "26 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is an important computer vision problem with numerous\nreal-world applications including human-computer interaction, autonomous\nvehicles, robotics, motion-based recognition, video indexing, surveillance and\nsecurity. In this paper, we aim to extensively review the latest trends and\nadvances in the tracking algorithms and evaluate the robustness of trackers in\nthe presence of noise. The first part of this work comprises a comprehensive\nsurvey of recently proposed tracking algorithms. We broadly categorize trackers\ninto correlation filter based trackers and the others as non-correlation filter\ntrackers. Each category is further classified into various types of trackers\nbased on the architecture of the tracking mechanism. In the second part of this\nwork, we experimentally evaluate tracking algorithms for robustness in the\npresence of additive white Gaussian noise. Multiple levels of additive noise\nare added to the Object Tracking Benchmark (OTB) 2015, and the precision and\nsuccess rates of the tracking algorithms are evaluated. Some algorithms\nsuffered more performance degradation than others, which brings to light a\npreviously unexplored aspect of the tracking algorithms. The relative rank of\nthe algorithms based on their performance on benchmark datasets may change in\nthe presence of noise. Our study concludes that no single tracker is able to\nachieve the same efficiency in the presence of noise as under noise-free\nconditions; thus, there is a need to include a parameter for robustness to\nnoise when evaluating newly proposed tracking algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 01:48:27 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 01:12:32 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Fiaz", "Mustansar", ""], ["Mahmood", "Arif", ""], ["Jung", "Soon Ki", ""]]}, {"id": "1802.03101", "submitter": "Martin Loncaric", "authors": "Martin Loncaric and Bowei Liu and Ryan Weber", "title": "Convolutional Hashing for Automated Scene Matching", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful new loss function and training scheme for learning\nbinary hash functions. In particular, we demonstrate our method by creating for\nthe first time a neural network that outperforms state-of-the-art Haar wavelets\nand color layout descriptors at the task of automated scene matching. By\naccurately relating distance on the manifold of network outputs to distance in\nHamming space, we achieve a 100-fold reduction in nontrivial false positive\nrate and significantly higher true positive rate. We expect our insights to\nprovide large wins for hashing models applied to other information retrieval\nhashing tasks as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 02:11:18 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Loncaric", "Martin", ""], ["Liu", "Bowei", ""], ["Weber", "Ryan", ""]]}, {"id": "1802.03133", "submitter": "Liang Lin", "authors": "Guangrun Wang and Jiefeng Peng and Ping Luo and Xinjiang Wang and\n  Liang Lin", "title": "Batch Kalman Normalization: Towards Training Deep Neural Networks with\n  Micro-Batches", "comments": "We presented how to improve and accelerate the training of DNNs,\n  particularly under the context of micro-batches. (Submitted to IJCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an indispensable component, Batch Normalization (BN) has successfully\nimproved the training of deep neural networks (DNNs) with mini-batches, by\nnormalizing the distribution of the internal representation for each hidden\nlayer. However, the effectiveness of BN would diminish with scenario of\nmicro-batch (e.g., less than 10 samples in a mini-batch), since the estimated\nstatistics in a mini-batch are not reliable with insufficient samples. In this\npaper, we present a novel normalization method, called Batch Kalman\nNormalization (BKN), for improving and accelerating the training of DNNs,\nparticularly under the context of micro-batches. Specifically, unlike the\nexisting solutions treating each hidden layer as an isolated system, BKN treats\nall the layers in a network as a whole system, and estimates the statistics of\na certain layer by considering the distributions of all its preceding layers,\nmimicking the merits of Kalman Filtering. BKN has two appealing properties.\nFirst, it enables more stable training and faster convergence compared to\nprevious works. Second, training DNNs using BKN performs substantially better\nthan those using BN and its variants, especially when very small mini-batches\nare presented. On the image classification benchmark of ImageNet, using BKN\npowered networks we improve upon the best-published model-zoo results: reaching\n74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves\nthe comparable accuracy with extremely smaller batch size, such as 64 times\nsmaller on CIFAR-10/100 and 8 times smaller on ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 05:19:16 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:01:50 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Wang", "Guangrun", ""], ["Peng", "Jiefeng", ""], ["Luo", "Ping", ""], ["Wang", "Xinjiang", ""], ["Lin", "Liang", ""]]}, {"id": "1802.03151", "submitter": "Seyed Ali Osia", "authors": "Seyed Ali Osia, Ali Taheri, Ali Shahin Shamsabadi, Kleomenis Katevas,\n  Hamed Haddadi, Hamid R. Rabiee", "title": "Deep Private-Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model\nwhich is trained and evaluated based on information theoretic constraints.\nUsing the selective exchange of information between a user's device and a\nservice provider, DPFE enables the user to prevent certain sensitive\ninformation from being shared with a service provider, while allowing them to\nextract approved information using their model. We introduce and utilize the\nlog-rank privacy, a novel measure to assess the effectiveness of DPFE in\nremoving sensitive information and compare different models based on their\naccuracy-privacy tradeoff. We then implement and evaluate the performance of\nDPFE on smartphones to understand its complexity, resource demands, and\nefficiency tradeoffs. Our results on benchmark image datasets demonstrate that\nunder moderate resource utilization, DPFE can achieve high accuracy for primary\ntasks while preserving the privacy of sensitive features.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:12:29 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:32:47 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Osia", "Seyed Ali", ""], ["Taheri", "Ali", ""], ["Shamsabadi", "Ali Shahin", ""], ["Katevas", "Kleomenis", ""], ["Haddadi", "Hamed", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1802.03154", "submitter": "Lakshmanan Nataraj", "authors": "Tajuddin Manhar Mohammed, Jason Bunk, Lakshmanan Nataraj, Jawadul H.\n  Bappy, Arjuna Flenner, B.S. Manjunath, Shivkumar Chandrasekaran, Amit K.\n  Roy-Chowdhury, Lawrence Peterson", "title": "Boosting Image Forgery Detection using Resampling Features and Copy-move\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic image forgeries involve a combination of splicing, resampling,\ncloning, region removal and other methods. While resampling detection\nalgorithms are effective in detecting splicing and resampling, copy-move\ndetection algorithms excel in detecting cloning and region removal. In this\npaper, we combine these complementary approaches in a way that boosts the\noverall accuracy of image manipulation detection. We use the copy-move\ndetection method as a pre-filtering step and pass those images that are\nclassified as untampered to a deep learning based resampling detection\nframework. Experimental results on various datasets including the 2017 NIST\nNimble Challenge Evaluation dataset comprising nearly 10,000 pristine and\ntampered images shows that there is a consistent increase of 8%-10% in\ndetection rates, when copy-move algorithm is combined with different resampling\ndetection algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:22:19 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 08:35:54 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Mohammed", "Tajuddin Manhar", ""], ["Bunk", "Jason", ""], ["Nataraj", "Lakshmanan", ""], ["Bappy", "Jawadul H.", ""], ["Flenner", "Arjuna", ""], ["Manjunath", "B. S.", ""], ["Chandrasekaran", "Shivkumar", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Peterson", "Lawrence", ""]]}, {"id": "1802.03192", "submitter": "Franziska Boenisch", "authors": "Franziska Boenisch, Benjamin Rosemann, Benjamin Wild, Fernando Wario,\n  David Dormagen, Tim Landgraf", "title": "Tracking all members of a honey bee colony over their lifetime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational approaches to the analysis of collective behavior in social\ninsects increasingly rely on motion paths as an intermediate data layer from\nwhich one can infer individual behaviors or social interactions. Honey bees are\na popular model for learning and memory. Previous experience has been shown to\naffect and modulate future social interactions. So far, no lifetime history\nobservations have been reported for all bees of a colony. In a previous work we\nintroduced a tracking system customized to track up to $4000$ bees over several\nweeks. In this contribution we present an in-depth description of the\nunderlying multi-step algorithm which both produces the motion paths, and also\nimproves the marker decoding accuracy significantly. We automatically tracked\n${\\sim}2000$ marked honey bees over 10 weeks with inexpensive recording\nhardware using markers without any error correction bits. We found that the\nproposed two-step tracking reduced incorrect ID decodings from initially\n${\\sim}13\\%$ to around $2\\%$ post-tracking. Alongside this paper, we publish\nthe first trajectory dataset for all bees in a colony, extracted from ${\\sim}\n4$ million images. We invite researchers to join the collective scientific\neffort to investigate this intriguing animal system. All components of our\nsystem are open-source.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 10:29:11 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 10:31:02 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Boenisch", "Franziska", ""], ["Rosemann", "Benjamin", ""], ["Wild", "Benjamin", ""], ["Wario", "Fernando", ""], ["Dormagen", "David", ""], ["Landgraf", "Tim", ""]]}, {"id": "1802.03237", "submitter": "Xiaotian Li", "authors": "Xiaotian Li, Juha Ylioinas, Juho Kannala", "title": "Full-Frame Scene Coordinate Regression for Image-Based Localization", "comments": "RSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based localization, or camera relocalization, is a fundamental problem\nin computer vision and robotics, and it refers to estimating camera pose from\nan image. Recent state-of-the-art approaches use learning based methods, such\nas Random Forests (RFs) and Convolutional Neural Networks (CNNs), to regress\nfor each pixel in the image its corresponding position in the scene's world\ncoordinate frame, and solve the final pose via a RANSAC-based optimization\nscheme using the predicted correspondences. In this paper, instead of in a\npatch-based manner, we propose to perform the scene coordinate regression in a\nfull-frame manner to make the computation efficient at test time and, more\nimportantly, to add more global context to the regression process to improve\nthe robustness. To do so, we adopt a fully convolutional encoder-decoder neural\nnetwork architecture which accepts a whole image as input and produces scene\ncoordinate predictions for all pixels in the image. However, using more global\ncontext is prone to overfitting. To alleviate this issue, we propose to use\ndata augmentation to generate more data for training. In addition to the data\naugmentation in 2D image space, we also augment the data in 3D space. We\nevaluate our approach on the publicly available 7-Scenes dataset, and\nexperiments show that it has better scene coordinate predictions and achieves\nstate-of-the-art results in localization with improved robustness on the\nhardest frames (e.g., frames with repeated structures).\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 12:55:30 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 16:17:40 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Li", "Xiaotian", ""], ["Ylioinas", "Juha", ""], ["Kannala", "Juho", ""]]}, {"id": "1802.03243", "submitter": "Gaurav Yengera", "authors": "Andru Putra Twinanda, Gaurav Yengera, Didier Mutter, Jacques Marescaux\n  and Nicolas Padoy", "title": "RSDNet: Learning to Predict Remaining Surgery Duration from Laparoscopic\n  Videos Without Manual Annotations", "comments": "10 pages, IEEE Transactions on Medical Imaging, 2018", "journal-ref": null, "doi": "10.1109/TMI.2018.2878055", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate surgery duration estimation is necessary for optimal OR planning,\nwhich plays an important role in patient comfort and safety as well as resource\noptimization. It is, however, challenging to preoperatively predict surgery\nduration since it varies significantly depending on the patient condition,\nsurgeon skills, and intraoperative situation. In this paper, we propose a deep\nlearning pipeline, referred to as RSDNet, which automatically estimates the\nremaining surgery duration (RSD) intraoperatively by using only visual\ninformation from laparoscopic videos. Previous state-of-the-art approaches for\nRSD prediction are dependent on manual annotation, whose generation requires\nexpensive expert knowledge and is time-consuming, especially considering the\nnumerous types of surgeries performed in a hospital and the large number of\nlaparoscopic videos available. A crucial feature of RSDNet is that it does not\ndepend on any manual annotation during training, making it easily scalable to\nmany kinds of surgeries. The generalizability of our approach is demonstrated\nby testing the pipeline on two large datasets containing different types of\nsurgeries: 120 cholecystectomy and 170 gastric bypass videos. The experimental\nresults also show that the proposed network significantly outperforms a\ntraditional method of estimating RSD without utilizing manual annotation.\nFurther, this work provides a deeper insight into the deep learning network\nthrough visualization and interpretation of the features that are automatically\nlearned.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:07:46 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:52:45 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Twinanda", "Andru Putra", ""], ["Yengera", "Gaurav", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1802.03248", "submitter": "Zicheng Liao", "authors": "Chaowei Fang, Zicheng Liao, Yizhou Yu", "title": "Piecewise Flat Embedding for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat\nEmbedding (PFE) -- for image segmentation. Based on the theory of sparse signal\nrecovery, piecewise flat embedding with diverse channels attempts to recover a\npiecewise constant image representation with sparse region boundaries and\nsparse cluster value scattering. The resultant piecewise flat embedding\nexhibits interesting properties such as suppressing slowly varying signals, and\noffers an image representation with higher region identifiability which is\ndesirable for image segmentation or high-level semantic analysis tasks. We\nformulate our embedding as a variant of the Laplacian Eigenmap embedding with\nan $L_{1,p} (0<p\\leq1)$ regularization term to promote sparse solutions. First,\nwe devise a two-stage numerical algorithm based on Bregman iterations to\ncompute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize\nthis algorithm through iterative reweighting to solve the general\n$L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE\ninto two existing image segmentation frameworks, segmentation based on\nclustering and hierarchical segmentation based on contour detection.\nExperiments on four major benchmark datasets, BSDS500, MSRC, Stanford\nBackground Dataset, and PASCAL Context, show that segmentation algorithms\nincorporating our embedding achieve significantly improved results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:19:14 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 06:58:28 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 11:24:59 GMT"}, {"version": "v4", "created": "Thu, 17 May 2018 04:42:53 GMT"}, {"version": "v5", "created": "Sun, 20 May 2018 07:22:43 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Fang", "Chaowei", ""], ["Liao", "Zicheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "1802.03252", "submitter": "Jun Xiang", "authors": "Jun Xiang, Guoshuai Zhang, Jianhua Hou, Nong Sang, Rui Huang", "title": "Multiple Target Tracking by Learning Feature Representation and Distance\n  Metric Jointly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a robust affinity model is the key issue in multiple target\ntracking (MTT). This paper proposes a novel affinity model by learning feature\nrepresentation and distance metric jointly in a unified deep architecture.\nSpecifically, we design a CNN network to obtain appearance cue tailored towards\nperson Re-ID, and an LSTM network for motion cue to predict target position,\nrespectively. Both cues are combined with a triplet loss function, which\nperforms end-to-end learning of the fused features in a desired embedding\nspace. Experiments in the challenging MOT benchmark demonstrate, that even by a\nsimple Linear Assignment strategy fed with affinity scores of our method, very\ncompetitive results are achieved when compared with the most recent\nstate-of-theart approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:34:21 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Xiang", "Jun", ""], ["Zhang", "Guoshuai", ""], ["Hou", "Jianhua", ""], ["Sang", "Nong", ""], ["Huang", "Rui", ""]]}, {"id": "1802.03254", "submitter": "Michael Ying Yang", "authors": "Wentong Liao, Michael Ying Yang, Ni Zhan, Bodo Rosenhahn", "title": "Triplet-based Deep Similarity Learning for Person Re-Identification", "comments": "ICCV Workshops 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, person re-identification (re-id) catches great attention in\nboth computer vision community and industry. In this paper, we propose a new\nframework for person re-identification with a triplet-based deep similarity\nlearning using convolutional neural networks (CNNs). The network is trained\nwith triplet input: two of them have the same class labels and the other one is\ndifferent. It aims to learn the deep feature representation, with which the\ndistance within the same class is decreased, while the distance between the\ndifferent classes is increased as much as possible. Moreover, we trained the\nmodel jointly on six different datasets, which differs from common practice -\none model is just trained on one dataset and tested also on the same one.\nHowever, the enormous number of possible triplet data among the large number of\ntraining samples makes the training impossible. To address this challenge, a\ndouble-sampling scheme is proposed to generate triplets of images as effective\nas possible. The proposed framework is evaluated on several benchmark datasets.\nThe experimental results show that, our method is effective for the task of\nperson re-identification and it is comparable or even outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:40:59 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Zhan", "Ni", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1802.03257", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Wentong Liao, Yanpeng Cao, Bodo Rosenhahn", "title": "Video Event Recognition and Anomaly Detection by Combining Gaussian\n  Process and Hierarchical Dirichlet Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an unsupervised learning framework for analyzing\nactivities and interactions in surveillance videos. In our framework, three\nlevels of video events are connected by Hierarchical Dirichlet Process (HDP)\nmodel: low-level visual features, simple atomic activities, and multi-agent\ninteractions. Atomic activities are represented as distribution of low-level\nfeatures, while complicated interactions are represented as distribution of\natomic activities. This learning process is unsupervised. Given a training\nvideo sequence, low-level visual features are extracted based on optic flow and\nthen clustered into different atomic activities and video clips are clustered\ninto different interactions. The HDP model automatically decide the number of\nclusters, i.e. the categories of atomic activities and interactions. Based on\nthe learned atomic activities and interactions, a training dataset is generated\nto train the Gaussian Process (GP) classifier. Then the trained GP models work\nin newly captured video to classify interactions and detect abnormal events in\nreal time. Furthermore, the temporal dependencies between video events learned\nby HDP-Hidden Markov Models (HMM) are effectively integrated into GP classifier\nto enhance the accuracy of the classification in newly captured videos. Our\nframework couples the benefits of the generative model (HDP) with the\ndiscriminant model (GP). We provide detailed experiments showing that our\nframework enjoys favorable performance in video event classification in\nreal-time in a crowded traffic scene.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 13:46:41 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yang", "Michael Ying", ""], ["Liao", "Wentong", ""], ["Cao", "Yanpeng", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1802.03268", "submitter": "Hieu Pham", "authors": "Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean", "title": "Efficient Neural Architecture Search via Parameter Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:14:37 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 03:34:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Pham", "Hieu", ""], ["Guan", "Melody Y.", ""], ["Zoph", "Barret", ""], ["Le", "Quoc V.", ""], ["Dean", "Jeff", ""]]}, {"id": "1802.03269", "submitter": "Michael Ying Yang", "authors": "Lihang Liu, Weiyao Lin, Lisheng Wu, Yong Yu, Michael Ying Yang", "title": "Unsupervised Deep Domain Adaptation for Pedestrian Detection", "comments": "ECCV Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of unsupervised domain adaptation on the\ntask of pedestrian detection in crowded scenes. First, we utilize an iterative\nalgorithm to iteratively select and auto-annotate positive pedestrian samples\nwith high confidence as the training samples for the target domain. Meanwhile,\nwe also reuse negative samples from the source domain to compensate for the\nimbalance between the amount of positive samples and negative samples. Second,\nbased on the deep network we also design an unsupervised regularizer to\nmitigate influence from data noise. More specifically, we transform the last\nfully connected layer into two sub-layers - an element-wise multiply layer and\na sum layer, and add the unsupervised regularizer to further improve the domain\nadaptation accuracy. In experiments for pedestrian detection, the proposed\nmethod boosts the recall value by nearly 30% while the precision stays almost\nthe same. Furthermore, we perform our method on standard domain adaptation\nbenchmarks on both supervised and unsupervised settings and also achieve\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:15:35 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Liu", "Lihang", ""], ["Lin", "Weiyao", ""], ["Wu", "Lisheng", ""], ["Yu", "Yong", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1802.03274", "submitter": "Neil Clancy", "authors": "Timur Kuzhagaliyev, Neil T. Clancy, Mirek Janatka, Kevin Tchaka,\n  Francisco Vasconcelos, Matthew J. Clarkson, Kurinchi Gurusamy, David J.\n  Hawkes, Brian Davidson, Danail Stoyanov", "title": "Augmented Reality needle ablation guidance tool for Irreversible\n  Electroporation in the pancreas", "comments": "6 pages, 5 figures. Proc. SPIE 10576 (2018) Copyright 2018 Society of\n  Photo Optical Instrumentation Engineers (SPIE). One print or electronic copy\n  may be made for personal use only. Systematic reproduction and distribution,\n  duplication of any material in this publication for a fee or for commercial\n  purposes, or modification of the contents of the publication are prohibited", "journal-ref": null, "doi": "10.1117/12.2293671", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irreversible electroporation (IRE) is a soft tissue ablation technique\nsuitable for treatment of inoperable tumours in the pancreas. The process\ninvolves applying a high voltage electric field to the tissue containing the\nmass using needle electrodes, leaving cancerous cells irreversibly damaged and\nvulnerable to apoptosis. Efficacy of the treatment depends heavily on the\naccuracy of needle placement and requires a high degree of skill from the\noperator. In this paper, we describe an Augmented Reality (AR) system designed\nto overcome the challenges associated with planning and guiding the needle\ninsertion process. Our solution, based on the HoloLens (Microsoft, USA)\nplatform, tracks the position of the headset, needle electrodes and ultrasound\n(US) probe in space. The proof of concept implementation of the system uses\nthis tracking data to render real-time holographic guides on the HoloLens,\ngiving the user insight into the current progress of needle insertion and an\nindication of the target needle trajectory. The operator's field of view is\naugmented using visual guides and real-time US feed rendered on a holographic\nplane, eliminating the need to consult external monitors. Based on these early\nprototypes, we are aiming to develop a system that will lower the skill level\nrequired for IRE while increasing overall accuracy of needle insertion and,\nhence, the likelihood of successful treatment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:25:56 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Kuzhagaliyev", "Timur", ""], ["Clancy", "Neil T.", ""], ["Janatka", "Mirek", ""], ["Tchaka", "Kevin", ""], ["Vasconcelos", "Francisco", ""], ["Clarkson", "Matthew J.", ""], ["Gurusamy", "Kurinchi", ""], ["Hawkes", "David J.", ""], ["Davidson", "Brian", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1802.03275", "submitter": "Michael Ying Yang", "authors": "Oliver Mueller, Michael Ying Yang, Bodo Rosenhahn", "title": "Slice Sampling Particle Belief Propagation", "comments": "published in ICCV 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in continuous label Markov random fields is a challenging task. We\nuse particle belief propagation (PBP) for solving the inference problem in\ncontinuous label space. Sampling particles from the belief distribution is\ntypically done by using Metropolis-Hastings Markov chain Monte Carlo methods\nwhich involves sampling from a proposal distribution. This proposal\ndistribution has to be carefully designed depending on the particular model and\ninput data to achieve fast convergence. We propose to avoid dependence on a\nproposal distribution by introducing a slice sampling based PBP algorithm. The\nproposed approach shows superior convergence performance on an image denoising\ntoy example. Our findings are validated on a challenging relational 2D feature\ntracking application.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:27:58 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Mueller", "Oliver", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1802.03279", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Matthias Reso, Jun Tang, Wentong Liao, Bodo\n  Rosenhahn", "title": "Temporally Object-based Video Co-Segmentation", "comments": "ISVC 2015 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an unsupervised video object co-segmentation\nframework based on the primary object proposals to extract the common\nforeground object(s) from a given video set. In addition to the objectness\nattributes and motion coherence our framework exploits the temporal consistency\nof the object-like regions between adjacent frames to enrich the set of\noriginal object proposals. We call the enriched proposal sets temporal proposal\nstreams, as they are composed of the most similar proposals from each frame\naugmented with predicted proposals using temporally consistent superpixel\ninformation. The temporal proposal streams represent all the possible region\ntubes of the objects. Therefore, we formulate a graphical model to select a\nproposal stream for each object in which the pairwise potentials consist of the\nappearance dissimilarity between different streams in the same video and also\nthe similarity between the streams in different videos. This model is suitable\nfor single (multiple) foreground objects in two (more) videos, which can be\nsolved by any existing energy minimization method. We evaluate our proposed\nframework by comparing it to other video co-segmentation algorithms. Our method\nachieves improved performance on state-of-the-art benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 14:32:12 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yang", "Michael Ying", ""], ["Reso", "Matthias", ""], ["Tang", "Jun", ""], ["Liao", "Wentong", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1802.03318", "submitter": "Alexander Wong", "authors": "Audrey G. Chung, Paul Fieguth, and Alexander Wong", "title": "Nature vs. Nurture: The Role of Environmental Resources in Evolutionary\n  Deep Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary deep intelligence synthesizes highly efficient deep neural\nnetworks architectures over successive generations. Inspired by the nature\nversus nurture debate, we propose a study to examine the role of external\nfactors on the network synthesis process by varying the availability of\nsimulated environmental resources. Experimental results were obtained for\nnetworks synthesized via asexual evolutionary synthesis (1-parent) and sexual\nevolutionary synthesis (2-parent, 3-parent, and 5-parent) using a 10% subset of\nthe MNIST dataset. Results show that a lower environmental factor model\nresulted in a more gradual loss in performance accuracy and decrease in storage\nsize. This potentially allows significantly reduced storage size with minimal\nto no drop in performance accuracy, and the best networks were synthesized\nusing the lowest environmental factor models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:58:58 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Chung", "Audrey G.", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1802.03327", "submitter": "Pablo Negri", "authors": "Pablo Negri", "title": "Shapes Characterization on Address Event Representation Using Histograms\n  of Oriented Events and an Extended LBP Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Address Event Representation is a thriving technology that could change\ndigital image processing paradigm. This paper proposes a methodology to\ncharacterize the shape of objects using the streaming of asynchronous events. A\nnew descriptor that enhances spikes connectivity is associated with two\noriented histogram based representations. This paper uses these features to\ndevelop both a non-supervised and a supervised multi-classification framework\nto recognize poker symbols from the Poker-DVS public dataset. The\naforementioned framework, which uses a very limited number of events and a\nsimple class modeling, yields results that challenge more sophisticated\nmethodologies proposed by the state of the art. A feature family based on\ncontext shapes is applied to the more challenging 2015 Poker-DVS dataset with a\nsupervised classifier obtaining an accuracy of 98.5 %. The system is also\napplied to the MNIST-DVS dataset yielding an accuracy of 94.6 % and 96.3 % on\ndigit recognition, for scales 4 and 8 respectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:23:21 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Negri", "Pablo", ""]]}, {"id": "1802.03345", "submitter": "Tobias Gr\\\"uning", "authors": "Tobias Gr\\\"uning, Gundram Leifert, Tobias Strau{\\ss}, Johannes\n  Michael, Roger Labahn", "title": "A Two-Stage Method for Text Line Detection in Historical Documents", "comments": "to be published in IJDAR", "journal-ref": "International Journal on Document Analysis and Recognition\n  (IJDAR), (2019), 1-18", "doi": "10.1007/s10032-019-00332-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a two-stage text line detection method for historical\ndocuments. Each detected text line is represented by its baseline. In a first\nstage, a deep neural network called ARU-Net labels pixels to belong to one of\nthe three classes: baseline, separator or other. The separator class marks\nbeginning and end of each text line. The ARU-Net is trainable from scratch with\nmanageably few manually annotated example images (less than 50). This is\nachieved by utilizing data augmentation strategies. The network predictions are\nused as input for the second stage which performs a bottom-up clustering to\nbuild baselines. The developed method is capable of handling complex layouts as\nwell as curved and arbitrarily oriented text lines. It substantially\noutperforms current state-of-the-art approaches. For example, for the complex\ntrack of the cBAD: ICDAR2017 Competition on Baseline Detection the F-value is\nincreased from 0.859 to 0.922. The framework to train and run the ARU-Net is\nopen source.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:52:17 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 09:45:21 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Gr\u00fcning", "Tobias", ""], ["Leifert", "Gundram", ""], ["Strau\u00df", "Tobias", ""], ["Michael", "Johannes", ""], ["Labahn", "Roger", ""]]}, {"id": "1802.03374", "submitter": "Amarjot Singh", "authors": "Amarjot Singh and Nick Kingsbury", "title": "Generative ScatterNet Hybrid Deep Learning (G-SHDL) Network with\n  Structural Priors for Semantic Image Segmentation", "comments": "Accepted at the IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generative ScatterNet hybrid deep learning (G-SHDL)\nnetwork for semantic image segmentation. The proposed generative architecture\nis able to train rapidly from relatively small labeled datasets using the\nintroduced structural priors. In addition, the number of filters in each layer\nof the architecture is optimized resulting in a computationally efficient\narchitecture. The G-SHDL network produces state-of-the-art classification\nperformance against unsupervised and semi-supervised learning on two image\ndatasets. Advantages of the G-SHDL network over supervised methods are\ndemonstrated with experiments performed on training datasets of reduced size.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 18:19:51 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 15:56:37 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Singh", "Amarjot", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1802.03390", "submitter": "Junkyung Kim", "authors": "Matthew Ricci, Junkyung Kim, Thomas Serre", "title": "Same-different problems strain convolutional neural networks", "comments": "6 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust and efficient recognition of visual relations in images is a\nhallmark of biological vision. We argue that, despite recent progress in visual\nrecognition, modern machine vision algorithms are severely limited in their\nability to learn visual relations. Through controlled experiments, we\ndemonstrate that visual-relation problems strain convolutional neural networks\n(CNNs). The networks eventually break altogether when rote memorization becomes\nimpossible, as when intra-class variability exceeds network capacity. Motivated\nby the comparable success of biological vision, we argue that feedback\nmechanisms including attention and perceptual grouping may be the key\ncomputational components underlying abstract visual reasoning.\\\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 18:55:34 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 22:29:20 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 17:00:23 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Ricci", "Matthew", ""], ["Kim", "Junkyung", ""], ["Serre", "Thomas", ""]]}, {"id": "1802.03446", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Pros and Cons of GAN Evaluation Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models, in particular generative adversarial networks (GANs), have\nreceived significant attention recently. A number of GAN variants have been\nproposed and have been utilized in many applications. Despite large strides in\nterms of theoretical progress, evaluating and comparing GANs remains a daunting\ntask. While several measures have been introduced, as of yet, there is no\nconsensus as to which measure best captures strengths and limitations of models\nand should be used for fair model comparison. As in other areas of computer\nvision and machine learning, it is critical to settle on one or few good\nmeasures to steer the progress in this field. In this paper, I review and\ncritically discuss more than 24 quantitative and 5 qualitative measures for\nevaluating generative models with a particular emphasis on GAN-derived models.\nI also provide a set of 7 desiderata followed by an evaluation of whether a\ngiven measure or a family of measures is compatible with them.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 21:05:32 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 23:16:12 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 19:53:53 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 19:14:26 GMT"}, {"version": "v5", "created": "Wed, 24 Oct 2018 02:20:59 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "1802.03479", "submitter": "Tingran Gao", "authors": "Tingran Gao, Shahar Z. Kovalsky, Ingrid Daubechies", "title": "Gaussian Process Landmarking on Manifolds", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a means of improving analysis of biological shapes, we propose an\nalgorithm for sampling a Riemannian manifold by sequentially selecting points\nwith maximum uncertainty under a Gaussian process model. This greedy strategy\nis known to be near-optimal in the experimental design literature, and appears\nto outperform the use of user-placed landmarks in representing the geometry of\nbiological objects in our application. In the noiseless regime, we establish an\nupper bound for the mean squared prediction error (MSPE) in terms of the number\nof samples and geometric quantities of the manifold, demonstrating that the\nMSPE for our proposed sequential design decays at a rate comparable to the\noracle rate achievable by any sequential or non-sequential optimal design; to\nour knowledge this is the first result of this type for sequential experimental\ndesign. The key is to link the greedy algorithm to reduced basis methods in the\ncontext of model reduction for partial differential equations. We expect this\napproach will find additional applications in other fields of research.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:50:10 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 16:15:31 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 05:28:25 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 20:07:46 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Gao", "Tingran", ""], ["Kovalsky", "Shahar Z.", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1802.03480", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Nikos Komodakis", "title": "GraphVAE: Towards Generation of Small Graphs Using Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning on graphs has become a popular research topic with many\napplications. However, past work has concentrated on learning graph embedding\ntasks, which is in contrast with advances in generative models for images and\ntext. Is it possible to transfer this progress to the domain of graphs? We\npropose to sidestep hurdles associated with linearization of such discrete\nstructures by having a decoder output a probabilistic fully-connected graph of\na predefined maximum size directly at once. Our method is formulated as a\nvariational autoencoder. We evaluate on the challenging task of molecule\ngeneration.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:57:46 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1802.03494", "submitter": "Song Han", "authors": "Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han", "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression is a critical technique to efficiently deploy neural\nnetwork models on mobile devices which have limited computation resources and\ntight power budgets. Conventional model compression techniques rely on\nhand-crafted heuristics and rule-based policies that require domain experts to\nexplore the large design space trading off among model size, speed, and\naccuracy, which is usually sub-optimal and time-consuming. In this paper, we\npropose AutoML for Model Compression (AMC) which leverage reinforcement\nlearning to provide the model compression policy. This learning-based\ncompression policy outperforms conventional rule-based compression policy by\nhaving higher compression ratio, better preserving the accuracy and freeing\nhuman labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than\nthe handcrafted model compression policy for VGG-16 on ImageNet. We applied\nthis automated, push-the-button compression pipeline to MobileNet and achieved\n1.81x speedup of measured inference latency on an Android phone and 1.43x\nspeedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:32:44 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 19:34:30 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 04:44:59 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 03:25:50 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["He", "Yihui", ""], ["Lin", "Ji", ""], ["Liu", "Zhijian", ""], ["Wang", "Hanrui", ""], ["Li", "Li-Jia", ""], ["Han", "Song", ""]]}, {"id": "1802.03495", "submitter": "Zilong Zhong", "authors": "Zilong Zhong and Jonathan Li", "title": "Generative Adversarial Networks and Probabilistic Graph Models for\n  Hyperspectral Image Classification", "comments": "Accepted by AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High spectral dimensionality and the shortage of annotations make\nhyperspectral image (HSI) classification a challenging problem. Recent studies\nsuggest that convolutional neural networks can learn discriminative spatial\nfeatures, which play a paramount role in HSI interpretation. However, most of\nthese methods ignore the distinctive spectral-spatial characteristic of\nhyperspectral data. In addition, a large amount of unlabeled data remains an\nunexploited gold mine for efficient data use. Therefore, we proposed an\nintegration of generative adversarial networks (GANs) and probabilistic\ngraphical models for HSI classification. Specifically, we used a\nspectral-spatial generator and a discriminator to identify land cover\ncategories of hyperspectral cubes. Moreover, to take advantage of a large\namount of unlabeled data, we adopted a conditional random field to refine the\npreliminary classification results generated by GANs. Experimental results\nobtained using two commonly studied datasets demonstrate that the proposed\nframework achieved encouraging classification accuracy using a small number of\ndata for training.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:33:52 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Zhong", "Zilong", ""], ["Li", "Jonathan", ""]]}, {"id": "1802.03499", "submitter": "Chuanyun Xu Dr.", "authors": "Chuanyun Xu, Yang Zhang, Xin Feng, YongXing Ge, Yihao Zhang, Jianwu\n  Long", "title": "Local Contrast Learning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a deep model from small data is yet an opening and challenging\nproblem. We focus on one-shot classification by deep learning approach based on\na small quantity of training samples. We proposed a novel deep learning\napproach named Local Contrast Learning (LCL) based on the key insight about a\nhuman cognitive behavior that human recognizes the objects in a specific\ncontext by contrasting the objects in the context or in her/his memory. LCL is\nused to train a deep model that can contrast the recognizing sample with a\ncouple of contrastive samples randomly drawn and shuffled. On one-shot\nclassification task on Omniglot, the deep model based LCL with 122 layers and\n1.94 millions of parameters, which was trained on a tiny dataset with only 60\nclasses and 20 samples per class, achieved the accuracy 97.99% that outperforms\nhuman and state-of-the-art established by Bayesian Program Learning (BPL)\ntrained on 964 classes. LCL is a fundamental idea which can be applied to\nalleviate parametric model's overfitting resulted by lack of training samples.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:54:44 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Xu", "Chuanyun", ""], ["Zhang", "Yang", ""], ["Feng", "Xin", ""], ["Ge", "YongXing", ""], ["Zhang", "Yihao", ""], ["Long", "Jianwu", ""]]}, {"id": "1802.03505", "submitter": "Emanuele Sansone", "authors": "Emanuele Sansone and Hafiz Tiomoko Ali and Sun Jiacheng", "title": "Coulomb Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the true density in high-dimensional feature spaces is a well-known\nproblem in machine learning. In this work, we consider generative autoencoders\nbased on maximum-mean discrepancy (MMD) and provide theoretical insights. In\nparticular, (i) we prove that MMD coupled with Coulomb kernels has optimal\nconvergence properties, which are similar to convex functionals, thus improving\nthe training of autoencoders, and (ii) we provide a probabilistic bound on the\ngeneralization performance, highlighting some fundamental conditions to achieve\nbetter generalization. We validate the theory on synthetic examples and on the\npopular dataset of celebrities' faces, showing that our model, called Coulomb\nautoencoders, outperform the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 02:37:31 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 15:18:59 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 14:18:52 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 12:32:58 GMT"}, {"version": "v5", "created": "Thu, 21 Feb 2019 12:07:34 GMT"}, {"version": "v6", "created": "Tue, 26 Nov 2019 10:25:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sansone", "Emanuele", ""], ["Ali", "Hafiz Tiomoko", ""], ["Jiacheng", "Sun", ""]]}, {"id": "1802.03510", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Dang-Khoa Le Tan, Anh-Dzung Doan, Thanh-Toan Do,\n  Tuan-Anh Bui, Mengxuan Tan and Ngai-Man Cheung", "title": "On-device Scalable Image-based Localization via Prioritized Cascade\n  Search and Fast One-Many RANSAC", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2881829", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design of an entire on-device system for large-scale urban\nlocalization using images. The proposed design integrates compact image\nretrieval and 2D-3D correspondence search to estimate the location in extensive\ncity regions. Our design is GPS agnostic and does not require network\nconnection. In order to overcome the resource constraints of mobile devices, we\npropose a system design that leverages the scalability advantage of image\nretrieval and accuracy of 3D model-based localization. Furthermore, we propose\na new hashing-based cascade search for fast computation of 2D-3D\ncorrespondences. In addition, we propose a new one-many RANSAC for accurate\npose estimation. The new one-many RANSAC addresses the challenge of repetitive\nbuilding structures (e.g. windows, balconies) in urban localization. Extensive\nexperiments demonstrate that our 2D-3D correspondence search achieves\nstate-of-the-art localization accuracy on multiple benchmark datasets.\nFurthermore, our experiments on a large Google Street View (GSV) image dataset\nshow the potential of large-scale localization entirely on a typical mobile\ndevice.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 03:23:07 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 01:56:59 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Tan", "Dang-Khoa Le", ""], ["Doan", "Anh-Dzung", ""], ["Do", "Thanh-Toan", ""], ["Bui", "Tuan-Anh", ""], ["Tan", "Mengxuan", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1802.03515", "submitter": "Wenhao Ding", "authors": "Wenhao Ding, Shuaijun Li, Guilin Zhang, Xiangyu Lei and Huihuan Qian", "title": "Vehicle Pose and Shape Estimation through Multiple Monocular Vision", "comments": "8 pages, 8 figures, published in ROBIO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an accurate approach to estimate vehicles' pose and\nshape from off-board multiview images. The images are taken by monocular\ncameras and have small overlaps. We utilize state-of-the-art convolutional\nneural networks (CNNs) to extract vehicles' semantic keypoints and introduce a\nCross Projection Optimization (CPO) method to estimate the 3D pose. During the\niterative CPO process, an adaptive shape adjustment method named Hierarchical\nWireframe Constraint (HWC) is implemented to estimate the shape. Our approach\nis evaluated under both simulated and real-world scenes for performance\nverification. It's shown that our algorithm outperforms other existing\nmonocular and stereo methods for vehicles' pose and shape estimation. This\napproach provides a new and robust solution for off-board visual vehicle\nlocalization and tracking, which can be applied to massive surveillance camera\nnetworks for intelligent transportation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 03:56:19 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 07:29:26 GMT"}, {"version": "v3", "created": "Sat, 3 Mar 2018 04:06:11 GMT"}, {"version": "v4", "created": "Sat, 14 Jul 2018 17:59:32 GMT"}, {"version": "v5", "created": "Sun, 11 Nov 2018 04:53:47 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ding", "Wenhao", ""], ["Li", "Shuaijun", ""], ["Zhang", "Guilin", ""], ["Lei", "Xiangyu", ""], ["Qian", "Huihuan", ""]]}, {"id": "1802.03518", "submitter": "Maur\\'icio Pamplona Segundo", "authors": "Rodrigo Minetto, Mauricio Pamplona Segundo, Sudeep Sarkar", "title": "Hydra: an Ensemble of Convolutional Neural Networks for Geospatial Land\n  Classification", "comments": "12 pages, 14 figures, 5 tables", "journal-ref": null, "doi": "10.1109/TGRS.2019.2906883", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this paper Hydra, an ensemble of convolutional neural networks\n(CNN) for geospatial land classification. The idea behind Hydra is to create an\ninitial CNN that is coarsely optimized but provides a good starting pointing\nfor further optimization, which will serve as the Hydra's body. Then, the\nobtained weights are fine-tuned multiple times with different augmentation\ntechniques, crop styles, and classes weights to form an ensemble of CNNs that\nrepresent the Hydra's heads. By doing so, we prompt convergence to different\nendpoints, which is a desirable aspect for ensembles. With this framework, we\nwere able to reduce the training time while maintaining the classification\nperformance of the ensemble. We created ensembles for our experiments using two\nstate-of-the-art CNN architectures, ResNet and DenseNet. We have demonstrated\nthe application of our Hydra framework in two datasets, FMOW and NWPU-RESISC45,\nachieving results comparable to the state-of-the-art for the former and the\nbest reported performance so far for the latter. Code and CNN models are\navailable at https://github.com/maups/hydra-fmow\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 04:16:36 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 18:03:28 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Minetto", "Rodrigo", ""], ["Segundo", "Mauricio Pamplona", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1802.03528", "submitter": "Xintao Duan", "authors": "Xintao Duan and Haoxian Song", "title": "Coverless information hiding based on Generative Model", "comments": "4 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new coverless image information hiding method based on generative model is\nproposed, we feed the secret image to the generative model database, and\ngenerate a meaning-normal and independent image different from the secret\nimage, then, the generated image is transmitted to the receiver and is fed to\nthe generative model database to generate another image visually the same as\nthe secret image. So we only need to transmit the meaning-normal image which is\nnot related to the secret image, and we can achieve the same effect as the\ntransmission of the secret image. This is the first time to propose the\ncoverless image information hiding method based on generative model, compared\nwith the traditional image steganography, the transmitted image does not embed\nany information of the secret image in this method, therefore, can effectively\nresist steganalysis tools. Experimental results show that our method has high\ncapacity, safety and reliability.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 06:16:33 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Duan", "Xintao", ""], ["Song", "Haoxian", ""]]}, {"id": "1802.03531", "submitter": "Jiajie Wang", "authors": "Jiajie Wang, Jiangchao Yao, Ya Zhang, Rui Zhang", "title": "Collaborative Learning for Weakly Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection has recently received much attention,\nsince it only requires image-level labels instead of the bounding-box labels\nconsumed in strongly supervised learning. Nevertheless, the save in labeling\nexpense is usually at the cost of model accuracy. In this paper, we propose a\nsimple but effective weakly supervised collaborative learning framework to\nresolve this problem, which trains a weakly supervised learner and a strongly\nsupervised learner jointly by enforcing partial feature sharing and prediction\nconsistency. For object detection, taking WSDDN-like architecture as weakly\nsupervised detector sub-network and Faster-RCNN-like architecture as strongly\nsupervised detector sub-network, we propose an end-to-end Weakly Supervised\nCollaborative Detection Network. As there is no strong supervision available to\ntrain the Faster-RCNN-like sub-network, a new prediction consistency loss is\ndefined to enforce consistency of predictions between the two sub-networks as\nwell as within the Faster-RCNN-like sub-networks. At the same time, the two\ndetectors are designed to partially share features to further guarantee the\nmodel consistency at perceptual level. Extensive experiments on PASCAL VOC 2007\nand 2012 data sets have demonstrated the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 06:36:52 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wang", "Jiajie", ""], ["Yao", "Jiangchao", ""], ["Zhang", "Ya", ""], ["Zhang", "Rui", ""]]}, {"id": "1802.03542", "submitter": "Soonam Lee", "authors": "Soonam Lee and Chichen Fu and Paul Salama and Kenneth W. Dunn and\n  Edward J. Delp", "title": "Tubule segmentation of fluorescence microscopy images based on\n  convolutional neural networks with inhomogeneity correction", "comments": "IS&T International Symposium on Electronic Imaging 2018", "journal-ref": null, "doi": "10.2352/ISSN.2470-1173.2018.15.COIMG-199", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy has become a widely used tool for studying various\nbiological structures of in vivo tissue or cells. However, quantitative\nanalysis of these biological structures remains a challenge due to their\ncomplexity which is exacerbated by distortions caused by lens aberrations and\nlight scattering. Moreover, manual quantification of such image volumes is an\nintractable and error-prone process, making the need for automated image\nanalysis methods crucial. This paper describes a segmentation method for\ntubular structures in fluorescence microscopy images using convolutional neural\nnetworks with data augmentation and inhomogeneity correction. The segmentation\nresults of the proposed method are visually and numerically compared with other\nmicroscopy segmentation methods. Experimental results indicate that the\nproposed method has better performance with correctly segmenting and\nidentifying multiple tubular structures compared to other methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 08:03:22 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Lee", "Soonam", ""], ["Fu", "Chichen", ""], ["Salama", "Paul", ""], ["Dunn", "Kenneth W.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1802.03581", "submitter": "KyungPyo Ko", "authors": "Kyung Pyo Ko, Kwang Hee Lee, Mi So Jang, Gun Hong Park", "title": "2-gram-based Phonetic Feature Generation for Convolutional Neural\n  Network in Assessment of Trademark Similarity", "comments": "10 pages, 6 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trademark is a mark used to identify various commodities. If same or\nsimilar trademark is registered for the same or similar commodity, the\npurchaser of the goods may be confused. Therefore, in the process of trademark\nregistration examination, the examiner judges whether the trademark is the same\nor similar to the other applied or registered trademarks. The confusion in\ntrademarks is based on the visual, phonetic or conceptual similarity of the\nmarks. In this paper, we focus specifically on the phonetic similarity between\ntrademarks. We propose a method to generate 2D phonetic feature for\nconvolutional neural network in assessment of trademark similarity. This\nproposed algorithm is tested with 12,553 trademark phonetic similar pairs and\n34,020 trademark phonetic non-similar pairs from 2010 to 2016. As a result, we\nhave obtained approximately 92% judgment accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 12:50:34 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ko", "Kyung Pyo", ""], ["Lee", "Kwang Hee", ""], ["Jang", "Mi So", ""], ["Park", "Gun Hong", ""]]}, {"id": "1802.03584", "submitter": "Botong Wu", "authors": "Botong Wu, Zhen Zhou, Jianwei Wang and Yizhou Wang", "title": "Joint Learning for Pulmonary Nodule Segmentation, Attributes and\n  Malignancy Prediction", "comments": "5 papers, accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refer to the literature of lung nodule classification, many studies adopt\nConvolutional Neural Networks (CNN) to directly predict the malignancy of lung\nnodules with original thoracic Computed Tomography (CT) and nodule location.\nHowever, these studies cannot tell how the CNN works in terms of predicting the\nmalignancy of the given nodule, e.g., it's hard to conclude that whether the\nregion within the nodule or the contextual information matters according to the\noutput of the CNN. In this paper, we propose an interpretable and multi-task\nlearning CNN -- Joint learning for \\textbf{P}ulmonary \\textbf{N}odule\n\\textbf{S}egmentation \\textbf{A}ttributes and \\textbf{M}alignancy\n\\textbf{P}rediction (PN-SAMP). It is able to not only accurately predict the\nmalignancy of lung nodules, but also provide semantic high-level attributes as\nwell as the areas of detected nodules. Moreover, the combination of nodule\nsegmentation, attributes and malignancy prediction is helpful to improve the\nperformance of each single task. In addition, inspired by the fact that\nradiologists often change window widths and window centers to help to make\ndecision on uncertain nodules, PN-SAMP mixes multiple WW/WC together to gain\ninformation for the raw CT input images. To verify the effectiveness of the\nproposed method, the evaluation is implemented on the public LIDC-IDRI dataset,\nwhich is one of the largest dataset for lung nodule malignancy prediction.\nExperiments indicate that the proposed PN-SAMP achieves significant improvement\nwith respect to lung nodule classification, and promising performance on lung\nnodule segmentation and attribute learning, compared with the-state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 13:11:55 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wu", "Botong", ""], ["Zhou", "Zhen", ""], ["Wang", "Jianwei", ""], ["Wang", "Yizhou", ""]]}, {"id": "1802.03601", "submitter": "Mei Wang", "authors": "Mei Wang, Weihong Deng", "title": "Deep Visual Domain Adaptation: A Survey", "comments": "Manuscript accepted by Neurocomputing 2018", "journal-ref": "Neurocomputing, 2018, 312: 135-153", "doi": "10.1016/j.neucom.2018.05.083", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep domain adaption has emerged as a new learning technique to address the\nlack of massive amounts of labeled data. Compared to conventional methods,\nwhich learn shared feature subspaces or reuse important source instances with\nshallow representations, deep domain adaption methods leverage deep networks to\nlearn more transferable representations by embedding domain adaptation in the\npipeline of deep learning. There have been comprehensive surveys for shallow\ndomain adaption, but few timely reviews the emerging deep learning based\nmethods. In this paper, we provide a comprehensive survey of deep domain\nadaptation methods for computer vision applications with four major\ncontributions. First, we present a taxonomy of different deep domain adaption\nscenarios according to the properties of data that define how two domains are\ndiverged. Second, we summarize deep domain adaption approaches into several\ncategories based on training loss, and analyze and compare briefly the\nstate-of-the-art methods under these categories. Third, we overview the\ncomputer vision applications that go beyond image classification, such as face\nrecognition, semantic segmentation and object detection. Fourth, some potential\ndeficiencies of current methods and several future directions are highlighted.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:35:27 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 14:09:07 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 07:50:24 GMT"}, {"version": "v4", "created": "Fri, 25 May 2018 02:36:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Mei", ""], ["Deng", "Weihong", ""]]}, {"id": "1802.03605", "submitter": "Matthew Guzdial", "authors": "Matthew Guzdial and Mark O. Riedl", "title": "Combinets: Creativity via Recombination of Neural Networks", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the defining characteristics of human creativity is the ability to\nmake conceptual leaps, creating something surprising from typical knowledge. In\ncomparison, deep neural networks often struggle to handle cases outside of\ntheir training data, which is especially problematic for problems with limited\ntraining data. Approaches exist to transfer knowledge from problems with\nsufficient data to those with insufficient data, but they tend to require\nadditional training or a domain-specific method of transfer. We present a new\napproach, conceptual expansion, that serves as a general representation for\nreusing existing trained models to derive new models without backpropagation.\nWe evaluate our approach on few-shot variations of two tasks: image\nclassification and image generation, and outperform standard transfer learning\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:54:10 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 03:38:08 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 15:16:21 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 21:44:51 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Guzdial", "Matthew", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1802.03617", "submitter": "Zhang Li", "authors": "Tao Tan, Zhang Li, Haixia Liu, Ping Liu, Wenfang Tang, Hui Li, Yue\n  Sun, Yusheng Yan, Keyu Li, Tao Xu, Shanshan Wan, Ke Lou, Jun Xu, Huiming\n  Ying, Quchang Ouyang, Yuling Tang, Zheyu Hu, and Qiang Li", "title": "Optimize transfer learning for lung diseases in bronchoscopy using a new\n  concept: sequential fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bronchoscopy inspection as a follow-up procedure from the radiological\nimaging plays a key role in lung disease diagnosis and determining treatment\nplans for the patients. Doctors needs to make a decision whether to biopsy the\npatients timely when performing bronchoscopy. However, the doctors also needs\nto be very selective with biopsies as biopsies may cause uncontrollable\nbleeding of the lung tissue which is life-threaten. To help doctors to be more\nselective on biopsies and provide a second opinion on diagnosis, in this work,\nwe propose a computer-aided diagnosis (CAD) system for lung diseases including\ncancers and tuberculosis (TB). The system is developed based on transfer\nlearning. We propose a novel transfer learning method: sentential fine-tuning .\nCompared to traditional fine-tuning methods, our methods achieves the best\nperformance. We obtained a overall accuracy of 77.0% a dataset of 81 normal\ncases, 76 tuberculosis cases and 277 lung cancer cases while the other\ntraditional transfer learning methods achieve an accuracy of 73% and 68%. . The\ndetection accuracy of our method for cancers, TB and normal cases are 87%, 54%\nand 91% respectively. This indicates that the CAD system has potential to\nimprove lung disease diagnosis accuracy in bronchoscopy and it also might be\nused to be more selective with biopsies.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 16:43:53 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Tan", "Tao", ""], ["Li", "Zhang", ""], ["Liu", "Haixia", ""], ["Liu", "Ping", ""], ["Tang", "Wenfang", ""], ["Li", "Hui", ""], ["Sun", "Yue", ""], ["Yan", "Yusheng", ""], ["Li", "Keyu", ""], ["Xu", "Tao", ""], ["Wan", "Shanshan", ""], ["Lou", "Ke", ""], ["Xu", "Jun", ""], ["Ying", "Huiming", ""], ["Ouyang", "Quchang", ""], ["Tang", "Yuling", ""], ["Hu", "Zheyu", ""], ["Li", "Qiang", ""]]}, {"id": "1802.03646", "submitter": "Yukun Ding", "authors": "Yukun Ding, Jinglan Liu, Jinjun Xiong, Yiyu Shi", "title": "On the Universal Approximability and Complexity Bounds of Quantized ReLU\n  Neural Networks", "comments": "Published in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is a key step to deploy large neural networks on\nresource-constrained platforms. As a popular compression technique,\nquantization constrains the number of distinct weight values and thus reducing\nthe number of bits required to represent and store each weight. In this paper,\nwe study the representation power of quantized neural networks. First, we prove\nthe universal approximability of quantized ReLU networks on a wide class of\nfunctions. Then we provide upper bounds on the number of weights and the memory\nsize for a given approximation error bound and the bit-width of weights for\nfunction-independent and function-dependent structures. Our results reveal\nthat, to attain an approximation error bound of $\\epsilon$, the number of\nweights needed by a quantized network is no more than\n$\\mathcal{O}\\left(\\log^5(1/\\epsilon)\\right)$ times that of an unquantized\nnetwork. This overhead is of much lower order than the lower bound of the\nnumber of weights needed for the error bound, supporting the empirical success\nof various quantization techniques. To the best of our knowledge, this is the\nfirst in-depth study on the complexity bounds of quantized neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 19:43:42 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 23:16:45 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 22:51:22 GMT"}, {"version": "v4", "created": "Sat, 12 Jan 2019 21:54:14 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Ding", "Yukun", ""], ["Liu", "Jinglan", ""], ["Xiong", "Jinjun", ""], ["Shi", "Yiyu", ""]]}, {"id": "1802.03680", "submitter": "Favyen Bastani", "authors": "Favyen Bastani, Songtao He, Sofiane Abbar, Mohammad Alizadeh, Hari\n  Balakrishnan, Sanjay Chawla, Sam Madden, David DeWitt", "title": "RoadTracer: Automatic Extraction of Road Networks from Aerial Images", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping road networks is currently both expensive and labor-intensive.\nHigh-resolution aerial imagery provides a promising avenue to automatically\ninfer a road network. Prior work uses convolutional neural networks (CNNs) to\ndetect which pixels belong to a road (segmentation), and then uses complex\npost-processing heuristics to infer graph connectivity. We show that these\nsegmentation methods have high error rates because noisy CNN outputs are\ndifficult to correct. We propose RoadTracer, a new method to automatically\nconstruct accurate road network maps from aerial images. RoadTracer uses an\niterative search process guided by a CNN-based decision function to derive the\nroad network graph directly from the output of the CNN. We compare our approach\nwith a segmentation method on fifteen cities, and find that at a 5% error rate,\nRoadTracer correctly captures 45% more junctions across these cities.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 02:38:17 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 22:12:54 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Bastani", "Favyen", ""], ["He", "Songtao", ""], ["Abbar", "Sofiane", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hari", ""], ["Chawla", "Sanjay", ""], ["Madden", "Sam", ""], ["DeWitt", "David", ""]]}, {"id": "1802.03714", "submitter": "Jiawei Su <", "authors": "Jiawei Su, Danilo Vasconcellos Vargas, Sanjiva Prasad, Daniele\n  Sgandurra, Yaokai Feng, Kouichi Sakurai", "title": "Lightweight Classification of IoT Malware based on Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is an extension of the traditional Internet,\nwhich allows a very large number of smart devices, such as home appliances,\nnetwork cameras, sensors and controllers to connect to one another to share\ninformation and improve user experiences. Current IoT devices are typically\nmicro-computers for domain-specific computations rather than traditional\nfunctionspecific embedded devices. Therefore, many existing attacks, targeted\nat traditional computers connected to the Internet, may also be directed at IoT\ndevices. For example, DDoS attacks have become very common in IoT environments,\nas these environments currently lack basic security monitoring and protection\nmechanisms, as shown by the recent Mirai and Brickerbot IoT botnets. In this\npaper, we propose a novel light-weight approach for detecting DDos malware in\nIoT environments.We firstly extract one-channel gray-scale images converted\nfrom binaries, and then utilize a lightweight convolutional neural network for\nclassifying IoT malware families. The experimental results show that the\nproposed system can achieve 94.0% accuracy for the classification of goodware\nand DDoS malware, and 81.8% accuracy for the classification of goodware and two\nmain malware families.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 09:03:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Su", "Jiawei", ""], ["Vargas", "Danilo Vasconcellos", ""], ["Prasad", "Sanjiva", ""], ["Sgandurra", "Daniele", ""], ["Feng", "Yaokai", ""], ["Sakurai", "Kouichi", ""]]}, {"id": "1802.03750", "submitter": "Zheng Qin", "authors": "Zheng Qin, Zhaoning Zhang, Xiaotao Chen, Yuxing Peng", "title": "FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy", "comments": "5 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Fast-Downsampling MobileNet (FD-MobileNet), an efficient and\naccurate network for very limited computational budgets (e.g., 10-140 MFLOPs).\nOur key idea is applying an aggressive downsampling strategy to MobileNet\nframework. In FD-MobileNet, we perform 32$\\times$ downsampling within 12\nlayers, only half the layers in the original MobileNet. This design brings\nthree advantages: (i) It remarkably reduces the computational cost. (ii) It\nincreases the information capacity and achieves significant performance\nimprovements. (iii) It is engineering-friendly and provides fast actual\ninference speed. Experiments on ILSVRC 2012 and PASCAL VOC 2007 datasets\ndemonstrate that FD-MobileNet consistently outperforms MobileNet and achieves\ncomparable results with ShuffleNet under different computational budgets, for\ninstance, surpassing MobileNet by 5.5% on the ILSVRC 2012 top-1 accuracy and\n3.6% on the VOC 2007 mAP under a complexity of 12 MFLOPs. On an ARM-based\ndevice, FD-MobileNet achieves 1.11$\\times$ inference speedup over MobileNet and\n1.82$\\times$ over ShuffleNet under the same complexity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 15:01:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Qin", "Zheng", ""], ["Zhang", "Zhaoning", ""], ["Chen", "Xiaotao", ""], ["Peng", "Yuxing", ""]]}, {"id": "1802.03752", "submitter": "Sourav Mishra", "authors": "Sourav Mishra, Toshihiko Yamasaki, Hideaki Imaizumi", "title": "Supervised classification of Dermatological diseases by Deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces a deep-learning based efficient classifier for common\ndermatological conditions, aimed at people without easy access to skin\nspecialists. We report approximately 80% accuracy, in a situation where primary\ncare doctors have attained 57% success rate, according to recent literature.\nThe rationale of its design is centered on deploying and updating it on\nhandheld devices in near future. Dermatological diseases are common in every\npopulation and have a wide spectrum in severity. With a shortage of\ndermatological expertise being observed in several countries, machine learning\nsolutions can augment medical services and advise regarding existence of common\ndiseases. The paper implements supervised classification of nine distinct\nconditions which have high occurrence in East Asian countries. Our current\nattempt establishes that deep learning based techniques are viable avenues for\npreliminary information to aid patients.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 15:34:20 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 16:17:12 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 17:23:02 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mishra", "Sourav", ""], ["Yamasaki", "Toshihiko", ""], ["Imaizumi", "Hideaki", ""]]}, {"id": "1802.03769", "submitter": "Yu-Sheng Chen", "authors": "Nai-Sheng Syu, Yu-Sheng Chen, Yung-Yu Chuang", "title": "Learning Deep Convolutional Networks for Demosaicing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive study of applying the convolutional\nneural network (CNN) to solving the demosaicing problem. The paper presents two\nCNN models that learn end-to-end mappings between the mosaic samples and the\noriginal image patches with full information. In the case the Bayer color\nfilter array (CFA) is used, an evaluation with ten competitive methods on\npopular benchmarks confirms that the data-driven, automatically learned\nfeatures by the CNN models are very effective. Experiments show that the\nproposed CNN models can perform equally well in both the sRGB space and the\nlinear space. It is also demonstrated that the CNN model can perform joint\ndenoising and demosaicing. The CNN model is very flexible and can be easily\nadopted for demosaicing with any CFA design. We train CNN models for\ndemosaicing with three different CFAs and obtain better results than existing\nmethods. With the great flexibility to be coupled with any CFA, we present the\nfirst data-driven joint optimization of the CFA design and the demosaicing\nmethod using CNN. Experiments show that the combination of the automatically\ndiscovered CFA pattern and the automatically devised demosaicing method\nsignificantly outperforms the current best demosaicing results. Visual\ncomparisons confirm that the proposed methods reduce more visual artifacts than\nexisting methods. Finally, we show that the CNN model is also effective for the\nmore general demosaicing problem with spatially varying exposure and color and\ncan be used for taking images of higher dynamic ranges with a single shot. The\nproposed models and the thorough experiments together demonstrate that CNN is\nan effective and versatile tool for solving the demosaicing problem.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 16:57:50 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Syu", "Nai-Sheng", ""], ["Chen", "Yu-Sheng", ""], ["Chuang", "Yung-Yu", ""]]}, {"id": "1802.03803", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti, N. Siddharth, Puneet K. Dokania, Philip H.S. Torr", "title": "FlipDial: A Generative Model for Two-Way Visual Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FlipDial, a generative model for visual dialogue that\nsimultaneously plays the role of both participants in a visually-grounded\ndialogue. Given context in the form of an image and an associated caption\nsummarising the contents of the image, FlipDial learns both to answer questions\nand put forward questions, capable of generating entire sequences of dialogue\n(question-answer pairs) which are diverse and relevant to the image. To do\nthis, FlipDial relies on a simple but surprisingly powerful idea: it uses\nconvolutional neural networks (CNNs) to encode entire dialogues directly,\nimplicitly capturing dialogue context, and conditional VAEs to learn the\ngenerative model. FlipDial outperforms the state-of-the-art model in the\nsequential answering task (one-way visual dialogue) on the VisDial dataset by 5\npoints in Mean Rank using the generated answers. We are the first to extend\nthis paradigm to full two-way visual dialogue, where our model is capable of\ngenerating both questions and answers in sequence based on a visual input, for\nwhich we propose a set of novel evaluation measures and metrics.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:40:16 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 13:59:08 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Massiceti", "Daniela", ""], ["Siddharth", "N.", ""], ["Dokania", "Puneet K.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1802.03835", "submitter": "Jong Hwan Ko", "authors": "Jong Hwan Ko, Taesik Na, Mohammad Faisal Amir, Saibal Mukhopadhyay", "title": "Edge-Host Partitioning of Deep Neural Networks with Feature Space\n  Encoding for Resource-Constrained Internet-of-Things Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces partitioning an inference task of a deep neural network\nbetween an edge and a host platform in the IoT environment. We present a DNN as\nan encoding pipeline, and propose to transmit the output feature space of an\nintermediate layer to the host. The lossless or lossy encoding of the feature\nspace is proposed to enhance the maximum input rate supported by the edge\nplatform and/or reduce the energy of the edge platform. Simulation results show\nthat partitioning a DNN at the end of convolutional (feature extraction) layers\ncoupled with feature space encoding enables significant improvement in the\nenergy-efficiency and throughput over the baseline configurations that perform\nthe entire inference at the edge or at the host.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 23:04:36 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ko", "Jong Hwan", ""], ["Na", "Taesik", ""], ["Amir", "Mohammad Faisal", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1802.03881", "submitter": "Sang-Woo Lee", "authors": "Sang-Woo Lee, Yu-Jung Heo, Byoung-Tak Zhang", "title": "Answerer in Questioner's Mind: Information Theoretic Approach to\n  Goal-Oriented Visual Dialog", "comments": "Selected for a spotlight presentation at NIPS, 2018. Camera ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal-oriented dialog has been given attention due to its numerous\napplications in artificial intelligence. Goal-oriented dialogue tasks occur\nwhen a questioner asks an action-oriented question and an answerer responds\nwith the intent of letting the questioner know a correct action to take. To ask\nthe adequate question, deep learning and reinforcement learning have been\nrecently applied. However, these approaches struggle to find a competent\nrecurrent neural questioner, owing to the complexity of learning a series of\nsentences. Motivated by theory of mind, we propose \"Answerer in Questioner's\nMind\" (AQM), a novel information theoretic algorithm for goal-oriented dialog.\nWith AQM, a questioner asks and infers based on an approximated probabilistic\nmodel of the answerer. The questioner figures out the answerer's intention via\nselecting a plausible question by explicitly calculating the information gain\nof the candidate intentions and possible answers to each question. We test our\nframework on two goal-oriented visual dialog tasks: \"MNIST Counting Dialog\" and\n\"GuessWhat?!\". In our experiments, AQM outperforms comparative algorithms by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:08:06 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 07:31:24 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 05:07:23 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Lee", "Sang-Woo", ""], ["Heo", "Yu-Jung", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1802.03931", "submitter": "Hyomin Choi", "authors": "Hyomin Choi and Ivan V. Bajic", "title": "Deep feature compression for collaborative object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that the efficiency of deep neural networks in\nmobile applications can be significantly improved by distributing the\ncomputational workload between the mobile device and the cloud. This paradigm,\ntermed collaborative intelligence, involves communicating feature data between\nthe mobile and the cloud. The efficiency of such approach can be further\nimproved by lossy compression of feature data, which has not been examined to\ndate. In this work we focus on collaborative object detection and study the\nimpact of both near-lossless and lossy compression of feature data on its\naccuracy. We also propose a strategy for improving the accuracy under lossy\nfeature compression. Experiments indicate that using this strategy, the\ncommunication overhead can be reduced by up to 70% without sacrificing\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 08:26:07 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Choi", "Hyomin", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1802.03934", "submitter": "Xiaochuan Fan", "authors": "Xiaochuan Fan, Hao Guo, Kang Zheng, Wei Feng, Song Wang", "title": "Object Detection with Mask-based Feature Encoding", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based Convolutional Neural Networks (R-CNNs) have achieved great\nsuccess in the field of object detection. The existing R-CNNs usually divide a\nRegion-of-Interest (ROI) into grids, and then localize objects by utilizing the\nspatial information reflected by the relative position of each grid in the ROI.\nIn this paper, we propose a novel feature-encoding approach, where spatial\ninformation is represented through the spatial distributions of visual\npatterns. In particular, we design a Mask Weight Network (MWN) to learn a set\nof masks and then apply channel-wise masking operations to ROI feature map,\nfollowed by a global pooling and a cheap fully-connected layer. We integrate\nthe newly designed feature encoder into the Faster R-CNN architecture. The\nresulting new Faster R-CNNs can preserve the object-detection accuracy of the\nstandard Faster R-CNNs by using substantially fewer parameters. Compared to\nR-FCNs using state-of-art PS ROI pooling and deformable PS ROI pooling, the new\nFaster R-CNNs can produce higher object-detection accuracy with good run-time\nefficiency. We also show that a specifically designed and learned MWN can\ncapture global contextual information and further improve the object-detection\naccuracy. Validation experiments are conducted on both PASCAL VOC and MS COCO\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 08:44:39 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Fan", "Xiaochuan", ""], ["Guo", "Hao", ""], ["Zheng", "Kang", ""], ["Feng", "Wei", ""], ["Wang", "Song", ""]]}, {"id": "1802.03943", "submitter": "Franziska Schirrmacher", "authors": "Franziska Schirrmacher, Thomas K\\\"ohler, Tobias Lindenberger, Lennart\n  Husvogt, J\\\"urgen Endres, James G. Fujimoto, Joachim Hornegger, Arnd\n  D\\\"orfler, Philip Hoelter, Andreas K. Maier", "title": "Temporal and volumetric denoising via quantile sparse image prior", "comments": "Accepted for MICCAI2017 special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an universal and structure-preserving regularization\nterm, called quantile sparse image (QuaSI) prior. The prior is suitable for\ndenoising images from various medical imaging modalities. We demonstrate its\neffectiveness on volumetric optical coherence tomography (OCT) and computed\ntomography (CT) data, which show different noise and image characteristics. OCT\noffers high-resolution scans of the human retina but is inherently impaired by\nspeckle noise. CT on the other hand has a lower resolution and shows\nhigh-frequency noise. For the purpose of denoising, we propose a variational\nframework based on the QuaSI prior and a Huber data fidelity model that can\nhandle 3-D and 3-D+t data. Efficient optimization is facilitated through the\nuse of an alternating direction method of multipliers (ADMM) scheme and the\nlinearization of the quantile filter. Experiments on multiple datasets\nemphasize the excellent performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 09:09:55 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 10:41:02 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 07:04:18 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Schirrmacher", "Franziska", ""], ["K\u00f6hler", "Thomas", ""], ["Lindenberger", "Tobias", ""], ["Husvogt", "Lennart", ""], ["Endres", "J\u00fcrgen", ""], ["Fujimoto", "James G.", ""], ["Hornegger", "Joachim", ""], ["D\u00f6rfler", "Arnd", ""], ["Hoelter", "Philip", ""], ["Maier", "Andreas K.", ""]]}, {"id": "1802.03980", "submitter": "Silvio Giancola", "authors": "Silvio Giancola, Jens Schneider, Peter Wonka and Bernard S. Ghanem", "title": "Integration of Absolute Orientation Measurements in the KinectFusion\n  Reconstruction pipeline", "comments": "CVPR Workshop on Visual Odometry and Computer Vision Applications\n  Based on Location Clues 2018", "journal-ref": null, "doi": "10.1109/CVPRW.2018.00198", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how absolute orientation measurements provided by\nlow-cost but high-fidelity IMU sensors can be integrated into the KinectFusion\npipeline. We show that integration improves both runtime, robustness and\nquality of the 3D reconstruction. In particular, we use this orientation data\nto seed and regularize the ICP registration technique. We also present a\ntechnique to filter the pairs of 3D matched points based on the distribution of\ntheir distances. This filter is implemented efficiently on the GPU. Estimating\nthe distribution of the distances helps control the number of iterations\nnecessary for the convergence of the ICP algorithm. Finally, we show\nexperimental results that highlight improvements in robustness, a speed-up of\nalmost 12%, and a gain in tracking quality of 53% for the ATE metric on the\nFreiburg benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 11:18:40 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 13:13:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Giancola", "Silvio", ""], ["Schneider", "Jens", ""], ["Wonka", "Peter", ""], ["Ghanem", "Bernard S.", ""]]}, {"id": "1802.03989", "submitter": "Fahad Sohrab", "authors": "Fahad Sohrab, Jenni Raitoharju, Moncef Gabbouj, Alexandros Iosifidis", "title": "Subspace Support Vector Data Description", "comments": "6 pages, submitted/accepted, ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for solving one-class classification\nproblems. The proposed approach, namely Subspace Support Vector Data\nDescription, maps the data to a subspace that is optimized for one-class\nclassification. In that feature space, the optimal hypersphere enclosing the\ntarget class is then determined. The method iteratively optimizes the data\nmapping along with data description in order to define a compact class\nrepresentation in a low-dimensional feature space. We provide both linear and\nnon-linear mappings for the proposed method. Experiments on 14 publicly\navailable datasets indicate that the proposed Subspace Support Vector Data\nDescription provides better performance compared to baselines and other\nrecently proposed one-class classification methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 11:46:23 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 23:05:27 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 21:02:02 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Sohrab", "Fahad", ""], ["Raitoharju", "Jenni", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1802.04034", "submitter": "Yusuke Tsuzuku", "authors": "Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama", "title": "Lipschitz-Margin Training: Scalable Certification of Perturbation\n  Invariance for Deep Neural Networks", "comments": "To appear in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High sensitivity of neural networks against malicious perturbations on inputs\ncauses security concerns. To take a steady step towards robust classifiers, we\naim to create neural network models provably defended from perturbations. Prior\ncertification work requires strong assumptions on network structures and\nmassive computational costs, and thus the range of their applications was\nlimited. From the relationship between the Lipschitz constants and prediction\nmargins, we present a computationally efficient calculation technique to\nlower-bound the size of adversarial perturbations that can deceive networks,\nand that is widely applicable to various complicated networks. Moreover, we\npropose an efficient training procedure that robustifies networks and\nsignificantly improves the provably guarded areas around data points. In\nexperimental evaluations, our method showed its ability to provide a\nnon-trivial guarantee and enhance robustness for even large networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 13:37:09 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 06:14:31 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 19:02:36 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tsuzuku", "Yusuke", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1802.04073", "submitter": "Asim Muhammad", "authors": "Muhammad Asim, Fahad Shamshad, and Ali Ahmed", "title": "Blind Image Deconvolution using Deep Generative Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to regularize the \\textit{ill-posed} and\n\\textit{non-linear} blind image deconvolution (blind deblurring) using deep\ngenerative networks as priors. We employ two separate generative models --- one\ntrained to produce sharp images while the other trained to generate blur\nkernels from lower-dimensional parameters. To deblur, we propose an alternating\ngradient descent scheme operating in the latent lower-dimensional space of each\nof the pretrained generative models. Our experiments show promising deblurring\nresults on images even under large blurs, and heavy noise. To address the\nshortcomings of generative models such as mode collapse, we augment our\ngenerative priors with classical image priors and report improved performance\non complex image datasets. The deblurring performance depends on how well the\nrange of the generator spans the image class. Interestingly, our experiments\nshow that even an untrained structured (convolutional) generative networks acts\nas an image prior in the image deblurring context allowing us to extend our\nresults to more diverse natural image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:39:04 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 10:30:28 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 18:29:07 GMT"}, {"version": "v4", "created": "Tue, 26 Feb 2019 20:14:35 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Asim", "Muhammad", ""], ["Shamshad", "Fahad", ""], ["Ahmed", "Ali", ""]]}, {"id": "1802.04087", "submitter": "Min Xu", "authors": "Chang Liu, Xiangrui Zeng, Ruogu Lin, Xiaodan Liang, Zachary Freyberg,\n  Eric Xing, Min Xu", "title": "Deep learning based supervised semantic segmentation of Electron\n  Cryo-Subtomograms", "comments": "9 pages", "journal-ref": "IEEE International Conference on Image Processing (ICIP) 2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for\nthe 3D visualization of cellular structure and organization at submolecular\nresolution. It enables analyzing the native structures of macromolecular\ncomplexes and their spatial organization inside single cells. However, due to\nthe high degree of structural complexity and practical imaging limitations,\nsystematic macromolecular structural recovery inside CECT images remains\nchallenging. Particularly, the recovery of a macromolecule is likely to be\nbiased by its neighbor structures due to the high molecular crowding. To reduce\nthe bias, here we introduce a novel 3D convolutional neural network inspired by\nFully Convolutional Network and Encoder-Decoder Architecture for the supervised\nsegmentation of macromolecules of interest in subtomograms. The tests of our\nmodels on realistically simulated CECT data demonstrate that our new approach\nhas significantly improved segmentation performance compared to our baseline\napproach. Also, we demonstrate that the proposed model has generalization\nability to segment new structures that do not exist in training data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:54:49 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Liu", "Chang", ""], ["Zeng", "Xiangrui", ""], ["Lin", "Ruogu", ""], ["Liang", "Xiaodan", ""], ["Freyberg", "Zachary", ""], ["Xing", "Eric", ""], ["Xu", "Min", ""]]}, {"id": "1802.04145", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Xiuyuan Cheng, Robert Calderbank, Guillermo Sapiro", "title": "DCFNet: Deep Neural Network with Decomposed Convolutional Filters", "comments": "Published at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters in a Convolutional Neural Network (CNN) contain model parameters\nlearned from enormous amounts of data. In this paper, we suggest to decompose\nconvolutional filters in CNN as a truncated expansion with pre-fixed bases,\nnamely the Decomposed Convolutional Filters network (DCFNet), where the\nexpansion coefficients remain learned from data. Such a structure not only\nreduces the number of trainable parameters and computation, but also imposes\nfilter regularity by bases truncation. Through extensive experiments, we\nconsistently observe that DCFNet maintains accuracy for image classification\ntasks with a significant reduction of model parameters, particularly with\nFourier-Bessel (FB) bases, and even with random bases. Theoretically, we\nanalyze the representation stability of DCFNet with respect to input\nvariations, and prove representation stability under generic assumptions on the\nexpansion coefficients. The analysis is consistent with the empirical\nobservations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 15:58:54 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 20:55:55 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 23:58:20 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Qiu", "Qiang", ""], ["Cheng", "Xiuyuan", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1802.04216", "submitter": "Gr\\'egory Rogez", "authors": "Gr\\'egory Rogez and Cordelia Schmid", "title": "Image-based Synthesis for Deep 3D Human Pose Estimation", "comments": "accepted to appear in IJCV (with minor revisions). Follow-up to NIPS\n  2016 arXiv:1607.02046", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D human pose estimation in the wild. A\nsignificant challenge is the lack of training data, i.e., 2D images of humans\nannotated with 3D poses. Such data is necessary to train state-of-the-art CNN\narchitectures. Here, we propose a solution to generate a large set of\nphotorealistic synthetic images of humans with 3D pose annotations. We\nintroduce an image-based synthesis engine that artificially augments a dataset\nof real images with 2D human pose annotations using 3D motion capture data.\nGiven a candidate 3D pose, our algorithm selects for each joint an image whose\n2D pose locally matches the projected 3D pose. The selected images are then\ncombined to generate a new synthetic image by stitching local image patches in\na kinematically constrained manner. The resulting images are used to train an\nend-to-end CNN for full-body 3D pose estimation. We cluster the training data\ninto a large number of pose classes and tackle pose estimation as a $K$-way\nclassification problem. Such an approach is viable only with large training\nsets such as ours. Our method outperforms most of the published works in terms\nof 3D pose estimation in controlled environments (Human3.6M) and shows\npromising results for real-world images (LSP). This demonstrates that CNNs\ntrained on artificial images generalize well to real images. Compared to data\ngenerated from more classical rendering engines, our synthetic images do not\nrequire any domain adaptation or fine-tuning stage.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 17:59:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Rogez", "Gr\u00e9gory", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1802.04392", "submitter": "Fan Tang", "authors": "Fan Tang and Weiming Dong and Yiping Meng and Chongyang Ma and Fuzhang\n  Wu and Xinrui Li and Tong-Yee Lee", "title": "Image Retargetability", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications could benefit from the ability to automatically\nretarget an image to different aspect ratios and resolutions, while preserving\nits visually and semantically important content. However, not all images can be\nequally well processed that way. In this work, we introduce the notion of image\nretargetability to describe how well a particular image can be handled by\ncontent-aware image retargeting. We propose to learn a deep convolutional\nneural network to rank photo retargetability in which the relative ranking of\nphoto retargetability is directly modeled in the loss function. Our model\nincorporates joint learning of meaningful photographic attributes and image\ncontent information which can help regularize the complicated retargetability\nrating problem. To train and analyze this model, we have collected a database\nwhich contains retargetability scores and meaningful image attributes assigned\nby six expert raters. Experiments demonstrate that our unified model can\ngenerate retargetability rankings that are highly consistent with human labels.\nTo further validate our model, we show applications of image retargetability in\nretargeting method selection, retargeting method assessment and photo collage\ngeneration.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 23:23:50 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 10:49:25 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Tang", "Fan", ""], ["Dong", "Weiming", ""], ["Meng", "Yiping", ""], ["Ma", "Chongyang", ""], ["Wu", "Fuzhang", ""], ["Li", "Xinrui", ""], ["Lee", "Tong-Yee", ""]]}, {"id": "1802.04402", "submitter": "Qiangui Huang", "authors": "Qiangui Huang, Weiyue Wang, Ulrich Neumann", "title": "Recurrent Slice Networks for 3D Segmentation of Point Clouds", "comments": "camera ready version for cvpr 2018 spotlight. codes are available\n  here https://github.com/qianguih/RSNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are an efficient data format for 3D data. However, existing 3D\nsegmentation methods for point clouds either do not model local dependencies\n\\cite{pointnet} or require added computations \\cite{kd-net,pointnet2}. This\nwork presents a novel 3D segmentation framework, RSNet\\footnote{Codes are\nreleased here https://github.com/qianguih/RSNet}, to efficiently model local\nstructures in point clouds. The key component of the RSNet is a lightweight\nlocal dependency module. It is a combination of a novel slice pooling layer,\nRecurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice\npooling layer is designed to project features of unordered points onto an\nordered sequence of feature vectors so that traditional end-to-end learning\nalgorithms (RNNs) can be applied. The performance of RSNet is validated by\ncomprehensive experiments on the S3DIS\\cite{stanford}, ScanNet\\cite{scannet},\nand ShapeNet \\cite{shapenet} datasets. In its simplest form, RSNets surpass all\nprevious state-of-the-art methods on these benchmarks. And comparisons against\nprevious state-of-the-art methods \\cite{pointnet, pointnet2} demonstrate the\nefficiency of RSNets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:04:27 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 19:22:24 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Huang", "Qiangui", ""], ["Wang", "Weiyue", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1802.04403", "submitter": "Haque Ishfaq", "authors": "Haque Ishfaq, Assaf Hoogi and Daniel Rubin", "title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "comments": "After submission, we realized that our work is very similar to work\n  done in \"Bayesian representation learning with oracle constraints\" by\n  Karaletsos et al (arXiv:1506.05011). This paper somehow didn't come into our\n  notice earlier and now that we know the idea we presented in our paper was\n  already explored there, we decided to withdraw our paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning has been demonstrated to be highly effective in learning\nsemantic representation and encoding information that can be used to measure\ndata similarity, by relying on the embedding learned from metric learning. At\nthe same time, variational autoencoder (VAE) has widely been used to\napproximate inference and proved to have a good performance for directed\nprobabilistic models. However, for traditional VAE, the data label or feature\ninformation are intractable. Similarly, traditional representation learning\napproaches fail to represent many salient aspects of the data. In this project,\nwe propose a novel integrated framework to learn latent embedding in VAE by\nincorporating deep metric learning. The features are learned by optimizing a\ntriplet loss on the mean vectors of VAE in conjunction with standard evidence\nlower bound (ELBO) of VAE. This approach, which we call Triplet based\nVariational Autoencoder (TVAE), allows us to capture more fine-grained\ninformation in the latent embedding. Our model is tested on MNIST data set and\nachieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma &\nWelling, 2013) achieves triplet accuracy of 75.08%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:05:19 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 15:31:47 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Ishfaq", "Haque", ""], ["Hoogi", "Assaf", ""], ["Rubin", "Daniel", ""]]}, {"id": "1802.04427", "submitter": "Mina Khoshdeli", "authors": "Mina Khoshdeli and Bahram Parvin", "title": "Deep Learning Models Delineates Multiple Nuclear Phenotypes in H&E\n  Stained Histology Sections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear segmentation is an important step for profiling aberrant regions of\nhistology sections. However, segmentation is a complex problem as a result of\nvariations in nuclear geometry (e.g., size, shape), nuclear type (e.g.,\nepithelial, fibroblast), and nuclear phenotypes (e.g., vesicular, aneuploidy).\nThe problem is further complicated as a result of variations in sample\npreparation. It is shown and validated that fusion of very deep convolutional\nnetworks overcomes (i) complexities associated with multiple nuclear\nphenotypes, and (ii) separation of overlapping nuclei. The fusion relies on\nintegrating of networks that learn region- and boundary-based representations.\nThe system has been validated on a diverse set of nuclear phenotypes that\ncorrespond to the breast and brain histology sections.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 01:50:38 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 19:02:52 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Khoshdeli", "Mina", ""], ["Parvin", "Bahram", ""]]}, {"id": "1802.04441", "submitter": "Li Liu", "authors": "Li Liu, Jie Chen, Guoying Zhao, Paul Fieguth, Xilin Chen, Matti\n  Pietik\\\"ainen", "title": "Texture Classification in Extreme Scale Variations using GANet", "comments": "submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2903300", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in texture recognition often concentrates on recognizing textures\nwith intraclass variations such as illumination, rotation, viewpoint and small\nscale changes. In contrast, in real-world applications a change in scale can\nhave a dramatic impact on texture appearance, to the point of changing\ncompletely from one texture category to another. As a result, texture\nvariations due to changes in scale are amongst the hardest to handle. In this\nwork we conduct the first study of classifying textures with extreme variations\nin scale. To address this issue, we first propose and then reduce scale\nproposals on the basis of dominant texture patterns. Motivated by the\nchallenges posed by this problem, we propose a new GANet network where we use a\nGenetic Algorithm to change the units in the hidden layers during network\ntraining, in order to promote the learning of more informative semantic texture\npatterns. Finally, we adopt a FVCNN (Fisher Vector pooling of a Convolutional\nNeural Network filter bank) feature encoder for global texture representation.\n  Because extreme scale variations are not necessarily present in most standard\ntexture databases, to support the proposed extreme-scale aspects of texture\nunderstanding we are developing a new dataset, the Extreme Scale Variation\nTextures (ESVaT), to test the performance of our framework. It is demonstrated\nthat the proposed framework significantly outperforms gold-standard texture\nfeatures by more than 10% on ESVaT. We also test the performance of our\nproposed approach on the KTHTIPS2b and OS datasets and a further dataset\nsynthetically derived from Forrest, showing superior performance compared to\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 02:29:57 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Liu", "Li", ""], ["Chen", "Jie", ""], ["Zhao", "Guoying", ""], ["Fieguth", "Paul", ""], ["Chen", "Xilin", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1802.04467", "submitter": "Mohan Nikam", "authors": "Mohan Nikam", "title": "An Optimized Architecture for Unpaired Image-to-Image Translation", "comments": "Accepted to be published in Springer Advances in Intelligent Systems\n  and Computing (AISC) Series 11156. Accepted for presentation in Springer\n  ICANI (International Conference on Advanced computing, Networking and\n  Informatics)-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired Image-to-Image translation aims to convert the image from one domain\n(input domain A) to another domain (target domain B), without providing paired\nexamples for the training. The state-of-the-art, Cycle-GAN demonstrated the\npower of Generative Adversarial Networks with Cycle-Consistency Loss. While its\nresults are promising, there is scope for optimization in the training process.\nThis paper introduces a new neural network architecture, which only learns the\ntranslation from domain A to B and eliminates the need for reverse mapping (B\nto A), by introducing a new Deviation-loss term. Furthermore, few other\nimprovements to the Cycle-GAN are found and utilized in this new architecture,\ncontributing to significantly lesser training duration.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 05:56:18 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Nikam", "Mohan", ""]]}, {"id": "1802.04546", "submitter": "Markus Hofinger", "authors": "Markus Hofinger, Thomas Pock and Thomas Moosbrugger", "title": "Robust Deformation Estimation in Wood-Composite Materials using\n  Variational Optical Flow", "comments": "8 pages, 8 figures, originally published in 23 rd Computer Vision\n  Winter Workshop proceedings 2018\n  http://cmp.felk.cvut.cz/cvww2018/papers/28.pdf", "journal-ref": "23rd Computer Vision Winter Workshop proceedings February 2018\n  page 97-104", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wood-composite materials are widely used today as they homogenize humidity\nrelated directional deformations. Quantification of these deformations as\ncoefficients is important for construction and engineering and topic of current\nresearch but still a manual process.\n  This work introduces a novel computer vision approach that automatically\nextracts these properties directly from scans of the wooden specimens, taken at\ndifferent humidity levels during the long lasting humidity conditioning\nprocess. These scans are used to compute a humidity dependent deformation field\nfor each pixel, from which the desired coefficients can easily be calculated.\n  The overall method includes automated registration of the wooden blocks,\nnumerical optimization to compute a variational optical flow field which is\nfurther used to calculate dense strain fields and finally the engineering\ncoefficients and their variance throughout the wooden blocks. The methods\nregularization is fully parameterizable which allows to model and suppress\nartifacts due to surface appearance changes of the specimens from mold, cracks,\netc. that typically arise in the conditioning process.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 10:34:59 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Hofinger", "Markus", ""], ["Pock", "Thomas", ""], ["Moosbrugger", "Thomas", ""]]}, {"id": "1802.04557", "submitter": "Benjamin Wild", "authors": "Benjamin Wild, Leon Sixt, Tim Landgraf", "title": "Automatic localization and decoding of honeybee markers using deep\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The honeybee is a fascinating model animal to investigate how collective\nbehavior emerges from (inter-)actions of thousands of individuals. Bees may\nacquire unique memories throughout their lives. These experiences affect social\ninteractions even over large time frames. Tracking and identifying all bees in\nthe colony over their lifetimes therefore may likely shed light on the\ninterplay of individual differences and colony behavior. This paper proposes a\nsoftware pipeline based on two deep convolutional neural networks for the\nlocalization and decoding of custom binary markers that honeybees carry from\ntheir first to the last day in their life. We show that this approach\noutperforms similar systems proposed in recent literature. By opening this\nsoftware for the public, we hope that the resulting datasets will help\nadvancing the understanding of honeybee collective intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 11:03:30 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 14:27:21 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Wild", "Benjamin", ""], ["Sixt", "Leon", ""], ["Landgraf", "Tim", ""]]}, {"id": "1802.04636", "submitter": "Markos Georgopoulos", "authors": "Markos Georgopoulos, Yannis Panagakis and Maja Pantic", "title": "Modeling of Facial Aging and Kinship: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational facial models that capture properties of facial cues related to\naging and kinship increasingly attract the attention of the research community,\nenabling the development of reliable methods for age progression, age\nestimation, age-invariant facial characterization, and kinship verification\nfrom visual data. In this paper, we review recent advances in modeling of\nfacial aging and kinship. In particular, we provide an up-to date, complete\nlist of available annotated datasets and an in-depth analysis of geometric,\nhand-crafted, and learned facial representations that are used for facial aging\nand kinship characterization. Moreover, evaluation protocols and metrics are\nreviewed and notable experimental results for each surveyed task are analyzed.\nThis survey allows us to identify challenges and discuss future research\ndirections for the development of robust facial models in real-world\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 14:26:40 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 18:05:13 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Georgopoulos", "Markos", ""], ["Panagakis", "Yannis", ""], ["Pantic", "Maja", ""]]}, {"id": "1802.04645", "submitter": "Tianli Liao", "authors": "Tianli Liao and Nan Li", "title": "Single-Perspective Warps in Natural Image Stitching", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2934344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results of image stitching can be perceptually divided into\nsingle-perspective and multiple-perspective. Compared to the\nmultiple-perspective result, the single-perspective result excels in\nperspective consistency but suffers from projective distortion. In this paper,\nwe propose two single-perspective warps for natural image stitching. The first\none is a parametric warp, which is a combination of the\nas-projective-as-possible warp and the quasi-homography warp via dual-feature.\nThe second one is a mesh-based warp, which is determined by optimizing a total\nenergy function that simultaneously emphasizes different characteristics of the\nsingle-perspective warp, including alignment, naturalness, distortion and\nsaliency. A comprehensive evaluation demonstrates that the proposed warp\noutperforms some state-of-the-art warps, including homography, APAP,\nAutoStitch, SPHP and GSP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 14:40:24 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 10:47:37 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Liao", "Tianli", ""], ["Li", "Nan", ""]]}, {"id": "1802.04657", "submitter": "Jeff  (Jun) Zhang", "authors": "Jeff Zhang, Tianyu Gu, Kanad Basu, Siddharth Garg", "title": "Analyzing and Mitigating the Impact of Permanent Faults on a Systolic\n  Array Based Neural Network Accelerator", "comments": "To appear at IEEE VLSI Test Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their growing popularity and computational cost, deep neural networks\n(DNNs) are being targeted for hardware acceleration. A popular architecture for\nDNN acceleration, adopted by the Google Tensor Processing Unit (TPU), utilizes\na systolic array based matrix multiplication unit at its core. This paper deals\nwith the design of fault-tolerant, systolic array based DNN accelerators for\nhigh defect rate technologies. To this end, we empirically show that the\nclassification accuracy of a baseline TPU drops significantly even at extremely\nlow fault rates (as low as $0.006\\%$). We then propose two novel strategies,\nfault-aware pruning (FAP) and fault-aware pruning+retraining (FAP+T), that\nenable the TPU to operate at fault rates of up to $50\\%$, with negligible drop\nin classification accuracy (as low as $0.1\\%$) and no run-time performance\noverhead. The FAP+T does introduce a one-time retraining penalty per TPU chip\nbefore it is deployed, but we propose optimizations that reduce this one-time\npenalty to under 12 minutes. The penalty is then amortized over the entire\nlifetime of the TPU's operation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:51:35 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 06:19:12 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Zhang", "Jeff", ""], ["Gu", "Tianyu", ""], ["Basu", "Kanad", ""], ["Garg", "Siddharth", ""]]}, {"id": "1802.04668", "submitter": "Akisato Kimura", "authors": "Yusuke Mukuta, Akisato Kimura, David B Adrian, Zoubin Ghahramani", "title": "Weakly supervised collective feature learning from curated media", "comments": "Published in the Proceedings of AAAI Conferenrence on Artificial\n  Intelligence (AAAI2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art in feature learning relies on the supervised\nlearning of large-scale datasets consisting of target content items and their\nrespective category labels. However, constructing such large-scale\nfully-labeled datasets generally requires painstaking manual effort. One\npossible solution to this problem is to employ community contributed text tags\nas weak labels, however, the concepts underlying a single text tag strongly\ndepends on the users. We instead present a new paradigm for learning\ndiscriminative features by making full use of the human curation process on\nsocial networking services (SNSs). During the process of content curation, SNS\nusers collect content items manually from various sources and group them by\ncontext, all for their own benefit. Due to the nature of this process, we can\nassume that (1) content items in the same group share the same semantic concept\nand (2) groups sharing the same images might have related semantic concepts.\nThrough these insights, we can define human curated groups as weak labels from\nwhich our proposed framework can learn discriminative features as a\nrepresentation in the space of semantic concepts the users intended when\ncreating the groups. We show that this feature learning can be formulated as a\nproblem of link prediction for a bipartite graph whose nodes corresponds to\ncontent items and human curated groups, and propose a novel method for feature\nlearning based on sparse coding or network fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:05:10 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Mukuta", "Yusuke", ""], ["Kimura", "Akisato", ""], ["Adrian", "David B", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1802.04692", "submitter": "Jingfan Fan", "authors": "Jingfan Fan, Xiaohuan Cao, Pew-Thian Yap, and Dinggang Shen", "title": "BIRNet: Brain Image Registration Using Dual-Supervised Fully\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep learning approach for image registration by\npredicting deformation from image appearance. Since obtaining ground-truth\ndeformation fields for training can be challenging, we design a fully\nconvolutional network that is subject to dual-guidance: (1) Coarse guidance\nusing deformation fields obtained by an existing registration method; and (2)\nFine guidance using image similarity. The latter guidance helps avoid overly\nrelying on the supervision from the training deformation fields, which could be\ninaccurate. For effective training, we further improve the deep convolutional\nnetwork with gap filling, hierarchical loss, and multi-source strategies.\nExperiments on a variety of datasets show promising registration accuracy and\nefficiency compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:49:34 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Fan", "Jingfan", ""], ["Cao", "Xiaohuan", ""], ["Yap", "Pew-Thian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1802.04723", "submitter": "Xin Li", "authors": "Weishong Dong, Ming Yuan, Xin Li and Guangming Shi", "title": "Joint Demosaicing and Denoising with Perceptual Optimization on a\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image demosaicing - one of the most important early stages in digital camera\npipelines - addressed the problem of reconstructing a full-resolution image\nfrom so-called color-filter-arrays. Despite tremendous progress made in the\npase decade, a fundamental issue that remains to be addressed is how to assure\nthe visual quality of reconstructed images especially in the presence of noise\ncorruption. Inspired by recent advances in generative adversarial networks\n(GAN), we present a novel deep learning approach toward joint demosaicing and\ndenoising (JDD) with perceptual optimization in order to ensure the visual\nquality of reconstructed images. The key contributions of this work include: 1)\nwe have developed a GAN-based approach toward image demosacing in which a\ndiscriminator network with both perceptual and adversarial loss functions are\nused for quality assurance; 2) we propose to optimize the perceptual quality of\nreconstructed images by the proposed GAN in an end-to-end manner. Such\nend-to-end optimization of GAN is particularly effective for jointly exploiting\nthe gain brought by each modular component (e.g., residue learning in the\ngenerative network and perceptual loss in the discriminator network). Our\nextensive experimental results have shown convincingly improved performance\nover existing state-of-the-art methods in terms of both subjective and\nobjective quality metrics with a comparable computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:40:12 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Dong", "Weishong", ""], ["Yuan", "Ming", ""], ["Li", "Xin", ""], ["Shi", "Guangming", ""]]}, {"id": "1802.04735", "submitter": "Teofilo De Campos", "authors": "Andre Bernardes Soares Guedes and Teofilo Emidio de Campos and Adrian\n  Hilton", "title": "Semantic Scene Completion Combining Colour and Depth: preliminary\n  experiments", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene completion is the task of producing a complete 3D voxel\nrepresentation of volumetric occupancy with semantic labels for a scene from a\nsingle-view observation. We built upon the recent work of Song et al. (CVPR\n2017), who proposed SSCnet, a method that performs scene completion and\nsemantic labelling in a single end-to-end 3D convolutional network. SSCnet uses\nonly depth maps as input, even though depth maps are usually obtained from\ndevices that also capture colour information, such as RGBD sensors and stereo\ncameras. In this work, we investigate the potential of the RGB colour channels\nto improve SSCnet.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:59:40 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Guedes", "Andre Bernardes Soares", ""], ["de Campos", "Teofilo Emidio", ""], ["Hilton", "Adrian", ""]]}, {"id": "1802.04738", "submitter": "Sergio Caccamo S", "authors": "Sergio Caccamo, Esra Ataer-Cansizoglu and Yuichi Taguchi", "title": "Joint 3D Reconstruction of a Static Scene and Moving Objects", "comments": "This paper has been accepted and presented in 3DV-2017 conference\n  held at Qingdao, China. Video experiments: https://youtu.be/goflUxzG2VI", "journal-ref": "Proceedings International Conference on 3D Vision 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for simultaneous 3D reconstruction of static regions\nand rigidly moving objects in a scene. An RGB-D frame is represented as a\ncollection of features, which are points and planes. We classify the features\ninto static and dynamic regions and grow separate maps, static and object maps,\nfor each of them. To robustly classify the features in each frame, we fuse\nmultiple RANSAC-based registration results obtained by registering different\ngroups of the features to different maps, including (1) all the features to the\nstatic map, (2) all the features to each object map, and (3) subsets of the\nfeatures, each forming a segment, to each object map. This multi-group\nregistration approach is designed to overcome the following challenges: scenes\ncan be dominated by static regions, making object tracking more difficult; and\nmoving object might have larger pose variation between frames compared to the\nstatic regions. We show qualitative results from indoor scenes with objects in\nvarious shapes. The technique enables on-the-fly object model generation to be\nused for robotic manipulation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 17:05:55 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Caccamo", "Sergio", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Taguchi", "Yuichi", ""]]}, {"id": "1802.04762", "submitter": "Haiguang Wen", "authors": "Haiguang Wen, Kuan Han, Junxing Shi, Yizhen Zhang, Eugenio\n  Culurciello, Zhongming Liu", "title": "Deep Predictive Coding Network for Object Recognition", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the predictive coding theory in neuroscience, we designed a\nbi-directional and recurrent neural net, namely deep predictive coding networks\n(PCN). It has feedforward, feedback, and recurrent connections. Feedback\nconnections from a higher layer carry the prediction of its lower-layer\nrepresentation; feedforward connections carry the prediction errors to its\nhigher-layer. Given image input, PCN runs recursive cycles of bottom-up and\ntop-down computation to update its internal representations and reduce the\ndifference between bottom-up input and top-down prediction at every layer.\nAfter multiple cycles of recursive updating, the representation is used for\nimage classification. With benchmark data (CIFAR-10/100, SVHN, and MNIST), PCN\nwas found to always outperform its feedforward-only counterpart: a model\nwithout any mechanism for recurrent dynamics. Its performance tended to improve\ngiven more cycles of computation over time. In short, PCN reuses a single\narchitecture to recursively run bottom-up and top-down processes. As a\ndynamical system, PCN can be unfolded to a feedforward model that becomes\ndeeper and deeper over time, while refining it representation towards more\naccurate and definitive object recognition.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 17:49:33 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 12:55:24 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wen", "Haiguang", ""], ["Han", "Kuan", ""], ["Shi", "Junxing", ""], ["Zhang", "Yizhen", ""], ["Culurciello", "Eugenio", ""], ["Liu", "Zhongming", ""]]}, {"id": "1802.04834", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, John K. Tsotsos", "title": "Challenging Images For Minds and Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no denying the tremendous leap in the performance of machine\nlearning methods in the past half-decade. Some might even say that specific\nsub-fields in pattern recognition, such as machine-vision, are as good as\nsolved, reaching human and super-human levels. Arguably, lack of training data\nand computation power are all that stand between us and solving the remaining\nones. In this position paper we underline cases in vision which are challenging\nto machines and even to human observers. This is to show limitations of\ncontemporary models that are hard to ameliorate by following the current trend\nto increase training data, network capacity or computational power. Moreover,\nwe claim that attempting to do so is in principle a suboptimal approach. We\nprovide a taster of such examples in hope to encourage and challenge the\nmachine learning community to develop new directions to solve the said\ndifficulties.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 19:50:41 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1802.04877", "submitter": "Natasha Jaques", "authors": "Natasha Jaques, Jennifer McCleary, Jesse Engel, David Ha, Fred\n  Bertsch, Rosalind Picard, Douglas Eck", "title": "Learning via social awareness: Improving a deep generative sketching\n  model with facial feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quest towards general artificial intelligence (AI), researchers have\nexplored developing loss functions that act as intrinsic motivators in the\nabsence of external rewards. This paper argues that such research has\noverlooked an important and useful intrinsic motivator: social interaction. We\nposit that making an AI agent aware of implicit social feedback from humans can\nallow for faster learning of more generalizable and useful representations, and\ncould potentially impact AI safety. We collect social feedback in the form of\nfacial expression reactions to samples from Sketch RNN, an LSTM-based\nvariational autoencoder (VAE) designed to produce sketch drawings. We use a\nLatent Constraints GAN (LC-GAN) to learn from the facial feedback of a small\ngroup of viewers, by optimizing the model to produce sketches that it predicts\nwill lead to more positive facial expressions. We show in multiple independent\nevaluations that the model trained with facial feedback produced sketches that\nare more highly rated, and induce significantly more positive facial\nexpressions. Thus, we establish that implicit social feedback can improve the\noutput of a deep learning model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:19:10 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 18:45:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Jaques", "Natasha", ""], ["McCleary", "Jennifer", ""], ["Engel", "Jesse", ""], ["Ha", "David", ""], ["Bertsch", "Fred", ""], ["Picard", "Rosalind", ""], ["Eck", "Douglas", ""]]}, {"id": "1802.04881", "submitter": "David G\\\"uera", "authors": "Sri Kalyan Yarlagadda, David G\\\"uera, Paolo Bestagini, Fengqing Maggie\n  Zhu, Stefano Tubaro, Edward J. Delp", "title": "Satellite Image Forgery Detection and Localization Using GAN and\n  One-Class Classifier", "comments": "Presented at the IS&T International Symposium on Electronic Imaging\n  (EI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current satellite imaging technology enables shooting high-resolution\npictures of the ground. As any other kind of digital images, overhead pictures\ncan also be easily forged. However, common image forensic techniques are often\ndeveloped for consumer camera images, which strongly differ in their nature\nfrom satellite ones (e.g., compression schemes, post-processing, sensors,\netc.). Therefore, many accurate state-of-the-art forensic algorithms are bound\nto fail if blindly applied to overhead image analysis. Development of novel\nforensic tools for satellite images is paramount to assess their authenticity\nand integrity. In this paper, we propose an algorithm for satellite image\nforgery detection and localization. Specifically, we consider the scenario in\nwhich pixels within a region of a satellite image are replaced to add or remove\nan object from the scene. Our algorithm works under the assumption that no\nforged images are available for training. Using a generative adversarial\nnetwork (GAN), we learn a feature representation of pristine satellite images.\nA one-class support vector machine (SVM) is trained on these features to\ndetermine their distribution. Finally, image forgeries are detected as\nanomalies. The proposed algorithm is validated against different kinds of\nsatellite images containing forgeries of different size and shape.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:28:58 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Yarlagadda", "Sri Kalyan", ""], ["G\u00fcera", "David", ""], ["Bestagini", "Paolo", ""], ["Zhu", "Fengqing Maggie", ""], ["Tubaro", "Stefano", ""], ["Delp", "Edward J.", ""]]}, {"id": "1802.04894", "submitter": "Boyu Zhang", "authors": "Boyu Zhang, Yingtao Zhang, H. D. Cheng, Min Xian, Shan Gai, Olivia\n  Cheng, Kuan Huang", "title": "Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A\n  Survey", "comments": "10 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoarthritis (OA) is one of the major health issues among the elderly\npopulation. MRI is the most popular technology to observe and evaluate the\nprogress of OA course. However, the extreme labor cost of MRI analysis makes\nthe process inefficient and expensive. Also, due to human error and subjective\nnature, the inter- and intra-observer variability is rather high.\nComputer-aided knee MRI segmentation is currently an active research field\nbecause it can alleviate doctors and radiologists from the time consuming and\ntedious job, and improve the diagnosis performance which has immense potential\nfor both clinic and scientific research. In the past decades, researchers have\ninvestigated automatic/semi-automatic knee MRI segmentation methods\nextensively. However, to the best of our knowledge, there is no comprehensive\nsurvey paper in this field yet. In this survey paper, we classify the existing\nmethods by their principles and discuss the current research status and point\nout the future research trend in-depth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 23:26:01 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhang", "Boyu", ""], ["Zhang", "Yingtao", ""], ["Cheng", "H. D.", ""], ["Xian", "Min", ""], ["Gai", "Shan", ""], ["Cheng", "Olivia", ""], ["Huang", "Kuan", ""]]}, {"id": "1802.04914", "submitter": "Yan Wang", "authors": "Houdong Hu, Yan Wang, Linjun Yang, Pavel Komlev, Li Huang, Xi Chen,\n  Jiapei Huang, Ye Wu, Meenaz Merchant, Arun Sacheti", "title": "Web-Scale Responsive Visual Search at Bing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a web-scale general visual search system deployed\nin Microsoft Bing. The system accommodates tens of billions of images in the\nindex, with thousands of features for each image, and can respond in less than\n200 ms. In order to overcome the challenges in relevance, latency, and\nscalability in such large scale of data, we employ a cascaded learning-to-rank\nframework based on various latest deep learning visual features, and deploy in\na distributed heterogeneous computing platform. Quantitative and qualitative\nexperiments show that our system is able to support various applications on\nBing website and apps.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:15:22 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 23:56:07 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Hu", "Houdong", ""], ["Wang", "Yan", ""], ["Yang", "Linjun", ""], ["Komlev", "Pavel", ""], ["Huang", "Li", ""], ["Chen", "Xi", ""], ["Huang", "Jiapei", ""], ["Wu", "Ye", ""], ["Merchant", "Meenaz", ""], ["Sacheti", "Arun", ""]]}, {"id": "1802.04936", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Esteban Moro, Manuel Cebrian and Iyad Rahwan", "title": "MemeSequencer: Sparse Matching for Embedding Image Macros", "comments": "9 pages (+2 pages references), camera ready version for International\n  World Wide Web Conference (WWW) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the creation, mutation, and propagation of social media\ncontent on the Internet is an essential problem in computational social\nscience, affecting areas ranging from marketing to political mobilization. A\nfirst step towards understanding the evolution of images online is the analysis\nof rapidly modifying and propagating memetic imagery or `memes'. However, a\npitfall in proceeding with such an investigation is the current incapability to\nproduce a robust semantic space for such imagery, capable of understanding\ndifferences in Image Macros. In this study, we provide a first step in the\nsystematic study of image evolution on the Internet, by proposing an algorithm\nbased on sparse representations and deep learning to decouple various types of\ncontent in such images and produce a rich semantic embedding. We demonstrate\nthe benefits of our approach on a variety of tasks pertaining to memes and\nImage Macros, such as image clustering, image retrieval, topic prediction and\nvirality prediction, surpassing the existing methods on each. In addition to\nits utility on quantitative tasks, our method opens up the possibility of\nobtaining the first large-scale understanding of the evolution and propagation\nof memetic imagery.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 02:53:20 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Moro", "Esteban", ""], ["Cebrian", "Manuel", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1802.04962", "submitter": "Dong-Jin Kim", "authors": "Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, Youngjin Yoon, In So Kweon", "title": "Disjoint Multi-task Learning between Heterogeneous Human-centric Tasks", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior understanding is arguably one of the most important mid-level\ncomponents in artificial intelligence. In order to efficiently make use of\ndata, multi-task learning has been studied in diverse computer vision tasks\nincluding human behavior understanding. However, multi-task learning relies on\ntask specific datasets and constructing such datasets can be cumbersome. It\nrequires huge amounts of data, labeling efforts, statistical consideration etc.\nIn this paper, we leverage existing single-task datasets for human action\nclassification and captioning data for efficient human behavior learning. Since\nthe data in each dataset has respective heterogeneous annotations, traditional\nmulti-task learning is not effective in this scenario. To this end, we propose\na novel alternating directional optimization method to efficiently learn from\nthe heterogeneous data. We demonstrate the effectiveness of our model and show\nperformance improvements on both classification and sentence retrieval tasks in\ncomparison to the models trained on each of the single-task datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 05:36:14 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Kim", "Dong-Jin", ""], ["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Yoon", "Youngjin", ""], ["Kweon", "In So", ""]]}, {"id": "1802.04977", "submitter": "Jangho Kim", "authors": "Jangho Kim, SeongUk Park, Nojun Kwak", "title": "Paraphrasing Complex Network: Network Compression via Factor Transfer", "comments": "Advances in Neural Information Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many researchers have sought ways of model compression to reduce the size of\na deep neural network (DNN) with minimal performance degradation in order to\nuse DNNs in embedded systems. Among the model compression methods, a method\ncalled knowledge transfer is to train a student network with a stronger teacher\nnetwork. In this paper, we propose a novel knowledge transfer method which uses\nconvolutional operations to paraphrase teacher's knowledge and to translate it\nfor the student. This is done by two convolutional modules, which are called a\nparaphraser and a translator. The paraphraser is trained in an unsupervised\nmanner to extract the teacher factors which are defined as paraphrased\ninformation of the teacher network. The translator located at the student\nnetwork extracts the student factors and helps to translate the teacher factors\nby mimicking them. We observed that our student network trained with the\nproposed factor transfer method outperforms the ones trained with conventional\nknowledge transfer methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 07:46:15 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 06:59:24 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 12:42:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kim", "Jangho", ""], ["Park", "SeongUk", ""], ["Kwak", "Nojun", ""]]}, {"id": "1802.04979", "submitter": "Kunfeng Wang", "authors": "Kunfeng Wang, Chao Gou, Fei-Yue Wang", "title": "M4CD: A Robust Change Detection Method for Intelligent Visual\n  Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust change detection method for intelligent\nvisual surveillance. This method, named M4CD, includes three major steps.\nFirstly, a sample-based background model that integrates color and texture cues\nis built and updated over time. Secondly, multiple heterogeneous features\n(including brightness variation, chromaticity variation, and texture variation)\nare extracted by comparing the input frame with the background model, and a\nmulti-source learning strategy is designed to online estimate the probability\ndistributions for both foreground and background. The three features are\napproximately conditionally independent, making multi-source learning feasible.\nPixel-wise foreground posteriors are then estimated with Bayes rule. Finally,\nthe Markov random field (MRF) optimization and heuristic post-processing\ntechniques are used sequentially to improve accuracy. In particular, a\ntwo-layer MRF model is constructed to represent pixel-based and\nsuperpixel-based contextual constraints compactly. Experimental results on the\nCDnet dataset indicate that M4CD is robust under complex environments and ranks\namong the top methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 07:51:59 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Wang", "Kunfeng", ""], ["Gou", "Chao", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1802.05023", "submitter": "Arno Solin", "authors": "Ari Heljakka, Arno Solin, Juho Kannala", "title": "Recursive Chaining of Reversible Image-to-image Translators For Face\n  Aging", "comments": "To appear in Advanced Concepts for Intelligent Vision Systems (ACIVS)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the modeling and simulation of progressive changes over\ntime, such as human face aging. By treating the age phases as a sequence of\nimage domains, we construct a chain of transformers that map images from one\nage domain to the next. Leveraging recent adversarial image translation\nmethods, our approach requires no training samples of the same individual at\ndifferent ages. Here, the model must be flexible enough to translate a child\nface to a young adult, and all the way through the adulthood to old age. We\nfind that some transformers in the chain can be recursively applied on their\nown output to cover multiple phases, compressing the chain. The structure of\nthe chain also unearths information about the underlying physical process. We\ndemonstrate the performance of our method with precise and intuitive metrics,\nand visually match with the face aging state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 10:22:38 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 19:24:00 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Heljakka", "Ari", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1802.05097", "submitter": "Boguslaw Obara", "authors": "Cigdem Sazak, Carl J. Nelson, Boguslaw Obara", "title": "The Multiscale Bowler-Hat Transform for Vessel Enhancement in 3D\n  Biomedical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement and detection of 3D vessel-like structures has long been an open\nproblem as most existing image processing methods fail in many aspects,\nincluding a lack of uniform enhancement between vessels of different radii and\na lack of enhancement at the junctions.\n  Here, we propose a method based on mathematical morphology to enhance 3D\nvessel-like structures in biomedical images. The proposed method, 3D bowler-hat\ntransform, combines sphere and line structuring elements to enhance vessel-like\nstructures. The proposed method is validated on synthetic and real data and\ncompared with state-of-the-art methods.\n  Our results show that the proposed method achieves a high-quality vessel-like\nstructures enhancement in both synthetic and real biomedical images, and is\nable to cope with variations in vessels thickness throughout vascular networks\nwhile remaining robust at junctions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 13:59:46 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Sazak", "Cigdem", ""], ["Nelson", "Carl J.", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1802.05160", "submitter": "Xi Zhang", "authors": "Xiaolin Wu, Xi Zhang and Xiao Shu", "title": "Cognitive Deficit of Deep Learning in Numerosity", "comments": "Accepted for presentation at the AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subitizing, or the sense of small natural numbers, is an innate cognitive\nfunction of humans and primates; it responds to visual stimuli prior to the\ndevelopment of any symbolic skills, language or arithmetic. Given successes of\ndeep learning (DL) in tasks of visual intelligence and given the primitivity of\nnumber sense, a tantalizing question is whether DL can comprehend numbers and\nperform subitizing. But somewhat disappointingly, extensive experiments of the\ntype of cognitive psychology demonstrate that the examples-driven black box DL\ncannot see through superficial variations in visual representations and distill\nthe abstract notion of natural number, a task that children perform with high\naccuracy and confidence. The failure is apparently due to the learning method\nnot the CNN computational machinery itself. A recurrent neural network capable\nof subitizing does exist, which we construct by encoding a mechanism of\nmathematical morphology into the CNN convolutional kernels. Also, we\ninvestigate, using subitizing as a test bed, the ways to aid the black box DL\nby cognitive priors derived from human insight. Our findings are mixed and\ninteresting, pointing to both cognitive deficit of pure DL, and some measured\nsuccesses of boosting DL by predetermined cognitive implements. This case study\nof DL in cognitive computing is meaningful for visual numerosity represents a\nminimum level of human intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 15:01:52 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 12:35:52 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 03:14:50 GMT"}, {"version": "v4", "created": "Sun, 11 Nov 2018 14:47:31 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wu", "Xiaolin", ""], ["Zhang", "Xi", ""], ["Shu", "Xiao", ""]]}, {"id": "1802.05176", "submitter": "Paulo Ferreira", "authors": "Paulo Ferreira", "title": "Sampling Superquadric Point Clouds with Normals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superquadrics provide a compact representation of common shapes and have been\nused both for object/surface modelling in computer graphics and as object-part\nrepresentation in computer vision and robotics. Superquadrics refer to a family\nof shapes: here we deal with the superellipsoids and superparaboloids. Due to\nthe strong non-linearities involved in the equations, uniform or\nclose-to-uniform sampling is not attainable through a naive approach of direct\nsampling from the parametric formulation. This is specially true for more\n`cubic' superquadrics (with shape parameters close to $0.1$). We extend a\nprevious solution of 2D close-to-uniform uniform sampling of superellipses to\nthe superellipsoid (3D) case and derive our own for the superparaboloid.\nAdditionally, we are able to provide normals for each sampled point. To the\nbest of our knowledge, this is the first complete approach for close-to-uniform\nsampling of superellipsoids and superparaboloids in one single framework. We\npresent derivations, pseudocode and qualitative and quantitative results using\nour code, which is available online.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:04:27 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Ferreira", "Paulo", ""]]}, {"id": "1802.05203", "submitter": "Hongwei Li", "authors": "Hongwei Li, Gongfa Jiang, Jianguo Zhang, Ruixuan Wang, Zhaolei Wang,\n  Wei-Shi Zheng and Bjoern Menze", "title": "Fully Convolutional Network Ensembles for White Matter Hyperintensities\n  Segmentation in MR Images", "comments": "final version in NeuroImage", "journal-ref": "Neuroimage. 2018 Aug 17. pii: S1053-8119(18)30597-4", "doi": "10.1016/j.neuroimage.2018.07.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White matter hyperintensities (WMH) are commonly found in the brains of\nhealthy elderly individuals and have been associated with various neurological\nand geriatric disorders. In this paper, we present a study using deep fully\nconvolutional network and ensemble models to automatically detect such WMH\nusing fluid attenuation inversion recovery (FLAIR) and T1 magnetic resonance\n(MR) scans. The algorithm was evaluated and ranked 1 st in the WMH Segmentation\nChallenge at MICCAI 2017. In the evaluation stage, the implementation of the\nalgorithm was submitted to the challenge organizers, who then independently\ntested it on a hidden set of 110 cases from 5 scanners. Averaged dice score,\nprecision and robust Hausdorff distance obtained on held-out test datasets were\n80%, 84% and 6.30mm respectively. These were the highest achieved in the\nchallenge, suggesting the proposed method is the state-of-the-art. In this\npaper, we provide detailed descriptions and quantitative analysis on key\ncomponents of the system. Furthermore, a study of cross-scanner evaluation is\npresented to discuss how the combination of modalities and data augmentation\naffect the generalization capability of the system. The adaptability of the\nsystem to different scanners and protocols is also investigated. A quantitative\nstudy is further presented to test the effect of ensemble size. Additionally,\nsoftware and models of our method are made publicly available. The\neffectiveness and generalization capability of the proposed system show its\npotential for real-world clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 16:49:41 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 21:50:46 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 09:47:47 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Li", "Hongwei", ""], ["Jiang", "Gongfa", ""], ["Zhang", "Jianguo", ""], ["Wang", "Ruixuan", ""], ["Wang", "Zhaolei", ""], ["Zheng", "Wei-Shi", ""], ["Menze", "Bjoern", ""]]}, {"id": "1802.05214", "submitter": "Ayan Chakrabarti", "authors": "Francesco Pittaluga, Sanjeev J. Koppal, Ayan Chakrabarti", "title": "Learning Privacy Preserving Encodings through Adversarial Training", "comments": "To appear in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to learn privacy-preserving encodings of images that\ninhibit inference of chosen private attributes, while allowing recovery of\nother desirable information. Rather than simply inhibiting a given fixed\npre-trained estimator, our goal is that an estimator be unable to learn to\naccurately predict the private attributes even with knowledge of the encoding\nfunction. We use a natural adversarial optimization-based formulation for\nthis---training the encoding function against a classifier for the private\nattribute, with both modeled as deep neural networks. The key contribution of\nour work is a stable and convergent optimization approach that is successful at\nlearning an encoder with our desired properties---maintaining utility while\ninhibiting inference of private attributes, not just within the adversarial\noptimization, but also by classifiers that are trained after the encoder is\nfixed. We adopt a rigorous experimental protocol for verification wherein\nclassifiers are trained exhaustively till saturation on the fixed encoders. We\nevaluate our approach on tasks of real-world complexity---learning\nhigh-dimensional encodings that inhibit detection of different scene\ncategories---and find that it yields encoders that are resilient at maintaining\nprivacy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 17:04:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 21:33:11 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 19:24:57 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Pittaluga", "Francesco", ""], ["Koppal", "Sanjeev J.", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1802.05300", "submitter": "Mantas Mazeika", "authors": "Dan Hendrycks, Mantas Mazeika, Duncan Wilson, Kevin Gimpel", "title": "Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe\n  Noise", "comments": "NeurIPS 2018. PyTorch code available at\n  https://github.com/mmazeika/glc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing importance of massive datasets used for deep learning makes\nrobustness to label noise a critical property for classifiers to have. Sources\nof label noise include automatic labeling, non-expert labeling, and label\ncorruption by data poisoning adversaries. Numerous previous works assume that\nno source of labels can be trusted. We relax this assumption and assume that a\nsmall subset of the training data is trusted. This enables substantial label\ncorruption robustness performance gains. In addition, particularly severe label\nnoise can be combated by using a set of trusted data with clean labels. We\nutilize trusted data by proposing a loss correction technique that utilizes\ntrusted examples in a data-efficient manner to mitigate the effects of label\nnoise on deep neural network classifiers. Across vision and natural language\nprocessing tasks, we experiment with various label noises at several strengths,\nand show that our method significantly outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 19:48:50 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 19:07:19 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 17:41:23 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 19:55:36 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mazeika", "Mantas", ""], ["Wilson", "Duncan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1802.05342", "submitter": "Haraldur Hallgr\\'imsson", "authors": "Haraldur T. Hallgr\\'imsson, Matthew Cieslak, Luca Foschini, Scott T.\n  Grafton, Ambuj K. Singh", "title": "Spatial Coherence of Oriented White Matter Microstructure: Applications\n  to White Matter Regions Associated with Genetic Similarity", "comments": null, "journal-ref": "NeuroImage (2018)", "doi": "10.1016/j.neuroimage.2018.01.050", "report-no": null, "categories": "stat.AP cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to discover differences between populations with respect\nto the spatial coherence of their oriented white matter microstructure in\narbitrarily shaped white matter regions. This method is applied to diffusion\nMRI scans of a subset of the Human Connectome Project dataset: 57 pairs of\nmonozygotic and 52 pairs of dizygotic twins. After controlling for\nmorphological similarity between twins, we identify 3.7% of all white matter as\nbeing associated with genetic similarity (35.1k voxels, $p < 10^{-4}$, false\ndiscovery rate 1.5%), 75% of which spatially clusters into twenty-two\ncontiguous white matter regions. Furthermore, we show that the orientation\nsimilarity within these regions generalizes to a subset of 47 pairs of non-twin\nsiblings, and show that these siblings are on average as similar as dizygotic\ntwins. The regions are located in deep white matter including the superior\nlongitudinal fasciculus, the optic radiations, the middle cerebellar peduncle,\nthe corticospinal tract, and within the anterior temporal lobe, as well as the\ncerebellum, brain stem, and amygdalae.\n  These results extend previous work using undirected fractional anisotrophy\nfor measuring putative heritable influences in white matter. Our\nmultidirectional extension better accounts for crossing fiber connections\nwithin voxels. This bottom up approach has at its basis a novel measurement of\ncoherence within neighboring voxel dyads between subjects, and avoids some of\nthe fundamental ambiguities encountered with tractographic approaches to white\nmatter analysis that estimate global connectivity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 22:31:14 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Hallgr\u00edmsson", "Haraldur T.", ""], ["Cieslak", "Matthew", ""], ["Foschini", "Luca", ""], ["Grafton", "Scott T.", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1802.05384", "submitter": "Thibault Groueix M.", "authors": "Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell,\n  Mathieu Aubry", "title": "AtlasNet: A Papier-M\\^ach\\'e Approach to Learning 3D Surface Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for learning to generate the surface of 3D shapes. Our\napproach represents a 3D shape as a collection of parametric surface elements\nand, in contrast to methods generating voxel grids or point clouds, naturally\ninfers a surface representation of the shape. Beyond its novelty, our new shape\ngeneration framework, AtlasNet, comes with significant advantages, such as\nimproved precision and generalization capabilities, and the possibility to\ngenerate a shape of arbitrary resolution without memory issues. We demonstrate\nthese benefits and compare to strong baselines on the ShapeNet benchmark for\ntwo applications: (i) auto-encoding shapes, and (ii) single-view reconstruction\nfrom a still image. We also provide results showing its potential for other\napplications, such as morphing, parametrization, super-resolution, matching,\nand co-segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 02:07:30 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 10:42:48 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 16:00:34 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Groueix", "Thibault", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Russell", "Bryan C.", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1802.05385", "submitter": "Congzheng Song", "authors": "Congzheng Song, Vitaly Shmatikov", "title": "Fooling OCR Systems with Adversarial Text Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that state-of-the-art optical character recognition (OCR)\nbased on deep learning is vulnerable to adversarial images. Minor modifications\nto images of printed text, which do not change the meaning of the text to a\nhuman reader, cause the OCR system to \"recognize\" a different text where\ncertain words chosen by the adversary are replaced by their semantic opposites.\nThis completely changes the meaning of the output produced by the OCR system\nand by the NLP applications that use OCR for preprocessing their inputs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 02:08:19 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Song", "Congzheng", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "1802.05415", "submitter": "Sumeet Sohan Singh", "authors": "Sumeet S. Singh", "title": "Teaching Machines to Code: Neural Markup Generation with Visual\n  Attention", "comments": "For datasets, visualizations and ancillary material see:\n  https://untrix.github.io/i2l . For source code go to:\n  https://github.com/untrix/im2latex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural transducer model with visual attention that learns to\ngenerate LaTeX markup of a real-world math formula given its image. Applying\nsequence modeling and transduction techniques that have been very successful\nacross modalities such as natural language, image, handwriting, speech and\naudio; we construct an image-to-markup model that learns to produce\nsyntactically and semantically correct LaTeX markup code over 150 words long\nand achieves a BLEU score of 89%; improving upon the previous state-of-art for\nthe Im2Latex problem. We also demonstrate with heat-map visualization how\nattention helps in interpreting the model and can pinpoint (detect and\nlocalize) symbols on the image accurately despite having been trained without\nany bounding box data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 06:17:51 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 21:36:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Singh", "Sumeet S.", ""]]}, {"id": "1802.05451", "submitter": "Roei Herzig", "authors": "Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, Amir\n  Globerson", "title": "Mapping Images to Scene Graphs with Permutation-Invariant Structured\n  Prediction", "comments": "Paper is accepted for NIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine understanding of complex images is a key goal of artificial\nintelligence. One challenge underlying this task is that visual scenes contain\nmultiple inter-related objects, and that global context plays an important role\nin interpreting the scene. A natural modeling framework for capturing such\neffects is structured prediction, which optimizes over complex labels, while\nmodeling within-label interactions. However, it is unclear what principles\nshould guide the design of a structured prediction model that utilizes the\npower of deep learning components. Here we propose a design principle for such\narchitectures that follows from a natural requirement of permutation\ninvariance. We prove a necessary and sufficient characterization for\narchitectures that follow this invariance, and discuss its implication on model\ndesign. Finally, we show that the resulting model achieves new state of the art\nresults on the Visual Genome scene graph labeling benchmark, outperforming all\nrecent approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:50:15 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 18:07:49 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 13:26:04 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 21:48:19 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Herzig", "Roei", ""], ["Raboh", "Moshiko", ""], ["Chechik", "Gal", ""], ["Berant", "Jonathan", ""], ["Globerson", "Amir", ""]]}, {"id": "1802.05518", "submitter": "Aline Sindel", "authors": "Aline Sindel, Katharina Breininger, Johannes K\\\"a{\\ss}er, Andreas\n  Hess, Andreas Maier, Thomas K\\\"ohler", "title": "Learning from a Handful Volumes: MRI Resolution Enhancement with\n  Volumetric Super-Resolution Forests", "comments": "Preprint submitted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) enables 3-D imaging of anatomical\nstructures. However, the acquisition of MR volumes with high spatial resolution\nleads to long scan times. To this end, we propose volumetric super-resolution\nforests (VSRF) to enhance MRI resolution retrospectively. Our method learns a\nlocally linear mapping between low-resolution and high-resolution volumetric\nimage patches by employing random forest regression. We customize features\nsuitable for volumetric MRI to train the random forest and propose a median\ntree ensemble for robust regression. VSRF outperforms state-of-the-art\nexample-based super-resolution in term of image quality and efficiency for\nmodel training and inference in different MRI datasets. It is also superior to\nunsupervised methods with just a handful or even a single volume to assemble\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 13:23:31 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Sindel", "Aline", ""], ["Breininger", "Katharina", ""], ["K\u00e4\u00dfer", "Johannes", ""], ["Hess", "Andreas", ""], ["Maier", "Andreas", ""], ["K\u00f6hler", "Thomas", ""]]}, {"id": "1802.05521", "submitter": "Sanaullah Manzoor", "authors": "M Faisal, Sanaullah Manzoor", "title": "Deep Learning for Lip Reading using Audio-Visual Information for Urdu\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human lip-reading is a challenging task. It requires not only knowledge of\nunderlying language but also visual clues to predict spoken words. Experts need\ncertain level of experience and understanding of visual expressions learning to\ndecode spoken words. Now-a-days, with the help of deep learning it is possible\nto translate lip sequences into meaningful words. The speech recognition in the\nnoisy environments can be increased with the visual information [1]. To\ndemonstrate this, in this project, we have tried to train two different\ndeep-learning models for lip-reading: first one for video sequences using\nspatiotemporal convolution neural network, Bi-gated recurrent neural network\nand Connectionist Temporal Classification Loss, and second for audio that\ninputs the MFCC features to a layer of LSTM cells and output the sequence. We\nhave also collected a small audio-visual dataset to train and test our model.\nOur target is to integrate our both models to improve the speech recognition in\nthe noisy environment\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 13:28:19 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Faisal", "M", ""], ["Manzoor", "Sanaullah", ""]]}, {"id": "1802.05522", "submitter": "Reza Mahjourian", "authors": "Reza Mahjourian, Martin Wicke, Anelia Angelova", "title": "Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using\n  3D Geometric Constraints", "comments": "Upload CVPR camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for unsupervised learning of depth and ego-motion\nfrom monocular video. Unsupervised learning removes the need for separate\nsupervisory signals (depth or ego-motion ground truth, or multi-view video).\nPrior work in unsupervised depth learning uses pixel-wise or gradient-based\nlosses, which only consider pixels in small local neighborhoods. Our main\ncontribution is to explicitly consider the inferred 3D geometry of the scene,\nenforcing consistency of the estimated 3D point clouds and ego-motion across\nconsecutive frames. This is a challenging task and is solved by a novel\n(approximate) backpropagation algorithm for aligning 3D structures.\n  We combine this novel 3D-based loss with 2D losses based on photometric\nquality of frame reconstructions using estimated depth and ego-motion from\nadjacent frames. We also incorporate validity masks to avoid penalizing areas\nin which no useful information exists.\n  We test our algorithm on the KITTI dataset and on a video dataset captured on\nan uncalibrated mobile phone camera. Our proposed approach consistently\nimproves depth estimates on both datasets, and outperforms the state-of-the-art\nfor both depth and ego-motion. Because we only require a simple video, learning\ndepth and ego-motion on large and varied datasets becomes possible. We\ndemonstrate this by training on the low quality uncalibrated video dataset and\nevaluating on KITTI, ranking among top performing prior methods which are\ntrained on KITTI itself.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 13:33:21 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 00:31:55 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mahjourian", "Reza", ""], ["Wicke", "Martin", ""], ["Angelova", "Anelia", ""]]}, {"id": "1802.05584", "submitter": "Il Yong Chun", "authors": "Il Yong Chun and Jeffrey A. Fessler", "title": "Convolutional Analysis Operator Learning: Acceleration and Convergence", "comments": "22 pages, 11 figures, fixed incorrect math theorem numbers in fig. 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional operator learning is gaining attention in many signal\nprocessing and computer vision applications. Learning kernels has mostly relied\non so-called patch-domain approaches that extract and store many overlapping\npatches across training signals. Due to memory demands, patch-domain methods\nhave limitations when learning kernels from large datasets -- particularly with\nmulti-layered structures, e.g., convolutional neural networks -- or when\napplying the learned kernels to high-dimensional signal recovery problems. The\nso-called convolution approach does not store many overlapping patches, and\nthus overcomes the memory problems particularly with careful algorithmic\ndesigns; it has been studied within the \"synthesis\" signal model, e.g.,\nconvolutional dictionary learning. This paper proposes a new convolutional\nanalysis operator learning (CAOL) framework that learns an analysis sparsifying\nregularizer with the convolution perspective, and develops a new convergent\nBlock Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve\nthe corresponding block multi-nonconvex problems. To learn diverse filters\nwithin the CAOL framework, this paper introduces an orthogonality constraint\nthat enforces a tight-frame filter condition, and a regularizer that promotes\ndiversity between filters. Numerical experiments show that, with sharp\nmajorizers, BPEG-M significantly accelerates the CAOL convergence rate compared\nto the state-of-the-art block proximal gradient (BPG) method. Numerical\nexperiments for sparse-view computational tomography show that a convolutional\nsparsifying regularizer learned via CAOL significantly improves reconstruction\nquality compared to a conventional edge-preserving regularizer. Using more and\nwider kernels in a learned regularizer better preserves edges in reconstructed\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:51:38 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 19:11:31 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 01:29:57 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 07:19:14 GMT"}, {"version": "v5", "created": "Fri, 9 Aug 2019 21:28:38 GMT"}, {"version": "v6", "created": "Thu, 22 Aug 2019 12:45:28 GMT"}, {"version": "v7", "created": "Wed, 11 Sep 2019 10:03:54 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Chun", "Il Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1802.05591", "submitter": "Davy Neven", "authors": "Davy Neven, Bert De Brabandere, Stamatios Georgoulis, Marc Proesmans\n  and Luc Van Gool", "title": "Towards End-to-End Lane Detection: an Instance Segmentation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cars are incorporating an increasing number of driver assist features,\namong which automatic lane keeping. The latter allows the car to properly\nposition itself within the road lanes, which is also crucial for any subsequent\nlane departure or trajectory planning decision in fully autonomous cars.\nTraditional lane detection methods rely on a combination of highly-specialized,\nhand-crafted features and heuristics, usually followed by post-processing\ntechniques, that are computationally expensive and prone to scalability due to\nroad scene variations. More recent approaches leverage deep learning models,\ntrained for pixel-wise lane segmentation, even when no markings are present in\nthe image due to their big receptive field. Despite their advantages, these\nmethods are limited to detecting a pre-defined, fixed number of lanes, e.g.\nego-lanes, and can not cope with lane changes. In this paper, we go beyond the\naforementioned limitations and propose to cast the lane detection problem as an\ninstance segmentation problem - in which each lane forms its own instance -\nthat can be trained end-to-end. To parametrize the segmented lane instances\nbefore fitting the lane, we further propose to apply a learned perspective\ntransformation, conditioned on the image, in contrast to a fixed \"bird's-eye\nview\" transformation. By doing so, we ensure a lane fitting which is robust\nagainst road plane changes, unlike existing approaches that rely on a fixed,\npre-defined transformation. In summary, we propose a fast lane detection\nalgorithm, running at 50 fps, which can handle a variable number of lanes and\ncope with lane changes. We verify our method on the tuSimple dataset and\nachieve competitive results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 15:09:19 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Neven", "Davy", ""], ["De Brabandere", "Bert", ""], ["Georgoulis", "Stamatios", ""], ["Proesmans", "Marc", ""], ["Van Gool", "Luc", ""]]}, {"id": "1802.05622", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "Conditioning of three-dimensional generative adversarial networks for\n  pore and reservoir-scale models", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistical modeling of petrophysical properties is a key step in modern\nintegrated oil and gas reservoir studies. Recently, generative adversarial\nnetworks (GAN) have been shown to be a successful method for generating\nunconditional simulations of pore- and reservoir-scale models. This\ncontribution leverages the differentiable nature of neural networks to extend\nGANs to the conditional simulation of three-dimensional pore- and\nreservoir-scale models. Based on the previous work of Yeh et al. (2016), we use\na content loss to constrain to the conditioning data and a perceptual loss\nobtained from the evaluation of the GAN discriminator network. The technique is\ntested on the generation of three-dimensional micro-CT images of a Ketton\nlimestone constrained by two-dimensional cross-sections, and on the simulation\nof the Maules Creek alluvial aquifer constrained by one-dimensional sections.\nOur results show that GANs represent a powerful method for sampling conditioned\npore and reservoir samples for stochastic reservoir evaluation workflows.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 15:34:23 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1802.05637", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Masanori Koyama", "title": "cGANs with Projection Discriminator", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, projection based way to incorporate the conditional\ninformation into the discriminator of GANs that respects the role of the\nconditional information in the underlining probabilistic model. This approach\nis in contrast with most frameworks of conditional GANs used in application\ntoday, which use the conditional information by concatenating the (embedded)\nconditional vector to the feature vectors. With this modification, we were able\nto significantly improve the quality of the class conditional image generation\non ILSVRC2012 (ImageNet) 1000-class image dataset from the current\nstate-of-the-art result, and we achieved this with a single pair of a\ndiscriminator and a generator. We were also able to extend the application to\nsuper-resolution and succeeded in producing highly discriminative\nsuper-resolution images. This new structure also enabled high quality category\ntransformation based on parametric functional transformation of conditional\nbatch normalization layers in the generator.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:19:21 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 00:02:46 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Miyato", "Takeru", ""], ["Koyama", "Masanori", ""]]}, {"id": "1802.05656", "submitter": "Hongming Shan", "authors": "Hongming Shan, Yi Zhang, Qingsong Yang, Uwe Kruger, Mannudeep K.\n  Kalra, Ling Sun, Wenxiang Cong, Ge Wang", "title": "3D Convolutional Encoder-Decoder Network for Low-Dose CT via Transfer\n  Learning from a 2D Trained Network", "comments": "To be published in the IEEE TMI", "journal-ref": "IEEE Transactions on Medical Imaging 37(6) (2018) 1522-1534", "doi": "10.1109/TMI.2018.2832217", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dose computed tomography (CT) has attracted a major attention in the\nmedical imaging field, since CT-associated x-ray radiation carries health risks\nfor patients. The reduction of CT radiation dose, however, compromises the\nsignal-to-noise ratio, and may compromise the image quality and the diagnostic\nperformance. Recently, deep-learning-based algorithms have achieved promising\nresults in low-dose CT denoising, especially convolutional neural network (CNN)\nand generative adversarial network (GAN). This article introduces a Contracting\nPath-based Convolutional Encoder-decoder (CPCE) network in 2D and 3D\nconfigurations within the GAN framework for low-dose CT denoising. A novel\nfeature of our approach is that an initial 3D CPCE denoising model can be\ndirectly obtained by extending a trained 2D CNN and then fine-tuned to\nincorporate 3D spatial information from adjacent slices. Based on the transfer\nlearning from 2D to 3D, the 3D network converges faster and achieves a better\ndenoising performance than that trained from scratch. By comparing the CPCE\nwith recently published methods based on the simulated Mayo dataset and the\nreal MGH dataset, we demonstrate that the 3D CPCE denoising model has a better\nperformance, suppressing image noise and preserving subtle structures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:51:16 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 21:28:25 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Shan", "Hongming", ""], ["Zhang", "Yi", ""], ["Yang", "Qingsong", ""], ["Kruger", "Uwe", ""], ["Kalra", "Mannudeep K.", ""], ["Sun", "Ling", ""], ["Cong", "Wenxiang", ""], ["Wang", "Ge", ""]]}, {"id": "1802.05701", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Anil A Bharath", "title": "Inverting The Generator Of A Generative Adversarial Network (II)", "comments": "Under review at IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) learn a deep generative model that is\nable to synthesise novel, high-dimensional data samples. New data samples are\nsynthesised by passing latent samples, drawn from a chosen prior distribution,\nthrough the generative model. Once trained, the latent space exhibits\ninteresting properties, that may be useful for down stream tasks such as\nclassification or retrieval. Unfortunately, GANs do not offer an \"inverse\nmodel\", a mapping from data space back to latent space, making it difficult to\ninfer a latent representation for a given data sample. In this paper, we\nintroduce a technique, inversion, to project data samples, specifically images,\nto the latent space using a pre-trained GAN. Using our proposed inversion\ntechnique, we are able to identify which attributes of a dataset a trained GAN\nis able to model and quantify GAN performance, based on a reconstruction loss.\nWe demonstrate how our proposed inversion technique may be used to\nquantitatively compare performance of various GAN models trained on three image\ndatasets. We provide code for all of our experiments,\nhttps://github.com/ToniCreswell/InvertingGAN.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 18:50:20 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Creswell", "Antonia", ""], ["Bharath", "Anil A", ""]]}, {"id": "1802.05747", "submitter": "Tianyun Zhang", "authors": "Tianyun Zhang, Shaokai Ye, Yipeng Zhang, Yanzhi Wang, Makan Fardad", "title": "Systematic Weight Pruning of DNNs using Alternating Direction Method of\n  Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic weight pruning framework of deep neural networks\n(DNNs) using the alternating direction method of multipliers (ADMM). We first\nformulate the weight pruning problem of DNNs as a constrained nonconvex\noptimization problem, and then adopt the ADMM framework for systematic weight\npruning. We show that ADMM is highly suitable for weight pruning due to the\ncomputational efficiency it offers. We achieve a much higher compression ratio\ncompared with prior work while maintaining the same test accuracy, together\nwith a faster convergence rate. Our models are released at\nhttps://github.com/KaiqiZhang/admm-pruning\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 20:22:42 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 02:53:53 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhang", "Tianyun", ""], ["Ye", "Shaokai", ""], ["Zhang", "Yipeng", ""], ["Wang", "Yanzhi", ""], ["Fardad", "Makan", ""]]}, {"id": "1802.05751", "submitter": "Dustin Tran", "authors": "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, {\\L}ukasz Kaiser, Noam\n  Shazeer, Alexander Ku, Dustin Tran", "title": "Image Transformer", "comments": "Appears in International Conference on Machine Learning, 2018. Code\n  available at https://github.com/tensorflow/tensor2tensor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation has been successfully cast as an autoregressive sequence\ngeneration or transformation problem. Recent work has shown that self-attention\nis an effective way of modeling textual sequences. In this work, we generalize\na recently proposed model architecture based on self-attention, the\nTransformer, to a sequence modeling formulation of image generation with a\ntractable likelihood. By restricting the self-attention mechanism to attend to\nlocal neighborhoods we significantly increase the size of images the model can\nprocess in practice, despite maintaining significantly larger receptive fields\nper layer than typical convolutional neural networks. While conceptually\nsimple, our generative models significantly outperform the current state of the\nart in image generation on ImageNet, improving the best published negative\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\nconfiguration of our architecture. In a human evaluation study, we find that\nimages generated by our super-resolution model fool human observers three times\nmore often than the previous state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 20:37:15 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 18:44:17 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 23:27:07 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Parmar", "Niki", ""], ["Vaswani", "Ashish", ""], ["Uszkoreit", "Jakob", ""], ["Kaiser", "\u0141ukasz", ""], ["Shazeer", "Noam", ""], ["Ku", "Alexander", ""], ["Tran", "Dustin", ""]]}, {"id": "1802.05763", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Qide Dong, Xiang Chen", "title": "ASP:A Fast Adversarial Attack Example Generation Framework based on\n  Adversarial Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the excellent accuracy and feasibility, the Neural Networks have been\nwidely applied into the novel intelligent applications and systems. However,\nwith the appearance of the Adversarial Attack, the NN based system performance\nbecomes extremely vulnerable:the image classification results can be\narbitrarily misled by the adversarial examples, which are crafted images with\nhuman unperceivable pixel-level perturbation. As this raised a significant\nsystem security issue, we implemented a series of investigations on the\nadversarial attack in this work: We first identify an image's pixel\nvulnerability to the adversarial attack based on the adversarial saliency\nanalysis. By comparing the analyzed saliency map and the adversarial\nperturbation distribution, we proposed a new evaluation scheme to\ncomprehensively assess the adversarial attack precision and efficiency. Then,\nwith a novel adversarial saliency prediction method, a fast adversarial example\ngeneration framework, namely \"ASP\", is proposed with significant attack\nefficiency improvement and dramatic computation cost reduction. Compared to the\nprevious methods, experiments show that ASP has at most 12 times speed-up for\nadversarial example generation, 2 times lower perturbation rate, and high\nattack success rate of 87% on both MNIST and Cifar10. ASP can be also well\nutilized to support the data-hungry NN adversarial training. By reducing the\nattack success rate as much as 90%, ASP can quickly and effectively enhance the\ndefense capability of NN based system to the adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 21:07:05 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 01:08:50 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 22:32:05 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Yu", "Fuxun", ""], ["Dong", "Qide", ""], ["Chen", "Xiang", ""]]}, {"id": "1802.05766", "submitter": "Yan Zhang", "authors": "Yan Zhang, Jonathon Hare, Adam Pr\\\"ugel-Bennett", "title": "Learning to Count Objects in Natural Images for Visual Question\n  Answering", "comments": "Published in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) models have struggled with counting objects\nin natural images so far. We identify a fundamental problem due to soft\nattention in these models as a cause. To circumvent this problem, we propose a\nneural network component that allows robust counting from object proposals.\nExperiments on a toy task show the effectiveness of this component and we\nobtain state-of-the-art accuracy on the number category of the VQA v2 dataset\nwithout negatively affecting other categories, even outperforming ensemble\nmodels with our single model. On a difficult balanced pair metric, the\ncomponent gives a substantial improvement in counting over a strong baseline by\n6.6%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 21:16:59 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Zhang", "Yan", ""], ["Hare", "Jonathon", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "1802.05798", "submitter": "Anand Bhattad", "authors": "Anand Bhattad, Jason Rock, David Forsyth", "title": "Detecting Anomalous Faces with 'No Peeking' Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting anomalous faces has important applications. For example, a system\nmight tell when a train driver is incapacitated by a medical event, and assist\nin adopting a safe recovery strategy. These applications are demanding, because\nthey require accurate detection of rare anomalies that may be seen only at\nruntime. Such a setting causes supervised methods to perform poorly. We\ndescribe a method for detecting an anomalous face image that meets these\nrequirements. We construct a feature vector that reliably has large entries for\nanomalous images, then use various simple unsupervised methods to score the\nimage based on the feature. Obvious constructions (autoencoder codes;\nautoencoder residuals) are defeated by a 'peeking' behavior in autoencoders.\nOur feature construction removes rectangular patches from the image, predicts\nthe likely content of the patch conditioned on the rest of the image using a\nspecially trained autoencoder, then compares the result to the image. High\nscores suggest that the patch was difficult for an autoencoder to predict, and\nso is likely anomalous. We demonstrate that our method can identify real\nanomalous face images in pools of typical images, taken from celeb-A, that is\nmuch larger than usual in state-of-the-art experiments. A control experiment\nbased on our method with another set of normal celebrity images - a 'typical\nset', but nonceleb-A are not identified as anomalous; confirms this is not due\nto special properties of celeb-A.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:35:37 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bhattad", "Anand", ""], ["Rock", "Jason", ""], ["Forsyth", "David", ""]]}, {"id": "1802.05800", "submitter": "Deboleena Roy", "authors": "Deboleena Roy, Priyadarshini Panda, Kaushik Roy", "title": "Tree-CNN: A Hierarchical Deep Convolutional Neural Network for\n  Incremental Learning", "comments": "8 pages, 6 figures, 7 tables Accepted in Neural Networks, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Deep Convolutional Neural Networks (DCNNs) have shown\nremarkable performance in most computer vision tasks. These tasks traditionally\nuse a fixed dataset, and the model, once trained, is deployed as is. Adding new\ninformation to such a model presents a challenge due to complex training\nissues, such as \"catastrophic forgetting\", and sensitivity to hyper-parameter\ntuning. However, in this modern world, data is constantly evolving, and our\ndeep learning models are required to adapt to these changes. In this paper, we\npropose an adaptive hierarchical network structure composed of DCNNs that can\ngrow and learn as new data becomes available. The network grows in a tree-like\nfashion to accommodate new classes of data, while preserving the ability to\ndistinguish the previously trained classes. The network organizes the\nincrementally available data into feature-driven super-classes and improves\nupon existing hierarchical CNN models by adding the capability of self-growth.\nThe proposed hierarchical model, when compared against fine-tuning a deep\nnetwork, achieves significant reduction of training effort, while maintaining\ncompetitive accuracy on CIFAR-10 and CIFAR-100.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:36:56 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 18:45:12 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 15:58:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Roy", "Deboleena", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1802.05816", "submitter": "Luciano Oliveira", "authors": "Marcelo Santos and Luciano Oliveira", "title": "ISEC: Iterative over-Segmentation via Edge Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Several image pattern recognition tasks rely on superpixel generation as a\nfundamental step. Image analysis based on superpixels facilitates\ndomain-specific applications, also speeding up the overall processing time of\nthe task. Recent superpixel methods have been designed to fit boundary\nadherence, usually regulating the size and shape of each superpixel in order to\nmitigate the occurrence of undersegmentation failures. Superpixel regularity\nand compactness sometimes imposes an excessive number of segments in the image,\nwhich ultimately decreases the efficiency of the final segmentation, specially\nin video segmentation. We propose here a novel method to generate superpixels,\ncalled iterative over-segmentation via edge clustering (ISEC), which addresses\nthe over-segmentation problem from a different perspective in contrast to\nrecent state-of-the-art approaches. ISEC iteratively clusters edges extracted\nfrom the image objects, providing adaptive superpixels in size, shape and\nquantity, while preserving suitable adherence to the real object boundaries.\nAll this is achieved at a very low computational cost. Experiments show that\nISEC stands out from existing methods, meeting a favorable balance between\nsegmentation stability and accurate representation of motion discontinuities,\nwhich are features specially suitable to video segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 01:52:11 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Santos", "Marcelo", ""], ["Oliveira", "Luciano", ""]]}, {"id": "1802.05878", "submitter": "Leonardo Parisi", "authors": "Andrea Cavagna, Stefania Melillo, Leonardo Parisi, Federico\n  Ricci-Tersenghi", "title": "SpaRTA - Tracking across occlusions via global partitioning of 3D clouds\n  of points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any 3D tracking algorithm has to deal with occlusions: multiple targets get\nso close to each other that the loss of their identities becomes likely. In the\nbest case scenario, trajectories are interrupted, thus curbing the completeness\nof the data-set; in the worse case scenario, identity switches arise,\npotentially affecting in severe ways the very quality of the data. Here, we\npresent a novel tracking method that addresses the problem of occlusions within\nlarge groups of featureless objects by means of three steps: i) it represents\neach target as a cloud of points in 3D; ii) once a 3D cluster corresponding to\nan occlusion occurs, it defines a partitioning problem by introducing a cost\nfunction that uses both attractive and repulsive spatio-temporal proximity\nlinks; iii) it minimizes the cost function through a semi-definite optimization\ntechnique specifically designed to cope with link frustration. The algorithm is\nindependent of the specific experimental method used to collect the data. By\nperforming tests on public data-sets, we show that the new algorithm produces a\nsignificant improvement over the state-of-the-art tracking methods, both by\nreducing the number of identity switches and by increasing the accuracy of the\nactual positions of the targets in real space.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 09:40:16 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Cavagna", "Andrea", ""], ["Melillo", "Stefania", ""], ["Parisi", "Leonardo", ""], ["Ricci-Tersenghi", "Federico", ""]]}, {"id": "1802.05879", "submitter": "Helena Pei\\'c Tukuljac", "authors": "Helena Pei\\'c Tukuljac, Thach Pham Vu, Herv\\'e Lissek, Pierre\n  Vandergheynst", "title": "Joint Estimation of Room Geometry and Modes with Compressed Sensing", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acoustical behavior of a room for a given position of microphone and sound\nsource is usually described using the room impulse response. If we rely on the\nstandard uniform sampling, the estimation of room impulse response for\narbitrary positions in the room requires a large number of measurements. In\norder to lower the required sampling rate, some solutions have emerged that\nexploit the sparse representation of the room wavefield in the terms of plane\nwaves in the low-frequency domain. The plane wave representation has a simple\nform in rectangular rooms. In our solution, we observe the basic axial modes of\nthe wave vector grid for extraction of the room geometry and then we propagate\nthe knowledge to higher order modes out of the low-pass version of the\nmeasurements. Estimation of the approximate structure of the $k$-space should\nlead to the reduction in the terms of number of required measurements and in\nthe increase of the speed of the reconstruction without great losses of\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 09:41:56 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Tukuljac", "Helena Pei\u0107", ""], ["Vu", "Thach Pham", ""], ["Lissek", "Herv\u00e9", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1802.05891", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Andreas Schneider, Thomas Gerig, Bernhard Egger,\n  Andreas Morel-Forster, Thomas Vetter", "title": "Training Deep Face Recognition Systems with Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have significantly increased the performance\nof face recognition systems. The performance and reliability of these models\ndepend heavily on the amount and quality of the training data. However, the\ncollection of annotated large datasets does not scale well and the control over\nthe quality of the data decreases with the size of the dataset. In this work,\nwe explore how synthetically generated data can be used to decrease the number\nof real-world images needed for training deep face recognition systems. In\nparticular, we make use of a 3D morphable face model for the generation of\nimages with arbitrary amounts of facial identities and with full control over\nimage variations, such as pose, illumination, and background. In our\nexperiments with an off-the-shelf face recognition software we observe the\nfollowing phenomena: 1) The amount of real training data needed to train\ncompetitive deep face recognition systems can be reduced significantly. 2)\nCombining large-scale real-world data with synthetic data leads to an increased\nperformance. 3) Models trained only on synthetic data with strong variations in\npose, illumination, and background perform very well across different datasets\neven without dataset adaptation. 4) The real-to-virtual performance gap can be\nclosed when using synthetic data for pre-training, followed by fine-tuning with\nreal-world images. 5) There are no observable negative effects of pre-training\nwith synthetic data. Thus, any face recognition system in our experiments\nbenefits from using synthetic face images. The synthetic data generator, as\nwell as all experiments, are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:05:18 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Kortylewski", "Adam", ""], ["Schneider", "Andreas", ""], ["Gerig", "Thomas", ""], ["Egger", "Bernhard", ""], ["Morel-Forster", "Andreas", ""], ["Vetter", "Thomas", ""]]}, {"id": "1802.05902", "submitter": "Luca Donati", "authors": "Luca Donati, Simone Cesano, Andrea Prati", "title": "A complete hand-drawn sketch vectorization framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectorizing hand-drawn sketches is a challenging task, which is of paramount\nimportance for creating CAD vectorized versions for the fashion and creative\nworkflows. This paper proposes a complete framework that automatically\ntransforms noisy and complex hand-drawn sketches with different stroke types in\na precise, reliable and highly-simplified vectorized model. The proposed\nframework includes a novel line extraction algorithm based on a\nmulti-resolution application of Pearson's cross correlation and a new unbiased\nthinning algorithm that can get rid of scribbles and variable-width strokes to\nobtain clean 1-pixel lines. Other contributions include variants of pruning,\nmerging and edge linking procedures to post-process the obtained paths.\nFinally, a modification of the original Schneider's vectorization algorithm is\ndesigned to obtain fewer control points in the resulting Bezier splines. All\nthe proposed steps of the framework have been extensively tested and compared\nwith state-of-the-art algorithms, showing (both qualitatively and\nquantitatively) its outperformance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:47:05 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Donati", "Luca", ""], ["Cesano", "Simone", ""], ["Prati", "Andrea", ""]]}, {"id": "1802.05908", "submitter": "Matthias Fey", "authors": "Nils M. Kriege, Matthias Fey, Denis Fisseler, Petra Mutzel, Frank\n  Weichert", "title": "Recognizing Cuneiform Signs Using Graph Based Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cuneiform script constitutes one of the earliest systems of writing and\nis realized by wedge-shaped marks on clay tablets. A tremendous number of\ncuneiform tablets have already been discovered and are incrementally\ndigitalized and made available to automated processing. As reading cuneiform\nscript is still a manual task, we address the real-world application of\nrecognizing cuneiform signs by two graph based methods with complementary\nruntime characteristics. We present a graph model for cuneiform signs together\nwith a tailored distance measure based on the concept of the graph edit\ndistance. We propose efficient heuristics for its computation and demonstrate\nits effectiveness in classification tasks experimentally. To this end, the\ndistance measure is used to implement a nearest neighbor classifier leading to\na high computational cost for the prediction phase with increasing training set\nsize. In order to overcome this issue, we propose to use CNNs adapted to graphs\nas an alternative approach shifting the computational cost to the training\nphase. We demonstrate the practicability of both approaches in an extensive\nexperimental comparison regarding runtime and prediction accuracy. Although\ncurrently available annotated real-world data is still limited, we obtain a\nhigh accuracy using CNNs, in particular, when the training set is enriched by\naugmented examples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:28:28 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 07:52:49 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Kriege", "Nils M.", ""], ["Fey", "Matthias", ""], ["Fisseler", "Denis", ""], ["Mutzel", "Petra", ""], ["Weichert", "Frank", ""]]}, {"id": "1802.05911", "submitter": "Mehmet Baygin", "authors": "Mehmet Baygin, Mehmet Karakose, Alisan Sarimaden, Erhan Akin", "title": "An Image Processing based Object Counting Approach for Machine Vision\n  Application", "comments": "International Conference on Advances and Innovations in Engineering\n  (ICAIE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine vision applications are low cost and high precision measurement\nsystems which are frequently used in production lines. With these systems that\nprovide contactless control and measurement, production facilities are able to\nreach high production numbers without errors. Machine vision operations such as\nproduct counting, error control, dimension measurement can be performed through\na camera. In this paper, a machine vision application is proposed, which can\nperform object-independent product counting. The proposed approach is based on\nOtsu thresholding and Hough transformation and performs automatic counting\nindependently of product type and color. Basically one camera is used in the\nsystem. Through this camera, an image of the products passing through a\nconveyor is taken and various image processing algorithms are applied to these\nimages. In this approach using images obtained from a real experimental setup,\na real-time machine vision application was installed. As a result of the\nexperimental studies performed, it has been determined that the proposed\napproach gives fast, accurate and reliable results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:31:35 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Baygin", "Mehmet", ""], ["Karakose", "Mehmet", ""], ["Sarimaden", "Alisan", ""], ["Akin", "Erhan", ""]]}, {"id": "1802.05914", "submitter": "Florian Dubost", "authors": "Florian Dubost, Hieab Adams, Gerda Bortsova, M. Arfan Ikram, Wiro\n  Niessen, Meike Vernooij, Marleen de Bruijne", "title": "3D Regression Neural Network for the Quantification of Enlarged\n  Perivascular Spaces in Brain MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enlarged perivascular spaces (EPVS) in the brain are an emerging imaging\nmarker for cerebral small vessel disease, and have been shown to be related to\nincreased risk of various neurological diseases, including stroke and dementia.\nAutomatic quantification of EPVS would greatly help to advance research into\nits etiology and its potential as a risk indicator of disease. We propose a\nconvolutional network regression method to quantify the extent of EPVS in the\nbasal ganglia from 3D brain MRI. We first segment the basal ganglia and\nsubsequently apply a 3D convolutional regression network designed for small\nobject detection within this region of interest. The network takes an image as\ninput, and outputs a quantification score of EPVS. The network has\nsignificantly more convolution operations than pooling ones and no final\nactivation, allowing it to span the space of real numbers. We validated our\napproach using a dataset of 2000 brain MRI scans scored visually. Experiments\nwith varying sizes of training and test sets showed that a good performance can\nbe achieved with a training set of only 200 scans. With a training set of 1000\nscans, the intraclass correlation coefficient (ICC) between our scoring method\nand the expert's visual score was 0.74. Our method outperforms by a large\nmargin - more than 0.10 - four more conventional automated approaches based on\nintensities, scale-invariant feature transform, and random forest. We show that\nthe network learns the structures of interest and investigate the influence of\nhyper-parameters on the performance. We also evaluate the reproducibility of\nour network using a set of 60 subjects scanned twice (scan-rescan\nreproducibility). On this set our network achieves an ICC of 0.93, while the\nintrarater agreement reaches 0.80. Furthermore, the automatic EPVS scoring\ncorrelates similarly to age as visual scoring.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:49:59 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 22:52:32 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Dubost", "Florian", ""], ["Adams", "Hieab", ""], ["Bortsova", "Gerda", ""], ["Ikram", "M. Arfan", ""], ["Niessen", "Wiro", ""], ["Vernooij", "Meike", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1802.05957", "submitter": "Takeru Miyato", "authors": "Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida", "title": "Spectral Normalization for Generative Adversarial Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in the study of generative adversarial networks is the\ninstability of its training. In this paper, we propose a novel weight\nnormalization technique called spectral normalization to stabilize the training\nof the discriminator. Our new normalization technique is computationally light\nand easy to incorporate into existing implementations. We tested the efficacy\nof spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we\nexperimentally confirmed that spectrally normalized GANs (SN-GANs) is capable\nof generating images of better or equal quality relative to the previous\ntraining stabilization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 14:41:39 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Miyato", "Takeru", ""], ["Kataoka", "Toshiki", ""], ["Koyama", "Masanori", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1802.05998", "submitter": "Tomas Teijeiro", "authors": "Tom\\'as Teijeiro, Constantino A. Garc\\'ia, Daniel Castro and Paulo\n  F\\'elix", "title": "Abductive reasoning as the basis to reproduce expert criteria in ECG\n  Atrial Fibrillation identification", "comments": "15 pages, 6 figures, 6 tables", "journal-ref": null, "doi": "10.1088/1361-6579/aad7e4", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This work aims at providing a new method for the automatic\ndetection of atrial fibrillation, other arrhythmia and noise on short single\nlead ECG signals, emphasizing the importance of the interpretability of the\nclassification results.\n  Approach: A morphological and rhythm description of the cardiac behavior is\nobtained by a knowledge-based interpretation of the signal using the\n\\textit{Construe} abductive framework. Then, a set of meaningful features are\nextracted for each individual heartbeat and as a summary of the full record.\nThe feature distributions were used to elucidate the expert criteria underlying\nthe labeling of the 2017 Physionet/CinC Challenge dataset, enabling a manual\npartial relabeling to improve the consistency of the classification rules.\nFinally, state-of-the-art machine learning methods are combined to provide an\nanswer on the basis of the feature values.\n  Main results: The proposal tied for the first place in the official stage of\nthe Challenge, with a combined $F_1$ score of 0.83, and was even improved in\nthe follow-up stage to 0.85 with a significant simplification of the model.\n  Significance: This approach demonstrates the potential of \\textit{Construe}\nto provide robust and valuable descriptions of temporal data even with\nsignificant amounts of noise and artifacts. Also, we discuss the importance of\na consistent classification criteria in manually labeled training datasets, and\nthe fundamental advantages of knowledge-based approaches to formalize and\nvalidate that criteria.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:06:42 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Teijeiro", "Tom\u00e1s", ""], ["Garc\u00eda", "Constantino A.", ""], ["Castro", "Daniel", ""], ["F\u00e9lix", "Paulo", ""]]}, {"id": "1802.06091", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, John K. Tsotsos", "title": "Bridging Cognitive Programs and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While great advances are made in pattern recognition and machine learning,\nthe successes of such fields remain restricted to narrow applications and seem\nto break down when training data is scarce, a shift in domain occurs, or when\nintelligent reasoning is required for rapid adaptation to new environments. In\nthis work, we list several of the shortcomings of modern machine-learning\nsolutions, specifically in the contexts of computer vision and in reinforcement\nlearning and suggest directions to explore in order to try to ameliorate these\nweaknesses.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 19:19:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1802.06117", "submitter": "Zachary Daniels", "authors": "Zachary A. Daniels, Dimitris N. Metaxas", "title": "Scenarios: A New Representation for Complex Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability for computational agents to reason about the high-level content\nof real world scene images is important for many applications. Existing\nattempts at addressing the problem of complex scene understanding lack\nrepresentational power, efficiency, and the ability to create robust\nmeta-knowledge about scenes. In this paper, we introduce scenarios as a new way\nof representing scenes. The scenario is a simple, low-dimensional, data-driven\nrepresentation consisting of sets of frequently co-occurring objects and is\nuseful for a wide range of scene understanding tasks. We learn scenarios from\ndata using a novel matrix factorization method which we integrate into a new\nneural network architecture, the ScenarioNet. Using ScenarioNet, we can recover\nsemantic information about real world scene images at three levels of\ngranularity: 1) scene categories, 2) scenarios, and 3) objects. Training a\nsingle ScenarioNet model enables us to perform scene classification, scenario\nrecognition, multi-object recognition, content-based scene image retrieval, and\ncontent-based image comparison. In addition to solving many tasks in a single,\nunified framework, ScenarioNet is more computationally efficient than other\nCNNs because it requires significantly fewer parameters while achieving similar\nperformance on benchmark tasks and is more interpretable because it produces\nexplanations when making decisions. We validate the utility of scenarios and\nScenarioNet on a diverse set of scene understanding tasks on several benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 20:49:30 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Daniels", "Zachary A.", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1802.06130", "submitter": "Sungjoon Choi", "authors": "Sungjoon Choi, John Isidoro, Pascal Getreuer, Peyman Milanfar", "title": "Fast, Trainable, Multiscale Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising is a fundamental imaging problem. Versatile but fast filtering has\nbeen demanded for mobile camera systems. We present an approach to multiscale\nfiltering which allows real-time applications on low-powered devices. The key\nidea is to learn a set of kernels that upscales, filters, and blends patches of\ndifferent scales guided by local structure analysis. This approach is trainable\nso that learned filters are capable of treating diverse noise patterns and\nartifacts. Experimental results show that the presented approach produces\ncomparable results to state-of-the-art algorithms while processing time is\norders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 21:27:30 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Choi", "Sungjoon", ""], ["Isidoro", "John", ""], ["Getreuer", "Pascal", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1802.06140", "submitter": "Maryam Khanian", "authors": "Maryam Khanian, Ali Sharifi Boroujerdi, Michael Breuss", "title": "Real-Time 3D Shape of Micro-Details", "comments": "18 Pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the growing demand for interactive environments, we propose an\naccurate real-time 3D shape reconstruction technique. To provide a reliable 3D\nreconstruction which is still a challenging task when dealing with real-world\napplications, we integrate several components including (i) Photometric Stereo\n(PS), (ii) perspective Cook-Torrance reflectance model that enables PS to deal\nwith a broad range of possible real-world object reflections, (iii) realistic\nlightening situation, (iv) a Recurrent Optimization Network (RON) and finally\n(v) heuristic Dijkstra Gaussian Mean Curvature (DGMC) initialization approach.\nWe demonstrate the potential benefits of our hybrid model by providing 3D shape\nwith highly-detailed information from micro-prints for the first time. All\nreal-world images are taken by a mobile phone camera under a simple setup as a\nconsumer-level equipment. In addition, complementary synthetic experiments\nconfirm the beneficial properties of our novel method and its superiority over\nthe state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 21:55:16 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Khanian", "Maryam", ""], ["Boroujerdi", "Ali Sharifi", ""], ["Breuss", "Michael", ""]]}, {"id": "1802.06181", "submitter": "Naji Khosravan", "authors": "Naji Khosravan and Ulas Bagci", "title": "Semi-supervised multi-task learning for lung cancer diagnosis", "comments": "Accepted for publication at IEEE EMBC (40th International Engineering\n  in Medicine and Biology Conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of lung nodules is of great importance in lung cancer\nscreening. Existing research recognizes the critical role played by CAD systems\nin early detection and diagnosis of lung nodules. However, many CAD systems,\nwhich are used as cancer detection tools, produce a lot of false positives (FP)\nand require a further FP reduction step. Furthermore, guidelines for early\ndiagnosis and treatment of lung cancer are consist of different shape and\nvolume measurements of abnormalities. Segmentation is at the heart of our\nunderstanding of nodules morphology making it a major area of interest within\nthe field of computer aided diagnosis systems. This study set out to test the\nhypothesis that joint learning of false positive (FP) nodule reduction and\nnodule segmentation can improve the computer aided diagnosis (CAD) systems'\nperformance on both tasks. To support this hypothesis we propose a 3D deep\nmulti-task CNN to tackle these two problems jointly. We tested our system on\nLUNA16 dataset and achieved an average dice similarity coefficient (DSC) of 91%\nas segmentation accuracy and a score of nearly 92% for FP reduction. As a proof\nof our hypothesis, we showed improvements of segmentation and FP reduction\ntasks over two baselines. Our results support that joint training of these two\ntasks through a multi-task learning approach improves system performance on\nboth. We also showed that a semi-supervised approach can be used to overcome\nthe limitation of lack of labeled data for the 3D segmentation task.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 03:49:32 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 19:08:13 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Khosravan", "Naji", ""], ["Bagci", "Ulas", ""]]}, {"id": "1802.06194", "submitter": "Praveen Krishnan", "authors": "Praveen Krishnan and C.V. Jawahar", "title": "HWNet v2: An Efficient Word Image Representation for Handwritten\n  Documents", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning an efficient holistic representation for\nhandwritten word images. The proposed method uses a deep convolutional neural\nnetwork with traditional classification loss. The major strengths of our work\nlie in: (i) the efficient usage of synthetic data to pre-train a deep network,\n(ii) an adapted version of the ResNet-34 architecture with the region of\ninterest pooling (referred to as HWNet v2) which learns discriminative features\nfor variable sized word images, and (iii) a realistic augmentation of training\ndata with multiple scales and distortions which mimics the natural process of\nhandwriting. We further investigate the process of transfer learning to reduce\nthe domain gap between synthetic and real domain, and also analyze the\ninvariances learned at different layers of the network using visualization\ntechniques proposed in the literature.\n  Our representation leads to a state-of-the-art word spotting performance on\nstandard handwritten datasets and historical manuscripts in different languages\nwith minimal representation size. On the challenging IAM dataset, our method is\nfirst to report an mAP of around 0.90 for word spotting with a representation\nsize of just 32 dimensions. Furthermore, we also present results on printed\ndocument datasets in English and Indic scripts which validates the generic\nnature of the proposed framework for learning word image representation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 05:12:03 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 09:07:02 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Krishnan", "Praveen", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1802.06205", "submitter": "SeyyedHossein Hasanpour Matikolaee", "authors": "Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad\n  Sabokrou and Ehsan Adeli", "title": "Towards Principled Design of Deep Convolutional Networks: Introducing\n  SimpNet", "comments": "The Submitted version to the IEEE TIP on December 2017, replaced high\n  resolution images with low-res counterparts due to arXiv size limitation, 19\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major winning Convolutional Neural Networks (CNNs), such as VGGNet, ResNet,\nDenseNet, \\etc, include tens to hundreds of millions of parameters, which\nimpose considerable computation and memory overheads. This limits their\npractical usage in training and optimizing for real-world applications. On the\ncontrary, light-weight architectures, such as SqueezeNet, are being proposed to\naddress this issue. However, they mainly suffer from low accuracy, as they have\ncompromised between the processing power and efficiency. These inefficiencies\nmostly stem from following an ad-hoc designing procedure. In this work, we\ndiscuss and propose several crucial design principles for an efficient\narchitecture design and elaborate intuitions concerning different aspects of\nthe design procedure. Furthermore, we introduce a new layer called {\\it\nSAF-pooling} to improve the generalization power of the network while keeping\nit simple by choosing best features. Based on such principles, we propose a\nsimple architecture called {\\it SimpNet}. We empirically show that SimpNet\nprovides a good trade-off between the computation/memory efficiency and the\naccuracy solely based on these primitive but crucial principles. SimpNet\noutperforms the deeper and more complex architectures such as VGGNet, ResNet,\nWideResidualNet \\etc, on several well-known benchmarks, while having 2 to 25\ntimes fewer number of parameters and operations. We obtain state-of-the-art\nresults (in terms of a balance between the accuracy and the number of involved\nparameters) on standard datasets, such as CIFAR10, CIFAR100, MNIST and SVHN.\nThe implementations are available at\n\\href{url}{https://github.com/Coderx7/SimpNet}.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 07:53:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Hasanpour", "Seyyed Hossein", ""], ["Rouhani", "Mohammad", ""], ["Fayyaz", "Mohsen", ""], ["Sabokrou", "Mohammad", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1802.06214", "submitter": "Rajesh Kumar Muthu", "authors": "P. S. Prashanth Rao, Rajesh Kumar Muthu", "title": "A New De-blurring Technique for License Plate Images with Robust Length\n  Estimation", "comments": "Accepted and Published in 2017 IEEE International Conference on\n  Intelligent Computing and Control (I2C2),23 Jun - 24 Jun 2017, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing a license plate clearly while seeing a surveillance camera\nsnapshot is often important in cases where the troublemaker vehicle(s) have to\nbe identified. In many real world situations, these images are blurred due to\nfast motion of the vehicle and cannot be recognized by the human eye. For this\nkind of blurring, the kernel involved can be said to be a linear uniform\nconvolution described by its angle and length. We propose a new de-blurring\ntechnique in this paper to parametrically estimate the kernel as accurately as\npossible with emphasis on the length estimation process. We use a technique\nwhich employs Hough transform in estimating the kernel angle. To accurately\nestimate the kernel length, a novel approach using the cepstral transform is\nintroduced. We compare the de-blurred results obtained using our scheme with\nthose of other recently introduced blind de-blurring techniques. The\ncomparisons corroborate that our scheme can remove a large blur from the image\ncaptured by the camera to recover vital semantic information about the license\nplate.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 08:59:03 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Rao", "P. S. Prashanth", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "1802.06259", "submitter": "Lingyang Chu", "authors": "Lingyang Chu, Xia Hu, Juhua Hu, Lanjun Wang, Jian Pei", "title": "Exact and Consistent Interpretation for Piecewise Linear Neural\n  Networks: A Closed Form Solution", "comments": "KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong intelligent machines powered by deep neural networks are increasingly\ndeployed as black boxes to make decisions in risk-sensitive domains, such as\nfinance and medical. To reduce potential risk and build trust with users, it is\ncritical to interpret how such machines make their decisions. Existing works\ninterpret a pre-trained neural network by analyzing hidden neurons, mimicking\npre-trained models or approximating local predictions. However, these methods\ndo not provide a guarantee on the exactness and consistency of their\ninterpretation. In this paper, we propose an elegant closed form solution named\n$OpenBox$ to compute exact and consistent interpretations for the family of\nPiecewise Linear Neural Networks (PLNN). The major idea is to first transform a\nPLNN into a mathematically equivalent set of linear classifiers, then interpret\neach linear classifier by the features that dominate its prediction. We further\napply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse\nconstraints on improving the interpretability of PLNNs. The extensive\nexperiments on both synthetic and real world data sets clearly demonstrate the\nexactness and consistency of our interpretation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 16:47:32 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 17:21:14 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chu", "Lingyang", ""], ["Hu", "Xia", ""], ["Hu", "Juhua", ""], ["Wang", "Lanjun", ""], ["Pei", "Jian", ""]]}, {"id": "1802.06260", "submitter": "Naji Khosravan", "authors": "Naji Khosravan, Haydar Celik, Baris Turkbey, Elizabeth Jones, Bradford\n  Wood, Ulas Bagci", "title": "A Collaborative Computer Aided Diagnosis (C-CAD) System with\n  Eye-Tracking, Sparse Attentional Model, and Deep Learning", "comments": "Submitted to Medical Image Analysis Journal (MedIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are at least two categories of errors in radiology screening that can\nlead to suboptimal diagnostic decisions and interventions:(i)human fallibility\nand (ii)complexity of visual search. Computer aided diagnostic (CAD) tools are\ndeveloped to help radiologists to compensate for some of these errors. However,\ndespite their significant improvements over conventional screening strategies,\nmost CAD systems do not go beyond their use as second opinion tools due to\nproducing a high number of false positives, which human interpreters need to\ncorrect. In parallel with efforts in computerized analysis of radiology scans,\nseveral researchers have examined behaviors of radiologists while screening\nmedical images to better understand how and why they miss tumors, how they\ninteract with the information in an image, and how they search for unknown\npathology in the images. Eye-tracking tools have been instrumental in exploring\nanswers to these fundamental questions. In this paper, we aim to develop a\nparadigm shift CAD system, called collaborative CAD (C-CAD), that unifies both\nof the above mentioned research lines: CAD and eye-tracking. We design an\neye-tracking interface providing radiologists with a real radiology reading\nroom experience. Then, we propose a novel algorithm that unifies eye-tracking\ndata and a CAD system. Specifically, we present a new graph based clustering\nand sparsification algorithm to transform eye-tracking data (gaze) into a\nsignal model to interpret gaze patterns quantitatively and qualitatively. The\nproposed C-CAD collaborates with radiologists via eye-tracking technology and\nhelps them to improve diagnostic decisions. The C-CAD learns radiologists'\nsearch efficiency by processing their gaze patterns. To do this, the C-CAD uses\na deep learning algorithm in a newly designed multi-task learning platform to\nsegment and diagnose cancers simultaneously.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 17:20:50 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 15:06:18 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Khosravan", "Naji", ""], ["Celik", "Haydar", ""], ["Turkbey", "Baris", ""], ["Jones", "Elizabeth", ""], ["Wood", "Bradford", ""], ["Bagci", "Ulas", ""]]}, {"id": "1802.06367", "submitter": "Xingyu Liu", "authors": "Xingyu Liu, Jeff Pool, Song Han, William J. Dally", "title": "Efficient Sparse-Winograd Convolutional Neural Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are computationally intensive, which\nlimits their application on mobile devices. Their energy is dominated by the\nnumber of multiplies needed to perform the convolutions. Winograd's minimal\nfiltering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can\nreduce the operation count, but these two methods cannot be directly combined\n$-$ applying the Winograd transform fills in the sparsity in both the weights\nand the activations. We propose two modifications to Winograd-based CNNs to\nenable these methods to exploit sparsity. First, we move the ReLU operation\ninto the Winograd domain to increase the sparsity of the transformed\nactivations. Second, we prune the weights in the Winograd domain to exploit\nstatic weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet\ndatasets, our method reduces the number of multiplications by $10.4\\times$,\n$6.8\\times$ and $10.8\\times$ respectively with loss of accuracy less than\n$0.1\\%$, outperforming previous baselines by $2.0\\times$-$3.0\\times$. We also\nshow that moving ReLU to the Winograd domain allows more aggressive pruning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 12:29:05 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liu", "Xingyu", ""], ["Pool", "Jeff", ""], ["Han", "Song", ""], ["Dally", "William J.", ""]]}, {"id": "1802.06399", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Jie Shen, Doruk Cetin, Maja Pantic", "title": "Visual-Only Recognition of Normal, Whispered and Silent Speech", "comments": "Accepted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Silent speech interfaces have been recently proposed as a way to enable\ncommunication when the acoustic signal is not available. This introduces the\nneed to build visual speech recognition systems for silent and whispered\nspeech. However, almost all the recently proposed systems have been trained on\nvocalised data only. This is in contrast with evidence in the literature which\nsuggests that lip movements change depending on the speech mode. In this work,\nwe introduce a new audiovisual database which is publicly available and\ncontains normal, whispered and silent speech. To the best of our knowledge,\nthis is the first study which investigates the differences between the three\nspeech modes using the visual modality only. We show that an absolute decrease\nin classification rate of up to 3.7% is observed when training and testing on\nnormal and whispered, respectively, and vice versa. An even higher decrease of\nup to 8.5% is reported when the models are tested on silent speech. This\nreveals that there are indeed visual differences between the 3 speech modes and\nthe common assumption that vocalized training data can be used directly to\ntrain a silent speech recognition system may not be true.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 16:40:46 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Petridis", "Stavros", ""], ["Shen", "Jie", ""], ["Cetin", "Doruk", ""], ["Pantic", "Maja", ""]]}, {"id": "1802.06404", "submitter": "Satrya Fajri Pratama", "authors": "Satrya Fajri Pratama, Azah Kamilah Muda, Yun-Huoy Choo, Ramon\n  Carb\\'o-Dorca, and Ajith Abraham", "title": "Using 3D Hahn Moments as A Computational Representation of ATS Drugs\n  Molecular Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The campaign against drug abuse is fought by all countries, most notably on\nATS drugs. The technical limitations of the current test kits to detect new\nbrand of ATS drugs present a challenge to law enforcement authorities and\nforensic laboratories. Meanwhile, new molecular imaging devices which allowed\nmankind to characterize the physical 3D molecular structure have been recently\nintroduced, and it can be used to remedy the limitations of existing drug test\nkits. Thus, a new type of 3D molecular structure representation technique\nshould be developed to cater the 3D molecular structure acquired physically\nusing these molecular imaging devices. One of the applications of image\nprocessing methods to represent a 3D image is 3D moments, and this study\nformulates a new 3D moments technique, namely 3D Hahn moments, to represent the\n3D molecular structure of ATS drugs. The performance of the proposed technique\nwas analysed using drug chemical structures obtained from UNODC for the ATS\ndrugs, while non-ATS drugs are obtained randomly from ChemSpider database. The\nevaluation shows the technique is qualified to be further explored in the\nfuture works to be fully compatible with ATS drug identification domain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 17:03:22 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 14:44:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Pratama", "Satrya Fajri", ""], ["Muda", "Azah Kamilah", ""], ["Choo", "Yun-Huoy", ""], ["Carb\u00f3-Dorca", "Ramon", ""], ["Abraham", "Ajith", ""]]}, {"id": "1802.06424", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Feipeng Cai,\n  Georgios Tzimiropoulos, Maja Pantic", "title": "End-to-end Audiovisual Speech Recognition", "comments": "Accepted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several end-to-end deep learning approaches have been recently presented\nwhich extract either audio or visual features from the input images or audio\nsignals and perform speech recognition. However, research on end-to-end\naudiovisual models is very limited. In this work, we present an end-to-end\naudiovisual model based on residual networks and Bidirectional Gated Recurrent\nUnits (BGRUs). To the best of our knowledge, this is the first audiovisual\nfusion model which simultaneously learns to extract features directly from the\nimage pixels and audio waveforms and performs within-context word recognition\non a large publicly available dataset (LRW). The model consists of two streams,\none for each modality, which extract features directly from mouth regions and\nraw waveforms. The temporal dynamics in each stream/modality are modeled by a\n2-layer BGRU and the fusion of multiple streams/modalities takes place via\nanother 2-layer BGRU. A slight improvement in the classification rate over an\nend-to-end audio-only and MFCC-based model is reported in clean audio\nconditions and low levels of noise. In presence of high levels of noise, the\nend-to-end audiovisual model significantly outperforms both audio-only models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 19:07:31 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 11:58:14 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Petridis", "Stavros", ""], ["Stafylakis", "Themos", ""], ["Ma", "Pingchuan", ""], ["Cai", "Feipeng", ""], ["Tzimiropoulos", "Georgios", ""], ["Pantic", "Maja", ""]]}, {"id": "1802.06430", "submitter": "Arjun Nitin Bhagoji", "authors": "Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang,\n  and Prateek Mittal", "title": "DARTS: Deceiving Autonomous Cars with Toxic Signs", "comments": "Submitted to ACM CCS 2018; Extended version of [1801.02780] Rogue\n  Signs: Deceiving Traffic Sign Recognition with Malicious Ads and Logos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign recognition is an integral part of autonomous cars. Any\nmisclassification of traffic signs can potentially lead to a multitude of\ndisastrous consequences, ranging from a life-threatening accident to even a\nlarge-scale interruption of transportation services relying on autonomous cars.\nIn this paper, we propose and examine security attacks against sign recognition\nsystems for Deceiving Autonomous caRs with Toxic Signs (we call the proposed\nattacks DARTS). In particular, we introduce two novel methods to create these\ntoxic signs. First, we propose Out-of-Distribution attacks, which expand the\nscope of adversarial examples by enabling the adversary to generate these\nstarting from an arbitrary point in the image space compared to prior attacks\nwhich are restricted to existing training/test data (In-Distribution). Second,\nwe present the Lenticular Printing attack, which relies on an optical\nphenomenon to deceive the traffic sign recognition system. We extensively\nevaluate the effectiveness of the proposed attacks in both virtual and\nreal-world settings and consider both white-box and black-box threat models.\nOur results demonstrate that the proposed attacks are successful under both\nsettings and threat models. We further show that Out-of-Distribution attacks\ncan outperform In-Distribution attacks on classifiers defended using the\nadversarial training defense, exposing a new attack vector for these defenses.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 19:39:28 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 02:50:49 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 20:05:27 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Sitawarin", "Chawin", ""], ["Bhagoji", "Arjun Nitin", ""], ["Mosenia", "Arsalan", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""]]}, {"id": "1802.06446", "submitter": "Jakob Weiss", "authors": "Jakob Weiss, Nicola Rieke, Mohammad Ali Nasseri, Mathias Maier,\n  Abouzar Eslami, Nassir Navab", "title": "Fast 5DOF Needle Tracking in iOCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Intraoperative Optical Coherence Tomography (iOCT) is an\nincreasingly available imaging technique for ophthalmic microsurgery that\nprovides high-resolution cross-sectional information of the surgical scene. We\npropose to build on its desirable qualities and present a method for tracking\nthe orientation and location of a surgical needle. Thereby, we enable direct\nanalysis of instrument-tissue interaction directly in OCT space without complex\nmultimodal calibration that would be required with traditional instrument\ntracking methods. Method. The intersection of the needle with the iOCT scan is\ndetected by a peculiar multi-step ellipse fitting that takes advantage of the\ndirectionality of the modality. The geometric modelling allows us to use the\nellipse parameters and provide them into a latency aware estimator to infer the\n5DOF pose during needle movement. Results. Experiments on phantom data and\nex-vivo porcine eyes indicate that the algorithm retains angular precision\nespecially during lateral needle movement and provides a more robust and\nconsistent estimation than baseline methods. Conclusion. Using solely\ncrosssectional iOCT information, we are able to successfully and robustly\nestimate a 5DOF pose of the instrument in less than 5.5 ms on a CPU.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 21:15:54 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Weiss", "Jakob", ""], ["Rieke", "Nicola", ""], ["Nasseri", "Mohammad Ali", ""], ["Maier", "Mathias", ""], ["Eslami", "Abouzar", ""], ["Navab", "Nassir", ""]]}, {"id": "1802.06454", "submitter": "Shuang Ma", "authors": "Shuang Ma, Jianlong Fu, Chang Wen Chen, Tao Mei", "title": "DA-GAN: Instance-level Image Translation by Deep Attention Generative\n  Adversarial Networks (with Supplementary Materials)", "comments": "Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image translation, which aims in translating two independent\nsets of images, is challenging in discovering the correct correspondences\nwithout paired data. Existing works build upon Generative Adversarial Network\n(GAN) such that the distribution of the translated images are indistinguishable\nfrom the distribution of the target set. However, such set-level constraints\ncannot learn the instance-level correspondences (e.g. aligned semantic parts in\nobject configuration task). This limitation often results in false positives\n(e.g. geometric or semantic artifacts), and further leads to mode collapse\nproblem. To address the above issues, we propose a novel framework for\ninstance-level image translation by Deep Attention GAN (DA-GAN). Such a design\nenables DA-GAN to decompose the task of translating samples from two sets into\ntranslating instances in a highly-structured latent space. Specifically, we\njointly learn a deep attention encoder, and the instancelevel correspondences\ncould be consequently discovered through attending on the learned instance\npairs. Therefore, the constraints could be exploited on both set-level and\ninstance-level. Comparisons against several state-ofthe- arts demonstrate the\nsuperiority of our approach, and the broad application capability, e.g, pose\nmorphing, data augmentation, etc., pushes the margin of domain translation\nproblem.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 22:16:19 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Ma", "Shuang", ""], ["Fu", "Jianlong", ""], ["Chen", "Chang Wen", ""], ["Mei", "Tao", ""]]}, {"id": "1802.06459", "submitter": "Nelson Nauata Junior", "authors": "Nelson Nauata, Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao,\n  Greg Mori", "title": "Structured Label Inference for Visual Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data such as images and videos contain a rich source of structured\nsemantic labels as well as a wide range of interacting components. Visual\ncontent could be assigned with fine-grained labels describing major components,\ncoarse-grained labels depicting high level abstractions, or a set of labels\nrevealing attributes. Such categorization over different, interacting layers of\nlabels evinces the potential for a graph-based encoding of label information.\nIn this paper, we exploit this rich structure for performing graph-based\ninference in label space for a number of tasks: multi-label image and video\nclassification and action detection in untrimmed videos. We consider the use of\nthe Bidirectional Inference Neural Network (BINN) and Structured Inference\nNeural Network (SINN) for performing graph-based inference in label space and\npropose a Long Short-Term Memory (LSTM) based extension for exploiting activity\nprogression on untrimmed videos. The methods were evaluated on (i) the Animal\nwith Attributes (AwA), Scene Understanding (SUN) and NUS-WIDE datasets for\nmulti-label image classification, (ii) the first two releases of the YouTube-8M\nlarge scale dataset for multi-label video classification, and (iii) the\nTHUMOS'14 and MultiTHUMOS video datasets for action detection. Our results\ndemonstrate the effectiveness of structured label inference in these\nchallenging tasks, achieving significant improvements against baselines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 22:37:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Nauata", "Nelson", ""], ["Hu", "Hexiang", ""], ["Zhou", "Guang-Tong", ""], ["Deng", "Zhiwei", ""], ["Liao", "Zicheng", ""], ["Mori", "Greg", ""]]}, {"id": "1802.06464", "submitter": "Tat-Jun Chin Dr", "authors": "Tat-Jun Chin and Zhipeng Cai and Frank Neumann", "title": "Robust Fitting in Computer Vision: Easy or Hard?", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust model fitting plays a vital role in computer vision, and research into\nalgorithms for robust fitting continues to be active. Arguably the most popular\nparadigm for robust fitting in computer vision is consensus maximisation, which\nstrives to find the model parameters that maximise the number of inliers.\nDespite the significant developments in algorithms for consensus maximisation,\nthere has been a lack of fundamental analysis of the problem in the computer\nvision literature. In particular, whether consensus maximisation is \"tractable\"\nremains a question that has not been rigorously dealt with, thus making it\ndifficult to assess and compare the performance of proposed algorithms,\nrelative to what is theoretically achievable. To shed light on these issues, we\npresent several computational hardness results for consensus maximisation. Our\nresults underline the fundamental intractability of the problem, and resolve\nseveral ambiguities existing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 22:54:50 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 00:52:02 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 05:42:28 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Chin", "Tat-Jun", ""], ["Cai", "Zhipeng", ""], ["Neumann", "Frank", ""]]}, {"id": "1802.06474", "submitter": "Yijun Li", "authors": "Yijun Li and Ming-Yu Liu and Xueting Li and Ming-Hsuan Yang and Jan\n  Kautz", "title": "A Closed-form Solution to Photorealistic Image Stylization", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic image stylization concerns transferring style of a reference\nphoto to a content photo with the constraint that the stylized photo should\nremain photorealistic. While several photorealistic image stylization methods\nexist, they tend to generate spatially inconsistent stylizations with\nnoticeable artifacts. In this paper, we propose a method to address these\nissues. The proposed method consists of a stylization step and a smoothing\nstep. While the stylization step transfers the style of the reference photo to\nthe content photo, the smoothing step ensures spatially consistent\nstylizations. Each of the steps has a closed-form solution and can be computed\nefficiently. We conduct extensive experimental validations. The results show\nthat the proposed method generates photorealistic stylization outputs that are\nmore preferred by human subjects as compared to those by the competing methods\nwhile running much faster. Source code and additional results are available at\nhttps://github.com/NVIDIA/FastPhotoStyle .\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 00:34:18 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 21:58:01 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 17:38:42 GMT"}, {"version": "v4", "created": "Thu, 26 Jul 2018 05:49:02 GMT"}, {"version": "v5", "created": "Fri, 27 Jul 2018 01:14:23 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Li", "Yijun", ""], ["Liu", "Ming-Yu", ""], ["Li", "Xueting", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1802.06488", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, Francis Li, Brendan Chwyl", "title": "Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network\n  for Real-time Embedded Object Detection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a major challenge in computer vision, involving both\nobject classification and object localization within a scene. While deep neural\nnetworks have been shown in recent years to yield very powerful techniques for\ntackling the challenge of object detection, one of the biggest challenges with\nenabling such object detection networks for widespread deployment on embedded\ndevices is high computational and memory requirements. Recently, there has been\nan increasing focus in exploring small deep neural network architectures for\nobject detection that are more suitable for embedded devices, such as Tiny YOLO\nand SqueezeDet. Inspired by the efficiency of the Fire microarchitecture\nintroduced in SqueezeNet and the object detection performance of the\nsingle-shot detection macroarchitecture introduced in SSD, this paper\nintroduces Tiny SSD, a single-shot detection deep convolutional neural network\nfor real-time embedded object detection that is composed of a highly optimized,\nnon-uniform Fire sub-network stack and a non-uniform sub-network stack of\nhighly optimized SSD-based auxiliary convolutional feature layers designed\nspecifically to minimize model size while maintaining object detection\nperformance. The resulting Tiny SSD possess a model size of 2.3MB (~26X smaller\nthan Tiny YOLO) while still achieving an mAP of 61.3% on VOC 2007 (~4.2% higher\nthan Tiny YOLO). These experimental results show that very small deep neural\nnetwork architectures can be designed for real-time object detection that are\nwell-suited for embedded scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 01:57:46 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Chwyl", "Brendan", ""]]}, {"id": "1802.06515", "submitter": "Marcelo Cicconet", "authors": "M. Cicconet, H. Elliott, D.L. Richmond, D. Wainstock, M. Walsh", "title": "Image Forensics: Detecting duplication of scientific images with\n  manipulation-invariant image similarity", "comments": "12 pages; 6 figures; keywords: siamese network, similarity metric,\n  image forensics, image manipulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation and re-use of images in scientific publications is a concerning\nproblem that currently lacks a scalable solution. Current tools for detecting\nimage duplication are mostly manual or semi-automated, despite the availability\nof an overwhelming target dataset for a learning-based approach. This paper\naddresses the problem of determining if, given two images, one is a manipulated\nversion of the other by means of copy, rotation, translation, scale,\nperspective transform, histogram adjustment, or partial erasing. We propose a\ndata-driven solution based on a 3-branch Siamese Convolutional Neural Network.\nThe ConvNet model is trained to map images into a 128-dimensional space, where\nthe Euclidean distance between duplicate images is smaller than or equal to 1,\nand the distance between unique images is greater than 1. Our results suggest\nthat such an approach has the potential to improve surveillance of the\npublished and in-peer-review literature for image manipulation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 04:41:24 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 17:34:29 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 16:27:48 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Cicconet", "M.", ""], ["Elliott", "H.", ""], ["Richmond", "D. L.", ""], ["Wainstock", "D.", ""], ["Walsh", "M.", ""]]}, {"id": "1802.06527", "submitter": "Pingping Zhang Mr", "authors": "Pingping Zhang, Wei Liu, Huchuan Lu, Chunhua Shen", "title": "Salient Object Detection by Lossless Feature Reflection", "comments": "Accepted by IJCAI-2018, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection, which aims to identify and locate the most salient\npixels or regions in images, has been attracting more and more interest due to\nits various real-world applications. However, this vision task is quite\nchallenging, especially under complex image scenes. Inspired by the intrinsic\nreflection of natural images, in this paper we propose a novel feature learning\nframework for large-scale salient object detection. Specifically, we design a\nsymmetrical fully convolutional network (SFCN) to learn complementary saliency\nfeatures under the guidance of lossless feature reflection. The location\ninformation, together with contextual and semantic information, of salient\nobjects are jointly utilized to supervise the proposed network for more\naccurate saliency predictions. In addition, to overcome the blurry boundary\nproblem, we propose a new structural loss function to learn clear object\nboundaries and spatially consistent saliency. The coarse prediction results are\neffectively refined by these structural information for performance\nimprovements. Extensive experiments on seven saliency detection datasets\ndemonstrate that our approach achieves consistently superior performance and\noutperforms the very recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 05:59:08 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 03:19:49 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Zhang", "Pingping", ""], ["Liu", "Wei", ""], ["Lu", "Huchuan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1802.06547", "submitter": "Lei Xu", "authors": "Lei Xu, Alexandros Iosifidis, Moncef Gabbouj", "title": "Weighted Linear Discriminant Analysis based on Class Saliency\n  Information", "comments": "Submitted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new variant of Linear Discriminant Analysis to\novercome underlying drawbacks of traditional LDA and other LDA variants\ntargeting problems involving imbalanced classes. Traditional LDA sets\nassumptions related to Gaussian class distribution and neglects influence of\noutlier classes, that might hurt in performance. We exploit intuitions coming\nfrom a probabilistic interpretation of visual saliency estimation in order to\ndefine saliency of a class in multi-class setting. Such information is then\nused to redefine the between-class and within-class scatters in a more robust\nmanner. Compared to traditional LDA and other weight-based LDA variants, the\nproposed method has shown certain improvements on facial image classification\nproblems in publicly available datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:34:41 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Xu", "Lei", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1802.06573", "submitter": "Ruofan Zhou", "authors": "Ruofan Zhou, Radhakrishna Achanta, Sabine S\\\"usstrunk", "title": "Deep Residual Network for Joint Demosaicing and Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital photography, two image restoration tasks have been studied\nextensively and resolved independently: demosaicing and super-resolution. Both\nthese tasks are related to resolution limitations of the camera. Performing\nsuper-resolution on a demosaiced images simply exacerbates the artifacts\nintroduced by demosaicing. In this paper, we show that such accumulation of\nerrors can be easily averted by jointly performing demosaicing and\nsuper-resolution. To this end, we propose a deep residual network for learning\nan end-to-end mapping between Bayer images and high-resolution images. By\ntraining on high-quality samples, our deep residual demosaicing and\nsuper-resolution network is able to recover high-quality super-resolved images\nfrom low-resolution Bayer mosaics in a single step without producing the\nartifacts common to such processing when the two operations are done\nseparately. We perform extensive experiments to show that our deep residual\nnetwork achieves demosaiced and super-resolved images that are superior to the\nstate-of-the-art both qualitatively and in terms of PSNR and SSIM metrics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 09:52:59 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Zhou", "Ruofan", ""], ["Achanta", "Radhakrishna", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1802.06619", "submitter": "Timur Khanipov", "authors": "Timur M. Khanipov", "title": "Ensemble computation approach to the Hough transform", "comments": "22 pages, 8 TikZ figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is demonstrated that the classical Hough transform with shift-elevation\nparametrization of digital straight lines has additive complexity of at most\n$\\mathcal{O}(n^3 / \\log n)$ on a $n\\times n$ image. The proof is constructive\nand uses ensemble computation approach to build summation circuits. The\nproposed method has similarities with the fast Hough transform (FHT) and may be\nconsidered a form of the \"divide and conquer\" technique. It is based on the\nfact that lines with close slopes can be decomposed into common components,\nallowing generalization for other pattern families. When applied to FHT\npatterns, the algorithm yields exactly the $\\Theta(n^2\\log n)$ FHT asymptotics\nwhich might suggest that the actual classical Hough transform circuits could\nsmaller size than $\\Theta(n^3/ \\log n)$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 13:28:18 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Khanipov", "Timur M.", ""]]}, {"id": "1802.06624", "submitter": "Dian Pratiwi", "authors": "Putri Kurniasih, Dian Pratiwi", "title": "Osteoarthritis Disease Detection System using Self Organizing Maps\n  Method based on Ossa Manus X-Ray", "comments": "6 pages, 12 figures, 1 table", "journal-ref": "International Journal of Computer Applications, Foundation of\n  Computer Science (FCS), NY, USA. Volume 173 - Number 3, 2017", "doi": "10.5120/ijca2017915278", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoarthritis is a disease found in the world, including in Indonesia. The\npurpose of this study was to detect the disease Osteoarthritis using Self\nOrganizing mapping (SOM), and to know the procedure of artificial intelligence\non the methods of Self Organizing Mapping (SOM). In this system, there are\nseveral stages to preserve to detect disease Osteoarthritis using Self\nOrganizing maps is the result of photographic images rontgen Ossa Manus normal\nand sick with the resolution (150 x 200 pixels) do the repair phase contrast,\nthe Gray scale, thresholding process, Histogram of process , and do the last\nprocess, where the process of doing training (Training) and testing on images\nthat have kept the shape data (.text). the conclusion is the result of testing\nby using a data image, where 42 of data have 12 Normal image data and image\ndata 30 sick. On the results of the process of training data there are 8 X-ray\nimage revealed normal right and 19 data x-ray image of pain expressed is\ncorrect. Then the accuracy on the process of training was 96.42%, and in the\nprocess of testing normal true image 4 obtained revealed Normal, 9 data pain\nstated true pain and 1 data imagery hurts stated incorrectly, then the accuracy\ngained from the results of testing are 92,8%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 13:43:05 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kurniasih", "Putri", ""], ["Pratiwi", "Dian", ""]]}, {"id": "1802.06627", "submitter": "Beranger Dumont", "authors": "Beranger Dumont, Simona Maggio, Pablo Montalvo", "title": "Robustness of Rotation-Equivariant Networks to Adversarial Perturbations", "comments": "4 pages + references; public implementation of Spatially Transformed\n  Adversarial Examples can be found at https://github.com/rakutentech/stAdv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be vulnerable to adversarial\nexamples: very small perturbations of the input having a dramatic impact on the\npredictions. A wealth of adversarial attacks and distance metrics to quantify\nthe similarity between natural and adversarial images have been proposed,\nrecently enlarging the scope of adversarial examples with geometric\ntransformations beyond pixel-wise attacks. In this context, we investigate the\nrobustness to adversarial attacks of new Convolutional Neural Network\narchitectures providing equivariance to rotations. We found that\nrotation-equivariant networks are significantly less vulnerable to\ngeometric-based attacks than regular networks on the MNIST, CIFAR-10, and\nImageNet datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 13:49:15 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 15:24:31 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Dumont", "Beranger", ""], ["Maggio", "Simona", ""], ["Montalvo", "Pablo", ""]]}, {"id": "1802.06645", "submitter": "Tuan N.A. Hoang", "authors": "Tuan Hoang and Thanh-Toan Do and Huu Le and Dang-Khoa Le-Tan and\n  Ngai-Man Cheung", "title": "Simultaneous Compression and Quantization: A Joint Approach for\n  Efficient Unsupervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For unsupervised data-dependent hashing, the two most important requirements\nare to preserve similarity in the low-dimensional feature space and to minimize\nthe binary quantization loss. A well-established hashing approach is Iterative\nQuantization (ITQ), which addresses these two requirements in separate steps.\nIn this paper, we revisit the ITQ approach and propose novel formulations and\nalgorithms to the problem. Specifically, we propose a novel approach, named\nSimultaneous Compression and Quantization (SCQ), to jointly learn to compress\n(reduce dimensionality) and binarize input data in a single formulation under\nstrict orthogonal constraint. With this approach, we introduce a loss function\nand its relaxed version, termed Orthonormal Encoder (OnE) and Orthogonal\nEncoder (OgE) respectively, which involve challenging binary and orthogonal\nconstraints. We propose to attack the optimization using novel algorithms based\non recent advances in cyclic coordinate descent approach. Comprehensive\nexperiments on unsupervised image retrieval demonstrate that our proposed\nmethods consistently outperform other state-of-the-art hashing methods.\nNotably, our proposed methods outperform recent deep neural networks and GAN\nbased hashing in accuracy, while being very computationally-efficient.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 14:36:06 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 18:06:55 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 03:35:06 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Hoang", "Tuan", ""], ["Do", "Thanh-Toan", ""], ["Le", "Huu", ""], ["Le-Tan", "Dang-Khoa", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1802.06664", "submitter": "Gerard Pons", "authors": "Gerard Pons and David Masip", "title": "Multi-task, multi-label and multi-domain learning with residual\n  convolutional networks for emotion recognition", "comments": "Preprint submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated emotion recognition in the wild from facial images remains a\nchallenging problem. Although recent advances in Deep Learning have supposed a\nsignificant breakthrough in this topic, strong changes in pose, orientation and\npoint of view severely harm current approaches. In addition, the acquisition of\nlabeled datasets is costly, and current state-of-the-art deep learning\nalgorithms cannot model all the aforementioned difficulties. In this paper, we\npropose to apply a multi-task learning loss function to share a common feature\nrepresentation with other related tasks. Particularly we show that emotion\nrecognition benefits from jointly learning a model with a detector of facial\nAction Units (collective muscle movements). The proposed loss function\naddresses the problem of learning multiple tasks with heterogeneously labeled\ndata, improving previous multi-task approaches. We validate the proposal using\ntwo datasets acquired in non controlled environments, and an application to\npredict compound facial emotion expressions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 15:16:36 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Pons", "Gerard", ""], ["Masip", "David", ""]]}, {"id": "1802.06713", "submitter": "Amit Kumar", "authors": "Amit Kumar, Rama Chellappa", "title": "Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face\n  Alignment", "comments": "CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap regression has been used for landmark localization for quite a while\nnow. Most of the methods use a very deep stack of bottleneck modules for\nheatmap classification stage, followed by heatmap regression to extract the\nkeypoints. In this paper, we present a single dendritic CNN, termed as Pose\nConditioned Dendritic Convolution Neural Network (PCD-CNN), where a\nclassification network is followed by a second and modular classification\nnetwork, trained in an end to end fashion to obtain accurate landmark points.\nFollowing a Bayesian formulation, we disentangle the 3D pose of a face image\nexplicitly by conditioning the landmark estimation on pose, making it different\nfrom multi-tasking approaches. Extensive experimentation shows that\nconditioning on pose reduces the localization error by making it agnostic to\nface pose. The proposed model can be extended to yield variable number of\nlandmark points and hence broadening its applicability to other datasets.\nInstead of increasing depth or width of the network, we train the CNN\nefficiently with Mask-Softmax Loss and hard sample mining to achieve upto\n$15\\%$ reduction in error compared to state-of-the-art methods for extreme and\nmedium pose face images from challenging datasets including AFLW, AFW, COFW and\nIBUG.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:15:06 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 16:11:47 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 16:38:55 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kumar", "Amit", ""], ["Chellappa", "Rama", ""]]}, {"id": "1802.06724", "submitter": "Ali Javidani", "authors": "Ali Javidani, Ahmad Mahmoudi-Aznaveh", "title": "Learning Representative Temporal Features for Action Recognition", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel video classification method is presented that aims to\nrecognize different categories of third-person videos efficiently. Our\nmotivation is to achieve a light model that could be trained with insufficient\ntraining data. With this intuition, the processing of the 3-dimensional video\ninput is broken to 1D in temporal dimension on top of the 2D in spatial. The\nprocesses related to 2D spatial frames are being done by utilizing pre-trained\nnetworks with no training phase. The only step which involves training is to\nclassify the 1D time series resulted from the description of the 2D signals. As\na matter of fact, optical flow images are first calculated from consecutive\nframes and described by pre-trained CNN networks. Their dimension is then\nreduced using PCA. By stacking the description vectors beside each other, a\nmulti-channel time series is created for each video. Each channel of the time\nseries represents a specific feature and follows it over time. The main focus\nof the proposed method is to classify the obtained time series effectively.\nTowards this, the idea is to let the machine learn temporal features. This is\ndone by training a multi-channel one dimensional Convolutional Neural Network\n(1D-CNN). The 1D-CNN learns the features along the only temporal dimension.\nHence, the number of training parameters decreases significantly which would\nresult in the trainability of the method on even smaller datasets. It is\nillustrated that the proposed method could reach the state-of-the-art results\non two public datasets UCF11, jHMDB and competitive results on HMDB51.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:36:24 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 13:51:46 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 13:12:10 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 18:34:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Javidani", "Ali", ""], ["Mahmoudi-Aznaveh", "Ahmad", ""]]}, {"id": "1802.06757", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Guillem Cucurull, Pau Rodr\\'iguez, V. Oguz Yazici, Josep M. Gonfaus,\n  F. Xavier Roca, Jordi Gonz\\`alez", "title": "Deep Inference of Personality Traits by Integrating Image and Word Use\n  in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media, as a major platform for communication and information exchange,\nis a rich repository of the opinions and sentiments of 2.3 billion users about\na vast spectrum of topics. To sense the whys of certain social user's demands\nand cultural-driven interests, however, the knowledge embedded in the 1.8\nbillion pictures which are uploaded daily in public profiles has just started\nto be exploited since this process has been typically been text-based.\nFollowing this trend on visual-based social analysis, we present a novel\nmethodology based on Deep Learning to build a combined image-and-text based\npersonality trait model, trained with images posted together with words found\nhighly correlated to specific personality traits. So the key contribution here\nis to explore whether OCEAN personality trait modeling can be addressed based\non images, here called \\emph{Mind{P}ics}, appearing with certain tags with\npsychological insights. We found that there is a correlation between those\nposted images and their accompanying texts, which can be successfully modeled\nusing deep neural networks for personality estimation. The experimental results\nare consistent with previous cyber-psychology results based on texts or images.\nIn addition, classification results on some traits show that some patterns\nemerge in the set of images corresponding to a specific text, in essence to\nthose representing an abstract concept. These results open new avenues of\nresearch for further refining the proposed personality model under the\nsupervision of psychology experts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 11:58:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Cucurull", "Guillem", ""], ["Rodr\u00edguez", "Pau", ""], ["Yazici", "V. Oguz", ""], ["Gonfaus", "Josep M.", ""], ["Roca", "F. Xavier", ""], ["Gonz\u00e0lez", "Jordi", ""]]}, {"id": "1802.06772", "submitter": "Salim Arslan", "authors": "Salim Arslan", "title": "Connectivity-Driven Parcellation Methods for the Human Cerebral Cortex", "comments": "Abstract is summarised to satisfy the character limit imposed by\n  Arxiv. Please refer to the pdf for the full text. Forked from\n  https://spiral.imperial.ac.uk/handle/10044/1/54760", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we present robust and fully-automated methods for the\nsubdivision of the entire human cerebral cortex based on connectivity\ninformation. Our contributions are four-fold: First, we propose a clustering\napproach to delineate a cortical parcellation that provides a reliable\nabstraction of the brain's functional organisation. Second, we cast the\nparcellation problem as a feature reduction problem and make use of manifold\nlearning and image segmentation techniques to identify cortical regions with\ndistinct structural connectivity patterns. Third, we present a multi-layer\ngraphical model that combines within- and between-subject connectivity, which\nis then decomposed into a cortical parcellation that can represent the whole\npopulation, while accounting for the variability across subjects. Finally, we\nconduct a large-scale, systematic comparison of existing parcellation methods,\nwith a focus on providing some insight into the reliability of brain\nparcellations in terms of reflecting the underlying connectivity, as well as,\nrevealing their impact on network analysis.\n  We evaluate the proposed parcellation methods on publicly available data from\nthe Human Connectome Project and a plethora of quantitative and qualitative\nevaluation techniques investigated in the literature. Experiments across\nmultiple resolutions demonstrate the accuracy of the presented methods at both\nsubject and group levels with regards to reproducibility and fidelity to the\ndata. The neuro-biological interpretation of the proposed parcellations is also\ninvestigated by comparing parcel boundaries with well-structured properties of\nthe cerebral cortex. Results show the advantage of connectivity-driven\nparcellations over traditional approaches in terms of better fitting the\nunderlying connectivity.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 13:00:27 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Arslan", "Salim", ""]]}, {"id": "1802.06806", "submitter": "Ashish Shrivastava", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel", "title": "Divide, Denoise, and Defend against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks, although shown to be a successful class of machine\nlearning algorithms, are known to be extremely unstable to adversarial\nperturbations. Improving the robustness of neural networks against these\nattacks is important, especially for security-critical applications. To defend\nagainst such attacks, we propose dividing the input image into multiple\npatches, denoising each patch independently, and reconstructing the image,\nwithout losing significant image content. We call our method D3. This proposed\ndefense mechanism is non-differentiable which makes it non-trivial for an\nadversary to apply gradient-based attacks. Moreover, we do not fine-tune the\nnetwork with adversarial examples, making it more robust against unknown\nattacks. We present an analysis of the tradeoff between accuracy and robustness\nagainst adversarial attacks. We evaluate our method under black-box, grey-box,\nand white-box settings. On the ImageNet dataset, our method outperforms the\nstate-of-the-art by 19.7% under grey-box setting, and performs comparably under\nblack-box setting. For the white-box setting, the proposed method achieves\n34.4% accuracy compared to the 0% reported in the recent works.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 19:01:56 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 22:32:22 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Shrivastava", "Ashish", ""], ["Tuzel", "Oncel", ""]]}, {"id": "1802.06816", "submitter": "Nilaksh Das", "authors": "Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei\n  Li, Li Chen, Michael E. Kounavis, Duen Horng Chau", "title": "Shield: Fast, Practical Defense and Vaccination for Deep Learning using\n  JPEG Compression", "comments": "under review at KDD'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing body of research in adversarial machine learning has\ndemonstrated that deep neural networks (DNNs) are highly vulnerable to\nadversarially generated images. This underscores the urgent need for practical\ndefense that can be readily deployed to combat attacks in real-time. Observing\nthat many attack strategies aim to perturb image pixels in ways that are\nvisually imperceptible, we place JPEG compression at the core of our proposed\nShield defense framework, utilizing its capability to effectively \"compress\naway\" such pixel manipulation. To immunize a DNN model from artifacts\nintroduced by compression, Shield \"vaccinates\" a model by re-training it with\ncompressed images, where different compression levels are applied to generate\nmultiple vaccinated models that are ultimately used together in an ensemble\ndefense. On top of that, Shield adds an additional layer of protection by\nemploying randomization at test time that compresses different regions of an\nimage using random compression levels, making it harder for an adversary to\nestimate the transformation performed. This novel combination of vaccination,\nensembling, and randomization makes Shield a fortified multi-pronged\nprotection. We conducted extensive, large-scale experiments using the ImageNet\ndataset, and show that our approaches eliminate up to 94% of black-box attacks\nand 98% of gray-box attacks delivered by the recent, strongest attacks, such as\nCarlini-Wagner's L2 and DeepFool. Our approaches are fast and work without\nrequiring knowledge about the model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 19:13:42 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Das", "Nilaksh", ""], ["Shanbhogue", "Madhuri", ""], ["Chen", "Shang-Tse", ""], ["Hohman", "Fred", ""], ["Li", "Siwei", ""], ["Chen", "Li", ""], ["Kounavis", "Michael E.", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1802.06822", "submitter": "Zheng Shou", "authors": "Zheng Shou, Junting Pan, Jonathan Chan, Kazuyuki Miyazawa, Hassan\n  Mansour, Anthony Vetro, Xavier Giro-i-Nieto, Shih-Fu Chang", "title": "Online Detection of Action Start in Untrimmed, Streaming Videos", "comments": "Accepted by ECCV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to tackle a novel task in action detection - Online Detection of\nAction Start (ODAS) in untrimmed, streaming videos. The goal of ODAS is to\ndetect the start of an action instance, with high categorization accuracy and\nlow detection latency. ODAS is important in many applications such as early\nalert generation to allow timely security or emergency response. We propose\nthree novel methods to specifically address the challenges in training ODAS\nmodels: (1) hard negative samples generation based on Generative Adversarial\nNetwork (GAN) to distinguish ambiguous background, (2) explicitly modeling the\ntemporal consistency between data around action start and data succeeding\naction start, and (3) adaptive sampling strategy to handle the scarcity of\ntraining data. We conduct extensive experiments using THUMOS'14 and\nActivityNet. We show that our proposed methods lead to significant performance\ngains and improve the state-of-the-art methods. An ablation study confirms the\neffectiveness of each proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 19:39:05 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 18:11:23 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 05:15:15 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Shou", "Zheng", ""], ["Pan", "Junting", ""], ["Chan", "Jonathan", ""], ["Miyazawa", "Kazuyuki", ""], ["Mansour", "Hassan", ""], ["Vetro", "Anthony", ""], ["Giro-i-Nieto", "Xavier", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1802.06857", "submitter": "Jian Zhang", "authors": "Emilio Parisotto, Devendra Singh Chaplot, Jian Zhang, Ruslan\n  Salakhutdinov", "title": "Global Pose Estimation with an Attention-based Recurrent Network", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability for an agent to localize itself within an environment is crucial\nfor many real-world applications. For unknown environments, Simultaneous\nLocalization and Mapping (SLAM) enables incremental and concurrent building of\nand localizing within a map. We present a new, differentiable architecture,\nNeural Graph Optimizer, progressing towards a complete neural network solution\nfor SLAM by designing a system composed of a local pose estimation model, a\nnovel pose selection module, and a novel graph optimization process. The entire\narchitecture is trained in an end-to-end fashion, enabling the network to\nautomatically learn domain-specific features relevant to the visual odometry\nand avoid the involved process of feature engineering. We demonstrate the\neffectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom\nenvironment.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:17:10 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Parisotto", "Emilio", ""], ["Chaplot", "Devendra Singh", ""], ["Zhang", "Jian", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1802.06865", "submitter": "Jonas Teuwen", "authors": "Timothy de Moor and Alejandro Rodriguez-Ruiz and Albert Gubern\n  M\\'erida and Ritse Mann and Jonas Teuwen", "title": "Automated soft tissue lesion detection and segmentation in digital\n  mammography using a u-net deep learning network", "comments": "To appear in IWBI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided detection or decision support systems aim to improve breast\ncancer screening programs by helping radiologists to evaluate digital\nmammography (DM) exams. Commonly such methods proceed in two steps: selection\nof candidate regions for malignancy, and later classification as either\nmalignant or not. In this study, we present a candidate detection method based\non deep learning to automatically detect and additionally segment soft tissue\nlesions in DM. A database of DM exams (mostly bilateral and two views) was\ncollected from our institutional archive. In total, 7196 DM exams (28294 DM\nimages) acquired with systems from three different vendors (General Electric,\nSiemens, Hologic) were collected, of which 2883 contained malignant lesions\nverified with histopathology. Data was randomly split on an exam level into\ntraining (50\\%), validation (10\\%) and testing (40\\%) of deep neural network\nwith u-net architecture. The u-net classifies the image but also provides\nlesion segmentation. Free receiver operating characteristic (FROC) analysis was\nused to evaluate the model, on an image and on an exam level. On an image\nlevel, a maximum sensitivity of 0.94 at 7.93 false positives (FP) per image was\nachieved. Similarly, per exam a maximum sensitivity of 0.98 at 7.81 FP per\nimage was achieved. In conclusion, the method could be used as a candidate\nselection model with high accuracy and with the additional information of\nlesion segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:39:49 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 09:21:11 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["de Moor", "Timothy", ""], ["Rodriguez-Ruiz", "Alejandro", ""], ["M\u00e9rida", "Albert Gubern", ""], ["Mann", "Ritse", ""], ["Teuwen", "Jonas", ""]]}, {"id": "1802.06869", "submitter": "Anna Choromanska", "authors": "Yunfei Teng, Anna Choromanska, Mariusz Bojarski", "title": "Invertible Autoencoder for domain adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised image-to-image translation aims at finding a mapping between\nthe source ($A$) and target ($B$) image domains, where in many applications\naligned image pairs are not available at training. This is an ill-posed\nlearning problem since it requires inferring the joint probability distribution\nfrom marginals. Joint learning of coupled mappings $F_{AB}: A \\rightarrow B$\nand $F_{BA}: B \\rightarrow A$ is commonly used by the state-of-the-art methods,\nlike CycleGAN [Zhu et al., 2017], to learn this translation by introducing\ncycle consistency requirement to the learning problem, i.e. $F_{AB}(F_{BA}(B))\n\\approx B$ and $F_{BA}(F_{AB}(A)) \\approx A$. Cycle consistency enforces the\npreservation of the mutual information between input and translated images.\nHowever, it does not explicitly enforce $F_{BA}$ to be an inverse operation to\n$F_{AB}$. We propose a new deep architecture that we call invertible\nautoencoder (InvAuto) to explicitly enforce this relation. This is done by\nforcing an encoder to be an inverted version of the decoder, where\ncorresponding layers perform opposite mappings and share parameters. The\nmappings are constrained to be orthonormal. The resulting architecture leads to\nthe reduction of the number of trainable parameters (up to $2$ times). We\npresent image translation results on benchmark data sets and demonstrate\nstate-of-the art performance of our approach. Finally, we test the proposed\ndomain adaptation method on the task of road video conversion. We demonstrate\nthat the videos converted with InvAuto have high quality and show that the\nNVIDIA neural-network-based end-to-end learning system for autonomous driving,\nknown as PilotNet, trained on real road videos performs well when tested on the\nconverted ones.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:00:32 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Teng", "Yunfei", ""], ["Choromanska", "Anna", ""], ["Bojarski", "Mariusz", ""]]}, {"id": "1802.06897", "submitter": "Patrick Emami", "authors": "Patrick Emami, Panos M. Pardalos, Lily Elefteriadou, Sanjay Ranka", "title": "Machine Learning Methods for Data Association in Multi-Object Tracking", "comments": "Accepted for publication in ACM Computing Surveys", "journal-ref": "Volume 53, Issue 4 (August 2020)", "doi": "10.1145/3394659", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association is a key step within the multi-object tracking pipeline that\nis notoriously challenging due to its combinatorial nature. A popular and\ngeneral way to formulate data association is as the NP-hard multidimensional\nassignment problem (MDAP). Over the last few years, data-driven approaches to\nassignment have become increasingly prevalent as these techniques have started\nto mature. We focus this survey solely on learning algorithms for the\nassignment step of multi-object tracking, and we attempt to unify various\nmethods by highlighting their connections to linear assignment as well as to\nthe MDAP. First, we review probabilistic and end-to-end optimization approaches\nto data association, followed by methods that learn association affinities from\ndata. We then compare the performance of the methods presented in this survey,\nand conclude by discussing future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 22:42:56 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 22:42:17 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Emami", "Patrick", ""], ["Pardalos", "Panos M.", ""], ["Elefteriadou", "Lily", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1802.06898", "submitter": "Alex Zihao Zhu", "authors": "Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis", "title": "EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based\n  Cameras", "comments": "9 pages, 5 figures, 1 table. Accompanying video:\n  https://youtu.be/eMHZBSoq0sE. Dataset:\n  https://daniilidis-group.github.io/mvsec/, Robotics: Science and Systems 2018", "journal-ref": null, "doi": "10.15607/RSS.2018.XIV.062", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras have shown great promise in a variety of situations where\nframe based cameras suffer, such as high speed motions and high dynamic range\nscenes. However, developing algorithms for event measurements requires a new\nclass of hand crafted algorithms. Deep learning has shown great success in\nproviding model free solutions to many problems in the vision community, but\nexisting networks have been developed with frame based images in mind, and\nthere does not exist the wealth of labeled data for events as there does for\nimages for supervised training. To these points, we present EV-FlowNet, a novel\nself-supervised deep learning pipeline for optical flow estimation for event\nbased cameras. In particular, we introduce an image based representation of a\ngiven event stream, which is fed into a self-supervised neural network as the\nsole input. The corresponding grayscale images captured from the same camera at\nthe same time as the events are then used as a supervisory signal to provide a\nloss function at training time, given the estimated flow from the network. We\nshow that the resulting network is able to accurately predict optical flow from\nevents only in a variety of different scenes, with performance competitive to\nimage based networks. This method not only allows for accurate estimation of\ndense optical flow, but also provides a framework for the transfer of other\nself-supervised methods to the event-based domain.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 22:47:52 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 14:44:49 GMT"}, {"version": "v3", "created": "Sun, 22 Jul 2018 15:36:48 GMT"}, {"version": "v4", "created": "Mon, 13 Aug 2018 15:32:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Zhu", "Alex Zihao", ""], ["Yuan", "Liangzhe", ""], ["Chaney", "Kenneth", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1802.06924", "submitter": "Oisin Mac Aodha", "authors": "Oisin Mac Aodha and Shihan Su and Yuxin Chen and Pietro Perona and\n  Yisong Yue", "title": "Teaching Categories to Human Learners with Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computer-assisted teaching with explanations.\nConventional approaches for machine teaching typically only provide feedback at\nthe instance level e.g., the category or label of the instance. However, it is\nintuitive that clear explanations from a knowledgeable teacher can\nsignificantly improve a student's ability to learn a new concept. To address\nthese existing limitations, we propose a teaching framework that provides\ninterpretable explanations as feedback and models how the learner incorporates\nthis additional information. In the case of images, we show that we can\nautomatically generate explanations that highlight the parts of the image that\nare responsible for the class label. Experiments on human learners illustrate\nthat, on average, participants achieve better test set performance on\nchallenging categorization tasks when taught with our interpretable approach\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 00:49:38 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Mac Aodha", "Oisin", ""], ["Su", "Shihan", ""], ["Chen", "Yuxin", ""], ["Perona", "Pietro", ""], ["Yue", "Yisong", ""]]}, {"id": "1802.06926", "submitter": "Yang Gao", "authors": "Yang Gao, Shouyan Guo, Kaimin Huang, Jiaxin Chen, Qian Gong, Yang Zou,\n  Tong Bai and Gary Overett", "title": "Scale Optimization for Full-Image-CNN Vehicle Detection", "comments": "Accepted by 2017 IEEE Intelligent Vehicles Symposium (IV). Link:\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995812", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art general object detection methods make use of shared\nfull-image convolutional features (as in Faster R-CNN). This achieves a\nreasonable test-phase computation time while enjoys the discriminative power\nprovided by large Convolutional Neural Network (CNN) models. Such designs excel\non benchmarks which contain natural images but which have very unnatural\ndistributions, i.e. they have an unnaturally high-frequency of the target\nclasses and a bias towards a \"friendly\" or \"dominant\" object scale. In this\npaper we present further study of the use and adaptation of the Faster R-CNN\nobject detection method for datasets presenting natural scale distribution and\nunbiased real-world object frequency. In particular, we show that better\nalignment of the detector scale sensitivity to the extant distribution improves\nvehicle detection performance. We do this by modifying both the selection of\nRegion Proposals, and through using more scale-appropriate full-image\nconvolution features within the CNN model. By selecting better scales in the\nregion proposal input and by combining feature maps through careful design of\nthe convolutional neural network, we improve performance on smaller objects. We\nsignificantly increase detection AP for the KITTI dataset car class from 76.3%\non our baseline Faster R-CNN detector to 83.6% in our improved detector.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 00:54:15 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Gao", "Yang", ""], ["Guo", "Shouyan", ""], ["Huang", "Kaimin", ""], ["Chen", "Jiaxin", ""], ["Gong", "Qian", ""], ["Zou", "Yang", ""], ["Bai", "Tong", ""], ["Overett", "Gary", ""]]}, {"id": "1802.06927", "submitter": "Nishant Desai", "authors": "Vinay Uday Prabhu, Nishant Desai, John Whaley", "title": "On Lyapunov exponents and adversarial perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we would like to disseminate a serendipitous discovery\ninvolving Lyapunov exponents of a 1-D time series and their use in serving as a\nfiltering defense tool against a specific kind of deep adversarial\nperturbation. To this end, we use the state-of-the-art CleverHans library to\ngenerate adversarial perturbations against a standard Convolutional Neural\nNetwork (CNN) architecture trained on the MNIST as well as the Fashion-MNIST\ndatasets. We empirically demonstrate how the Lyapunov exponents computed on the\nflattened 1-D vector representations of the images served as highly\ndiscriminative features that could be to pre-classify images as adversarial or\nlegitimate before feeding the image into the CNN for classification. We also\nexplore the issue of possible false-alarms when the input images are noisy in a\nnon-adversarial sense.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 01:09:22 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Prabhu", "Vinay Uday", ""], ["Desai", "Nishant", ""], ["Whaley", "John", ""]]}, {"id": "1802.06935", "submitter": "Qi Chang", "authors": "Qi Chang and Gene Cheung and Yao Zhao and Xiaolong Li and Rongrong Ni", "title": "Non-Local Graph-Based Prediction For Reversible Data Hiding In Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible data hiding (RDH) is desirable in applications where both the\nhidden message and the cover medium need to be recovered without loss. Among\nmany RDH approaches is prediction-error expansion (PEE), containing two steps:\ni) prediction of a target pixel value, and ii) embedding according to the value\nof prediction-error. In general, higher prediction performance leads to larger\nembedding capacity and/or lower signal distortion. Leveraging on recent\nadvances in graph signal processing (GSP), we pose pixel prediction as a\ngraph-signal restoration problem, where the appropriate edge weights of the\nunderlying graph are computed using a similar patch searched in a semi-local\nneighborhood. Specifically, for each candidate patch, we first examine\neigenvalues of its structure tensor to estimate its local smoothness. If\nsufficiently smooth, we pose a maximum a posteriori (MAP) problem using either\na quadratic Laplacian regularizer or a graph total variation (GTV) term as\nsignal prior. While the MAP problem using the first prior has a closed-form\nsolution, we design an efficient algorithm for the second prior using\nalternating direction method of multipliers (ADMM) with nested proximal\ngradient descent. Experimental results show that with better quality GSP-based\nprediction, at low capacity the visual quality of the embedded image exceeds\nstate-of-the-art methods noticeably.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 02:28:51 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Chang", "Qi", ""], ["Cheung", "Gene", ""], ["Zhao", "Yao", ""], ["Li", "Xiaolong", ""], ["Ni", "Rongrong", ""]]}, {"id": "1802.06955", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, and\n  Vijayan K. Asari", "title": "Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net)\n  for Medical Image Segmentation", "comments": "12 pages, 21 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning (DL) based semantic segmentation methods have been providing\nstate-of-the-art performance in the last few years. More specifically, these\ntechniques have been successfully applied to medical image classification,\nsegmentation, and detection tasks. One deep learning technique, U-Net, has\nbecome one of the most popular for these applications. In this paper, we\npropose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well\nas a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net\nmodels, which are named RU-Net and R2U-Net respectively. The proposed models\nutilize the power of U-Net, Residual Network, as well as RCNN. There are\nseveral advantages of these proposed architectures for segmentation tasks.\nFirst, a residual unit helps when training deep architecture. Second, feature\naccumulation with recurrent residual convolutional layers ensures better\nfeature representation for segmentation tasks. Third, it allows us to design\nbetter U-Net architecture with same number of network parameters with better\nperformance for medical image segmentation. The proposed models are tested on\nthree benchmark datasets such as blood vessel segmentation in retina images,\nskin cancer segmentation, and lung lesion segmentation. The experimental\nresults show superior performance on segmentation tasks compared to equivalent\nmodels including U-Net and residual U-Net (ResU-Net).\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 03:59:39 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 23:28:10 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 14:49:28 GMT"}, {"version": "v4", "created": "Thu, 10 May 2018 15:09:26 GMT"}, {"version": "v5", "created": "Tue, 29 May 2018 17:57:31 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Hasan", "Mahmudul", ""], ["Yakopcic", "Chris", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1802.06960", "submitter": "Pingping Zhang", "authors": "Pingping Zhang, Luyao Wang, Dong Wang, Huchuan Lu, Chunhua Shen", "title": "Agile Amulet: Real-Time Salient Object Detection with Contextual\n  Attention", "comments": "10 pages, 4 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an Agile Aggregating Multi-Level feaTure framework (Agile\nAmulet) for salient object detection. The Agile Amulet builds on previous works\nto predict saliency maps using multi-level convolutional features. Compared to\nprevious works, Agile Amulet employs some key innovations to improve training\nand testing speed while also increase prediction accuracy. More specifically,\nwe first introduce a contextual attention module that can rapidly highlight\nmost salient objects or regions with contextual pyramids. Thus, it effectively\nguides the learning of low-layer convolutional features and tells the backbone\nnetwork where to look. The contextual attention module is a fully convolutional\nmechanism that simultaneously learns complementary features and predicts\nsaliency scores at each pixel. In addition, we propose a novel method to\naggregate multi-level deep convolutional features. As a result, we are able to\nuse the integrated side-output features of pre-trained convolutional networks\nalone, which significantly reduces the model parameters leading to a model size\nof 67 MB, about half of Amulet. Compared to other deep learning based saliency\nmethods, Agile Amulet is of much lighter-weight, runs faster (30 fps in\nreal-time) and achieves higher performance on seven public benchmarks in terms\nof both quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 04:14:08 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Zhang", "Pingping", ""], ["Wang", "Luyao", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1802.06964", "submitter": "Min-Kook Choi", "authors": "Min-Kook Choi, Jaehyeong Park, Jihun Jung, Heechul Jung, Jin-Hee Lee,\n  Woong Jae Won, Woo Young Jung, Jincheol Kim, Soon Kwon", "title": "Co-occurrence matrix analysis-based semi-supervised training for object\n  detection", "comments": "Submitted to International Conference on Image Processing (ICIP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important factors in training object recognition networks\nusing convolutional neural networks (CNNs) is the provision of annotated data\naccompanying human judgment. Particularly, in object detection or semantic\nsegmentation, the annotation process requires considerable human effort. In\nthis paper, we propose a semi-supervised learning (SSL)-based training\nmethodology for object detection, which makes use of automatic labeling of\nun-annotated data by applying a network previously trained from an annotated\ndataset. Because an inferred label by the trained network is dependent on the\nlearned parameters, it is often meaningless for re-training the network. To\ntransfer a valuable inferred label to the unlabeled data, we propose a\nre-alignment method based on co-occurrence matrix analysis that takes into\naccount one-hot-vector encoding of the estimated label and the correlation\nbetween the objects in the image. We used an MS-COCO detection dataset to\nverify the performance of the proposed SSL method and deformable neural\nnetworks (D-ConvNets) as an object detector for basic training. The performance\nof the existing state-of-the-art detectors (DConvNets, YOLO v2, and single shot\nmulti-box detector (SSD)) can be improved by the proposed SSL method without\nusing the additional model parameter or modifying the network architecture.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 04:39:49 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Choi", "Min-Kook", ""], ["Park", "Jaehyeong", ""], ["Jung", "Jihun", ""], ["Jung", "Heechul", ""], ["Lee", "Jin-Hee", ""], ["Won", "Woong Jae", ""], ["Jung", "Woo Young", ""], ["Kim", "Jincheol", ""], ["Kwon", "Soon", ""]]}, {"id": "1802.06971", "submitter": "Jiang Bian", "authors": "Jiang Bian, Dayong Tian, Yuanyan Tang, Dacheng Tao", "title": "A survey on trajectory clustering analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper comprehensively surveys the development of trajectory clustering.\nConsidering the critical role of trajectory data mining in modern intelligent\nsystems for surveillance security, abnormal behavior detection, crowd behavior\nanalysis, and traffic control, trajectory clustering has attracted growing\nattention. Existing trajectory clustering methods can be grouped into three\ncategories: unsupervised, supervised and semi-supervised algorithms. In spite\nof achieving a certain level of development, trajectory clustering is limited\nin its success by complex conditions such as application scenarios and data\ndimensions. This paper provides a holistic understanding and deep insight into\ntrajectory clustering, and presents a comprehensive analysis of representative\nmethods and promising future directions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 05:28:51 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Bian", "Jiang", ""], ["Tian", "Dayong", ""], ["Tang", "Yuanyan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1802.06983", "submitter": "Pingping Zhang", "authors": "Fei Li, Pingping Zhang, Huchuan Lu", "title": "Unsupervised Band Selection of Hyperspectral Images via Multi-dictionary\n  Sparse Representation", "comments": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology, including 13 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images have far more spectral bands than ordinary multispectral\nimages. Rich band information provides more favorable conditions for the\ntremendous applications. However, significant increase in the dimensionality of\nspectral bands may lead to the curse of dimensionality, especially for\nclassification applications. Furthermore, there are a large amount of redundant\ninformation among the raw image cubes due to water absorptions, sensor noises\nand other influence factors. Band selection is a direct and effective method to\nremove redundant information and reduce the spectral dimension for decreasing\ncomputational complexity and avoiding the curse of dimensionality. In this\npaper, we present a novel learning framework for band selection based on the\nidea of sparse representation. More specifically, first each band is\napproximately represented by the linear combination of other bands, then the\noriginal band image can be represented by a multi-dictionary learning\nmechanism. As a result, a group of weights can be obtained by sparse\noptimization for all bands. Finally, the specific bands will be selected, if\nthey get higher weights than other bands in the representation of the original\nimage. Experimental results on three widely used hyperspectral datasets show\nthat our proposed algorithm achieves better performance in hyperspectral image\nclassification, when compared with other state-of-art band selection methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 07:04:09 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Li", "Fei", ""], ["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""]]}, {"id": "1802.07008", "submitter": "Amin Fehri", "authors": "Amin Fehri (CMM), Santiago Velasco-Forero (CMM), Fernand Meyer (CMM)", "title": "Segmentation hi\\'erarchique faiblement supervis\\'ee", "comments": "in French", "journal-ref": "26e colloque GRETSI, Sep 2017, Juan-les-Pins, France", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning an image into a set of\nmeaningful regions according to some criteria. Hierarchical segmentation has\nemerged as a major trend in this regard as it favors the emergence of important\nregions at different scales. On the other hand, many methods allow us to have\nprior information on the position of structures of interest in the images. In\nthis paper, we present a versatile hierarchical segmentation method that takes\ninto account any prior spatial information and outputs a hierarchical\nsegmentation that emphasizes the contours or regions of interest while\npreserving the important structures in the image. An application of this method\nto the weakly-supervised segmentation problem is presented.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 08:42:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Fehri", "Amin", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Meyer", "Fernand", "", "CMM"]]}, {"id": "1802.07021", "submitter": "Yuehong Huang", "authors": "Yuehong Huang, Yu-Chee Tseng", "title": "Fusing Video and Inertial Sensor Data for Walking Person Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autonomous computer system (such as a robot) typically needs to identify,\nlocate, and track persons appearing in its sight. However, most solutions have\ntheir limitations regarding efficiency, practicability, or environmental\nconstraints. In this paper, we propose an effective and practical system which\ncombines video and inertial sensors for person identification (PID). Persons\nwho do different activities are easy to identify. To show the robustness and\npotential of our system, we propose a walking person identification (WPID)\nmethod to identify persons walking at the same time. By comparing features\nderived from both video and inertial sensor data, we can associate sensors in\nsmartphones with human objects in videos. Results show that the correctly\nidentified rate of our WPID method can up to 76% in 2 seconds.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:16:21 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Huang", "Yuehong", ""], ["Tseng", "Yu-Chee", ""]]}, {"id": "1802.07042", "submitter": "Alex Hern\\'andez Garc\\'ia", "authors": "Alex Hern\\'andez-Garc\\'ia, Peter K\\\"onig", "title": "Do deep nets really need weight decay and dropout?", "comments": "Minor changes: more explicit legend of the figure, clearer second\n  paragraph of the introduction, details of weight decay and dropout in the\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive success of modern deep neural networks on computer vision\ntasks has been achieved through models of very large capacity compared to the\nnumber of available training examples. This overparameterization is often said\nto be controlled with the help of different regularization techniques, mainly\nweight decay and dropout. However, since these techniques reduce the effective\ncapacity of the model, typically even deeper and wider architectures are\nrequired to compensate for the reduced capacity. Therefore, there seems to be a\nwaste of capacity in this practice. In this paper we build upon recent research\nthat suggests that explicit regularization may not be as important as widely\nbelieved and carry out an ablation study that concludes that weight decay and\ndropout may not be necessary for object recognition if enough data augmentation\nis introduced.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 10:12:23 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 10:16:35 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 16:37:49 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Hern\u00e1ndez-Garc\u00eda", "Alex", ""], ["K\u00f6nig", "Peter", ""]]}, {"id": "1802.07045", "submitter": "Roee Litman", "authors": "Simon Korman and Roee Litman", "title": "Latent RANSAC", "comments": "presented in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that can evaluate a RANSAC hypothesis in constant time,\ni.e. independent of the size of the data. A key observation here is that\ncorrect hypotheses are tightly clustered together in the latent parameter\ndomain. In a manner similar to the generalized Hough transform we seek to find\nthis cluster, only that we need as few as two votes for a successful detection.\nRapidly locating such pairs of similar hypotheses is made possible by adapting\nthe recent \"Random Grids\" range-search technique. We only perform the usual\n(costly) hypothesis verification stage upon the discovery of a close pair of\nhypotheses. We show that this event rarely happens for incorrect hypotheses,\nenabling a significant speedup of the RANSAC pipeline. The suggested approach\nis applied and tested on three robust estimation problems: camera localization,\n3D rigid alignment and 2D-homography estimation. We perform rigorous testing on\nboth synthetic and real datasets, demonstrating an improvement in efficiency\nwithout a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D\nalignment results on the challenging \"Redwood\" loop-closure challenge.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 10:16:11 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 14:41:36 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Korman", "Simon", ""], ["Litman", "Roee", ""]]}, {"id": "1802.07064", "submitter": "Xiaochuan Yin", "authors": "Xiaochuan Yin, Henglai Wei, Penghong lin, Xiangwei Wang, Qijun Chen", "title": "Novel View Synthesis for Large-scale Scene using Adversarial Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis aims to synthesize new images from different viewpoints\nof given images. Most of previous works focus on generating novel views of\ncertain objects with a fixed background. However, for some applications, such\nas virtual reality or robotic manipulations, large changes in background may\noccur due to the egomotion of the camera. Generated images of a large-scale\nenvironment from novel views may be distorted if the structure of the\nenvironment is not considered. In this work, we propose a novel fully\nconvolutional network, that can take advantage of the structural information\nexplicitly by incorporating the inverse depth features. The inverse depth\nfeatures are obtained from CNNs trained with sparse labeled depth values. This\nframework can easily fuse multiple images from different viewpoints. To fill\nthe missing textures in the generated image, adversarial loss is applied, which\ncan also improve the overall image quality. Our method is evaluated on the\nKITTI dataset. The results show that our method can generate novel views of\nlarge-scale scene without distortion. The effectiveness of our approach is\ndemonstrated through qualitative and quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 11:21:11 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Yin", "Xiaochuan", ""], ["Wei", "Henglai", ""], ["lin", "Penghong", ""], ["Wang", "Xiangwei", ""], ["Chen", "Qijun", ""]]}, {"id": "1802.07072", "submitter": "Jonas Geiping", "authors": "Jonas Geiping and Michael Moeller", "title": "Composite Optimization by Nonconvex Majorization-Minimization", "comments": "38 pages, 12 figures, accepted for publication in SIIMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimization of a nonconvex composite function can model a variety of\nimaging tasks. A popular class of algorithms for solving such problems are\nmajorization-minimization techniques which iteratively approximate the\ncomposite nonconvex function by a majorizing function that is easy to minimize.\nMost techniques, e.g. gradient descent, utilize convex majorizers in order to\nguarantee that the majorizer is easy to minimize. In our work we consider a\nnatural class of nonconvex majorizers for these functions, and show that these\nmajorizers are still sufficient for a globally convergent optimization scheme.\nNumerical results illustrate that by applying this scheme, one can often obtain\nsuperior local optima compared to previous majorization-minimization methods,\nwhen the nonconvex majorizers are solved to global optimality. Finally, we\nillustrate the behavior of our algorithm for depth super-resolution from raw\ntime-of-flight data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 11:59:33 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 10:33:44 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Geiping", "Jonas", ""], ["Moeller", "Michael", ""]]}, {"id": "1802.07078", "submitter": "Chen Wang", "authors": "Chen Wang, Tete Ji, Thien-Minh Nguyen, Lihua Xie", "title": "Correlation Flow: Robust Optical Flow Using Kernel Cross-Correlators", "comments": "2018 International Conference on Robotics and Automation (ICRA 2018)", "journal-ref": "2018 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": "10.1109/ICRA.2018.8460569", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust velocity and position estimation is crucial for autonomous robot\nnavigation. The optical flow based methods for autonomous navigation have been\nreceiving increasing attentions in tandem with the development of micro\nunmanned aerial vehicles. This paper proposes a kernel cross-correlator (KCC)\nbased algorithm to determine optical flow using a monocular camera, which is\nnamed as correlation flow (CF). Correlation flow is able to provide reliable\nand accurate velocity estimation and is robust to motion blur. In addition, it\ncan also estimate the altitude velocity and yaw rate, which are not available\nby traditional methods. Autonomous flight tests on a quadcopter show that\ncorrelation flow can provide robust trajectory estimation with very low\nprocessing power. The source codes are released based on the ROS framework.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:19:03 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 06:29:18 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Wang", "Chen", ""], ["Ji", "Tete", ""], ["Nguyen", "Thien-Minh", ""], ["Xie", "Lihua", ""]]}, {"id": "1802.07088", "submitter": "Edouard Oyallon", "authors": "J\\\"orn-Henrik Jacobsen (IvI), Arnold Smeulders (IvI), Edouard Oyallon\n  (CVN, GALEN, SEQUEL, DI-ENS)", "title": "i-RevNet: Deep Invertible Networks", "comments": null, "journal-ref": "ICLR 2018 - International Conference on Learning Representations,\n  Apr 2018, Vancouver, Canada. 2018, https://iclr.cc/", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the success of deep convolutional networks is\nbased on progressively discarding uninformative variability about the input\nwith respect to the problem at hand. This is supported empirically by the\ndifficulty of recovering images from their hidden representations, in most\ncommonly used network architectures. In this paper we show via a one-to-one\nmapping that this loss of information is not a necessary condition to learn\nrepresentations that generalize well on complicated problems, such as ImageNet.\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can\nbe fully inverted up to the final projection onto the classes, i.e. no\ninformation is discarded. Building an invertible architecture is difficult, for\none, because the local inversion is ill-conditioned, we overcome this by\nproviding an explicit inverse. An analysis of i-RevNets learned representations\nsuggests an alternative explanation for the success of deep networks by a\nprogressive contraction and linear separation with depth. To shed light on the\nnature of the model learned by the i-RevNet we reconstruct linear\ninterpolations between natural image representations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:38:49 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", "", "IvI"], ["Smeulders", "Arnold", "", "IvI"], ["Oyallon", "Edouard", "", "CVN, GALEN, SEQUEL, DI-ENS"]]}, {"id": "1802.07094", "submitter": "Moritz Kampelm\\\"uhler", "authors": "Moritz Kampelm\\\"uhler, Michael G. M\\\"uller, Christoph Feichtenhofer", "title": "Camera-based vehicle velocity estimation from monocular video", "comments": "8 pages, 5 figures, in CVWW2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper documents the winning entry at the CVPR2017 vehicle velocity\nestimation challenge. Velocity estimation is an emerging task in autonomous\ndriving which has not yet been thoroughly explored. The goal is to estimate the\nrelative velocity of a specific vehicle from a sequence of images. In this\npaper, we present a light-weight approach for directly regressing vehicle\nvelocities from their trajectories using a multilayer perceptron. Another\ncontribution is an explorative study of features for monocular vehicle velocity\nestimation. We find that light-weight trajectory based features outperform\ndepth and motion cues extracted from deep ConvNets, especially for far-distance\npredictions where current disparity and optical flow estimators are challenged\nsignificantly. Our light-weight approach is real-time capable on a single CPU\nand outperforms all competing entries in the velocity estimation challenge. On\nthe test set, we report an average error of 1.12 m/s which is comparable to a\n(ground-truth) system that combines LiDAR and radar techniques to achieve an\nerror of around 0.71 m/s.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:54:39 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Kampelm\u00fchler", "Moritz", ""], ["M\u00fcller", "Michael G.", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "1802.07095", "submitter": "Eddy Ilg", "authors": "Eddy Ilg, \\\"Ozg\\\"un \\c{C}i\\c{c}ek, Silvio Galesso, Aaron Klein, Osama\n  Makansi, Frank Hutter, Thomas Brox", "title": "Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow", "comments": "Accepted to ECCV 2018 as poster. See Video at:\n  https://youtu.be/HvyovWSo8uE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation can be formulated as an end-to-end supervised\nlearning problem, which yields estimates with a superior accuracy-runtime\ntradeoff compared to alternative methodology. In this paper, we make such\nnetworks estimate their local uncertainty about the correctness of their\nprediction, which is vital information when building decisions on top of the\nestimations. For the first time we compare several strategies and techniques to\nestimate uncertainty in a large-scale computer vision task like optical flow\nestimation. Moreover, we introduce a new network architecture utilizing the\nWinner-Takes-All loss and show that this can provide complementary hypotheses\nand uncertainty estimates efficiently with a single forward pass and without\nthe need for sampling or ensembles. Finally, we demonstrate the quality of the\ndifferent uncertainty estimates, which is clearly above previous confidence\nmeasures on optical flow and allows for interactive frame rates.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:56:39 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 12:44:14 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 07:22:32 GMT"}, {"version": "v4", "created": "Thu, 20 Dec 2018 15:17:23 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Ilg", "Eddy", ""], ["\u00c7i\u00e7ek", "\u00d6zg\u00fcn", ""], ["Galesso", "Silvio", ""], ["Klein", "Aaron", ""], ["Makansi", "Osama", ""], ["Hutter", "Frank", ""], ["Brox", "Thomas", ""]]}, {"id": "1802.07101", "submitter": "Yongcheng Jing", "authors": "Yongcheng Jing, Yang Liu, Yezhou Yang, Zunlei Feng, Yizhou Yu, Dacheng\n  Tao, Mingli Song", "title": "Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields", "comments": "Accepted by ECCV2018. Supplementary material:\n  https://yongchengjing.com/pdf/strokeControllable_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fast Style Transfer methods have been recently proposed to transfer a\nphotograph to an artistic style in real-time. This task involves controlling\nthe stroke size in the stylized results, which remains an open challenge. In\nthis paper, we present a stroke controllable style transfer network that can\nachieve continuous and spatial stroke size control. By analyzing the factors\nthat influence the stroke size, we propose to explicitly account for the\nreceptive field and the style image scales. We propose a StrokePyramid module\nto endow the network with adaptive receptive fields, and two training\nstrategies to achieve faster convergence and augment new stroke sizes upon a\ntrained model respectively. By combining the proposed runtime control\nstrategies, our network can achieve continuous changes in stroke sizes and\nproduce distinct stroke sizes in different spatial regions within the same\noutput image.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 13:21:53 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 13:31:25 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 08:50:46 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 02:26:47 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Jing", "Yongcheng", ""], ["Liu", "Yang", ""], ["Yang", "Yezhou", ""], ["Feng", "Zunlei", ""], ["Yu", "Yizhou", ""], ["Tao", "Dacheng", ""], ["Song", "Mingli", ""]]}, {"id": "1802.07129", "submitter": "Il Yong Chun", "authors": "Il Yong Chun and Jeffrey A. Fessler", "title": "Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for\n  Iterative Image Recovery", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"extreme\" computational imaging that collects extremely undersampled or\nnoisy measurements, obtaining an accurate image within a reasonable computing\ntime is challenging. Incorporating image mapping convolutional neural networks\n(CNN) into iterative image recovery has great potential to resolve this issue.\nThis paper 1) incorporates image mapping CNN using identical convolutional\nkernels in both encoders and decoders into a block coordinate descent (BCD)\nsignal recovery method and 2) applies alternating direction method of\nmultipliers to train the aforementioned image mapping CNN. We refer to the\nproposed recurrent network as BCD-Net using identical encoding-decoding CNN\nstructures. Numerical experiments show that, for a) denoising low\nsignal-to-noise-ratio images and b) extremely undersampled magnetic resonance\nimaging, the proposed BCD-Net achieves significantly more accurate image\nrecovery, compared to BCD-Net using distinct encoding-decoding structures\nand/or the conventional image recovery model using both wavelets and total\nvariation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 14:37:30 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 22:16:11 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chun", "Il Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1802.07163", "submitter": "Liam Cattell", "authors": "Liam Cattell, Gustavo K. Rohde", "title": "Transport-Based Pattern Theory: A Signal Transformation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific fields imaging is used to relate a certain physical\nquantity to other dependent variables. Therefore, images can be considered as a\nmap from a real-world coordinate system to the non-negative measurements being\nacquired. In this work we describe an approach for simultaneous modeling and\ninference of such data, using the mathematics of optimal transport. To achieve\nthis, we describe a numerical implementation of the linear optimal transport\ntransform, based on the solution of the Monge-Ampere equation, which uses\nBrenier's theorem to characterize the solution of the Monge functional as the\nderivative of a convex potential function. We use our implementation of the\ntransform to compute a curl-free mapping between two images, and show that it\nis able to match images with lower error that existing methods. Moreover, we\nprovide theoretical justification for properties of the linear optimal\ntransport framework observed in the literature, including a theorem for the\nlinear separation of data classes. Finally, we use our optimal transport method\nto empirically demonstrate that the linear separability theorem holds, by\nrendering non-linearly separable data as linearly separable following transform\nto transport space.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 15:56:44 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 13:29:53 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Cattell", "Liam", ""], ["Rohde", "Gustavo K.", ""]]}, {"id": "1802.07210", "submitter": "Oscar Rahnama", "authors": "Oscar Rahnama and Duncan Frost and Ondrej Miksik and Philip H.S. Torr", "title": "Real-Time Dense Stereo Matching With ELAS on FPGA Accelerated Embedded\n  Devices", "comments": "8 pages, 7 figures, 2 tables", "journal-ref": "IEEE Robotics and Automation Letters, vol. 3, no. 3, pp.\n  2008-2015, July 2018", "doi": "10.1109/LRA.2018.2800786", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications in low-power real-time robotics, stereo cameras are the\nsensors of choice for depth perception as they are typically cheaper and more\nversatile than their active counterparts. Their biggest drawback, however, is\nthat they do not directly sense depth maps; instead, these must be estimated\nthrough data-intensive processes. Therefore, appropriate algorithm selection\nplays an important role in achieving the desired performance characteristics.\n  Motivated by applications in space and mobile robotics, we implement and\nevaluate a FPGA-accelerated adaptation of the ELAS algorithm. Despite offering\none of the best trade-offs between efficiency and accuracy, ELAS has only been\nshown to run at 1.5-3 fps on a high-end CPU. Our system preserves all\nintriguing properties of the original algorithm, such as the slanted plane\npriors, but can achieve a frame rate of 47fps whilst consuming under 4W of\npower. Unlike previous FPGA based designs, we take advantage of both components\non the CPU/FPGA System-on-Chip to showcase the strategy necessary to accelerate\nmore complex and computationally diverse algorithms for such low power,\nreal-time systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 17:24:28 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Rahnama", "Oscar", ""], ["Frost", "Duncan", ""], ["Miksik", "Ondrej", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1802.07303", "submitter": "Mengran Gou", "authors": "Mengran Gou, Fei Xiong, Octavia Camps, Mario Sznaier", "title": "MoNet: Moments Embedding Network", "comments": "Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear pooling has been recently proposed as a feature encoding layer,\nwhich can be used after the convolutional layers of a deep network, to improve\nperformance in multiple vision tasks. Different from conventional global\naverage pooling or fully connected layer, bilinear pooling gathers 2nd order\ninformation in a translation invariant fashion. However, a serious drawback of\nthis family of pooling layers is their dimensionality explosion. Approximate\npooling methods with compact properties have been explored towards resolving\nthis weakness. Additionally, recent results have shown that significant\nperformance gains can be achieved by adding 1st order information and applying\nmatrix normalization to regularize unstable higher order information. However,\ncombining compact pooling with matrix normalization and other order information\nhas not been explored until now. In this paper, we unify bilinear pooling and\nthe global Gaussian embedding layers through the empirical moment matrix. In\naddition, we propose a novel sub-matrix square-root layer, which can be used to\nnormalize the output of the convolution layer directly and mitigate the\ndimensionality problem with off-the-shelf compact pooling methods. Our\nexperiments on three widely used fine-grained classification datasets\nillustrate that our proposed architecture, MoNet, can achieve similar or better\nperformance than with the state-of-art G2DeNet. Furthermore, when combined with\ncompact pooling technique, MoNet obtains comparable performance with encoded\nfeatures with 96% less dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:54:58 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 19:34:39 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Gou", "Mengran", ""], ["Xiong", "Fei", ""], ["Camps", "Octavia", ""], ["Sznaier", "Mario", ""]]}, {"id": "1802.07351", "submitter": "Yao Lu", "authors": "Yao Lu, Jack Valmadre, Heng Wang, Juho Kannala, Mehrtash Harandi,\n  Philip H. S. Torr", "title": "Devon: Deformable Volume Network for Learning Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural network models estimate large displacement optical\nflow in multi-resolution and use warping to propagate the estimation between\ntwo resolutions. Despite their impressive results, it is known that there are\ntwo problems with the approach. First, the multi-resolution estimation of\noptical flow fails in situations where small objects move fast. Second, warping\ncreates artifacts when occlusion or dis-occlusion happens. In this paper, we\npropose a new neural network module, Deformable Cost Volume, which alleviates\nthe two problems. Based on this module, we designed the Deformable Volume\nNetwork (Devon) which can estimate multi-scale optical flow in a single high\nresolution. Experiments show Devon is more suitable in handling small objects\nmoving fast and achieves comparable results to the state-of-the-art methods in\npublic benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 21:53:19 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 07:35:11 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Lu", "Yao", ""], ["Valmadre", "Jack", ""], ["Wang", "Heng", ""], ["Kannala", "Juho", ""], ["Harandi", "Mehrtash", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1802.07412", "submitter": "He Zhang", "authors": "He Zhang, Vishal M. Patel", "title": "Density-aware Single Image De-raining using a Multi-stream Dense Network", "comments": "Accepted in CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image rain streak removal is an extremely challenging problem due to\nthe presence of non-uniform rain densities in images. We present a novel\ndensity-aware multi-stream densely connected convolutional neural network-based\nalgorithm, called DID-MDN, for joint rain density estimation and de-raining.\nThe proposed method enables the network itself to automatically determine the\nrain-density information and then efficiently remove the corresponding\nrain-streaks guided by the estimated rain-density label. To better characterize\nrain-streaks with different scales and shapes, a multi-stream densely connected\nde-raining network is proposed which efficiently leverages features from\ndifferent scales. Furthermore, a new dataset containing images with\nrain-density labels is created and used to train the proposed density-aware\nnetwork. Extensive experiments on synthetic and real datasets demonstrate that\nthe proposed method achieves significant improvements over the recent\nstate-of-the-art methods. In addition, an ablation study is performed to\ndemonstrate the improvements obtained by different modules in the proposed\nmethod. Code can be found at: https://github.com/hezhangsprinter\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 03:16:31 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Zhang", "He", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1802.07416", "submitter": "Amir Babaeian", "authors": "Amir Babaeian", "title": "Angle constrained path to cluster multiple manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to cluster multiple intersected manifolds.\nThe algorithm chooses several landmark nodes randomly and then checks whether\nthere is an angle constrained path between each landmark node and every other\nnode in the neighborhood graph. When the points lie on different manifolds with\nintersection they should not be connected using a smooth path, thus the angle\nconstraint is used to prevent connecting points from one cluster to another\none. The resulting algorithm is implemented as a simple variation of Dijkstras\nalgorithm used in Isomap. However, Isomap was specifically designed for\ndimensionality reduction in the single-manifold setting, and in particular,\ncan-not handle intersections. Our method is simpler than the previous proposals\nin the literature and performs comparably to the best methods, both on\nsimulated and some real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 03:51:48 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Babaeian", "Amir", ""]]}, {"id": "1802.07421", "submitter": "Zhilei Liu", "authors": "Zhilei Liu, Guoxian Song, Jianfei Cai, Tat-Jen Cham, Juyong Zhang", "title": "Conditional Adversarial Synthesis of 3D Facial Action Units", "comments": null, "journal-ref": "NeuroComputing 355 (2019) 200-208", "doi": "10.1016/j.neucom.2019.05.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing deep learning-based approaches for fine-grained facial expression\nanalysis, such as those involving the estimation of Action Unit (AU)\nintensities, is difficult due to the lack of a large-scale dataset of real\nfaces with sufficiently diverse AU labels for training. In this paper, we\nconsider how AU-level facial image synthesis can be used to substantially\naugment such a dataset. We propose an AU synthesis framework that combines the\nwell-known 3D Morphable Model (3DMM), which intrinsically disentangles\nexpression parameters from other face attributes, with models that\nadversarially generate 3DMM expression parameters conditioned on given target\nAU labels, in contrast to the more conventional approach of generating facial\nimages directly. In this way, we are able to synthesize new combinations of\nexpression parameters and facial images from desired AU labels. Extensive\nquantitative and qualitative results on the benchmark DISFA dataset demonstrate\nthe effectiveness of our method on 3DMM facial expression parameter synthesis\nand data augmentation for deep learning-based AU intensity estimation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 04:31:51 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 02:03:44 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Liu", "Zhilei", ""], ["Song", "Guoxian", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""], ["Zhang", "Juyong", ""]]}, {"id": "1802.07437", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do, Tuan Hoang, Dang-Khoa Le Tan, Trung Pham, Huu Le,\n  Ngai-Man Cheung, Ian Reid", "title": "Binary Constrained Deep Hashing Network for Image Retrieval without\n  Manual Annotation", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact binary codes for image retrieval task using deep neural\nnetworks has attracted increasing attention recently. However, training deep\nhashing networks for the task is challenging due to the binary constraints on\nthe hash codes, the similarity preserving property, and the requirement for a\nvast amount of labelled images. To the best of our knowledge, none of the\nexisting methods has tackled all of these challenges completely in a unified\nframework. In this work, we propose a novel end-to-end deep learning approach\nfor the task, in which the network is trained to produce binary codes directly\nfrom image pixels without the need of manual annotation. In particular, to deal\nwith the non-smoothness of binary constraints, we propose a novel pairwise\nconstrained loss function, which simultaneously encodes the distances between\npairs of hash codes, and the binary quantization error. In order to train the\nnetwork with the proposed loss function, we propose an efficient parameter\nlearning algorithm. In addition, to provide similar / dissimilar training\nimages to train the network, we exploit 3D models reconstructed from unlabelled\nimages for automatic generation of enormous training image pairs. The extensive\nexperiments on image retrieval benchmark datasets demonstrate the improvements\nof the proposed method over the state-of-the-art compact representation methods\non the image retrieval problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 06:20:59 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 10:48:39 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 02:58:19 GMT"}, {"version": "v4", "created": "Fri, 20 Jul 2018 04:15:25 GMT"}, {"version": "v5", "created": "Mon, 23 Jul 2018 14:15:44 GMT"}, {"version": "v6", "created": "Thu, 2 Aug 2018 00:07:21 GMT"}, {"version": "v7", "created": "Tue, 18 Dec 2018 00:17:41 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Hoang", "Tuan", ""], ["Tan", "Dang-Khoa Le", ""], ["Pham", "Trung", ""], ["Le", "Huu", ""], ["Cheung", "Ngai-Man", ""], ["Reid", "Ian", ""]]}, {"id": "1802.07442", "submitter": "Nick Haber", "authors": "Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins", "title": "Learning to Play with Intrinsically-Motivated Self-Aware Agents", "comments": "In NIPS 2018. 10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants are experts at playing, with an amazing ability to generate novel\nstructured behaviors in unstructured environments that lack clear extrinsic\nreward signals. We seek to mathematically formalize these abilities using a\nneural network that implements curiosity-driven intrinsic motivation. Using a\nsimple but ecologically naturalistic simulated environment in which an agent\ncan move and interact with objects it sees, we propose a \"world-model\" network\nthat learns to predict the dynamic consequences of the agent's actions.\nSimultaneously, we train a separate explicit \"self-model\" that allows the agent\nto track the error map of its own world-model, and then uses the self-model to\nadversarially challenge the developing world-model. We demonstrate that this\npolicy causes the agent to explore novel and informative interactions with its\nenvironment, leading to the generation of a spectrum of complex behaviors,\nincluding ego-motion prediction, object attention, and object gathering.\nMoreover, the world-model that the agent learns supports improved performance\non object dynamics prediction, detection, localization and recognition tasks.\nTaken together, our results are initial steps toward creating flexible\nautonomous agents that self-supervise in complex novel physical environments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:01:43 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 20:08:46 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Haber", "Nick", ""], ["Mrowca", "Damian", ""], ["Fei-Fei", "Li", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1802.07447", "submitter": "Jie Cao", "authors": "Jie Cao, Yibo Hu, Bing Yu, Ran He, Zhenan Sun", "title": "Load Balanced GANs for Multi-view Face Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view face synthesis from a single image is an ill-posed problem and\noften suffers from serious appearance distortion. Producing photo-realistic and\nidentity preserving multi-view results is still a not well defined synthesis\nproblem. This paper proposes Load Balanced Generative Adversarial Networks\n(LB-GAN) to precisely rotate the yaw angle of an input face image to any\nspecified angle. LB-GAN decomposes the challenging synthesis problem into two\nwell constrained subtasks that correspond to a face normalizer and a face\neditor respectively. The normalizer first frontalizes an input image, and then\nthe editor rotates the frontalized image to a desired pose guided by a remote\ncode. In order to generate photo-realistic local details, the normalizer and\nthe editor are trained in a two-stage manner and regulated by a conditional\nself-cycle loss and an attention based L2 loss. Exhaustive experiments on\ncontrolled and uncontrolled environments demonstrate that the proposed method\nnot only improves the visual realism of multi-view synthetic images, but also\npreserves identity information well.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:10:36 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 05:02:30 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Cao", "Jie", ""], ["Hu", "Yibo", ""], ["Yu", "Bing", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1802.07452", "submitter": "Xueqing Deng", "authors": "Xueqing Deng, Yi Zhu and Shawn Newsam", "title": "Spatial Morphing Kernel Regression For Feature Interpolation", "comments": "accepted by ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, geotagged social media has become popular as a novel source\nfor geographic knowledge discovery. Ground-level images and videos provide a\ndifferent perspective than overhead imagery and can be applied to a range of\napplications such as land use mapping, activity detection, pollution mapping,\netc. The sparse and uneven distribution of this data presents a problem,\nhowever, for generating dense maps. We therefore investigate the problem of\nspatially interpolating the high-dimensional features extracted from sparse\nsocial media to enable dense labeling using standard classifiers. Further, we\nshow how prior knowledge about region boundaries can be used to improve the\ninterpolation through spatial morphing kernel regression. We show that an\ninterpolate-then-classify framework can produce dense maps from sparse\nobservations but that care must be taken in choosing the interpolation method.\nWe also show that the spatial morphing kernel improves the results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:30:51 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 17:05:23 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Deng", "Xueqing", ""], ["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1802.07460", "submitter": "Mei-Chen Yeh", "authors": "Yi-Nan Li and Mei-Chen Yeh", "title": "Learning Image Conditioned Label Space for Multilabel Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the task of multilabel image classification. Inspired by\nthe great success from deep convolutional neural networks (CNNs) for\nsingle-label visual-semantic embedding, we exploit extending these models for\nmultilabel images. Specifically, we propose an image-dependent ranking model,\nwhich returns a ranked list of labels according to its relevance to the input\nimage. In contrast to conventional CNN models that learn an image\nrepresentation (i.e. the image embedding vector), the developed model learns a\nmapping (i.e. a transformation matrix) from an image in an attempt to\ndifferentiate between its relevant and irrelevant labels. Despite the\nconceptual simplicity of our approach, experimental results on a public\nbenchmark dataset demonstrate that the proposed model achieves state-of-the-art\nperformance while using fewer training images than other multilabel\nclassification methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 08:12:23 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Yi-Nan", ""], ["Yeh", "Mei-Chen", ""]]}, {"id": "1802.07461", "submitter": "Nick Haber", "authors": "Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins", "title": "Emergence of Structured Behaviors from Curiosity-Based Intrinsic\n  Motivation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants are experts at playing, with an amazing ability to generate novel\nstructured behaviors in unstructured environments that lack clear extrinsic\nreward signals. We seek to replicate some of these abilities with a neural\nnetwork that implements curiosity-driven intrinsic motivation. Using a simple\nbut ecologically naturalistic simulated environment in which the agent can move\nand interact with objects it sees, the agent learns a world model predicting\nthe dynamic consequences of its actions. Simultaneously, the agent learns to\ntake actions that adversarially challenge the developing world model, pushing\nthe agent to explore novel and informative interactions with its environment.\nWe demonstrate that this policy leads to the self-supervised emergence of a\nspectrum of complex behaviors, including ego motion prediction, object\nattention, and object gathering. Moreover, the world model that the agent\nlearns supports improved performance on object dynamics prediction and\nlocalization tasks. Our results are a proof-of-principle that computational\nmodels of intrinsic motivation might account for key features of developmental\nvisuomotor learning in infants.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 08:13:12 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Haber", "Nick", ""], ["Mrowca", "Damian", ""], ["Fei-Fei", "Li", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1802.07465", "submitter": "Alexandre Cunha", "authors": "Fidel A. Guerrero-Pena, Pedro D. Marrero Fernandez, Tsang Ing Ren,\n  Mary Yui, Ellen Rothenberg, Alexandre Cunha", "title": "Multiclass Weighted Loss for Instance Segmentation of Cluttered Cells", "comments": "Submitted to IEEE ICIP 2018", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451187", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multiclass weighted loss function for instance segmentation\nof cluttered cells. We are primarily motivated by the need of developmental\nbiologists to quantify and model the behavior of blood T-cells which might help\nus in understanding their regulation mechanisms and ultimately help researchers\nin their quest for developing an effective immuno-therapy cancer treatment.\nSegmenting individual touching cells in cluttered regions is challenging as the\nfeature distribution on shared borders and cell foreground are similar thus\ndifficulting discriminating pixels into proper classes. We present two novel\nweight maps applied to the weighted cross entropy loss function which take into\naccount both class imbalance and cell geometry. Binary ground truth training\ndata is augmented so the learning model can handle not only foreground and\nbackground but also a third touching class. This framework allows training\nusing U-Net. Experiments with our formulations have shown superior results when\ncompared to other similar schemes, outperforming binary class models with\nsignificant improvement of boundary adequacy and instance detection. We\nvalidate our results on manually annotated microscope images of T-cells.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 08:41:48 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Guerrero-Pena", "Fidel A.", ""], ["Fernandez", "Pedro D. Marrero", ""], ["Ren", "Tsang Ing", ""], ["Yui", "Mary", ""], ["Rothenberg", "Ellen", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1802.07490", "submitter": "Shan Luo Dr", "authors": "Shan Luo, Wenzhen Yuan, Edward Adelson, Anthony G. Cohn and Raul\n  Fuentes", "title": "ViTac: Feature Sharing between Vision and Tactile Sensing for Cloth\n  Texture Recognition", "comments": "6 pages, 5 figures, Accepted for 2018 IEEE International Conference\n  on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and touch are two of the important sensing modalities for humans and\nthey offer complementary information for sensing the environment. Robots could\nalso benefit from such multi-modal sensing ability. In this paper, addressing\nfor the first time (to the best of our knowledge) texture recognition from\ntactile images and vision, we propose a new fusion method named Deep Maximum\nCovariance Analysis (DMCA) to learn a joint latent space for sharing features\nthrough vision and tactile sensing. The features of camera images and tactile\ndata acquired from a GelSight sensor are learned by deep neural networks. But\nthe learned features are of a high dimensionality and are redundant due to the\ndifferences between the two sensing modalities, which deteriorates the\nperception performance. To address this, the learned features are paired using\nmaximum covariance analysis. Results of the algorithm on a newly collected\ndataset of paired visual and tactile data relating to cloth textures show that\na good recognition performance of greater than 90\\% can be achieved by using\nthe proposed DMCA framework. In addition, we find that the perception\nperformance of either vision or tactile sensing can be improved by employing\nthe shared representation space, compared to learning from unimodal data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 10:06:14 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 14:57:30 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Luo", "Shan", ""], ["Yuan", "Wenzhen", ""], ["Adelson", "Edward", ""], ["Cohn", "Anthony G.", ""], ["Fuentes", "Raul", ""]]}, {"id": "1802.07512", "submitter": "Ligang Zhang", "authors": "Ligang Zhang, Brijesh Verma, David Stockwell, Sujan Chowdhury", "title": "Density Weighted Connectivity of Grass Pixels in Image Frames for\n  Biomass Estimation", "comments": "28 pages, accepted manuscript, Expert Systems with Applications", "journal-ref": null, "doi": "10.1016/j.eswa.2018.01.055", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the biomass of roadside grasses plays a significant\nrole in applications such as fire-prone region identification. Current\nsolutions heavily depend on field surveys, remote sensing measurements and\nimage processing using reference markers, which often demand big investments of\ntime, effort and cost. This paper proposes Density Weighted Connectivity of\nGrass Pixels (DWCGP) to automatically estimate grass biomass from roadside\nimage data. The DWCGP calculates the length of continuously connected grass\npixels along a vertical orientation in each image column, and then weights the\nlength by the grass density in a surrounding region of the column. Grass pixels\nare classified using feedforward artificial neural networks and the dominant\ntexture orientation at every pixel is computed using multi-orientation Gabor\nwavelet filter vote. Evaluations on a field survey dataset show that the DWCGP\nreduces Root-Mean-Square Error from 5.84 to 5.52 by additionally considering\ngrass density on top of grass height. The DWCGP shows robustness to\nnon-vertical grass stems and to changes of both Gabor filter parameters and\nsurrounding region widths. It also has performance close to human observation\nand higher than eight baseline approaches, as well as promising results for\nclassifying low vs. high fire risk and identifying fire-prone road regions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 11:07:05 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Zhang", "Ligang", ""], ["Verma", "Brijesh", ""], ["Stockwell", "David", ""], ["Chowdhury", "Sujan", ""]]}, {"id": "1802.07584", "submitter": "Biyi Fang", "authors": "Biyi Fang, Jillian Co, Mi Zhang", "title": "DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level\n  Sign Language Translation", "comments": null, "journal-ref": null, "doi": "10.1145/3131672.3131693", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an undeniable communication barrier between deaf people and people\nwith normal hearing ability. Although innovations in sign language translation\ntechnology aim to tear down this communication barrier, the majority of\nexisting sign language translation systems are either intrusive or constrained\nby resolution or ambient lighting conditions. Moreover, these existing systems\ncan only perform single-sign ASL translation rather than sentence-level\ntranslation, making them much less useful in daily-life communication\nscenarios. In this work, we fill this critical gap by presenting DeepASL, a\ntransformative deep learning-based sign language translation technology that\nenables ubiquitous and non-intrusive American Sign Language (ASL) translation\nat both word and sentence levels. DeepASL uses infrared light as its sensing\nmechanism to non-intrusively capture the ASL signs. It incorporates a novel\nhierarchical bidirectional deep recurrent neural network (HB-RNN) and a\nprobabilistic framework based on Connectionist Temporal Classification (CTC)\nfor word-level and sentence-level ASL translation respectively. To evaluate its\nperformance, we have collected 7,306 samples from 11 participants, covering 56\ncommonly used ASL words and 100 ASL sentences. DeepASL achieves an average\n94.5% word-level translation accuracy and an average 8.2% word error rate on\ntranslating unseen ASL sentences. Given its promising performance, we believe\nDeepASL represents a significant step towards breaking the communication\nbarrier between deaf people and hearing majority, and thus has the significant\npotential to fundamentally change deaf people's lives.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:29:36 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 20:54:56 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 00:21:43 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Fang", "Biyi", ""], ["Co", "Jillian", ""], ["Zhang", "Mi", ""]]}, {"id": "1802.07589", "submitter": "Shaoning Zeng", "authors": "Shaoning Zeng, Bob Zhang, Yanghao Zhang and Jianping Gou", "title": "Collaboratively Weighting Deep and Classic Representation via L2\n  Regularization for Image Classification", "comments": "ACML 2018", "journal-ref": "PMLR Volume 95: Asian Conference on Machine Learning, 14-16\n  November 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks provide a powerful feature learning\ncapability for image classification. The deep image features can be utilized to\ndeal with many image understanding tasks like image classification and object\nrecognition. However, the robustness obtained in one dataset can be hardly\nreproduced in the other domain, which leads to inefficient models far from\nstate-of-the-art. We propose a deep collaborative weight-based classification\n(DeepCWC) method to resolve this problem, by providing a novel option to fully\ntake advantage of deep features in classic machine learning. It firstly\nperforms the L2-norm based collaborative representation on the original images,\nas well as the deep features extracted by deep CNN models. Then, two distance\nvectors, obtained based on the pair of linear representations, are fused\ntogether via a novel collaborative weight. This collaborative weight enables\ndeep and classic representations to weigh each other. We observed the\ncomplementarity between two representations in a series of experiments on 10\nfacial and object datasets. The proposed DeepCWC produces very promising\nclassification results, and outperforms many other benchmark methods,\nespecially the ones claimed for Fashion-MNIST. The code is going to be\npublished in our public repository.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:46:48 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 10:18:00 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zeng", "Shaoning", ""], ["Zhang", "Bob", ""], ["Zhang", "Yanghao", ""], ["Gou", "Jianping", ""]]}, {"id": "1802.07590", "submitter": "Mohamed Hajaj PhD", "authors": "Mohamed Hajaj, Duncan Gillies", "title": "Batch Normalization and the impact of batch structure on the behavior of\n  deep convolution networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Batch normalization was introduced in 2015 to speed up training of deep\nconvolution networks by normalizing the activations across the current batch to\nhave zero mean and unity variance. The results presented here show an\ninteresting aspect of batch normalization, where controlling the shape of the\ntraining batches can influence what the network will learn. If training batches\nare structured as balanced batches (one image per class), and inference is also\ncarried out on balanced test batches, using the batch's own means and\nvariances, then the conditional results will improve considerably. The network\nuses the strong information about easy images in a balanced batch, and\npropagates it through the shared means and variances to help decide the\nidentity of harder images on the same batch. Balancing the test batches\nrequires the labels of the test images, which are not available in practice,\nhowever further investigation can be done using batch structures that are less\nstrict and might not require the test image labels. The conditional results\nshow the error rate almost reduced to zero for nontrivial datasets with small\nnumber of classes such as the CIFAR10.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:47:18 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Hajaj", "Mohamed", ""], ["Gillies", "Duncan", ""]]}, {"id": "1802.07591", "submitter": "Vaclav Skala", "authors": "Vaclav Skala", "title": "Least Square Error Method Robustness of Computation: What is not usually\n  considered and taught", "comments": null, "journal-ref": null, "doi": "10.15439/978-83-946253-7-5", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many practical applications based on the Least Square Error (LSE)\napproximation. It is based on a square error minimization 'on a vertical' axis.\nThe LSE method is simple and easy also for analytical purposes. However, if\ndata span is large over several magnitudes or non-linear LSE is used, severe\nnumerical instability can be expected. The presented contribution describes a\nsimple method for large span of data LSE computation. It is especially\nconvenient if large span of data are to be processed, when the 'standard'\npseudoinverse matrix is ill conditioned. It is actually based on a LSE solution\nusing orthogonal basis vectors instead of orthonormal basis vectors. The\npresented approach has been used for a linear regression as well as for\napproximation using radial basis functions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 13:55:09 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Skala", "Vaclav", ""]]}, {"id": "1802.07623", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting,\n  Karthikeyan Shanmugam and Payel Das", "title": "Explanations based on the Missing: Towards Contrastive Explanations with\n  Pertinent Negatives", "comments": null, "journal-ref": null, "doi": null, "report-no": "accepted to NIPS 2018", "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel method that provides contrastive\nexplanations justifying the classification of an input by a black box\nclassifier such as a deep neural network. Given an input we find what should be\n%necessarily and minimally and sufficiently present (viz. important object\npixels in an image) to justify its classification and analogously what should\nbe minimally and necessarily \\emph{absent} (viz. certain background pixels). We\nargue that such explanations are natural for humans and are used commonly in\ndomains such as health care and criminology. What is minimally but critically\n\\emph{absent} is an important part of an explanation, which to the best of our\nknowledge, has not been explicitly identified by current explanation methods\nthat explain predictions of neural networks. We validate our approach on three\nreal datasets obtained from diverse domains; namely, a handwritten digits\ndataset MNIST, a large procurement fraud dataset and a brain activity strength\ndataset. In all three cases, we witness the power of our approach in generating\nprecise explanations that are also easy for human experts to understand and\nevaluate.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 15:51:38 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 16:08:36 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Chen", "Pin-Yu", ""], ["Luss", "Ronny", ""], ["Tu", "Chun-Chen", ""], ["Ting", "Paishun", ""], ["Shanmugam", "Karthikeyan", ""], ["Das", "Payel", ""]]}, {"id": "1802.07648", "submitter": "Nicolas Gillis", "authors": "Maryam Abdolali, Nicolas Gillis, Mohammad Rahmati", "title": "Scalable and Robust Sparse Subspace Clustering Using Randomized\n  Clustering and Multilayer Graphs", "comments": "25 pages, v2: typos corrected", "journal-ref": "Signal Processing 163, pp. 166-180, 2019", "doi": "10.1016/j.sigpro.2019.05.017", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC) is one of the current state-of-the-art\nmethods for partitioning data points into the union of subspaces, with strong\ntheoretical guarantees. However, it is not practical for large data sets as it\nrequires solving a LASSO problem for each data point, where the number of\nvariables in each LASSO problem is the number of data points. To improve the\nscalability of SSC, we propose to select a few sets of anchor points using a\nrandomized hierarchical clustering method, and, for each set of anchor points,\nsolve the LASSO problems for each data point allowing only anchor points to\nhave a non-zero weight (this reduces drastically the number of variables). This\ngenerates a multilayer graph where each layer corresponds to a different set of\nanchor points. Using the Grassmann manifold of orthogonal matrices, the shared\nconnectivity among the layers is summarized within a single subspace. Finally,\nwe use $k$-means clustering within that subspace to cluster the data points,\nsimilarly as done by spectral clustering in SSC. We show on both synthetic and\nreal-world data sets that the proposed method not only allows SSC to scale to\nlarge-scale data sets, but that it is also much more robust as it performs\nsignificantly better on noisy data and on data with close susbspaces and\noutliers, while it is not prone to oversegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 16:21:42 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 06:59:12 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Abdolali", "Maryam", ""], ["Gillis", "Nicolas", ""], ["Rahmati", "Mohammad", ""]]}, {"id": "1802.07653", "submitter": "Babajide Ayinde", "authors": "Babajide O. Ayinde and Jacek M. Zurada", "title": "Building Efficient ConvNets using Redundant Feature Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient technique to prune deep and/or wide\nconvolutional neural network models by eliminating redundant features (or\nfilters). Previous studies have shown that over-sized deep neural network\nmodels tend to produce a lot of redundant features that are either shifted\nversion of one another or are very similar and show little or no variations;\nthus resulting in filtering redundancy. We propose to prune these redundant\nfeatures along with their connecting feature maps according to their\ndifferentiation and based on their relative cosine distances in the feature\nspace, thus yielding smaller network size with reduced inference costs and\ncompetitive performance. We empirically show on select models and CIFAR-10\ndataset that inference costs can be reduced by 40% for VGG-16, 27% for\nResNet-56, and 39% for ResNet-110.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 16:31:28 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Ayinde", "Babajide O.", ""], ["Zurada", "Jacek M.", ""]]}, {"id": "1802.07672", "submitter": "Mohamed Hajaj PhD", "authors": "Mohamed Hajaj, Duncan Gillies", "title": "Learning Multiple Categories on Deep Convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolution networks have proved very successful with big datasets such\nas the 1000-classes ImageNet. Results show that the error rate increases slowly\nas the size of the dataset increases. Experiments presented here may explain\nwhy these networks are very effective in solving big recognition problems. If\nthe big task is made up of multiple smaller tasks, then the results show the\nability of deep convolution networks to decompose the complex task into a\nnumber of smaller tasks and to learn them simultaneously. The results show that\nthe performance of solving the big task on a single network is very close to\nthe average performance of solving each of the smaller tasks on a separate\nnetwork. Experiments also show the advantage of using task specific or category\nlabels in combination with class labels.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:11:31 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Hajaj", "Mohamed", ""], ["Gillies", "Duncan", ""]]}, {"id": "1802.07687", "submitter": "Emily Denton", "authors": "Emily Denton and Rob Fergus", "title": "Stochastic Video Generation with a Learned Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating video frames that accurately predict future world states is\nchallenging. Existing approaches either fail to capture the full distribution\nof outcomes, or yield blurry generations, or both. In this paper we introduce\nan unsupervised video generation model that learns a prior model of uncertainty\nin a given environment. Video frames are generated by drawing samples from this\nprior and combining them with a deterministic estimate of the future frame. The\napproach is simple and easily trained end-to-end on a variety of datasets.\nSample generations are both varied and sharp, even many frames into the future,\nand compare favorably to those from existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:36:27 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 17:39:23 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Denton", "Emily", ""], ["Fergus", "Rob", ""]]}, {"id": "1802.07769", "submitter": "Shadrokh Samavi", "authors": "Mahdi Ahmadi, Ali Emami, Mohsen Hajabdollahi, S.M.Reza Soroushmehr,\n  Nader Karimi, Shadrokh Samavi, Kayvan Najarian", "title": "Lossless Compression of Angiogram Foreground with Visual Quality\n  Preservation of Background", "comments": "4 pages , 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By increasing the volume of telemedicine information, the need for medical\nimage compression has become more important. In angiographic images, a small\nratio of the entire image usually belongs to the vasculature that provides\ncrucial information for diagnosis. Other parts of the image are diagnostically\nless important and can be compressed with higher compression ratio. However,\nthe quality of those parts affect the visual perception of the image as well.\nExisting methods compress foreground and background of angiographic images\nusing different techniques. In this paper we first utilize convolutional neural\nnetwork to segment vessels and then represent a hierarchical block processing\nalgorithm capable of both eliminating the background redundancies and\npreserving the overall visual quality of angiograms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:42:50 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Ahmadi", "Mahdi", ""], ["Emami", "Ali", ""], ["Hajabdollahi", "Mohsen", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07770", "submitter": "Joao Batista Monteiro Filho", "authors": "Jo\\~ao Monteiro, Isabela Albuquerque, Zahid Akhtar, Tiago H. Falk", "title": "Generalizable Adversarial Examples Detection Based on Bi-model Decision\n  Mismatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications of artificial neural networks have yielded remarkable\nperformance gains in a wide range of tasks. However, recent studies have\ndiscovered that such modelling strategy is vulnerable to Adversarial Examples,\ni.e. examples with subtle perturbations often too small and imperceptible to\nhumans, but that can easily fool neural networks. Defense techniques against\nadversarial examples have been proposed, but ensuring robust performance\nagainst varying or novel types of attacks remains an open problem. In this\nwork, we focus on the detection setting, in which case attackers become\nidentifiable while models remain vulnerable. Particularly, we employ the\ndecision layer of independently trained models as features for posterior\ndetection. The proposed framework does not require any prior knowledge of\nadversarial examples generation techniques, and can be directly employed along\nwith unmodified off-the-shelf models. Experiments on the standard MNIST and\nCIFAR10 datasets deliver empirical evidence that such detection approach\ngeneralizes well across not only different adversarial examples generation\nmethods but also quality degradation attacks. Non-linear binary classifiers\ntrained on top of our proposed features can achieve a high detection rate\n(>90%) in a set of white-box attacks and maintain such performance when tested\nagainst unseen attacks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:43:08 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 18:42:02 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 20:29:57 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Monteiro", "Jo\u00e3o", ""], ["Albuquerque", "Isabela", ""], ["Akhtar", "Zahid", ""], ["Falk", "Tiago H.", ""]]}, {"id": "1802.07778", "submitter": "Shadrokh Samavi", "authors": "Mina Nasr-Esfahani, Majid Mohrekesh, Mojtaba Akbari, S.M.Reza\n  Soroushmehr, Ebrahim Nasr-Esfahani, Nader Karimi, Shadrokh Samavi, Kayvan\n  Najarian", "title": "Left Ventricle Segmentation in Cardiac MR Images Using Fully\n  Convolutional Network", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis, especially segmenting a specific organ, has an\nimportant role in developing clinical decision support systems. In cardiac\nmagnetic resonance (MR) imaging, segmenting the left and right ventricles helps\nphysicians diagnose different heart abnormalities. There are challenges for\nthis task, including the intensity and shape similarity between left ventricle\nand other organs, inaccurate boundaries and presence of noise in most of the\nimages. In this paper we propose an automated method for segmenting the left\nventricle in cardiac MR images. We first automatically extract the region of\ninterest, and then employ it as an input of a fully convolutional network. We\ntrain the network accurately despite the small number of left ventricle pixels\nin comparison with the whole image. Thresholding on the output map of the fully\nconvolutional network and selection of regions based on their roundness are\nperformed in our proposed post-processing phase. The Dice score of our method\nreaches 87.24% by applying this algorithm on the York dataset of heart images.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:01:35 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Nasr-Esfahani", "Mina", ""], ["Mohrekesh", "Majid", ""], ["Akbari", "Mojtaba", ""], ["Soroushmehr", "S. M. Reza", ""], ["Nasr-Esfahani", "Ebrahim", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07781", "submitter": "Shadrokh Samavi", "authors": "Atefe Rajaeefar, Ali Emami, S.M.Reza Soroushmehr, Nader Karimi,\n  Shadrokh Samavi, Kayvan Najarian", "title": "Lossless Image Compression Algorithm for Wireless Capsule Endoscopy by\n  Content-Based Classification of Image Blocks", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in capsule endoscopy systems have introduced new methods and\ncapabilities. The capsule endoscopy system, by observing the entire digestive\ntract, has significantly improved diagnosing gastrointestinal disorders and\ndiseases. The system has challenges such as the need to enhance the quality of\nthe transmitted images, low frame rates of transmission, and battery lifetime\nthat need to be addressed. One of the important parts of a capsule endoscopy\nsystem is the image compression unit. Better compression of images increases\nthe frame rate and hence improves the diagnosis process. In this paper a high\nprecision compression algorithm with high compression ratio is proposed. In\nthis algorithm we use the similarity between frames to compress the data more\nefficiently.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:09:46 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Rajaeefar", "Atefe", ""], ["Emami", "Ali", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07786", "submitter": "Shadrokh Samavi", "authors": "Hamidreza Zarrabi, Mohsen Hajabdollahi, S.M.Reza Soroushmehr, Nader\n  Karimi, Shadrokh Samavi, Kayvan Najarian", "title": "Reversible Image Watermarking for Health Informatics Systems Using\n  Distortion Compensation in Wavelet Domain", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible image watermarking guaranties restoration of both original cover\nand watermark logo from the watermarked image. Capacity and distortion of the\nimage under reversible watermarking are two important parameters. In this study\na reversible watermarking is investigated with focusing on increasing the\nembedding capacity and reducing the distortion in medical images. Integer\nwavelet transform is used for embedding where in each iteration, one watermark\nbit is embedded in one transform coefficient. We devise a novel approach that\nwhen a coefficient is modified in an iteration, the produced distortion is\ncompensated in the next iteration. This distortion compensation method would\nresult in low distortion rate. The proposed method is tested on four types of\nmedical images including MRI of brain, cardiac MRI, MRI of breast, and\nintestinal polyp images. Using a one-level wavelet transform, maximum capacity\nof 1.5 BPP is obtained. Experimental results demonstrate that the proposed\nmethod is superior to the state-of-the-art works in terms of capacity and\ndistortion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:17:39 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Zarrabi", "Hamidreza", ""], ["Hajabdollahi", "Mohsen", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07788", "submitter": "Shadrokh Samavi", "authors": "Mohsen Hajabdollahi, Reza Esfandiarpoor, S.M.Reza Soroushmehr, Nader\n  Karimi, Shadrokh Samavi, Kayvan Najarian", "title": "Segmentation of Bleeding Regions in Wireless Capsule Endoscopy Images an\n  Approach for inside Capsule Video Summarization", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless capsule endoscopy (WCE) is an effective means of diagnosis of\ngastrointestinal disorders. Detection of informative scenes by WCE could reduce\nthe length of transmitted videos and can help with the diagnosis. In this paper\nwe propose a simple and efficient method for segmentation of the bleeding\nregions in WCE captured images. Suitable color channels are selected and\nclassified by a multi-layer perceptron (MLP) structure. The MLP structure is\nquantized such that the implementation does not require multiplications. The\nproposed method is tested by simulation on WCE bleeding image dataset. The\nproposed structure is designed considering hardware resource constrains that\nexist in WCE systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:27:21 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Hajabdollahi", "Mohsen", ""], ["Esfandiarpoor", "Reza", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07789", "submitter": "Philipe Ambrozio Dias", "authors": "Philipe A. Dias and Henry Medeiros", "title": "Semantic Segmentation Refinement by Monte Carlo Region Growing of High\n  Confidence Detections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent improvements using fully convolutional networks, in general,\nthe segmentation produced by most state-of-the-art semantic segmentation\nmethods does not show satisfactory adherence to the object boundaries. We\npropose a method to refine the segmentation results generated by such deep\nlearning models. Our method takes as input the confidence scores generated by a\npixel-dense segmentation network and re-labels pixels with low confidence\nlevels. The re-labeling approach employs a region growing mechanism that\naggregates these pixels to neighboring areas with high confidence scores and\nsimilar appearance. In order to correct the labels of pixels that were\nincorrectly classified with high confidence level by the semantic segmentation\nalgorithm, we generate multiple region growing steps through a Monte Carlo\nsampling of the seeds of the regions. Our method improves the accuracy of a\nstate-of-the-art fully convolutional semantic segmentation approach on the\npublicly available COCO and PASCAL datasets, and it shows significantly better\nresults on selected sequences of the finely-annotated DAVIS dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:29:12 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Dias", "Philipe A.", ""], ["Medeiros", "Henry", ""]]}, {"id": "1802.07794", "submitter": "Shadrokh Samavi", "authors": "Shima Rafiei, Nader Karimi, Behzad Mirmahboub, S.M. Reza Soroushmehr,\n  Banafsheh Felfelian, Shadrokh Samavi, Kayvan Najarian", "title": "Liver Segmentation in Abdominal CT Images by Adaptive 3D Region Growing", "comments": "Table 1 of the paper contains comparisons and results that are not\n  correct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic liver segmentation plays an important role in computer-aided\ndiagnosis and treatment. Manual segmentation of organs is a difficult and\ntedious task and so prone to human errors. In this paper, we propose an\nadaptive 3D region growing with subject-specific conditions. For this aim we\nuse the intensity distribution of most probable voxels in prior map along with\nlocation prior. We also incorporate the boundary of target organs to restrict\nthe region growing. In order to obtain strong edges and high contrast, we\npropose an effective contrast enhancement algorithm to facilitate more accurate\nsegmentation. In this paper, 92.56% Dice score is achieved. We compare our\nmethod with the method of hard thresholding on Deeds prior map and also with\nthe majority voting on Deeds registration with 13 organs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:35:18 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 18:32:18 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Rafiei", "Shima", ""], ["Karimi", "Nader", ""], ["Mirmahboub", "Behzad", ""], ["Soroushmehr", "S. M. Reza", ""], ["Felfelian", "Banafsheh", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07796", "submitter": "D. Khu\\^e L\\^e-Huu", "authors": "D. Khu\\^e L\\^e-Huu and Nikos Paragios", "title": "Continuous Relaxation of MAP Inference: A Nonconvex Perspective", "comments": "Accepted for publication at the 2018 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a nonconvex continuous relaxation of MAP inference in\ndiscrete Markov random fields (MRFs). We show that for arbitrary MRFs, this\nrelaxation is tight, and a discrete stationary point of it can be easily\nreached by a simple block coordinate descent algorithm. In addition, we study\nthe resolution of this relaxation using popular gradient methods, and further\npropose a more effective solution using a multilinear decomposition framework\nbased on the alternating direction method of multipliers (ADMM). Experiments on\nmany real-world problems demonstrate that the proposed ADMM significantly\noutperforms other nonconvex relaxation based methods, and compares favorably\nwith state of the art MRF optimization algorithms in different settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:42:58 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 22:12:55 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["L\u00ea-Huu", "D. Khu\u00ea", ""], ["Paragios", "Nikos", ""]]}, {"id": "1802.07800", "submitter": "Shadrokh Samavi", "authors": "Shima Rafiei, Ebrahim Nasr-Esfahani, S.M.Reza Soroushmehr, Nader\n  Karimi, Shadrokh Samavi, Kayvan Najarian", "title": "Liver segmentation in CT images using three dimensional to two\n  dimensional fully convolutional network", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for CT scan analysis is growing for pre-diagnosis and therapy of\nabdominal organs. Automatic organ segmentation of abdominal CT scan can help\nradiologists analyze the scans faster and segment organ images with fewer\nerrors. However, existing methods are not efficient enough to perform the\nsegmentation process for victims of accidents and emergencies situations. In\nthis paper we propose an efficient liver segmentation with our 3D to 2D fully\nconnected network (3D-2D-FCN). The segmented mask is enhanced by means of\nconditional random field on the organ's border. Consequently, we segment a\ntarget liver in less than a minute with Dice score of 93.52.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 20:50:48 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 15:55:53 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Rafiei", "Shima", ""], ["Nasr-Esfahani", "Ebrahim", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.07804", "submitter": "Shadrokh Samavi", "authors": "M. Hajabdollahi, R. Esfandiarpoor, S.M.R. Soroushmehr, N. Karimi, S.\n  Samavi, K. Najarian", "title": "Low complexity convolutional neural network for vessel segmentation in\n  portable retinal diagnostic devices", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel information is helpful in retinal disease screening and\ndiagnosis. Retinal vessel segmentation provides useful information about\nvessels and can be used by physicians during intraocular surgery and retinal\ndiagnostic operations. Convolutional neural networks (CNNs) are powerful tools\nfor classification and segmentation of medical images. Complexity of CNNs makes\nit difficult to implement them in portable devices such as binocular indirect\nophthalmoscopes. In this paper a simplification approach is proposed for CNNs\nbased on combination of quantization and pruning. Fully connected layers are\nquantized and convolutional layers are pruned to have a simple and efficient\nnetwork structure. Experiments on images of the STARE dataset show that our\nsimplified network is able to segment retinal vessels with acceptable accuracy\nand low complexity.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 21:01:22 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Hajabdollahi", "M.", ""], ["Esfandiarpoor", "R.", ""], ["Soroushmehr", "S. M. R.", ""], ["Karimi", "N.", ""], ["Samavi", "S.", ""], ["Najarian", "K.", ""]]}, {"id": "1802.07845", "submitter": "Zhenhua Chen", "authors": "Zhenhua Chen, David Crandall, Robert Templeman", "title": "Detecting Small, Densely Distributed Objects with Filter-Amplifier\n  Networks and Loss Boosting", "comments": "rejected by a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting small, densely distributed objects is a significant challenge:\nsmall objects often contain less distinctive information compared to larger\nones, and finer-grained precision of bounding box boundaries are required. In\nthis paper, we propose two techniques for addressing this problem. First, we\nestimate the likelihood that each pixel belongs to an object boundary rather\nthan predicting coordinates of bounding boxes (as YOLO, Faster-RCNN and SSD\ndo), by proposing a new architecture called Filter-Amplifier Networks (FANs).\nSecond, we introduce a technique called Loss Boosting (LB) which attempts to\nsoften the loss imbalance problem on each image. We test our algorithm on the\nproblem of detecting electrical components on a new, realistic, diverse dataset\nof printed circuit boards (PCBs), as well as the problem of detecting vehicles\nin the Vehicle Detection in Aerial Imagery (VEDAI) dataset.\n  Experiments show that our method works significantly better than current\nstate-of-the-art algorithms with respect to accuracy, recall and average IoU.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 23:17:36 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 06:17:11 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chen", "Zhenhua", ""], ["Crandall", "David", ""], ["Templeman", "Robert", ""]]}, {"id": "1802.07846", "submitter": "Avi Ben-Cohen", "authors": "Avi Ben-Cohen, Eyal Klang, Stephen P. Raskin, Shelly Soffer, Simona\n  Ben-Haim, Eli Konen, Michal Marianne Amitai and Hayit Greenspan", "title": "Cross-Modality Synthesis from CT to PET using FCN and GAN Networks for\n  Improved Automated Lesion Detection", "comments": "Preprint submitted to Engineering applications of artificial\n  intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel system for generation of virtual PET images\nusing CT scans. We combine a fully convolutional network (FCN) with a\nconditional generative adversarial network (GAN) to generate simulated PET data\nfrom given input CT data. The synthesized PET can be used for false-positive\nreduction in lesion detection solutions. Clinically, such solutions may enable\nlesion detection and drug treatment evaluation in a CT-only environment, thus\nreducing the need for the more expensive and radioactive PET/CT scan. Our\ndataset includes 60 PET/CT scans from Sheba Medical center. We used 23 scans\nfor training and 37 for testing. Different schemes to achieve the synthesized\noutput were qualitatively compared. Quantitative evaluation was conducted using\nan existing lesion detection software, combining the synthesized PET as a false\npositive reduction layer for the detection of malignant lesions in the liver.\nCurrent results look promising showing a 28% reduction in the average false\npositive per case from 2.9 to 2.1. The suggested solution is comprehensive and\ncan be expanded to additional body organs, and different modalities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 23:25:19 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 09:14:59 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ben-Cohen", "Avi", ""], ["Klang", "Eyal", ""], ["Raskin", "Stephen P.", ""], ["Soffer", "Shelly", ""], ["Ben-Haim", "Simona", ""], ["Konen", "Eli", ""], ["Amitai", "Michal Marianne", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1802.07854", "submitter": "Siddharth Siddharth", "authors": "Siddharth, Akshay Rangesh, Eshed Ohn-Bar, and Mohan M. Trivedi", "title": "Driver Hand Localization and Grasp Analysis: A Vision-based Real-time\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting hand regions and their grasp information from images robustly in\nreal-time is critical for occupants' safety and in-vehicular infotainment\napplications. It must however, be noted that naturalistic driving scenes suffer\nfrom rapidly changing illumination and occlusion. This is aggravated by the\nfact that hands are highly deformable objects, and change in appearance\nfrequently. This work addresses the task of accurately localizing driver hands\nand classifying the grasp state of each hand. We use a fast ConvNet to first\ndetect likely hand regions. Next, a pixel-based skin classifier that takes into\naccount the global illumination changes is used to refine the hand detections\nand remove false positives. This step generates a pixel-level mask for each\nhand. Finally, we study each such masked regions and detect if the driver is\ngrasping the wheel, or in some cases a mobile phone. Through evaluation we\ndemonstrate that our method can outperform state-of-the-art pixel based hand\ndetectors, while running faster (at 35 fps) than other deep ConvNet based\nframeworks even for grasp analysis. Hand mask cues are shown to be crucial when\nanalyzing a set of driver hand gestures (wheel/mobile phone grasp and no-grasp)\nin naturalistic driving settings. The proposed detection and localization\npipeline hence can act as a general framework for real-time hand detection and\ngesture classification.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 00:14:49 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Siddharth", "", ""], ["Rangesh", "Akshay", ""], ["Ohn-Bar", "Eshed", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1802.07856", "submitter": "Darius Lam", "authors": "Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael\n  Laielli, Matthew Klaric, Yaroslav Bulatov, Brendan McCord", "title": "xView: Objects in Context in Overhead Imagery", "comments": "Initial submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new large-scale dataset for the advancement of object\ndetection techniques and overhead object detection research. This satellite\nimagery dataset enables research progress pertaining to four key computer\nvision frontiers. We utilize a novel process for geospatial category detection\nand bounding box annotation with three stages of quality control. Our data is\ncollected from WorldView-3 satellites at 0.3m ground sample distance, providing\nhigher resolution imagery than most public satellite imagery datasets. We\ncompare xView to other object detection datasets in both natural and overhead\nimagery domains and then provide a baseline analysis using the Single Shot\nMultiBox Detector. xView is one of the largest and most diverse publicly\navailable object-detection datasets to date, with over 1 million objects across\n60 classes in over 1,400 km^2 of imagery.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 00:26:46 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Lam", "Darius", ""], ["Kuzma", "Richard", ""], ["McGee", "Kevin", ""], ["Dooley", "Samuel", ""], ["Laielli", "Michael", ""], ["Klaric", "Matthew", ""], ["Bulatov", "Yaroslav", ""], ["McCord", "Brendan", ""]]}, {"id": "1802.07866", "submitter": "Jacky Chow", "authors": "Jacky C.K. Chow", "title": "Multi-Sensor Integration for Indoor 3D Reconstruction", "comments": "PhD Thesis, 2014, University of Calgary (Canada),\n  http://hdl.handle.net/11023/1484", "journal-ref": null, "doi": "10.13140/RG.2.2.10534.42566", "report-no": "UCGE Reports Number 20399", "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Outdoor maps and navigation information delivered by modern services and\ntechnologies like Google Maps and Garmin navigators have revolutionized the\nlifestyle of many people. Motivated by the desire for similar navigation\nsystems for indoor usage from consumers, advertisers, emergency\nrescuers/responders, etc., many indoor environments such as shopping malls,\nmuseums, casinos, airports, transit stations, offices, and schools need to be\nmapped. Typically, the environment is first reconstructed by capturing many\npoint clouds from various stations and defining their spatial relationships.\nCurrently, there is a lack of an accurate, rigorous, and speedy method for\nrelating point clouds in indoor, urban, satellite-denied environments. This\nthesis presents a novel and automatic way for fusing calibrated point clouds\nobtained using a terrestrial laser scanner and the Microsoft Kinect by\nintegrating them with a low-cost inertial measurement unit. The developed\nsystem, titled the Scannect, is the first joint static-kinematic indoor 3D\nmapper.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 01:04:28 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Chow", "Jacky C. K.", ""]]}, {"id": "1802.07869", "submitter": "Srikrishna Karanam", "authors": "Georgios Georgakis and Srikrishna Karanam and Ziyan Wu and Jan Ernst\n  and Jana Kosecka", "title": "End-to-end learning of keypoint detector and descriptor for pose\n  invariant 3D matching", "comments": "9 pages, 9 figures, 3 tables, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding correspondences between images or 3D scans is at the heart of many\ncomputer vision and image retrieval applications and is often enabled by\nmatching local keypoint descriptors. Various learning approaches have been\napplied in the past to different stages of the matching pipeline, considering\ndetector, descriptor, or metric learning objectives. These objectives were\ntypically addressed separately and most previous work has focused on image\ndata. This paper proposes an end-to-end learning framework for keypoint\ndetection and its representation (descriptor) for 3D depth maps or 3D scans,\nwhere the two can be jointly optimized towards task-specific objectives without\na need for separate annotations. We employ a Siamese architecture augmented by\na sampling layer and a novel score loss function which in turn affects the\nselection of region proposals. The positive and negative examples are obtained\nautomatically by sampling corresponding region proposals based on their\nconsistency with known 3D pose labels. Matching experiments with depth data on\nmultiple benchmark datasets demonstrate the efficacy of the proposed approach,\nshowing significant improvements over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 01:58:43 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 15:00:23 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Georgakis", "Georgios", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Ernst", "Jan", ""], ["Kosecka", "Jana", ""]]}, {"id": "1802.07888", "submitter": "Hyunjung Shim Dr.", "authors": "Junsuk Choe, Joo Hyun Park, Hyunjung Shim", "title": "Improved Techniques For Weakly-Supervised Object Localization", "comments": "Submitted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved technique for weakly-supervised object localization.\nConventional methods have a limitation that they focus only on most\ndiscriminative parts of the target objects. The recent study addressed this\nissue and resolved this limitation by augmenting the training data for less\ndiscriminative parts. To this end, we employ an effective data augmentation for\nimproving the accuracy of the object localization. In addition, we introduce\nimproved learning techniques by optimizing Convolutional Neural Networks (CNN)\nbased on the state-of-the-art model. Based on extensive experiments, we\nevaluate the effectiveness of the proposed approach both qualitatively and\nquantitatively. Especially, we observe that our method improves the Top-1\nlocalization accuracy by 21.4 - 37.3% depending on configurations, compared to\nthe current state-of-the-art technique of the weakly-supervised object\nlocalization.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 02:53:28 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 03:49:05 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Choe", "Junsuk", ""], ["Park", "Joo Hyun", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1802.07898", "submitter": "Fabien Baradel", "authors": "Fabien Baradel, Christian Wolf, Julien Mille, Graham W. Taylor", "title": "Glimpse Clouds: Human Activity Recognition from Unstructured Feature\n  Points", "comments": "CVPR 2018 - project page:\n  https://fabienbaradel.github.io/cvpr18_glimpseclouds/", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for human activity recognition from RGB data that does\nnot rely on any pose information during test time and does not explicitly\ncalculate pose information internally. Instead, a visual attention module\nlearns to predict glimpse sequences in each frame. These glimpses correspond to\ninterest points in the scene that are relevant to the classified activities. No\nspatial coherence is forced on the glimpse locations, which gives the module\nliberty to explore different points at each frame and better optimize the\nprocess of scrutinizing visual information. Tracking and sequentially\nintegrating this kind of unstructured data is a challenge, which we address by\nseparating the set of glimpses from a set of recurrent tracking/recognition\nworkers. These workers receive glimpses, jointly performing subsequent motion\ntracking and activity prediction. The glimpses are soft-assigned to the\nworkers, optimizing coherence of the assignments in space, time and feature\nspace using an external memory module. No hard decisions are taken, i.e. each\nglimpse point is assigned to all existing workers, albeit with different\nimportance. Our methods outperform state-of-the-art methods on the largest\nhuman activity recognition dataset available to-date; NTU RGB+D Dataset, and on\na smaller human action recognition dataset Northwestern-UCLA Multiview Action\n3D Dataset. Our code is publicly available at\nhttps://github.com/fabienbaradel/glimpse_clouds.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 04:09:30 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 18:21:07 GMT"}, {"version": "v3", "created": "Fri, 3 Aug 2018 11:57:32 GMT"}, {"version": "v4", "created": "Tue, 21 Aug 2018 08:21:02 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Baradel", "Fabien", ""], ["Wolf", "Christian", ""], ["Mille", "Julien", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1802.07918", "submitter": "Pingping Zhang Mr", "authors": "Ju Dai, Pingping Zhang, Huchuan Lu, Hongyu Wang", "title": "Video Person Re-identification by Temporal Residual Learning", "comments": "Submitted to IEEE Transactions on Image Processing, including 5\n  figures and 4 tables. The first two authors contribute equally to this work", "journal-ref": null, "doi": "10.1109/TIP.2018.2878505", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel feature learning framework for video person\nre-identification (re-ID). The proposed framework largely aims to exploit the\nadequate temporal information of video sequences and tackle the poor spatial\nalignment of moving pedestrians. More specifically, for exploiting the temporal\ninformation, we design a temporal residual learning (TRL) module to\nsimultaneously extract the generic and specific features of consecutive frames.\nThe TRL module is equipped with two bi-directional LSTM (BiLSTM), which are\nrespectively responsible to describe a moving person in different aspects,\nproviding complementary information for better feature representations. To deal\nwith the poor spatial alignment in video re-ID datasets, we propose a\nspatial-temporal transformer network (ST^2N) module. Transformation parameters\nin the ST^2N module are learned by leveraging the high-level semantic\ninformation of the current frame as well as the temporal context knowledge from\nother frames. The proposed ST^2N module with less learnable parameters allows\neffective person alignments under significant appearance changes. Extensive\nexperimental results on the large-scale MARS, PRID2011, ILIDS-VID and SDU-VID\ndatasets demonstrate that the proposed method achieves consistently superior\nperformance and outperforms most of the very recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:13:53 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Dai", "Ju", ""], ["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""], ["Wang", "Hongyu", ""]]}, {"id": "1802.07929", "submitter": "Yuanchao Bai", "authors": "Yuanchao Bai, Gene Cheung, Xianming Liu, Wen Gao", "title": "Graph-Based Blind Image Deblurring From a Single Photograph", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2874290", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring, i.e., deblurring without knowledge of the blur\nkernel, is a highly ill-posed problem. The problem can be solved in two parts:\ni) estimate a blur kernel from the blurry image, and ii) given estimated blur\nkernel, de-convolve blurry input to restore the target image. In this paper, we\npropose a graph-based blind image deblurring algorithm by interpreting an image\npatch as a signal on a weighted graph. Specifically, we first argue that a\nskeleton image---a proxy that retains the strong gradients of the target but\nsmooths out the details---can be used to accurately estimate the blur kernel\nand has a unique bi-modal edge weight distribution. Then, we design a\nreweighted graph total variation (RGTV) prior that can efficiently promote a\nbi-modal edge weight distribution given a blurry patch. Further, to analyze\nRGTV in the graph frequency domain, we introduce a new weight function to\nrepresent RGTV as a graph $l_1$-Laplacian regularizer. This leads to a graph\nspectral filtering interpretation of the prior with desirable properties,\nincluding robustness to noise and blur, strong piecewise smooth (PWS) filtering\nand sharpness promotion. Minimizing a blind image deblurring objective with\nRGTV results in a non-convex non-differentiable optimization problem. We\nleverage the new graph spectral interpretation for RGTV to design an efficient\nalgorithm that solves for the skeleton image and the blur kernel alternately.\nSpecifically for Gaussian blur, we propose a further speedup strategy for blind\nGaussian deblurring using accelerated graph spectral filtering. Finally, with\nthe computed blur kernel, recent non-blind image deblurring algorithms can be\napplied to restore the target image. Experimental results demonstrate that our\nalgorithm successfully restores latent sharp images and outperforms\nstate-of-the-art methods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:48:18 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Bai", "Yuanchao", ""], ["Cheung", "Gene", ""], ["Liu", "Xianming", ""], ["Gao", "Wen", ""]]}, {"id": "1802.07931", "submitter": "Sikun Lin", "authors": "Sikun Lin, Pan Hui", "title": "Where's YOUR focus: Personalized Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual attention is subjective and biased according to the personal\npreference of the viewer, however, current works of saliency detection are\ngeneral and objective, without counting the factor of the observer. This will\nmake the attention prediction for a particular person not accurate enough. In\nthis work, we present the novel idea of personalized attention prediction and\ndevelop Personalized Attention Network (PANet), a convolutional network that\npredicts saliency in images with personal preference. The model consists of two\nstreams which share common feature extraction layers, and one stream is\nresponsible for saliency prediction, while the other is adapted from the\ndetection model and used to fit user preference. We automatically collect user\npreference from their albums and leaves them freedom to define what and how\nmany categories their preferences are divided into. To train PANet, we\ndynamically generate ground truth saliency maps upon existing detection labels\nand saliency labels, and the generation parameters are based upon our collected\ndatasets consists of 1k images. We evaluate the model with saliency prediction\nmetrics and test the trained model on different preference vectors. The results\nhave shown that our system is much better than general models in personalized\nsaliency prediction and is efficient to use for different preferences.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 07:58:18 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Lin", "Sikun", ""], ["Hui", "Pan", ""]]}, {"id": "1802.07934", "submitter": "Wei-Chih Hung", "authors": "Wei-Chih Hung, Yi-Hsuan Tsai, Yan-Ting Liou, Yen-Yu Lin, Ming-Hsuan\n  Yang", "title": "Adversarial Learning for Semi-Supervised Semantic Segmentation", "comments": "Accepted in BMVC 2018. Code and models available at\n  https://github.com/hfslyc/AdvSemiSeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for semi-supervised semantic segmentation using an\nadversarial network. While most existing discriminators are trained to classify\ninput images as real or fake on the image level, we design a discriminator in a\nfully convolutional manner to differentiate the predicted probability maps from\nthe ground truth segmentation distribution with the consideration of the\nspatial resolution. We show that the proposed discriminator can be used to\nimprove semantic segmentation accuracy by coupling the adversarial loss with\nthe standard cross entropy loss of the proposed model. In addition, the fully\nconvolutional discriminator enables semi-supervised learning through\ndiscovering the trustworthy regions in predicted results of unlabeled images,\nthereby providing additional supervisory signals. In contrast to existing\nmethods that utilize weakly-labeled images, our method leverages unlabeled\nimages to enhance the segmentation model. Experimental results on the PASCAL\nVOC 2012 and Cityscapes datasets demonstrate the effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:13:20 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 22:56:44 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Hung", "Wei-Chih", ""], ["Tsai", "Yi-Hsuan", ""], ["Liou", "Yan-Ting", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1802.07956", "submitter": "Borja Bovcon", "authors": "Borja Bovcon, Rok Mandeljc, Janez Per\\v{s}, Matej Kristan", "title": "Stereo obstacle detection for unmanned surface vehicles by IMU-assisted\n  semantic segmentation", "comments": "14 pages, 18 figures, new publicly available multi-modal obstacle\n  detection dataset", "journal-ref": null, "doi": "10.1016/j.robot.2018.02.017", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new obstacle detection algorithm for unmanned surface vehicles (USVs) is\npresented. A state-of-the-art graphical model for semantic segmentation is\nextended to incorporate boat pitch and roll measurements from the on-board\ninertial measurement unit (IMU), and a stereo verification algorithm that\nconsolidates tentative detections obtained from the segmentation is proposed.\nThe IMU readings are used to estimate the location of horizon line in the\nimage, which automatically adjusts the priors in the probabilistic semantic\nsegmentation model. We derive the equations for projecting the horizon into\nimages, propose an efficient optimization algorithm for the extended graphical\nmodel, and offer a practical IMU-camera-USV calibration procedure. Using an USV\nequipped with multiple synchronized sensors, we captured a new challenging\nmulti-modal dataset, and annotated its images with water edge and obstacles.\nExperimental results show that the proposed algorithm significantly outperforms\nthe state of the art, with nearly 30% improvement in water-edge detection\naccuracy, an over 21% reduction of false positive rate, an almost 60% reduction\nof false negative rate, and an over 65% increase of true positive rate, while\nits Matlab implementation runs in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 09:52:43 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Bovcon", "Borja", ""], ["Mandeljc", "Rok", ""], ["Per\u0161", "Janez", ""], ["Kristan", "Matej", ""]]}, {"id": "1802.07957", "submitter": "Pingping Zhang Dr", "authors": "Pingping Zhang, Wei Liu, Dong Wang, Yinjie Lei, Hongyu Wang, Chunhua\n  Shen, Huchuan Lu", "title": "Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal\n  Discriminative Saliency Maps", "comments": "12 pages, 9 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel effective non-rigid object tracking\nframework based on the spatial-temporal consistent saliency detection. In\ncontrast to most existing trackers that utilize a bounding box to specify the\ntracked target, the proposed framework can extract accurate regions of the\ntarget as tracking outputs. It achieves a better description of the non-rigid\nobjects and reduces the background pollution for the tracking model.\nFurthermore, our model has several unique features. First, a tailored fully\nconvolutional neural network (TFCN) is developed to model the local saliency\nprior for a given image region, which not only provides the pixel-wise outputs\nbut also integrates the semantic information. Second, a novel multi-scale\nmulti-region mechanism is proposed to generate local saliency maps that\neffectively consider visual perceptions with different spatial layouts and\nscale variations. Subsequently, local saliency maps are fused via a weighted\nentropy method, resulting in a final discriminative saliency map. Finally, we\npresent a non-rigid object tracking algorithm based on the predicted saliency\nmaps. By utilizing a spatial-temporal consistent saliency map (STCSM), we\nconduct target-background classification and use a simple fine-tuning scheme\nfor online updating. Extensive experiments demonstrate that the proposed\nalgorithm achieves competitive performance in both saliency detection and\nvisual tracking, especially outperforming other related trackers on the\nnon-rigid object tracking datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 09:55:29 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 02:18:06 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Zhang", "Pingping", ""], ["Liu", "Wei", ""], ["Wang", "Dong", ""], ["Lei", "Yinjie", ""], ["Wang", "Hongyu", ""], ["Shen", "Chunhua", ""], ["Lu", "Huchuan", ""]]}, {"id": "1802.07971", "submitter": "Jean-Yves Franceschi", "authors": "Jean-Yves Franceschi (LIP), Alhussein Fawzi, Omar Fawzi (LIP)", "title": "Robustness of classifiers to uniform $\\ell\\_p$ and Gaussian noise", "comments": null, "journal-ref": "21st International Conference on Artificial Intelligence and\n  Statistics (AISTATS) 2018, Apr 2018, Playa Blanca, Spain. 2018,\n  http://www.aistats.org/", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of classifiers to various kinds of random noise\nmodels. In particular, we consider noise drawn uniformly from the $\\ell\\_p$\nball for $p \\in [1, \\infty]$ and Gaussian noise with an arbitrary covariance\nmatrix. We characterize this robustness to random noise in terms of the\ndistance to the decision boundary of the classifier. This analysis applies to\nlinear classifiers as well as classifiers with locally approximately flat\ndecision boundaries, a condition which is satisfied by state-of-the-art deep\nneural networks. The predicted robustness is verified experimentally.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 10:31:21 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Franceschi", "Jean-Yves", "", "LIP"], ["Fawzi", "Alhussein", "", "LIP"], ["Fawzi", "Omar", "", "LIP"]]}, {"id": "1802.08057", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa, Angshul\n  Majumdar", "title": "MagnifyMe: Aiding Cross Resolution Face Recognition via Identity Aware\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancing low resolution images via super-resolution or image synthesis for\ncross-resolution face recognition has been well studied. Several image\nprocessing and machine learning paradigms have been explored for addressing the\nsame. In this research, we propose Synthesis via Deep Sparse Representation\nalgorithm for synthesizing a high resolution face image from a low resolution\ninput image. The proposed algorithm learns multi-level sparse representation\nfor both high and low resolution gallery images, along with an identity aware\ndictionary and a transformation function between the two representations for\nface identification scenarios. With low resolution test data as input, the high\nresolution test image is synthesized using the identity aware dictionary and\ntransformation which is then used for face recognition. The performance of the\nproposed SDSR algorithm is evaluated on four databases, including one real\nworld dataset. Experimental results and comparison with existing seven\nalgorithms demonstrate the efficacy of the proposed algorithm in terms of both\nface identification and image quality measures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 14:26:53 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1802.08077", "submitter": "Lingkun Luo Dr.", "authors": "Lingkun Luo, Liming Chen, Ying lu, Shiqiang Hu", "title": "Discriminative Label Consistent Domain Adaptation", "comments": "12 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1712.10042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) is transfer learning which aims to learn an effective\npredictor on target data from source data despite data distribution mismatch\nbetween source and target. We present in this paper a novel unsupervised DA\nmethod for cross-domain visual recognition which simultaneously optimizes the\nthree terms of a theoretically established error bound. Specifically, the\nproposed DA method iteratively searches a latent shared feature subspace where\nnot only the divergence of data distributions between the source domain and the\ntarget domain is decreased as most state-of-the-art DA methods do, but also the\ninter-class distances are increased to facilitate discriminative learning.\nMoreover, the proposed DA method sparsely regresses class labels from the\nfeatures achieved in the shared subspace while minimizing the prediction errors\non the source data and ensuring label consistency between source and target.\nData outliers are also accounted for to further avoid negative knowledge\ntransfer. Comprehensive experiments and in-depth analysis verify the\neffectiveness of the proposed DA method which consistently outperforms the\nstate-of-the-art DA methods on standard DA benchmarks, i.e., 12 cross-domain\nimage classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 13:30:52 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Luo", "Lingkun", ""], ["Chen", "Liming", ""], ["lu", "Ying", ""], ["Hu", "Shiqiang", ""]]}, {"id": "1802.08080", "submitter": "Aditya Golatkar", "authors": "Aditya Golatkar, Deepak Anand and Amit Sethi", "title": "Classification of Breast Cancer Histology using Deep Learning", "comments": "8 pages. Published at ICIAR 2018, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast Cancer is a major cause of death worldwide among women. Hematoxylin\nand Eosin (H&E) stained breast tissue samples from biopsies are observed under\nmicroscopes for the primary diagnosis of breast cancer. In this paper, we\npropose a deep learning-based method for classification of H&E stained breast\ntissue images released for BACH challenge 2018 by fine-tuning Inception-v3\nconvolutional neural network (CNN) proposed by Szegedy et al. These images are\nto be classified into four classes namely, i) normal tissue, ii) benign tumor,\niii) in-situ carcinoma and iv) invasive carcinoma. Our strategy is to extract\npatches based on nuclei density instead of random or grid sampling, along with\nrejection of patches that are not rich in nuclei (non-epithelial) regions for\ntraining and testing. Every patch (nuclei-dense region) in an image is\nclassified in one of the four above mentioned categories. The class of the\nentire image is determined using majority voting over the nuclear classes. We\nobtained an average four class accuracy of 85% and an average two class\n(non-cancer vs. carcinoma) accuracy of 93%, which improves upon a previous\nbenchmark by Araujo et al.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 14:56:38 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:58:20 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Golatkar", "Aditya", ""], ["Anand", "Deepak", ""], ["Sethi", "Amit", ""]]}, {"id": "1802.08122", "submitter": "Wei Li", "authors": "Wei Li, Xiatian Zhu, Shaogang Gong", "title": "Harmonious Attention Network for Person Re-Identification", "comments": "Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (re-id) methods either assume the\navailability of well-aligned person bounding box images as model input or rely\non constrained attention selection mechanisms to calibrate misaligned images.\nThey are therefore sub-optimal for re-id matching in arbitrarily aligned person\nimages potentially with large human pose variations and unconstrained\nauto-detection errors. In this work, we show the advantages of jointly learning\nattention selection and feature representation in a Convolutional Neural\nNetwork (CNN) by maximising the complementary information of different levels\nof visual attention subject to re-id discriminative learning constraints.\nSpecifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model for\njoint learning of soft pixel attention and hard regional attention along with\nsimultaneous optimisation of feature representations, dedicated to optimise\nperson re-id in uncontrolled (misaligned) images. Extensive comparative\nevaluations validate the superiority of this new HA-CNN model for person re-id\nover a wide variety of state-of-the-art methods on three large-scale benchmarks\nincluding CUHK03, Market-1501, and DukeMTMC-ReID.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 16:04:55 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Li", "Wei", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1802.08129", "submitter": "Dong Huk Park", "authors": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt\n  Schiele, Trevor Darrell, Marcus Rohrbach", "title": "Multimodal Explanations: Justifying Decisions and Pointing to the\n  Evidence", "comments": "arXiv admin note: text overlap with arXiv:1612.04757", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models that are both effective and explainable are desirable in many\nsettings; prior explainable models have been unimodal, offering either\nimage-based visualization of attention weights or text-based generation of\npost-hoc justifications. We propose a multimodal approach to explanation, and\nargue that the two modalities provide complementary explanatory strengths. We\ncollect two new datasets to define and evaluate this task, and propose a novel\nmodel which can provide joint textual rationale generation and attention\nvisualization. Our datasets define visual and textual justifications of a\nclassification decision for activity recognition tasks (ACT-X) and for visual\nquestion answering tasks (VQA-X). We quantitatively show that training with the\ntextual explanations not only yields better textual justification models, but\nalso better localizes the evidence that supports the decision. We also\nqualitatively show cases where visual explanation is more insightful than\ntextual explanation, and vice versa, supporting our thesis that multimodal\nexplanation models offer significant benefits over unimodal approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 19:12:03 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Park", "Dong Huk", ""], ["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Rohrbach", "Anna", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1802.08195", "submitter": "Gamaleldin Elsayed", "authors": "Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot,\n  Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein", "title": "Adversarial Examples that Fool both Computer Vision and Time-Limited\n  Humans", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial examples: small changes\nto images can cause computer vision models to make mistakes such as identifying\na school bus as an ostrich. However, it is still an open question whether\nhumans are prone to similar mistakes. Here, we address this question by\nleveraging recent techniques that transfer adversarial examples from computer\nvision models with known parameters and architecture to other models with\nunknown parameters and architecture, and by matching the initial processing of\nthe human visual system. We find that adversarial examples that strongly\ntransfer across computer vision models influence the classifications made by\ntime-limited human observers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:40:51 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 18:46:56 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 03:02:41 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Elsayed", "Gamaleldin F.", ""], ["Shankar", "Shreya", ""], ["Cheung", "Brian", ""], ["Papernot", "Nicolas", ""], ["Kurakin", "Alex", ""], ["Goodfellow", "Ian", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1802.08200", "submitter": "Michael Jacobs", "authors": "Vishwa S. Parekh, Katarzyna J. Macura, Susan Harvey, Ihab Kamel, Riham\n  EI-Khouli, David A. Bluemke, Michael A. Jacobs", "title": "Multiparametric Deep Learning Tissue Signatures for a Radiological\n  Biomarker of Breast Cancer: Preliminary Results", "comments": "Deep Learning, Machine learning, Magnetic resonance imaging,\n  multiparametric MRI, Breast, Cancer, Diffusion, tissue biomarkers", "journal-ref": "Medical physics 2020 47 (1), 75-88", "doi": "10.1002/mp.13849", "report-no": null, "categories": "physics.med-ph cs.AI cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new paradigm is beginning to emerge in Radiology with the advent of\nincreased computational capabilities and algorithms. This has led to the\nability of real time learning by computer systems of different lesion types to\nhelp the radiologist in defining disease. For example, using a deep learning\nnetwork, we developed and tested a multiparametric deep learning (MPDL) network\nfor segmentation and classification using multiparametric magnetic resonance\nimaging (mpMRI) radiological images. The MPDL network was constructed from\nstacked sparse autoencoders with inputs from mpMRI. Evaluation of MPDL\nconsisted of cross-validation, sensitivity, and specificity. Dice similarity\nbetween MPDL and post-DCE lesions were evaluated. We demonstrate high\nsensitivity and specificity for differentiation of malignant from benign\nlesions of 90% and 85% respectively with an AUC of 0.93. The Integrated MPDL\nmethod accurately segmented and classified different breast tissue from\nmultiparametric breast MRI using deep leaning tissue signatures.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 02:51:59 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Parekh", "Vishwa S.", ""], ["Macura", "Katarzyna J.", ""], ["Harvey", "Susan", ""], ["Kamel", "Ihab", ""], ["EI-Khouli", "Riham", ""], ["Bluemke", "David A.", ""], ["Jacobs", "Michael A.", ""]]}, {"id": "1802.08216", "submitter": "Shikhar Sharma", "authors": "Shikhar Sharma, Dendi Suhubdy, Vincent Michalski, Samira Ebrahimi\n  Kahou, Yoshua Bengio", "title": "ChatPainter: Improving Text to Image Generation using Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic images from text descriptions on a dataset like\nMicrosoft Common Objects in Context (MS COCO), where each image can contain\nseveral objects, is a challenging task. Prior work has used text captions to\ngenerate images. However, captions might not be informative enough to capture\nthe entire image and insufficient for the model to be able to understand which\nobjects in the images correspond to which words in the captions. We show that\nadding a dialogue that further describes the scene leads to significant\nimprovement in the inception score and in the quality of generated images on\nthe MS COCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:15:40 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Sharma", "Shikhar", ""], ["Suhubdy", "Dendi", ""], ["Michalski", "Vincent", ""], ["Kahou", "Samira Ebrahimi", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1802.08218", "submitter": "Danna Gurari", "authors": "Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen\n  Grauman, Jiebo Luo, and Jeffrey P. Bigham", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of algorithms to automatically answer visual questions currently is\nmotivated by visual question answering (VQA) datasets constructed in artificial\nVQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising\nfrom a natural VQA setting. VizWiz consists of over 31,000 visual questions\noriginating from blind people who each took a picture using a mobile phone and\nrecorded a spoken question about it, together with 10 crowdsourced answers per\nvisual question. VizWiz differs from the many existing VQA datasets because (1)\nimages are captured by blind photographers and so are often poor quality, (2)\nquestions are spoken and so are more conversational, and (3) often visual\nquestions cannot be answered. Evaluation of modern algorithms for answering\nvisual questions and deciding if a visual question is answerable reveals that\nVizWiz is a challenging dataset. We introduce this dataset to encourage a\nlarger community to develop more generalized algorithms that can assist blind\npeople.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:16:53 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 19:52:08 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 15:53:07 GMT"}, {"version": "v4", "created": "Wed, 9 May 2018 17:26:40 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Gurari", "Danna", ""], ["Li", "Qing", ""], ["Stangl", "Abigale J.", ""], ["Guo", "Anhong", ""], ["Lin", "Chi", ""], ["Grauman", "Kristen", ""], ["Luo", "Jiebo", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1802.08219", "submitter": "Nathaniel Thomas", "authors": "Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai\n  Kohlhoff, Patrick Riley", "title": "Tensor field networks: Rotation- and translation-equivariant neural\n  networks for 3D point clouds", "comments": "changes for NIPS submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tensor field neural networks, which are locally equivariant to\n3D rotations, translations, and permutations of points at every layer. 3D\nrotation equivariance removes the need for data augmentation to identify\nfeatures in arbitrary orientations. Our network uses filters built from\nspherical harmonics; due to the mathematical consequences of this filter\nchoice, each layer accepts as input (and guarantees as output) scalars,\nvectors, and higher-order tensors, in the geometric sense of these terms. We\ndemonstrate the capabilities of tensor field networks with tasks in geometry,\nphysics, and chemistry.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:17:31 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 18:58:16 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 20:09:34 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Thomas", "Nathaniel", ""], ["Smidt", "Tess", ""], ["Kearnes", "Steven", ""], ["Yang", "Lusann", ""], ["Li", "Li", ""], ["Kohlhoff", "Kai", ""], ["Riley", "Patrick", ""]]}, {"id": "1802.08241", "submitter": "Amir Gholami", "authors": "Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, Michael W. Mahoney", "title": "Hessian-based Analysis of Large Batch Training and Robustness to\n  Adversaries", "comments": "Presented in NeurIPS'18 conference", "journal-ref": "NeurIPS 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large batch size training of Neural Networks has been shown to incur accuracy\nloss when trained with the current methods. The exact underlying reasons for\nthis are still not completely understood. Here, we study large batch size\ntraining through the lens of the Hessian operator and robust optimization. In\nparticular, we perform a Hessian based study to analyze exactly how the\nlandscape of the loss function changes when training with large batch size. We\ncompute the true Hessian spectrum, without approximation, by back-propagating\nthe second derivative. Extensive experiments on multiple networks show that\nsaddle-points are not the cause for generalization gap of large batch size\ntraining, and the results consistently show that large batch converges to\npoints with noticeably higher Hessian spectrum. Furthermore, we show that\nrobust training allows one to favor flat areas, as points with large Hessian\nspectrum show poor robustness to adversarial perturbation. We further study\nthis relationship, and provide empirical and theoretical proof that the inner\nloop for robust training is a saddle-free optimization problem \\textit{almost\neverywhere}. We present detailed experiments with five different network\narchitectures, including a residual network, tested on MNIST, CIFAR-10, and\nCIFAR-100 datasets. We have open sourced our method which can be accessed at\n[1].\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:55:00 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 19:41:12 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 22:25:00 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 19:58:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yao", "Zhewei", ""], ["Gholami", "Amir", ""], ["Lei", "Qi", ""], ["Keutzer", "Kurt", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1802.08275", "submitter": "Hang Su", "authors": "Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos\n  Kalogerakis, Ming-Hsuan Yang, and Jan Kautz", "title": "SPLATNet: Sparse Lattice Networks for Point Cloud Processing", "comments": "Camera-ready, accepted to CVPR 2018 (oral); project website:\n  http://vis-www.cs.umass.edu/splatnet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a network architecture for processing point clouds that directly\noperates on a collection of points represented as a sparse set of samples in a\nhigh-dimensional lattice. Naively applying convolutions on this lattice scales\npoorly, both in terms of memory and computational cost, as the size of the\nlattice increases. Instead, our network uses sparse bilateral convolutional\nlayers as building blocks. These layers maintain efficiency by using indexing\nstructures to apply convolutions only on occupied parts of the lattice, and\nallow flexible specifications of the lattice structure enabling hierarchical\nand spatially-aware feature learning, as well as joint 2D-3D reasoning. Both\npoint-based and image-based representations can be easily incorporated in a\nnetwork with such layers and the resulting model can be trained in an\nend-to-end manner. We present results on 3D segmentation tasks where our\napproach outperforms existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 19:30:09 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 22:16:31 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 18:36:16 GMT"}, {"version": "v4", "created": "Wed, 9 May 2018 14:22:41 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Su", "Hang", ""], ["Jampani", "Varun", ""], ["Sun", "Deqing", ""], ["Maji", "Subhransu", ""], ["Kalogerakis", "Evangelos", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1802.08290", "submitter": "Jinjiang Guo", "authors": "Jinjiang Guo, Pengyuan Ren, Aiguo Gu, Jian Xu, Weixin Wu", "title": "Locally Adaptive Learning Loss for Semantic Image Segmentation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel locally adaptive learning estimator for enhancing the\ninter- and intra- discriminative capabilities of Deep Neural Networks, which\ncan be used as improved loss layer for semantic image segmentation tasks. Most\nloss layers compute pixel-wise cost between feature maps and ground truths,\nignoring spatial layouts and interactions between neighboring pixels with same\nobject category, and thus networks cannot be effectively sensitive to\nintra-class connections. Stride by stride, our method firstly conducts adaptive\npooling filter operating over predicted feature maps, aiming to merge predicted\ndistributions over a small group of neighboring pixels with same category, and\nthen it computes cost between the merged distribution vector and their category\nlabel. Such design can make groups of neighboring predictions from same\ncategory involved into estimations on predicting correctness with respect to\ntheir category, and hence train networks to be more sensitive to regional\nconnections between adjacent pixels based on their categories. In the\nexperiments on Pascal VOC 2012 segmentation datasets, the consistently improved\nresults show that our proposed approach achieves better segmentation masks\nagainst previous counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 05:18:35 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 03:17:34 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Guo", "Jinjiang", ""], ["Ren", "Pengyuan", ""], ["Gu", "Aiguo", ""], ["Xu", "Jian", ""], ["Wu", "Weixin", ""]]}, {"id": "1802.08310", "submitter": "Xuefeng Peng", "authors": "Xuefeng Peng, Jiebo Luo, Catherine Glenn, Li-Kai Chi, Jingyao Zhan", "title": "Sleep-deprived Fatigue Pattern Analysis using Large-Scale Selfies from\n  Social Med", "comments": null, "journal-ref": "Special Session on Intelligent Data Mining, IEEE Big Data\n  Conference, Boston, MA, December 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexities of fatigue have drawn much attention from researchers across\nvarious disciplines. Short-term fatigue may cause safety issue while driving;\nthus, dynamic systems were designed to track driver fatigue. Long-term fatigue\ncould lead to chronic syndromes, and eventually affect individuals physical and\npsychological health. Traditional methodologies of evaluating fatigue not only\nrequire sophisticated equipment but also consume enormous time. In this paper,\nwe attempt to develop a novel and efficient method to predict individual's\nfatigue rate by scrutinizing human facial cues. Our goal is to predict fatigue\nrate based on a selfie. To associate the fatigue rate with user behaviors, we\nhave collected nearly 1-million timeline posts from 10,480 users on Instagram.\nWe first detect all the faces and identify their demographics using automatic\nalgorithms. Next, we investigate the fatigue distribution by weekday over\ndifferent age, gender, and ethnic groups. This work represents a promising way\nto assess sleep-deprived fatigue, and our study provides a viable and efficient\ncomputational framework for user fatigue modeling in large-scale via social\nmedia.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 21:31:31 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Peng", "Xuefeng", ""], ["Luo", "Jiebo", ""], ["Glenn", "Catherine", ""], ["Chi", "Li-Kai", ""], ["Zhan", "Jingyao", ""]]}, {"id": "1802.08362", "submitter": "Alaaeldin El-Nouby", "authors": "Alaaeldin El-Nouby, Graham W. Taylor", "title": "Real-Time End-to-End Action Detection with Two-Stream Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stream networks have been very successful for solving the problem of\naction detection. However, prior work using two-stream networks train both\nstreams separately, which prevents the network from exploiting regularities\nbetween the two streams. Moreover, unlike the visual stream, the dominant forms\nof optical flow computation typically do not maximally exploit GPU parallelism.\nWe present a real-time end-to-end trainable two-stream network for action\ndetection. First, we integrate the optical flow computation in our framework by\nusing Flownet2. Second, we apply early fusion for the two streams and train the\nwhole pipeline jointly end-to-end. Finally, for better network initialization,\nwe transfer from the task of action recognition to action detection by\npre-training our framework using the recently released large-scale Kinetics\ndataset. Our experimental results show that training the pipeline jointly\nend-to-end with fine-tuning the optical flow for the objective of action\ndetection improves detection performance significantly. Additionally, we\nobserve an improvement when initializing with parameters pre-trained using\nKinetics. Last, we show that by integrating the optical flow computation, our\nframework is more efficient, running at real-time speeds (up to 31 fps).\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:21:46 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["El-Nouby", "Alaaeldin", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1802.08369", "submitter": "Zhang Qiang", "authors": "Qiang Zhang, Qiangqiang Yuan, Chao Zeng, Xinghua Li, Yancong Wei", "title": "Missing Data Reconstruction in Remote Sensing image with a Unified\n  Spatial-Temporal-Spectral Deep Convolutional Neural Network", "comments": "To be published in IEEE Transactions on Geoscience and Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2018.2810208", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the internal malfunction of satellite sensors and poor atmospheric\nconditions such as thick cloud, the acquired remote sensing data often suffer\nfrom missing information, i.e., the data usability is greatly reduced. In this\npaper, a novel method of missing information reconstruction in remote sensing\nimages is proposed. The unified spatial-temporal-spectral framework based on a\ndeep convolutional neural network (STS-CNN) employs a unified deep\nconvolutional neural network combined with spatial-temporal-spectral\nsupplementary information. In addition, to address the fact that most methods\ncan only deal with a single missing information reconstruction task, the\nproposed approach can solve three typical missing information reconstruction\ntasks: 1) dead lines in Aqua MODIS band 6; 2) the Landsat ETM+ Scan Line\nCorrector (SLC)-off problem; and 3) thick cloud removal. It should be noted\nthat the proposed model can use multi-source data (spatial, spectral, and\ntemporal) as the input of the unified framework. The results of both simulated\nand real-data experiments demonstrate that the proposed model exhibits high\neffectiveness in the three missing information reconstruction tasks listed\nabove.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:42:13 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhang", "Qiang", ""], ["Yuan", "Qiangqiang", ""], ["Zeng", "Chao", ""], ["Li", "Xinghua", ""], ["Wei", "Yancong", ""]]}, {"id": "1802.08402", "submitter": "Shadrokh Samavi", "authors": "Mojtaba Akbari, Majid Mohrekesh, S.M.Reza Soroushmehr, Nader Karimi,\n  Shadrokh Samavi, Kayvan Najarian", "title": "Adaptive specular reflection detection and inpainting in colonoscopy\n  video frames", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colonoscopy video frames might be contaminated by bright spots with\nunsaturated values known as specular reflection. Detection and removal of such\nreflections could enhance the quality of colonoscopy images and facilitate\ndiagnosis procedure. In this paper we propose a novel two-phase method for this\npurpose, consisting of detection and removal phases. In the detection phase, we\nemploy both HSV and RGB color space information for segmentation of specular\nreflections. We first train a non-linear SVM for selecting a color space based\non image statistical features extracted from each channel of the color spaces.\nThen, a cost function for detection of specular reflections is introduced. In\nthe removal phase, we propose a two-step inpainting method which consists of\nappropriate replacement patch selection and removal of the blockiness effects.\nThe proposed method is evaluated by testing on an available colonoscopy image\ndatabase where accuracy and Dice score of 99.68% and 71.79% are achieved\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:25:21 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Akbari", "Mojtaba", ""], ["Mohrekesh", "Majid", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1802.08515", "submitter": "Agostino Martinelli", "authors": "Agostino Martinelli", "title": "Closed-form solution to cooperative visual-inertial structure from\n  motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of visual-inertial sensor fusion in the\ncooperative case and it provides new theoretical contributions, which regard\nits observability and its resolvability in closed form. The case of two agents\nis investigated. Each agent is equipped with inertial sensors (accelerometer\nand gyroscope) and with a monocular camera. By using the monocular camera, each\nagent can observe the other agent. No additional camera observations (e.g., of\nexternal point features in the environment) are considered. All the inertial\nsensors are assumed to be affected by a bias. First, the entire observable\nstate is analytically derived. This state includes the absolute scale, the\nrelative velocity between the two agents, the three Euler angles that express\nthe rotation between the two agent frames and all the accelerometer and\ngyroscope biases. Second, the paper provides the extension of the closed-form\nsolution given in [19] (which holds for a single agent) to the aforementioned\ncooperative case. The impact of the presence of the bias on the performance of\nthis closed-form solution is investigated. As in the case of a single agent,\nthis performance is significantly sensitive to the presence of a bias on the\ngyroscope, while, the presence of a bias on the accelerometer is negligible.\nFinally, a simple and effective method to obtain the gyroscope bias is\nproposed. Extensive simulations clearly show that the proposed method is\nsuccessful. It is amazing that, it is possible to automatically retrieve the\nabsolute scale and simultaneously calibrate the gyroscopes not only without any\nprior knowledge (as in [13]), but also without external point features in the\nenvironment.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:11:36 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Martinelli", "Agostino", ""]]}, {"id": "1802.08516", "submitter": "Joel Vidal", "authors": "Joel Vidal, Chyi-Yeu Lin, Robert Mart\\'i", "title": "6D Pose Estimation using an Improved Method based on Point Pair Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Point Pair Feature (Drost et al. 2010) has been one of the most\nsuccessful 6D pose estimation method among model-based approaches as an\nefficient, integrated and compromise alternative to the traditional local and\nglobal pipelines. During the last years, several variations of the algorithm\nhave been proposed. Among these extensions, the solution introduced by\nHinterstoisser et al. (2016) is a major contribution. This work presents a\nvariation of this PPF method applied to the SIXD Challenge datasets presented\nat the 3rd International Workshop on Recovering 6D Object Pose held at the ICCV\n2017. We report an average recall of 0.77 for all datasets and overall recall\nof 0.82, 0.67, 0.85, 0.37, 0.97 and 0.96 for hinterstoisser, tless, tudlight,\nrutgers, tejani and doumanoglou datasets, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:11:39 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Vidal", "Joel", ""], ["Lin", "Chyi-Yeu", ""], ["Mart\u00ed", "Robert", ""]]}, {"id": "1802.08530", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell", "title": "Training wide residual networks for deployment using a single bit for\n  each weight", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": "ICLR 2018 - International Conference on Learning Representations,\n  Apr 2018, Vancouver, Canada. 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:54:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["McDonnell", "Mark D.", ""]]}, {"id": "1802.08562", "submitter": "Artsiom Sanakoyeu", "authors": "Artsiom Sanakoyeu, Miguel A. Bautista, Bj\\\"orn Ommer", "title": "Deep Unsupervised Learning of Visual Similarities", "comments": "arXiv admin note: text overlap with arXiv:1608.08792", "journal-ref": "Pattern Recognition Volume 78, June 2018, Pages 331-343", "doi": "10.1016/j.patcog.2018.01.036", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar learning of visual similarities in an unsupervised manner is a\nproblem of paramount importance to Computer Vision. In this context, however,\nthe recent breakthrough in deep learning could not yet unfold its full\npotential. With only a single positive sample, a great imbalance between one\npositive and many negatives, and unreliable relationships between most samples,\ntraining of Convolutional Neural networks is impaired. In this paper we use\nweak estimates of local similarities and propose a single optimization problem\nto extract batches of samples with mutually consistent relations. Conflicting\nrelations are distributed over different batches and similar samples are\ngrouped into compact groups. Learning visual similarities is then framed as a\nsequence of categorization tasks. The CNN then consolidates transitivity\nrelations within and between groups and learns a single representation for all\nsamples without the need for labels. The proposed unsupervised approach has\nshown competitive performance on detailed posture analysis and object\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 04:11:59 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Sanakoyeu", "Artsiom", ""], ["Bautista", "Miguel A.", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1802.08568", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Subham Mukherjee, Aneeshan Sain, Ankan Kumar\n  Bhunia, Partha Pratim Roy, Umapada Pal", "title": "Indic Handwritten Script Identification using Offline-Online Multimodal\n  Deep Network", "comments": "Accepted in Information Fusion, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach of word-level Indic script\nidentification using only character-level data in training stage. The\nadvantages of using character level data for training have been outlined in\nsection I. Our method uses a multimodal deep network which takes both offline\nand online modality of the data as input in order to explore the information\nfrom both the modalities jointly for script identification task. We take\nhandwritten data in either modality as input and the opposite modality is\ngenerated through intermodality conversion. Thereafter, we feed this\noffline-online modality pair to our network. Hence, along with the advantage of\nutilizing information from both the modalities, it can work as a single\nframework for both offline and online script identification simultaneously\nwhich alleviates the need for designing two separate script identification\nmodules for individual modality. One more major contribution is that we propose\na novel conditional multimodal fusion scheme to combine the information from\noffline and online modality which takes into account the real origin of the\ndata being fed to our network and thus it combines adaptively. An exhaustive\nexperiment has been done on a data set consisting of English and six Indic\nscripts. Our proposed framework clearly outperforms different frameworks based\non traditional classifiers along with handcrafted features and deep learning\nbased methods with a clear margin. Extensive experiments show that using only\ncharacter level training data can achieve state-of-art performance similar to\nthat obtained with traditional training using word level data in our framework.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 14:49:28 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 17:04:48 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 22:49:04 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Mukherjee", "Subham", ""], ["Sain", "Aneeshan", ""], ["Bhunia", "Ankan Kumar", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1802.08632", "submitter": "Jannik Quehl", "authors": "Jannik Quehl, Haohao Hu, Sascha Wirges, Martin Lauer", "title": "An Approach to Vehicle Trajectory Prediction Using Automatically\n  Generated Traffic Maps", "comments": "6 Pages, 9 figures, submitted to IV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory and intention prediction of traffic participants is an important\ntask in automated driving and crucial for safe interaction with the\nenvironment. In this paper, we present a new approach to vehicle trajectory\nprediction based on automatically generated maps containing statistical\ninformation about the behavior of traffic participants in a given area. These\nmaps are generated based on trajectory observations using image processing and\nmap matching techniques and contain all typical vehicle movements and\nprobabilities in the considered area. Our prediction approach matches an\nobserved trajectory to a behavior contained in the map and uses this\ninformation to generate a prediction. We evaluated our approach on a dataset\ncontaining over 14000 trajectories and found that it produces significantly\nmore precise mid-term predictions compared to motion model-based prediction\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:48:35 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 08:25:46 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Quehl", "Jannik", ""], ["Hu", "Haohao", ""], ["Wirges", "Sascha", ""], ["Lauer", "Martin", ""]]}, {"id": "1802.08645", "submitter": "Seitaro Shinagawa", "authors": "Seitaro Shinagawa, Koichiro Yoshino, Sakriani Sakti, Yu Suzuki,\n  Satoshi Nakamura", "title": "Interactive Image Manipulation with Natural Language Instruction\n  Commands", "comments": "accepted at NIPS 2017 ViGIL workshop\n  (https://nips2017vigil.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an interactive image-manipulation system with natural language\ninstruction, which can generate a target image from a source image and an\ninstruction that describes the difference between the source and the target\nimage. The system makes it possible to modify a generated image interactively\nand make natural language conditioned image generation more controllable. We\nconstruct a neural network that handles image vectors in latent space to\ntransform the source vector to the target vector by using the vector of\ninstruction. The experimental results indicate that the proposed framework\nsuccessfully generates the target image by using a source image and an\ninstruction on manipulation in our dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 17:20:13 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shinagawa", "Seitaro", ""], ["Yoshino", "Koichiro", ""], ["Sakti", "Sakriani", ""], ["Suzuki", "Yu", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1802.08655", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Nishant Ravikumar, Stephan Ellman, Andreas Maier", "title": "Comparative Analysis of Unsupervised Algorithms for Breast MRI Lesion\n  Segmentation", "comments": "6 pages, submitted to Bildverarbeitung in der Medizin 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of breast lesions is a crucial step in evaluating the\ncharacteristics of tumors. However, this is a challenging task, since breast\nlesions have sophisticated shape, topological structure, and variation in the\nintensity distribution. In this paper, we evaluated the performance of three\nunsupervised algorithms for the task of breast Magnetic Resonance (MRI) lesion\nsegmentation, namely, Gaussian Mixture Model clustering, K-means clustering and\na marker-controlled Watershed transformation based method. All methods were\napplied on breast MRI slices following selection of regions of interest (ROIs)\nby an expert radiologist and evaluated on 106 subjects' images, which include\n59 malignant and 47 benign lesions. Segmentation accuracy was evaluated by\ncomparing our results with ground truth masks, using the Dice similarity\ncoefficient (DSC), Jaccard index (JI), Hausdorff distance and precision-recall\nmetrics. The results indicate that the marker-controlled Watershed\ntransformation outperformed all other algorithms investigated.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 17:40:40 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Ellman", "Stephan", ""], ["Maier", "Andreas", ""]]}, {"id": "1802.08686", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Hamza Fawzi, Omar Fawzi", "title": "Adversarial vulnerability for any classifier", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite achieving impressive performance, state-of-the-art classifiers remain\nhighly vulnerable to small, imperceptible, adversarial perturbations. This\nvulnerability has proven empirically to be very intricate to address. In this\npaper, we study the phenomenon of adversarial perturbations under the\nassumption that the data is generated with a smooth generative model. We derive\nfundamental upper bounds on the robustness to perturbations of any\nclassification function, and prove the existence of adversarial perturbations\nthat transfer well across different classifiers with small risk. Our analysis\nof the robustness also provides insights onto key properties of generative\nmodels, such as their smoothness and dimensionality of latent space. We\nconclude with numerical experimental results showing that our bounds provide\ninformative baselines to the maximal achievable robustness on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:46:05 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:44:52 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Fawzi", "Hamza", ""], ["Fawzi", "Omar", ""]]}, {"id": "1802.08701", "submitter": "Utsav Gewali", "authors": "Utsav B. Gewali, Sildomar T. Monteiro, and Eli Saber", "title": "Machine learning based hyperspectral image analysis: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral sensors enable the study of the chemical properties of scene\nmaterials remotely for the purpose of identification, detection, and chemical\ncomposition analysis of objects in the environment. Hence, hyperspectral images\ncaptured from earth observing satellites and aircraft have been increasingly\nimportant in agriculture, environmental monitoring, urban planning, mining, and\ndefense. Machine learning algorithms due to their outstanding predictive power\nhave become a key tool for modern hyperspectral image analysis. Therefore, a\nsolid understanding of machine learning techniques have become essential for\nremote sensing researchers and practitioners. This paper reviews and compares\nrecent machine learning-based hyperspectral image analysis methods published in\nliterature. We organize the methods by the image analysis task and by the type\nof machine learning algorithm, and present a two-way mapping between the image\nanalysis tasks and the types of machine learning algorithms that can be applied\nto them. The paper is comprehensive in coverage of both hyperspectral image\nanalysis tasks and machine learning algorithms. The image analysis tasks\nconsidered are land cover classification, target detection, unmixing, and\nphysical parameter estimation. The machine learning algorithms covered are\nGaussian models, linear regression, logistic regression, support vector\nmachines, Gaussian mixture model, latent linear models, sparse linear models,\nGaussian mixture models, ensemble learning, directed graphical models,\nundirected graphical models, clustering, Gaussian processes, Dirichlet\nprocesses, and deep learning. We also discuss the open challenges in the field\nof hyperspectral image analysis and explore possible future directions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 19:11:25 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 07:31:43 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Gewali", "Utsav B.", ""], ["Monteiro", "Sildomar T.", ""], ["Saber", "Eli", ""]]}, {"id": "1802.08717", "submitter": "Maciej Mazurowski", "authors": "Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir", "title": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 04:00:55 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mazurowski", "Maciej A.", ""], ["Buda", "Mateusz", ""], ["Saha", "Ashirbani", ""], ["Bashir", "Mustafa R.", ""]]}, {"id": "1802.08722", "submitter": "Michel Melo Silva", "authors": "Michel Melo Silva, Washington Luis Souza Ramos, Joao Klock Ferreira,\n  Felipe Cadar Chamone, Mario Fernando Montenegro Campos, Erickson Rangel\n  Nascimento", "title": "A Weighted Sparse Sampling and Smoothing Frame Transition Approach for\n  Semantic Fast-Forward First-Person Videos", "comments": "Accepted for publication in the IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR) 2018. Link to the project wesite:\n  https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/", "journal-ref": null, "doi": "10.1109/CVPR.2018.00253", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the advances in the technology of low-cost digital cameras and the\npopularity of the self-recording culture, the amount of visual data on the\nInternet is going to the opposite side of the available time and patience of\nthe users. Thus, most of the uploaded videos are doomed to be forgotten and\nunwatched in a computer folder or website. In this work, we address the problem\nof creating smooth fast-forward videos without losing the relevant content. We\npresent a new adaptive frame selection formulated as a weighted minimum\nreconstruction problem, which combined with a smoothing frame transition method\naccelerates first-person videos emphasizing the relevant segments and avoids\nvisual discontinuities. The experiments show that our method is able to\nfast-forward videos to retain as much relevant information and smoothness as\nthe state-of-the-art techniques in less time. We also present a new 80-hour\nmultimodal (RGB-D, IMU, and GPS) dataset of first-person videos with\nannotations for recorder profile, frame scene, activities, interaction, and\nattention.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:19:59 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 20:11:00 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 20:26:32 GMT"}, {"version": "v4", "created": "Thu, 4 Apr 2019 13:07:47 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Silva", "Michel Melo", ""], ["Ramos", "Washington Luis Souza", ""], ["Ferreira", "Joao Klock", ""], ["Chamone", "Felipe Cadar", ""], ["Campos", "Mario Fernando Montenegro", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "1802.08726", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui", "title": "Longitudinal Face Aging in the Wild - Recent Deep Learning Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Aging has raised considerable attentions and interest from the computer\nvision community in recent years. Numerous approaches ranging from purely image\nprocessing techniques to deep learning structures have been proposed in\nliterature. In this paper, we aim to give a review of recent developments of\nmodern deep learning based approaches, i.e. Deep Generative Models, for Face\nAging task. Their structures, formulation, learning algorithms as well as\nsynthesized results are also provided with systematic discussions. Moreover,\nthe aging databases used in most methods to learn the aging process are also\nreviewed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:29:50 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Quach", "Kha Gia", ""], ["Bui", "Tien D.", ""]]}, {"id": "1802.08735", "submitter": "Rui Shu", "authors": "Rui Shu, Hung H. Bui, Hirokazu Narui, Stefano Ermon", "title": "A DIRT-T Approach to Unsupervised Domain Adaptation", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation refers to the problem of leveraging labeled data in a\nsource domain to learn an accurate model in a target domain where labels are\nscarce or unavailable. A recent approach for finding a common representation of\nthe two domains is via domain adversarial training (Ganin & Lempitsky, 2015),\nwhich attempts to induce a feature extractor that matches the source and target\nfeature distributions in some feature space. However, domain adversarial\ntraining faces two critical limitations: 1) if the feature extraction function\nhas high-capacity, then feature distribution matching is a weak constraint, 2)\nin non-conservative domain adaptation (where no single classifier can perform\nwell in both the source and target domains), training the model to do well on\nthe source domain hurts performance on the target domain. In this paper, we\naddress these issues through the lens of the cluster assumption, i.e., decision\nboundaries should not cross high-density data regions. We propose two novel and\nrelated models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,\nwhich combines domain adversarial training with a penalty term that punishes\nthe violation the cluster assumption; 2) the Decision-boundary Iterative\nRefinement Training with a Teacher (DIRT-T) model, which takes the VADA model\nas initialization and employs natural gradient steps to further minimize the\ncluster assumption violation. Extensive empirical results demonstrate that the\ncombination of these two models significantly improve the state-of-the-art\nperformance on the digit, traffic sign, and Wi-Fi recognition domain adaptation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:57:28 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 13:45:46 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Shu", "Rui", ""], ["Bui", "Hung H.", ""], ["Narui", "Hirokazu", ""], ["Ermon", "Stefano", ""]]}, {"id": "1802.08753", "submitter": "Amirhossein Jabalameli", "authors": "Amirhossein Jabalameli, Nabil Ettehadi, Aman Behal", "title": "Edge-Based Recognition of Novel Objects for Robotic Grasping", "comments": null, "journal-ref": null, "doi": "10.3390/robotics8030063", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of grasping novel objects in\nunstructured environments. To address this problem, consideration of the object\ngeometry, reachability and force closure analysis are required. We propose a\nframework for grasping unknown objects by localizing contact regions on the\ncontours formed by a set of depth edges in a single view 2D depth image.\nAccording to the edge geometric features obtained from analyzing the data of\nthe depth map, the contact regions are determined. Finally,We validate the\nperformance of the approach by applying it to the scenes with both single and\nmultiple objects, using Baxter manipulator.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 22:38:55 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Jabalameli", "Amirhossein", ""], ["Ettehadi", "Nabil", ""], ["Behal", "Aman", ""]]}, {"id": "1802.08755", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh and Mohan M. Trivedi", "title": "No Blind Spots: Full-Surround Multi-Object Tracking for Autonomous\n  Vehicles using Cameras & LiDARs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online multi-object tracking (MOT) is extremely important for high-level\nspatial reasoning and path planning for autonomous and highly-automated\nvehicles. In this paper, we present a modular framework for tracking multiple\nobjects (vehicles), capable of accepting object proposals from different sensor\nmodalities (vision and range) and a variable number of sensors, to produce\ncontinuous object tracks. This work is a generalization of the MDP framework\nfor MOT, with some key extensions - First, we track objects across multiple\ncameras and across different sensor modalities. This is done by fusing object\nproposals across sensors accurately and efficiently. Second, the objects of\ninterest (targets) are tracked directly in the real world. This is a departure\nfrom traditional techniques where objects are simply tracked in the image\nplane. Doing so allows the tracks to be readily used by an autonomous agent for\nnavigation and related tasks.\n  To verify the effectiveness of our approach, we test it on real world highway\ndata collected from a heavily sensorized testbed capable of capturing\nfull-surround information. We demonstrate that our framework is well-suited to\ntrack objects through entire maneuvers around the ego-vehicle, some of which\ntake more than a few minutes to complete. We also leverage the modularity of\nour approach by comparing the effects of including/excluding different sensors,\nchanging the total number of sensors, and the quality of object proposals on\nthe final tracking result.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 22:48:29 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 23:45:52 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 19:50:46 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2019 18:55:42 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1802.08774", "submitter": "Amy Jin", "authors": "Amy Jin, Serena Yeung, Jeffrey Jopling, Jonathan Krause, Dan Azagury,\n  Arnold Milstein, Li Fei-Fei", "title": "Tool Detection and Operative Skill Assessment in Surgical Videos Using\n  Region-Based Convolutional Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1806.02031 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Five billion people in the world lack access to quality surgical care.\nSurgeon skill varies dramatically, and many surgical patients suffer\ncomplications and avoidable harm. Improving surgical training and feedback\nwould help to reduce the rate of complications, half of which have been shown\nto be preventable. To do this, it is essential to assess operative skill, a\nprocess that currently requires experts and is manual, time consuming, and\nsubjective. In this work, we introduce an approach to automatically assess\nsurgeon performance by tracking and analyzing tool movements in surgical\nvideos, leveraging region-based convolutional neural networks. In order to\nstudy this problem, we also introduce a new dataset, m2cai16-tool-locations,\nwhich extends the m2cai16-tool dataset with spatial bounds of tools. While\nprevious methods have addressed tool presence detection, ours is the first to\nnot only detect presence but also spatially localize surgical tools in\nreal-world laparoscopic surgical videos. We show that our method both\neffectively detects the spatial bounds of tools as well as significantly\noutperforms existing methods on tool presence detection. We further demonstrate\nthe ability of our method to assess surgical quality through analysis of tool\nusage patterns, movement range, and economy of motion.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 00:55:34 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 00:54:59 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Jin", "Amy", ""], ["Yeung", "Serena", ""], ["Jopling", "Jeffrey", ""], ["Krause", "Jonathan", ""], ["Azagury", "Dan", ""], ["Milstein", "Arnold", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1802.08781", "submitter": "Ligang Zhang", "authors": "Ligang Zhang, Brijesh Verma", "title": "Superpixel based Class-Semantic Texton Occurrences for Natural Roadside\n  Vegetation Segmentation", "comments": "This is a pre-print of an article published in Machine Vision and\n  Applications. The final authenticated version is available online at:\n  https://doi.org/10.1007/s00138-017-0833-7", "journal-ref": "Machine Vision and Applications (2017) 28: 293", "doi": "10.1007/s00138-017-0833-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vegetation segmentation from roadside data is a field that has received\nrelatively little attention in present studies, but can be of great potentials\nin a wide range of real-world applications, such as road safety assessment and\nvegetation condition monitoring. In this paper, we present a novel approach\nthat generates class-semantic color-texture textons and aggregates superpixel\nbased texton occurrences for vegetation segmentation in natural roadside\nimages. Pixel-level class-semantic textons are first learnt by generating two\nindividual sets of bag-of-word visual dictionaries from color and filter-bank\ntexture features separately for each object class using manually cropped\ntraining data. For a testing image, it is first oversegmented into a set of\nhomogeneous superpixels. The color and texture features of all pixels in each\nsuperpixel are extracted and further mapped to one of the learnt textons using\nthe nearest distance metric, resulting in a color and a texture texton\noccurrence matrix. The color and texture texton occurrences are aggregated\nusing a linear mixing method over each superpixel and the segmentation is\nfinally achieved using a simple yet effective majority voting strategy.\nEvaluations on two public image datasets from videos collected by the\nDepartment of Transport and Main Roads (DTMR), Queensland, Australia, and a\npublic roadside grass dataset show high accuracy of the proposed approach. We\nalso demonstrate the effectiveness of the approach for vegetation segmentation\nin real-world scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 01:51:41 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhang", "Ligang", ""], ["Verma", "Brijesh", ""]]}, {"id": "1802.08784", "submitter": "Ligang Zhang", "authors": "Ligang Zhang, Brijesh Verma, Dian Tjondronegoro, Vinod Chandran", "title": "Facial Expression Analysis under Partial Occlusion: A Survey", "comments": "Authors pre-print of the article accepted for publication in ACM\n  Computing Surveys (accepted on 02-Nov-2017)", "journal-ref": "ACM Computing Surveys 51, 2, Article 25 (February 2018)", "doi": "10.1145/3158369", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic machine-based Facial Expression Analysis (FEA) has made substantial\nprogress in the past few decades driven by its importance for applications in\npsychology, security, health, entertainment and human computer interaction. The\nvast majority of completed FEA studies are based on non-occluded faces\ncollected in a controlled laboratory environment. Automatic expression\nrecognition tolerant to partial occlusion remains less understood, particularly\nin real-world scenarios. In recent years, efforts investigating techniques to\nhandle partial occlusion for FEA have seen an increase. The context is right\nfor a comprehensive perspective of these developments and the state of the art\nfrom this perspective. This survey provides such a comprehensive review of\nrecent advances in dataset creation, algorithm development, and investigations\nof the effects of occlusion critical for robust performance in FEA systems. It\noutlines existing challenges in overcoming partial occlusion and discusses\npossible opportunities in advancing the technology. To the best of our\nknowledge, it is the first FEA survey dedicated to occlusion and aimed at\npromoting better informed and benchmarked future work.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 02:01:22 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhang", "Ligang", ""], ["Verma", "Brijesh", ""], ["Tjondronegoro", "Dian", ""], ["Chandran", "Vinod", ""]]}, {"id": "1802.08790", "submitter": "Ligang Zhang", "authors": "Ligang Zhang, Brijesh Verma, David Stockwell, Sujan Chowdhury", "title": "Spatially Constrained Location Prior for Scene Parsing", "comments": "authors' pre-print version of a article published in IJCNN 2016", "journal-ref": "International Joint Conference on Neural Networks (IJCNN), 2016,\n  pp. 1480-1486", "doi": "10.1109/IJCNN.2016.7727373", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic context is an important and useful cue for scene parsing in\ncomplicated natural images with a substantial amount of variations in objects\nand the environment. This paper proposes Spatially Constrained Location Prior\n(SCLP) for effective modelling of global and local semantic context in the\nscene in terms of inter-class spatial relationships. Unlike existing studies\nfocusing on either relative or absolute location prior of objects, the SCLP\neffectively incorporates both relative and absolute location priors by\ncalculating object co-occurrence frequencies in spatially constrained image\nblocks. The SCLP is general and can be used in conjunction with various visual\nfeature-based prediction models, such as Artificial Neural Networks and Support\nVector Machine (SVM), to enforce spatial contextual constraints on class\nlabels. Using SVM classifiers and a linear regression model, we demonstrate\nthat the incorporation of SCLP achieves superior performance compared to the\nstate-of-the-art methods on the Stanford background and SIFT Flow datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 03:21:01 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhang", "Ligang", ""], ["Verma", "Brijesh", ""], ["Stockwell", "David", ""], ["Chowdhury", "Sujan", ""]]}, {"id": "1802.08793", "submitter": "Qian Huang", "authors": "Qian Huang, Weixin Zhu, Yang Zhao, Linsen Chen, Yao Wang, Tao Yue, Xun\n  Cao", "title": "Multispectral Image Intrinsic Decomposition via Low Rank Constraint", "comments": "Proceedings of the IEEE International Conference on Computer Vision\n  and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral images contain many clues of surface characteristics of the\nobjects, thus can be widely used in many computer vision tasks, e.g.,\nrecolorization and segmentation. However, due to the complex illumination and\nthe geometry structure of natural scenes, the spectra curves of a same surface\ncan look very different. In this paper, a Low Rank Multispectral Image\nIntrinsic Decomposition model (LRIID) is presented to decompose the shading and\nreflectance from a single multispectral image. We extend the Retinex model,\nwhich is proposed for RGB image intrinsic decomposition, for multispectral\ndomain. Based on this, a low rank constraint is proposed to reduce the\nill-posedness of the problem and make the algorithm solvable. A dataset of 12\nimages is given with the ground truth of shadings and reflectance, so that the\nobjective evaluations can be conducted. The experiments demonstrate the\neffectiveness of proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 04:04:39 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Huang", "Qian", ""], ["Zhu", "Weixin", ""], ["Zhao", "Yang", ""], ["Chen", "Linsen", ""], ["Wang", "Yao", ""], ["Yue", "Tao", ""], ["Cao", "Xun", ""]]}, {"id": "1802.08795", "submitter": "Nina Narodytska", "authors": "Svyatoslav Korneev, Nina Narodytska, Luca Pulina, Armando Tacchella,\n  Nikolaj Bjorner, Mooly Sagiv", "title": "Constrained Image Generation Using Binarized Neural Networks with\n  Decision Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of binary image generation with given properties.\nThis problem arises in a number of practical applications, including generation\nof artificial porous medium for an electrode of lithium-ion batteries, for\ncomposed materials, etc. A generated image represents a porous medium and, as\nsuch, it is subject to two sets of constraints: topological constraints on the\nstructure and process constraints on the physical process over this structure.\nTo perform image generation we need to define a mapping from a porous medium to\nits physical process parameters. For a given geometry of a porous medium, this\nmapping can be done by solving a partial differential equation (PDE). However,\nembedding a PDE solver into the search procedure is computationally expensive.\nWe use a binarized neural network to approximate a PDE solver. This allows us\nto encode the entire problem as a logical formula. Our main contribution is\nthat, for the first time, we show that this problem can be tackled using\ndecision procedures. Our experiments show that our model is able to produce\nrandom constrained images that satisfy both topological and process\nconstraints.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 04:12:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Korneev", "Svyatoslav", ""], ["Narodytska", "Nina", ""], ["Pulina", "Luca", ""], ["Tacchella", "Armando", ""], ["Bjorner", "Nikolaj", ""], ["Sagiv", "Mooly", ""]]}, {"id": "1802.08797", "submitter": "Yulun Zhang", "authors": "Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu", "title": "Residual Dense Network for Image Super-Resolution", "comments": "To appear in CVPR 2018 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very deep convolutional neural network (CNN) has recently achieved great\nsuccess for image super-resolution (SR) and offered hierarchical features as\nwell. However, most deep CNN based SR models do not make full use of the\nhierarchical features from the original low-resolution (LR) images, thereby\nachieving relatively-low performance. In this paper, we propose a novel\nresidual dense network (RDN) to address this problem in image SR. We fully\nexploit the hierarchical features from all the convolutional layers.\nSpecifically, we propose residual dense block (RDB) to extract abundant local\nfeatures via dense connected convolutional layers. RDB further allows direct\nconnections from the state of preceding RDB to all the layers of current RDB,\nleading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is\nthen used to adaptively learn more effective features from preceding and\ncurrent local features and stabilizes the training of wider network. After\nfully obtaining dense local features, we use global feature fusion to jointly\nand adaptively learn global hierarchical features in a holistic way. Extensive\nexperiments on benchmark datasets with different degradation models show that\nour RDN achieves favorable performance against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 04:40:06 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 02:53:58 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Yulun", ""], ["Tian", "Yapeng", ""], ["Kong", "Yu", ""], ["Zhong", "Bineng", ""], ["Fu", "Yun", ""]]}, {"id": "1802.08808", "submitter": "Yanting Hu", "authors": "Yanting Hu, Xinbo Gao, Jie Li, Yuanfei Huang, and Hanzi Wang", "title": "Single Image Super-Resolution via Cascaded Multi-Scale Cross Network", "comments": "12 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep convolutional neural networks have achieved significant improvements\nin accuracy and speed for single image super-resolution. However, as the depth\nof network grows, the information flow is weakened and the training becomes\nharder and harder. On the other hand, most of the models adopt a single-stream\nstructure with which integrating complementary contextual information under\ndifferent receptive fields is difficult. To improve information flow and to\ncapture sufficient knowledge for reconstructing the high-frequency details, we\npropose a cascaded multi-scale cross network (CMSC) in which a sequence of\nsubnetworks is cascaded to infer high resolution features in a coarse-to-fine\nmanner. In each cascaded subnetwork, we stack multiple multi-scale cross (MSC)\nmodules to fuse complementary multi-scale information in an efficient way as\nwell as to improve information flow across the layers. Meanwhile, by\nintroducing residual-features learning in each stage, the relative information\nbetween high-resolution and low-resolution features is fully utilized to\nfurther boost reconstruction performance. We train the proposed network with\ncascaded-supervision and then assemble the intermediate predictions of the\ncascade to achieve high quality image reconstruction. Extensive quantitative\nand qualitative evaluations on benchmark datasets illustrate the superiority of\nour proposed method over state-of-the-art super-resolution methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 06:16:34 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Hu", "Yanting", ""], ["Gao", "Xinbo", ""], ["Li", "Jie", ""], ["Huang", "Yuanfei", ""], ["Wang", "Hanzi", ""]]}, {"id": "1802.08817", "submitter": "Chong Luo", "authors": "Anfeng He, Chong Luo, Xinmei Tian, Wenjun Zeng", "title": "A Twofold Siamese Network for Real-Time Object Tracking", "comments": "Accepted by CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing that Semantic features learned in an image classification task and\nAppearance features learned in a similarity matching task complement each\nother, we build a twofold Siamese network, named SA-Siam, for real-time object\ntracking. SA-Siam is composed of a semantic branch and an appearance branch.\nEach branch is a similarity-learning Siamese network. An important design\nchoice in SA-Siam is to separately train the two branches to keep the\nheterogeneity of the two types of features. In addition, we propose a channel\nattention mechanism for the semantic branch. Channel-wise weights are computed\naccording to the channel activations around the target position. While the\ninherited architecture from SiamFC \\cite{SiamFC} allows our tracker to operate\nbeyond real-time, the twofold design and the attention mechanism significantly\nimprove the tracking performance. The proposed SA-Siam outperforms all other\nreal-time trackers by a large margin on OTB-2013/50/100 benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 07:57:52 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["He", "Anfeng", ""], ["Luo", "Chong", ""], ["Tian", "Xinmei", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1802.08831", "submitter": "Mai Zhu", "authors": "Mai Zhu, Bo Chang, Chong Fu", "title": "Convolutional Neural Networks combined with Runge-Kutta Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A convolutional neural network for image classification can be constructed\nmathematically since it can be regarded as a multi-period dynamical system. In\nthis paper, a novel approach is proposed to construct network models from the\ndynamical systems view. Since a pre-activation residual network can be deemed\nan approximation of a time-dependent dynamical system using the forward Euler\nmethod, higher order Runge-Kutta methods (RK methods) can be utilized to build\nnetwork models in order to achieve higher accuracy. The model constructed in\nsuch a way is referred to as the Runge-Kutta Convolutional Neural Network\n(RKNet). RK methods also provide an interpretation of Dense Convolutional\nNetworks (DenseNets) and Convolutional Neural Networks with Alternately Updated\nClique (CliqueNets) from the dynamical systems view. The proposed methods are\nevaluated on benchmark datasets: CIFAR-10/100, SVHN and ImageNet. The\nexperimental results are consistent with the theoretical properties of RK\nmethods and support the dynamical systems interpretation. Moreover, the\nexperimental results show that the RKNets are superior to the state-of-the-art\nnetwork models on CIFAR-10 and on par on CIFAR-100, SVHN and ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 10:31:24 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 03:58:18 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 13:36:03 GMT"}, {"version": "v4", "created": "Sat, 14 Apr 2018 08:22:15 GMT"}, {"version": "v5", "created": "Sat, 19 May 2018 02:17:21 GMT"}, {"version": "v6", "created": "Thu, 17 Jan 2019 08:59:44 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Zhu", "Mai", ""], ["Chang", "Bo", ""], ["Fu", "Chong", ""]]}, {"id": "1802.08833", "submitter": "Tatiana Tommasi", "authors": "Gabriele Angeletti, Barbara Caputo, Tatiana Tommasi", "title": "Adaptive Deep Learning through Visual Domain Localization", "comments": "Accepted at ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commercial robot, trained by its manufacturer to recognize a predefined\nnumber and type of objects, might be used in many settings, that will in\ngeneral differ in their illumination conditions, background, type and degree of\nclutter, and so on. Recent computer vision works tackle this generalization\nissue through domain adaptation methods, assuming as source the visual domain\nwhere the system is trained and as target the domain of deployment. All\napproaches assume to have access to images from all classes of the target\nduring training, an unrealistic condition in robotics applications. We address\nthis issue proposing an algorithm that takes into account the specific needs of\nrobot vision. Our intuition is that the nature of the domain shift experienced\nmostly in robotics is local. We exploit this through the learning of maps that\nspatially ground the domain and quantify the degree of shift, embedded into an\nend-to-end deep domain adaptation architecture. By explicitly localizing the\nroots of the domain shift we significantly reduce the number of parameters of\nthe architecture to tune, we gain the flexibility necessary to deal with subset\nof categories in the target domain at training time, and we provide a clear\nfeedback on the rationale behind any classification decision, which can be\nexploited in human-robot interactions. Experiments on two different settings of\nthe iCub World database confirm the suitability of our method for robot vision.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 10:36:51 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Angeletti", "Gabriele", ""], ["Caputo", "Barbara", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "1802.08872", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz, Nathan B. Jacobs, Marco A. Contreras, and Chase H. Clark", "title": "Deep learning for conifer/deciduous classification of airborne LiDAR 3D\n  point clouds representing individual trees", "comments": "Under review as of the date of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study was to investigate the use of deep learning for\nconiferous/deciduous classification of individual trees from airborne LiDAR\ndata. To enable efficient processing by a deep convolutional neural network\n(CNN), we designed two discrete representations using leaf-off and leaf-on\nLiDAR data: a digital surface model with four channels (DSMx4) and a set of\nfour 2D views (4x2D). A training dataset of labeled tree crowns was generated\nvia segmentation of tree crowns, followed by co-registration with field data.\nPotential mislabels due to GPS error or tree leaning were corrected using a\nstatistical ensemble filtering procedure. Because the training data was heavily\nunbalanced (~8% conifers), we trained an ensemble of CNNs on random balanced\nsub-samples of augmented data (180 rotational variations per instance). The\n4x2D representation yielded similar classification accuracies to the DSMx4\nrepresentation (~82% coniferous and ~90% deciduous) while converging faster.\nThe data augmentation improved the classification accuracies, but more real\ntraining instances (especially coniferous) likely results in much stronger\nimprovements. Leaf-off LiDAR data were the primary source of useful\ninformation, which is likely due to the perennial nature of coniferous foliage.\nLiDAR intensity values also proved to be useful, but normalization yielded no\nsignificant improvements. Lastly, the classification accuracies of overstory\ntrees (~90%) were more balanced than those of understory trees (~90% deciduous\nand ~65% coniferous), which is likely due to the incomplete capture of\nunderstory tree crowns via airborne LiDAR. Automatic derivation of optimal\nfeatures via deep learning provide the opportunity for remarkable improvements\nin prediction tasks where captured data are not friendly to human visual system\n- likely yielding sub-optimal human-designed features.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 16:10:39 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Hamraz", "Hamid", ""], ["Jacobs", "Nathan B.", ""], ["Contreras", "Marco A.", ""], ["Clark", "Chase H.", ""]]}, {"id": "1802.08894", "submitter": "Gabriele Partel", "authors": "Gabriele Partel (1), Giorgia Milli (2) and Carolina W\\\"ahlby ((1)\n  Centre for Image Analysis, Uppsala University, Sweden, (2) Politecnico di\n  Torino, Italy)", "title": "Improving Recall of In Situ Sequencing by Self-Learned Features and a\n  Graphical Model", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based sequencing of mRNA makes it possible to see where in a tissue\nsample a given gene is active, and thus discern large numbers of different cell\ntypes in parallel. This is crucial for gaining a better understanding of tissue\ndevelopment and disease such as cancer. Signals are collected over multiple\nstaining and imaging cycles, and signal density together with noise makes\nsignal decoding challenging. Previous approaches have led to low signal recall\nin efforts to maintain high sensitivity. We propose an approach where signal\ncandidates are generously included, and true-signal probability at the cycle\nlevel is self-learned using a convolutional neural network. Signal candidates\nand probability predictions are thereafter fed into a graphical model searching\nfor signal candidates across sequencing cycles. The graphical model combines\nintensity, probability and spatial distance to find optimal paths representing\ndecoded signal sequences. We evaluate our approach in relation to\nstate-of-the-art, and show that we increase recall by $27\\%$ at maintained\nsensitivity. Furthermore, visual examination shows that most of the now\ncorrectly resolved signals were previously lost due to high signal density.\nThus, the proposed approach has the potential to significantly improve further\nanalysis of spatial statistics in in situ sequencing experiments.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 18:53:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Partel", "Gabriele", ""], ["Milli", "Giorgia", ""], ["W\u00e4hlby", "Carolina", ""]]}, {"id": "1802.08909", "submitter": "Sunrita Poddar", "authors": "Sunrita Poddar, Yasir Mohsin, Deidra Ansah, Bijoy Thattaliyath, Ravi\n  Ashwath, Mathews Jacob", "title": "Free-breathing cardiac MRI using bandlimited manifold modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel bandlimited manifold framework and an algorithm to\nrecover freebreathing and ungated cardiac MR images from highly undersampled\nmeasurements. The image frames in the free breathing and ungated dataset are\nassumed to be points on a bandlimited manifold. We introduce a novel kernel\nlow-rank algorithm to estimate the manifold structure (Laplacian) from a\nnavigator-based acquisition scheme. The structure of the manifold is then used\nto recover the images from highly undersampled measurements. A computationally\nefficient algorithm, which relies on the bandlimited approximation of the\nLaplacian matrix, is used to recover the images. The proposed scheme is\ndemonstrated on several patients with different breathing patterns and cardiac\nrates, without requiring the need for manually tuning the reconstruction\nparameters in each case. The proposed scheme enabled the recovery of\nfree-breathing and ungated data, providing reconstructions that are\nqualitatively similar to breath-held scans performed on the same patients. This\nshows the potential of the technique as a clinical protocol for free-breathing\ncardiac scans.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 20:43:23 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Poddar", "Sunrita", ""], ["Mohsin", "Yasir", ""], ["Ansah", "Deidra", ""], ["Thattaliyath", "Bijoy", ""], ["Ashwath", "Ravi", ""], ["Jacob", "Mathews", ""]]}, {"id": "1802.08925", "submitter": "Aaron Lee", "authors": "Cecilia S. Lee, Ariel J. Tyring, Yue Wu, Sa Xiao, Ariel S. Rokem,\n  Nicolaas P. Deruyter, Qinqin Zhang, Adnan Tufail, Ruikang K. Wang, Aaron Y.\n  Lee", "title": "Generating retinal flow maps from structural optical coherence\n  tomography with artificial intelligence", "comments": "Under revision at Nature Communications. Submitted on June 5th 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in artificial intelligence (AI) for computer\nvision, its application in medical imaging has been limited by the burden and\nlimits of expert-generated labels. We used images from optical coherence\ntomography angiography (OCTA), a relatively new imaging modality that measures\nperfusion of the retinal vasculature, to train an AI algorithm to generate\nvasculature maps from standard structural optical coherence tomography (OCT)\nimages of the same retinae, both exceeding the ability and bypassing the need\nfor expert labeling. Deep learning was able to infer perfusion of\nmicrovasculature from structural OCT images with similar fidelity to OCTA and\nsignificantly better than expert clinicians (P < 0.00001). OCTA suffers from\nneed of specialized hardware, laborious acquisition protocols, and motion\nartifacts; whereas our model works directly from standard OCT which are\nubiquitous and quick to obtain, and allows unlocking of large volumes of\npreviously collected standard OCT data both in existing clinical trials and\nclinical practice. This finding demonstrates a novel application of AI to\nmedical imaging, whereby subtle regularities between different modalities are\nused to image the same body part and AI is used to generate detailed and\naccurate inferences of tissue function from structure imaging.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 22:51:43 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Lee", "Cecilia S.", ""], ["Tyring", "Ariel J.", ""], ["Wu", "Yue", ""], ["Xiao", "Sa", ""], ["Rokem", "Ariel S.", ""], ["Deruyter", "Nicolaas P.", ""], ["Zhang", "Qinqin", ""], ["Tufail", "Adnan", ""], ["Wang", "Ruikang K.", ""], ["Lee", "Aaron Y.", ""]]}, {"id": "1802.08936", "submitter": "Ryan Szeto", "authors": "Ryan Szeto, Simon Stent, German Ros, Jason J. Corso", "title": "A Dataset To Evaluate The Representations Learned By Video Prediction\n  Models", "comments": "Accepted to ICLR 2018 Workshop Track. Fixed Figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. Using several instantiations\nof the dataset in which variation is explicitly controlled, we highlight issues\nin an existing state-of-the-art approach and propose the use of a performance\nmetric with greater semantic meaning to improve experimental interpretability.\nOur dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols .\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 01:01:17 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 13:06:40 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 00:45:02 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Szeto", "Ryan", ""], ["Stent", "Simon", ""], ["Ros", "German", ""], ["Corso", "Jason J.", ""]]}, {"id": "1802.08937", "submitter": "Xinye Zheng", "authors": "Xinye Zheng, Jianbo Ye, Yukun Chen, Stephen Wistar, Jia Li, Jose A.\n  Piedra-Fern\\'andez, Michael A. Steinberg, James Z. Wang", "title": "Detecting Comma-shaped Clouds for Severe Weather Forecasting using Shape\n  and Motion", "comments": "Under submission", "journal-ref": null, "doi": "10.1109/TGRS.2018.2887206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meteorologists use shapes and movements of clouds in satellite images as\nindicators of several major types of severe storms. Satellite imaginary data\nare in increasingly higher resolution, both spatially and temporally, making it\nimpossible for humans to fully leverage the data in their forecast. Automatic\nsatellite imagery analysis methods that can find storm-related cloud patterns\nas soon as they are detectable are in demand. We propose a machine learning and\npattern recognition based approach to detect \"comma-shaped\" clouds in satellite\nimages, which are specific cloud distribution patterns strongly associated with\nthe cyclone formulation. In order to detect regions with the targeted movement\npatterns, our method is trained on manually annotated cloud examples\nrepresented by both shape and motion-sensitive features. Sliding windows in\ndifferent scales are used to ensure that dense clouds will be captured, and we\nimplement effective selection rules to shrink the region of interest among\nthese sliding windows. Finally, we evaluate the method on a hold-out annotated\ncomma-shaped cloud dataset and cross-match the results with recorded storm\nevents in the severe weather database. The validated utility and accuracy of\nour method suggest a high potential for assisting meteorologists in weather\nforecasting.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 01:15:47 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 01:50:08 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 18:27:28 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zheng", "Xinye", ""], ["Ye", "Jianbo", ""], ["Chen", "Yukun", ""], ["Wistar", "Stephen", ""], ["Li", "Jia", ""], ["Piedra-Fern\u00e1ndez", "Jose A.", ""], ["Steinberg", "Michael A.", ""], ["Wang", "James Z.", ""]]}, {"id": "1802.08948", "submitter": "Pengyuan Lyu", "authors": "Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, Xiang Bai", "title": "Multi-Oriented Scene Text Detection via Corner Localization and Region\n  Segmentation", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous deep learning based state-of-the-art scene text detection methods\ncan be roughly classified into two categories. The first category treats scene\ntext as a type of general objects and follows general object detection paradigm\nto localize scene text by regressing the text box locations, but troubled by\nthe arbitrary-orientation and large aspect ratios of scene text. The second one\nsegments text regions directly, but mostly needs complex post processing. In\nthis paper, we present a method that combines the ideas of the two types of\nmethods while avoiding their shortcomings. We propose to detect scene text by\nlocalizing corner points of text bounding boxes and segmenting text regions in\nrelative positions. In inference stage, candidate boxes are generated by\nsampling and grouping corner points, which are further scored by segmentation\nmaps and suppressed by NMS. Compared with previous methods, our method can\nhandle long oriented text naturally and doesn't need complex post processing.\nThe experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text\ndemonstrate that the proposed algorithm achieves better or comparable results\nin both accuracy and efficiency. Based on VGG16, it achieves an F-measure of\n84.3% on ICDAR2015 and 81.5% on MSRA-TD500.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 03:34:12 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 03:50:15 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Lyu", "Pengyuan", ""], ["Yao", "Cong", ""], ["Wu", "Wenhao", ""], ["Yan", "Shuicheng", ""], ["Bai", "Xiang", ""]]}, {"id": "1802.08960", "submitter": "Andres Milioto", "authors": "Andres Milioto, Cyrill Stachniss", "title": "Bonnet: An Open-Source Training and Deployment Framework for Semantic\n  Segmentation in Robotics using CNNs", "comments": "To be published in to IEEE International Conference on Robotics and\n  Automation 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to interpret a scene is an important capability for a robot that\nis supposed to interact with its environment. The knowledge of what is in front\nof the robot is, for example, relevant for navigation, manipulation, or\nplanning. Semantic segmentation labels each pixel of an image with a class\nlabel and thus provides a detailed semantic annotation of the surroundings to\nthe robot. Convolutional neural networks (CNNs) are popular methods for\naddressing this type of problem. The available software for training and the\nintegration of CNNs for real robots, however, is quite fragmented and often\ndifficult to use for non-experts, despite the availability of several\nhigh-quality open-source frameworks for neural network implementation and\ntraining. In this paper, we propose a tool called Bonnet, which addresses this\nfragmentation problem by building a higher abstraction that is specific for the\nsemantic segmentation task. It provides a modular approach to simplify the\ntraining of a semantic segmentation CNN independently of the used dataset and\nthe intended task. Furthermore, we also address the deployment on a real\nrobotic platform. Thus, we do not propose a new CNN approach in this paper.\nInstead, we provide a stable and easy-to-use tool to make this technology more\napproachable in the context of autonomous systems. In this sense, we aim at\nclosing a gap between computer vision research and its use in robotics\nresearch. We provide an open-source codebase for training and deployment. The\ntraining interface is implemented in Python using TensorFlow and the deployment\ninterface provides a C++ library that can be easily integrated in an existing\nrobotics codebase, a ROS node, and two standalone applications for label\nprediction in images and videos.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 06:47:30 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 17:08:34 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Milioto", "Andres", ""], ["Stachniss", "Cyrill", ""]]}, {"id": "1802.09026", "submitter": "Xiaoxiang Zhu", "authors": "Jian Kang, Marco K\\\"orner, Yuanyuan Wang, Hannes Taubenb\\\"ock, Xiao\n  Xiang Zhu", "title": "Building Instance Classification Using Street View Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2018.02.006", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land-use classification based on spaceborne or aerial remote sensing images\nhas been extensively studied over the past decades. Such classification is\nusually a patch-wise or pixel-wise labeling over the whole image. But for many\napplications, such as urban population density mapping or urban utility\nplanning, a classification map based on individual buildings is much more\ninformative. However, such semantic classification still poses some fundamental\nchallenges, for example, how to retrieve fine boundaries of individual\nbuildings. In this paper, we proposed a general framework for classifying the\nfunctionality of individual buildings. The proposed method is based on\nConvolutional Neural Networks (CNNs) which classify facade structures from\nstreet view images, such as Google StreetView, in addition to remote sensing\nimages which usually only show roof structures. Geographic information was\nutilized to mask out individual buildings, and to associate the corresponding\nstreet view images. We created a benchmark dataset which was used for training\nand evaluating CNNs. In addition, the method was applied to generate building\nclassification maps on both region and city scales of several cities in Canada\nand the US. Keywords: CNN, Building instance classification, Street view\nimages, OpenStreetMap\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 16:04:34 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kang", "Jian", ""], ["K\u00f6rner", "Marco", ""], ["Wang", "Yuanyuan", ""], ["Taubenb\u00f6ck", "Hannes", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1802.09052", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Yifan Sun and Brian Eriksson and Wenlin Wang and Vaneet\n  Aggarwal", "title": "Wide Compression: Tensor Ring Nets", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated state-of-the-art performance in a\nvariety of real-world applications. In order to obtain performance gains, these\nnetworks have grown larger and deeper, containing millions or even billions of\nparameters and over a thousand layers. The trade-off is that these large\narchitectures require an enormous amount of memory, storage, and computation,\nthus limiting their usability. Inspired by the recent tensor ring\nfactorization, we introduce Tensor Ring Networks (TR-Nets), which significantly\ncompress both the fully connected layers and the convolutional layers of deep\nneural networks. Our results show that our TR-Nets approach {is able to\ncompress LeNet-5 by $11\\times$ without losing accuracy}, and can compress the\nstate-of-the-art Wide ResNet by $243\\times$ with only 2.3\\% degradation in\n{Cifar10 image classification}. Overall, this compression scheme shows promise\nin scientific computing and deep learning, especially for emerging\nresource-constrained devices such as smartphones, wearables, and IoT devices.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:09:04 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wang", "Wenqi", ""], ["Sun", "Yifan", ""], ["Eriksson", "Brian", ""], ["Wang", "Wenlin", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1802.09058", "submitter": "Chenchen Zhu", "authors": "Chenchen Zhu, Ran Tao, Khoa Luu, Marios Savvides", "title": "Seeing Small Faces from Robust Anchor's Perspective", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel anchor design to support anchor-based face\ndetection for superior scale-invariant performance, especially on tiny faces.\nTo achieve this, we explicitly address the problem that anchor-based detectors\ndrop performance drastically on faces with tiny sizes, e.g. less than 16x16\npixels. In this paper, we investigate why this is the case. We discover that\ncurrent anchor design cannot guarantee high overlaps between tiny faces and\nanchor boxes, which increases the difficulty of training. The new Expected Max\nOverlapping (EMO) score is proposed which can theoretically explain the low\noverlapping issue and inspire several effective strategies of new anchor design\nleading to higher face overlaps, including anchor stride reduction with new\nnetwork architectures, extra shifted anchors, and stochastic face shifting.\nComprehensive experiments show that our proposed method significantly\noutperforms the baseline anchor-based detector, while consistently achieving\nstate-of-the-art results on challenging face detection datasets with\ncompetitive runtime speed.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:48:53 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhu", "Chenchen", ""], ["Tao", "Ran", ""], ["Luu", "Khoa", ""], ["Savvides", "Marios", ""]]}, {"id": "1802.09070", "submitter": "Dimitris Kastaniotis", "authors": "Dimitris Kastaniotis, Ioanna Ntinou, Dimitrios Tsourounis, George\n  Economou and Spiros Fotopoulos", "title": "Attention-Aware Generative Adversarial Networks (ATA-GANs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a novel approach for training Generative Adversarial\nNetworks (GANs). Using the attention maps produced by a Teacher- Network we are\nable to improve the quality of the generated images as well as perform weakly\nobject localization on the generated images. To this end, we generate images of\nHEp-2 cells captured with Indirect Imunofluoresence (IIF) and study the ability\nof our network to perform a weakly localization of the cell. Firstly, we\ndemonstrate that whilst GANs can learn the mapping between the input domain and\nthe target distribution efficiently, the discriminator network is not able to\ndetect the regions of interest. Secondly, we present a novel attention transfer\nmechanism which allows us to enforce the discriminator to put emphasis on the\nregions of interest via transfer learning. Thirdly, we show that this leads to\nmore realistic images, as the discriminator learns to put emphasis on the area\nof interest. Fourthly, the proposed method allows one to generate both images\nas well as attention maps which can be useful for data annotation e.g in object\ndetection.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 19:40:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Kastaniotis", "Dimitris", ""], ["Ntinou", "Ioanna", ""], ["Tsourounis", "Dimitrios", ""], ["Economou", "George", ""], ["Fotopoulos", "Spiros", ""]]}, {"id": "1802.09088", "submitter": "Ehsan Adeli", "authors": "Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, Ehsan Adeli", "title": "Adversarially Learned One-Class Classifier for Novelty Detection", "comments": "CVPR 2018 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is the process of identifying the observation(s) that\ndiffer in some respect from the training observations (the target class). In\nreality, the novelty class is often absent during training, poorly sampled or\nnot well defined. Therefore, one-class classifiers can efficiently model such\nproblems. However, due to the unavailability of data from the novelty class,\ntraining an end-to-end deep network is a cumbersome task. In this paper,\ninspired by the success of generative adversarial networks for training deep\nmodels in unsupervised and semi-supervised settings, we propose an end-to-end\narchitecture for one-class classification. Our architecture is composed of two\ndeep networks, each of which trained by competing with each other while\ncollaborating to understand the underlying concept in the target class, and\nthen classify the testing samples. One network works as the novelty detector,\nwhile the other supports it by enhancing the inlier samples and distorting the\noutliers. The intuition is that the separability of the enhanced inliers and\ndistorted outliers is much better than deciding on the original samples. The\nproposed framework applies to different related applications of anomaly and\noutlier detection in images and videos. The results on MNIST and Caltech-256\nimage datasets, along with the challenging UCSD Ped2 dataset for video anomaly\ndetection illustrate that our proposed method learns the target class\neffectively and is superior to the baseline and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 21:34:27 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 06:41:55 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Khalooei", "Mohammad", ""], ["Fathy", "Mahmood", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1802.09129", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Sibei Yang, Yizhou Yu", "title": "Multi-Evidence Filtering and Fusion for Multi-Label Classification,\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\n  Learning", "comments": "accepted by IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised object detection and semantic segmentation require object or even\npixel level annotations. When there exist image level labels only, it is\nchallenging for weakly supervised algorithms to achieve accurate predictions.\nThe accuracy achieved by top weakly supervised algorithms is still\nsignificantly lower than their fully supervised counterparts. In this paper, we\npropose a novel weakly supervised curriculum learning pipeline for multi-label\nobject recognition, detection and semantic segmentation. In this pipeline, we\nfirst obtain intermediate object localization and pixel labeling results for\nthe training images, and then use such results to train task-specific deep\nnetworks in a fully supervised manner. The entire process consists of four\nstages, including object localization in the training images, filtering and\nfusing object instances, pixel labeling for the training images, and\ntask-specific network training. To obtain clean object instances in the\ntraining images, we propose a novel algorithm for filtering, fusing and\nclassifying object instances collected from multiple solution mechanisms. In\nthis algorithm, we incorporate both metric learning and density-based\nclustering to filter detected object instances. Experiments show that our\nweakly supervised pipeline achieves state-of-the-art results in multi-label\nimage classification as well as weakly supervised object detection and very\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\nPASCAL VOC 2007 and PASCAL VOC 2012.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 02:07:19 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ge", "Weifeng", ""], ["Yang", "Sibei", ""], ["Yu", "Yizhou", ""]]}, {"id": "1802.09153", "submitter": "Yukun Ding", "authors": "Jinglan Liu, Jiaxin Zhang, Yukun Ding, Xiaowei Xu, Meng Jiang, Yiyu\n  Shi", "title": "PBGen: Partial Binarization of Deconvolution-Based Generators for Edge\n  Intelligence", "comments": "17 pages, paper re-organized", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the binarization of the deconvolution-based generator in a\nGAN for memory saving and speedup of image construction. Our study suggests\nthat different from convolutional neural networks (including the discriminator)\nwhere all layers can be binarized, only some of the layers in the generator can\nbe binarized without significant performance loss. Supported by theoretical\nanalysis and verified by experiments, a direct metric based on the dimension of\ndeconvolution operations is established, which can be used to quickly decide\nwhich layers in the generator can be binarized. Our results also indicate that\nboth the generator and the discriminator should be binarized simultaneously for\nbalanced competition and better performance. Experimental results based on\nCelebA suggest that directly applying state-of-the-art binarization techniques\nto all the layers of the generator will lead to 2.83$\\times$ performance loss\nmeasured by sliced Wasserstein distance compared with the original generator,\nwhile applying them to selected layers only can yield up to 25.81$\\times$\nsaving in memory consumption, and 1.96$\\times$ and 1.32$\\times$ speedup in\ninference and training respectively with little performance loss.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 03:50:09 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 01:34:52 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 04:15:55 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Liu", "Jinglan", ""], ["Zhang", "Jiaxin", ""], ["Ding", "Yukun", ""], ["Xu", "Xiaowei", ""], ["Jiang", "Meng", ""], ["Shi", "Yiyu", ""]]}, {"id": "1802.09178", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang and Yuanpu Xie and Lin Yang", "title": "Photographic Text-to-Image Synthesis with a Hierarchically-nested\n  Adversarial Network", "comments": "CVPR2018 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method to deal with the challenging task of\ngenerating photographic images conditioned on semantic image descriptions. Our\nmethod introduces accompanying hierarchical-nested adversarial objectives\ninside the network hierarchies, which regularize mid-level representations and\nassist generator training to capture the complex image statistics. We present\nan extensile single-stream generator architecture to better adapt the jointed\ndiscriminators and push generated images up to high resolutions. We adopt a\nmulti-purpose adversarial loss to encourage more effective image and text\ninformation usage in order to improve the semantic consistency and image\nfidelity simultaneously. Furthermore, we introduce a new visual-semantic\nsimilarity measure to evaluate the semantic consistency of generated images.\nWith extensive experimental validation on three public datasets, our method\nsignificantly improves previous state of the arts on all datasets over\ndifferent evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 06:33:32 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 21:05:39 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zhang", "Zizhao", ""], ["Xie", "Yuanpu", ""], ["Yang", "Lin", ""]]}, {"id": "1802.09227", "submitter": "U\\u{g}ur Kart", "authors": "U\\u{g}ur Kart, Joni-Kristian K\\\"am\\\"ar\\\"ainen, Ji\\v{r}\\'i Matas, Lixin\n  Fan, Francesco Cricri", "title": "Depth Masked Discriminative Correlation Filter", "comments": "6 pages, accepted to ICPR 2018. \\copyright 2018 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Depth information provides a strong cue for occlusion detection and handling,\nbut has been largely omitted in generic object tracking until recently due to\nlack of suitable benchmark datasets and applications. In this work, we propose\na Depth Masked Discriminative Correlation Filter (DM-DCF) which adopts novel\ndepth segmentation based occlusion detection that stops correlation filter\nupdating and depth masking which adaptively adjusts the spatial support for\ncorrelation filter. In Princeton RGBD Tracking Benchmark, our DM-DCF is among\nthe state-of-the-art in overall ranking and the winner on multiple categories.\nMoreover, since it is based on DCF, ``DM-DCF`` runs an order of magnitude\nfaster than its competitors making it suitable for time constrained\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 10:00:14 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 11:04:03 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kart", "U\u011fur", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Fan", "Lixin", ""], ["Cricri", "Francesco", ""]]}, {"id": "1802.09232", "submitter": "Diogo Luvizon", "authors": "Diogo C. Luvizon and David Picard and Hedi Tabia", "title": "2D/3D Pose Estimation and Action Recognition using Multitask Deep\n  Learning", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition and human pose estimation are closely related but both\nproblems are generally handled as distinct tasks in the literature. In this\nwork, we propose a multitask framework for jointly 2D and 3D pose estimation\nfrom still images and human action recognition from video sequences. We show\nthat a single architecture can be used to solve the two problems in an\nefficient way and still achieves state-of-the-art results. Additionally, we\ndemonstrate that optimization from end-to-end leads to significantly higher\naccuracy than separated learning. The proposed architecture can be trained with\ndata from different categories simultaneously in a seamlessly way. The reported\nresults on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the\neffectiveness of our method on the targeted tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 10:16:48 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 13:39:45 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Luvizon", "Diogo C.", ""], ["Picard", "David", ""], ["Tabia", "Hedi", ""]]}, {"id": "1802.09261", "submitter": "Dominik Schlegel", "authors": "Dominik Schlegel and Giorgio Grisetti", "title": "HBST: A Hamming Distance embedding Binary Search Tree for Visual Place\n  Recognition", "comments": "Submitted to IEEE Robotics and Automation Letters (RA-L) 2018 with\n  International Conference on Intelligent Robots and Systems (IROS) 2018\n  option, 8 pages, 10 figures", "journal-ref": "IEEE Robotics and Automation Letters (Volume: 3, Issue: 4, Oct.\n  2018, Pages: 3741 - 3748)", "doi": "10.1109/LRA.2018.2856542", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable and efficient Visual Place Recognition is a major building block of\nmodern SLAM systems. Leveraging on our prior work, in this paper we present a\nHamming Distance embedding Binary Search Tree (HBST) approach for binary\nDescriptor Matching and Image Retrieval. HBST allows for descriptor Search and\nInsertion in logarithmic time by exploiting particular properties of binary\nFeature descriptors. We support the idea behind our search structure with a\nthorough analysis on the exploited descriptor properties and their effects on\ncompleteness and complexity of search and insertion. To validate our claims we\nconducted comparative experiments for HBST and several state-of-the-art methods\non a broad range of publicly available datasets. HBST is available as a compact\nopen-source C++ header-only library.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 11:56:11 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 22:37:05 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Schlegel", "Dominik", ""], ["Grisetti", "Giorgio", ""]]}, {"id": "1802.09292", "submitter": "Krishna Murthy Jatavallabhula", "authors": "Parv Parkhiya, Rishabh Khawad, J. Krishna Murthy, Brojeshwar Bhowmick,\n  K. Madhava Krishna", "title": "Constructing Category-Specific Models for Monocular Object-SLAM", "comments": "Accepted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new paradigm for real-time object-oriented SLAM with a monocular\ncamera. Contrary to previous approaches, that rely on object-level models, we\nconstruct category-level models from CAD collections which are now widely\navailable. To alleviate the need for huge amounts of labeled data, we develop a\nrendering pipeline that enables synthesis of large datasets from a limited\namount of manually labeled data. Using data thus synthesized, we learn\ncategory-level models for object deformations in 3D, as well as discriminative\nobject features in 2D. These category models are instance-independent and aid\nin the design of object landmark observations that can be incorporated into a\ngeneric monocular SLAM framework. Where typical object-SLAM approaches usually\nsolve only for object and camera poses, we also estimate object shape\non-the-fly, allowing for a wide range of objects from the category to be\npresent in the scene. Moreover, since our 2D object features are learned\ndiscriminatively, the proposed object-SLAM system succeeds in several scenarios\nwhere sparse feature-based monocular SLAM fails due to insufficient features or\nparallax. Also, the proposed category-models help in object instance retrieval,\nuseful for Augmented Reality (AR) applications. We evaluate the proposed\nframework on multiple challenging real-world scenes and show --- to the best of\nour knowledge --- first results of an instance-independent monocular\nobject-SLAM system and the benefits it enjoys over feature-based SLAM methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 13:42:40 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Parkhiya", "Parv", ""], ["Khawad", "Rishabh", ""], ["Murthy", "J. Krishna", ""], ["Bhowmick", "Brojeshwar", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1802.09298", "submitter": "Jatavallabhula Krishna Murthy", "authors": "Sarthak Sharma, Junaid Ahmed Ansari, J. Krishna Murthy, K. Madhava\n  Krishna", "title": "Beyond Pixels: Leveraging Geometry and Shape Cues for Online\n  Multi-Object Tracking", "comments": "ICRA 2018 paper. Code available at\n  https://github.com/JunaidCS032/MOTBeyondPixels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces geometry and object shape and pose costs for\nmulti-object tracking in urban driving scenarios. Using images from a monocular\ncamera alone, we devise pairwise costs for object tracks, based on several 3D\ncues such as object pose, shape, and motion. The proposed costs are agnostic to\nthe data association method and can be incorporated into any optimization\nframework to output the pairwise data associations. These costs are easy to\nimplement, can be computed in real-time, and complement each other to account\nfor possible errors in a tracking-by-detection framework. We perform an\nextensive analysis of the designed costs and empirically demonstrate consistent\nimprovement over the state-of-the-art under varying conditions that employ a\nrange of object detectors, exhibit a variety in camera and object motions, and,\nmore importantly, are not reliant on the choice of the association framework.\nWe also show that, by using the simplest of associations frameworks (two-frame\nHungarian assignment), we surpass the state-of-the-art in multi-object-tracking\non road scenes. More qualitative and quantitative results can be found at the\nfollowing URL: https://junaidcs032.github.io/Geometry_ObjectShape_MOT/.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 13:53:16 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 10:43:55 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Sharma", "Sarthak", ""], ["Ansari", "Junaid Ahmed", ""], ["Murthy", "J. Krishna", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1802.09381", "submitter": "Jean-Philippe Vert", "authors": "Beyrem Khalfaoui (CBIO), Jean-Philippe Vert (CBIO, DMA)", "title": "DropLasso: A robust variant of Lasso for single cell RNA-seq data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell RNA sequencing (scRNA-seq) is a fast growing approach to measure\nthe genome-wide transcriptome of many individual cells in parallel, but results\nin noisy data with many dropout events. Existing methods to learn molecular\nsignatures from bulk transcriptomic data may therefore not be adapted to\nscRNA-seq data, in order to automatically classify individual cells into\npredefined classes. We propose a new method called DropLasso to learn a\nmolecular signature from scRNA-seq data. DropLasso extends the dropout\nregularisation technique, popular in neural network training, to esti- mate\nsparse linear models. It is well adapted to data corrupted by dropout noise,\nsuch as scRNA-seq data, and we clarify how it relates to elastic net\nregularisation. We provide promising results on simulated and real scRNA-seq\ndata, suggesting that DropLasso may be better adapted than standard regularisa-\ntions to infer molecular signatures from scRNA-seq data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:10:44 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Khalfaoui", "Beyrem", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO, DMA"]]}, {"id": "1802.09384", "submitter": "Sylvie Chambon Dr", "authors": "Hatem A. Rashwan, Sylvie Chambon, Pierre Gurdjos, G\\'eraldine Morin\n  and Vincent Charvillat", "title": "Using Curvilinear Features in Focus for Registering a Single Image to a\n  3D Object", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2911484", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of 2D/3D registration, this paper introduces an approach that\nallows to match features detected in two different modalities: photographs and\n3D models, by using a common 2D reprensentation. More precisely, 2D images are\nmatched with a set of depth images, representing the 3D model. After\nintroducing the concept of curvilinear saliency, related to curvature\nestimation, we propose a new ridge and valley detector for depth images\nrendered from 3D model. A variant of this detector is adapted to photographs,\nin particular by applying it in multi-scale and by combining this feature\ndetector with the principle of focus curves. Finally, a registration algorithm\nfor determining the correct viewpoint of the 3D model and thus the pose is\nproposed. It is based on using histogram of gradients features adapted to the\nfeatures manipulated in 2D and in 3D, and the introduction of repeatability\nscores. The results presented highlight the quality of the features detected,\nin term of repeatability, and also the interest of the approach for\nregistration and pose estimation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:13:54 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Rashwan", "Hatem A.", ""], ["Chambon", "Sylvie", ""], ["Gurdjos", "Pierre", ""], ["Morin", "G\u00e9raldine", ""], ["Charvillat", "Vincent", ""]]}, {"id": "1802.09424", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Nishant Ravikumar, AmirAbbas Davari, Stephan Ellmann,\n  Andreas Maier", "title": "Classification of breast cancer histology images using transfer learning", "comments": "8 pages, Submitted to 15th International Conference on Image Analysis\n  and Recognition (ICAIR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the leading causes of mortality in women. Early\ndetection and treatment are imperative for improving survival rates, which have\nsteadily increased in recent years as a result of more sophisticated\ncomputer-aided-diagnosis (CAD) systems. A critical component of breast cancer\ndiagnosis relies on histopathology, a laborious and highly subjective process.\nConsequently, CAD systems are essential to reduce inter-rater variability and\nsupplement the analyses conducted by specialists. In this paper, a\ntransfer-learning based approach is proposed, for the task of breast histology\nimage classification into four tissue sub-types, namely, normal, benign,\n\\textit{in situ} carcinoma and invasive carcinoma. The histology images,\nprovided as part of the BACH 2018 grand challenge, were first normalized to\ncorrect for color variations resulting from inconsistencies during slide\npreparation. Subsequently, image patches were extracted and used to fine-tune\nGoogle`s Inception-V3 and ResNet50 convolutional neural networks (CNNs), both\npre-trained on the ImageNet database, enabling them to learn domain-specific\nfeatures, necessary to classify the histology images. The ResNet50 network\n(based on residual learning) achieved a test classification accuracy of 97.50%\nfor four classes, outperforming the Inception-V3 network which achieved an\naccuracy of 91.25%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 16:10:40 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Davari", "AmirAbbas", ""], ["Ellmann", "Stephan", ""], ["Maier", "Andreas", ""]]}, {"id": "1802.09431", "submitter": "Can Zhao", "authors": "Can Zhao, Aaron Carass, Blake E. Dewey, and Jerry L. Prince", "title": "Self Super-Resolution for Magnetic Resonance Images using Deep Networks", "comments": "Accepted by IEEE International Symposium on Biomedical Imaging (ISBI)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution magnetic resonance~(MR) imaging~(MRI) is desirable in many\nclinical applications, however, there is a trade-off between resolution, speed\nof acquisition, and noise. It is common for MR images to have worse\nthrough-plane resolution~(slice thickness) than in-plane resolution. In these\nMRI images, high frequency information in the through-plane direction is not\nacquired, and cannot be resolved through interpolation. To address this issue,\nsuper-resolution methods have been developed to enhance spatial resolution. As\nan ill-posed problem, state-of-the-art super-resolution methods rely on the\npresence of external/training atlases to learn the transform from low\nresolution~(LR) images to high resolution~(HR) images. For several reasons,\nsuch HR atlas images are often not available for MRI sequences. This paper\npresents a self super-resolution~(SSR) algorithm, which does not use any\nexternal atlas images, yet can still resolve HR images only reliant on the\nacquired LR image. We use a blurred version of the input image to create\ntraining data for a state-of-the-art super-resolution deep network. The trained\nnetwork is applied to the original input image to estimate the HR image. Our\nSSR result shows a significant improvement on through-plane resolution compared\nto competing SSR methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 16:17:18 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhao", "Can", ""], ["Carass", "Aaron", ""], ["Dewey", "Blake E.", ""], ["Prince", "Jerry L.", ""]]}, {"id": "1802.09567", "submitter": "Rayson Laroca", "authors": "Rayson Laroca, Evair Severo, Luiz A. Zanlorensi, Luiz S. Oliveira,\n  Gabriel Resende Gon\\c{c}alves, William Robson Schwartz, David Menotti", "title": "A Robust Real-Time Automatic License Plate Recognition Based on the YOLO\n  Detector", "comments": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489629", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic License Plate Recognition (ALPR) has been a frequent topic of\nresearch due to many practical applications. However, many of the current\nsolutions are still not robust in real-world situations, commonly depending on\nmany constraints. This paper presents a robust and efficient ALPR system based\non the state-of-the-art YOLO object detector. The Convolutional Neural Networks\n(CNNs) are trained and fine-tuned for each ALPR stage so that they are robust\nunder different conditions (e.g., variations in camera, lighting, and\nbackground). Specially for character segmentation and recognition, we design a\ntwo-stage approach employing simple data augmentation tricks such as inverted\nLicense Plates (LPs) and flipped characters. The resulting ALPR approach\nachieved impressive results in two datasets. First, in the SSIG dataset,\ncomposed of 2,000 frames from 101 vehicle videos, our system achieved a\nrecognition rate of 93.53% and 47 Frames Per Second (FPS), performing better\nthan both Sighthound and OpenALPR commercial systems (89.80% and 93.03%,\nrespectively) and considerably outperforming previous results (81.80%). Second,\ntargeting a more realistic scenario, we introduce a larger public dataset,\ncalled UFPR-ALPR dataset, designed to ALPR. This dataset contains 150 videos\nand 4,500 frames captured when both camera and vehicles are moving and also\ncontains different types of vehicles (cars, motorcycles, buses and trucks). In\nour proposed dataset, the trial versions of commercial systems achieved\nrecognition rates below 70%. On the other hand, our system performed better,\nwith recognition rate of 78.33% and 35 FPS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 19:31:56 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 18:17:09 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 13:19:01 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2018 15:01:50 GMT"}, {"version": "v5", "created": "Tue, 3 Apr 2018 17:50:43 GMT"}, {"version": "v6", "created": "Sat, 28 Apr 2018 12:54:38 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Laroca", "Rayson", ""], ["Severo", "Evair", ""], ["Zanlorensi", "Luiz A.", ""], ["Oliveira", "Luiz S.", ""], ["Gon\u00e7alves", "Gabriel Resende", ""], ["Schwartz", "William Robson", ""], ["Menotti", "David", ""]]}, {"id": "1802.09575", "submitter": "David K\\\"ugler", "authors": "David K\\\"ugler, Jannik Sehring, Andrei Stefanov, Igor Stenin, Julia\n  Kristin, Thomas Klenzner, J\\\"org Schipper, Anirban Mukhopadhyay", "title": "i3PosNet: Instrument Pose Estimation from X-Ray in temporal bone surgery", "comments": "Accepted at International journal of computer assisted radiology and\n  surgery pending publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Accurate estimation of the position and orientation (pose) of\nsurgical instruments is crucial for delicate minimally invasive temporal bone\nsurgery. Current techniques lack in accuracy and/or line-of-sight constraints\n(conventional tracking systems) or expose the patient to prohibitive ionizing\nradiation (intra-operative CT). A possible solution is to capture the\ninstrument with a c-arm at irregular intervals and recover the pose from the\nimage.\n  Methods: i3PosNet infers the position and orientation of instruments from\nimages using a pose estimation network. Said framework considers localized\npatches and outputs pseudo-landmarks. The pose is reconstructed from\npseudo-landmarks by geometric considerations.\n  Results: We show i3PosNet reaches errors less than 0.05mm. It outperforms\nconventional image registration-based approaches reducing average and maximum\nerrors by at least two thirds. i3PosNet trained on synthetic images generalizes\nto real x-rays without any further adaptation.\n  Conclusion: The translation of Deep Learning based methods to surgical\napplications is difficult, because large representative datasets for training\nand testing are not available. This work empirically shows sub-millimeter pose\nestimation trained solely based on synthetic training data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:00:40 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 18:51:15 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["K\u00fcgler", "David", ""], ["Sehring", "Jannik", ""], ["Stefanov", "Andrei", ""], ["Stenin", "Igor", ""], ["Kristin", "Julia", ""], ["Klenzner", "Thomas", ""], ["Schipper", "J\u00f6rg", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "1802.09623", "submitter": "Biao Zhao", "authors": "Biao Zhao, Shigang Yue", "title": "A Resilient Image Matching Method with an Affine Invariant Feature\n  Detector and Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature matching is to seek, localize and identify the similarities\nacross the images. The matched local features between different images can\nindicate the similarities of their content. Resilience of image feature\nmatching to large view point changes is challenging for a lot of applications\nsuch as 3D object reconstruction, object recognition and navigation, etc, which\nneed accurate and robust feature matching from quite different view points. In\nthis paper we propose a novel image feature matching algorithm, integrating our\nprevious proposed Affine Invariant Feature Detector (AIFD) and new proposed\nAffine Invariant Feature Descriptor (AIFDd). Both stages of this new proposed\nalgorithm can provide sufficient resilience to view point changes. With\nsystematic experiments, we can prove that the proposed method of feature\ndetector and descriptor outperforms other state-of-the-art feature matching\nalgorithms especially on view points robustness. It also performs well under\nother conditions such as the change of illumination, rotation and compression,\netc.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 11:36:58 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhao", "Biao", ""], ["Yue", "Shigang", ""]]}, {"id": "1802.09653", "submitter": "Lujo Bauer", "authors": "Mahmood Sharif, Lujo Bauer, and Michael K. Reiter", "title": "On the Suitability of $L_p$-norms for Creating and Preventing\n  Adversarial Examples", "comments": "Appeared in CV-COPS/CVPRW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Much research effort has been devoted to better understanding adversarial\nexamples, which are specially crafted inputs to machine-learning models that\nare perceptually similar to benign inputs, but are classified differently\n(i.e., misclassified). Both algorithms that create adversarial examples and\nstrategies for defending against them typically use $L_p$-norms to measure the\nperceptual similarity between an adversarial input and its benign original.\nPrior work has already shown, however, that two images need not be close to\neach other as measured by an $L_p$-norm to be perceptually similar. In this\nwork, we show that nearness according to an $L_p$-norm is not just unnecessary\nfor perceptual similarity, but is also insufficient. Specifically, focusing on\ndatasets (CIFAR10 and MNIST), $L_p$-norms, and thresholds used in prior work,\nwe show through online user studies that \"adversarial examples\" that are closer\nto their benign counterparts than required by commonly used $L_p$-norm\nthresholds can nevertheless be perceptually different to humans from the\ncorresponding benign examples. Namely, the perceptual distance between two\nimages that are \"near\" each other according to an $L_p$-norm can be high enough\nthat participants frequently classify the two images as representing different\nobjects or digits. Combined with prior work, we thus demonstrate that nearness\nof inputs as measured by $L_p$-norms is neither necessary nor sufficient for\nperceptual similarity, which has implications for both creating and defending\nagainst adversarial examples. We propose and discuss alternative similarity\nmetrics to stimulate future research in the area.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:04:12 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 03:54:59 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 13:48:36 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Sharif", "Mahmood", ""], ["Bauer", "Lujo", ""], ["Reiter", "Michael K.", ""]]}, {"id": "1802.09655", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang and Lin Yang and Yefeng Zheng", "title": "Translating and Segmenting Multimodal Medical Volumes with Cycle- and\n  Shape-Consistency Generative Adversarial Network", "comments": "CVPR2018 (a small revision in v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesized medical images have several important applications, e.g., as an\nintermedium in cross-modality image registration and as supplementary training\nsamples to boost the generalization capability of a classifier. Especially,\nsynthesized computed tomography (CT) data can provide X-ray attenuation map for\nradiation therapy planning. In this work, we propose a generic cross-modality\nsynthesis approach with the following targets: 1) synthesizing realistic\nlooking 3D images using unpaired training data, 2) ensuring consistent\nanatomical structures, which could be changed by geometric distortion in\ncross-modality synthesis and 3) improving volume segmentation by using\nsynthetic data for modalities with limited training samples. We show that these\ngoals can be achieved with an end-to-end 3D convolutional neural network (CNN)\ncomposed of mutually-beneficial generators and segmentors for image synthesis\nand segmentation tasks. The generators are trained with an adversarial loss, a\ncycle-consistency loss, and also a shape-consistency loss, which is supervised\nby segmentors, to reduce the geometric distortion. From the segmentation view,\nthe segmentors are boosted by synthetic data from generators in an online\nmanner. Generators and segmentors prompt each other alternatively in an\nend-to-end training fashion. With extensive experiments on a dataset including\na total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular\nvolumes, we show both tasks are beneficial to each other and coupling these two\ntasks results in better performance than solving them exclusively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:10:13 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 18:19:45 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zhang", "Zizhao", ""], ["Yang", "Lin", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1802.09659", "submitter": "Toru Tamaki", "authors": "Daisuke Ogawa, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda", "title": "Semantic segmentation of trajectories with agent models", "comments": "in Proc of FCV2018, 21/Feb/2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, such as trajectories clustering and classification, we often\ndivide a trajectory into segments as preprocessing. In this paper, we propose a\ntrajectory semantic segmentation method based on learned behavior models. In\nthe proposed method, we learn some behavior models from video sequences. Next,\nusing learned behavior models and a hidden Markov model, we segment a\ntrajectory into semantic segments. Comparing with the Ramer-Douglas-Peucker\nalgorithm, we show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:39:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Ogawa", "Daisuke", ""], ["Tamaki", "Toru", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""]]}, {"id": "1802.09662", "submitter": "Xuefei Zhe", "authors": "Xuefei Zhe, Shifeng Chen, Hong Yan", "title": "Directional Statistics-based Deep Metric Learning for Image\n  Classification and Retrieval", "comments": "codes will come soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep distance metric learning (DDML), which is proposed to learn image\nsimilarity metrics in an end-to-end manner based on the convolution neural\nnetwork, has achieved encouraging results in many computer vision\ntasks.$L2$-normalization in the embedding space has been used to improve the\nperformance of several DDML methods. However, the commonly used Euclidean\ndistance is no longer an accurate metric for $L2$-normalized embedding space,\ni.e., a hyper-sphere. Another challenge of current DDML methods is that their\nloss functions are usually based on rigid data formats, such as the triplet\ntuple. Thus, an extra process is needed to prepare data in specific formats. In\naddition, their losses are obtained from a limited number of samples, which\nleads to a lack of the global view of the embedding space. In this paper, we\nreplace the Euclidean distance with the cosine similarity to better utilize the\n$L2$-normalization, which is able to attenuate the curse of dimensionality.\nMore specifically, a novel loss function based on the von Mises-Fisher\ndistribution is proposed to learn a compact hyper-spherical embedding space.\nMoreover, a new efficient learning algorithm is developed to better capture the\nglobal structure of the embedding space. Experiments for both classification\nand retrieval tasks on several standard datasets show that our method achieves\nstate-of-the-art performance with a simpler training procedure. Furthermore, we\ndemonstrate that, even with a small number of convolutional layers, our model\ncan still obtain significantly better classification performance than the\nwidely used softmax loss.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:54:19 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 02:55:27 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Zhe", "Xuefei", ""], ["Chen", "Shifeng", ""], ["Yan", "Hong", ""]]}, {"id": "1802.09670", "submitter": "Shaobo Fang", "authors": "Shaobo Fang, Zeman Shao, Runyu Mao, Chichen Fu, Deborah A. Kerr, Carol\n  J. Boushey, Edward J. Delp, Fengqing Zhu", "title": "Single-View Food Portion Estimation: Learning Image-to-Energy Mappings\n  Using Generative Adversarial Networks", "comments": "2018 IEEE International Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growing concern of chronic diseases and other health problems\nrelated to diet, there is a need to develop accurate methods to estimate an\nindividual's food and energy intake. Measuring accurate dietary intake is an\nopen research problem. In particular, accurate food portion estimation is\nchallenging since the process of food preparation and consumption impose large\nvariations on food shapes and appearances. In this paper, we present a food\nportion estimation method to estimate food energy (kilocalories) from food\nimages using Generative Adversarial Networks (GAN). We introduce the concept of\nan \"energy distribution\" for each food image. To train the GAN, we design a\nfood image dataset based on ground truth food labels and segmentation masks for\neach food image as well as energy information associated with the food image.\nOur goal is to learn the mapping of the food image to the food energy. We can\nthen estimate food energy based on the energy distribution. We show that an\naverage energy estimation error rate of 10.89% can be obtained by learning the\nimage-to-energy mapping.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 01:25:03 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 07:09:12 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Fang", "Shaobo", ""], ["Shao", "Zeman", ""], ["Mao", "Runyu", ""], ["Fu", "Chichen", ""], ["Kerr", "Deborah A.", ""], ["Boushey", "Carol J.", ""], ["Delp", "Edward J.", ""], ["Zhu", "Fengqing", ""]]}, {"id": "1802.09723", "submitter": "Bowen Pan", "authors": "Bowen Pan, Wuwei Lin, Xiaolin Fang, Chaoqin Huang, Bolei Zhou, Cewu Lu", "title": "Recurrent Residual Module for Fast Inference in Videos", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have made impressive progress in\nmany video recognition tasks such as video pose estimation and video object\ndetection. However, CNN inference on video is computationally expensive due to\nprocessing dense frames individually. In this work, we propose a framework\ncalled Recurrent Residual Module (RRM) to accelerate the CNN inference for\nvideo recognition tasks. This framework has a novel design of using the\nsimilarity of the intermediate feature maps of two consecutive frames, to\nlargely reduce the redundant computation. One unique property of the proposed\nmethod compared to previous work is that feature maps of each frame are\nprecisely computed. The experiments show that, while maintaining the similar\nrecognition performance, our RRM yields averagely 2x acceleration on the\ncommonly used CNNs such as AlexNet, ResNet, deep compression model (thus 8-12x\nfaster than the original dense models using the efficient inference engine),\nand impressively 9x acceleration on some binary networks such as XNOR-Nets\n(thus 500x faster than the original model). We further verify the effectiveness\nof the RRM on speeding up CNNs for video pose estimation and video object\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:25:20 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Pan", "Bowen", ""], ["Lin", "Wuwei", ""], ["Fang", "Xiaolin", ""], ["Huang", "Chaoqin", ""], ["Zhou", "Bolei", ""], ["Lu", "Cewu", ""]]}, {"id": "1802.09745", "submitter": "Xin Li", "authors": "Xin Li and Mooi Choo Chuah", "title": "ReHAR: Robust and Efficient Human Activity Recognition", "comments": "Accepted by WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a scheme that can achieve a good performance in predicting single\nperson activities and group activities is a challenging task. In this paper, we\npropose a novel robust and efficient human activity recognition scheme called\nReHAR, which can be used to handle single person activities and group\nactivities prediction. First, we generate an optical flow image for each video\nframe. Then, both video frames and their corresponding optical flow images are\nfed into a Single Frame Representation Model to generate representations.\nFinally, an LSTM is used to pre- dict the final activities based on the\ngenerated representations. The whole model is trained end-to-end to allow\nmeaningful representations to be generated for the final activity recognition.\nWe evaluate ReHAR using two well-known datasets: the NCAA Basketball Dataset\nand the UCFSports Action Dataset. The experimental results show that the pro-\nposed ReHAR achieves a higher activity recognition accuracy with an order of\nmagnitude shorter computation time compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:58:01 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Li", "Xin", ""], ["Chuah", "Mooi Choo", ""]]}, {"id": "1802.09766", "submitter": "Bernhard C. Geiger", "authors": "Rana Ali Amjad and Bernhard C. Geiger", "title": "Learning Representations for Neural Network-Based Classification Using\n  the Information Bottleneck Principle", "comments": "16 pages, to appear in IEEE Trans. Pattern Analysis and Machine\n  Intelligence", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  42(9):2225-2239, 2020. (c) IEEE", "doi": "10.1109/TPAMI.2019.2909031", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this theory paper, we investigate training deep neural networks (DNNs) for\nclassification via minimizing the information bottleneck (IB) functional. We\nshow that the resulting optimization problem suffers from two severe issues:\nFirst, for deterministic DNNs, either the IB functional is infinite for almost\nall values of network parameters, making the optimization problem ill-posed, or\nit is piecewise constant, hence not admitting gradient-based optimization\nmethods. Second, the invariance of the IB functional under bijections prevents\nit from capturing properties of the learned representation that are desirable\nfor classification, such as robustness and simplicity. We argue that these\nissues are partly resolved for stochastic DNNs, DNNs that include a (hard or\nsoft) decision rule, or by replacing the IB functional with related, but more\nwell-behaved cost functions. We conclude that recent successes reported about\ntraining DNNs using the IB framework must be attributed to such solutions. As a\nside effect, our results indicate limitations of the IB framework for the\nanalysis of DNNs. We also note that rather than trying to repair the inherent\nproblems in the IB functional, a better approach may be to design regularizers\non latent representation enforcing the desired properties directly.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 08:24:19 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 08:27:43 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:41:48 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 06:40:42 GMT"}, {"version": "v5", "created": "Thu, 10 Jan 2019 10:58:31 GMT"}, {"version": "v6", "created": "Thu, 11 Apr 2019 11:20:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Amjad", "Rana Ali", ""], ["Geiger", "Bernhard C.", ""]]}, {"id": "1802.09778", "submitter": "Yan Li", "authors": "Yan Li, Junge Zhang, Kaiqi Huang, Jianguo Zhang", "title": "Mixed Supervised Object Detection with Robust Objectness Transfer", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2019). Together with Supplementary Materials. Note: The author list in\n  Google Scholar is INCORRECT. The right author list is 1) Yan Li, 2) Junge\n  Zhang, 3) Kaiqi Huang and 4) Jianguo Zhang. The official published version\n  can be found in https://ieeexplore.ieee.org/abstract/document/8304628", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of leveraging existing fully labeled\ncategories to improve the weakly supervised detection (WSD) of new object\ncategories, which we refer to as mixed supervised detection (MSD). Different\nfrom previous MSD methods that directly transfer the pre-trained object\ndetectors from existing categories to new categories, we propose a more\nreasonable and robust objectness transfer approach for MSD. In our framework,\nwe first learn domain-invariant objectness knowledge from the existing fully\nlabeled categories. The knowledge is modeled based on invariant features that\nare robust to the distribution discrepancy between the existing categories and\nnew categories; therefore the resulting knowledge would generalize well to new\ncategories and could assist detection models to reject distractors (e.g.,\nobject parts) in weakly labeled images of new categories. Under the guidance of\nlearned objectness knowledge, we utilize multiple instance learning (MIL) to\nmodel the concepts of both objects and distractors and to further improve the\nability of rejecting distractors in weakly labeled images. Our robust\nobjectness transfer approach outperforms the existing MSD methods, and achieves\nstate-of-the-art results on the challenging ILSVRC2013 detection dataset and\nthe PASCAL VOC datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 09:02:38 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 11:15:50 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 03:24:49 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Yan", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""], ["Zhang", "Jianguo", ""]]}, {"id": "1802.09816", "submitter": "Guillaume Charpiat", "authors": "Armand Zampieri (TITANE), Guillaume Charpiat (TAU), Yuliya Tarabalka\n  (TITANE)", "title": "Coarse to fine non-rigid registration: a chain of scale-specific neural\n  networks for multimodal image alignment with application to remote sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle here the problem of multimodal image non-rigid registration, which\nis of prime importance in remote sensing and medical imaging. The difficulties\nencountered by classical registration approaches include feature design and\nslow optimization by gradient descent. By analyzing these methods, we note the\nsignificance of the notion of scale. We design easy-to-train,\nfully-convolutional neural networks able to learn scale-specific features. Once\nchained appropriately, they perform global registration in linear time, getting\nrid of gradient descent schemes by predicting directly the deformation.We show\ntheir performance in terms of quality and speed through various tasks of remote\nsensing multimodal image alignment. In particular, we are able to register\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\nimages, and outperform current keypoint matching methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 10:47:06 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zampieri", "Armand", "", "TITANE"], ["Charpiat", "Guillaume", "", "TAU"], ["Tarabalka", "Yuliya", "", "TITANE"]]}, {"id": "1802.09834", "submitter": "Chaolong Li", "authors": "Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan Xu, Jian Yang", "title": "Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variations of human body skeletons may be considered as dynamic graphs, which\nare generic data representation for numerous real-world applications. In this\npaper, we propose a spatio-temporal graph convolution (STGC) approach for\nassembling the successes of local convolutional filtering and sequence learning\nability of autoregressive moving average. To encode dynamic graphs, the\nconstructed multi-scale local graph convolution filters, consisting of matrices\nof local receptive fields and signal mappings, are recursively performed on\nstructured graph data of temporal and spatial domain. The proposed model is\ngeneric and principled as it can be generalized into other dynamic models. We\ntheoretically prove the stability of STGC and provide an upper-bound of the\nsignal transformation to be learnt. Further, the proposed recursive model can\nbe stacked into a multi-layer architecture. To evaluate our model, we conduct\nextensive experiments on four benchmark skeleton-based action datasets,\nincluding the large-scale challenging NTU RGB+D. The experimental results\ndemonstrate the effectiveness of our proposed model and the improvement over\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 11:39:46 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Li", "Chaolong", ""], ["Cui", "Zhen", ""], ["Zheng", "Wenming", ""], ["Xu", "Chunyan", ""], ["Yang", "Jian", ""]]}, {"id": "1802.09841", "submitter": "M\\'elanie Ducoffe", "authors": "Melanie Ducoffe, Frederic Precioso", "title": "Adversarial Active Learning for Deep Networks: a Margin Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning strategy designed for deep neural networks.\nThe goal is to minimize the number of data annotation queried from an oracle\nduring training. Previous active learning strategies scalable for deep networks\nwere mostly based on uncertain sample selection. In this work, we focus on\nexamples lying close to the decision boundary. Based on theoretical works on\nmargin theory for active learning, we know that such examples may help to\nconsiderably decrease the number of annotations. While measuring the exact\ndistance to the decision boundaries is intractable, we propose to rely on\nadversarial examples. We do not consider anymore them as a threat instead we\nexploit the information they provide on the distribution of the input space in\norder to approximate the distance to decision boundaries. We demonstrate\nempirically that adversarial active queries yield faster convergence of CNNs\ntrained on MNIST, the Shoe-Bag and the Quick-Draw datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 12:02:33 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Ducoffe", "Melanie", ""], ["Precioso", "Frederic", ""]]}, {"id": "1802.09843", "submitter": "Francesco Verdoja", "authors": "Francesco Verdoja and Marco Grangetto", "title": "Graph Laplacian for Image Anomaly Detection", "comments": "Published in Machine Vision and Applications (Springer)", "journal-ref": "Machine Vision and Applications, vol. 31, no. 1, Feb. 2020", "doi": "10.1007/s00138-020-01059-4", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reed-Xiaoli detector (RXD) is recognized as the benchmark algorithm for image\nanomaly detection; however, it presents known limitations, namely the\ndependence over the image following a multivariate Gaussian model, the\nestimation and inversion of a high-dimensional covariance matrix, and the\ninability to effectively include spatial awareness in its evaluation. In this\nwork, a novel graph-based solution to the image anomaly detection problem is\nproposed; leveraging the graph Fourier transform, we are able to overcome some\nof RXD's limitations while reducing computational cost at the same time. Tests\nover both hyperspectral and medical images, using both synthetic and real\nanomalies, prove the proposed technique is able to obtain significant gains\nover performance by other algorithms in the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 12:08:06 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 08:45:46 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 10:43:13 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 08:55:34 GMT"}, {"version": "v5", "created": "Tue, 21 Jan 2020 08:54:00 GMT"}, {"version": "v6", "created": "Mon, 10 Feb 2020 12:55:14 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Verdoja", "Francesco", ""], ["Grangetto", "Marco", ""]]}, {"id": "1802.09850", "submitter": "Anil Kumar Vadathya Mr", "authors": "Akshat Dave, Anil Kumar Vadathya, Ramana Subramanyam, Rahul Baburajan,\n  Kaushik Mitra", "title": "Solving Inverse Computational Imaging Problems using Deep Pixel-level\n  Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Signal reconstruction is a challenging aspect of computational imaging as it\noften involves solving ill-posed inverse problems. Recently, deep feed-forward\nneural networks have led to state-of-the-art results in solving various inverse\nimaging problems. However, being task specific, these networks have to be\nlearned for each inverse problem. On the other hand, a more flexible approach\nwould be to learn a deep generative model once and then use it as a signal\nprior for solving various inverse problems. We show that among the various\nstate of the art deep generative models, autoregressive models are especially\nsuitable for our purpose for the following reasons. First, they explicitly\nmodel the pixel level dependencies and hence are capable of reconstructing\nlow-level details such as texture patterns and edges better. Second, they\nprovide an explicit expression for the image prior which can then be used for\nMAP based inference along with the forward model. Third, they can model long\nrange dependencies in images which make them ideal for handling global\nmultiplexing as encountered in various compressive imaging systems. We\ndemonstrate the efficacy of our proposed approach in solving three\ncomputational imaging problems: Single Pixel Camera (SPC), LiSens and FlatCam.\nFor both real and simulated cases, we obtain better reconstructions than the\nstate-of-the-art methods in terms of perceptual and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 12:23:27 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 00:37:58 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Dave", "Akshat", ""], ["Vadathya", "Anil Kumar", ""], ["Subramanyam", "Ramana", ""], ["Baburajan", "Rahul", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1802.09900", "submitter": "Di Tang", "authors": "Di Tang, XiaoFeng Wang, Kehuan Zhang", "title": "Query-Free Attacks on Industry-Grade Face Recognition Systems under\n  Resource Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To launch black-box attacks against a Deep Neural Network (DNN) based Face\nRecognition (FR) system, one needs to build \\textit{substitute} models to\nsimulate the target model, so the adversarial examples discovered from\nsubstitute models could also mislead the target model. Such\n\\textit{transferability} is achieved in recent studies through querying the\ntarget model to obtain data for training the substitute models. A real-world\ntarget, likes the FR system of law enforcement, however, is less accessible to\nthe adversary. To attack such a system, a substitute model with similar quality\nas the target model is needed to identify their common defects. This is hard\nsince the adversary often does not have the enough resources to train such a\npowerful model (hundreds of millions of images and rooms of GPUs are needed to\ntrain a commercial FR system).\n  We found in our research, however, that a resource-constrained adversary\ncould still effectively approximate the target model's capability to recognize\n\\textit{specific} individuals, by training \\textit{biased} substitute models on\nadditional images of those victims whose identities the attacker want to cover\nor impersonate. This is made possible by a new property we discovered, called\n\\textit{Nearly Local Linearity} (NLL), which models the observation that an\nideal DNN model produces the image representations (embeddings) whose distances\namong themselves truthfully describe the human perception of the differences\namong the input images. By simulating this property around the victim's images,\nwe significantly improve the transferability of black-box impersonation attacks\nby nearly 50\\%. Particularly, we successfully attacked a commercial system\ntrained over 20 million images, using 4 million images and 1/5 of the training\ntime but achieving 62\\% transferability in an impersonation attack and 89\\% in\na dodging attack.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 08:44:58 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 13:19:33 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Tang", "Di", ""], ["Wang", "XiaoFeng", ""], ["Zhang", "Kehuan", ""]]}, {"id": "1802.09917", "submitter": "Zhaobin Mo", "authors": "Zhaobin Mo, Sisi Li, Diange Yang, Ding Zhao", "title": "Extraction of V2V Encountering Scenarios from Naturalistic Driving\n  Database", "comments": "6 pages; 11 figures; Submitted to International Symposium on Advanced\n  Vehicle Control, Beijing, China, July 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is necessary to thoroughly evaluate the effectiveness and safety of\nConnected Vehicles (CVs) algorithm before their release and deployment. Current\nevaluation approach mainly relies on simulation platform with the\nsingle-vehicle driving model. The main drawback of it is the lack of network\nrealism. To overcome this problem, we extract naturalistic V2V encounters data\nfrom the database, and then separate the primary vehicle encounter category by\nclustering. A fast mining algorithm is proposed that can be applied to parallel\nquery for further process acceleration. 4,500 encounters are mined from a 275\nGB database collected in the Safety Pilot Model Program in Ann Arbor Michigan,\nUSA. K-means and Dynamic Time Warping (DTW) are used in clustering. Results\nshow this method can quickly mine and cluster primary driving scenarios from a\nlarge database. Our results separate the car-following, intersection and\nby-passing, which are the primary category of the vehicle encounter. We\nanticipate the work in the essay can become a general method to effectively\nextract vehicle encounters from any existing database that contains vehicular\nGPS information. What's more, the naturalistic data of different vehicle\nencounters can be applied in Connected Vehicles evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 14:40:04 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 16:22:25 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Mo", "Zhaobin", ""], ["Li", "Sisi", ""], ["Yang", "Diange", ""], ["Zhao", "Ding", ""]]}, {"id": "1802.09932", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Kaiwen Zhou, Hongying Liu, James Cheng, Ivor W. Tsang,\n  Lijun Zhang, Dacheng Tao, Licheng Jiao", "title": "VR-SGD: A Simple Stochastic Variance Reduction Method for Machine\n  Learning", "comments": "46 pages, 25 figures. IEEE Transactions on Knowledge and Data\n  Engineering, accepted in October, 2018. arXiv admin note: substantial text\n  overlap with arXiv:1704.04966", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple variant of the original SVRG, called\nvariance reduced stochastic gradient descent (VR-SGD). Unlike the choices of\nsnapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the\ntwo vectors of VR-SGD are set to the average and last iterate of the previous\nepoch, respectively. The settings allow us to use much larger learning rates,\nand also make our convergence analysis more challenging. We also design two\ndifferent update rules for smooth and non-smooth objective functions,\nrespectively, which means that VR-SGD can tackle non-smooth and/or non-strongly\nconvex problems directly without any reduction techniques. Moreover, we analyze\nthe convergence properties of VR-SGD for strongly convex problems, which show\nthat VR-SGD attains linear convergence. Different from its counterparts that\nhave no convergence guarantees for non-strongly convex problems, we also\nprovide the convergence guarantees of VR-SGD for this case, and empirically\nverify that VR-SGD with varying learning rates achieves similar performance to\nits momentum accelerated variant that has the optimal convergence rate\n$\\mathcal{O}(1/T^2)$. Finally, we apply VR-SGD to solve various machine\nlearning problems, such as convex and non-convex empirical risk minimization,\nand leading eigenvalue computation. Experimental results show that VR-SGD\nconverges significantly faster than SVRG and Prox-SVRG, and usually outperforms\nstate-of-the-art accelerated methods, e.g., Katyusha.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 12:56:07 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 15:03:47 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Shang", "Fanhua", ""], ["Zhou", "Kaiwen", ""], ["Liu", "Hongying", ""], ["Cheng", "James", ""], ["Tsang", "Ivor W.", ""], ["Zhang", "Lijun", ""], ["Tao", "Dacheng", ""], ["Jiao", "Licheng", ""]]}, {"id": "1802.09941", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Torsten Hoefler", "title": "Demystifying Parallel and Distributed Deep Learning: An In-Depth\n  Concurrency Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are becoming an important tool in modern\ncomputing applications. Accelerating their training is a major challenge and\ntechniques range from distributed algorithms to low-level circuit design. In\nthis survey, we describe the problem from a theoretical perspective, followed\nby approaches for its parallelization. We present trends in DNN architectures\nand the resulting implications on parallelization strategies. We then review\nand model the different types of concurrency in DNNs: from the single operator,\nthrough parallelism in network inference and training, to distributed deep\nlearning. We discuss asynchronous stochastic optimization, distributed system\narchitectures, communication schemes, and neural architecture search. Based on\nthose approaches, we extrapolate potential directions for parallelism in deep\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 08:47:34 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 08:36:28 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1802.09971", "submitter": "Tom Runia", "authors": "Tom F.H. Runia, Cees G.M. Snoek, Arnold W.M. Smeulders", "title": "Real-World Repetition Estimation by Div, Grad and Curl", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating repetition in video, such as performing\npush-ups, cutting a melon or playing violin. Existing work shows good results\nunder the assumption of static and stationary periodicity. As realistic video\nis rarely perfectly static and stationary, the often preferred Fourier-based\nmeasurements is inapt. Instead, we adopt the wavelet transform to better handle\nnon-static and non-stationary video dynamics. From the flow field and its\ndifferentials, we derive three fundamental motion types and three motion\ncontinuities of intrinsic periodicity in 3D. On top of this, the 2D perception\nof 3D periodicity considers two extreme viewpoints. What follows are 18\nfundamental cases of recurrent perception in 2D. In practice, to deal with the\nvariety of repetitive appearance, our theory implies measuring time-varying\nflow and its differentials (gradient, divergence and curl) over segmented\nforeground motion. For experiments, we introduce the new QUVA Repetition\ndataset, reflecting reality by including non-static and non-stationary videos.\nOn the task of counting repetitions in video, we obtain favorable results\ncompared to a deep learning alternative.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 15:42:34 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Runia", "Tom F. H.", ""], ["Snoek", "Cees G. M.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1802.09972", "submitter": "Michael Ying Yang", "authors": "Dayan Guan, Yanpeng Cao, Jun Liang, Yanlong Cao, Michael Ying Yang", "title": "Fusion of Multispectral Data Through Illumination-aware Deep Neural\n  Networks for Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral pedestrian detection has received extensive attention in recent\nyears as a promising solution to facilitate robust human target detection for\naround-the-clock applications (e.g. security surveillance and autonomous\ndriving). In this paper, we demonstrate illumination information encoded in\nmultispectral images can be utilized to significantly boost performance of\npedestrian detection. A novel illumination-aware weighting mechanism is present\nto accurately depict illumination condition of a scene. Such illumination\ninformation is incorporated into two-stream deep convolutional neural networks\nto learn multispectral human-related features under different illumination\nconditions (daytime and nighttime). Moreover, we utilized illumination\ninformation together with multispectral data to generate more accurate semantic\nsegmentation which are used to boost pedestrian detection accuracy. Putting all\nof the pieces together, we present a powerful framework for multispectral\npedestrian detection based on multi-task learning of illumination-aware\npedestrian detection and semantic segmentation. Our proposed method is trained\nend-to-end using a well-designed multi-task loss function and outperforms\nstate-of-the-art approaches on KAIST multispectral pedestrian dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 15:42:40 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Guan", "Dayan", ""], ["Cao", "Yanpeng", ""], ["Liang", "Jun", ""], ["Cao", "Yanlong", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1802.09975", "submitter": "Samuel Scheidegger", "authors": "Samuel Scheidegger, Joachim Benjaminsson, Emil Rosenberg, Amrit\n  Krishnan, Karl Granstrom", "title": "Mono-Camera 3D Multi-Object Tracking Using Deep Learning Detections and\n  PMBM Filtering", "comments": "8 pages, 2 figures, for associated videos, see https://goo.gl/AoydgW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular cameras are one of the most commonly used sensors in the automotive\nindustry for autonomous vehicles. One major drawback using a monocular camera\nis that it only makes observations in the two dimensional image plane and can\nnot directly measure the distance to objects. In this paper, we aim at filling\nthis gap by developing a multi-object tracking algorithm that takes an image as\ninput and produces trajectories of detected objects in a world coordinate\nsystem. We solve this by using a deep neural network trained to detect and\nestimate the distance to objects from a single input image. The detections from\na sequence of images are fed in to a state-of-the art Poisson multi-Bernoulli\nmixture tracking filter. The combination of the learned detector and the PMBM\nfilter results in an algorithm that achieves 3D tracking using only mono-camera\nimages as input. The performance of the algorithm is evaluated both in 3D world\ncoordinates, and 2D image coordinates, using the publicly available KITTI\nobject tracking dataset. The algorithm shows the ability to accurately track\nobjects, correctly handle data associations, even when there is a big overlap\nof the objects in the image, and is one of the top performing algorithms on the\nKITTI object tracking benchmark. Furthermore, the algorithm is efficient,\nrunning on average close to 20 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 15:46:15 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Scheidegger", "Samuel", ""], ["Benjaminsson", "Joachim", ""], ["Rosenberg", "Emil", ""], ["Krishnan", "Amrit", ""], ["Granstrom", "Karl", ""]]}, {"id": "1802.09985", "submitter": "Xinyu Gong", "authors": "Xinyu Gong, Haozhi Huang, Lin Ma, Fumin Shen, Wei Liu, Tong Zhang", "title": "Neural Stereoscopic Image Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer is an emerging technique which is able to endow\ndaily-life images with attractive artistic styles. Previous work has succeeded\nin applying convolutional neural networks (CNNs) to style transfer for\nmonocular images or videos. However, style transfer for stereoscopic images is\nstill a missing piece. Different from processing a monocular image, the two\nviews of a stylized stereoscopic pair are required to be consistent to provide\nobservers a comfortable visual experience. In this paper, we propose a novel\ndual path network for view-consistent style transfer on stereoscopic images.\nWhile each view of the stereoscopic pair is processed in an individual path, a\nnovel feature aggregation strategy is proposed to effectively share information\nbetween the two paths. Besides a traditional perceptual loss being used for\ncontrolling the style transfer quality in each view, a multi-layer view loss is\nleveraged to enforce the network to coordinate the learning of both the paths\nto generate view-consistent stylized results. Extensive experiments show that,\ncompared against previous methods, our proposed model can produce stylized\nstereoscopic images which achieve decent view consistency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:02:17 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 16:46:58 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 15:44:43 GMT"}, {"version": "v4", "created": "Fri, 27 Jul 2018 15:51:44 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Gong", "Xinyu", ""], ["Huang", "Haozhi", ""], ["Ma", "Lin", ""], ["Shen", "Fumin", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1802.09987", "submitter": "Edward Smith", "authors": "Edward Smith, Scott Fujimoto, and David Meger", "title": "Multi-View Silhouette and Depth Decomposition for High Resolution 3D\n  Object Representation", "comments": "22 pages, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scaling deep generative shape models to\nhigh-resolution. Drawing motivation from the canonical view representation of\nobjects, we introduce a novel method for the fast up-sampling of 3D objects in\nvoxel space through networks that perform super-resolution on the six\northographic depth projections. This allows us to generate high-resolution\nobjects with more efficient scaling than methods which work directly in 3D. We\ndecompose the problem of 2D depth super-resolution into silhouette and depth\nprediction to capture both structure and fine detail. This allows our method to\ngenerate sharp edges more easily than an individual network. We evaluate our\nwork on multiple experiments concerning high-resolution 3D objects, and show\nour system is capable of accurately predicting novel objects at resolutions as\nlarge as 512$\\mathbf{\\times}$512$\\mathbf{\\times}$512 -- the highest resolution\nreported for this task. We achieve state-of-the-art performance on 3D object\nreconstruction from RGB images on the ShapeNet dataset, and further demonstrate\nthe first effective 3D super-resolution method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:09:28 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 21:01:40 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 21:40:12 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Smith", "Edward", ""], ["Fujimoto", "Scott", ""], ["Meger", "David", ""]]}, {"id": "1802.09990", "submitter": "Saman Bashbaghi", "authors": "Saman Bashbaghi, Eric Granger, Robert Sabourin and Mostafa Parchami", "title": "Deep Learning Architectures for Face Recognition in Video Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) systems for video surveillance (VS) applications\nattempt to accurately detect the presence of target individuals over a\ndistributed network of cameras. In video-based FR systems, facial models of\ntarget individuals are designed a priori during enrollment using a limited\nnumber of reference still images or video data. These facial models are not\ntypically representative of faces being observed during operations due to large\nvariations in illumination, pose, scale, occlusion, blur, and to camera\ninter-operability. Specifically, in still-to-video FR application, a single\nhigh-quality reference still image captured with still camera under controlled\nconditions is employed to generate a facial model to be matched later against\nlower-quality faces captured with video cameras under uncontrolled conditions.\nCurrent video-based FR systems can perform well on controlled scenarios, while\ntheir performance is not satisfactory in uncontrolled scenarios mainly because\nof the differences between the source (enrollment) and the target (operational)\ndomains. Most of the efforts in this area have been toward the design of robust\nvideo-based FR systems in unconstrained surveillance environments. This chapter\npresents an overview of recent advances in still-to-video FR scenario through\ndeep convolutional neural networks (CNNs). In particular, deep learning\narchitectures proposed in the literature based on triplet-loss function (e.g.,\ncross-correlation matching CNN, trunk-branch ensemble CNN and HaarNet) and\nsupervised autoencoders (e.g., canonical face representation CNN) are reviewed\nand compared in terms of accuracy and computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:10:15 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 14:00:33 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Bashbaghi", "Saman", ""], ["Granger", "Eric", ""], ["Sabourin", "Robert", ""], ["Parchami", "Mostafa", ""]]}, {"id": "1802.10019", "submitter": "Hee Seok Lee", "authors": "Hee Seok Lee and Kang Kim", "title": "Simultaneous Traffic Sign Detection and Boundary Estimation using\n  Convolutional Neural Network", "comments": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2018.2801560", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel traffic sign detection system that simultaneously\nestimates the location and precise boundary of traffic signs using\nconvolutional neural network (CNN). Estimating the precise boundary of traffic\nsigns is important in navigation systems for intelligent vehicles where traffic\nsigns can be used as 3D landmarks for road environment. Previous traffic sign\ndetection systems, including recent methods based on CNN, only provide bounding\nboxes of traffic signs as output, and thus requires additional processes such\nas contour estimation or image segmentation to obtain the precise sign\nboundary. In this work, the boundary estimation of traffic signs is formulated\nas a 2D pose and shape class prediction problem, and this is effectively solved\nby a single CNN. With the predicted 2D pose and the shape class of a target\ntraffic sign in an input image, we estimate the actual boundary of the target\nsign by projecting the boundary of a corresponding template sign image into the\ninput image plane. By formulating the boundary estimation problem as a\nCNN-based pose and shape prediction task, our method is end-to-end trainable,\nand more robust to occlusion and small targets than other boundary estimation\nmethods that rely on contour estimation or image segmentation. The proposed\nmethod with architectural optimization provides an accurate traffic sign\nboundary estimation which is also efficient in compute, showing a detection\nframe rate higher than 7 frames per second on low-power mobile platforms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:51:04 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Lee", "Hee Seok", ""], ["Kim", "Kang", ""]]}, {"id": "1802.10033", "submitter": "Christoph Wick", "authors": "Christoph Wick, Christian Reul, Frank Puppe", "title": "Improving OCR Accuracy on Early Printed Books using Deep Convolutional\n  Networks", "comments": "16 pages, 4 figures, 8 tables, submitted to JLCL Volume 33 (2018),\n  Issue 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a combination of a convolutional and a LSTM network to\nimprove the accuracy of OCR on early printed books. While the standard model of\nline based OCR uses a single LSTM layer, we utilize a CNN- and Pooling-Layer\ncombination in advance of an LSTM layer. Due to the higher amount of trainable\nparameters the performance of the network relies on a high amount of training\nexamples to unleash its power. Hereby, the error is reduced by a factor of up\nto 44%, yielding a CER of 1% and below. To further improve the results we use a\nvoting mechanism to achieve character error rates (CER) below $0.5%$. The\nruntime of the deep model for training and prediction of a book behaves very\nsimilar to a shallow network.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:17:50 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Wick", "Christoph", ""], ["Reul", "Christian", ""], ["Puppe", "Frank", ""]]}, {"id": "1802.10036", "submitter": "Puyang Wang", "authors": "Puyang Wang and Vishal M. Patel", "title": "Generating High Quality Visible Images from SAR Images Using CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for generating high quality visible-like images\nfrom Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative\nAdversarial Network (GAN) architectures. The proposed approach is based on a\ncascaded network of convolutional neural nets (CNNs) for despeckling and image\ncolorization. The cascaded structure results in faster convergence during\ntraining and produces high quality visible images from the corresponding SAR\nimages. Experimental results on both simulated and real SAR images show that\nthe proposed method can produce visible-like images better compared to the\nrecent state-of-the-art deep learning-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:31:21 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Wang", "Puyang", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1802.10038", "submitter": "Christian Reul", "authors": "Christian Reul, Uwe Springmann, Christoph Wick, Frank Puppe", "title": "Improving OCR Accuracy on Early Printed Books by combining Pretraining,\n  Voting, and Active Learning", "comments": "Submitted to JLCL Volume 33 (2018), Issue 1: Special Issue on\n  Automatic Text and Layout Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We combine three methods which significantly improve the OCR accuracy of OCR\nmodels trained on early printed books: (1) The pretraining method utilizes the\ninformation stored in already existing models trained on a variety of typesets\n(mixed models) instead of starting the training from scratch. (2) Performing\ncross fold training on a single set of ground truth data (line images and their\ntranscriptions) with a single OCR engine (OCRopus) produces a committee whose\nmembers then vote for the best outcome by also taking the top-N alternatives\nand their intrinsic confidence values into account. (3) Following the principle\nof maximal disagreement we select additional training lines which the voters\ndisagree most on, expecting them to offer the highest information gain for a\nsubsequent training (active learning). Evaluations on six early printed books\nyielded the following results: On average the combination of pretraining and\nvoting improved the character accuracy by 46% when training five folds starting\nfrom the same mixed model. This number rose to 53% when using different models\nfor pretraining, underlining the importance of diverse voters. Incorporating\nactive learning improved the obtained results by another 16% on average\n(evaluated on three of the six books). Overall, the proposed methods lead to an\naverage error rate of 2.5% when training on only 60 lines. Using a substantial\nground truth pool of 1,000 lines brought the error rate down even further to\nless than 1% on average.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:35:36 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 08:54:49 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Reul", "Christian", ""], ["Springmann", "Uwe", ""], ["Wick", "Christoph", ""], ["Puppe", "Frank", ""]]}, {"id": "1802.10055", "submitter": "Abdul Wahab", "authors": "Jaejun Yoo and Abdul Wahab and Jong Chul Ye", "title": "A Mathematical Framework for Deep Learning in Elastic Source Imaging", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An inverse elastic source problem with sparse measurements is of concern. A\ngeneric mathematical framework is proposed which incorporates a low-\ndimensional manifold regularization in the conventional source reconstruction\nalgorithms thereby enhancing their performance with sparse datasets. It is\nrigorously established that the proposed framework is equivalent to the\nso-called \\emph{deep convolutional framelet expansion} in machine learning\nliterature for inverse problems. Apposite numerical examples are furnished to\nsubstantiate the efficacy of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 18:14:00 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 15:25:36 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 23:17:48 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Yoo", "Jaejun", ""], ["Wahab", "Abdul", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1802.10062", "submitter": "Xiaofan Zhang", "authors": "Yuhong Li, Xiaofan Zhang, Deming Chen", "title": "CSRNet: Dilated Convolutional Neural Networks for Understanding the\n  Highly Congested Scenes", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network for Congested Scene Recognition called CSRNet to provide\na data-driven and deep learning method that can understand highly congested\nscenes and perform accurate count estimation as well as present high-quality\ndensity maps. The proposed CSRNet is composed of two major components: a\nconvolutional neural network (CNN) as the front-end for 2D feature extraction\nand a dilated CNN for the back-end, which uses dilated kernels to deliver\nlarger reception fields and to replace pooling operations. CSRNet is an\neasy-trained model because of its pure convolutional structure. We demonstrate\nCSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the\nWorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art\nperformance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower\nMean Absolute Error (MAE) than the previous state-of-the-art method. We extend\nthe targeted applications for counting other objects, such as the vehicle in\nTRANCOS dataset. Results show that CSRNet significantly improves the output\nquality with 15.4% lower MAE than the previous state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 18:39:31 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 04:30:19 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 20:48:36 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 04:20:08 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Li", "Yuhong", ""], ["Zhang", "Xiaofan", ""], ["Chen", "Deming", ""]]}, {"id": "1802.10066", "submitter": "Nicolas Dobigeon", "authors": "\\'Etienne Monier, Thomas Oberlin, Nathalie Brun, Marcel Tenc\\'e, Marta\n  de Frutos and Nicolas Dobigeon", "title": "Reconstruction of partially sampled multi-band images - Application to\n  STEM-EELS imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron microscopy has shown to be a very powerful tool to map the chemical\nnature of samples at various scales down to atomic resolution. However, many\nsamples can not be analyzed with an acceptable signal-to-noise ratio because of\nthe radiation damage induced by the electron beam. This is particularly crucial\nfor electron energy loss spectroscopy (EELS) which acquires spectral-spatial\ndata and requires high beam intensity. Since scanning transmission electron\nmicroscopes (STEM) are able to acquire data cubes by scanning the electron\nprobe over the sample and recording a spectrum for each spatial position, it is\npossible to design the scan pattern and to sample only specific pixels. As a\nconsequence, partial acquisition schemes are now conceivable, provided a\nreconstruction of the full data cube is conducted as a post-processing step.\nThis paper proposes two reconstruction algorithms for multi-band images\nacquired by STEM-EELS which exploits the spectral structure and the spatial\nsmoothness of the image. The performance of the proposed schemes is illustrated\nthanks to experiments conducted on a realistic phantom dataset as well as real\nEELS spectrum-images.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 18:46:27 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Monier", "\u00c9tienne", ""], ["Oberlin", "Thomas", ""], ["Brun", "Nathalie", ""], ["Tenc\u00e9", "Marcel", ""], ["de Frutos", "Marta", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1802.10171", "submitter": "Ziyan Wu", "authors": "Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu", "title": "Tell Me Where to Look: Guided Attention Inference Network", "comments": "Accepted in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised learning with only coarse labels can obtain visual\nexplanations of deep neural network such as attention maps by back-propagating\ngradients. These attention maps are then available as priors for tasks such as\nobject localization and semantic segmentation. In one common framework we\naddress three shortcomings of previous approaches in modeling such attention\nmaps: We (1) first time make attention maps an explicit and natural component\nof the end-to-end training, (2) provide self-guidance directly on these maps by\nexploring supervision form the network itself to improve them, and (3)\nseamlessly bridge the gap between using weak and extra supervision if\navailable. Despite its simplicity, experiments on the semantic segmentation\ntask demonstrate the effectiveness of our methods. We clearly surpass the\nstate-of-the-art on Pascal VOC 2012 val. and test set. Besides, the proposed\nframework provides a way not only explaining the focus of the learner but also\nfeeding back with direct guidance towards specific tasks. Under mild\nassumptions our method can also be understood as a plug-in to existing weakly\nsupervised learners to improve their generalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 21:17:30 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Li", "Kunpeng", ""], ["Wu", "Ziyan", ""], ["Peng", "Kuan-Chuan", ""], ["Ernst", "Jan", ""], ["Fu", "Yun", ""]]}, {"id": "1802.10200", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Arash Mohammadi, and Konstantinos N. Plataniotis", "title": "Brain Tumor Type Classification via Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumor is considered as one of the deadliest and most common form of\ncancer both in children and in adults. Consequently, determining the correct\ntype of brain tumor in early stages is of significant importance to devise a\nprecise treatment plan and predict patient's response to the adopted treatment.\nIn this regard, there has been a recent surge of interest in designing\nConvolutional Neural Networks (CNNs) for the problem of brain tumor type\nclassification. However, CNNs typically require large amount of training data\nand can not properly handle input transformations. Capsule networks (referred\nto as CapsNets) are brand new machine learning architectures proposed very\nrecently to overcome these shortcomings of CNNs, and posed to revolutionize\ndeep learning solutions. Of particular interest to this work is that Capsule\nnetworks are robust to rotation and affine transformation, and require far less\ntraining data, which is the case for processing medical image datasets\nincluding brain Magnetic Resonance Imaging (MRI) images. In this paper, we\nfocus to achieve the following four objectives: (i) Adopt and incorporate\nCapsNets for the problem of brain tumor classification to design an improved\narchitecture which maximizes the accuracy of the classification problem at\nhand; (ii) Investigate the over-fitting problem of CapsNets based on a real set\nof MRI images; (iii) Explore whether or not CapsNets are capable of providing\nbetter fit for the whole brain images or just the segmented tumor, and; (iv)\nDevelop a visualization paradigm for the output of the CapsNet to better\nexplain the learned features. Our results show that the proposed approach can\nsuccessfully overcome CNNs for the brain tumor classification problem.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 22:55:54 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:19:02 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Afshar", "Parnian", ""], ["Mohammadi", "Arash", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "1802.10204", "submitter": "Arash Mohammadi", "authors": "Atefeh Shahroudnejad, Arash Mohammadi, and Konstantinos N. Plataniotis", "title": "Improved Explainability of Capsule Networks: Relevance Path by Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in signal processing and machine learning domains have\nresulted in an extensive surge of interest in deep learning models due to their\nunprecedented performance and high accuracy for different and challenging\nproblems of significant engineering importance. However, when such deep\nlearning architectures are utilized for making critical decisions such as the\nones that involve human lives (e.g., in medical applications), it is of\nparamount importance to understand, trust, and in one word \"explain\" the\nrational behind deep models' decisions. Currently, deep learning models are\ntypically considered as black-box systems, which do not provide any clue on\ntheir internal processing actions. Although some recent efforts have been\ninitiated to explain behavior and decisions of deep networks, explainable\nartificial intelligence (XAI) domain is still in its infancy. In this regard,\nwe consider capsule networks (referred to as CapsNets), which are novel deep\nstructures; recently proposed as an alternative counterpart to convolutional\nneural networks (CNNs), and posed to change the future of machine intelligence.\nIn this paper, we investigate and analyze structures and behaviors of the\nCapsNets and illustrate potential explainability properties of such networks.\nFurthermore, we show possibility of transforming deep learning architectures in\nto transparent networks via incorporation of capsules in different layers\ninstead of convolution layers of the CNNs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 23:08:17 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Shahroudnejad", "Atefeh", ""], ["Mohammadi", "Arash", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "1802.10240", "submitter": "Wenshan Wang", "authors": "Wenshan Wang, Su Yang, Weishan Zhang, Jiulong Zhang", "title": "Neural Aesthetic Image Reviewer", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is a rising interest in perceiving image aesthetics. The\nexisting works deal with image aesthetics as a classification or regression\nproblem. To extend the cognition from rating to reasoning, a deeper\nunderstanding of aesthetics should be based on revealing why a high- or\nlow-aesthetic score should be assigned to an image. From such a point of view,\nwe propose a model referred to as Neural Aesthetic Image Reviewer, which can\nnot only give an aesthetic score for an image, but also generate a textual\ndescription explaining why the image leads to a plausible rating score.\nSpecifically, we propose two multi-task architectures based on shared\naesthetically semantic layers and task-specific embedding layers at a high\nlevel for performance improvement on different tasks. To facilitate researches\non this problem, we collect the AVA-Reviews dataset, which contains 52,118\nimages and 312,708 comments in total. Through multi-task learning, the proposed\nmodels can rate aesthetic images as well as produce comments in an end-to-end\nmanner. It is confirmed that the proposed models outperform the baselines\naccording to the performance evaluation on the AVA-Reviews dataset. Moreover,\nwe demonstrate experimentally that our model can generate textual reviews\nrelated to aesthetics, which are consistent with human perception.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 02:58:30 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Wang", "Wenshan", ""], ["Yang", "Su", ""], ["Zhang", "Weishan", ""], ["Zhang", "Jiulong", ""]]}, {"id": "1802.10249", "submitter": "Lichao Mou", "authors": "Lichao Mou and Xiao Xiang Zhu", "title": "IM2HEIGHT: Height Estimation from Single Monocular Imagery via Fully\n  Residual Convolutional-Deconvolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle a very novel problem, namely height estimation from a\nsingle monocular remote sensing image, which is inherently ambiguous, and a\ntechnically ill-posed problem, with a large source of uncertainty coming from\nthe overall scale. We propose a fully convolutional-deconvolutional network\narchitecture being trained end-to-end, encompassing residual learning, to model\nthe ambiguous mapping between monocular remote sensing images and height maps.\nSpecifically, it is composed of two parts, i.e., convolutional sub-network and\ndeconvolutional sub-network. The former corresponds to feature extractor that\ntransforms the input remote sensing image to high-level multidimensional\nfeature representation, whereas the latter plays the role of a height generator\nthat produces height map from the feature extracted from the convolutional\nsub-network. Moreover, to preserve fine edge details of estimated height maps,\nwe introduce a skip connection to the network, which is able to shuttle\nlow-level visual information, e.g., object boundaries and edges, directly\nacross the network. To demonstrate the usefulness of single-view height\nprediction, we show a practical example of instance segmentation of buildings\nusing estimated height map. This paper, for the first time in the remote\nsensing community, attempts to estimate height from monocular vision. The\nproposed network is validated using a large-scale high resolution aerial image\ndata set covered an area of Berlin. Both visual and quantitative analysis of\nthe experimental results demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:32:36 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1802.10250", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Boyang Li, Vasili Ramanishka, Leonid Sigal, Kate Saenko", "title": "Joint Event Detection and Description in Continuous Video Streams", "comments": "WACV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense video captioning is a fine-grained video understanding task that\ninvolves two sub-problems: localizing distinct events in a long video stream,\nand generating captions for the localized events. We propose the Joint Event\nDetection and Description Network (JEDDi-Net), which solves the dense video\ncaptioning task in an end-to-end fashion. Our model continuously encodes the\ninput video stream with three-dimensional convolutional layers, proposes\nvariable-length temporal events based on pooled features, and generates their\ncaptions. Proposal features are extracted within each proposal segment through\n3D Segment-of-Interest pooling from shared video feature encoding. In order to\nexplicitly model temporal relationships between visual events and their\ncaptions in a single video, we also propose a two-level hierarchical captioning\nmodule that keeps track of context. On the large-scale ActivityNet Captions\ndataset, JEDDi-Net demonstrates improved results as measured by standard\nmetrics. We also present the first dense captioning results on the\nTACoS-MultiLevel dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:40:05 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 23:03:39 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 05:58:28 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Xu", "Huijuan", ""], ["Li", "Boyang", ""], ["Ramanishka", "Vasili", ""], ["Sigal", "Leonid", ""], ["Saenko", "Kate", ""]]}, {"id": "1802.10252", "submitter": "Dong Liu", "authors": "Dong Liu, Ke Sun, Zhangyang Wang, Runsheng Liu, Zheng-Jun Zha", "title": "Frank-Wolfe Network: An Interpretable Deep Structure for Non-Sparse\n  Coding", "comments": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology. Code and pretrained models: https://github.com/sunke123/FW-Net", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2936135", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of $L_p$-norm constrained coding is to convert signal into code\nthat lies inside an $L_p$-ball and most faithfully reconstructs the signal.\nPrevious works under the name of sparse coding considered the cases of $L_0$\nand $L_1$ norms. The cases with $p>1$ values, i.e. non-sparse coding studied in\nthis paper, remain a difficulty. We propose an interpretable deep structure\nnamely Frank-Wolfe Network (F-W Net), whose architecture is inspired by\nunrolling and truncating the Frank-Wolfe algorithm for solving an $L_p$-norm\nconstrained problem with $p\\geq 1$. We show that the Frank-Wolfe solver for the\n$L_p$-norm constraint leads to a novel closed-form nonlinear unit, which is\nparameterized by $p$ and termed $pool_p$. The $pool_p$ unit links the\nconventional pooling, activation, and normalization operations, making F-W Net\ndistinct from existing deep networks either heuristically designed or converted\nfrom projected gradient descent algorithms. We further show that the\nhyper-parameter $p$ can be made learnable instead of pre-chosen in F-W Net,\nwhich gracefully solves the non-sparse coding problem even with unknown $p$. We\nevaluate the performance of F-W Net on an extensive range of simulations as\nwell as the task of handwritten digit recognition, where F-W Net exhibits\nstrong learning capability. We then propose a convolutional version of F-W Net,\nand apply the convolutional F-W Net into image denoising and super-resolution\ntasks, where F-W Net all demonstrates impressive effectiveness, flexibility,\nand robustness.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:49:08 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 10:28:02 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 07:41:35 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 01:28:00 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Liu", "Dong", ""], ["Sun", "Ke", ""], ["Wang", "Zhangyang", ""], ["Liu", "Runsheng", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "1802.10280", "submitter": "Xuhao Chen", "authors": "Xuhao Chen", "title": "Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable accuracy in many artificial\nintelligence applications, e.g. computer vision, at the cost of a large number\nof parameters and high computational complexity. Weight pruning can compress\nDNN models by removing redundant parameters in the networks, but it brings\nsparsity in the weight matrix, and therefore makes the computation inefficient\non GPUs. Although pruning can remove more than 80% of the weights, it actually\nhurts inference performance (speed) when running models on GPUs.\n  Two major problems cause this unsatisfactory performance on GPUs. First,\nlowering convolution onto matrix multiplication reduces data reuse\nopportunities and wastes memory bandwidth. Second, the sparsity brought by\npruning makes the computation irregular, which leads to inefficiency when\nrunning on massively parallel GPUs. To overcome these two limitations, we\npropose Escort, an efficient sparse convolutional neural networks on GPUs.\nInstead of using the lowering method, we choose to compute the sparse\nconvolutions directly. We then orchestrate the parallelism and locality for the\ndirect sparse convolution kernel, and apply customized optimization techniques\nto further improve performance. Evaluation on NVIDIA GPUs show that Escort can\nimprove sparse convolution speed by 2.63x and 3.07x, and inference speed by\n1.43x and 1.69x, compared to CUBLAS and CUSPARSE respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 06:31:45 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 21:11:27 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Chen", "Xuhao", ""]]}, {"id": "1802.10316", "submitter": "Zhengfei Wang", "authors": "Guoxiong Xu, Zhengfei Wang, Hongshi Huang, Wenxin Li, Can Liu, Shilei\n  Liu", "title": "A Model for Medical Diagnosis Based on Plantar Pressure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of determining which disease or condition explains a person's\nsymptoms and signs can be very complicated and may be inaccurate in some cases.\nThe general belief is that diagnosing diseases relies on doctors' keen\nintuition, rich experience and professional equipment. In this work, we employ\nideas from recent advances in plantar pressure research and from the powerful\ncapacity of the convolutional neural network for learning representations.\nHere, we propose a model using convolutional neural network based on plantar\npressure for medical diagnosis. Our model learns a network that maps plantar\npressure data to its corresponding medical diagnostic label. We then apply our\nmodel to make the medical diagnosis on datasets we collected from cooperative\nhospital and achieve an accuracy of 98.36%. We demonstrate that the model base\non the convolutional neural network is competitive in medical diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:12:46 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Xu", "Guoxiong", ""], ["Wang", "Zhengfei", ""], ["Huang", "Hongshi", ""], ["Li", "Wenxin", ""], ["Liu", "Can", ""], ["Liu", "Shilei", ""]]}, {"id": "1802.10328", "submitter": "Tatsunori Taniai", "authors": "Tatsunori Taniai and Takanori Maehara", "title": "Neural Inverse Rendering for General Reflectance Photometric Stereo", "comments": "To appear in International Conference on Machine Learning 2018 (ICML\n  2018). 10 pages + 20 pages (appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel convolutional neural network architecture for photometric\nstereo (Woodham, 1980), a problem of recovering 3D object surface normals from\nmultiple images observed under varying illuminations. Despite its long history\nin computer vision, the problem still shows fundamental challenges for surfaces\nwith unknown general reflectance properties (BRDFs). Leveraging deep neural\nnetworks to learn complicated reflectance models is promising, but studies in\nthis direction are very limited due to difficulties in acquiring accurate\nground truth for training and also in designing networks invariant to\npermutation of input images. In order to address these challenges, we propose a\nphysics based unsupervised learning framework where surface normals and BRDFs\nare predicted by the network and fed into the rendering equation to synthesize\nobserved images. The network weights are optimized during testing by minimizing\nreconstruction loss between observed and synthesized images. Thus, our learning\nprocess does not require ground truth normals or even pre-training on external\nimages. Our method is shown to achieve the state-of-the-art performance on a\nchallenging real-world scene benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:47:20 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 09:15:37 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Taniai", "Tatsunori", ""], ["Maehara", "Takanori", ""]]}, {"id": "1802.10349", "submitter": "Yi-Hsuan Tsai", "authors": "Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan\n  Yang, Manmohan Chandraker", "title": "Learning to Adapt Structured Output Space for Semantic Segmentation", "comments": "Accepted in CVPR'18. Code and model are available at\n  https://github.com/wasidennis/AdaptSegNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network-based approaches for semantic segmentation rely\non supervision with pixel-level ground truth, but may not generalize well to\nunseen image domains. As the labeling process is tedious and labor intensive,\ndeveloping algorithms that can adapt source ground truth labels to the target\ndomain is of great interest. In this paper, we propose an adversarial learning\nmethod for domain adaptation in the context of semantic segmentation.\nConsidering semantic segmentations as structured outputs that contain spatial\nsimilarities between the source and target domains, we adopt adversarial\nlearning in the output space. To further enhance the adapted model, we\nconstruct a multi-level adversarial network to effectively perform output space\ndomain adaptation at different feature levels. Extensive experiments and\nablation study are conducted under various domain adaptation settings,\nincluding synthetic-to-real and cross-city scenarios. We show that the proposed\nmethod performs favorably against the state-of-the-art methods in terms of\naccuracy and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 10:39:26 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 20:50:23 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 06:45:05 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Tsai", "Yi-Hsuan", ""], ["Hung", "Wei-Chih", ""], ["Schulter", "Samuel", ""], ["Sohn", "Kihyuk", ""], ["Yang", "Ming-Hsuan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1802.10367", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do, Ming Cai, Trung Pham, Ian Reid", "title": "Deep-6DPose: Recovering 6D Object Pose from a Single RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects and their 6D poses from only RGB images is an important\ntask for many robotic applications. While deep learning methods have made\nsignificant progress in visual object detection and segmentation, the object\npose estimation task is still challenging. In this paper, we introduce an\nend-toend deep learning framework, named Deep-6DPose, that jointly detects,\nsegments, and most importantly recovers 6D poses of object instances from a\nsingle RGB image. In particular, we extend the recent state-of-the-art instance\nsegmentation network Mask R-CNN with a novel pose estimation branch to directly\nregress 6D object poses without any post-refinements. Our key technical\ncontribution is the decoupling of pose parameters into translation and rotation\nso that the rotation can be regressed via a Lie algebra representation. The\nresulting pose regression loss is differential and unconstrained, making the\ntraining tractable. The experiments on two standard pose benchmarking datasets\nshow that our proposed approach compares favorably with the state-of-the-art\nRGB-based multi-stage pose estimation methods. Importantly, due to the\nend-to-end architecture, Deep-6DPose is considerably faster than competing\nmulti-stage methods, offers an inference speed of 10 fps that is well suited\nfor robotic applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 11:27:41 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Cai", "Ming", ""], ["Pham", "Trung", ""], ["Reid", "Ian", ""]]}, {"id": "1802.10399", "submitter": "Bin Dai", "authors": "Bin Dai, Chen Zhu, David Wipf", "title": "Compressing Neural Networks using the Variational Information Bottleneck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks can be compressed to reduce memory and computational\nrequirements, or to increase accuracy by facilitating the use of a larger base\narchitecture. In this paper we focus on pruning individual neurons, which can\nsimultaneously trim model size, FLOPs, and run-time memory. To improve upon the\nperformance of existing compression algorithms we utilize the information\nbottleneck principle instantiated via a tractable variational bound.\nMinimization of this information theoretic bound reduces the redundancy between\nadjacent layers by aggregating useful information into a subset of neurons that\ncan be preserved. In contrast, the activations of disposable neurons are shut\noff via an attractive form of sparse regularization that emerges naturally from\nthis framework, providing tangible advantages over traditional sparsity\npenalties without contributing additional tuning parameters to the energy\nlandscape. We demonstrate state-of-the-art compression rates across an array of\ndatasets and network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 13:26:46 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 16:04:06 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 13:31:16 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Dai", "Bin", ""], ["Zhu", "Chen", ""], ["Wipf", "David", ""]]}, {"id": "1802.10419", "submitter": "Yibo Yang", "authors": "Yibo Yang, Zhisheng Zhong, Tiancheng Shen, Zhouchen Lin", "title": "Convolutional Neural Networks with Alternately Updated Clique", "comments": "Accepted in CVPR 2018 as Oral presentation. Code available at\n  https://github.com/iboing/CliqueNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving information flow in deep networks helps to ease the training\ndifficulties and utilize parameters more efficiently. Here we propose a new\nconvolutional neural network architecture with alternately updated clique\n(CliqueNet). In contrast to prior networks, there are both forward and backward\nconnections between any two layers in the same block. The layers are\nconstructed as a loop and are updated alternately. The CliqueNet has some\nunique properties. For each layer, it is both the input and output of any other\nlayer in the same block, so that the information flow among layers is\nmaximized. During propagation, the newly updated layers are concatenated to\nre-update previously updated layer, and parameters are reused for multiple\ntimes. This recurrent feedback structure is able to bring higher level visual\ninformation back to refine low-level filters and achieve spatial attention. We\nanalyze the features generated at different stages and observe that using\nrefined features leads to a better result. We adopt a multi-scale feature\nstrategy that effectively avoids the progressive growth of parameters.\nExperiments on image recognition datasets including CIFAR-10, CIFAR-100, SVHN\nand ImageNet show that our proposed models achieve the state-of-the-art\nperformance with fewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 14:04:38 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 09:33:40 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 07:58:04 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Yang", "Yibo", ""], ["Zhong", "Zhisheng", ""], ["Shen", "Tiancheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1802.10426", "submitter": "Hamed Alizadeh Ghazijahani", "authors": "Hossein Nejati, Hamed Alizadeh Ghazijahani, Milad Abdollahzadeh, Tooba\n  Malekzadeh, Ngai-Man Cheung, Kheng Hock Lee, Lian Leng Low", "title": "Fine-grained wound tissue analysis using deep neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue assessment for chronic wounds is the basis of wound grading and\nselection of treatment approaches. While several image processing approaches\nhave been proposed for automatic wound tissue analysis, there has been a\nshortcoming in these approaches for clinical practices. In particular,\nseemingly, all previous approaches have assumed only 3 tissue types in the\nchronic wounds, while these wounds commonly exhibit 7 distinct tissue types\nthat presence of each one changes the treatment procedure. In this paper, for\nthe first time, we investigate the classification of 7 wound issue types. We\nwork with wound professionals to build a new database of 7 types of wound\ntissue. We propose to use pre-trained deep neural networks for feature\nextraction and classification at the patch-level. We perform experiments to\ndemonstrate that our approach outperforms other state-of-the-art. We will make\nour database publicly available to facilitate research in wound assessment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 14:18:36 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Nejati", "Hossein", ""], ["Ghazijahani", "Hamed Alizadeh", ""], ["Abdollahzadeh", "Milad", ""], ["Malekzadeh", "Tooba", ""], ["Cheung", "Ngai-Man", ""], ["Lee", "Kheng Hock", ""], ["Low", "Lian Leng", ""]]}, {"id": "1802.10437", "submitter": "Keyan Ding", "authors": "Keyan Ding, Linfang Xiao", "title": "A Simple Method to improve Initialization Robustness for Active Contours\n  driven by Local Region Fitting Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active contour models based on local region fitting energy can segment images\nwith intensity inhomogeneity effectively, but their segmentation results are\neasy to error if the initial contour is inappropriate. In this paper, we\npresent a simple and universal method of improving the robustness of initial\ncontour for these local fitting-based models. The core idea of proposed method\nis exchanging the fitting values on the two sides of contour, so that the\nfitting values inside the contour are always larger (or smaller) than the\nvalues outside the contour in the process of curve evolution. In this way, the\nwhole curve will evolve along the inner (or outer) boundaries of object, and\nless likely to be stuck in the object or background. Experimental results have\nproved that using the proposed method can enhance the robustness of initial\ncontour and meanwhile keep the original advantages in the local fitting-based\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 14:43:00 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 02:36:40 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Ding", "Keyan", ""], ["Xiao", "Linfang", ""]]}, {"id": "1802.10478", "submitter": "Gang Bai", "authors": "Yanan Luo, Jie Zou, Chengfei Yao, Tao Li, Gang Bai", "title": "HSI-CNN: A Novel Convolution Neural Network for Hyperspectral Image", "comments": "6 pages, 8 figures, Under review as a conference paper at\n  International Conference on Pattern Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, the performance of hyperspectral image\n(HSI) classification has been greatly improved in recent years. The shortage of\ntraining samples has become a bottleneck for further improvement of\nperformance. In this paper, we propose a novel convolutional neural network\nframework for the characteristics of hyperspectral image data, called HSI-CNN.\nFirstly, the spectral-spatial feature is extracted from a target pixel and its\nneighbors. Then, a number of one-dimensional feature maps, obtained by\nconvolution operation on spectral-spatial features, are stacked into a\ntwo-dimensional matrix. Finally, the two-dimensional matrix considered as an\nimage is fed into standard CNN. This is why we call it HSI-CNN. In addition, we\nalso implements two depth network classification models, called HSI-CNN+XGBoost\nand HSI-CapsNet, in order to compare the performance of our framework.\nExperiments show that the performance of hyperspectral image classification is\nimproved efficiently with HSI-CNN framework. We evaluate the model's\nperformance using four popular HSI datasets, which are the Kennedy Space Center\n(KSC), Indian Pines (IP), Pavia University scene (PU) and Salinas scene (SA).\nAs far as we concerned, HSI-CNN has got the state-of-art accuracy among all\nmethods we have known on these datasets of 99.28%, 99.09%, 99.42%, 98.95%\nseparately.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 15:31:20 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Luo", "Yanan", ""], ["Zou", "Jie", ""], ["Yao", "Chengfei", ""], ["Li", "Tao", ""], ["Bai", "Gang", ""]]}, {"id": "1802.10508", "submitter": "Fabian Isensee", "authors": "Fabian Isensee, Philipp Kickingereder, Wolfgang Wick, Martin Bendszus,\n  Klaus H. Maier-Hein", "title": "Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution\n  to the BRATS 2017 Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of brain tumors is critical for clinical decision\nmaking. While manual segmentation is tedious, time consuming and subjective,\nthis task is at the same time very challenging to solve for automatic\nsegmentation methods. In this paper we present our most recent effort on\ndeveloping a robust segmentation algorithm in the form of a convolutional\nneural network. Our network architecture was inspired by the popular U-Net and\nhas been carefully modified to maximize brain tumor segmentation performance.\nWe use a dice loss function to cope with class imbalances and use extensive\ndata augmentation to successfully prevent overfitting. Our method beats the\ncurrent state of the art on BraTS 2015, is one of the leading methods on the\nBraTS 2017 validation set (dice scores of 0.896, 0.797 and 0.732 for whole\ntumor, tumor core and enhancing tumor, respectively) and achieves very good\nDice scores on the test set (0.858 for whole, 0.775 for core and 0.647 for\nenhancing tumor). We furthermore take part in the survival prediction\nsubchallenge by training an ensemble of a random forest regressor and\nmultilayer perceptrons on shape features describing the tumor subregions. Our\napproach achieves 52.6% accuracy, a Spearman correlation coefficient of 0.496\nand a mean square error of 209607 on the test set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:19:45 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Isensee", "Fabian", ""], ["Kickingereder", "Philipp", ""], ["Wick", "Wolfgang", ""], ["Bendszus", "Martin", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1802.10548", "submitter": "Carlos Xavier Hernandez", "authors": "Carlos X. Hern\\'andez and Mohammad M. Sultan and Vijay S. Pande", "title": "Using Deep Learning for Segmentation and Counting within Microscopy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell counting is a ubiquitous, yet tedious task that would greatly benefit\nfrom automation. From basic biological questions to clinical trials, cell\ncounts provide key quantitative feedback that drive research. Unfortunately,\ncell counting is most commonly a manual task and can be time-intensive. The\ntask is made even more difficult due to overlapping cells, existence of\nmultiple focal planes, and poor imaging quality, among other factors. Here, we\ndescribe a convolutional neural network approach, using a recently described\nfeature pyramid network combined with a VGG-style neural network, for\nsegmenting and subsequent counting of cells in a given microscopy image.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:31:16 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Hern\u00e1ndez", "Carlos X.", ""], ["Sultan", "Mohammad M.", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1802.10554", "submitter": "Loic Peter", "authors": "Lo\\\"ic Peter and Marcel Tella-Amo and Dzhoshkun Ismail Shakir and\n  George Attilakos and Ruwan Wimalasundera and Jan Deprest and S\\'ebastien\n  Ourselin and Tom Vercauteren", "title": "Retrieval and Registration of Long-Range Overlapping Frames for Scalable\n  Mosaicking of In Vivo Fetoscopy", "comments": "Accepted for publication in International Journal of Computer\n  Assisted Radiology and Surgery (IJCARS)", "journal-ref": null, "doi": "10.1007/s11548-018-1728-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The standard clinical treatment of Twin-to-Twin Transfusion Syndrome\nconsists in the photo-coagulation of undesired anastomoses located on the\nplacenta which are responsible to a blood transfer between the two twins. While\nbeing the standard of care procedure, fetoscopy suffers from a limited\nfield-of-view of the placenta resulting in missed anastomoses. To facilitate\nthe task of the clinician, building a global map of the placenta providing a\nlarger overview of the vascular network is highly desired. Methods: To overcome\nthe challenging visual conditions inherent to in vivo sequences (low contrast,\nobstructions or presence of artifacts, among others), we propose the following\ncontributions: (i) robust pairwise registration is achieved by aligning the\norientation of the image gradients, and (ii) difficulties regarding long-range\nconsistency (e.g. due to the presence of outliers) is tackled via a bag-of-word\nstrategy, which identifies overlapping frames of the sequence to be registered\nregardless of their respective location in time. Results: In addition to visual\ndifficulties, in vivo sequences are characterised by the intrinsic absence of\ngold standard. We present mosaics motivating qualitatively our methodological\nchoices and demonstrating their promising aspect. We also demonstrate\nsemi-quantitatively, via visual inspection of registration results, the\nefficacy of our registration approach in comparison to two standard baselines.\nConclusion: This paper proposes the first approach for the construction of\nmosaics of placenta in in vivo fetoscopy sequences. Robustness to visual\nchallenges during registration and long-range temporal consistency are\nproposed, offering first positive results on in vivo data for which standard\nmosaicking techniques are not applicable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:44:02 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Peter", "Lo\u00efc", ""], ["Tella-Amo", "Marcel", ""], ["Shakir", "Dzhoshkun Ismail", ""], ["Attilakos", "George", ""], ["Wimalasundera", "Ruwan", ""], ["Deprest", "Jan", ""], ["Ourselin", "S\u00e9bastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1802.10560", "submitter": "Mark Kliger", "authors": "Mark Kliger and Shachar Fleishman", "title": "Novelty Detection with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of a classifier to recognize unknown inputs is important for many\nclassification-based systems. We discuss the problem of simultaneous\nclassification and novelty detection, i.e. determining whether an input is from\nthe known set of classes and from which specific class, or from an unknown\ndomain and does not belong to any of the known classes. We propose a method\nbased on the Generative Adversarial Networks (GAN) framework. We show that a\nmulti-class discriminator trained with a generator that generates samples from\na mixture of nominal and novel data distributions is the optimal novelty\ndetector. We approximate that generator with a mixture generator trained with\nthe Feature Matching loss and empirically show that the proposed method\noutperforms conventional methods for novelty detection. Our findings\ndemonstrate a simple, yet powerful new application of the GAN framework for the\ntask of novelty detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:50:19 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kliger", "Mark", ""], ["Fleishman", "Shachar", ""]]}, {"id": "1802.10591", "submitter": "Dongdong Chen", "authors": "Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, Gang Hua", "title": "Stereoscopic Neural Style Transfer", "comments": "Accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first attempt at stereoscopic neural style transfer,\nwhich responds to the emerging demand for 3D movies or AR/VR. We start with a\ncareful examination of applying existing monocular style transfer methods to\nleft and right views of stereoscopic images separately. This reveals that the\noriginal disparity consistency cannot be well preserved in the final\nstylization results, which causes 3D fatigue to the viewers. To address this\nissue, we incorporate a new disparity loss into the widely adopted style loss\nfunction by enforcing the bidirectional disparity constraint in non-occluded\nregions. For a practical real-time solution, we propose the first feed-forward\nnetwork by jointly training a stylization sub-network and a disparity\nsub-network, and integrate them in a feature level middle domain. Our disparity\nsub-network is also the first end-to-end network for simultaneous bidirectional\ndisparity and occlusion mask estimation. Finally, our network is effectively\nextended to stereoscopic videos, by considering both temporal coherence and\ndisparity consistency. We will show that the proposed method clearly\noutperforms the baseline algorithms both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:58:10 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 22:47:39 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Liao", "Jing", ""], ["Yu", "Nenghai", ""], ["Hua", "Gang", ""]]}]