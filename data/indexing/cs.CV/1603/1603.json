[{"id": "1603.00110", "submitter": "Pan Ji", "authors": "Pan Ji, Hongdong Li, Mathieu Salzmann, and Yiran Zhong", "title": "Robust Multi-body Feature Tracker: A Segmentation-free Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature tracking is a fundamental problem in computer vision, with\napplications in many computer vision tasks, such as visual SLAM and action\nrecognition. This paper introduces a novel multi-body feature tracker that\nexploits a multi-body rigidity assumption to improve tracking robustness under\na general perspective camera model. A conventional approach to addressing this\nproblem would consist of alternating between solving two subtasks: motion\nsegmentation and feature tracking under rigidity constraints for each segment.\nThis approach, however, requires knowing the number of motions, as well as\nassigning points to motion groups, which is typically sensitive to the motion\nestimates. By contrast, here, we introduce a segmentation-free solution to\nmulti-body feature tracking that bypasses the motion assignment step and\nreduces to solving a series of subproblems with closed-form solutions. Our\nexperiments demonstrate the benefits of our approach in terms of tracking\naccuracy and robustness to noise.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 01:46:44 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 03:03:52 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Ji", "Pan", ""], ["Li", "Hongdong", ""], ["Salzmann", "Mathieu", ""], ["Zhong", "Yiran", ""]]}, {"id": "1603.00124", "submitter": "Yanwei Pang", "authors": "Jiale Cao, Yanwei Pang, and Xuelong Li", "title": "Learning Multilayer Channel Features for Pedestrian Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2694224", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection based on the combination of Convolutional Neural Network\n(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved\ngreat success. Generally, HOG+LUV are used to generate the candidate proposals\nand then CNN classifies these proposals. Despite its success, there is still\nroom for improvement. For example, CNN classifies these proposals by the\nfull-connected layer features while proposal scores and the features in the\ninner-layers of CNN are ignored. In this paper, we propose a unifying framework\ncalled Multilayer Channel Features (MCF) to overcome the drawback. It firstly\nintegrates HOG+LUV with each layer of CNN into a multi-layer image channels.\nBased on the multi-layer image channels, a multi-stage cascade AdaBoost is then\nlearned. The weak classifiers in each stage of the multi-stage cascade is\nlearned from the image channels of corresponding layer. With more abundant\nfeatures, MCF achieves the state-of-the-art on Caltech pedestrian dataset\n(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves\n7.98% miss rate. As many non-pedestrian detection windows can be quickly\nrejected by the first few stages, it accelerates detection speed by 1.43 times.\nBy eliminating the highly overlapped detection windows with lower scores after\nthe first stage, it's 4.07 times faster with negligible performance loss.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 03:25:45 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Cao", "Jiale", ""], ["Pang", "Yanwei", ""], ["Li", "Xuelong", ""]]}, {"id": "1603.00128", "submitter": "Yanwei Pang", "authors": "Xiaoheng Jiang, Yanwei Pang, Manli Sun, and Xuelong Li", "title": "Cascaded Subpatch Networks for Effective CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Convolutional Neural Networks (CNNs) use either a linear or\nnon-linear filter to extract features from an image patch (region) of spatial\nsize $ H\\times W $ (Typically, $ H $ is small and is equal to $ W$, e.g., $ H $\nis 5 or 7). Generally, the size of the filter is equal to the size $ H\\times W\n$ of the input patch. We argue that the representation ability of equal-size\nstrategy is not strong enough. To overcome the drawback, we propose to use\nsubpatch filter whose spatial size $ h\\times w $ is smaller than $ H\\times W $.\nThe proposed subpatch filter consists of two subsequent filters. The first one\nis a linear filter of spatial size $ h\\times w $ and is aimed at extracting\nfeatures from spatial domain. The second one is of spatial size $ 1\\times 1 $\nand is used for strengthening the connection between different input feature\nchannels and for reducing the number of parameters. The subpatch filter\nconvolves with the input patch and the resulting network is called a subpatch\nnetwork. Taking the output of one subpatch network as input, we further repeat\nconstructing subpatch networks until the output contains only one neuron in\nspatial domain. These subpatch networks form a new network called Cascaded\nSubpatch Network (CSNet). The feature layer generated by CSNet is called csconv\nlayer. For the whole input image, we construct a deep neural network by\nstacking a sequence of csconv layers. Experimental results on four benchmark\ndatasets demonstrate the effectiveness and compactness of the proposed CSNet.\nFor example, our CSNet reaches a test error of $ 5.68\\% $ on the CIFAR10\ndataset without model averaging. To the best of our knowledge, this is the best\nresult ever obtained on the CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 03:44:49 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Jiang", "Xiaoheng", ""], ["Pang", "Yanwei", ""], ["Sun", "Manli", ""], ["Li", "Xuelong", ""]]}, {"id": "1603.00132", "submitter": "Zexi Hu", "authors": "Zexi Hu, Yuefang Gao, Dong Wang, Xuhong Tian", "title": "A Universal Update-pacing Framework For Visual Tracking", "comments": "Submitted to ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework to alleviate the model drift problem in\nvisual tracking, which is based on paced updates and trajectory selection.\nGiven a base tracker, an ensemble of trackers is generated, in which each\ntracker's update behavior will be paced and then traces the target object\nforward and backward to generate a pair of trajectories in an interval. Then,\nwe implicitly perform self-examination based on trajectory pair of each tracker\nand select the most robust tracker. The proposed framework can effectively\nleverage temporal context of sequential frames and avoid to learn corrupted\ninformation. Extensive experiments on the standard benchmark suggest that the\nproposed framework achieves superior performance against state-of-the-art\ntrackers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 04:12:41 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Hu", "Zexi", ""], ["Gao", "Yuefang", ""], ["Wang", "Dong", ""], ["Tian", "Xuhong", ""]]}, {"id": "1603.00146", "submitter": "Yu Zhang", "authors": "Yu Zhang, Stephen Wistar, Jia Li, Michael Steinberg, and James Z. Wang", "title": "Storm Detection by Visual Learning Using Satellite Images", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2016.2618929", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers are widely utilized in today's weather forecasting as a powerful\ntool to leverage an enormous amount of data. Yet, despite the availability of\nsuch data, current techniques often fall short of producing reliable detailed\nstorm forecasts. Each year severe thunderstorms cause significant damage and\nloss of life, some of which could be avoided if better forecasts were\navailable. We propose a computer algorithm that analyzes satellite images from\nhistorical archives to locate visual signatures of severe thunderstorms for\nshort-term predictions. While computers are involved in weather forecasts to\nsolve numerical models based on sensory data, they are less competent in\nforecasting based on visual patterns from satellite images. In our system, we\nextract and summarize important visual storm evidence from satellite image\nsequences in the way that meteorologists interpret the images. In particular,\nthe algorithm extracts and fits local cloud motion from image sequences to\nmodel the storm-related cloud patches. Image data from the year 2008 have been\nadopted to train the model, and historical thunderstorm reports in continental\nUS from 2000 through 2013 have been used as the ground-truth and priors in the\nmodeling process. Experiments demonstrate the usefulness and potential of the\nalgorithm for producing more accurate thunderstorm forecasts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 05:26:59 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zhang", "Yu", ""], ["Wistar", "Stephen", ""], ["Li", "Jia", ""], ["Steinberg", "Michael", ""], ["Wang", "James Z.", ""]]}, {"id": "1603.00150", "submitter": "Dylan Campbell", "authors": "Dylan Campbell and Lars Petersson", "title": "GOGMA: Globally-Optimal Gaussian Mixture Alignment", "comments": "Manuscript in press 2016 IEEE Conference on Computer Vision and\n  Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture alignment is a family of approaches that are frequently used\nfor robustly solving the point-set registration problem. However, since they\nuse local optimisation, they are susceptible to local minima and can only\nguarantee local optimality. Consequently, their accuracy is strongly dependent\non the quality of the initialisation. This paper presents the first\nglobally-optimal solution to the 3D rigid Gaussian mixture alignment problem\nunder the L2 distance between mixtures. The algorithm, named GOGMA, employs a\nbranch-and-bound approach to search the space of 3D rigid motions SE(3),\nguaranteeing global optimality regardless of the initialisation. The geometry\nof SE(3) was used to find novel upper and lower bounds for the objective\nfunction and local optimisation was integrated into the scheme to accelerate\nconvergence without voiding the optimality guarantee. The evaluation\nempirically supported the optimality proof and showed that the method performed\nmuch more robustly on two challenging datasets than an existing\nglobally-optimal registration solution.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 05:38:50 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Campbell", "Dylan", ""], ["Petersson", "Lars", ""]]}, {"id": "1603.00173", "submitter": "Giuseppe Sergioli", "authors": "Giuseppe Sergioli, Enrica Santucci, Luca Didaci, Jaroslaw A. Miszczak,\n  Roberto Giuntini", "title": "Pattern recognition on the quantum Bloch sphere", "comments": null, "journal-ref": "Soft Comput (2017)", "doi": "10.1007/s00500-016-2478-2", "report-no": null, "categories": "quant-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework suitable for describing pattern recognition task\nusing the mathematical language of density matrices. In particular, we provide\na one-to-one correspondence between patterns and pure density operators. This\ncorrespondence enables us to: i) represent the Nearest Mean Classifier (NMC) in\nterms of quantum objects, ii) introduce a Quantum Classifier (QC). By comparing\nthe QC with the NMC on different 2D datasets, we show the first classifier can\nprovide additional information that are particularly beneficial on a classical\ncomputer with respect to the second classifier.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 08:11:53 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 09:46:24 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Sergioli", "Giuseppe", ""], ["Santucci", "Enrica", ""], ["Didaci", "Luca", ""], ["Miszczak", "Jaroslaw A.", ""], ["Giuntini", "Roberto", ""]]}, {"id": "1603.00275", "submitter": "Korsuk Sirinukunwattana", "authors": "Korsuk Sirinukunwattana, Josien P. W. Pluim, Hao Chen, Xiaojuan Qi,\n  Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J. Matuszewski, Elia Bruni,\n  Urko Sanchez, Anton B\\\"ohm, Olaf Ronneberger, Bassem Ben Cheikh, Daniel\n  Racoceanu, Philipp Kainz, Michael Pfeiffer, Martin Urschler, David R. J.\n  Snead, Nasir M. Rajpoot", "title": "Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal adenocarcinoma originating in intestinal glandular structures is\nthe most common form of colon cancer. In clinical practice, the morphology of\nintestinal glands, including architectural appearance and glandular formation,\nis used by pathologists to inform prognosis and plan the treatment of\nindividual patients. However, achieving good inter-observer as well as\nintra-observer reproducibility of cancer grading is still a major challenge in\nmodern pathology. An automated approach which quantifies the morphology of\nglands is a solution to the problem. This paper provides an overview to the\nGland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at\nMICCAI'2015. Details of the challenge, including organization, dataset and\nevaluation criteria, are presented, along with the method descriptions and\nevaluation results from the top performing methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 13:53:48 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 14:18:51 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Sirinukunwattana", "Korsuk", ""], ["Pluim", "Josien P. W.", ""], ["Chen", "Hao", ""], ["Qi", "Xiaojuan", ""], ["Heng", "Pheng-Ann", ""], ["Guo", "Yun Bo", ""], ["Wang", "Li Yang", ""], ["Matuszewski", "Bogdan J.", ""], ["Bruni", "Elia", ""], ["Sanchez", "Urko", ""], ["B\u00f6hm", "Anton", ""], ["Ronneberger", "Olaf", ""], ["Cheikh", "Bassem Ben", ""], ["Racoceanu", "Daniel", ""], ["Kainz", "Philipp", ""], ["Pfeiffer", "Michael", ""], ["Urschler", "Martin", ""], ["Snead", "David R. J.", ""], ["Rajpoot", "Nasir M.", ""]]}, {"id": "1603.00284", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Stephen Becker", "title": "Dual Smoothing and Level Set Techniques for Variational Matrix\n  Decomposition", "comments": "38 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1406.1089", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the robust principal component analysis (RPCA) problem, and\nreview a range of old and new convex formulations for the problem and its\nvariants. We then review dual smoothing and level set techniques in convex\noptimization, present several novel theoretical results, and apply the\ntechniques on the RPCA problem. In the final sections, we show a range of\nnumerical experiments for simulated and real-world problems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 14:33:12 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Becker", "Stephen", ""]]}, {"id": "1603.00370", "submitter": "Cijo Jose", "authors": "Cijo Jose and Francois Fleuret", "title": "Scalable Metric Learning via Weighted Approximate Rank Component\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the large-scale learning of Mahalanobis distances, with\na particular focus on person re-identification.\n  We propose a metric learning formulation called Weighted Approximate Rank\nComponent Analysis (WARCA). WARCA optimizes the precision at top ranks by\ncombining the WARP loss with a regularizer that favors orthonormal linear\nmappings, and avoids rank-deficient embeddings. Using this new regularizer\nallows us to adapt the large-scale WSABIE procedure and to leverage the Adam\nstochastic optimization algorithm, which results in an algorithm that scales\ngracefully to very large data-sets. Also, we derive a kernelized version which\nallows to take advantage of state-of-the-art features for re-identification\nwhen data-set size permits kernel computation.\n  Benchmarks on recent and standard re-identification data-sets show that our\nmethod beats existing state-of-the-art techniques both in term of accuracy and\nspeed. We also provide experimental analysis to shade lights on the properties\nof the regularizer we use, and how it improves performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 17:32:09 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 14:56:19 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Jose", "Cijo", ""], ["Fleuret", "Francois", ""]]}, {"id": "1603.00437", "submitter": "Tales Cesar de Oliveira Imbiriba", "authors": "Tales Imbiriba (1), Jos\\'e Carlos Moreira Bermudez (1), C\\'edric\n  Richard (2) ((1) Federal University of Santa Catarina, Florian\\'opolis, SC,\n  Brazil, (2) Universit\\'e de Nice Sophia-Antipolis, CNRS, Nice, France)", "title": "Technical Report: Band selection for nonlinear unmixing of hyperspectral\n  images as a maximal clique problem", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2676344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based nonlinear mixing models have been applied to unmix spectral\ninformation of hyperspectral images when the type of mixing occurring in the\nscene is too complex or unknown. Such methods, however, usually require the\ninversion of matrices of sizes equal to the number of spectral bands. Reducing\nthe computational load of these methods remains a challenge in large scale\napplications. This paper proposes a centralized method for band selection (BS)\nin the reproducing kernel Hilbert space (RKHS). It is based upon the coherence\ncriterion, which sets the largest value allowed for correlations between the\nbasis kernel functions characterizing the unmixing model. We show that the\nproposed BS approach is equivalent to solving a maximum clique problem (MCP),\nthat is, searching for the biggest complete subgraph in a graph. Furthermore,\nwe devise a strategy for selecting the coherence threshold and the Gaussian\nkernel bandwidth using coherence bounds for linearly independent bases.\nSimulation results illustrate the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:08:51 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 22:53:26 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1603.00438", "submitter": "Team Lear", "authors": "Mattis Paulin (LEAR), Julien Mairal (LEAR), Matthijs Douze (LEAR),\n  Zaid Harchaoui (NYU), Florent Perronnin, Cordelia Schmid (LEAR)", "title": "Convolutional Patch Representations for Image Retrieval: an Unsupervised\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently received a lot of\nattention due to their ability to model local stationary structures in natural\nimages in a multi-scale fashion, when learning all model parameters with\nsupervision. While excellent performance was achieved for image classification\nwhen large amounts of labeled visual data are available, their success for\nun-supervised tasks such as image retrieval has been moderate so far. Our paper\nfocuses on this latter setting and explores several methods for learning patch\ndescriptors without supervision with application to matching and instance-level\nretrieval. To that effect, we propose a new family of convolutional descriptors\nfor patch representation , based on the recently introduced convolutional\nkernel networks. We show that our descriptor, named Patch-CKN, performs better\nthan SIFT as well as other convolutional networks learned by artificially\nintroducing supervision and is significantly faster to train. To demonstrate\nits effectiveness, we perform an extensive evaluation on standard benchmarks\nfor patch and image retrieval where we obtain state-of-the-art results. We also\nintroduce a new dataset called RomePatches, which allows to simultaneously\nstudy descriptor performance for patch and image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:13:07 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Paulin", "Mattis", "", "LEAR"], ["Mairal", "Julien", "", "LEAR"], ["Douze", "Matthijs", "", "LEAR"], ["Harchaoui", "Zaid", "", "NYU"], ["Perronnin", "Florent", "", "LEAR"], ["Schmid", "Cordelia", "", "LEAR"]]}, {"id": "1603.00489", "submitter": "Archith Bency", "authors": "Archith J. Bency, Heesung Kwon, Hyungtae Lee, S. Karthikeyan and B. S.\n  Manjunath", "title": "Weakly Supervised Localization using Deep Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object localization is an important computer vision problem with a variety of\napplications. The lack of large scale object-level annotations and the relative\nabundance of image-level labels makes a compelling case for weak supervision in\nthe object localization task. Deep Convolutional Neural Networks are a class of\nstate-of-the-art methods for the related problem of object recognition. In this\npaper, we describe a novel object localization algorithm which uses\nclassification networks trained on only image labels. This weakly supervised\nmethod leverages local spatial and semantic patterns captured in the\nconvolutional layers of classification networks. We propose an efficient beam\nsearch based approach to detect and localize multiple objects in images. The\nproposed method significantly outperforms the state-of-the-art in standard\nobject localization data-sets with a 8 point increase in mAP scores.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 21:07:09 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 01:10:00 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Bency", "Archith J.", ""], ["Kwon", "Heesung", ""], ["Lee", "Hyungtae", ""], ["Karthikeyan", "S.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1603.00502", "submitter": "J.T. Turner", "authors": "JT Turner, Kalyan Gupta, Brendan Morris, David W. Aha", "title": "Keypoint Density-based Region Proposal for Fine-Grained Object Detection\n  and Classification using Regions with Convolutional Neural Network Features", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent advances in regional Convolutional Neural Networks (CNNs)\nenable them to outperform conventional techniques on standard object detection\nand classification tasks, their response time is still slow for real-time\nperformance. To address this issue, we propose a method for region proposal as\nan alternative to selective search, which is used in current state-of-the art\nobject detection algorithms. We evaluate our Keypoint Density-based Region\nProposal (KDRP) approach and show that it speeds up detection and\nclassification on fine-grained tasks by 100% versus the existing selective\nsearch region proposal technique without compromising classification accuracy.\nKDRP makes the application of CNNs to real-time detection and classification\nfeasible.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 21:48:58 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Turner", "JT", ""], ["Gupta", "Kalyan", ""], ["Morris", "Brendan", ""], ["Aha", "David W.", ""]]}, {"id": "1603.00546", "submitter": "Jan Egger", "authors": "Jan Egger, Philip Voglreiter, Mark Dokter, Michael Hofmann, Xiaojun\n  Chen, Wolfram G. Zoller, Dieter Schmalstieg, Alexander Hann", "title": "US-Cut: Interactive Algorithm for rapid Detection and Segmentation of\n  Liver Tumors in Ultrasound Acquisitions", "comments": "6 pages, 6 figures, 1 table, 32 references", "journal-ref": "SPIE Medical Imaging Conference 2016, Paper 9790-47", "doi": "10.1117/12.2216509", "report-no": null, "categories": "cs.CV cs.CE cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is the most commonly used liver imaging modality worldwide.\nIt plays an important role in follow-up of cancer patients with liver\nmetastases. We present an interactive segmentation approach for liver tumors in\nUS acquisitions. Due to the low image quality and the low contrast between the\ntumors and the surrounding tissue in US images, the segmentation is very\nchallenging. Thus, the clinical practice still relies on manual measurement and\noutlining of the tumors in the US images. We target this problem by applying an\ninteractive segmentation algorithm to the US data, allowing the user to get\nreal-time feedback of the segmentation results. The algorithm has been\ndeveloped and tested hand-in-hand by physicians and computer scientists to make\nsure a future practical usage in a clinical setting is feasible. To cover\ntypical acquisitions from the clinical routine, the approach has been evaluated\nwith dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic\n(darker) or isoechoic (similar) in comparison to the surrounding liver tissue.\nDue to the interactive real-time behavior of the approach, it was possible even\nin difficult cases to find satisfying segmentations of the tumors within\nseconds and without parameter settings, and the average tumor deviation was\nonly 1.4mm compared with manual measurements. However, the long term goal is to\nease the volumetric acquisition of liver tumors in order to evaluate for\ntreatment response. Additional aim is the registration of intraoperative US\nimages via the interactive segmentations to the patient's pre-interventional CT\nacquisitions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 01:42:48 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Egger", "Jan", ""], ["Voglreiter", "Philip", ""], ["Dokter", "Mark", ""], ["Hofmann", "Michael", ""], ["Chen", "Xiaojun", ""], ["Zoller", "Wolfram G.", ""], ["Schmalstieg", "Dieter", ""], ["Hann", "Alexander", ""]]}, {"id": "1603.00550", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha", "title": "Synthesized Classifiers for Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given semantic descriptions of object classes, zero-shot learning aims to\naccurately recognize objects of the unseen classes, from which no examples are\navailable at the training stage, by associating them to the seen classes, from\nwhich labeled examples are provided. We propose to tackle this problem from the\nperspective of manifold learning. Our main idea is to align the semantic space\nthat is derived from external information to the model space that concerns\nitself with recognizing visual features. To this end, we introduce a set of\n\"phantom\" object classes whose coordinates live in both the semantic space and\nthe model space. Serving as bases in a dictionary, they can be optimized from\nlabeled data such that the synthesized real object classifiers achieve optimal\ndiscriminative performance. We demonstrate superior accuracy of our approach\nover the state of the art on four benchmark datasets for zero-shot learning,\nincluding the full ImageNet Fall 2011 dataset with more than 20,000 unseen\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 01:59:22 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 18:49:13 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 21:48:48 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Chao", "Wei-Lun", ""], ["Gong", "Boqing", ""], ["Sha", "Fei", ""]]}, {"id": "1603.00560", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Learnt quasi-transitive similarity for retrieval from large collections\n  of faces", "comments": "International Conference on Computer Vision and Pattern Recognition,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in identity-based retrieval of face sets from large\nunlabelled collections acquired in uncontrolled environments. Given a baseline\nalgorithm for measuring the similarity of two face sets, the meta-algorithm\nintroduced in this paper seeks to leverage the structure of the data corpus to\nmake the best use of the available baseline. In particular, we show how partial\ntransitivity of inter-personal similarity can be exploited to improve the\nretrieval of particularly challenging sets which poorly match the query under\nthe baseline measure. We: (i) describe the use of proxy sets as a means of\ncomputing the similarity between two sets, (ii) introduce transitivity\nmeta-features based on the similarity of salient modes of appearance variation\nbetween sets, (iii) show how quasi-transitivity can be learnt from such\nfeatures without any labelling or manual intervention, and (iv) demonstrate the\neffectiveness of the proposed methodology through experiments on the\nnotoriously challenging YouTube database and two successful baselines from the\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 03:04:37 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 01:34:24 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1603.00802", "submitter": "Christoph Adami", "authors": "Ali Tehrani-Saleh and Christoph Adami", "title": "Flies as Ship Captains? Digital Evolution Unravels Selective Pressures\n  to Avoid Collision in Drosophila", "comments": "8 pages, 10 figures, submitted to 15th Artificial Life conference\n  (ALife 2016)", "journal-ref": "Proceedings Artificial Life 15 (C. Gershenson, T. Froese, J.M.\n  Sisqueiros, W. Aguilar, E.J. Izquierdo, H. Sayama, eds.) MIT Press\n  (Cambridge, MA, 2016), pp. 554-561", "doi": "10.7551/978-0-262-33936-0-ch089", "report-no": null, "categories": "q-bio.PE cs.CV nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flies that walk in a covered planar arena on straight paths avoid colliding\nwith each other, but which of the two flies stops is not random.\nHigh-throughput video observations, coupled with dedicated experiments with\ncontrolled robot flies have revealed that flies utilize the type of optic flow\non their retina as a determinant of who should stop, a strategy also used by\nship captains to determine which of two ships on a collision course should\nthrow engines in reverse. We use digital evolution to test whether this\nstrategy evolves when collision avoidance is the sole penalty. We find that the\nstrategy does indeed evolve in a narrow range of cost/benefit ratios, for\nexperiments in which the \"regressive motion\" cue is error free. We speculate\nthat these stringent conditions may not be sufficient to evolve the strategy in\nreal flies, pointing perhaps to auxiliary costs and benefits not modeled in our\nstudy\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 17:36:31 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Tehrani-Saleh", "Ali", ""], ["Adami", "Christoph", ""]]}, {"id": "1603.00816", "submitter": "Kezhi Li", "authors": "Kezhi Li, Daniel Holland", "title": "A Nonlinear Weighted Total Variation Image Reconstruction Algorithm for\n  Electrical Capacitance Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new iterative image reconstruction algorithm for electrical capacitance\ntomography (ECT) is proposed that is based on iterative soft thresholding of a\ntotal variation penalty and adaptive reweighted compressive sensing. This\nalgorithm encourages sharp changes in the ECT image and overcomes the\ndisadvantage of the $l_1$ minimization by equipping the total variation with an\nadaptive weighting depending on the reconstructed image. Moreover, the\nnon-linear effect is also partially reduced due to the adoption of an updated\nsensitivity matrix. Simulation results show that the proposed algorithm\nrecovers ECT images more precisely than existing state-of-the-art algorithms\nand therefore is suitable for the imaging of multiphase systems in industrial\nor medical applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 18:24:32 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 15:01:41 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Li", "Kezhi", ""], ["Holland", "Daniel", ""]]}, {"id": "1603.00831", "submitter": "Anton Milan", "authors": "Anton Milan, Laura Leal-Taixe, Ian Reid, Stefan Roth, Konrad Schindler", "title": "MOT16: A Benchmark for Multi-Object Tracking", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.01942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for reseach.\n  Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was\nlaunched with the goal of collecting existing and new data and creating a\nframework for the standardized evaluation of multiple object tracking methods.\nThe first release of the benchmark focuses on multiple people tracking, since\npedestrians are by far the most studied object in the tracking community. This\npaper accompanies a new release of the MOTChallenge benchmark. Unlike the\ninitial release, all videos of MOT16 have been carefully annotated following a\nconsistent protocol. Moreover, it not only offers a significant increase in the\nnumber of labeled boxes, but also provides multiple object classes beside\npedestrians and the level of visibility for every single object of interest.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 19:07:56 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 23:55:38 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Milan", "Anton", ""], ["Leal-Taixe", "Laura", ""], ["Reid", "Ian", ""], ["Roth", "Stefan", ""], ["Schindler", "Konrad", ""]]}, {"id": "1603.00841", "submitter": "Jhony Heriberto Giraldo Zuluaga Engineer", "authors": "Jhony Giraldo and Augusto Salazar", "title": "Automatic segmentation of lizard spots using an active contour model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal biometrics is a challenging task. In the literature, many algorithms\nhave been used, e.g. penguin chest recognition, elephant ears recognition and\nleopard stripes pattern recognition, but to use technology to a large extent in\nthis area of research, still a lot of work has to be done. One important target\nin animal biometrics is to automate the segmentation process, so in this paper\nwe propose a segmentation algorithm for extracting the spots of Diploglossus\nmillepunctatus, an endangered lizard species. The automatic segmentation is\nachieved with a combination of preprocessing, active contours and morphology.\nThe parameters of each stage of the segmentation algorithm are found using an\noptimization procedure, which is guided by the ground truth. The results show\nthat automatic segmentation of spots is possible. A 78.37 % of correct\nsegmentation in average is reached.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 19:42:55 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Giraldo", "Jhony", ""], ["Salazar", "Augusto", ""]]}, {"id": "1603.00845", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor and Xavier\n  Giro-i-Nieto", "title": "Shallow and Deep Convolutional Networks for Saliency Prediction", "comments": "Preprint of the paper accepted at 2016 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR). Source code and models available at\n  https://github.com/imatge-upc/saliency-2016-cvpr. Junting Pan and Kevin\n  McGuinness contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of salient areas in images has been traditionally addressed\nwith hand-crafted features based on neuroscience principles. This paper,\nhowever, addresses the problem with a completely data-driven approach by\ntraining a convolutional neural network (convnet). The learning process is\nformulated as a minimization of a loss function that measures the Euclidean\ndistance of the predicted saliency map with the provided ground truth. The\nrecent publication of large datasets of saliency prediction has provided enough\ndata to train end-to-end architectures that are both fast and accurate. Two\ndesigns are proposed: a shallow convnet trained from scratch, and a another\ndeeper solution whose first three layers are adapted from another network\ntrained for classification. To the authors knowledge, these are the first\nend-to-end CNNs trained and tested for the purpose of saliency prediction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 19:54:02 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Pan", "Junting", ""], ["McGuinness", "Kevin", ""], ["Sayrol", "Elisa", ""], ["O'Connor", "Noel", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1603.00912", "submitter": "Lei Wang", "authors": "Lei Wang, Yongun Zhang", "title": "LiDAR Ground Filtering Algorithm for Urban Areas Using Scan Line Based\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of separating ground points from airborne LiDAR\npoint cloud data in urban areas. A novel ground filtering method using scan\nline segmentation is proposed here, which we call SLSGF. It utilizes the scan\nline information in LiDAR data to segment the LiDAR data. The similarity\nmeasurements are designed to make it possible to segment complex roof\nstructures into a single segment as much as possible so the topological\nrelationships between the roof and the ground are simpler, which will benefit\nthe labeling process. In the labeling process, the initial ground segments are\ndetected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011\nare used to test the accuracy of SLSGF; and our analytical and experimental\nresults show that this method is computationally-efficient and\nnoise-insensitive, thereby making a denoising process unnecessary before\nfiltering.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 22:09:45 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Wang", "Lei", ""], ["Zhang", "Yongun", ""]]}, {"id": "1603.00944", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Shijie Qiu, Youyong Kong, Longyu Jiang, Lotfi Senhadji,\n  Huazhong Shu", "title": "PCANet: An energy perspective", "comments": "37 pages, 23 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principal component analysis network (PCANet), which is one of the\nrecently proposed deep learning architectures, achieves the state-of-the-art\nclassification accuracy in various databases. However, the explanation of the\nPCANet is lacked. In this paper, we try to explain why PCANet works well from\nenergy perspective point of view based on a set of experiments. The impact of\nvarious parameters on the error rate of PCANet is analyzed in depth. It was\nfound that this error rate is correlated with the logarithm of energy of image.\nThe proposed energy explanation approach can be used as a testing method for\nchecking if every step of the constructed networks is necessary.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 01:26:25 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Wu", "Jiasong", ""], ["Qiu", "Shijie", ""], ["Kong", "Youyong", ""], ["Jiang", "Longyu", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1603.00960", "submitter": "Jan Egger", "authors": "Jan Egger, Christopher Nimsky", "title": "Cellular Automata Segmentation of the Boundary between the Compacta of\n  Vertebral Bodies and Surrounding Structures", "comments": "6 pages, 5 figures, 1 table, 42 references", "journal-ref": "SPIE Medical Imaging Conference 2016, Paper 9787-52", "doi": "10.1117/12.2209039", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the aging population, spinal diseases get more and more common\nnowadays; e.g., lifetime risk of osteoporotic fracture is 40% for white women\nand 13% for white men in the United States. Thus the numbers of surgical spinal\nprocedures are also increasing with the aging population and precise diagnosis\nplays a vital role in reducing complication and recurrence of symptoms. Spinal\nimaging of vertebral column is a tedious process subjected to interpretation\nerrors. In this contribution, we aim to reduce time and error for vertebral\ninterpretation by applying and studying the GrowCut-algorithm for boundary\nsegmentation between vertebral body compacta and surrounding structures.\nGrowCut is a competitive region growing algorithm using cellular automata. For\nour study, vertebral T2-weighted Magnetic Resonance Imaging (MRI) scans were\nfirst manually outlined by neurosurgeons. Then, the vertebral bodies were\nsegmented in the medical images by a GrowCut-trained physician using the\nsemi-automated GrowCut-algorithm. Afterwards, results of both segmentation\nprocesses were compared using the Dice Similarity Coefficient (DSC) and the\nHausdorff Distance (HD) which yielded to a DSC of 82.99+/-5.03% and a HD of\n18.91+/-7.2 voxel, respectively. In addition, the times have been measured\nduring the manual and the GrowCut segmentations, showing that a\nGrowCut-segmentation - with an average time of less than six minutes\n(5.77+/-0.73) - is significantly shorter than a pure manual outlining.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:35:07 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Egger", "Jan", ""], ["Nimsky", "Christopher", ""]]}, {"id": "1603.00961", "submitter": "Jan Egger", "authors": "Tobias L\\\"uddemann, Jan Egger", "title": "Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via\n  User-Defined Templates", "comments": "6 pages, 4 figures, 1 table, 43 references", "journal-ref": "SPIE Medical Imaging Conference 2016, Paper 9784-113", "doi": "10.1117/12.2216226", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among all types of cancer, gynecological malignancies belong to the 4th most\nfrequent type of cancer among women. Besides chemotherapy and external beam\nradiation, brachytherapy is the standard procedure for the treatment of these\nmalignancies. In the progress of treatment planning, localization of the tumor\nas the target volume and adjacent organs of risks by segmentation is crucial to\naccomplish an optimal radiation distribution to the tumor while simultaneously\npreserving healthy tissue. Segmentation is performed manually and represents a\ntime-consuming task in clinical daily routine. This study focuses on the\nsegmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecological\nbrachytherapy. The proposed segmentation method uses an interactive,\ngraph-based segmentation scheme with a user-defined template. The scheme\ncreates a directed two dimensional graph, followed by the minimal cost closed\nset computation on the graph, resulting in an outlining of the rectum. The\ngraphs outline is dynamically adapted to the last calculated cut. Evaluation\nwas performed by comparing manual segmentations of the rectum/sigmoid colon to\nresults achieved with the proposed method. The comparison of the algorithmic to\nmanual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%,\nin comparison to 83.97+/-8.08% for the comparison of two manual segmentations\nof the same physician. Utilizing the proposed methodology resulted in a median\ntime of 128 seconds per dataset, compared to 300 seconds needed for pure manual\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:39:59 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["L\u00fcddemann", "Tobias", ""], ["Egger", "Jan", ""]]}, {"id": "1603.00993", "submitter": "Kanji Tanaka", "authors": "Tanaka Kanji", "title": "Self-localization from Images with Small Overlap", "comments": "8 pages, 9 figures, Draft of a paper submitted to an International\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of visual features from deep convolutional neural\nnetworks (DCNN) in visual robot self-localization, it has become important and\npractical to address more general self-localization scenarios. In this paper,\nwe address the scenario of self-localization from images with small overlap. We\nexplicitly introduce a localization difficulty index as a decreasing function\nof view overlap between query and relevant database images and investigate\nperformance versus difficulty for challenging cross-view self-localization\ntasks. We then reformulate the self-localization as a scalable\nbag-of-visual-features (BoVF) scene retrieval and present an efficient solution\ncalled PCA-NBNN, aiming to facilitate fast and yet discriminative\ncorrespondence between partially overlapping images. The proposed approach\nadopts recent findings in discriminativity preserving encoding of DCNN features\nusing principal component analysis (PCA) and cross-domain scene matching using\nnaive Bayes nearest neighbor distance metric (NBNN). We experimentally\ndemonstrate that the proposed PCA-NBNN framework frequently achieves comparable\nresults to previous DCNN features and that the BoVF model is significantly more\nefficient. We further address an important alternative scenario of\n\"self-localization from images with NO overlap\" and report the result.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 06:39:37 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Kanji", "Tanaka", ""]]}, {"id": "1603.01006", "submitter": "Manuel Marin-Jimenez", "authors": "F.M. Castro and M.J. Marin-Jimenez and N. Guil and N. Perez de la\n  Blanca", "title": "Automatic learning of gait signatures for people identification", "comments": "Proof of concept paper. Technical report on the use of ConvNets (CNN)\n  for gait recognition. Data and code:\n  http://www.uco.es/~in1majim/research/cnngaitof.html", "journal-ref": null, "doi": null, "report-no": "2016-03", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work targets people identification in video based on the way they walk\n(i.e. gait). While classical methods typically derive gait signatures from\nsequences of binary silhouettes, in this work we explore the use of\nconvolutional neural networks (CNN) for learning high-level descriptors from\nlow-level motion features (i.e. optical flow components). We carry out a\nthorough experimental evaluation of the proposed CNN architecture on the\nchallenging TUM-GAID dataset. The experimental results indicate that using\nspatio-temporal cuboids of optical flow as input data for CNN allows to obtain\nstate-of-the-art results on the gait task with an image resolution eight times\nlower than the previously reported results (i.e. 80x60 pixels).\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 08:07:14 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 16:07:07 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Castro", "F. M.", ""], ["Marin-Jimenez", "M. J.", ""], ["Guil", "N.", ""], ["de la Blanca", "N. Perez", ""]]}, {"id": "1603.01046", "submitter": "Teemu Helenius", "authors": "Teemu Helenius and Samuli Siltanen", "title": "Photographic dataset: random peppercorns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a photographic dataset collected for testing image processing\nalgorithms. The idea is to have sets of different but statistically similar\nimages. In this work the images show randomly distributed peppercorns. The\ndataset is made available at www.fips.fi/photographic_dataset.php .\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 10:24:07 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Helenius", "Teemu", ""], ["Siltanen", "Samuli", ""]]}, {"id": "1603.01056", "submitter": "Chao Wang", "authors": "Chao Wang", "title": "A novel and automatic pectoral muscle identification algorithm for\n  mediolateral oblique (MLO) view mammograms using ImageJ", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pectoral muscle identification is often required for breast cancer risk\nanalysis, such as estimating breast density. Traditional methods are\noverwhelmingly based on manual visual assessment or straight line fitting for\nthe pectoral muscle boundary, which are inefficient and inaccurate since\npectoral muscle in mammograms can have curved boundaries.\n  This paper proposes a novel and automatic pectoral muscle identification\nalgorithm for MLO view mammograms. It is suitable for both scanned film and\nfull field digital mammograms. This algorithm is demonstrated using a public\ndomain software ImageJ. A validation of this algorithm has been performed using\nreal-world data and it shows promising result.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 11:20:23 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Wang", "Chao", ""]]}, {"id": "1603.01067", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural", "title": "Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain\n  Decoding", "comments": "13 pages, 10 figures, submitted to JSTSP Special Issue on Advanced\n  Signal Processing in Brain Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)\nbrain volumes recorded during a cognitive stimulus by a graph which consists of\na set of local meshes. The corresponding cognitive process, encoded in the\nbrain, is then represented by these meshes each of which is estimated assuming\na linear relationship among the voxel time series in a predefined locality.\nFirst, we define the concept of locality in two neighborhood systems, namely,\nthe spatial and functional neighborhoods. Then, we construct spatially and\nfunctionally local meshes around each voxel, called seed voxel, by connecting\nit either to its spatial or functional p-nearest neighbors. The mesh formed\naround a voxel is a directed sub-graph with a star topology, where the\ndirection of the edges is taken towards the seed voxel at the center of the\nmesh. We represent the time series recorded at each seed voxel in terms of\nlinear combination of the time series of its p-nearest neighbors in the mesh.\nThe relationships between a seed voxel and its neighbors are represented by the\nedge weights of each mesh, and are estimated by solving a linear regression\nequation. The estimated mesh edge weights lead to a better representation of\ninformation in the brain for encoding and decoding of the cognitive tasks. We\ntest our model on a visual object recognition and emotional memory retrieval\nexperiments using Support Vector Machines that are trained using the mesh edge\nweights as features. In the experimental analysis, we observe that the edge\nweights of the spatial and functional meshes perform better than the\nstate-of-the-art brain decoding models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 12:06:00 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Onal", "Itir", ""], ["Ozay", "Mete", ""], ["Mizrak", "Eda", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1603.01068", "submitter": "Paolo Bestagini", "authors": "Luca Bondi and Luca Baroffio and David G\\\"uera and Paolo Bestagini and\n  Edward J. Delp and Stefano Tubaro", "title": "First Steps Toward Camera Model Identification with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2641006", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the camera model used to shoot a picture enables to solve a wide\nseries of forensic problems, from copyright infringement to ownership\nattribution. For this reason, the forensic community has developed a set of\ncamera model identification algorithms that exploit characteristic traces left\non acquired images by the processing pipelines specific of each camera model.\nIn this paper, we investigate a novel approach to solve camera model\nidentification problem. Specifically, we propose a data-driven algorithm based\non convolutional neural networks, which learns features characterizing each\ncamera model directly from the acquired pictures. Results on a well-known\ndataset of 18 camera models show that: (i) the proposed method outperforms\nup-to-date state-of-the-art algorithms on classification of 64x64 color image\npatches; (ii) features learned by the proposed network generalize to camera\nmodels never used for training.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 12:10:47 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 09:29:28 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Bondi", "Luca", ""], ["Baroffio", "Luca", ""], ["G\u00fcera", "David", ""], ["Bestagini", "Paolo", ""], ["Delp", "Edward J.", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1603.01076", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka, Diane Larlus, Albert Gordo and Jon Almazan", "title": "What is the right way to represent document images?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the problem of document image representation based\non visual features. We propose a comprehensive experimental study that compares\nthree types of visual document image representations: (1) traditional so-called\nshallow features, such as the RunLength and the Fisher-Vector descriptors, (2)\ndeep features based on Convolutional Neural Networks, and (3) features\nextracted from hybrid architectures that take inspiration from the two previous\nones.\n  We evaluate these features in several tasks (i.e. classification, clustering,\nand retrieval) and in different setups (e.g. domain transfer) using several\npublic and in-house datasets. Our results show that deep features generally\noutperform other types of features when there is no domain shift and the new\ntask is closely related to the one used to train the model. However, when a\nlarge domain or task shift is present, the Fisher-Vector shallow features\ngeneralize better and often obtain the best results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 12:46:51 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 17:38:52 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 16:38:25 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Csurka", "Gabriela", ""], ["Larlus", "Diane", ""], ["Gordo", "Albert", ""], ["Almazan", "Jon", ""]]}, {"id": "1603.01096", "submitter": "Yubao Sun", "authors": "Qingshan Liu, Yubao Sun, Cantian Wang, Tongliang Liu and Dacheng Tao", "title": "Elastic Net Hypergraph Learning for Image Clustering and Semi-supervised\n  Classification", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2621671", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph model is emerging as a very effective tool for learning the complex\nstructures and relationships hidden in data. Generally, the critical purpose of\ngraph-oriented learning algorithms is to construct an informative graph for\nimage clustering and classification tasks. In addition to the classical\n$K$-nearest-neighbor and $r$-neighborhood methods for graph construction,\n$l_1$-graph and its variants are emerging methods for finding the neighboring\nsamples of a center datum, where the corresponding ingoing edge weights are\nsimultaneously derived by the sparse reconstruction coefficients of the\nremaining samples. However, the pair-wise links of $l_1$-graph are not capable\nof capturing the high order relationships between the center datum and its\nprominent data in sparse reconstruction. Meanwhile, from the perspective of\nvariable selection, the $l_1$ norm sparse constraint, regarded as a LASSO\nmodel, tends to select only one datum from a group of data that are highly\ncorrelated and ignore the others. To simultaneously cope with these drawbacks,\nwe propose a new elastic net hypergraph learning model, which consists of two\nsteps. In the first step, the Robust Matrix Elastic Net model is constructed to\nfind the canonically related samples in a somewhat greedy way, achieving the\ngrouping effect by adding the $l_2$ penalty to the $l_1$ constraint. In the\nsecond step, hypergraph is used to represent the high order relationships\nbetween each datum and its prominent samples by regarding them as a hyperedge.\nSubsequently, hypergraph Laplacian matrix is constructed for further analysis.\nNew hypergraph learning algorithms, including unsupervised clustering and\nmulti-class semi-supervised classification, are then derived. Extensive\nexperiments on face and handwriting databases demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 13:37:23 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Liu", "Qingshan", ""], ["Sun", "Yubao", ""], ["Wang", "Cantian", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1603.01249", "submitter": "Rajeev Ranjan", "authors": "Rajeev Ranjan, Vishal M. Patel and Rama Chellappa", "title": "HyperFace: A Deep Multi-task Learning Framework for Face Detection,\n  Landmark Localization, Pose Estimation, and Gender Recognition", "comments": "Accepted in Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for simultaneous face detection, landmarks\nlocalization, pose estimation and gender recognition using deep convolutional\nneural networks (CNN). The proposed method called, HyperFace, fuses the\nintermediate layers of a deep CNN using a separate CNN followed by a multi-task\nlearning algorithm that operates on the fused features. It exploits the synergy\namong the tasks which boosts up their individual performances. Additionally, we\npropose two variants of HyperFace: (1) HyperFace-ResNet that builds on the\nResNet-101 model and achieves significant improvement in performance, and (2)\nFast-HyperFace that uses a high recall fast face detector for generating region\nproposals to improve the speed of the algorithm. Extensive experiments show\nthat the proposed models are able to capture both global and local information\nin faces and performs significantly better than many competitive algorithms for\neach of these four tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 20:30:53 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 23:49:53 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 01:34:20 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1603.01250", "submitter": "Yani Ioannou", "authors": "Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie\n  Shotton, Matthew Brown, and Antonio Criminisi", "title": "Decision Forests, Convolutional Networks and the Models in-Between", "comments": "Microsoft Research Technical Report", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2015-58", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the connections between two state of the art\nclassifiers: decision forests (DFs, including decision jungles) and\nconvolutional neural networks (CNNs). Decision forests are computationally\nefficient thanks to their conditional computation property (computation is\nconfined to only a small region of the tree, the nodes along a single branch).\nCNNs achieve state of the art accuracy, thanks to their representation learning\ncapabilities. We present a systematic analysis of how to fuse conditional\ncomputation with representation learning and achieve a continuum of hybrid\nmodels with different ratios of accuracy vs. efficiency. We call this new\nfamily of hybrid models conditional networks. Conditional networks can be\nthought of as: i) decision trees augmented with data transformation operators,\nor ii) CNNs, with block-diagonal sparse weight matrices, and explicit data\nrouting functions. Experimental validation is performed on the common task of\nimage classification on both the CIFAR and Imagenet datasets. Compared to state\nof the art CNNs, our hybrid models yield the same accuracy with a fraction of\nthe compute cost and much smaller number of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 20:41:47 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Ioannou", "Yani", ""], ["Robertson", "Duncan", ""], ["Zikic", "Darko", ""], ["Kontschieder", "Peter", ""], ["Shotton", "Jamie", ""], ["Brown", "Matthew", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1603.01292", "submitter": "Abhineet Singh", "authors": "Abhineet Singh, Ankush Roy, Xi Zhang, Martin Jagersand", "title": "Modular Decomposition and Analysis of Registration based Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new way to study registration based trackers by\ndecomposing them into three constituent sub modules: appearance model, state\nspace model and search method. It is often the case that when a new tracker is\nintroduced in literature, it only contributes to one or two of these sub\nmodules while using existing methods for the rest. Since these are often\nselected arbitrarily by the authors, they may not be optimal for the new\nmethod. In such cases, our breakdown can help to experimentally find the best\ncombination of methods for these sub modules while also providing a framework\nwithin which the contributions of the new tracker can be clearly demarcated and\nthus studied better. We show how existing trackers can be broken down using the\nsuggested methodology and compare the performance of the default configuration\nchosen by the authors against other possible combinations to demonstrate the\nnew insights that can be gained by such an approach. We also present an open\nsource system that provides a convenient interface to plug in a new method for\nany sub module and test it against all possible combinations of methods for the\nother two sub modules while also serving as a fast and efficient solution for\npractical tracking requirements.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 21:50:09 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 18:39:12 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Singh", "Abhineet", ""], ["Roy", "Ankush", ""], ["Zhang", "Xi", ""], ["Jagersand", "Martin", ""]]}, {"id": "1603.01359", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Learning deep representation of multityped objects and tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep multitask architecture to integrate multityped\nrepresentations of multimodal objects. This multitype exposition is less\nabstract than the multimodal characterization, but more machine-friendly, and\nthus is more precise to model. For example, an image can be described by\nmultiple visual views, which can be in the forms of bag-of-words (counts) or\ncolor/texture histograms (real-valued). At the same time, the image may have\nseveral social tags, which are best described using a sparse binary vector. Our\ndeep model takes as input multiple type-specific features, narrows the\ncross-modality semantic gaps, learns cross-type correlation, and produces a\nhigh-level homogeneous representation. At the same time, the model supports\nheterogeneously typed tasks. We demonstrate the capacity of the model on two\napplications: social image retrieval and multiple concept prediction. The deep\narchitecture produces more compact representation, naturally integrates\nmultiviews and multimodalities, exploits better side information, and most\nimportantly, performs competitively against baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 06:34:24 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1603.01417", "submitter": "Richard Socher", "authors": "Caiming Xiong, Stephen Merity, Richard Socher", "title": "Dynamic Memory Networks for Visual and Textual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 10:40:28 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Xiong", "Caiming", ""], ["Merity", "Stephen", ""], ["Socher", "Richard", ""]]}, {"id": "1603.01633", "submitter": "Ulugbek Kamilov", "authors": "Ulugbek S. Kamilov and Petros T. Boufounos", "title": "Depth Superresolution using Motion Adaptive Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial resolution of depth sensors is often significantly lower compared to\nthat of conventional optical cameras. Recent work has explored the idea of\nimproving the resolution of depth using higher resolution intensity as a side\ninformation. In this paper, we demonstrate that further incorporating temporal\ninformation in videos can significantly improve the results. In particular, we\npropose a novel approach that improves depth resolution, exploiting the\nspace-time redundancy in the depth and intensity using motion-adaptive low-rank\nregularization. Experiments confirm that the proposed approach substantially\nimproves the quality of the estimated high-resolution depth. Our approach can\nbe a first component in systems using vision techniques that rely on high\nresolution depth information.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 21:16:21 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kamilov", "Ulugbek S.", ""], ["Boufounos", "Petros T.", ""]]}, {"id": "1603.01670", "submitter": "Tao Wei", "authors": "Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen", "title": "Network Morphism", "comments": "Under review for ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 02:06:43 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 16:36:00 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Wei", "Tao", ""], ["Wang", "Changhu", ""], ["Rui", "Yong", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1603.01684", "submitter": "Chenxing Xia", "authors": "Hanling Zhang and Chenxing Xia", "title": "Saliency Detection combining Multi-layer Integration algorithm with\n  background prior and energy function", "comments": "25 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1505.07192 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an improved mechanism for saliency detection.\nFirstly,based on a neoteric background prior selecting four corners of an image\nas background,we use color and spatial contrast with each superpixel to obtain\na salinecy map(CBP). Inspired by reverse-measurement methods to improve the\naccuracy of measurement in Engineering,we employ the Objectness labels as\nforeground prior based on part of information of CBP to construct a\nmap(OFP).Further,an original energy function is applied to optimize both of\nthem respectively and a single-layer saliency map(SLP)is formed by merging the\nabove twos.Finally,to deal with the scale problem,we obtain our multi-layer\nmap(MLP) by presenting an integration algorithm to take advantage of multiple\nsaliency maps. Quantitative and qualitative experiments on three datasets\ndemonstrate that our method performs favorably against the state-of-the-art\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 06:12:44 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Zhang", "Hanling", ""], ["Xia", "Chenxing", ""]]}, {"id": "1603.01695", "submitter": "Meng-Che Chuang", "authors": "Meng-Che Chuang, Jenq-Neng Hwang, Jian-Hui Ye, Shih-Chia Huang,\n  Kresimir Williams", "title": "Underwater Fish Tracking for Moving Cameras based on Deformable Multiple\n  Kernels", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fishery surveys that call for the use of single or multiple underwater\ncameras have been an emerging technology as a non-extractive mean to estimate\nthe abundance of fish stocks. Tracking live fish in an open aquatic environment\nposts challenges that are different from general pedestrian or vehicle tracking\nin surveillance applications. In many rough habitats fish are monitored by\ncameras installed on moving platforms, where tracking is even more challenging\ndue to inapplicability of background models. In this paper, a novel tracking\nalgorithm based on the deformable multiple kernels (DMK) is proposed to address\nthese challenges. Inspired by the deformable part model (DPM) technique, a set\nof kernels is defined to represent the holistic object and several parts that\nare arranged in a deformable configuration. Color histogram, texture histogram\nand the histogram of oriented gradients (HOG) are extracted and serve as object\nfeatures. Kernel motion is efficiently estimated by the mean-shift algorithm on\ncolor and texture features to realize tracking. Furthermore, the HOG-feature\ndeformation costs are adopted as soft constraints on kernel positions to\nmaintain the part configuration. Experimental results on practical video set\nfrom underwater moving cameras show the reliable performance of the proposed\nmethod with much less computational cost comparing with state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 08:20:25 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Chuang", "Meng-Che", ""], ["Hwang", "Jenq-Neng", ""], ["Ye", "Jian-Hui", ""], ["Huang", "Shih-Chia", ""], ["Williams", "Kresimir", ""]]}, {"id": "1603.01696", "submitter": "Meng-Che Chuang", "authors": "Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams", "title": "A Feature Learning and Object Recognition Framework for Underwater Fish\n  Images", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live fish recognition is one of the most crucial elements of fisheries survey\napplications where vast amount of data are rapidly acquired. Different from\ngeneral scenarios, challenges to underwater image recognition are posted by\npoor image quality, uncontrolled objects and environment, as well as difficulty\nin acquiring representative samples. Also, most existing feature extraction\ntechniques are hindered from automation due to involving human supervision.\nToward this end, we propose an underwater fish recognition framework that\nconsists of a fully unsupervised feature learning technique and an\nerror-resilient classifier. Object parts are initialized based on saliency and\nrelaxation labeling to match object parts correctly. A non-rigid part model is\nthen learned based on fitness, separation and discrimination criteria. For the\nclassifier, an unsupervised clustering approach generates a binary class\nhierarchy, where each node is a classifier. To exploit information from\nambiguous images, the notion of partial classification is introduced to assign\ncoarse labels by optimizing the \"benefit\" of indecision made by the classifier.\nExperiments show that the proposed framework achieves high accuracy on both\npublic and self-collected underwater fish images with high uncertainty and\nclass imbalance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 08:33:18 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Chuang", "Meng-Che", ""], ["Hwang", "Jenq-Neng", ""], ["Williams", "Kresimir", ""]]}, {"id": "1603.01739", "submitter": "Subhamoy Mandal", "authors": "Viswanath P Sudarshan, Tobias Weiser, Phalgun Chintala, Subhamoy\n  Mandal, and Rahul Dutta", "title": "Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for\n  in Vitro Embryo Culture", "comments": "IEEE BHI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual observation of Cumulus Oocyte Complexes provides only limited\ninformation about its functional competence, whereas the molecular evaluations\nmethods are cumbersome or costly. Image analysis of mammalian oocytes can\nprovide attractive alternative to address this challenge. However, it is\ncomplex, given the huge number of oocytes under inspection and the subjective\nnature of the features inspected for identification. Supervised machine\nlearning methods like random forest with annotations from expert biologists can\nmake the analysis task standardized and reduces inter-subject variability. We\npresent a semi-automatic framework for predicting the class an oocyte belongs\nto, based on multi-object parametric segmentation on the acquired microscopic\nimage followed by a feature based classification using random forests.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 16:27:43 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Sudarshan", "Viswanath P", ""], ["Weiser", "Tobias", ""], ["Chintala", "Phalgun", ""], ["Mandal", "Subhamoy", ""], ["Dutta", "Rahul", ""]]}, {"id": "1603.01768", "submitter": "Arek Champandard", "authors": "Alex J. Champandard", "title": "Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) have proven highly effective at image\nsynthesis and style transfer. For most users, however, using them as tools can\nbe a challenging task due to their unpredictable behavior that goes against\ncommon intuitions. This paper introduces a novel concept to augment such\ngenerative architectures with semantic annotations, either by manually\nauthoring pixel labels or using existing solutions for semantic segmentation.\nThe result is a content-aware generative algorithm that offers meaningful\ncontrol over the outcome. Thus, we increase the quality of images generated by\navoiding common glitches, make the results look significantly more plausible,\nand extend the functional range of these algorithms---whether for portraits or\nlandscapes, etc. Applications include semantic style transfer and turning\ndoodles with few colors into masterful paintings!\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 23:09:51 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Champandard", "Alex J.", ""]]}, {"id": "1603.01772", "submitter": "Pavel Dourbal", "authors": "Pavel Dourbal and Mikhail Pekker", "title": "Fast calculation of correlations in recognition systems", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally efficient classification system architecture is proposed. It\nutilizes fast tensor-vector multiplication algorithm to apply linear operators\nupon input signals . The approach is applicable to wide variety of recognition\nsystem architectures ranging from single stage matched filter bank classifiers\nto complex neural networks with unlimited number of hidden layers.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 00:30:34 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Dourbal", "Pavel", ""], ["Pekker", "Mikhail", ""]]}, {"id": "1603.01801", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "Variational methods for Conditional Multimodal Deep Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of conditional modality learning,\nwhereby one is interested in generating one modality given the other. While it\nis straightforward to learn a joint distribution over multiple modalities using\na deep multimodal architecture, we observe that such models aren't very\neffective at conditional generation. Hence, we address the problem by learning\nconditional distributions between the modalities. We use variational methods\nfor maximizing the corresponding conditional log-likelihood. The resultant deep\nmodel, which we refer to as conditional multimodal autoencoder (CMMA), forces\nthe latent representation obtained from a single modality alone to be `close'\nto the joint representation obtained from multiple modalities. We use the\nproposed model to generate faces from attributes. We show that the faces\ngenerated from attributes using the proposed model, are qualitatively and\nquantitatively more representative of the attributes from which they were\ngenerated, than those obtained by other deep generative models. We also propose\na secondary task, whereby the existing faces are modified by modifying the\ncorresponding attributes. We observe that the modifications in face introduced\nby the proposed model are representative of the corresponding modifications in\nattributes.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 07:33:03 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 10:57:41 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1603.01842", "submitter": "James  Peters Ph.D.", "authors": "Enoch A-iyeh, James F. Peters", "title": "Proximal groupoid patterns In digital images", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this article is on the detection and classification of patterns\nbased on groupoids. The approach hinges on descriptive proximity of points in a\nset based on the neighborliness property. This approach lends support to image\nanalysis and understanding and in studying nearness of image segments. A\npractical application of the approach is in terms of the analysis of natural\nimages for pattern identification and classification.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 16:39:34 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["A-iyeh", "Enoch", ""], ["Peters", "James F.", ""]]}, {"id": "1603.01864", "submitter": "Felipe Codevilla", "authors": "Joel D. O. Gaya, Felipe Codevilla, Amanda C. Duarte, Paulo L. Drews-Jr\n  and Silvia S. Botelho", "title": "Single Image Restoration for Participating Media Based on Prior Fusion", "comments": "This paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": "10.1109/MCG.2018.2881388", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method to restore degraded images captured in a\nparticipating media -- fog, turbid water, sand storm, etc. Differently from the\nrelated work that only deal with a medium, we obtain generality by using an\nimage formation model and a fusion of new image priors. The model considers the\nimage color variation produced by the medium. The proposed restoration method\nis based on the fusion of these priors and supported by statistics collected on\nimages acquired in both non-participating and participating media. The key of\nthe method is to fuse two complementary measures --- local contrast and color\ndata. The obtained results on underwater and foggy images demonstrate the\ncapabilities of the proposed method. Moreover, we evaluated our method using a\nspecial dataset for which a ground-truth image is available.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 19:54:18 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 19:19:36 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Gaya", "Joel D. O.", ""], ["Codevilla", "Felipe", ""], ["Duarte", "Amanda C.", ""], ["Drews-Jr", "Paulo L.", ""], ["Botelho", "Silvia S.", ""]]}, {"id": "1603.01942", "submitter": "Xiaqing Pan", "authors": "Xiaqing Pan, Sachin Chachada, C.-C. Jay Kuo", "title": "A Two-Stage Shape Retrieval (TSR) Method with Global and Local Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust two-stage shape retrieval (TSR) method is proposed to address the 2D\nshape retrieval problem. Most state-of-the-art shape retrieval methods are\nbased on local features matching and ranking. Their retrieval performance is\nnot robust since they may retrieve globally dissimilar shapes in high ranks. To\novercome this challenge, we decompose the decision process into two stages. In\nthe first irrelevant cluster filtering (ICF) stage, we consider both global and\nlocal features and use them to predict the relevance of gallery shapes with\nrespect to the query. Irrelevant shapes are removed from the candidate shape\nset. After that, a local-features-based matching and ranking (LMR) method\nfollows in the second stage. We apply the proposed TSR system to MPEG-7,\nKimia99 and Tari1000 three datasets and show that it outperforms all other\nexisting methods. The robust retrieval performance of the TSR system is\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 05:33:00 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 05:50:14 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 04:22:41 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Pan", "Xiaqing", ""], ["Chachada", "Sachin", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1603.01976", "submitter": "Guanbin Li", "authors": "Guanbin Li and Yizhou Yu", "title": "Deep Contrast Learning for Salient Object Detection", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection has recently witnessed substantial progress due to\npowerful features extracted using deep convolutional neural networks (CNNs).\nHowever, existing CNN-based methods operate at the patch level instead of the\npixel level. Resulting saliency maps are typically blurry, especially near the\nboundary of salient objects. Furthermore, image patches are treated as\nindependent samples even when they are overlapping, giving rise to significant\nredundancy in computation and storage. In this CVPR 2016 paper, we propose an\nend-to-end deep contrast network to overcome the aforementioned limitations.\nOur deep network consists of two complementary components, a pixel-level fully\nconvolutional stream and a segment-wise spatial pooling stream. The first\nstream directly produces a saliency map with pixel-level accuracy from an input\nimage. The second stream extracts segment-wise features very efficiently, and\nbetter models saliency discontinuities along object boundaries. Finally, a\nfully connected CRF model can be optionally incorporated to improve spatial\ncoherence and contour localization in the fused result from these two streams.\nExperimental results demonstrate that our deep model significantly improves the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 08:50:33 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1603.02003", "submitter": "Paul Upchurch", "authors": "Paul Upchurch and Noah Snavely and Kavita Bala", "title": "From A to Z: Supervised Transfer of Style and Content Using Deep Neural\n  Network Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new neural network architecture for solving single-image\nanalogies - the generation of an entire set of stylistically similar images\nfrom just a single input image. Solving this problem requires separating image\nstyle from content. Our network is a modified variational autoencoder (VAE)\nthat supports supervised training of single-image analogies and in-network\nevaluation of outputs with a structured similarity objective that captures\npixel covariances. On the challenging task of generating a 62-letter font from\na single example letter we produce images with 22.4% lower dissimilarity to the\nground truth than state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 11:04:17 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Upchurch", "Paul", ""], ["Snavely", "Noah", ""], ["Bala", "Kavita", ""]]}, {"id": "1603.02008", "submitter": "Patrizio Frosini", "authors": "Patrizio Frosini", "title": "Position paper: Towards an observer-oriented theory of shape comparison", "comments": "Preprint of the position paper submitted to the Eurographics Workshop\n  on 3D Object Retrieval (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper we suggest a possible metric approach to shape\ncomparison that is based on a mathematical formalization of the concept of\nobserver, seen as a collection of suitable operators acting on a metric space\nof functions. These functions represent the set of data that are accessible to\nthe observer, while the operators describe the way the observer elaborates the\ndata and enclose the invariance that he/she associates with them. We expose\nthis model and illustrate some theoretical reasons that justify its possible\nuse for shape comparison.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 11:16:28 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Frosini", "Patrizio", ""]]}, {"id": "1603.02028", "submitter": "Sylvain Chevallier", "authors": "Hugo Martin, Sylvain Chevallier and Eric Monacelli", "title": "Adaptive Visualisation System for Construction Building Information\n  Models Using Saliency", "comments": "10 pages, 5 figures, to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building Information Modeling (BIM) is a recent construction process based on\na 3D model, containing every component related to the building achievement.\nArchitects, structure engineers, method engineers, and others participant to\nthe building process work on this model through the design-to-construction\ncycle. The high complexity and the large amount of information included in\nthese models raise several issues, delaying its wide adoption in the industrial\nworld. One of the most important is the visualization: professionals have\ndifficulties to find out the relevant information for their job. Actual\nsolutions suffer from two limitations: the BIM models information are processed\nmanually and insignificant information are simply hidden, leading to\ninconsistencies in the building model. This paper describes a system relying on\nan ontological representation of the building information to label\nautomatically the building elements. Depending on the user's department, the\nvisualization is modified according to these labels by automatically adjusting\nthe colors and image properties based on a saliency model. The proposed\nsaliency model incorporates several adaptations to fit the specificities of\narchitectural images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 12:25:33 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Martin", "Hugo", ""], ["Chevallier", "Sylvain", ""], ["Monacelli", "Eric", ""]]}, {"id": "1603.02078", "submitter": "Jiang Liu", "authors": "Lan Wang, Chenqiang Gao, Jiang Liu, Deyu Meng", "title": "A novel learning-based frame pooling method for Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting complex events in a large video collection crawled from video\nwebsites is a challenging task. When applying directly good image-based feature\nrepresentation, e.g., HOG, SIFT, to videos, we have to face the problem of how\nto pool multiple frame feature representations into one feature representation.\nIn this paper, we propose a novel learning-based frame pooling method. We\nformulate the pooling weight learning as an optimization problem and thus our\nmethod can automatically learn the best pooling weight configuration for each\nspecific event category. Experimental results conducted on TRECVID MED 2011\nreveal that our method outperforms the commonly used average pooling and max\npooling strategies on both high-level and low-level 2D image features.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 14:15:55 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 02:59:56 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Wang", "Lan", ""], ["Gao", "Chenqiang", ""], ["Liu", "Jiang", ""], ["Meng", "Deyu", ""]]}, {"id": "1603.02139", "submitter": "Li Zhang", "authors": "Li Zhang, Tao Xiang, Shaogang Gong", "title": "Learning a Discriminative Null Space for Person Re-identification", "comments": "accepted by CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (re-id) methods focus on learning the\noptimal distance metrics across camera views. Typically a person's appearance\nis represented using features of thousands of dimensions, whilst only hundreds\nof training samples are available due to the difficulties in collecting matched\ntraining images. With the number of training samples much smaller than the\nfeature dimension, the existing methods thus face the classic small sample size\n(SSS) problem and have to resort to dimensionality reduction techniques and/or\nmatrix regularisation, which lead to loss of discriminative power. In this\nwork, we propose to overcome the SSS problem in re-id distance metric learning\nby matching people in a discriminative null space of the training data. In this\nnull space, images of the same person are collapsed into a single point thus\nminimising the within-class scatter to the extreme and maximising the relative\nbetween-class separation simultaneously. Importantly, it has a fixed dimension,\na closed-form solution and is very efficient to compute. Extensive experiments\ncarried out on five person re-identification benchmarks including VIPeR,\nPRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats\nthe state-of-the-art alternatives, often by a big margin.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 16:26:07 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Zhang", "Li", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1603.02194", "submitter": "Oren Barkan", "authors": "Oren Barkan, Jonathan Weill and Amir Averbuch", "title": "Gaussian Process Regression for Out-of-Sample Extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning methods are useful for high dimensional data analysis. Many\nof the existing methods produce a low dimensional representation that attempts\nto describe the intrinsic geometric structure of the original data. Typically,\nthis process is computationally expensive and the produced embedding is limited\nto the training data. In many real life scenarios, the ability to produce\nembedding of unseen samples is essential. In this paper we propose a Bayesian\nnon-parametric approach for out-of-sample extension. The method is based on\nGaussian Process Regression and independent of the manifold learning algorithm.\nAdditionally, the method naturally provides a measure for the degree of\nabnormality for a newly arrived data point that did not participate in the\ntraining process. We derive the mathematical connection between the proposed\nmethod and the Nystrom extension and show that the latter is a special case of\nthe former. We present extensive experimental results that demonstrate the\nperformance of the proposed method and compare it to other existing\nout-of-sample extension methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:35:51 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 16:56:21 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Barkan", "Oren", ""], ["Weill", "Jonathan", ""], ["Averbuch", "Amir", ""]]}, {"id": "1603.02199", "submitter": "Sergey Levine", "authors": "Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning\n  and Large-Scale Data Collection", "comments": "This is an extended version of \"Learning Hand-Eye Coordination for\n  Robotic Grasping with Large-Scale Data Collection,\" ISER 2016. Draft modified\n  to correct typo in Algorithm 1 and add a link to the publicly available\n  dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a learning-based approach to hand-eye coordination for robotic\ngrasping from monocular images. To learn hand-eye coordination for grasping, we\ntrained a large convolutional neural network to predict the probability that\ntask-space motion of the gripper will result in successful grasps, using only\nmonocular camera images and independently of camera calibration or the current\nrobot pose. This requires the network to observe the spatial relationship\nbetween the gripper and objects in the scene, thus learning hand-eye\ncoordination. We then use this network to servo the gripper in real time to\nachieve successful grasps. To train our network, we collected over 800,000\ngrasp attempts over the course of two months, using between 6 and 14 robotic\nmanipulators at any given time, with differences in camera placement and\nhardware. Our experimental evaluation demonstrates that our method achieves\neffective real-time control, can successfully grasp novel objects, and corrects\nmistakes by continuous servoing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:53:00 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 23:01:46 GMT"}, {"version": "v3", "created": "Sat, 2 Apr 2016 23:50:24 GMT"}, {"version": "v4", "created": "Sun, 28 Aug 2016 23:32:37 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Levine", "Sergey", ""], ["Pastor", "Peter", ""], ["Krizhevsky", "Alex", ""], ["Quillen", "Deirdre", ""]]}, {"id": "1603.02200", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Pavan Turaga, Jingyong Su, Anuj Srivastava", "title": "Elastic Functional Coding of Riemannian Trajectories", "comments": "Under major revision at IEEE T-PAMI, 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2564409", "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual observations of dynamic phenomena, such as human actions, are often\nrepresented as sequences of smoothly-varying features . In cases where the\nfeature spaces can be structured as Riemannian manifolds, the corresponding\nrepresentations become trajectories on manifolds. Analysis of these\ntrajectories is challenging due to non-linearity of underlying spaces and\nhigh-dimensionality of trajectories. In vision problems, given the nature of\nphysical systems involved, these phenomena are better characterized on a\nlow-dimensional manifold compared to the space of Riemannian trajectories. For\ninstance, if one does not impose physical constraints of the human body, in\ndata involving human action analysis, the resulting representation space will\nhave highly redundant features. Learning an effective, low-dimensional\nembedding for action representations will have a huge impact in the areas of\nsearch and retrieval, visualization, learning, and recognition. The difficulty\nlies in inherent non-linearity of the domain and temporal variability of\nactions that can distort any traditional metric between trajectories. To\novercome these issues, we use the framework based on transported square-root\nvelocity fields (TSRVF); this framework has several desirable properties,\nincluding a rate-invariant metric and vector space representations. We propose\nto learn an embedding such that each action trajectory is mapped to a single\npoint in a low-dimensional Euclidean space, and the trajectories that differ\nonly in temporal rates map to the same point. We utilize the TSRVF\nrepresentation, and accompanying statistical summaries of Riemannian\ntrajectories, to extend existing coding methods such as PCA, KSVD and Label\nConsistent KSVD to Riemannian trajectories or more generally to Riemannian\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:53:25 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Anirudh", "Rushil", ""], ["Turaga", "Pavan", ""], ["Su", "Jingyong", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1603.02211", "submitter": "Rajesh  Kumar", "authors": "Rajesh Kumar, Vir V Phoha, and Rahul Raina", "title": "Authenticating users through their arm movement patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose four continuous authentication designs by using the\ncharacteristics of arm movements while individuals walk. The first design uses\nacceleration of arms captured by a smartwatch's accelerometer sensor, the\nsecond design uses the rotation of arms captured by a smartwatch's gyroscope\nsensor, third uses the fusion of both acceleration and rotation at the\nfeature-level and fourth uses the fusion at score-level. Each of these designs\nis implemented by using four classifiers, namely, k nearest neighbors (k-NN)\nwith Euclidean distance, Logistic Regression, Multilayer Perceptrons, and\nRandom Forest resulting in a total of sixteen authentication mechanisms. These\nauthentication mechanisms are tested under three different environments, namely\nan intra-session, inter-session on a dataset of 40 users and an inter-phase on\na dataset of 12 users. The sessions of data collection were separated by at\nleast ten minutes, whereas the phases of data collection were separated by at\nleast three months. Under the intra-session environment, all of the twelve\nauthentication mechanisms achieve a mean dynamic false accept rate (DFAR) of 0%\nand dynamic false reject rate (DFRR) of 0%. For the inter-session environment,\nfeature level fusion-based design with classifier k-NN achieves the best error\nrates that are a mean DFAR of 2.2% and DFRR of 4.2%. The DFAR and DFRR\nincreased from 5.68% and 4.23% to 15.03% and 14.62% respectively when feature\nlevel fusion-based design with classifier k-NN was tested under the inter-phase\nenvironment on a dataset of 12 users.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 19:15:39 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kumar", "Rajesh", ""], ["Phoha", "Vir V", ""], ["Raina", "Rahul", ""]]}, {"id": "1603.02252", "submitter": "Wenbin Li", "authors": "Wenbin Li and Darren Cosker and Matthew Brown", "title": "Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences", "comments": "Preprint version of our paper accepted by Journal of Intelligent and\n  Fuzzy Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is hard to densely track a nonrigid object in long term, which is a\nfundamental research issue in the computer vision community. This task often\nrelies on estimating pairwise correspondences between images over time where\nthe error is accumulated and leads to a drift issue. In this paper, we\nintroduce a novel optimization framework with an Anchor Patch constraint. It is\nsupposed to significantly reduce overall errors given long sequences containing\nnon-rigidly deformable objects. Our framework can be applied to any dense\ntracking algorithm, e.g. optical flow. We demonstrate the success of our\napproach by showing significant error reduction on 6 popular optical flow\nalgorithms applied to a range of real-world nonrigid benchmarks. We also\nprovide quantitative analysis of our approach given synthetic occlusions and\nimage noise.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:51:15 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Li", "Wenbin", ""], ["Cosker", "Darren", ""], ["Brown", "Matthew", ""]]}, {"id": "1603.02253", "submitter": "Wenbin Li", "authors": "Wenbin Li and Yang Chen and JeeHang Lee and Gang Ren and Darren Cosker", "title": "Blur Robust Optical Flow using Motion Channel", "comments": "Preprint of our paper accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to estimate optical flow given a realworld video sequence with\ncamera shake and other motion blur. In this paper, we first investigate the\nblur parameterization for video footage using near linear motion elements. we\nthen combine a commercial 3D pose sensor with an RGB camera, in order to film\nvideo footage of interest together with the camera motion. We illustrates that\nthis additional camera motion/trajectory channel can be embedded into a hybrid\nframework by interleaving an iterative blind deconvolution and warping based\noptical flow scheme. Our method yields improved accuracy within three other\nstate-of-the-art baselines given our proposed ground truth blurry sequences;\nand several other realworld sequences filmed by our imaging system.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:53:20 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Li", "Wenbin", ""], ["Chen", "Yang", ""], ["Lee", "JeeHang", ""], ["Ren", "Gang", ""], ["Cosker", "Darren", ""]]}, {"id": "1603.02345", "submitter": "Byeongkeun Kang", "authors": "Byeongkeun Kang, Kar-Han Tan, Nan Jiang, Hung-Shuo Tai, Daniel\n  Tretter, and Truong Q. Nguyen", "title": "Hand Segmentation for Hand-Object Interaction from Depth map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand segmentation for hand-object interaction is a necessary preprocessing\nstep in many applications such as augmented reality, medical application, and\nhuman-robot interaction. However, typical methods are based on color\ninformation which is not robust to objects with skin color, skin pigment\ndifference, and light condition variations. Thus, we propose hand segmentation\nmethod for hand-object interaction using only a depth map. It is challenging\nbecause of the small depth difference between a hand and objects during an\ninteraction. To overcome this challenge, we propose the two-stage random\ndecision forest (RDF) method consisting of detecting hands and segmenting\nhands. To validate the proposed method, we demonstrate results on the publicly\navailable dataset of hand segmentation for hand-object interaction. The\nproposed method achieves high accuracy in short processing time comparing to\nthe other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 00:22:59 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 01:02:20 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 03:20:52 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Kang", "Byeongkeun", ""], ["Tan", "Kar-Han", ""], ["Jiang", "Nan", ""], ["Tai", "Hung-Shuo", ""], ["Tretter", "Daniel", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1603.02447", "submitter": "Anithadevi Dhanuskodi", "authors": "D. Anithadevi and K. Perumal", "title": "A hybrid approach based segmentation technique for brain tumor in MRI\n  Images", "comments": "Attended conference for this paper in NCNHCIIS 2015 at Gandhigram\n  University. Published in AIRCC February 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Automatic image segmentation becomes very crucial for tumor detection in\nmedical image processing.In general, manual and semi automatic segmentation\ntechniques require more time and knowledge. However these drawbacks had\novercome by automatic segmentation still there needs to develop more\nappropriate techniques for medical image segmentation. Therefore, we proposed\nhybrid approach based image segmentation using the combined features of region\ngrowing and threshold based segmentation techniques. It is followed by\npre-processing stage to provide an accurate brain tumor extraction by the help\nof Magnetic Resonance Imaging (MRI). If the tumor has holes, the region growing\nsegmentation algorithm cannot reveal but the proposed hybrid segmentation\ntechnique can be achieved and the result as well improved. Hence the result\nused to made assessment with the various performance measures as DICE, JACCARD\nsimilarity, accuracy, sensitivity and specificity. These similarity measures\nhave been extensively used for evaluation with the ground truth of each\nprocessed image and its results are compared and analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 09:50:43 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Anithadevi", "D.", ""], ["Perumal", "K.", ""]]}, {"id": "1603.02466", "submitter": "Seba Susan", "authors": "Seba Susan, Madasu Hanmandlu", "title": "A non-extensive entropy feature and its application to texture\n  classification", "comments": null, "journal-ref": "Neurocomputing 120 (2013): 214-225", "doi": "10.1016/j.neucom.2012.09.043", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new probabilistic non-extensive entropy feature for\ntexture characterization, based on a Gaussian information measure. The\nhighlights of the new entropy are that it is bounded by finite limits and that\nit is non additive in nature. The non additive property of the proposed entropy\nmakes it useful for the representation of information content in the\nnon-extensive systems containing some degree of regularity or correlation. The\neffectiveness of the proposed entropy in representing the correlated random\nvariables is demonstrated by applying it for the texture classification problem\nsince textures found in nature are random and at the same time contain some\ndegree of correlation or regularity at some scale. The gray level co-occurrence\nprobabilities (GLCP) are used for computing the entropy function. The\nexperimental results indicate high degree of the classification accuracy. The\nperformance of the new entropy function is found superior to other forms of\nentropy such as Shannon, Renyi, Tsallis and Pal and Pal entropies on\ncomparison. Using the feature based polar interaction maps (FBIM) the proposed\nentropy is shown to be the best measure among the entropies compared for\nrepresenting the correlated textures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 10:31:55 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Susan", "Seba", ""], ["Hanmandlu", "Madasu", ""]]}, {"id": "1603.02518", "submitter": "Luisa Zintgraf", "authors": "Luisa M. Zintgraf, Taco S. Cohen, Max Welling", "title": "A New Method to Visualize Deep Neural Networks", "comments": "Please note that this version of the article is outdated. The new\n  version (published at ICLR2017) includes additional experiments on MRI scans\n  and can be found at arXiv:1702.04595", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for visualising the response of a deep neural network to\na specific input. For image data for instance our method will highlight areas\nthat provide evidence in favor of, and against choosing a certain class. The\nmethod overcomes several shortcomings of previous methods and provides great\nadditional insight into the decision making process of convolutional networks,\nwhich is important both to improve models and to accelerate the adoption of\nsuch methods in e.g. medicine. In experiments on ImageNet data, we illustrate\nhow the method works and can be applied in different ways to understand deep\nneural nets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 13:46:28 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 13:51:08 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 15:54:09 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zintgraf", "Luisa M.", ""], ["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1603.02617", "submitter": "Rigas Kouskouridas", "authors": "Caner Sahin, Rigas Kouskouridas and Tae-Kyun Kim", "title": "Iterative Hough Forest with Histogram of Control Points for 6 DoF Object\n  Registration from Depth Images", "comments": "IROS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art techniques proposed for 6D object pose recovery depend on\nocclusion-free point clouds to accurately register objects in 3D space. To\nreduce this dependency, we introduce a novel architecture called Iterative\nHough Forest with Histogram of Control Points that is capable of estimating\noccluded and cluttered objects' 6D pose given a candidate 2D bounding box. Our\nIterative Hough Forest is learnt using patches extracted only from the positive\nsamples. These patches are represented with Histogram of Control Points (HoCP),\na \"scale-variant\" implicit volumetric description, which we derive from\nrecently introduced Implicit B-Splines (IBS). The rich discriminative\ninformation provided by this scale-variance is leveraged during inference,\nwhere the initial pose estimation of the object is iteratively refined based on\nmore discriminative control points by using our Iterative Hough Forest. We\nconduct experiments on several test objects of a publicly available dataset to\ntest our architecture and to compare with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:33:44 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 12:43:53 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Sahin", "Caner", ""], ["Kouskouridas", "Rigas", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1603.02618", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham, Marco Baroni", "title": "The red one!: On learning to refer to things based on their\n  discriminative properties", "comments": "Accepted as an ACL-short sumbmission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a first step towards agents learning to communicate about their visual\nenvironment, we propose a system that, given visual representations of a\nreferent (cat) and a context (sofa), identifies their discriminative\nattributes, i.e., properties that distinguish them (has_tail). Moreover,\ndespite the lack of direct supervision at the attribute level, the model learns\nto assign plausible attributes to objects (sofa-has_cushion). Finally, we\npresent a preliminary experiment confirming the referential success of the\npredicted discriminative attributes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:39:46 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 17:04:15 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1603.02636", "submitter": "Lucas Beyer", "authors": "Lucas Beyer and Alexander Hermans and Bastian Leibe", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range\n  Data", "comments": "Lucas Beyer and Alexander Hermans contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the DROW detector, a deep learning based detector for 2D range\ndata. Laser scanners are lighting invariant, provide accurate range data, and\ntypically cover a large field of view, making them interesting sensors for\nrobotics applications. So far, research on detection in laser range data has\nbeen dominated by hand-crafted features and boosted classifiers, potentially\nlosing performance due to suboptimal design choices. We propose a Convolutional\nNeural Network (CNN) based detector for this task. We show how to effectively\napply CNNs for detection in 2D range data, and propose a depth preprocessing\nstep and voting scheme that significantly improve CNN performance. We\ndemonstrate our approach on wheelchairs and walkers, obtaining state of the art\ndetection results. Apart from the training data, none of our design choices\nlimits the detector to these two classes, though. We provide a ROS node for our\ndetector and release our dataset containing 464k laser scans, out of which 24k\nwere annotated.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:39:19 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 18:06:28 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Beyer", "Lucas", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "1603.02649", "submitter": "Aleksandar Dimitriev", "authors": "Aleksandar Dimitriev, Matej Kristan", "title": "A regularization-based approach for unsupervised image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised image segmentation algorithm, which aims to\nsegment an image into several coherent parts. It requires no user input, no\nsupervised learning phase and assumes an unknown number of segments. It\nachieves this by first over-segmenting the image into several hundred\nsuperpixels. These are iteratively joined on the basis of a discriminative\nclassifier trained on color and texture information obtained from each\nsuperpixel. The output of the classifier is regularized by a Markov random\nfield that lends more influence to neighbouring superpixels that are more\nsimilar. In each iteration, similar superpixels fall under the same label,\nuntil only a few coherent regions remain in the image. The algorithm was tested\non a standard evaluation data set, where it performs on par with\nstate-of-the-art algorithms in term of precision and greatly outperforms the\nstate of the art by reducing the oversegmentation of the object of interest.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 20:02:28 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Dimitriev", "Aleksandar", ""], ["Kristan", "Matej", ""]]}, {"id": "1603.02729", "submitter": "Yiannis Aloimonos", "authors": "Ruzena Bajcsy and Yiannis Aloimonos and John K. Tsotsos", "title": "Revisiting Active Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes in robotics, artificial intelligence and\ncomputer vision, a complete artificial agent necessarily must include active\nperception. A multitude of ideas and methods for how to accomplish this have\nalready appeared in the past, their broader utility perhaps impeded by\ninsufficient computational power or costly hardware. The history of these\nideas, perhaps selective due to our perspectives, is presented with the goal of\norganizing the past literature and highlighting the seminal contributions. We\nargue that those contributions are as relevant today as they were decades ago\nand, with the state of modern computational tools, are poised to find new life\nin the robotic perception systems of the next decade.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 22:48:26 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 18:29:08 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Bajcsy", "Ruzena", ""], ["Aloimonos", "Yiannis", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1603.02736", "submitter": "Umamahesh Srinivas", "authors": "Umamahesh Srinivas", "title": "Discriminative models for robust image classification", "comments": "Doctoral dissertation, Department of Electrical Engineering, The\n  Pennsylvania State University, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of real-world tasks involve the classification of images into\npre-determined categories. Designing image classification algorithms that\nexhibit robustness to acquisition noise and image distortions, particularly\nwhen the available training data are insufficient to learn accurate models, is\na significant challenge. This dissertation explores the development of\ndiscriminative models for robust image classification that exploit underlying\nsignal structure, via probabilistic graphical models and sparse signal\nrepresentations.\n  Probabilistic graphical models are widely used in many applications to\napproximate high-dimensional data in a reduced complexity set-up. Learning\ngraphical structures to approximate probability distributions is an area of\nactive research. Recent work has focused on learning graphs in a discriminative\nmanner with the goal of minimizing classification error. In the first part of\nthe dissertation, we develop a discriminative learning framework that exploits\nthe complementary yet correlated information offered by multiple\nrepresentations (or projections) of a given signal/image. Specifically, we\npropose a discriminative tree-based scheme for feature fusion by explicitly\nlearning the conditional correlations among such multiple projections in an\niterative manner. Experiments reveal the robustness of the resulting graphical\nmodel classifier to training insufficiency.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 23:15:18 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Srinivas", "Umamahesh", ""]]}, {"id": "1603.02814", "submitter": "Chunhua Shen", "authors": "Qi Wu, Chunhua Shen, Anton van den Hengel, Peng Wang, Anthony Dick", "title": "Image Captioning and Visual Question Answering Based on Attributes and\n  External Knowledge", "comments": "14 pages. arXiv admin note: text overlap with arXiv:1511.06973", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent progress in Vision-to-Language problems has been achieved through\na combination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs). This approach does not explicitly represent high-level\nsemantic concepts, but rather seeks to progress directly from image features to\ntext. In this paper we first propose a method of incorporating high-level\nconcepts into the successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art in both image captioning and\nvisual question answering. We further show that the same mechanism can be used\nto incorporate external knowledge, which is critically important for answering\nhigh level visual questions. Specifically, we design a visual question\nanswering model that combines an internal representation of the content of an\nimage with information extracted from a general knowledge base to answer a\nbroad range of image-based questions. It particularly allows questions to be\nasked about the contents of an image, even when the image itself does not\ncontain a complete answer. Our final model achieves the best reported results\non both image captioning and visual question answering on several benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 08:56:45 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 11:44:34 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Wang", "Peng", ""], ["Dick", "Anthony", ""]]}, {"id": "1603.02844", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Guosheng Lin, Chunhua Shen, Ian Reid", "title": "Fast Training of Triplet-based Deep Binary Embedding Networks", "comments": "Apeparing in Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2016. Code is at\n  https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to learn a mapping (or embedding) from images to a\ncompact binary space in which Hamming distances correspond to a ranking measure\nfor the image retrieval task.\n  We make use of a triplet loss because this has been shown to be most\neffective for ranking problems.\n  However, training in previous works can be prohibitively expensive due to the\nfact that optimization is directly performed on the triplet space, where the\nnumber of possible triplets for training is cubic in the number of training\nexamples.\n  To address this issue, we propose to formulate high-order binary codes\nlearning as a multi-label classification problem by explicitly separating\nlearning into two interleaved stages.\n  To solve the first stage, we design a large-scale high-order binary codes\ninference algorithm to reduce the high-order objective to a standard binary\nquadratic problem such that graph cuts can be used to efficiently infer the\nbinary code which serve as the label of each training datum.\n  In the second stage we propose to map the original image to compact binary\ncodes via carefully designed deep convolutional neural networks (CNNs) and the\nhashing function fitting can be solved by training binary CNN classifiers.\n  An incremental/interleaved optimization strategy is proffered to ensure that\nthese two steps are interactive with each other during training for better\naccuracy.\n  We conduct experiments on several benchmark datasets, which demonstrate both\nimproved training time (by as much as two orders of magnitude) as well as\nproducing state-of-the-art hashing for various retrieval tasks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 11:10:12 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 01:52:57 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Zhuang", "Bohan", ""], ["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1603.03101", "submitter": "Chen-Yu Lee", "authors": "Chen-Yu Lee and Simon Osindero", "title": "Recursive Recurrent Nets with Attention Modeling for OCR in the Wild", "comments": "accepted at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present recursive recurrent neural networks with attention modeling\n(R$^2$AM) for lexicon-free optical character recognition in natural scene\nimages. The primary advantages of the proposed method are: (1) use of recursive\nconvolutional neural networks (CNNs), which allow for parametrically efficient\nand effective image feature extraction; (2) an implicitly learned\ncharacter-level language model, embodied in a recurrent neural network which\navoids the need to use N-grams; and (3) the use of a soft-attention mechanism,\nallowing the model to selectively exploit image features in a coordinated way,\nand allowing for end-to-end training within a standard backpropagation\nframework. We validate our method with state-of-the-art performance on\nchallenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 23:49:51 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Osindero", "Simon", ""]]}, {"id": "1603.03183", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid", "title": "Exploring Context with Deep Structured models for Semantic Segmentation", "comments": "16 pages. Accepted to IEEE T. Pattern Analysis & Machine\n  Intelligence, 2017. Extended version of arXiv:1504.01013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art semantic image segmentation methods are mostly based on\ntraining deep convolutional neural networks (CNNs). In this work, we proffer to\nimprove semantic segmentation with the use of contextual information. In\nparticular, we explore `patch-patch' context and `patch-background' context in\ndeep CNNs. We formulate deep structured models by combining CNNs and\nConditional Random Fields (CRFs) for learning the patch-patch context between\nimage regions. Specifically, we formulate CNN-based pairwise potential\nfunctions to capture semantic correlations between neighboring patches.\nEfficient piecewise training of the proposed deep structured model is then\napplied in order to avoid repeated expensive CRF inference during the course of\nback propagation. For capturing the patch-background context, we show that a\nnetwork design with traditional multi-scale image inputs and sliding pyramid\npooling is very effective for improving performance. We perform comprehensive\nevaluation of the proposed method. We achieve new state-of-the-art performance\non a number of challenging semantic segmentation datasets including $NYUDv2$,\n$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,\n$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an\nintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 08:34:19 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2016 12:24:30 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 08:06:42 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Reid", "Ian", ""]]}, {"id": "1603.03234", "submitter": "Hanjiang Lai", "authors": "Hanjiang Lai, Pan Yan, Xiangbo Shu, Yunchao Wei, Shuicheng Yan", "title": "Instance-Aware Hashing for Multi-Label Image Retrieval", "comments": "has been accepted as a regular paper in the IEEE Transactions on\n  Image Processing, 2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2545300", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-preserving hashing is a commonly used method for nearest neighbour\nsearch in large-scale image retrieval. For image retrieval, deep-networks-based\nhashing methods are appealing since they can simultaneously learn effective\nimage representations and compact hash codes. This paper focuses on\ndeep-networks-based hashing for multi-label images, each of which may contain\nobjects of multiple categories. In most existing hashing methods, each image is\nrepresented by one piece of hash code, which is referred to as semantic\nhashing. This setting may be suboptimal for multi-label image retrieval. To\nsolve this problem, we propose a deep architecture that learns\n\\textbf{instance-aware} image representations for multi-label image data, which\nare organized in multiple groups, with each group containing the features for\none category. The instance-aware representations not only bring advantages to\nsemantic hashing, but also can be used in category-aware hashing, in which an\nimage is represented by multiple pieces of hash codes and each piece of code\ncorresponds to a category. Extensive evaluations conducted on several benchmark\ndatasets demonstrate that, for both semantic hashing and category-aware\nhashing, the proposed method shows substantial improvement over the\nstate-of-the-art supervised and unsupervised hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 12:21:50 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lai", "Hanjiang", ""], ["Yan", "Pan", ""], ["Shu", "Xiangbo", ""], ["Wei", "Yunchao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1603.03235", "submitter": "Amir Soleimani", "authors": "Amir Soleimani, Kazim Fouladi, Babak N. Araabi", "title": "UTSig: A Persian Offline Signature Dataset", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1049/iet-bmt.2015.0058", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pivotal role of datasets in signature verification systems motivates\nresearchers to collect signature samples. Distinct characteristics of Persian\nsignature demands for richer and culture-dependent offline signature datasets.\nThis paper introduces a new and public Persian offline signature dataset,\nUTSig, that consists of 8280 images from 115 classes. Each class has 27 genuine\nsignatures, 3 opposite-hand signatures, and 42 skilled forgeries made by 6\nforgers. Compared with the other public datasets, UTSig has more samples, more\nclasses, and more forgers. We considered various variables including signing\nperiod, writing instrument, signature box size, and number of observable\nsamples for forgers in the data collection procedure. By careful examination of\nmain characteristics of offline signature datasets, we observe that Persian\nsignatures have fewer numbers of branch points and end points. We propose and\nevaluate four different training and test setups for UTSig. Results of our\nexperiments show that training genuine samples along with opposite-hand samples\nand random forgeries can improve the performance in terms of equal error rate\nand minimum cost of log likelihood ratio.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 12:23:03 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 12:55:39 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 04:21:25 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 06:58:59 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Soleimani", "Amir", ""], ["Fouladi", "Kazim", ""], ["Araabi", "Babak N.", ""]]}, {"id": "1603.03304", "submitter": "Erik Bekkers", "authors": "Erik J. Bekkers and Marco Loog and Bart M. ter Haar Romeny and Remco\n  Duits", "title": "Template Matching via Densities on the Roto-Translation Group", "comments": "28 pages, 11 figures, 5 appendices. Paper accepted at ieee\n  transaction on Pattern Analysis and Machine Inteligence (ieee tPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2652452", "report-no": null, "categories": "cs.CV math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a template matching method for the detection of 2D image objects\nthat are characterized by orientation patterns. Our method is based on data\nrepresentations via orientation scores, which are functions on the space of\npositions and orientations, and which are obtained via a wavelet-type\ntransform. This new representation allows us to detect orientation patterns in\nan intuitive and direct way, namely via cross-correlations. Additionally, we\npropose a generalized linear regression framework for the construction of\nsuitable templates using smoothing splines. Here, it is important to recognize\na curved geometry on the position-orientation domain, which we identify with\nthe Lie group SE(2): the roto-translation group. Templates are then optimized\nin a B-spline basis, and smoothness is defined with respect to the curved\ngeometry. We achieve state-of-the-art results on three different applications:\ndetection of the optic nerve head in the retina (99.83% success rate on 1737\nimages), of the fovea in the retina (99.32% success rate on 1616 images), and\nof the pupil in regular camera images (95.86% on 1521 images). The high\nperformance is due to inclusion of both intensity and orientation features with\neffective geometric priors in the template matching. Moreover, our method is\nfast due to a cross-correlation based matching approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 15:46:57 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 09:19:37 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 08:21:44 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 14:44:33 GMT"}, {"version": "v5", "created": "Thu, 9 Mar 2017 15:05:21 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Bekkers", "Erik J.", ""], ["Loog", "Marco", ""], ["Romeny", "Bart M. ter Haar", ""], ["Duits", "Remco", ""]]}, {"id": "1603.03369", "submitter": "Ke Zhang", "authors": "Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman", "title": "Summary Transfer: Exemplar-based Subset Selection for Video\n  Summarization", "comments": "CVPR 2016 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization has unprecedented importance to help us digest, browse,\nand search today's ever-growing video collections. We propose a novel subset\nselection technique that leverages supervision in the form of human-created\nsummaries to perform automatic keyframe-based video summarization. The main\nidea is to nonparametrically transfer summary structures from annotated videos\nto unseen test videos. We show how to extend our method to exploit semantic\nside information about the video's category/genre to guide the transfer process\nby those training videos semantically consistent with the test input. We also\nshow how to generalize our method to subshot-based summarization, which not\nonly reduces computational costs but also provides more flexible ways of\ndefining visual similarity across subshots spanning several frames. We conduct\nextensive evaluation on several benchmarks and demonstrate promising results,\noutperforming existing methods in several settings.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 18:59:14 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 03:26:27 GMT"}, {"version": "v3", "created": "Fri, 29 Apr 2016 08:11:32 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Zhang", "Ke", ""], ["Chao", "Wei-Lun", ""], ["Sha", "Fei", ""], ["Grauman", "Kristen", ""]]}, {"id": "1603.03381", "submitter": "Armin Mustafa", "authors": "Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton", "title": "Temporally coherent 4D reconstruction of complex dynamic scenes", "comments": "To appear in The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016 . Video available at:\n  https://www.youtube.com/watch?v=bm_P13_-DsQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for reconstruction of 4D temporally coherent\nmodels of complex dynamic scenes. No prior knowledge is required of scene\nstructure or camera calibration allowing reconstruction from multiple moving\ncameras. Sparse-to-dense temporal correspondence is integrated with joint\nmulti-view segmentation and reconstruction to obtain a complete 4D\nrepresentation of static and dynamic objects. Temporal coherence is exploited\nto overcome visual ambiguities resulting in improved reconstruction of complex\nscenes. Robust joint segmentation and reconstruction of dynamic objects is\nachieved by introducing a geodesic star convexity constraint. Comparative\nevaluation is performed on a variety of unstructured indoor and outdoor dynamic\nscenes with hand-held cameras and multiple people. This demonstrates\nreconstruction of complete temporally coherent 4D scene models with improved\nnonrigid object segmentation and shape reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 19:16:43 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 11:17:48 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Mustafa", "Armin", ""], ["Kim", "Hansung", ""], ["Guillemaut", "Jean-Yves", ""], ["Hilton", "Adrian", ""]]}, {"id": "1603.03417", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky", "title": "Texture Networks: Feed-forward Synthesis of Textures and Stylized Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gatys et al. recently demonstrated that deep networks can generate beautiful\ntextures and stylized images from a single texture example. However, their\nmethods requires a slow and memory-consuming optimization process. We propose\nhere an alternative approach that moves the computational burden to a learning\nstage. Given a single example of a texture, our approach trains compact\nfeed-forward convolutional networks to generate multiple samples of the same\ntexture of arbitrary size and to transfer artistic style from a given image to\nany other image. The resulting networks are remarkably light-weight and can\ngenerate textures of quality comparable to Gatys~et~al., but hundreds of times\nfaster. More generally, our approach highlights the power and flexibility of\ngenerative feed-forward models trained with complex and expressive loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 20:45:40 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Lebedev", "Vadim", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1603.03541", "submitter": "Chenxia Wu", "authors": "Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese,\n  Ashutosh Saxena", "title": "Watch-n-Patch: Unsupervised Learning of Actions and Relations", "comments": "arXiv admin note: text overlap with arXiv:1512.04208", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large variation in the activities that humans perform in their\neveryday lives. We consider modeling these composite human activities which\ncomprises multiple basic level actions in a completely unsupervised setting.\nOur model learns high-level co-occurrence and temporal relations between the\nactions. We consider the video as a sequence of short-term action clips, which\ncontains human-words and object-words. An activity is about a set of\naction-topics and object-topics indicating which actions are present and which\nobjects are interacting with. We then propose a new probabilistic model\nrelating the words and the topics. It allows us to model long-range action\nrelations that commonly exist in the composite activities, which is challenging\nin previous works. We apply our model to the unsupervised action segmentation\nand clustering, and to a novel application that detects forgotten actions,\nwhich we call action patching. For evaluation, we contribute a new challenging\nRGB-D activity video dataset recorded by the new Kinect v2, which contains\nseveral human daily activities as compositions of multiple actions interacting\nwith different objects. Moreover, we develop a robotic system that watches\npeople and reminds people by applying our action patching algorithm. Our\nrobotic setup can be easily deployed on any assistive robot.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 07:13:59 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Wu", "Chenxia", ""], ["Zhang", "Jiemi", ""], ["Sener", "Ozan", ""], ["Selman", "Bart", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1603.03590", "submitter": "Till Kroeger", "authors": "Till Kroeger and Radu Timofte and Dengxin Dai and Luc Van Gool", "title": "Fast Optical Flow using Dense Inverse Search", "comments": "9 pages main paper + 16 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent works in optical flow extraction focus on the accuracy and\nneglect the time complexity. However, in real-life visual applications, such as\ntracking, activity detection and recognition, the time complexity is critical.\n  We propose a solution with very low time complexity and competitive accuracy\nfor the computation of dense optical flow. It consists of three parts: 1)\ninverse search for patch correspondences; 2) dense displacement field creation\nthrough patch aggregation along multiple scales; 3) variational refinement. At\nthe core of our Dense Inverse Search-based method (DIS) is the efficient search\nof correspondences inspired by the inverse compositional image alignment\nproposed by Baker and Matthews in 2001.\n  DIS is competitive on standard optical flow benchmarks with large\ndisplacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching the\ntemporal resolution of human's biological vision system. It is order(s) of\nmagnitude faster than state-of-the-art methods in the same range of accuracy,\nmaking DIS ideal for visual applications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 10:55:07 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Kroeger", "Till", ""], ["Timofte", "Radu", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1603.03657", "submitter": "Koen Groenland", "authors": "Koen Groenland, Sander Bohte", "title": "Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 15:16:09 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Groenland", "Koen", ""], ["Bohte", "Sander", ""]]}, {"id": "1603.03669", "submitter": "George Leifman", "authors": "G. Leifman, D. Rudoy, T. Swedish, E. Bayro-Corrochano and R. Raskar", "title": "Learning Gaze Transitions from Depth to Improve Video Saliency\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel Depth-Aware Video Saliency approach to\npredict human focus of attention when viewing RGBD videos on regular 2D\nscreens. We train a generative convolutional neural network which predicts a\nsaliency map for a frame, given the fixation map of the previous frame.\nSaliency estimation in this scenario is highly important since in the near\nfuture 3D video content will be easily acquired and yet hard to display. This\ncan be explained, on the one hand, by the dramatic improvement of 3D-capable\nacquisition equipment. On the other hand, despite the considerable progress in\n3D display technologies, most of the 3D displays are still expensive and\nrequire wearing special glasses. To evaluate the performance of our approach,\nwe present a new comprehensive database of eye-fixation ground-truth for RGBD\nvideos. Our experiments indicate that integrating depth into video saliency\ncalculation is beneficial. We demonstrate that our approach outperforms\nstate-of-the-art methods for video saliency, achieving 15% relative\nimprovement.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 15:53:58 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Leifman", "G.", ""], ["Rudoy", "D.", ""], ["Swedish", "T.", ""], ["Bayro-Corrochano", "E.", ""], ["Raskar", "R.", ""]]}, {"id": "1603.03783", "submitter": "Sachin Mehta", "authors": "Sachin Mehta and Balakrishnan Prabhakaran", "title": "Region Graph Based Method for Multi-Object Detection and Tracking using\n  Depth Cameras", "comments": "Accepted in IEEE Winter Conference in Computer Vision (WACV'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-object detection and tracking method using\ndepth cameras. Depth maps are very noisy and obscure in object detection. We\nfirst propose a region-based method to suppress high magnitude noise which\ncannot be filtered using spatial filters. Second, the proposed method detect\nRegion of Interests by temporal learning which are then tracked using weighted\ngraph-based approach. We demonstrate the performance of the proposed method on\nstandard depth camera datasets with and without object occlusions. Experimental\nresults show that the proposed method is able to suppress high magnitude noise\nin depth maps and detect/track the objects (with and without occlusion).\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:06:35 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Mehta", "Sachin", ""], ["Prabhakaran", "Balakrishnan", ""]]}, {"id": "1603.03856", "submitter": "Mo'taz Al-Hami", "authors": "Kristiyan Georgiev, Motaz Al-Hami, Rolf Lakaemper", "title": "Real-time 3D scene description using Spheres, Cones and Cylinders", "comments": "8 Pages, 16th International Conference on Advanced Robotics (ICAR\n  2013). Montevideo, Uruguay, November 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a novel real-time algorithm for finding 3D geometric\nprimitives (cylinders, cones and spheres) from 3D range data. In its core, it\nperforms a fast model fitting with a model update in constant time (O(1)) for\neach new data point added to the model. We use a three stage approach.The first\nstep inspects 1.5D sub spaces, to find ellipses. The next stage uses these\nellipses as input by examining their neighborhood structure to form sets of\ncandidates for the 3D geometric primitives. Finally, candidate ellipses are\nfitted to the geometric primitives. The complexity for point processing is\nO(n); additional time of lower order is needed for working on significantly\nsmaller amount of mid-level objects. This allows the approach to process 30\nframes per second on Kinect depth data, which suggests this approach as a\npre-processing step for 3D real-time higher level tasks in robotics, like\ntracking or feature based mapping.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 04:03:46 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Georgiev", "Kristiyan", ""], ["Al-Hami", "Motaz", ""], ["Lakaemper", "Rolf", ""]]}, {"id": "1603.03875", "submitter": "Zhe Wu", "authors": "Zhe Wu, Sai-Kit Yeung, Ping Tan", "title": "Towards Building an RGBD-M Scanner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a portable device to capture both shape and reflectance of an\nindoor scene. Consisting of a Kinect, an IR camera and several IR LEDs, our\ndevice allows the user to acquire data in a similar way as he/she scans with a\nsingle Kinect. Scene geometry is reconstructed by KinectFusion. To estimate\nreflectance from incomplete and noisy observations, 3D vertices of the same\nmaterial are identified by our material segmentation propagation algorithm.\nThen BRDF observations at these vertices are merged into a more complete and\naccurate BRDF for the material. Effectiveness of our device is demonstrated by\nquality results on real-world scenes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 09:00:24 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Wu", "Zhe", ""], ["Yeung", "Sai-Kit", ""], ["Tan", "Ping", ""]]}, {"id": "1603.03911", "submitter": "Laura Sevilla-Lara", "authors": "Laura Sevilla-Lara, Deqing Sun, Varun Jampani, Michael J. Black", "title": "Optical Flow with Semantic Segmentation and Localized Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing optical flow methods make generic, spatially homogeneous,\nassumptions about the spatial structure of the flow. In reality, optical flow\nvaries across an image depending on object class. Simply put, different objects\nmove differently. Here we exploit recent advances in static semantic scene\nsegmentation to segment the image into objects of different types. We define\ndifferent models of image motion in these regions depending on the type of\nobject. For example, we model the motion on roads with homographies, vegetation\nwith spatially smooth flow, and independently moving objects like cars and\nplanes with affine motion plus deviations. We then pose the flow estimation\nproblem using a novel formulation of localized layers, which addresses\nlimitations of traditional layered models for dealing with complex scene\nmotion. Our semantic flow method achieves the lowest error of any published\nmonocular method in the KITTI-2015 flow benchmark and produces qualitatively\nbetter flow and segmentation than recent top methods on a wide range of natural\nvideos.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 13:34:09 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 13:00:13 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Sevilla-Lara", "Laura", ""], ["Sun", "Deqing", ""], ["Jampani", "Varun", ""], ["Black", "Michael J.", ""]]}, {"id": "1603.03915", "submitter": "Baoguang Shi", "authors": "Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai", "title": "Robust Scene Text Recognition with Automatic Rectification", "comments": "Accepted by CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing text in natural images is a challenging task with many unsolved\nproblems. Different from those in documents, words in natural images often\npossess irregular shapes, which are caused by perspective distortion, curved\ncharacter placement, etc. We propose RARE (Robust text recognizer with\nAutomatic REctification), a recognition model that is robust to irregular text.\nRARE is a specially-designed deep neural network, which consists of a Spatial\nTransformer Network (STN) and a Sequence Recognition Network (SRN). In testing,\nan image is firstly rectified via a predicted Thin-Plate-Spline (TPS)\ntransformation, into a more \"readable\" image for the following SRN, which\nrecognizes text through a sequence recognition approach. We show that the model\nis able to recognize several types of irregular text, including perspective\ntext and curved text. RARE is end-to-end trainable, requiring only images and\nassociated text labels, making it convenient to train and deploy the model in\npractical systems. State-of-the-art or highly-competitive performance achieved\non several benchmarks well demonstrates the effectiveness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 13:58:27 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 14:44:54 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Shi", "Baoguang", ""], ["Wang", "Xinggang", ""], ["Lyu", "Pengyuan", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1603.03925", "submitter": "Quanzeng You", "authors": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo", "title": "Image Captioning with Semantic Attention", "comments": "10 pages, 5 figures, CVPR16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating a natural language description of an image has\nattracted interests recently both because of its importance in practical\napplications and because it connects two major artificial intelligence fields:\ncomputer vision and natural language processing. Existing approaches are either\ntop-down, which start from a gist of an image and convert it into words, or\nbottom-up, which come up with words describing various aspects of an image and\nthen combine them. In this paper, we propose a new algorithm that combines both\napproaches through a model of semantic attention. Our algorithm learns to\nselectively attend to semantic concept proposals and fuse them into hidden\nstates and outputs of recurrent neural networks. The selection and fusion form\na feedback connecting the top-down and bottom-up computation. We evaluate our\nalgorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental\nresults show that our algorithm significantly outperforms the state-of-the-art\napproaches consistently across different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 15:11:43 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["You", "Quanzeng", ""], ["Jin", "Hailin", ""], ["Wang", "Zhaowen", ""], ["Fang", "Chen", ""], ["Luo", "Jiebo", ""]]}, {"id": "1603.03958", "submitter": "Jeffrey Byrne", "authors": "Nate Crosswhite, Jeffrey Byrne, Omkar M. Parkhi, Chris Stauffer, Qiong\n  Cao and Andrew Zisserman", "title": "Template Adaptation for Face Verification and Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition performance evaluation has traditionally focused on\none-to-one verification, popularized by the Labeled Faces in the Wild dataset\nfor imagery and the YouTubeFaces dataset for videos. In contrast, the newly\nreleased IJB-A face recognition dataset unifies evaluation of one-to-many face\nidentification with one-to-one face verification over templates, or sets of\nimagery and videos for a subject. In this paper, we study the problem of\ntemplate adaptation, a form of transfer learning to the set of media in a\ntemplate. Extensive performance evaluations on IJB-A show a surprising result,\nthat perhaps the simplest method of template adaptation, combining deep\nconvolutional network features with template specific linear SVMs, outperforms\nthe state-of-the-art by a wide margin. We study the effects of template size,\nnegative set construction and classifier fusion on performance, then compare\ntemplate adaptation to convolutional networks with metric learning, 2D and 3D\nalignment. Our unexpected conclusion is that these other methods, when combined\nwith template adaptation, all achieve nearly the same top performance on IJB-A\nfor template-based face verification and identification.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 19:57:17 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 19:56:52 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 02:11:02 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Crosswhite", "Nate", ""], ["Byrne", "Jeffrey", ""], ["Parkhi", "Omkar M.", ""], ["Stauffer", "Chris", ""], ["Cao", "Qiong", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1603.03968", "submitter": "S. Morteza Safdarnejad", "authors": "S. Morteza Safdarnejad, Yousef Atoum, Xiaoming Liu", "title": "Temporally Robust Global Motion Compensation by Keypoint-based\n  Congealing", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global motion compensation (GMC) removes the impact of camera motion and\ncreates a video in which the background appears static over the progression of\ntime. Various vision problems, such as human activity recognition, background\nreconstruction, and multi-object tracking can benefit from GMC. Existing GMC\nalgorithms rely on sequentially processing consecutive frames, by estimating\nthe transformation mapping the two frames, and obtaining a composite\ntransformation to a global motion compensated coordinate. Sequential GMC\nsuffers from temporal drift of frames from the accurate global coordinate, due\nto either error accumulation or sporadic failures of motion estimation at a few\nframes. We propose a temporally robust global motion compensation (TRGMC)\nalgorithm which performs accurate and stable GMC, despite complicated and\nlong-term camera motion. TRGMC densely connects pairs of frames, by matching\nlocal keypoints of each frame. A joint alignment of these frames is formulated\nas a novel keypoint-based congealing problem, where the transformation of each\nframe is updated iteratively, such that the spatial coordinates for the start\nand end points of matched keypoints are identical. Experimental results\ndemonstrate that TRGMC has superior performance in a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 21:42:18 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Safdarnejad", "S. Morteza", ""], ["Atoum", "Yousef", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1603.03984", "submitter": "Rudrasis Chakraborty Mr", "authors": "Rudrasis Chakraborty, Dohyung Seo and Baba C. Vemuri", "title": "An efficient Exact-PGA algorithm for constant curvature manifolds", "comments": "Accepted in CVPR (IEEE Conference on Computer Vision and Pattern\n  Recognition) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold-valued datasets are widely encountered in many computer vision\ntasks. A non-linear analog of the PCA, called the Principal Geodesic Analysis\n(PGA) suited for data lying on Riemannian manifolds was reported in literature\na decade ago. Since the objective function in PGA is highly non-linear and hard\nto solve efficiently in general, researchers have proposed a linear\napproximation. Though this linear approximation is easy to compute, it lacks\naccuracy especially when the data exhibits a large variance. Recently, an\nalternative called exact PGA was proposed which tries to solve the optimization\nwithout any linearization. For general Riemannian manifolds, though it gives\nbetter accuracy than the original (linearized) PGA, for data that exhibit large\nvariance, the optimization is not computationally efficient. In this paper, we\npropose an efficient exact PGA for constant curvature Riemannian manifolds\n(CCM-EPGA). CCM-EPGA differs significantly from existing PGA algorithms in two\naspects, (i) the distance between a given manifold-valued data point and the\nprincipal submanifold is computed analytically and thus no optimization is\nrequired as in existing methods. (ii) Unlike the existing PGA algorithms, the\ndescent into codimension-1 submanifolds does not require any optimization but\nis accomplished through the use of the Rimeannian inverse Exponential map and\nthe parallel transport operations. We present theoretical and experimental\nresults for constant curvature Riemannian manifolds depicting favorable\nperformance of CCM-EPGA compared to existing PGA algorithms. We also present\ndata reconstruction from principal components and directions which has not been\npresented in literature in this setting.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 02:57:34 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Seo", "Dohyung", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1603.04000", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Learning Typographic Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typography is a ubiquitous art form that affects our understanding,\nperception, and trust in what we read. Thousands of different font-faces have\nbeen created with enormous variations in the characters. In this paper, we\nlearn the style of a font by analyzing a small subset of only four letters.\nFrom these four letters, we learn two tasks. The first is a discrimination\ntask: given the four letters and a new candidate letter, does the new letter\nbelong to the same font? Second, given the four basis letters, can we generate\nall of the other letters with the same characteristics as those in the basis\nset? We use deep neural networks to address both tasks, quantitatively and\nqualitatively measure the results in a variety of novel manners, and present a\nthorough investigation of the weaknesses and strengths of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 05:44:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "1603.04015", "submitter": "Jiaxin Cai", "authors": "Jia-xin Cai, Xin Tang, Lifang Zhang, Guocan Feng", "title": "Learning zeroth class dictionary for human action recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a discriminative two-phase dictionary learning framework is\nproposed for classifying human action by sparse shape representations, in which\nthe first-phase dictionary is learned on the selected discriminative frames and\nthe second-phase dictionary is built for recognition using reconstruction\nerrors of the first-phase dictionary as input features. We propose a \"zeroth\nclass\" trick for detecting undiscriminating frames of the test video and\neliminating them before voting on the action categories. Experimental results\non benchmarks demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 10:20:25 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 11:17:53 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 13:14:07 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Cai", "Jia-xin", ""], ["Tang", "Xin", ""], ["Zhang", "Lifang", ""], ["Feng", "Guocan", ""]]}, {"id": "1603.04026", "submitter": "Huamin Ren", "authors": "Huamin Ren, Hong Pan, S{\\o}ren Ingvor Olsen, Thomas B. Moeslund", "title": "A comprehensive study of sparse codes on abnormality detection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation has been applied successfully in abnormal event\ndetection, in which the baseline is to learn a dictionary accompanied by sparse\ncodes. While much emphasis is put on discriminative dictionary construction,\nthere are no comparative studies of sparse codes regarding abnormality\ndetection. We comprehensively study two types of sparse codes solutions -\ngreedy algorithms and convex L1-norm solutions - and their impact on\nabnormality detection performance. We also propose our framework of combining\nsparse codes with different detection methods. Our comparative experiments are\ncarried out from various angles to better understand the applicability of\nsparse codes, including computation time, reconstruction error, sparsity,\ndetection accuracy, and their performance combining various detection methods.\nExperiments show that combining OMP codes with maximum coordinate detection\ncould achieve state-of-the-art performance on the UCSD dataset [14].\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 13:13:10 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Ren", "Huamin", ""], ["Pan", "Hong", ""], ["Olsen", "S\u00f8ren Ingvor", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1603.04037", "submitter": "Umar Iqbal", "authors": "Umar Iqbal, Martin Garbade, Juergen Gall", "title": "Pose for Action - Action for Pose", "comments": "Accepted to FG-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose to utilize information about human actions to improve\npose estimation in monocular videos. To this end, we present a pictorial\nstructure model that exploits high-level information about activities to\nincorporate higher-order part dependencies by modeling action specific\nappearance models and pose priors. However, instead of using an additional\nexpensive action recognition framework, the action priors are efficiently\nestimated by our pose estimation framework. This is achieved by starting with a\nuniform action prior and updating the action prior during pose estimation. We\nalso show that learning the right amount of appearance sharing among action\nclasses improves the pose estimation. We demonstrate the effectiveness of the\nproposed method on two challenging datasets for pose estimation and action\nrecognition with over 80,000 test images.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 15:09:35 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 14:01:09 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Iqbal", "Umar", ""], ["Garbade", "Martin", ""], ["Gall", "Juergen", ""]]}, {"id": "1603.04042", "submitter": "Ning Xu", "authors": "Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas Huang", "title": "Deep Interactive Object Selection", "comments": "Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive object selection is a very important research problem and has\nmany applications. Previous algorithms require substantial user interactions to\nestimate the foreground and background distributions. In this paper, we present\na novel deep learning based algorithm which has a much better understanding of\nobjectness and thus can reduce user interactions to just a few clicks. Our\nalgorithm transforms user provided positive and negative clicks into two\nEuclidean distance maps which are then concatenated with the RGB channels of\nimages to compose (image, user interactions) pairs. We generate many of such\npairs by combining several random sampling strategies to model user click\npatterns and use them to fine tune deep Fully Convolutional Networks (FCNs).\nFinally the output probability maps of our FCN 8s model is integrated with\ngraph cut optimization to refine the boundary segments. Our model is trained on\nthe PASCAL segmentation dataset and evaluated on other datasets with different\nobject classes. Experimental results on both seen and unseen objects clearly\ndemonstrate that our algorithm has a good generalization ability and is\nsuperior to all existing interactive object selection approaches.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 15:42:34 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Xu", "Ning", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Yang", "Jimei", ""], ["Huang", "Thomas", ""]]}, {"id": "1603.04046", "submitter": "Mina Masoudifar", "authors": "Mina Masoudifar and Hamid Reza Pourreza", "title": "Image and Depth from a Single Defocused Image Using Coded Aperture\n  Photography", "comments": "18 pages, 14 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth from defocus and defocus deblurring from a single image are two\nchallenging problems that are derived from the finite depth of field in\nconventional cameras. Coded aperture imaging is one of the techniques that is\nused for improving the results of these two problems. Up to now, different\nmethods have been proposed for improving the results of either defocus\ndeblurring or depth estimation. In this paper, a multi-objective function is\nproposed for evaluating and designing aperture patterns with the aim of\nimproving the results of both depth from defocus and defocus deblurring.\nPattern evaluation is performed by considering the scene illumination condition\nand camera system specification. Based on the proposed criteria, a single\nasymmetric pattern is designed that is used for restoring a sharp image and a\ndepth map from a single input. Since the designed pattern is asymmetric,\ndefocus objects on the two sides of the focal plane can be distinguished. Depth\nestimation is performed by using a new algorithm, which is based on image\nquality assessment criteria and can distinguish between blurred objects lying\nin front or behind the focal plane. Extensive simulations as well as\nexperiments on a variety of real scenes are conducted to compare our aperture\nwith previously proposed ones.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 16:29:50 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Masoudifar", "Mina", ""], ["Pourreza", "Hamid Reza", ""]]}, {"id": "1603.04116", "submitter": "Fumin Shen Dr.", "authors": "Fumin Shen, Yadong Mu, Wei Liu, Yang Yang and Heng Tao Shen", "title": "Learning Binary Codes and Binary Weights for Efficient Classification", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generic formulation that significantly expedites the\ntraining and deployment of image classification models, particularly under the\nscenarios of many image categories and high feature dimensions. As a defining\nproperty, our method represents both the images and learned classifiers using\nbinary hash codes, which are simultaneously learned from the training data.\nClassifying an image thereby reduces to computing the Hamming distance between\nthe binary codes of the image and classifiers and selecting the class with\nminimal Hamming distance. Conventionally, compact hash codes are primarily used\nfor accelerating image search. Our work is first of its kind to represent\nclassifiers using binary codes. Specifically, we formulate multi-class image\nclassification as an optimization problem over binary variables. The\noptimization alternatively proceeds over the binary classifiers and image hash\ncodes. Profiting from the special property of binary codes, we show that the\nsub-problems can be efficiently solved through either a binary quadratic\nprogram (BQP) or linear program. In particular, for attacking the BQP problem,\nwe propose a novel bit-flipping procedure which enjoys high efficacy and local\noptimality guarantee. Our formulation supports a large family of empirical loss\nfunctions and is here instantiated by exponential / hinge losses. Comprehensive\nevaluations are conducted on several representative image benchmarks. The\nexperiments consistently observe reduced complexities of model training and\ndeployment, without sacrifice of accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:05:53 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Shen", "Fumin", ""], ["Mu", "Yadong", ""], ["Liu", "Wei", ""], ["Yang", "Yang", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1603.04117", "submitter": "Prateek  Singhal", "authors": "Prateek Singhal, Ruffin White, Henrik Christensen", "title": "Multi-modal Tracking for Object based SLAM", "comments": "Submitted to IROS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an on-line 3D visual object tracking framework for monocular\ncameras by incorporating spatial knowledge and uncertainty from semantic\nmapping along with high frequency measurements from visual odometry. Using a\ncombination of vision and odometry that are tightly integrated we can increase\nthe overall performance of object based tracking for semantic mapping. We\npresent a framework for integration of the two data-sources into a coherent\nframework through information based fusion/arbitration. We demonstrate the\nframework in the context of OmniMapper[1] and present results on 6 challenging\nsequences over multiple objects compared to data obtained from a motion capture\nsystems. We are able to achieve a mean error of 0.23m for per frame tracking\nshowing 9% relative error less than state of the art tracker.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:06:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Singhal", "Prateek", ""], ["White", "Ruffin", ""], ["Christensen", "Henrik", ""]]}, {"id": "1603.04132", "submitter": "Wenbo Dong", "authors": "Wenbo Dong, Volkan Isler", "title": "A Novel Method for the Extrinsic Calibration of a 2D Laser Rangefinder\n  and a Camera", "comments": "12 pages, 12 figures", "journal-ref": "IEEE Sensors Journal, 18(10): 4200-4211, 2018", "doi": "10.1109/JSEN.2018.2819082", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for extrinsically calibrating a camera and a 2D\nLaser Rangefinder (LRF) whose beams are invisible from the camera image. We\nshow that point-to-plane constraints from a single observation of a V-shaped\ncalibration pattern composed of two non-coplanar triangles suffice to uniquely\nconstrain the relative pose between two sensors. Next, we present an approach\nto obtain analytical solutions using point-to-plane constraints from single or\nmultiple observations. Along the way, we also show that previous solutions, in\ncontrast to our method, have inherent ambiguities and therefore must rely on a\ngood initial estimate. Real and synthetic experiments validate our method and\nshow that it achieves better accuracy than previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 04:15:54 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 02:33:51 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 21:59:52 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2018 23:21:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Dong", "Wenbo", ""], ["Isler", "Volkan", ""]]}, {"id": "1603.04134", "submitter": "Kanzhi Wu", "authors": "Kanzhi Wu and Xiaoyang Li and Ravindra Ranasinghe and Gamini\n  Dissanayake and Yong Liu", "title": "RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and\n  Shape Feature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel appearance and shape feature, RISAS, which is\nrobust to viewpoint, illumination, scale and rotation variations. RISAS\nconsists of a keypoint detector and a feature descriptor both of which utilise\ntexture and geometric information present in the appearance and shape channels.\nA novel response function based on the surface normals is used in combination\nwith the Harris corner detector for selecting keypoints in the scene. A\nstrategy that uses the depth information for scale estimation and background\nelimination is proposed to select the neighbourhood around the keypoints in\norder to build precise invariant descriptors. Proposed descriptor relies on the\nordering of both grayscale intensity and shape information in the\nneighbourhood. Comprehensive experiments which confirm the effectiveness of the\nproposed RGB-D feature when compared with CSHOT and LOIND are presented.\nFurthermore, we highlight the utility of incorporating texture and shape\ninformation in the design of both the detector and the descriptor by\ndemonstrating the enhanced performance of CSHOT and LOIND when combined with\nRISAS detector.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 04:39:49 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 09:09:37 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wu", "Kanzhi", ""], ["Li", "Xiaoyang", ""], ["Ranasinghe", "Ravindra", ""], ["Dissanayake", "Gamini", ""], ["Liu", "Yong", ""]]}, {"id": "1603.04139", "submitter": "Frank Nielsen", "authors": "Junlin Yao and Frank Nielsen", "title": "SSSC-AM: A Unified Framework for Video Co-Segmentation by Structured\n  Sparse Subspace Clustering with Appearance and Motion Features", "comments": "19 pages, 6 figures, 5 tables, extend ICIP 2016", "journal-ref": "IEEE International Conference on Image Processing (ICIP), 2016", "doi": "10.1109/ICIP.2016.7533102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video co-segmentation refers to the task of jointly segmenting common objects\nappearing in a given group of videos. In practice, high-dimensional data such\nas videos can be conceptually thought as being drawn from a union of subspaces\ncorresponding to categories rather than from a smooth manifold. Therefore,\nsegmenting data into respective subspaces --- subspace clustering --- finds\nwidespread applications in computer vision, including co-segmentation.\nState-of-the-art methods via subspace clustering seek to solve the problem in\ntwo steps:\n  First, an affinity matrix is built from data, with appearance features or\nmotion patterns. Second, the data are segmented by applying spectral clustering\nto the affinity matrix. However, this process is insufficient to obtain an\noptimal solution since it does not take into account the {\\em interdependence}\nof the affinity matrix with the segmentation. In this work, we present a novel\nunified video co-segmentation framework inspired by the recent Structured\nSparse Subspace Clustering ($\\mathrm{S^{3}C}$) based on the {\\em\nself-expressiveness} model. Our method yields more consistent segmentation\nresults. In order to improve the detectability of motion features with missing\ntrajectories due to occlusion or tracked points moving out of frames, we add an\nextra-dimensional signature to the motion trajectories. Moreover, we\nreformulate the $\\mathrm{S^{3}C}$ algorithm by adding the affine subspace\nconstraint in order to make it more suitable to segment rigid motions lying in\naffine subspaces of dimension at most $3$. Our experiments on MOViCS dataset\nshow that our framework achieves the highest overall performance among baseline\nalgorithms and demonstrate its robustness to heavy noise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 05:36:40 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 22:05:15 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Yao", "Junlin", ""], ["Nielsen", "Frank", ""]]}, {"id": "1603.04146", "submitter": "Shuhan Chen", "authors": "Shuhan Chen, Jindong Li, Xuelong Hu, Ping Zhou", "title": "Saliency Detection for Improving Object Proposals", "comments": "IEEE DSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposals greatly benefit object detection task in recent\nstate-of-the-art works. However, the existing object proposals usually have low\nlocalization accuracy at high intersection over union threshold. To address it,\nwe apply saliency detection to each bounding box to improve their quality in\nthis paper. We first present a geodesic saliency detection method in contour,\nwhich is designed to find closed contours. Then, we apply it to each candidate\nbox with multi-sizes, and refined boxes can be easily produced in the obtained\nsaliency maps which are further used to calculate saliency scores for proposal\nranking. Experiments on PASCAL VOC 2007 test dataset demonstrate the proposed\nrefinement approach can greatly improve existing models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 06:44:43 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 02:01:08 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 06:30:08 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Chen", "Shuhan", ""], ["Li", "Jindong", ""], ["Hu", "Xuelong", ""], ["Zhou", "Ping", ""]]}, {"id": "1603.04150", "submitter": "Sheng Huang", "authors": "Sheng Huang and Dan Yang and Bo Liu and Xiaohong Zhang", "title": "Regression-based Hypergraph Learning for Image Clustering and\n  Classification", "comments": "11pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inspired by the recently remarkable successes of Sparse Representation (SR),\nCollaborative Representation (CR) and sparse graph, we present a novel\nhypergraph model named Regression-based Hypergraph (RH) which utilizes the\nregression models to construct the high quality hypergraphs. Moreover, we plug\nRH into two conventional hypergraph learning frameworks, namely hypergraph\nspectral clustering and hypergraph transduction, to present Regression-based\nHypergraph Spectral Clustering (RHSC) and Regression-based Hypergraph\nTransduction (RHT) models for addressing the image clustering and\nclassification issues. Sparse Representation and Collaborative Representation\nare employed to instantiate two RH instances and their RHSC and RHT algorithms.\nThe experimental results on six popular image databases demonstrate that the\nproposed RH learning algorithms achieve promising image clustering and\nclassification performances, and also validate that RH can inherit the\ndesirable properties from both hypergraph models and regression models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 06:54:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Huang", "Sheng", ""], ["Yang", "Dan", ""], ["Liu", "Bo", ""], ["Zhang", "Xiaohong", ""]]}, {"id": "1603.04186", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Shimon Ullman", "title": "Visual Concept Recognition and Localization via Iterative Introspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been shown to develop internal\nrepresentations, which correspond closely to semantically meaningful objects\nand parts, although trained solely on class labels. Class Activation Mapping\n(CAM) is a recent method that makes it possible to easily highlight the image\nregions contributing to a network's classification decision. We build upon\nthese two developments to enable a network to re-examine informative image\nregions, which we term introspection. We propose a weakly-supervised iterative\nscheme, which shifts its center of attention to increasingly discriminative\nregions as it progresses, by alternating stages of classification and\nintrospection. We evaluate our method and show its effectiveness over a range\nof several datasets, where we obtain competitive or state-of-the-art results:\non Stanford-40 Actions, we set a new state-of the art of 81.74%. On\nFGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements\nover baselines, some of which include significantly more supervision.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:18:03 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 13:27:37 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.04203", "submitter": "Nauman Shahid", "authors": "Faisal Mahmood and Nauman Shahid and Pierre Vandergheynst and Ulf\n  Skoglund", "title": "Graph Based Sinogram Denoising for Tomographic Reconstructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited data and low dose constraints are common problems in a variety of\ntomographic reconstruction paradigms which lead to noisy and incomplete data.\nOver the past few years sinogram denoising has become an essential\npre-processing step for low dose Computed Tomographic (CT) reconstructions. We\npropose a novel sinogram denoising algorithm inspired by the modern field of\nsignal processing on graphs. Graph based methods often perform better than\nstandard filtering operations since they can exploit the signal structure. This\nmakes the sinogram an ideal candidate for graph based denoising since it\ngenerally has a piecewise smooth structure. We test our method with a variety\nof phantoms and different reconstruction methods. Our numerical study shows\nthat the proposed algorithm improves the performance of analytical filtered\nback-projection (FBP) and iterative methods ART (Kaczmarz) and SIRT\n(Cimmino).We observed that graph denoised sinogram always minimizes the error\nmeasure and improves the accuracy of the solution as compared to regular\nreconstructions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:58:23 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Mahmood", "Faisal", ""], ["Shahid", "Nauman", ""], ["Vandergheynst", "Pierre", ""], ["Skoglund", "Ulf", ""]]}, {"id": "1603.04223", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Gregory Cohen, Tara Julia Hamilton, Jonathan Tapson,\n  Andre van Schaik", "title": "Investigation of event-based memory surfaces for high-speed tracking,\n  unsupervised feature extraction and object recognition", "comments": "This is an updated version of a previously submitted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we compare event-based decaying and time based-decaying memory\nsurfaces for high-speed eventbased tracking, feature extraction, and object\nclassification using an event-based camera. The high-speed recognition task\ninvolves detecting and classifying model airplanes that are dropped free-hand\nclose to the camera lens so as to generate a challenging dataset exhibiting\nsignificant variance in target velocity. This variance motivated the\ninvestigation of event-based decaying memory surfaces in comparison to\ntime-based decaying memory surfaces to capture the temporal aspect of the\nevent-based data. These surfaces are then used to perform unsupervised feature\nextraction, tracking and recognition. In order to generate the memory surfaces,\nevent binning, linearly decaying kernels, and exponentially decaying kernels\nwere investigated with exponentially decaying kernels found to perform best.\nEvent-based decaying memory surfaces were found to outperform time-based\ndecaying memory surfaces in recognition especially when invariance to target\nvelocity was made a requirement. A range of network and receptive field sizes\nwere investigated. The system achieves 98.75% recognition accuracy within 156\nmilliseconds of an airplane entering the field of view, using only twenty-five\nevent-based feature extracting neurons in series with a linear classifier. By\ncomparing the linear classifier results to an ELM classifier, we find that a\nsmall number of event-based feature extractors can effectively project the\ncomplex spatio-temporal event patterns of the dataset to an almost linearly\nseparable representation in feature space.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 11:54:15 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 02:11:48 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 10:39:46 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Afshar", "Saeed", ""], ["Cohen", "Gregory", ""], ["Hamilton", "Tara Julia", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""]]}, {"id": "1603.04265", "submitter": "Tae Hyun Kim", "authors": "Tae Hyun Kim, Seungjun Nah, and Kyoung Mu Lee", "title": "Dynamic Scene Deblurring using a Locally Adaptive Linear Blur Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art video deblurring methods cannot handle blurry videos\nrecorded in dynamic scenes, since they are built under a strong assumption that\nthe captured scenes are static. Contrary to the existing methods, we propose a\nvideo deblurring algorithm that can deal with general blurs inherent in dynamic\nscenes. To handle general and locally varying blurs caused by various sources,\nsuch as moving objects, camera shake, depth variation, and defocus, we estimate\npixel-wise non-uniform blur kernels. We infer bidirectional optical flows to\nhandle motion blurs, and also estimate Gaussian blur maps to remove optical\nblur from defocus in our new blur model. Therefore, we propose a single energy\nmodel that jointly estimates optical flows, defocus blur maps and latent\nframes. We also provide a framework and efficient solvers to minimize the\nproposed energy model. By optimizing the energy model, we achieve significant\nimprovements in removing general blurs, estimating optical flows, and extending\ndepth-of-field in blurry frames. Moreover, in this work, to evaluate the\nperformance of non-uniform deblurring methods objectively, we have constructed\na new realistic dataset with ground truths. In addition, extensive experimental\non publicly available challenging video data demonstrate that the proposed\nmethod produces qualitatively superior performance than the state-of-the-art\nmethods which often fail in either deblurring or optical flow estimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 13:57:19 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Kim", "Tae Hyun", ""], ["Nah", "Seungjun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1603.04308", "submitter": "Anton Winschel", "authors": "Anton Winschel, Rainer Lienhart, Christian Eggert", "title": "Diversity in Object Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current top performing object recognition systems build on object proposals\nas a preprocessing step. Object proposal algorithms are designed to generate\ncandidate regions for generic objects, yet current approaches are limited in\ncapturing the vast variety of object characteristics. In this paper we analyze\nthe error modes of the state-of-the-art Selective Search object proposal\nalgorithm and suggest extensions to broaden its feature diversity in order to\nmitigate its error modes. We devise an edge grouping algorithm for handling\nobjects without clear boundaries. To further enhance diversity, we incorporate\nthe Edge Boxes proposal algorithm, which is based on fundamentally different\nprinciples than Selective Search. The combination of segmentations and edges\nprovides rich image information and feature diversity which is essential for\nobtaining high quality object proposals for generic objects. For a preset\namount of object proposals we achieve considerably better results by using our\ncombination of different strategies than using any single strategy alone.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 15:40:38 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Winschel", "Anton", ""], ["Lienhart", "Rainer", ""], ["Eggert", "Christian", ""]]}, {"id": "1603.04327", "submitter": "Ibrahim Sadek", "authors": "Ibrahim Sadek", "title": "Automatic Discrimination of Color Retinal Images using the Bag of Words\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) and age related macular degeneration (ARMD) are\namong the major causes of visual impairment worldwide. DR is mainly\ncharacterized by red spots, namely microaneurysms and bright lesions,\nspecifically exudates whereas ARMD is mainly identified by tiny yellow or white\ndeposits called drusen. Since exudates might be the only manifestation of the\nearly diabetic retinopathy, there is an increase demand for automatic\nretinopathy diagnosis. Exudates and drusen may share similar appearances, thus\ndiscriminating between them is of interest to enhance screening performance. In\nthis research, we investigative the role of bag of words approach in the\nautomatic diagnosis of retinopathy diabetes. We proposed to use a single based\nand multiple based methods for the construction of the visual dictionary by\ncombining the histogram of word occurrences from each dictionary and building a\nsingle histogram. The introduced approach is evaluated for automatic diagnosis\nof normal and abnormal color fundus images with bright lesions. This approach\nhas been implemented on 430 fundus images, including six publicly available\ndatasets, in addition to one local dataset. The mean accuracies reported are\n97.2% and 99.77% for single based and multiple based dictionaries respectively.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 16:26:32 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Sadek", "Ibrahim", ""]]}, {"id": "1603.04392", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Wei Ding and Caitlin Kuhlman and Aijun Chen and\n  Liping Di", "title": "Rapid building detection using machine learning", "comments": "Accepted to be published in Applied Intelligence 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes algorithms for performing discrete object detection,\nspecifically in the case of buildings, where usually only low quality RGB-only\ngeospatial reflective imagery is available. We utilize new candidate search and\nfeature extraction techniques to reduce the problem to a machine learning (ML)\nclassification task. Here we can harness the complex patterns of contrast\nfeatures contained in training data to establish a model of buildings. We avoid\ncostly sliding windows to generate candidates; instead we innovatively stitch\ntogether well known image processing techniques to produce candidates for\nbuilding detection that cover 80-85% of buildings. Reducing the number of\npossible candidates is important due to the scale of the problem. Each\ncandidate is subjected to classification which, although linear, costs time and\nprohibits large scale evaluation. We propose a candidate alignment algorithm to\nboost classification performance to 80-90% precision with a linear time\nalgorithm and show it has negligible cost. Also, we propose a new concept\ncalled a Permutable Haar Mesh (PHM) which we use to form and traverse a search\nspace to recover candidate buildings which were lost in the initial\npreprocessing phase.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:03:53 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Ding", "Wei", ""], ["Kuhlman", "Caitlin", ""], ["Chen", "Aijun", ""], ["Di", "Liping", ""]]}, {"id": "1603.04408", "submitter": "Alisher Abdulkhaev", "authors": "Alisher Abdulkhaev and Ozgur Yilmaz", "title": "U-CATCH: Using Color ATtribute of image patCHes in binary descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a simple yet very effective method for extracting\ncolor information through binary feature description framework. Our method\nexpands the dimension of binary comparisons into RGB and YCbCr spaces, showing\nmore than 100% matching improve ment compared to non-color binary descriptors\nfor a wide range of hard-to-match cases. The proposed method is general and can\nbe applied to any binary descriptor to make it color sensitive. It is faster\nthan classical binary descriptors for RGB sampling due to the abandonment of\ngrayscale conversion and has almost identical complexity (insignificant\ncompared to smoothing operation) for YCbCr sampling.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:32:57 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2016 17:59:20 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 05:17:34 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Abdulkhaev", "Alisher", ""], ["Yilmaz", "Ozgur", ""]]}, {"id": "1603.04525", "submitter": "Chunhua Shen", "authors": "Qichang Hu, Peng Wang, Chunhua Shen, Anton van den Hengel, Fatih\n  Porikli", "title": "Pushing the Limits of Deep CNNs for Pedestrian Detection", "comments": "Fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to other applications in computer vision, convolutional neural\nnetworks have under-performed on pedestrian detection. A breakthrough was made\nvery recently by using sophisticated deep CNN models, with a number of\nhand-crafted features, or explicit occlusion handling mechanism. In this work,\nwe show that by re-using the convolutional feature maps (CFMs) of a deep\nconvolutional neural network (DCNN) model as image features to train an\nensemble of boosted decision models, we are able to achieve the best reported\naccuracy without using specially designed learning algorithms. We empirically\nidentify and disclose important implementation details. We also show that pixel\nlabelling may be simply combined with a detector to boost the detection\nperformance. By adding complementary hand-crafted features such as optical\nflow, the DCNN based detector can be further improved. We set a new record on\nthe Caltech pedestrian dataset, lowering the log-average miss rate from\n$11.7\\%$ to $8.9\\%$, a relative improvement of $24\\%$. We also achieve a\ncomparable result to the state-of-the-art approaches on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 01:55:14 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 06:36:15 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Hu", "Qichang", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Porikli", "Fatih", ""]]}, {"id": "1603.04530", "submitter": "Jimei Yang", "authors": "Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang", "title": "Object Contour Detection with a Fully Convolutional Encoder-Decoder\n  Network", "comments": "Accepted by CVPR2016 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a deep learning algorithm for contour detection with a fully\nconvolutional encoder-decoder network. Different from previous low-level edge\ndetection, our algorithm focuses on detecting higher-level object contours. Our\nnetwork is trained end-to-end on PASCAL VOC with refined ground truth from\ninaccurate polygon annotations, yielding much higher precision in object\ncontour detection than previous methods. We find that the learned model\ngeneralizes well to unseen object classes from the same super-categories on MS\nCOCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning.\nBy combining with the multiscale combinatorial grouping algorithm, our method\ncan generate high-quality segmented object proposals, which significantly\nadvance the state-of-the-art on PASCAL VOC (improving average recall from 0.62\nto 0.67) with a relatively small amount of candidates ($\\sim$1660 per image).\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:11:49 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Yang", "Jimei", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Lee", "Honglak", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1603.04531", "submitter": "Haewoon Kwak", "authors": "Haewoon Kwak and Jisun An", "title": "Revealing the Hidden Patterns of News Photos: Analysis of Millions of\n  News Photos Using GDELT and Deep Learning-based Vision APIs", "comments": "Presented in the first workshop on NEws and publiC Opinion (NECO'16,\n  www.neco.io, colocated with ICWSM'16), Cologne, Germany, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze more than two million news photos published in\nJanuary 2016. We demonstrate i) which objects appear the most in news photos;\nii) what the sentiments of news photos are; iii) whether the sentiment of news\nphotos is aligned with the tone of the text; iv) how gender is treated; and v)\nhow differently political candidates are portrayed. To our best knowledge, this\nis the first large-scale study of news photo contents using deep learning-based\nvision APIs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:23:53 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 04:44:12 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kwak", "Haewoon", ""], ["An", "Jisun", ""]]}, {"id": "1603.04535", "submitter": "Ke Yan", "authors": "Ke Yan, Lu Kou, and David Zhang", "title": "Learning Domain-Invariant Subspace using Domain Features and\n  Independence Maximization", "comments": "Accepted", "journal-ref": null, "doi": "10.1109/TCYB.2016.2633306", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation algorithms are useful when the distributions of the\ntraining and the test data are different. In this paper, we focus on the\nproblem of instrumental variation and time-varying drift in the field of\nsensors and measurement, which can be viewed as discrete and continuous\ndistributional change in the feature space. We propose maximum independence\ndomain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this\nproblem. Domain features are first defined to describe the background\ninformation of a sample, such as the device label and acquisition time. Then,\nMIDA learns a subspace which has maximum independence with the domain features,\nso as to reduce the inter-domain discrepancy in distributions. A feature\naugmentation strategy is also designed to project samples according to their\nbackgrounds so as to improve the adaptation. The proposed algorithms are\nflexible and fast. Their effectiveness is verified by experiments on synthetic\ndatasets and four real-world ones on sensors, measurement, and computer vision.\nThey can greatly enhance the practicability of sensor systems, as well as\nextend the application scope of existing domain adaptation algorithms by\nuniformly handling different kinds of distributional change.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:56:22 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 01:39:22 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Yan", "Ke", ""], ["Kou", "Lu", ""], ["Zhang", "David", ""]]}, {"id": "1603.04550", "submitter": "Bat-Erdene Batsukh", "authors": "Bat-Erdene Batsukh, Ganbat Tsend", "title": "Effective Computer Model For Recognizing Nationality From Frontal Image", "comments": "5 pages in IJASCSE Volume 5, Issue 3 (March 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are introducing new effective computer model for extracting nationality\nfrom frontal image candidate using face part color, size and distances based on\ndeep research. Determining face part size, color, and distances is depending on\na variety of factors including image quality, lighting condition, rotation\nangle, occlusion and facial emotion. Therefore, first we need to detect a face\non the image then convert an image into the real input. After that, we can\ndetermine image candidate gender, face shape, key points and face parts.\nFinally, we will return the result, based on the comparison of sizes and\ndistances with the sample measurement table database. While we were measuring\nsamples, there were big differences between images by their gender and face\nshapes. Input images must be the frontal face image that has smooth lighting\nand does not have any rotation angle. The model can be used in military,\npolice, defense, healthcare, and technology sectors. Finally, Computer can\ndistinguish nationality from the face image.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:30:34 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Batsukh", "Bat-Erdene", ""], ["Tsend", "Ganbat", ""]]}, {"id": "1603.04588", "submitter": "Hawrenr Fang", "authors": "Hawren Fang", "title": "Classification with Repulsion Tensors: A Case Study on Face Recognition", "comments": "25 pages, 8 figures, 6 tables, unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dimensionality reduction methods for face recognition in a\nsupervised setting, using an image-as-matrix representation. A common procedure\nis to project image matrices into a smaller space in which the recognition is\nperformed. These methods are often called \"two-dimensional\" in the literature\nand there exist counterparts that use an image-as-vector representation. When\ntwo face images are close to each other in the input space they may remain\nclose after projection - but this is not desirable in the situation when these\ntwo images are from different classes, and this often affects the recognition\nperformance. We extend a previously developed `repulsion Laplacean' technique\nbased on adding terms to the objective function with the goal or creation a\nrepulsion energy between such images in the projected space. This scheme, which\nrelies on a repulsion graph, is generic and can be incorporated into various\ntwo-dimensional methods. It can be regarded as a multilinear generalization of\nthe repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp.\n2392--2402]. Experimental results demonstrate that the proposed methodology\noffers significant recognition improvement relative to the underlying\ntwo-dimensional methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 08:31:04 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Fang", "Hawren", ""]]}, {"id": "1603.04595", "submitter": "Olivier Mor\\`ere", "authors": "Olivier Mor\\`ere, Jie Lin, Antoine Veillard, Vijay Chandrasekhar,\n  Tomaso Poggio", "title": "Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval", "comments": "Image Instance Retrieval, CNN, Invariant Representation, Hashing,\n  Unsupervised Learning, Regularization. arXiv admin note: text overlap with\n  arXiv:1601.02093", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is the computation of very compact binary hashes for\nimage instance retrieval. Our approach has two novel contributions. The first\none is Nested Invariance Pooling (NIP), a method inspired from i-theory, a\nmathematical theory for computing group invariant transformations with\nfeed-forward neural networks. NIP is able to produce compact and\nwell-performing descriptors with visual representations extracted from\nconvolutional neural networks. We specifically incorporate scale, translation\nand rotation invariances but the scheme can be extended to any arbitrary sets\nof transformations. We also show that using moments of increasing order\nthroughout nesting is important. The NIP descriptors are then hashed to the\ntarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novel\nbatch-level regularization scheme specifically designed for the purpose of\nhashing (RBMH). A thorough empirical evaluation with state-of-the-art shows\nthat the results obtained both with the NIP descriptors and the NIP+RBMH hashes\nare consistently outstanding across a wide range of datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 08:56:33 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 14:11:18 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Mor\u00e8re", "Olivier", ""], ["Lin", "Jie", ""], ["Veillard", "Antoine", ""], ["Chandrasekhar", "Vijay", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1603.04614", "submitter": "Qingqun Ning", "authors": "Qingqun Ning, Jianke Zhu, Zhiyuan Zhong, Steven C.H. Hoi, Chun Chen", "title": "Scalable Image Retrieval by Sparse Product Quantization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional\nfeature indexing and retrieval is the crux of large-scale image retrieval. A\nrecent promising technique is Product Quantization, which attempts to index\nhigh-dimensional image features by decomposing the feature space into a\nCartesian product of low dimensional subspaces and quantizing each of them\nseparately. Despite the promising results reported, their quantization approach\nfollows the typical hard assignment of traditional quantization methods, which\nmay result in large quantization errors and thus inferior search performance.\nUnlike the existing approaches, in this paper, we propose a novel approach\ncalled Sparse Product Quantization (SPQ) to encoding the high-dimensional\nfeature vectors into sparse representation. We optimize the sparse\nrepresentations of the feature vectors by minimizing their quantization errors,\nmaking the resulting representation is essentially close to the original data\nin practice. Experiments show that the proposed SPQ technique is not only able\nto compress data, but also an effective encoding technique. We obtain\nstate-of-the-art results for ANN search on four public image datasets and the\npromising results of content-based image retrieval further validate the\nefficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 09:53:32 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Ning", "Qingqun", ""], ["Zhu", "Jianke", ""], ["Zhong", "Zhiyuan", ""], ["Hoi", "Steven C. H.", ""], ["Chen", "Chun", ""]]}, {"id": "1603.04619", "submitter": "Chunhua Shen", "authors": "Yao Li, Linqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "Image Co-localization by Mimicking a Good Detector's Confidence Score\n  Distribution", "comments": "Accepted to Proc. European Conf. Computer Vision 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of images containing objects from the same category, the task of\nimage co-localization is to identify and localize each instance. This paper\nshows that this problem can be solved by a simple but intriguing idea, that is,\na common object detector can be learnt by making its detection confidence\nscores distributed like those of a strongly supervised detector. More\nspecifically, we observe that given a set of object proposals extracted from an\nimage that contains the object of interest, an accurate strongly supervised\nobject detector should give high scores to only a small minority of proposals,\nand low scores to most of them. Thus, we devise an entropy-based objective\nfunction to enforce the above property when learning the common object\ndetector. Once the detector is learnt, we resort to a segmentation approach to\nrefine the localization. We show that despite its simplicity, our approach\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 10:21:47 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 11:09:05 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Li", "Yao", ""], ["Liu", "Linqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1603.04713", "submitter": "Wenjie Pei", "authors": "Wenjie Pei and David M.J. Tax and Laurens van der Maaten", "title": "Modeling Time Series Similarity with Siamese Recurrent Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional techniques for measuring similarities between time series are\nbased on handcrafted similarity measures, whereas more recent learning-based\napproaches cannot exploit external supervision. We combine ideas from\ntime-series modeling and metric learning, and study siamese recurrent networks\n(SRNs) that minimize a classification loss to learn a good similarity measure\nbetween time series. Specifically, our approach learns a vectorial\nrepresentation for each time series in such a way that similar time series are\nmodeled by similar representations, and dissimilar time series by dissimilar\nrepresentations. Because it is a similarity prediction models, SRNs are\nparticularly well-suited to challenging scenarios such as signature\nrecognition, in which each person is a separate class and very few examples per\nclass are available. We demonstrate the potential merits of SRNs in\nwithin-domain and out-of-domain classification experiments and in one-shot\nlearning experiments on tasks such as signature, voice, and sign language\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 14:57:44 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Pei", "Wenjie", ""], ["Tax", "David M. J.", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1603.04746", "submitter": "Liheng Bian", "authors": "Liheng Bian, Jinli Suo, Jaebum Chung, Xiaoze Ou, Changhuei Yang, Feng\n  Chen and Qionghai Dai", "title": "Fourier ptychographic reconstruction using Poisson maximum likelihood\n  and truncated Wirtinger gradient", "comments": null, "journal-ref": null, "doi": "10.1038/srep27384", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier ptychographic microscopy (FPM) is a novel computational coherent\nimaging technique for high space-bandwidth product imaging. Mathematically,\nFourier ptychographic (FP) reconstruction can be implemented as a phase\nretrieval optimization process, in which we only obtain low resolution\nintensity images corresponding to the sub-bands of the sample's high resolution\n(HR) spatial spectrum, and aim to retrieve the complex HR spectrum. In real\nsetups, the measurements always suffer from various degenerations such as\nGaussian noise, Poisson noise, speckle noise and pupil location error, which\nwould largely degrade the reconstruction. To efficiently address these\ndegenerations, we propose a novel FP reconstruction method under a gradient\ndescent optimization framework in this paper. The technique utilizes Poisson\nmaximum likelihood for better signal modeling, and truncated Wirtinger gradient\nfor error removal. Results on both simulated data and real data captured using\nour laser FPM setup show that the proposed method outperforms other\nstate-of-the-art algorithms. Also, we have released our source code for\nnon-commercial use.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 21:05:48 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Bian", "Liheng", ""], ["Suo", "Jinli", ""], ["Chung", "Jaebum", ""], ["Ou", "Xiaoze", ""], ["Yang", "Changhuei", ""], ["Chen", "Feng", ""], ["Dai", "Qionghai", ""]]}, {"id": "1603.04771", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti", "title": "A Neural Approach to Blind Motion Deblurring", "comments": "ECCV 2016. Project page at http://www.ttic.edu/chakrabarti/ndeblur", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for blind motion deblurring that uses a neural\nnetwork trained to compute estimates of sharp image patches from observations\nthat are blurred by an unknown motion kernel. Instead of regressing directly to\npatch intensities, this network learns to predict the complex Fourier\ncoefficients of a deconvolution filter to be applied to the input patch for\nrestoration. For inference, we apply the network independently to all\noverlapping patches in the observed image, and average its outputs to form an\ninitial estimate of the sharp image. We then explicitly estimate a single\nglobal blur kernel by relating this estimate to the observed image, and finally\nperform non-blind deconvolution with this kernel. Our method exhibits accuracy\nand robustness close to state-of-the-art iterative methods, while being much\nfaster when parallelized on GPU hardware.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 17:35:26 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 18:53:00 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Chakrabarti", "Ayan", ""]]}, {"id": "1603.04779", "submitter": "Yanghao Li", "authors": "Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou", "title": "Revisiting Batch Normalization For Practical Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have shown unprecedented success in various\ncomputer vision applications such as image classification and object detection.\nHowever, it is still a common annoyance during the training phase, that one has\nto prepare at least thousands of labeled images to fine-tune a network to a\nspecific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong\ndependency towards the training dataset, and the learned features cannot be\neasily transferred to a different but relevant task without fine-tuning. In\nthis paper, we propose a simple yet powerful remedy, called Adaptive Batch\nNormalization (AdaBN) to increase the generalization ability of a DNN. By\nmodulating the statistics in all Batch Normalization layers across the network,\nour approach achieves deep adaptation effect for domain adaptation tasks. In\ncontrary to other deep learning domain adaptation methods, our method does not\nrequire additional components, and is parameter-free. It archives\nstate-of-the-art performance despite its surprising simplicity. Furthermore, we\ndemonstrate that our method is complementary with other existing methods.\nCombining AdaBN with existing domain adaptation treatments may further improve\nmodel performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 17:44:32 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 03:57:19 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 08:41:43 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 06:11:30 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Li", "Yanghao", ""], ["Wang", "Naiyan", ""], ["Shi", "Jianping", ""], ["Liu", "Jiaying", ""], ["Hou", "Xiaodi", ""]]}, {"id": "1603.04833", "submitter": "Anirban Santara", "authors": "Debapriya Maji, Anirban Santara, Pabitra Mitra and Debdoot Sheet", "title": "Ensemble of Deep Convolutional Neural Networks for Learning to Detect\n  Retinal Vessels in Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision impairment due to pathological damage of the retina can largely be\nprevented through periodic screening using fundus color imaging. However the\nchallenge with large scale screening is the inability to exhaustively detect\nfine blood vessels crucial to disease diagnosis. In this work we present a\ncomputational imaging framework using deep and ensemble learning for reliable\ndetection of blood vessels in fundus color images. An ensemble of deep\nconvolutional neural networks is trained to segment vessel and non-vessel areas\nof a color fundus image. During inference, the responses of the individual\nConvNets of the ensemble are averaged to form the final segmentation. In\nexperimental evaluation with the DRIVE database, we achieve the objective of\nvessel detection with maximum average accuracy of 94.7\\% and area under ROC\ncurve of 0.9283.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 19:40:34 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Maji", "Debapriya", ""], ["Santara", "Anirban", ""], ["Mitra", "Pabitra", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1603.04838", "submitter": "Yongchao Xu", "authors": "Yongchao Xu (LRDE, LIGM, TSI), Thierry G\\'eraud (LRDE), Laurent Najman\n  (LIGM)", "title": "Hierarchical image simplification and segmentation based on\n  Mumford-Shah-salient level line selection", "comments": "Pattern Recognition Letters, Elsevier, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies, such as the tree of shapes, are popular representations for\nimage simplification and segmentation thanks to their multiscale structures.\nSelecting meaningful level lines (boundaries of shapes) yields to simplify\nimage while preserving intact salient structures. Many image simplification and\nsegmentation methods are driven by the optimization of an energy functional,\nfor instance the celebrated Mumford-Shah functional. In this paper, we propose\nan efficient approach to hierarchical image simplification and segmentation\nbased on the minimization of the piecewise-constant Mumford-Shah functional.\nThis method conforms to the current trend that consists in producing\nhierarchical results rather than a unique partition. Contrary to classical\napproaches which compute optimal hierarchical segmentations from an input\nhierarchy of segmentations, we rely on the tree of shapes, a unique and\nwell-defined representation equivalent to the image. Simply put, we compute for\neach level line of the image an attribute function that characterizes its\npersistence under the energy minimization. Then we stack the level lines from\nmeaningless ones to salient ones through a saliency map based on extinction\nvalues defined on the tree-based shape space. Qualitative illustrations and\nquantitative evaluation on Weizmann segmentation evaluation database\ndemonstrate the state-of-the-art performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 19:51:10 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 14:38:04 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Xu", "Yongchao", "", "LRDE, LIGM, TSI"], ["G\u00e9raud", "Thierry", "", "LRDE"], ["Najman", "Laurent", "", "LIGM"]]}, {"id": "1603.04868", "submitter": "Julian Straub", "authors": "Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III", "title": "Efficient Global Point Cloud Alignment using Bayesian Nonparametric\n  Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud alignment is a common problem in computer vision and robotics,\nwith applications ranging from 3D object recognition to reconstruction. We\npropose a novel approach to the alignment problem that utilizes Bayesian\nnonparametrics to describe the point cloud and surface normal densities, and\nbranch and bound (BB) optimization to recover the relative transformation. BB\nuses a novel, refinable, near-uniform tessellation of rotation space using 4D\ntetrahedra, leading to more efficient optimization compared to the common\naxis-angle tessellation. We provide objective function bounds for pruning given\nthe proposed tessellation, and prove that BB converges to the optimum of the\ncost function along with providing its computational complexity. Finally, we\nempirically demonstrate the efficiency of the proposed approach as well as its\nrobustness to real-world conditions such as missing data and partial overlap.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 20:02:38 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 16:20:14 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 03:46:37 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Straub", "Julian", ""], ["Campbell", "Trevor", ""], ["How", "Jonathan P.", ""], ["Fisher", "John W.", "III"]]}, {"id": "1603.04871", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan, Hao Zhang, Yangqing Jia, Thomas Breuel, Yizhou Yu", "title": "Combining the Best of Convolutional Layers and Recurrent Layers: A\n  Hybrid Network for Semantic Segmentation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art results of semantic segmentation are established by Fully\nConvolutional neural Networks (FCNs). FCNs rely on cascaded convolutional and\npooling layers to gradually enlarge the receptive fields of neurons, resulting\nin an indirect way of modeling the distant contextual dependence. In this work,\nwe advocate the use of spatially recurrent layers (i.e. ReNet layers) which\ndirectly capture global contexts and lead to improved feature representations.\nWe demonstrate the effectiveness of ReNet layers by building a Naive deep ReNet\n(N-ReNet), which achieves competitive performance on Stanford Background\ndataset. Furthermore, we integrate ReNet layers with FCNs, and develop a novel\nHybrid deep ReNet (H-ReNet). It enjoys a few remarkable properties, including\nfull-image receptive fields, end-to-end training, and efficient network\nexecution. On the PASCAL VOC 2012 benchmark, the H-ReNet improves the results\nof state-of-the-art approaches Piecewise, CRFasRNN and DeepParsing by 3.6%,\n2.3% and 0.2%, respectively, and achieves the highest IoUs for 13 out of the 20\nobject classes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 20:10:48 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Yan", "Zhicheng", ""], ["Zhang", "Hao", ""], ["Jia", "Yangqing", ""], ["Breuel", "Thomas", ""], ["Yu", "Yizhou", ""]]}, {"id": "1603.04908", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Hyun Soo Park, Stella X. Yu, and Jianbo Shi", "title": "First Person Action-Object Detection with EgoNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional third-person cameras mounted on robots, a first-person\ncamera, captures a person's visual sensorimotor object interactions from up\nclose. In this paper, we study the tight interplay between our momentary visual\nattention and motor action with objects from a first-person camera. We propose\na concept of action-objects---the objects that capture person's conscious\nvisual (watching a TV) or tactile (taking a cup) interactions. Action-objects\nmay be task-dependent but since many tasks share common person-object spatial\nconfigurations, action-objects exhibit a characteristic 3D spatial distance and\norientation with respect to the person.\n  We design a predictive model that detects action-objects using EgoNet, a\njoint two-stream network that holistically integrates visual appearance (RGB)\nand 3D spatial layout (depth and height) cues to predict per-pixel likelihood\nof action-objects. Our network also incorporates a first-person coordinate\nembedding, which is designed to learn a spatial distribution of the\naction-objects in the first-person data. We demonstrate EgoNet's predictive\npower, by showing that it consistently outperforms previous baseline\napproaches. Furthermore, EgoNet also exhibits a strong generalization ability,\ni.e., it predicts semantically meaningful objects in novel first-person\ndatasets. Our method's ability to effectively detect action-objects could be\nused to improve robots' understanding of human-object interactions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 22:29:03 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 16:59:28 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 18:04:17 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Bertasius", "Gedas", ""], ["Park", "Hyun Soo", ""], ["Yu", "Stella X.", ""], ["Shi", "Jianbo", ""]]}, {"id": "1603.04922", "submitter": "Yinda Zhang", "authors": "Yinda Zhang, Mingru Bai, Pushmeet Kohli, Shahram Izadi, Jianxiong Xiao", "title": "DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene\n  Understanding", "comments": "Accepted by ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have led to human-level performance on computer\nvision tasks, they have yet to demonstrate similar gains for holistic scene\nunderstanding. In particular, 3D context has been shown to be an extremely\nimportant cue for scene understanding - yet very little research has been done\non integrating context information with deep models. This paper presents an\napproach to embed 3D context into the topology of a neural network trained to\nperform holistic scene understanding. Given a depth image depicting a 3D scene,\nour network aligns the observed scene with a predefined 3D scene template, and\nthen reasons about the existence and location of each object within the scene\ntemplate. In doing so, our model recognizes multiple objects in a single\nforward pass of a 3D convolutional neural network, capturing both global scene\nand local object information simultaneously. To create training data for this\n3D network, we generate partly hallucinated depth images which are rendered by\nreplacing real objects with a repository of CAD models of the same object\ncategory. Extensive experiments demonstrate the effectiveness of our algorithm\ncompared to the state-of-the-arts. Source code and data are available at\nhttp://deepcontext.cs.princeton.edu.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 00:09:41 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 06:46:42 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 15:34:03 GMT"}, {"version": "v4", "created": "Wed, 16 Aug 2017 04:12:50 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zhang", "Yinda", ""], ["Bai", "Mingru", ""], ["Kohli", "Pushmeet", ""], ["Izadi", "Shahram", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1603.04930", "submitter": "Michael Iliadis", "authors": "Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos", "title": "Deep Fully-Connected Networks for Video Compressive Sensing", "comments": "14 pages, to appear in Elsevier Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a deep learning framework for video compressive\nsensing. The proposed formulation enables recovery of video frames in a few\nseconds at significantly improved reconstruction quality compared to previous\napproaches. Our investigation starts by learning a linear mapping between video\nsequences and corresponding measured frames which turns out to provide\npromising results. We then extend the linear formulation to deep\nfully-connected networks and explore the performance gains using deeper\narchitectures. Our analysis is always driven by the applicability of the\nproposed framework on existing compressive video architectures. Extensive\nsimulations on several video sequences document the superiority of our approach\nboth quantitatively and qualitatively. Finally, our analysis offers insights\ninto understanding how dataset sizes and number of layers affect reconstruction\nperformance while raising a few points for future investigation.\n  Code is available at Github: https://github.com/miliadis/DeepVideoCS\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 01:15:35 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 23:26:43 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Iliadis", "Michael", ""], ["Spinoulas", "Leonidas", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1603.04992", "submitter": "Ravi Garg", "authors": "Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, Ian Reid", "title": "Unsupervised CNN for Single View Depth Estimation: Geometry to the\n  Rescue", "comments": "Accepted for publication at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant weakness of most current deep Convolutional Neural Networks is\nthe need to train them using vast amounts of manu- ally labelled data. In this\nwork we propose a unsupervised framework to learn a deep convolutional neural\nnetwork for single view depth predic- tion, without requiring a pre-training\nstage or annotated ground truth depths. We achieve this by training the network\nin a manner analogous to an autoencoder. At training time we consider a pair of\nimages, source and target, with small, known camera motion between the two such\nas a stereo pair. We train the convolutional encoder for the task of predicting\nthe depth map for the source image. To do so, we explicitly generate an inverse\nwarp of the target image using the predicted depth and known inter-view\ndisplacement, to reconstruct the source image; the photomet- ric error in the\nreconstruction is the reconstruction loss for the encoder. The acquisition of\nthis training data is considerably simpler than for equivalent systems,\nrequiring no manual annotation, nor calibration of depth sensor to camera. We\nshow that our network trained on less than half of the KITTI dataset (without\nany further augmentation) gives com- parable performance to that of the state\nof art supervised methods for single view depth estimation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 08:57:15 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 03:20:46 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Garg", "Ravi", ""], ["BG", "Vijay Kumar", ""], ["Carneiro", "Gustavo", ""], ["Reid", "Ian", ""]]}, {"id": "1603.05015", "submitter": "Ravi Garg", "authors": "Ravi Garg, Anders Eriksson and Ian Reid", "title": "Non-linear Dimensionality Regularizer for Solving Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an ill-posed inverse problem of estimating causal factors from\nobservations, one of which is known to lie near some (un- known)\nlow-dimensional, non-linear manifold expressed by a predefined Mercer-kernel.\nSolving this problem requires simultaneous estimation of these factors and\nlearning the low-dimensional representation for them. In this work, we\nintroduce a novel non-linear dimensionality regulariza- tion technique for\nsolving such problems without pre-training. We re-formulate Kernel-PCA as an\nenergy minimization problem in which low dimensionality constraints are\nintroduced as regularization terms in the energy. To the best of our knowledge,\nours is the first at- tempt to create a dimensionality regularizer in the KPCA\nframework. Our approach relies on robustly penalizing the rank of the recovered\nfac- tors directly in the implicit feature space to create their\nlow-dimensional approximations in closed form. Our approach performs robust\nKPCA in the presence of missing data and noise. We demonstrate state-of-the-art\nresults on predicting missing entries in the standard oil flow dataset.\nAdditionally, we evaluate our method on the challenging problem of Non-Rigid\nStructure from Motion and our approach delivers promising results on CMU mocap\ndataset despite the presence of significant occlusions and noise.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 10:04:38 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Garg", "Ravi", ""], ["Eriksson", "Anders", ""], ["Reid", "Ian", ""]]}, {"id": "1603.05027", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Identity Mappings in Deep Residual Networks", "comments": "ECCV 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 10:53:56 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 09:40:08 GMT"}, {"version": "v3", "created": "Mon, 25 Jul 2016 15:18:32 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1603.05073", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Warren Rieutort-Louis, Ognjen Arandjelovic", "title": "Descriptor transition tables for object retrieval using unconstrained\n  cluttered video acquired using a consumer level handheld mobile device", "comments": "2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition and vision based retrieval of objects from large databases\nare tasks with a wide spectrum of potential applications. In this paper we\npropose a novel recognition method from video sequences suitable for retrieval\nfrom databases acquired in highly unconstrained conditions e.g. using a mobile\nconsumer-level device such as a phone. On the lowest level, we represent each\nsequence as a 3D mesh of densely packed local appearance descriptors. While\nimage plane geometry is captured implicitly by a large overlap of neighbouring\nregions from which the descriptors are extracted, 3D information is extracted\nby means of a descriptor transition table, learnt from a single sequence for\neach known gallery object. These allow us to connect local descriptors along\nthe 3rd dimension (which corresponds to viewpoint changes), thus resulting in a\nset of variable length Markov chains for each video. The matching of two sets\nof such chains is formulated as a statistical hypothesis test, whereby a subset\nof each is chosen to maximize the likelihood that the corresponding video\nsequences show the same object. The effectiveness of the proposed algorithm is\nempirically evaluated on the Amsterdam Library of Object Images and a new\nhighly challenging video data set acquired using a mobile phone. On both data\nsets our method is shown to be successful in recognition in the presence of\nbackground clutter and large viewpoint changes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 12:57:10 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 12:02:40 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Rieutort-Louis", "Warren", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "1603.05145", "submitter": "Qiyang Zhao", "authors": "Qiyang Zhao, Lewis D Griffin", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\n  Functions", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\nadversarial samples obtained by imperceptible perturbations of clean samples.\nWe hypothesize that this is caused by a failure to suppress unusual signals\nwithin network layers. As remedy we propose the use of Symmetric Activation\nFunctions (SAF) in non-linear signal transducer units. These units suppress\nsignals of exceptional magnitude. We prove that SAF networks can perform\nclassification tasks to arbitrary precision in a simplified situation. In\npractice, rather than use SAFs alone, we add them into CNNs to improve their\nrobustness. The modified CNNs can be easily trained using popular strategies\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\nthat the modified CNNs perform similarly to plain ones on clean samples, and\nare remarkably more robust against adversarial and nonsense samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 15:35:07 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Zhao", "Qiyang", ""], ["Griffin", "Lewis D", ""]]}, {"id": "1603.05154", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, M\\\"art Toots, Lars-G\\\"oran \\\"Ofverstedt and Ulf\n  Skoglund", "title": "2D Discrete Fourier Transform with Simultaneous Edge Artifact Removal\n  for Real-Time Applications", "comments": "IEEE 2015 International Conference on Field Programmable Technology\n  (FPT), Queenstown, New Zealand", "journal-ref": null, "doi": "10.1109/FPT.2015.7393157", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic and\ncomputationally intensive algorithm, with a vast variety of applications. 2D\nimages are, in general, non-periodic, but are assumed to be periodic while\ncalculating their DFTs. This leads to cross-shaped artifacts in the frequency\ndomain due to spectral leakage. These artifacts can have critical consequences\nif the DFTs are being used for further processing. In this paper we present a\nnovel FPGA-based design to calculate high-throughput 2D DFTs with simultaneous\nedge artifact removal. Standard approaches for removing these artifacts using\napodization functions or mirroring, either involve removing critical\nfrequencies or a surge in computation by increasing image size. We use a\nperiodic-plus-smooth decomposition based artifact removal algorithm optimized\nfor FPGA implementation, while still achieving real-time ($\\ge$23 frames per\nsecond) performance for a 512$\\times$512 size image stream. Our optimization\napproach leads to a significant decrease in external memory utilization thereby\navoiding memory conflicts and simplifies the design. We have tested our design\non a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC which\ngives us the advantage to further expand the design for industrial\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 15:52:13 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Mahmood", "Faisal", ""], ["Toots", "M\u00e4rt", ""], ["\u00d6fverstedt", "Lars-G\u00f6ran", ""], ["Skoglund", "Ulf", ""]]}, {"id": "1603.05201", "submitter": "Wenling Shang", "authors": "Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee", "title": "Understanding and Improving Convolutional Neural Networks via\n  Concatenated Rectified Linear Units", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have been used as a powerful\ntool to solve many problems of machine learning and computer vision. In this\npaper, we aim to provide insight on the property of convolutional neural\nnetworks, as well as a generic method to improve the performance of many CNN\narchitectures. Specifically, we first examine existing CNN models and observe\nan intriguing property that the filters in the lower layers form pairs (i.e.,\nfilters with opposite phase). Inspired by our observation, we propose a novel,\nsimple yet effective activation scheme called concatenated ReLU (CRelu) and\ntheoretically analyze its reconstruction property in CNNs. We integrate CRelu\ninto several state-of-the-art CNN architectures and demonstrate improvement in\ntheir recognition performance on CIFAR-10/100 and ImageNet datasets with fewer\ntrainable parameters. Our results suggest that better understanding of the\nproperties of CNNs can lead to significant performance improvement with a\nsimple modification.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 18:17:36 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 05:18:36 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Shang", "Wenling", ""], ["Sohn", "Kihyuk", ""], ["Almeida", "Diogo", ""], ["Lee", "Honglak", ""]]}, {"id": "1603.05279", "submitter": "Joseph Redmon", "authors": "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two efficient approximations to standard convolutional neural\nnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,\nthe filters are approximated with binary values resulting in 32x memory saving.\nIn XNOR-Networks, both the filters and the input to convolutional layers are\nbinary. XNOR-Networks approximate convolutions using primarily binary\noperations. This results in 58x faster convolutional operations and 32x memory\nsavings. XNOR-Nets offer the possibility of running state-of-the-art networks\non CPUs (rather than GPUs) in real-time. Our binary networks are simple,\naccurate, efficient, and work on challenging visual tasks. We evaluate our\napproach on the ImageNet classification task. The classification accuracy with\na Binary-Weight-Network version of AlexNet is only 2.9% less than the\nfull-precision AlexNet (in top-1 measure). We compare our method with recent\nnetwork binarization methods, BinaryConnect and BinaryNets, and outperform\nthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 20:56:21 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 22:56:17 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 22:13:54 GMT"}, {"version": "v4", "created": "Tue, 2 Aug 2016 20:59:14 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Rastegari", "Mohammad", ""], ["Ordonez", "Vicente", ""], ["Redmon", "Joseph", ""], ["Farhadi", "Ali", ""]]}, {"id": "1603.05285", "submitter": "Christoph Schn\\\"orr", "authors": "Freddie {\\AA}str\\\"om, Stefania Petra, Bernhard Schmitzer, Christoph\n  Schn\\\"orr", "title": "Image Labeling by Assignment", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0702-4", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel geometric approach to the image labeling problem.\nAbstracting from specific labeling applications, a general objective function\nis defined on a manifold of stochastic matrices, whose elements assign prior\ndata that are given in any metric space, to observed image measurements. The\ncorresponding Riemannian gradient flow entails a set of replicator equations,\none for each data point, that are spatially coupled by geometric averaging on\nthe manifold. Starting from uniform assignments at the barycenter as natural\ninitialization, the flow terminates at some global maximum, each of which\ncorresponds to an image labeling that uniquely assigns the prior data. Our\ngeometric variational approach constitutes a smooth non-convex inner\napproximation of the general image labeling problem, implemented with sparse\ninterior-point numerics in terms of parallel multiplicative updates that\nconverge efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 21:15:49 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["\u00c5str\u00f6m", "Freddie", ""], ["Petra", "Stefania", ""], ["Schmitzer", "Bernhard", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1603.05310", "submitter": "Vinay Venkataraman", "authors": "Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, and Pavan Turaga", "title": "Persistent Homology of Attractors For Action Recognition", "comments": "5 pages, Under review in International Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework for dynamical analysis of human\nactions from 3D motion capture data using topological data analysis. We model\nhuman actions using the topological features of the attractor of the dynamical\nsystem. We reconstruct the phase-space of time series corresponding to actions\nusing time-delay embedding, and compute the persistent homology of the\nphase-space reconstruction. In order to better represent the topological\nproperties of the phase-space, we incorporate the temporal adjacency\ninformation when computing the homology groups. The persistence of these\nhomology groups encoded using persistence diagrams are used as features for the\nactions. Our experiments with action recognition using these features\ndemonstrate that the proposed approach outperforms other baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 23:15:50 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Venkataraman", "Vinay", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Turaga", "Pavan", ""]]}, {"id": "1603.05335", "submitter": "Delu Zeng", "authors": "Tong Zhao, Lin Li, Xinghao Ding, Yue Huang and Delu Zeng", "title": "Saliency Detection with Spaces of Background-based Distribution", "comments": "5 pages, 6 figures, Accepted by IEEE Signal Processing Letters in\n  March 2016", "journal-ref": null, "doi": "10.1109/LSP.2016.2544781", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, an effective image saliency detection method is proposed by\nconstructing some novel spaces to model the background and redefine the\ndistance of the salient patches away from the background. Concretely, given the\nbackgroundness prior, eigendecomposition is utilized to create four spaces of\nbackground-based distribution (SBD) to model the background, in which a more\nappropriate metric (Mahalanobis distance) is quoted to delicately measure the\nsaliency of every image patch away from the background. After that, a coarse\nsaliency map is obtained by integrating the four adjusted Mahalanobis distance\nmaps, each of which is formed by the distances between all the patches and\nbackground in the corresponding SBD. To be more discriminative, the coarse\nsaliency map is further enhanced into the posterior probability map within\nBayesian perspective. Finally, the final saliency map is generated by properly\nrefining the posterior probability map with geodesic distance. Experimental\nresults on two usual datasets show that the proposed method is effective\ncompared with the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 02:18:30 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Zhao", "Tong", ""], ["Li", "Lin", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Zeng", "Delu", ""]]}, {"id": "1603.05414", "submitter": "Honghai Yu", "authors": "Honghai Yu, Pierre Moulin, Hong Wei Ng, Xiaoli Li", "title": "Variable-Length Hashing", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has emerged as a popular technique for large-scale similarity search.\nMost learning-based hashing methods generate compact yet correlated hash codes.\nHowever, this redundancy is storage-inefficient. Hence we propose a lossless\nvariable-length hashing (VLH) method that is both storage- and\nsearch-efficient. Storage efficiency is achieved by converting the fixed-length\nhash code into a variable-length code. Search efficiency is obtained by using a\nmultiple hash table structure. With VLH, we are able to deliberately add\nredundancy into hash codes to improve retrieval performance with little\nsacrifice in storage efficiency or search complexity. In particular, we propose\na block K-means hashing (B-KMH) method to obtain significantly improved\nretrieval performance with no increase in storage and marginal increase in\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 10:19:50 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Yu", "Honghai", ""], ["Moulin", "Pierre", ""], ["Ng", "Hong Wei", ""], ["Li", "Xiaoli", ""]]}, {"id": "1603.05474", "submitter": "Jiaolong Yang Dr.", "authors": "Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen,\n  Hongdong Li, Gang Hua", "title": "Neural Aggregation Network for Video Face Recognition", "comments": "Post CVPR2017 version with minor typo fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Neural Aggregation Network (NAN) for video face\nrecognition. The network takes a face video or face image set of a person with\na variable number of face images as its input, and produces a compact,\nfixed-dimension feature representation for recognition. The whole network is\ncomposed of two modules. The feature embedding module is a deep Convolutional\nNeural Network (CNN) which maps each face image to a feature vector. The\naggregation module consists of two attention blocks which adaptively aggregate\nthe feature vectors to form a single feature inside the convex hull spanned by\nthem. Due to the attention mechanism, the aggregation is invariant to the image\norder. Our NAN is trained with a standard classification or verification loss\nwithout any extra supervision signal, and we found that it automatically learns\nto advocate high-quality face images while repelling low-quality ones such as\nblurred, occluded and improperly exposed faces. The experiments on IJB-A,\nYouTube Face, Celebrity-1000 video face recognition benchmarks show that it\nconsistently outperforms naive aggregation methods and achieves the\nstate-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 13:30:45 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 12:38:10 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 06:02:06 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 08:08:14 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Yang", "Jiaolong", ""], ["Ren", "Peiran", ""], ["Zhang", "Dongqing", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Li", "Hongdong", ""], ["Hua", "Gang", ""]]}, {"id": "1603.05522", "submitter": "Sumeetpal S Singh", "authors": "Lan Jiang, Sumeetpal S. Singh", "title": "Tracking multiple moving objects in images using Markov Chain Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Bayesian state and parameter learning algorithm for multiple target\ntracking (MTT) models with image observations is proposed. Specifically, a\nMarkov chain Monte Carlo algorithm is designed to sample from the posterior\ndistribution of the unknown number of targets, their birth and death times,\nstates and model parameters, which constitutes the complete solution to the\ntracking problem. The conventional approach is to pre-process the images to\nextract point observations and then perform tracking. We model the image\ngeneration process directly to avoid potential loss of information when\nextracting point observations. Numerical examples show that our algorithm has\nimproved tracking performance over commonly used techniques, for both synthetic\nexamples and real florescent microscopy data, especially in the case of dim\ntargets with overlapping illuminated regions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 15:03:10 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1603.05600", "submitter": "Roozbeh Mottaghi", "authors": "Roozbeh Mottaghi, Mohammad Rastegari, Abhinav Gupta, Ali Farhadi", "title": "\"What happens if...\" Learning to Predict the Effect of Forces in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What happens if one pushes a cup sitting on a table toward the edge of the\ntable? How about pushing a desk against a wall? In this paper, we study the\nproblem of understanding the movements of objects as a result of applying\nexternal forces to them. For a given force vector applied to a specific\nlocation in an image, our goal is to predict long-term sequential movements\ncaused by that force. Doing so entails reasoning about scene geometry, objects,\ntheir attributes, and the physical rules that govern the movements of objects.\nWe design a deep neural network model that learns long-term sequential\ndependencies of object movements while taking into account the geometry and\nappearance of the scene by combining Convolutional and Recurrent Neural\nNetworks. Training our model requires a large-scale dataset of object movements\ncaused by external forces. To build a dataset of forces in scenes, we\nreconstructed all images in SUN RGB-D dataset in a physics simulator to\nestimate the physical movements of objects caused by external forces applied to\nthem. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which a\nvariety of external forces are applied to different types of objects resulting\nin more than 65,000 object movements represented in 3D. Our experimental\nevaluations show that the challenging task of predicting long-term movements of\nobjects as their reaction to external forces is possible from a single image.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 18:12:33 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Mottaghi", "Roozbeh", ""], ["Rastegari", "Mohammad", ""], ["Gupta", "Abhinav", ""], ["Farhadi", "Ali", ""]]}, {"id": "1603.05631", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Abhinav Gupta", "title": "Generative Image Modeling using Style and Structure Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current generative frameworks use end-to-end learning and generate images by\nsampling from uniform noise distribution. However, these approaches ignore the\nmost basic principle of image formation: images are product of: (a) Structure:\nthe underlying 3D model; (b) Style: the texture mapped onto structure. In this\npaper, we factorize the image generation process and propose Style and\nStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two\ncomponents: the Structure-GAN generates a surface normal map; the Style-GAN\ntakes the surface normal map as input and generates the 2D image. Apart from a\nreal vs. generated loss function, we use an additional loss with computed\nsurface normals from generated images. The two GANs are first trained\nindependently, and then merged together via joint learning. We show our S^2-GAN\nmodel is interpretable, generates more realistic images and can be used to\nlearn unsupervised RGBD representations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:33:20 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 03:54:23 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Wang", "Xiaolong", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1603.05763", "submitter": "Boshra Rajaei", "authors": "Boshra Rajaei, Rafael Grompone von Gioi, Jean-Michel Morel", "title": "From line segments to more organized Gestalts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reconsider the early computer vision bottom-up program,\naccording to which higher level features (geometric structures) in an image\ncould be built up recursively from elementary features by simple grouping\nprinciples coming from Gestalt theory. Taking advantage of the (recent)\nadvances in reliable line segment detectors, we propose three feature detectors\nthat constitute one step up in this bottom up pyramid. For any digital image,\nour unsupervised algorithm computes three classic Gestalts from the set of\npredetected line segments: good continuations, nonlocal alignments, and bars.\nThe methodology is based on a common stochastic {\\it a contrario model}\nyielding three simple detection formulas, characterized by their number of\nfalse alarms. This detection algorithm is illustrated on several digital\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 04:05:35 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Rajaei", "Boshra", ""], ["von Gioi", "Rafael Grompone", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "1603.05772", "submitter": "Matthias Nie{\\ss}ner", "authors": "Julien Valentin and Angela Dai and Matthias Nie{\\ss}ner and Pushmeet\n  Kohli and Philip Torr and Shahram Izadi and Cem Keskin", "title": "Learning to Navigate the Energy Landscape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel and efficient architecture for addressing\ncomputer vision problems that use `Analysis by Synthesis'. Analysis by\nsynthesis involves the minimization of the reconstruction error which is\ntypically a non-convex function of the latent target variables.\nState-of-the-art methods adopt a hybrid scheme where discriminatively trained\npredictors like Random Forests or Convolutional Neural Networks are used to\ninitialize local search algorithms. While these methods have been shown to\nproduce promising results, they often get stuck in local optima. Our method\ngoes beyond the conventional hybrid architecture by not only proposing multiple\naccurate initial solutions but by also defining a navigational structure over\nthe solution space that can be used for extremely efficient gradient-free local\nsearch. We demonstrate the efficacy of our approach on the challenging problem\nof RGB Camera Relocalization. To make the RGB camera relocalization problem\nparticularly challenging, we introduce a new dataset of 3D environments which\nare significantly larger than those found in other publicly-available datasets.\nOur experiments reveal that the proposed method is able to achieve\nstate-of-the-art camera relocalization results. We also demonstrate the\ngeneralizability of our approach on Hand Pose Estimation and Image Retrieval\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 05:45:39 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Valentin", "Julien", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip", ""], ["Izadi", "Shahram", ""], ["Keskin", "Cem", ""]]}, {"id": "1603.05782", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang and Alex Yong-Sang Chia", "title": "Unsupervised Cross-Media Hashing with Structure Preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the exponential growth of heterogeneous multimedia\ndata. The need for effective and accurate data retrieval from heterogeneous\ndata sources has attracted much research interest in cross-media retrieval.\nHere, given a query of any media type, cross-media retrieval seeks to find\nrelevant results of different media types from heterogeneous data sources. To\nfacilitate large-scale cross-media retrieval, we propose a novel unsupervised\ncross-media hashing method. Our method incorporates local affinity and distance\nrepulsion constraints into a matrix factorization framework. Correspondingly,\nthe proposed method learns hash functions that generates unified hash codes\nfrom different media types, while ensuring intrinsic geometric structure of the\ndata distribution is preserved. These hash codes empower the similarity between\ndata of different media types to be evaluated directly. Experimental results on\ntwo large-scale multimedia datasets demonstrate the effectiveness of the\nproposed method, where we outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 07:10:35 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Wang", "Xiangyu", ""], ["Chia", "Alex Yong-Sang", ""]]}, {"id": "1603.05835", "submitter": "Hendrik Dirks", "authors": "Hendrik Dirks", "title": "A Flexible Primal-Dual Toolbox", "comments": "10 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convex\nvariational problems in image processing and beyond. Such problems often\nconsist of non-differentiable parts and involve linear operators. The toolbox\nuses a primal-dual scheme to avoid (computationally) inefficient operator\ninversion and to get reliable error estimates. From the user-side,\n\\textbf{FlexBox} expects the primal formulation of the problem, automatically\ndecouples operators and dualizes the problem. For large-scale problems,\n\\textbf{FlexBox} also comes with a \\cpp-module, which can be used stand-alone\nor together with MATLAB via MEX-interfaces. Besides various pre-implemented\ndata-fidelities and regularization-terms, \\textbf{FlexBox} is able to handle\narbitrary operators while being easily extendable, due to its object-oriented\ndesign. The toolbox is available at\n\\href{http://www.flexbox.im}{http://www.flexbox.im}\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 11:01:23 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 13:11:14 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Dirks", "Hendrik", ""]]}, {"id": "1603.05875", "submitter": "Salehe Erfanian Ebadi", "authors": "Salehe Erfanian Ebadi, Valia Guerra Ones, Ebroul Izquierdo", "title": "Approximated Robust Principal Component Analysis for Improved General\n  Scene Background Subtraction", "comments": "arXiv admin note: text overlap with arXiv:1511.01245 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research reported in this paper addresses the fundamental task of\nseparation of locally moving or deforming image areas from a static or globally\nmoving background. It builds on the latest developments in the field of robust\nprincipal component analysis, specifically, the recently reported practical\nsolutions for the long-standing problem of recovering the low-rank and sparse\nparts of a large matrix made up of the sum of these two components. This\narticle addresses a few critical issues including: embedding global motion\nparameters in the matrix decomposition model, i.e., estimation of global motion\nparameters simultaneously with the foreground/background separation task,\nconsidering matrix block-sparsity rather than generic matrix sparsity as\nnatural feature in video processing applications, attenuating background\nghosting effects when foreground is subtracted, and more critically providing\nan extremely efficient algorithm to solve the low-rank/sparse matrix\ndecomposition task. The first aspect is important for background/foreground\nseparation in generic video sequences where the background usually obeys global\ndisplacements originated by the camera motion in the capturing process. The\nsecond aspect exploits the fact that in video processing applications the\nsparse matrix has a very particular structure, where the non-zero matrix\nentries are not randomly distributed but they build small blocks within the\nsparse matrix. The next feature of the proposed approach addresses removal of\nghosting effects originated from foreground silhouettes and the lack of\ninformation in the occluded background regions of the image. Finally, the\nproposed model also tackles algorithmic complexity by introducing an extremely\nefficient \"SVD-free\" technique that can be applied in most\nbackground/foreground separation tasks for conventional video processing.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 13:53:26 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Ebadi", "Salehe Erfanian", ""], ["Ones", "Valia Guerra", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1603.05930", "submitter": "Dawei Du", "authors": "Dawei Du and Honggang Qi and Longyin Wen and Qi Tian and Qingming\n  Huang and Siwei Lyu", "title": "Geometric Hypergraph Learning for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph based representation is widely used in visual tracking field by finding\ncorrect correspondences between target parts in consecutive frames. However,\nmost graph based trackers consider pairwise geometric relations between local\nparts. They do not make full use of the target's intrinsic structure, thereby\nmaking the representation easily disturbed by errors in pairwise affinities\nwhen large deformation and occlusion occur. In this paper, we propose a\ngeometric hypergraph learning based tracking method, which fully exploits\nhigh-order geometric relations among multiple correspondences of parts in\nconsecutive frames. Then visual tracking is formulated as the mode-seeking\nproblem on the hypergraph in which vertices represent correspondence hypotheses\nand hyperedges describe high-order geometric relations. Besides, a\nconfidence-aware sampling method is developed to select representative vertices\nand hyperedges to construct the geometric hypergraph for more robustness and\nscalability. The experiments are carried out on two challenging datasets\n(VOT2014 and Deform-SOT) to demonstrate that the proposed method performs\nfavorable against other existing trackers.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 17:32:30 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Du", "Dawei", ""], ["Qi", "Honggang", ""], ["Wen", "Longyin", ""], ["Tian", "Qi", ""], ["Huang", "Qingming", ""], ["Lyu", "Siwei", ""]]}, {"id": "1603.05955", "submitter": "Yin Yin", "authors": "Yin Yin, Sergei V. Fotin, Hrishikesh Haldankar, Jeffrey W.\n  Hoffmeister, and Senthil Periaswamy", "title": "Transferring Learned Microcalcification Group Detection from 2D\n  Mammography to 3D Digital Breast Tomosynthesis Using a Hierarchical Model and\n  Scope-based Normalization Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel hierarchical model is introduced to solve a general problem of\ndetecting groups of similar objects. Under this model, detection of groups is\nperformed in hierarchically organized layers while each layer represents a\nscope for target objects. The processing of these layers involves sequential\nextraction of appearance features for an individual object, consistency\nmeasurement features for nearby objects, and finally the distribution features\nfor all objects within the group. Using the concept of scope-based\nnormalization, the extracted features not only enhance local contrast of an\nindividual object, but also provide consistent characterization for all related\nobjects. As an example, a microcalcification group detection system for 2D\nmammography was developed, and then the learned model was transferred to 3D\ndigital breast tomosynthesis without any retraining or fine-tuning. The\ndetection system demonstrated state-of-the-art performance and detected 96% of\ncancerous lesions at the rate of 1.2 false positives per volume as measured on\nan independent tomosynthesis test set.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 18:46:36 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Yin", "Yin", ""], ["Fotin", "Sergei V.", ""], ["Haldankar", "Hrishikesh", ""], ["Hoffmeister", "Jeffrey W.", ""], ["Periaswamy", "Senthil", ""]]}, {"id": "1603.05959", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Christian Ledig, Virginia F.J. Newcombe,\n  Joanna P. Simpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben\n  Glocker", "title": "Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain\n  Lesion Segmentation", "comments": "This version was accepted in the journal Medical Image Analysis\n  (MedIA)", "journal-ref": null, "doi": "10.1016/j.media.2016.10.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dual pathway, 11-layers deep, three-dimensional Convolutional\nNeural Network for the challenging task of brain lesion segmentation. The\ndevised architecture is the result of an in-depth analysis of the limitations\nof current networks proposed for similar applications. To overcome the\ncomputational burden of processing 3D medical scans, we have devised an\nefficient and effective dense training scheme which joins the processing of\nadjacent image patches into one pass through the network while automatically\nadapting to the inherent class imbalance present in the data. Further, we\nanalyze the development of deeper, thus more discriminative 3D CNNs. In order\nto incorporate both local and larger contextual information, we employ a dual\npathway architecture that processes the input images at multiple scales\nsimultaneously. For post-processing of the network's soft segmentation, we use\na 3D fully connected Conditional Random Field which effectively removes false\npositives. Our pipeline is extensively evaluated on three challenging tasks of\nlesion segmentation in multi-channel MRI patient data with traumatic brain\ninjuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art\nfor all three applications, with top ranking performance on the public\nbenchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient,\nwhich allows its adoption in a variety of research and clinical settings. The\nsource code of our implementation is made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 19:07:01 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 19:59:22 GMT"}, {"version": "v3", "created": "Sun, 8 Jan 2017 13:55:35 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Ledig", "Christian", ""], ["Newcombe", "Virginia F. J.", ""], ["Simpson", "Joanna P.", ""], ["Kane", "Andrew D.", ""], ["Menon", "David K.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1603.06015", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Epameinondas Antonakos, Patrick Snape, Akshay\n  Asthana and Stefanos Zafeiriou", "title": "A Comprehensive Performance Evaluation of Deformable Face Tracking\n  \"In-the-Wild\"", "comments": "E. Antonakos and P. Snape contributed equally and have joint second\n  authorship", "journal-ref": null, "doi": "10.1007/s11263-017-0999-5", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, technologies such as face detection, facial landmark localisation\nand face recognition and verification have matured enough to provide effective\nand efficient solutions for imagery captured under arbitrary conditions\n(referred to as \"in-the-wild\"). This is partially attributed to the fact that\ncomprehensive \"in-the-wild\" benchmarks have been developed for face detection,\nlandmark localisation and recognition/verification. A very important technology\nthat has not been thoroughly evaluated yet is deformable face tracking\n\"in-the-wild\". Until now, the performance has mainly been assessed\nqualitatively by visually assessing the result of a deformable face tracking\ntechnology on short videos. In this paper, we perform the first, to the best of\nour knowledge, thorough evaluation of state-of-the-art deformable face tracking\npipelines using the recently introduced 300VW benchmark. We evaluate many\ndifferent architectures focusing mainly on the task of on-line deformable face\ntracking. In particular, we compare the following general strategies: (a)\ngeneric face detection plus generic facial landmark localisation, (b) generic\nmodel free tracking plus generic facial landmark localisation, as well as (c)\nhybrid approaches using state-of-the-art face detection, model free tracking\nand facial landmark localisation technologies. Our evaluation reveals future\navenues for further research on the topic.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 23:17:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 22:23:40 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Antonakos", "Epameinondas", ""], ["Snape", "Patrick", ""], ["Asthana", "Akshay", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1603.06028", "submitter": "Ethan Rudd", "authors": "Ethan M. Rudd, Andras Rozsa, Manuel G\\\"unther, Terrance E. Boult", "title": "A Survey of Stealth Malware: Attacks, Mitigation Measures, and Steps\n  Toward Autonomous Open World Solutions", "comments": "Pre-Print of a manuscript Accepted to IEEE Communications Surveys and\n  Tutorials (COMST) on December 1, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As our professional, social, and financial existences become increasingly\ndigitized and as our government, healthcare, and military infrastructures rely\nmore on computer technologies, they present larger and more lucrative targets\nfor malware. Stealth malware in particular poses an increased threat because it\nis specifically designed to evade detection mechanisms, spreading dormant, in\nthe wild for extended periods of time, gathering sensitive information or\npositioning itself for a high-impact zero-day attack. Policing the growing\nattack surface requires the development of efficient anti-malware solutions\nwith improved generalization to detect novel types of malware and resolve these\noccurrences with as little burden on human experts as possible. In this paper,\nwe survey malicious stealth technologies as well as existing solutions for\ndetecting and categorizing these countermeasures autonomously. While machine\nlearning offers promising potential for increasingly autonomous solutions with\nimproved generalization to new malware types, both at the network level and at\nthe host level, our findings suggest that several flawed assumptions inherent\nto most recognition algorithms prevent a direct mapping between the stealth\nmalware recognition problem and a machine learning solution. The most notable\nof these flawed assumptions is the closed world assumption: that no sample\nbelonging to a class outside of a static training set will appear at query\ntime. We present a formalized adaptive open world framework for stealth malware\nrecognition and relate it mathematically to research from other machine\nlearning domains.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 01:23:45 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 19:00:50 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Rudd", "Ethan M.", ""], ["Rozsa", "Andras", ""], ["G\u00fcnther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1603.06036", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Junchi Yan and Nils Persson and Weiyao Lin and\n  Hongyuan Zha", "title": "Fractal Dimension Invariant Filtering and Its CNN-based Implementation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractal analysis has been widely used in computer vision, especially in\ntexture image processing and texture analysis. The key concept of fractal-based\nimage model is the fractal dimension, which is invariant to bi-Lipschitz\ntransformation of image, and thus capable of representing intrinsic structural\ninformation of image robustly. However, the invariance of fractal dimension\ngenerally does not hold after filtering, which limits the application of\nfractal-based image model. In this paper, we propose a novel fractal dimension\ninvariant filtering (FDIF) method, extending the invariance of fractal\ndimension to filtering operations. Utilizing the notion of local\nself-similarity, we first develop a local fractal model for images. By adding a\nnonlinear post-processing step behind anisotropic filter banks, we demonstrate\nthat the proposed filtering method is capable of preserving the local\ninvariance of the fractal dimension of image. Meanwhile, we show that the FDIF\nmethod can be re-instantiated approximately via a CNN-based architecture, where\nthe convolution layer extracts anisotropic structure of image and the nonlinear\nlayer enhances the structure via preserving local fractal dimension of image.\nThe proposed filtering method provides us with a novel geometric interpretation\nof CNN-based image model. Focusing on a challenging image processing task ---\ndetecting complicated curves from the texture-like images, the proposed method\nobtains superior results to the state-of-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:29:57 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 05:03:45 GMT"}, {"version": "v3", "created": "Fri, 17 Mar 2017 03:10:56 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Xu", "Hongteng", ""], ["Yan", "Junchi", ""], ["Persson", "Nils", ""], ["Lin", "Weiyao", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1603.06041", "submitter": "Gucan Long", "authors": "Gucan Long, Laurent Kneip, Jose M. Alvarez, Hongdong Li", "title": "Learning Image Matching by Simply Watching Video", "comments": "The second version contains additional quantitative evaluation of\n  frame interpolation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an unsupervised learning based approach to the ubiquitous\ncomputer vision problem of image matching. We start from the insight that the\nproblem of frame-interpolation implicitly solves for inter-frame\ncorrespondences. This permits the application of analysis-by-synthesis: we\nfirstly train and apply a Convolutional Neural Network for frame-interpolation,\nthen obtain correspondences by inverting the learned CNN. The key benefit\nbehind this strategy is that the CNN for frame-interpolation can be trained in\nan unsupervised manner by exploiting the temporal coherency that is naturally\ncontained in real-world video sequences. The present model therefore learns\nimage matching by simply watching videos. Besides a promise to be more\ngenerally applicable, the presented approach achieves surprising performance\ncomparable to traditional empirically designed methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:45:44 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 04:35:49 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Long", "Gucan", ""], ["Kneip", "Laurent", ""], ["Alvarez", "Jose M.", ""], ["Li", "Hongdong", ""]]}, {"id": "1603.06059", "submitter": "Nasrin Mostafazadeh", "authors": "Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell,\n  Xiaodong He, Lucy Vanderwende", "title": "Generating Natural Questions About an Image", "comments": "Proceedings of the 54th Annual Meeting of the Association for\n  Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of work in the vision & language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision & language.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 07:27:15 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 06:54:58 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 01:20:49 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Mostafazadeh", "Nasrin", ""], ["Misra", "Ishan", ""], ["Devlin", "Jacob", ""], ["Mitchell", "Margaret", ""], ["He", "Xiaodong", ""], ["Vanderwende", "Lucy", ""]]}, {"id": "1603.06060", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy and Debdoot Sheet", "title": "DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout", "comments": "Accepted at Asian Conference on Pattern Recognition 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation deals with adapting behaviour of machine learning based\nsystems trained using samples in source domain to their deployment in target\ndomain where the statistics of samples in both domains are dissimilar. The task\nof directly training or adapting a learner in the target domain is challenged\nby lack of abundant labeled samples. In this paper we propose a technique for\ndomain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN)\nperformed in two stages: (i) unsupervised weight adaptation using systematic\ndropouts in mini-batch training, (ii) supervised fine-tuning with limited\nnumber of labeled samples in target domain. We experimentally evaluate\nperformance in the problem of retinal vessel segmentation where the SAE-DNN is\ntrained using large number of labeled samples in the source domain (DRIVE\ndataset) and adapted using less number of labeled samples in target domain\n(STARE dataset). The performance of SAE-DNN measured using $logloss$ in source\ndomain is $0.19$, without and with adaptation are $0.40$ and $0.18$, and $0.39$\nwhen trained exclusively with limited samples in target domain. The area under\nROC curve is observed respectively as $0.90$, $0.86$, $0.92$ and $0.87$. The\nhigh efficiency of vessel segmentation with DASA strongly substantiates our\nclaim.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 07:27:56 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1603.06093", "submitter": "Sergei Fedorov", "authors": "Sergei Fedorov, Olga Kacher", "title": "Large scale near-duplicate image retrieval using Triples of Adjacent\n  Ranked Features (TARF) with embedded geometric information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches to large-scale image retrieval are based on the construction\nof the inverted index of local image descriptors or visual words. A search in\nsuch an index usually results in a large number of candidates. This list of\ncandidates is then re-ranked with the help of a geometric verification, using a\nRANSAC algorithm, for example. In this paper we propose a feature\nrepresentation, which is built as a combination of three local descriptors. It\nallows one to significantly decrease the number of false matches and to shorten\nthe list of candidates after the initial search in the inverted index. This\ncombination of local descriptors is both reproducible and highly\ndiscriminative, and thus can be efficiently used for large-scale near-duplicate\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 13:36:02 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Fedorov", "Sergei", ""], ["Kacher", "Olga", ""]]}, {"id": "1603.06098", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov and Christoph H. Lampert", "title": "Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image\n  Segmentation", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new loss function for the weakly-supervised training of\nsemantic image segmentation models based on three guiding principles: to seed\nwith weak localization cues, to expand objects based on the information about\nwhich classes can occur in an image, and to constrain the segmentations to\ncoincide with object boundaries. We show experimentally that training a deep\nconvolutional neural network using the proposed loss function leads to\nsubstantially better segmentations than previous state-of-the-art methods on\nthe challenging PASCAL VOC 2012 dataset. We furthermore give insight into the\nworking mechanism of our method by a detailed experimental study that\nillustrates how the segmentation quality is affected by each term of the\nproposed loss function as well as their combinations.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 14:13:42 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 17:36:50 GMT"}, {"version": "v3", "created": "Sat, 6 Aug 2016 18:49:45 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1603.06121", "submitter": "Alina Zare", "authors": "Matthew Cook, Alina Zare, Dominic Ho", "title": "Buried object detection using handheld WEMI with task-driven extended\n  functions of multiple instances", "comments": "Proceedings of the SPIE, 2016", "journal-ref": null, "doi": "10.1117/12.2223349", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many effective supervised discriminative dictionary learning methods have\nbeen developed in the literature. However, when training these algorithms,\nprecise ground-truth of the training data is required to provide very accurate\npoint-wise labels. Yet, in many applications, accurate labels are not always\nfeasible. This is especially true in the case of buried object detection in\nwhich the size of the objects are not consistent. In this paper, a new multiple\ninstance dictionary learning algorithm for detecting buried objects using a\nhandheld WEMI sensor is detailed. The new algorithm, Task Driven Extended\nFunctions of Multiple Instances, can overcome data that does not have very\nprecise point-wise labels and still learn a highly discriminative dictionary.\nResults are presented and discussed on measured WEMI data.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 17:46:45 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Cook", "Matthew", ""], ["Zare", "Alina", ""], ["Ho", "Dominic", ""]]}, {"id": "1603.06140", "submitter": "Brendan Alvey", "authors": "Brendan Alvey, Alina Zare, Matthew Cook, Dominic K. Ho", "title": "Adaptive coherence estimator (ACE) for explosive hazard detection using\n  wideband electromagnetic induction (WEMI)", "comments": "Proceedings of the SPIE, 2016, Corrected reference formatting and\n  figure", "journal-ref": null, "doi": "10.1117/12.2223347", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive coherence estimator (ACE) estimates the squared cosine of the\nangle between a known target vector and a sample vector in a whitened\ncoordinate space. The space is whitened according to an estimation of the\nbackground statistics, which directly effects the performance of the statistic\nas a target detector. In this paper, the ACE detection statistic is used to\ndetect buried explosive hazards with data from a Wideband Electromagnetic\nInduction (WEMI) sensor. Target signatures are based on a dictionary defined\nusing a Discrete Spectrum of Relaxation Frequencies (DSRF) model. Results are\nsummarized as a receiver operator curve (ROC) and compared to other leading\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 20:33:50 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 00:17:05 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 23:38:01 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Alvey", "Brendan", ""], ["Zare", "Alina", ""], ["Cook", "Matthew", ""], ["Ho", "Dominic K.", ""]]}, {"id": "1603.06169", "submitter": "Alexander G\\'omez Villa", "authors": "Alexander Gomez, Augusto Salazar and Francisco Vargas", "title": "Towards Automatic Wild Animal Monitoring: Identification of Animal\n  Species in Camera-trap Images using Very Deep Convolutional Neural Networks", "comments": "Submitted to ECCV16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non intrusive monitoring of animals in the wild is possible using camera\ntrapping framework, which uses cameras triggered by sensors to take a burst of\nimages of animals in their habitat. However camera trapping framework produces\na high volume of data (in the order on thousands or millions of images), which\nmust be analyzed by a human expert. In this work, a method for animal species\nidentification in the wild using very deep convolutional neural networks is\npresented. Multiple versions of the Snapshot Serengeti dataset were used in\norder to probe the ability of the method to cope with different challenges that\ncamera-trap images demand. The method reached 88.9% of accuracy in Top-1 and\n98.1% in Top-5 in the evaluation set using a residual network topology. Also,\nthe results show that the proposed method outperforms previous approximations\nand proves that recognition in camera-trap images can be automated.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 00:47:46 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 00:53:37 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Gomez", "Alexander", ""], ["Salazar", "Augusto", ""], ["Vargas", "Francisco", ""]]}, {"id": "1603.06180", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Marcus Rohrbach, Trevor Darrell", "title": "Segmentation from Natural Language Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we approach the novel problem of segmenting an image based on a\nnatural language expression. This is different from traditional semantic\nsegmentation over a predefined set of semantic classes, as e.g., the phrase\n\"two men sitting on the right bench\" requires segmenting only the two people on\nthe right bench and no one standing or sitting on another bench. Previous\napproaches suitable for this task were limited to a fixed set of categories\nand/or rectangular regions. To produce pixelwise segmentation for the language\nexpression, we propose an end-to-end trainable recurrent and convolutional\nnetwork model that jointly learns to process visual and linguistic information.\nIn our model, a recurrent LSTM network is used to encode the referential\nexpression into a vector representation, and a fully convolutional network is\nused to a extract a spatial feature map from the image and output a spatial\nresponse map for the target object. We demonstrate on a benchmark dataset that\nour model can produce quality segmentation output from the natural language\nexpression, and outperforms baseline methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 04:10:53 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Hu", "Ronghang", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""]]}, {"id": "1603.06182", "submitter": "Haimin Zhang", "authors": "Haimin Zhang", "title": "Modelling Temporal Information Using Discrete Fourier Transform for\n  Video Classification", "comments": "to be revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, video classification attracts intensive research efforts. However,\nmost existing works are based on framelevel visual features, which might fail\nto model the temporal information, e.g. characteristics accumulated along time.\nIn order to capture video temporal information, we propose to analyse features\nin frequency domain transformed by discrete Fourier transform (DFT features).\nFrame-level features are firstly extract by a pre-trained deep convolutional\nneural network (CNN). Then, time domain features are transformed and\ninterpolated into DFT features. CNN and DFT features are further encoded by\nusing different pooling methods and fused for video classification. In this\nway, static image features extracted from a pre-trained deep CNN and temporal\ninformation represented by DFT features are jointly considered for video\nclassification. We test our method for video emotion classification and action\nrecognition. Experimental results demonstrate that combining DFT features can\neffectively capture temporal information and therefore improve the performance\nof both video emotion classification and action recognition. Our approach has\nachieved a state-of-the-art performance on the largest video emotion dataset\n(VideoEmotion-8 dataset) and competitive results on UCF-101.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 04:28:21 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 00:42:37 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 07:29:40 GMT"}, {"version": "v4", "created": "Thu, 21 Jul 2016 01:17:17 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 00:48:55 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Zhang", "Haimin", ""]]}, {"id": "1603.06201", "submitter": "Gong Cheng", "authors": "Gong Cheng, Junwei Han", "title": "A Survey on Object Detection in Optical Remote Sensing Images", "comments": "This manuscript is the accepted version for ISPRS Journal of\n  Photogrammetry and Remote Sensing", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 117: 11-28,\n  2016", "doi": "10.1016/j.isprsjprs.2016.03.014", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection in optical remote sensing images, being a fundamental but\nchallenging problem in the field of aerial and satellite image analysis, plays\nan important role for a wide range of applications and is receiving significant\nattention in recent years. While enormous methods exist, a deep review of the\nliterature concerning generic object detection is still lacking. This paper\naims to provide a review of the recent progress in this field. Different from\nseveral previously published surveys that focus on a specific object class such\nas building and road, we concentrate on more generic object categories\nincluding, but are not limited to, road, building, tree, vehicle, ship,\nairport, urban-area. Covering about 270 publications we survey 1) template\nmatching-based object detection methods, 2) knowledge-based object detection\nmethods, 3) object-based image analysis (OBIA)-based object detection methods,\n4) machine learning-based object detection methods, and 5) five publicly\navailable datasets and three standard evaluation metrics. We also discuss the\nchallenges of current studies and propose two promising research directions,\nnamely deep learning-based feature representation and weakly supervised\nlearning-based geospatial object detection. It is our hope that this survey\nwill be beneficial for the researchers to have better understanding of this\nresearch field.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 11:09:30 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 03:13:29 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Cheng", "Gong", ""], ["Han", "Junwei", ""]]}, {"id": "1603.06208", "submitter": "Asako Kanezaki", "authors": "Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida", "title": "RotationNet: Joint Object Categorization and Pose Estimation Using\n  Multiviews from Unsupervised Viewpoints", "comments": "24 pages, 23 figures. Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Convolutional Neural Network (CNN)-based model \"RotationNet,\"\nwhich takes multi-view images of an object as input and jointly estimates its\npose and object category. Unlike previous approaches that use known viewpoint\nlabels for training, our method treats the viewpoint labels as latent\nvariables, which are learned in an unsupervised manner during the training\nusing an unaligned object dataset. RotationNet is designed to use only a\npartial set of multi-view images for inference, and this property makes it\nuseful in practical scenarios where only partial views are available. Moreover,\nour pose alignment strategy enables one to obtain view-specific feature\nrepresentations shared across classes, which is important to maintain high\naccuracy in both object categorization and pose estimation. Effectiveness of\nRotationNet is demonstrated by its superior performance to the state-of-the-art\nmethods of 3D object classification on 10- and 40-class ModelNet datasets. We\nalso show that RotationNet, even trained without known poses, achieves the\nstate-of-the-art performance on an object pose estimation dataset. The code is\navailable on https://github.com/kanezaki/rotationnet\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 12:20:06 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 09:59:46 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 10:17:17 GMT"}, {"version": "v4", "created": "Fri, 23 Mar 2018 09:52:06 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Kanezaki", "Asako", ""], ["Matsushita", "Yasuyuki", ""], ["Nishida", "Yoshifumi", ""]]}, {"id": "1603.06327", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Dongbo Min, Stephen Lin, and Kwanghoon Sohn", "title": "Deep Self-Convolutional Activations Descriptor for Dense Cross-Modal\n  Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel descriptor, called deep self-convolutional activations\n(DeSCA), designed for establishing dense correspondences between images taken\nunder different imaging modalities, such as different spectral ranges or\nlighting conditions. Motivated by descriptors based on local self-similarity\n(LSS), we formulate a novel descriptor by leveraging LSS in a deep\narchitecture, leading to better discriminative power and greater robustness to\nnon-rigid image deformations than state-of-the-art cross-modality descriptors.\nThe DeSCA first computes self-convolutions over a local support window for\nrandomly sampled patches, and then builds self-convolution activations by\nperforming an average pooling through a hierarchical formulation within a deep\nconvolutional architecture. Finally, the feature responses on the\nself-convolution activations are encoded through a spatial pyramid pooling in a\ncircular configuration. In contrast to existing convolutional neural networks\n(CNNs) based descriptors, the DeSCA is training-free (i.e., randomly sampled\npatches are utilized as the convolution kernels), is robust to cross-modal\nimaging, and can be densely computed in an efficient manner that significantly\nreduces computational redundancy. The state-of-the-art performance of DeSCA on\nchallenging cases of cross-modal image pairs is demonstrated through extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 05:30:48 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Kim", "Seungryong", ""], ["Min", "Dongbo", ""], ["Lin", "Stephen", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1603.06359", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Kihong Park, Kwanghoon Sohn, and Stephen Lin", "title": "Unified Depth Prediction and Intrinsic Image Decomposition from a Single\n  Image via Joint Convolutional Neural Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for jointly predicting a depth map and intrinsic images\nfrom single-image input. The two tasks are formulated in a synergistic manner\nthrough a joint conditional random field (CRF) that is solved using a novel\nconvolutional neural network (CNN) architecture, called the joint convolutional\nneural field (JCNF) model. Tailored to our joint estimation problem, JCNF\ndiffers from previous CNNs in its sharing of convolutional activations and\nlayers between networks for each task, its inference in the gradient domain\nwhere there exists greater correlation between depth and intrinsic images, and\nthe incorporation of a gradient scale network that learns the confidence of\nestimated gradients in order to effectively balance them in the solution. This\napproach is shown to surpass state-of-the-art methods both on single-image\ndepth estimation and on intrinsic image decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 09:10:38 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Kim", "Seungryong", ""], ["Park", "Kihong", ""], ["Sohn", "Kwanghoon", ""], ["Lin", "Stephen", ""]]}, {"id": "1603.06398", "submitter": "Liqian Ma", "authors": "Liqian Ma, Jue Wang, Eli Shechtman, Kalyan Sunkavalli, Shimin Hu", "title": "Appearance Harmonization for Single Image Shadow Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadows often create unwanted artifacts in photographs, and removing them can\nbe very challenging. Previous shadow removal methods often produce de-shadowed\nregions that are visually inconsistent with the rest of the image. In this work\nwe propose a fully automatic shadow region harmonization approach that improves\nthe appearance compatibility of the de-shadowed region as typically produced by\nprevious methods. It is based on a shadow-guided patch-based image synthesis\napproach that reconstructs the shadow region using patches sampled from\nnon-shadowed regions. The result is then refined based on the reconstruction\nconfidence to handle unique image patterns. Many shadow removal results and\ncomparisons are show the effectiveness of our improvement. Quantitative\nevaluation on a benchmark dataset suggests that our automatic shadow\nharmonization approach effectively improves upon the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 12:01:36 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Ma", "Liqian", ""], ["Wang", "Jue", ""], ["Shechtman", "Eli", ""], ["Sunkavalli", "Kalyan", ""], ["Hu", "Shimin", ""]]}, {"id": "1603.06400", "submitter": "Ikenna Odinaka", "authors": "Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P.\n  MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani\n  Krishnamurthy, Anuj Kapadia, Lawrence Carin, and David J. Brady", "title": "Joint System and Algorithm Design for Computationally Efficient Fan Beam\n  Coded Aperture X-ray Coherent Scatter Imaging", "comments": "This paper has been submitted to IEEE Transactions on Computational\n  Imaging for consideration. 18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In x-ray coherent scatter tomography, tomographic measurements of the forward\nscatter distribution are used to infer scatter densities within a volume. A\nradiopaque 2D pattern placed between the object and the detector array enables\nthe disambiguation between different scatter events. The use of a fan beam\nsource illumination to speed up data acquisition relative to a pencil beam\npresents computational challenges. To facilitate the use of iterative\nalgorithms based on a penalized Poisson log-likelihood function, efficient\ncomputational implementation of the forward and backward models are needed. Our\nproposed implementation exploits physical symmetries and structural properties\nof the system and suggests a joint system-algorithm design, where the system\ndesign choices are influenced by computational considerations, and in turn lead\nto reduced reconstruction time. Computational-time speedups of approximately\n146 and 32 are achieved in the computation of the forward and backward models,\nrespectively. Results validating the forward model and reconstruction algorithm\nare presented on simulated analytic and Monte Carlo data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:35:57 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Odinaka", "Ikenna", ""], ["O'Sullivan", "Joseph A.", ""], ["Politte", "David G.", ""], ["MacCabe", "Kenneth P.", ""], ["Kaganovsky", "Yan", ""], ["Greenberg", "Joel A.", ""], ["Lakshmanan", "Manu", ""], ["Krishnamurthy", "Kalyani", ""], ["Kapadia", "Anuj", ""], ["Carin", "Lawrence", ""], ["Brady", "David J.", ""]]}, {"id": "1603.06432", "submitter": "Artem Rozantsev Mr.", "authors": "Artem Rozantsev, Mathieu Salzmann, Pascal Fua", "title": "Beyond Sharing Weights for Deep Domain Adaptation", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2018.2814042", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a classifier trained on data coming from a specific domain\ntypically degrades when applied to a related but different one. While\nannotating many samples from the new domain would address this issue, it is\noften too expensive or impractical. Domain Adaptation has therefore emerged as\na solution to this problem; It leverages annotated data from a source domain,\nin which it is abundant, to train a classifier to operate in a target domain,\nin which it is either sparse or even lacking altogether. In this context, the\nrecent trend consists of learning deep architectures whose weights are shared\nfor both domains, which essentially amounts to learning domain invariant\nfeatures.\n  Here, we show that it is more effective to explicitly model the shift from\none domain to the other. To this end, we introduce a two-stream architecture,\nwhere one operates in the source domain and the other in the target domain. In\ncontrast to other approaches, the weights in corresponding layers are related\nbut not shared. We demonstrate that this both yields higher accuracy than\nstate-of-the-art methods on several object recognition and detection tasks and\nconsistently outperforms networks with shared weights in both supervised and\nunsupervised settings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 14:20:41 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 13:51:31 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Rozantsev", "Artem", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1603.06433", "submitter": "Wolfgang Konen K", "authors": "Wolfgang Konen", "title": "Illumination-invariant image mosaic calculation based on logarithmic\n  search", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes an improved image mosaicking algorithm. It is\nbased on Jain's logarithmic search algorithm [Jain 1981] which is coupled to\nthe method of Kourogi (1999} for matching images in a video sequence.\nLogarithmic search has a better invariance against illumination changes than\nthe original optical-flow-based method of Kourogi.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 14:23:00 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Konen", "Wolfgang", ""]]}, {"id": "1603.06463", "submitter": "Sebastian Bach", "authors": "Sebastian Bach, Alexander Binder, Klaus-Robert M\\\"uller, Wojciech\n  Samek", "title": "Controlling Explanatory Heatmap Resolution and Semantics via\n  Decomposition Depth", "comments": "5 pages, 1 table, 1 figure with 40 embedded images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application of the Layer-wise Relevance Propagation (LRP)\nalgorithm to state of the art deep convolutional neural networks and Fisher\nVector classifiers to compare the image perception and prediction strategies of\nboth classifiers with the use of visualized heatmaps. Layer-wise Relevance\nPropagation (LRP) is a method to compute scores for individual components of an\ninput image, denoting their contribution to the prediction of the classifier\nfor one particular test point. We demonstrate the impact of different choices\nof decomposition cut-off points during the LRP-process, controlling the\nresolution and semantics of the heatmap on test images from the PASCAL VOC 2007\ntest data set.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 15:42:22 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 14:37:01 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 08:38:26 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Bach", "Sebastian", ""], ["Binder", "Alexander", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1603.06470", "submitter": "Yongxin Yang", "authors": "Guosheng Hu, Xiaojiang Peng, Yongxin Yang, Timothy Hospedales, Jakob\n  Verbeek", "title": "Frankenstein: Learning Deep Face Representations using Small Data", "comments": "IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have recently proven extremely effective\nfor difficult face recognition problems in uncontrolled settings. To train such\nnetworks, very large training sets are needed with millions of labeled images.\nFor some applications, such as near-infrared (NIR) face recognition, such large\ntraining datasets are not publicly available and difficult to collect. In this\nwork, we propose a method to generate very large training datasets of synthetic\nimages by compositing real face images in a given dataset. We show that this\nmethod enables to learn models from as few as 10,000 training images, which\nperform on par with models trained from 500,000 images. Using our approach we\nalso obtain state-of-the-art results on the CASIA NIR-VIS2.0 heterogeneous face\nrecognition dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 15:55:30 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 00:33:09 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 15:25:54 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Hu", "Guosheng", ""], ["Peng", "Xiaojiang", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1603.06496", "submitter": "Sheng Zou", "authors": "Sheng Zou and Alina Zare", "title": "Instance Influence Estimation for Hyperspectral Target Signature\n  Characterization using Extended Functions of Multiple Instances", "comments": "Published, Proceedings of the SPIE, 2016", "journal-ref": null, "doi": "10.1117/12.2228154", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Extended Functions of Multiple Instances (eFUMI) algorithm is a\ngeneralization of Multiple Instance Learning (MIL). In eFUMI, only bag level\n(i.e. set level) labels are needed to estimate target signatures from mixed\ndata. The training bags in eFUMI are labeled positive if any data point in a\nbag contains or represents any proportion of the target signature and are\nlabeled as a negative bag if all data points in the bag do not represent any\ntarget. From these imprecise labels, eFUMI has been shown to be effective at\nestimating target signatures in hyperspectral subpixel target detection\nproblems. One motivating scenario for the use of eFUMI is where an analyst\ncircles objects/regions of interest in a hyperspectral scene such that the\ntarget signatures of these objects can be estimated and be used to determine\nwhether other instances of the object appear elsewhere in the image collection.\nThe regions highlighted by the analyst serve as the imprecise labels for eFUMI.\nOften, an analyst may want to iteratively refine their imprecise labels. In\nthis paper, we present an approach for estimating the influence on the\nestimated target signature if the label for a particular input data point is\nmodified. This \"instance influence estimation\" guides an analyst to focus on\n(re-)labeling the data points that provide the largest change in the resulting\nestimated target signature and, thus, reduce the amount of time an analyst\nneeds to spend refining the labels for a hyperspectral scene. Results are shown\non real hyperspectral sub-pixel target detection data sets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:54:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Zou", "Sheng", ""], ["Zare", "Alina", ""]]}, {"id": "1603.06531", "submitter": "Otkrist Gupta", "authors": "Otkrist Gupta, Dan Raviv, Ramesh Raskar", "title": "Deep video gesture recognition using illumination invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present architectures based on deep neural nets for gesture\nrecognition in videos, which are invariant to local scaling. We amalgamate\nautoencoder and predictor architectures using an adaptive weighting scheme\ncoping with a reduced size labeled dataset, while enriching our models from\nenormous unlabeled sets. We further improve robustness to lighting conditions\nby introducing a new adaptive filer based on temporal local scale\nnormalization. We provide superior results over known methods, including recent\nreported approaches based on neural nets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 18:33:29 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1603.06554", "submitter": "Mohamed Amer", "authors": "Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar", "title": "Action-Affect Classification and Morphing using Multi-Task\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most recent work focused on affect from facial expressions, and not as much\non body. This work focuses on body affect analysis. Affect does not occur in\nisolation. Humans usually couple affect with an action in natural interactions;\nfor example, a person could be talking and smiling. Recognizing body affect in\nsequences requires efficient algorithms to capture both the micro movements\nthat differentiate between happy and sad and the macro variations between\ndifferent actions. We depart from traditional approaches for time-series data\nanalytics by proposing a multi-task learning model that learns a shared\nrepresentation that is well-suited for action-affect classification as well as\ngeneration. For this paper we choose Conditional Restricted Boltzmann Machines\nto be our building block. We propose a new model that enhances the CRBM model\nwith a factored multi-task component to become Multi-Task Conditional\nRestricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two\npublicly available datasets, the Body Affect dataset and the Tower Game\ndataset, and show superior classification performance improvement over the\nstate-of-the-art, as well as the generative abilities of our model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:38:07 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Shields", "Timothy J.", ""], ["Amer", "Mohamed R.", ""], ["Ehrlich", "Max", ""], ["Tamrakar", "Amir", ""]]}, {"id": "1603.06568", "submitter": "Haimin Zhang", "authors": "Haimin Zhang and Min Xu", "title": "Modelling Temporal Information Using Discrete Fourier Transform for\n  Recognizing Emotions in User-generated Videos", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1603.06182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread of user-generated Internet videos, emotion recognition in\nthose videos attracts increasing research efforts. However, most existing works\nare based on framelevel visual features and/or audio features, which might fail\nto model the temporal information, e.g. characteristics accumulated along time.\nIn order to capture video temporal information, in this paper, we propose to\nanalyse features in frequency domain transformed by discrete Fourier transform\n(DFT features). Frame-level features are firstly extract by a pre-trained deep\nconvolutional neural network (CNN). Then, time domain features are transferred\nand interpolated into DFT features. CNN and DFT features are further encoded\nand fused for emotion classification. By this way, static image features\nextracted from a pre-trained deep CNN and temporal information represented by\nDFT features are jointly considered for video emotion recognition. Experimental\nresults demonstrate that combining DFT features can effectively capture\ntemporal information and therefore improve emotion recognition performance. Our\napproach has achieved a state-of-the-art performance on the largest video\nemotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to\n62.6%.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 04:46:00 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 00:53:23 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Zhang", "Haimin", ""], ["Xu", "Min", ""]]}, {"id": "1603.06655", "submitter": "Zhen Dong", "authors": "Zhen Dong, Su Jia, Chi Zhang, Mingtao Pei", "title": "Input Aggregated Network for Face Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural network has shown promising performance in face image\nrecognition. The inputs of most networks are face images, and there is hardly\nany work reported in literature on network with face videos as input. To\nsufficiently discover the useful information contained in face videos, we\npresent a novel network architecture called input aggregated network which is\nable to learn fixed-length representations for variable-length face videos. To\naccomplish this goal, an aggregation unit is designed to model a face video\nwith various frames as a point on a Riemannian manifold, and the mapping unit\naims at mapping the point into high-dimensional space where face videos\nbelonging to the same subject are close-by and others are distant. These two\nunits together with the frame representation unit build an end-to-end learning\nsystem which can learn representations of face videos for the specific tasks.\nExperiments on two public face video datasets demonstrate the effectiveness of\nthe proposed network.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 01:27:50 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Dong", "Zhen", ""], ["Jia", "Su", ""], ["Zhang", "Chi", ""], ["Pei", "Mingtao", ""]]}, {"id": "1603.06668", "submitter": "Gustav Larsson", "authors": "Gustav Larsson, Michael Maire, Gregory Shakhnarovich", "title": "Learning Representations for Automatic Colorization", "comments": "ECCV 2016 (Project page:\n  http://people.cs.uchicago.edu/~larsson/colorization/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully automatic image colorization system. Our approach\nleverages recent advances in deep networks, exploiting both low-level and\nsemantic representations. As many scene elements naturally appear according to\nmultimodal color distributions, we train our model to predict per-pixel color\nhistograms. This intermediate output can be used to automatically generate a\ncolor image, or further manipulated prior to image formation. On both fully and\npartially automatic colorization tasks, we outperform existing methods. We also\nexplore colorization as a vehicle for self-supervised visual representation\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 04:08:01 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 07:28:21 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 17:50:50 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Larsson", "Gustav", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1603.06669", "submitter": "Ivan Wang-Hei Ho", "authors": "Jieshi Chen, Benjamin Carrion Schafer, Ivan Wang-Hei Ho", "title": "Implementation of a FPGA-Based Feature Detection and Networking System\n  for Real-time Traffic Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing demand of real-time traffic monitoring nowadays,\nsoftware-based image processing can hardly meet the real-time data processing\nrequirement due to the serial data processing nature. In this paper, the\nimplementation of a hardware-based feature detection and networking system\nprototype for real-time traffic monitoring as well as data transmission is\npresented. The hardware architecture of the proposed system is mainly composed\nof three parts: data collection, feature detection, and data transmission.\nOverall, the presented prototype can tolerate a high data rate of about 60\nframes per second. By integrating the feature detection and data transmission\nfunctions, the presented system can be further developed for various VANET\napplication scenarios to improve road safety and traffic efficiency. For\nexample, detection of vehicles that violate traffic rules, parking enforcement,\netc.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 04:09:34 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Chen", "Jieshi", ""], ["Schafer", "Benjamin Carrion", ""], ["Ho", "Ivan Wang-Hei", ""]]}, {"id": "1603.06678", "submitter": "Masaki Satoh", "authors": "Masaki Satoh", "title": "Stitching Stabilizer: Two-frame-stitching Video Stabilization for\n  Embedded Systems", "comments": "17 pages. For a supplemental video, see\n  https://www.youtube.com/watch?v=LmyPXfGZRb0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional electronic video stabilization, the stabilized frame is\nobtained by cropping the input frame to cancel camera shake. While a small\ncropping size results in strong stabilization, it does not provide us\nsatisfactory results from the viewpoint of image quality, because it narrows\nthe angle of view. By fusing several frames, we can effectively expand the area\nof input frames, and achieve strong stabilization even with a large cropping\nsize. Several methods for doing so have been studied. However, their\ncomputational costs are too high for embedded systems such as smartphones.\n  We propose a simple, yet surprisingly effective algorithm, called the\nstitching stabilizer. It stitches only two frames together with a minimal\ncomputational cost. It can achieve real-time processes in embedded systems, for\nFull HD and 30 FPS videos. To clearly show the effect, we apply it to\nhyperlapse. Using several clips, we show it produces more strongly stabilized\nand natural results than the existing solutions from Microsoft and Instagram.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 05:42:31 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Satoh", "Masaki", ""]]}, {"id": "1603.06680", "submitter": "Mohammad Rostami", "authors": "Mohammad Rostami, Zhou Wang", "title": "Image Super-Resolution Based on Sparsity Prior via Smoothed $l_0$ Norm", "comments": "Proceedings of the 2011 Symposium on Advanced Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim to tackle the problem of reconstructing a\nhigh-resolution image from a single low-resolution input image, known as single\nimage super-resolution. In the literature, sparse representation has been used\nto address this problem, where it is assumed that both low-resolution and\nhigh-resolution images share the same sparse representation over a pair of\ncoupled jointly trained dictionaries. This assumption enables us to use the\ncompressed sensing theory to find the jointly sparse representation via the\nlow-resolution image and then use it to recover the high-resolution image.\nHowever, sparse representation of a signal over a known dictionary is an\nill-posed, combinatorial optimization problem. Here we propose an algorithm\nthat adopts the smoothed $l_0$-norm (SL0) approach to find the jointly sparse\nrepresentation. Improved quality of the reconstructed image is obtained for\nmost images in terms of both peak signal-to-noise-ratio (PSNR) and structural\nsimilarity (SSIM) measures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 06:30:19 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Rostami", "Mohammad", ""], ["Wang", "Zhou", ""]]}, {"id": "1603.06759", "submitter": "Yanwei Pang", "authors": "Yanwei Pang, Manli Sun, Xiaoheng Jiang, Xuelong Li", "title": "Convolution in Convolution for Network in Network", "comments": "A method of Convolutional Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network in Netwrok (NiN) is an effective instance and an important extension\nof Convolutional Neural Network (CNN) consisting of alternating convolutional\nlayers and pooling layers. Instead of using a linear filter for convolution,\nNiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to\nreplace the linear filter. Because of the powerfulness of MLP and $ 1\\times 1 $\nconvolutions in spatial domain, NiN has stronger ability of feature\nrepresentation and hence results in better recognition rate. However, MLP\nitself consists of fully connected layers which give rise to a large number of\nparameters. In this paper, we propose to replace dense shallow MLP with sparse\nshallow MLP. One or more layers of the sparse shallow MLP are sparely connected\nin the channel dimension or channel-spatial domain. The proposed method is\nimplemented by applying unshared convolution across the channel dimension and\napplying shared convolution across the spatial dimension in some computational\nlayers. The proposed method is called CiC. Experimental results on the CIFAR10\ndataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the\neffectiveness of the proposed CiC method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 12:33:11 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Pang", "Yanwei", ""], ["Sun", "Manli", ""], ["Jiang", "Xiaoheng", ""], ["Li", "Xuelong", ""]]}, {"id": "1603.06765", "submitter": "Xiao Liu", "authors": "Xiao Liu, Tian Xia, Jiang Wang, Yi Yang, Feng Zhou, Yuanqing Lin", "title": "Fully Convolutional Attention Networks for Fine-Grained Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained recognition is challenging due to its subtle local inter-class\ndifferences versus large intra-class variations such as poses. A key to address\nthis problem is to localize discriminative parts to extract pose-invariant\nfeatures. However, ground-truth part annotations can be expensive to acquire.\nMoreover, it is hard to define parts for many fine-grained classes. This work\nintroduces Fully Convolutional Attention Networks (FCANs), a reinforcement\nlearning framework to optimally glimpse local discriminative regions adaptive\nto different fine-grained domains. Compared to previous methods, our approach\nenjoys three advantages: 1) the weakly-supervised reinforcement learning\nprocedure requires no expensive part annotations; 2) the fully-convolutional\narchitecture speeds up both training and testing; 3) the greedy reward strategy\naccelerates the convergence of the learning. We demonstrate the effectiveness\nof our method with extensive experiments on four challenging fine-grained\nbenchmark datasets, including CUB-200-2011, Stanford Dogs, Stanford Cars and\nFood-101.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 12:45:20 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 11:46:30 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 11:12:45 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 02:08:15 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Liu", "Xiao", ""], ["Xia", "Tian", ""], ["Wang", "Jiang", ""], ["Yang", "Yi", ""], ["Zhou", "Feng", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1603.06777", "submitter": "Bert De Brabandere", "authors": "Bert Moons, Bert De Brabandere, Luc Van Gool, Marian Verhelst", "title": "Energy-Efficient ConvNets Through Approximate Computing", "comments": "Published in IEEE Winter Conference on Applications of Computer\n  Vision (WACV 2016)", "journal-ref": null, "doi": "10.1109/WACV.2016.7477614", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently ConvNets or convolutional neural networks (CNN) have come up as\nstate-of-the-art classification and detection algorithms, achieving near-human\nperformance in visual detection. However, ConvNet algorithms are typically very\ncomputation and memory intensive. In order to be able to embed ConvNet-based\nclassification into wearable platforms and embedded systems such as smartphones\nor ubiquitous electronics for the internet-of-things, their energy consumption\nshould be reduced drastically. This paper proposes methods based on approximate\ncomputing to reduce energy consumption in state-of-the-art ConvNet\naccelerators. By combining techniques both at the system- and circuit level, we\ncan gain energy in the systems arithmetic: up to 30x without losing\nclassification accuracy and more than 100x at 99% classification accuracy,\ncompared to the commonly used 16-bit fixed point number format.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 13:20:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Moons", "Bert", ""], ["De Brabandere", "Bert", ""], ["Van Gool", "Luc", ""], ["Verhelst", "Marian", ""]]}, {"id": "1603.06812", "submitter": "Yaniv Romano", "authors": "Yaniv Romano and Michael Elad", "title": "Con-Patch: When a Patch Meets its Context", "comments": "Accepted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2576402", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the similarity between patches in images is a fundamental building\nblock in various tasks. Naturally, the patch-size has a major impact on the\nmatching quality, and on the consequent application performance. Under the\nassumption that our patch database is sufficiently sampled, using large patches\n(e.g. 21-by-21) should be preferred over small ones (e.g. 7-by-7). However,\nthis \"dense-sampling\" assumption is rarely true; in most cases large patches\ncannot find relevant nearby examples. This phenomenon is a consequence of the\ncurse of dimensionality, stating that the database-size should grow\nexponentially with the patch-size to ensure proper matches. This explains the\nfavored choice of small patch-size in most applications.\n  Is there a way to keep the simplicity and work with small patches while\ngetting some of the benefits that large patches provide? In this work we offer\nsuch an approach. We propose to concatenate the regular content of a\nconventional (small) patch with a compact representation of its (large)\nsurroundings - its context. Therefore, with a minor increase of the dimensions\n(e.g. with additional 10 values to the patch representation), we\nimplicitly/softly describe the information of a large patch. The additional\ndescriptors are computed based on a self-similarity behavior of the patch\nsurrounding.\n  We show that this approach achieves better matches, compared to the use of\nconventional-size patches, without the need to increase the database-size.\nAlso, the effectiveness of the proposed method is tested on three distinct\nproblems: (i) External natural image denoising, (ii) Depth image\nsuper-resolution, and (iii) Motion-compensated frame-rate up-conversion.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:44:28 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 14:28:01 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 14:14:58 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1603.06829", "submitter": "Otkrist Gupta", "authors": "Otkrist Gupta, Dan Raviv and Ramesh Raskar", "title": "Multi-velocity neural networks for gesture recognition in videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new action recognition deep neural network which adaptively\nlearns the best action velocities in addition to the classification. While deep\nneural networks have reached maturity for image understanding tasks, we are\nstill exploring network topologies and features to handle the richer\nenvironment of video clips. Here, we tackle the problem of multiple velocities\nin action recognition, and provide state-of-the-art results for gesture\nrecognition, on known and new collected datasets. We further provide the\ntraining steps for our semi-supervised network, suited to learn from huge\nunlabeled datasets with only a fraction of labeled examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 15:26:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1603.06895", "submitter": "Deanne Proctor", "authors": "D. D. Proctor", "title": "A Selection of Giant Radio Sources from NVSS", "comments": "20 pages of text, 6 figures, 22 pages tables, total 55 pages. The\n  stub for Table 6 is followed by the complete machine readable file. To be\n  published in The Astrophysical Journal Supplement. Revision 1: Corrected\n  typos, references updated/corrected, addition to acknowledgments. Five\n  candidates identified as SNR (Thanks to D. A. Green)", "journal-ref": null, "doi": "10.3847/0067-0049/224/2/18", "report-no": "LLNL-JRNL-686322", "categories": "astro-ph.GA cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results of the application of pattern recognition techniques to the problem\nof identifying Giant Radio Sources (GRS) from the data in the NVSS catalog are\npresented and issues affecting the process are explored. Decision-tree pattern\nrecognition software was applied to training set source pairs developed from\nknown NVSS large angular size radio galaxies. The full training set consisted\nof 51,195 source pairs, 48 of which were known GRS for which each lobe was\nprimarily represented by a single catalog component. The source pairs had a\nmaximum separation of 20 arc minutes and a minimum component area of 1.87\nsquare arc minutes at the 1.4 mJy level. The importance of comparing resulting\nprobability distributions of the training and application sets for cases of\nunknown class ratio is demonstrated. The probability of correctly ranking a\nrandomly selected (GRS, non-GRS) pair from the best of the tested classifiers\nwas determined to be 97.8 +/- 1.5%. The best classifiers were applied to the\nover 870,000 candidate pairs from the entire catalog. Images of higher ranked\nsources were visually screened and a table of over sixteen hundred candidates,\nincluding morphological annotation, is presented. These systems include doubles\nand triples, Wide-Angle Tail (WAT) and Narrow-Angle Tail (NAT), S- or Z-shaped\nsystems, and core-jets and resolved cores. While some resolved lobe systems are\nrecovered with this technique, generally it is expected that such systems would\nrequire a different approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:18:16 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 01:29:20 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Proctor", "D. D.", ""]]}, {"id": "1603.06937", "submitter": "Alejandro Newell", "authors": "Alejandro Newell, Kaiyu Yang, Jia Deng", "title": "Stacked Hourglass Networks for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel convolutional network architecture for the task\nof human pose estimation. Features are processed across all scales and\nconsolidated to best capture the various spatial relationships associated with\nthe body. We show how repeated bottom-up, top-down processing used in\nconjunction with intermediate supervision is critical to improving the\nperformance of the network. We refer to the architecture as a \"stacked\nhourglass\" network based on the successive steps of pooling and upsampling that\nare done to produce a final set of predictions. State-of-the-art results are\nachieved on the FLIC and MPII benchmarks outcompeting all recent methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 19:56:42 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 19:19:37 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Newell", "Alejandro", ""], ["Yang", "Kaiyu", ""], ["Deng", "Jia", ""]]}, {"id": "1603.06987", "submitter": "Lamberto Ballan", "authors": "Lamberto Ballan, Francesco Castaldo, Alexandre Alahi, Francesco\n  Palmieri, Silvio Savarese", "title": "Knowledge Transfer for Scene-specific Motion Prediction", "comments": "Accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When given a single frame of the video, humans can not only interpret the\ncontent of the scene, but also they are able to forecast the near future. This\nability is mostly driven by their rich prior knowledge about the visual world,\nboth in terms of (i) the dynamics of moving agents, as well as (ii) the\nsemantic of the scene. In this work we exploit the interplay between these two\nkey elements to predict scene-specific motion patterns. First, we extract patch\ndescriptors encoding the probability of moving to the adjacent patches, and the\nprobability of being in that particular patch or changing behavior. Then, we\nintroduce a Dynamic Bayesian Network which exploits this scene specific\nknowledge for trajectory prediction. Experimental results demonstrate that our\nmethod is able to accurately predict trajectories and transfer predictions to a\nnovel scene characterized by similar elements.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 21:19:42 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 01:05:52 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Ballan", "Lamberto", ""], ["Castaldo", "Francesco", ""], ["Alahi", "Alexandre", ""], ["Palmieri", "Francesco", ""], ["Savarese", "Silvio", ""]]}, {"id": "1603.06995", "submitter": "Zhicheng Cui", "authors": "Zhicheng Cui and Wenlin Chen and Yixin Chen", "title": "Multi-Scale Convolutional Neural Networks for Time Series Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification (TSC), the problem of predicting class labels of\ntime series, has been around for decades within the community of data mining\nand machine learning, and found many important applications such as biomedical\nengineering and clinical prediction. However, it still remains challenging and\nfalls short of classification accuracy and efficiency. Traditional approaches\ntypically involve extracting discriminative features from the original time\nseries using dynamic time warping (DTW) or shapelet transformation, based on\nwhich an off-the-shelf classifier can be applied. These methods are ad-hoc and\nseparate the feature extraction part with the classification part, which limits\ntheir accuracy performance. Plus, most existing methods fail to take into\naccount the fact that time series often have features at different time scales.\nTo address these problems, we propose a novel end-to-end neural network model,\nMulti-Scale Convolutional Neural Networks (MCNN), which incorporates feature\nextraction and classification in a single framework. Leveraging a novel\nmulti-branch layer and learnable convolutional layers, MCNN automatically\nextracts features at different scales and frequencies, leading to superior\nfeature representation. MCNN is also computationally efficient, as it naturally\nleverages GPU computing. We conduct comprehensive empirical evaluation with\nvarious existing methods on a large number of benchmark datasets, and show that\nMCNN advances the state-of-the-art by achieving superior accuracy performance\nthan other leading methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 21:37:33 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 04:51:24 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 18:58:29 GMT"}, {"version": "v4", "created": "Wed, 11 May 2016 04:48:21 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Cui", "Zhicheng", ""], ["Chen", "Wenlin", ""], ["Chen", "Yixin", ""]]}, {"id": "1603.07022", "submitter": "Alberto Pretto", "authors": "Marco Imperoli and Alberto Pretto", "title": "Active Detection and Localization of Textureless Objects in Cluttered\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an active object detection and localization framework\nthat combines a robust untextured object detection and 3D pose estimation\nalgorithm with a novel next-best-view selection strategy. We address the\ndetection and localization problems by proposing an edge-based registration\nalgorithm that refines the object position by minimizing a cost directly\nextracted from a 3D image tensor that encodes the minimum distance to an edge\npoint in a joint direction/location space. We face the next-best-view problem\nby exploiting a sequential decision process that, for each step, selects the\nnext camera position which maximizes the mutual information between the state\nand the next observations. We solve the intrinsic intractability of this\nsolution by generating observations that represent scene realizations, i.e.\ncombination samples of object hypothesis provided by the object detector, while\nmodeling the state by means of a set of constantly resampled particles.\nExperiments performed on different real world, challenging datasets confirm the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 22:55:03 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Imperoli", "Marco", ""], ["Pretto", "Alberto", ""]]}, {"id": "1603.07027", "submitter": "Ethan Rudd", "authors": "Ethan Rudd, Manuel G\\\"unther, and Terrance Boult", "title": "MOON: A Mixed Objective Optimization Network for the Recognition of\n  Facial Attributes", "comments": "Post-print of manuscript accepted to the European Conference on\n  Computer Vision (ECCV) 2016\n  http://link.springer.com/chapter/10.1007%2F978-3-319-46454-1_2", "journal-ref": null, "doi": "10.1007/978-3-319-46454-1_2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute recognition, particularly facial, extracts many labels for each\nimage. While some multi-task vision problems can be decomposed into separate\ntasks and stages, e.g., training independent models for each task, for a\ngrowing set of problems joint optimization across all tasks has been shown to\nimprove performance. We show that for deep convolutional neural network (DCNN)\nfacial attribute extraction, multi-task optimization is better. Unfortunately,\nit can be difficult to apply joint optimization to DCNNs when training data is\nimbalanced, and re-balancing multi-label data directly is structurally\ninfeasible, since adding/removing data to balance one label will change the\nsampling of the other labels. This paper addresses the multi-label imbalance\nproblem by introducing a novel mixed objective optimization network (MOON) with\na loss function that mixes multiple task objectives with domain adaptive\nre-weighting of propagated loss. Experiments demonstrate that not only does\nMOON advance the state of the art in facial attribute recognition, but it also\noutperforms independently trained DCNNs using the same data. When using facial\nattributes for the LFW face recognition task, we show that our balanced (domain\nadapted) network outperforms the unbalanced trained network.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 23:21:26 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 16:56:07 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Rudd", "Ethan", ""], ["G\u00fcnther", "Manuel", ""], ["Boult", "Terrance", ""]]}, {"id": "1603.07054", "submitter": "Dangwei Li", "authors": "Dangwei Li, Zhang Zhang, Xiaotang Chen, Haibin Ling, Kaiqi Huang", "title": "A Richly Annotated Dataset for Pedestrian Attribute Recognition", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to improve the dataset foundation for pedestrian\nattribute recognition in real surveillance scenarios. Recognition of human\nattributes, such as gender, and clothes types, has great prospects in real\napplications. However, the development of suitable benchmark datasets for\nattribute recognition remains lagged behind. Existing human attribute datasets\nare collected from various sources or an integration of pedestrian\nre-identification datasets. Such heterogeneous collection poses a big challenge\non developing high quality fine-grained attribute recognition algorithms.\nFurthermore, human attribute recognition are generally severely affected by\nenvironmental or contextual factors, such as viewpoints, occlusions and body\nparts, while existing attribute datasets barely care about them. To tackle\nthese problems, we build a Richly Annotated Pedestrian (RAP) dataset from real\nmulti-camera surveillance scenarios with long term collection, where data\nsamples are annotated with not only fine-grained human attributes but also\nenvironmental and contextual factors. RAP has in total 41,585 pedestrian\nsamples, each of which is annotated with 72 attributes as well as viewpoints,\nocclusions, body parts information. To our knowledge, the RAP dataset is the\nlargest pedestrian attribute dataset, which is expected to greatly promote the\nstudy of large-scale attribute recognition systems. Furthermore, we empirically\nanalyze the effects of different environmental and contextual factors on\npedestrian attribute recognition. Experimental results demonstrate that\nviewpoints, occlusions and body parts information could assist attribute\nrecognition a lot in real applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 02:41:59 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 02:54:02 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 06:42:25 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Li", "Dangwei", ""], ["Zhang", "Zhang", ""], ["Chen", "Xiaotang", ""], ["Ling", "Haibin", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1603.07057", "submitter": "Tal Hassner", "authors": "Iacopo Masi, Anh Tuan Tran, Jatuporn Toy Leksut, Tal Hassner and\n  Gerard Medioni", "title": "Do We Really Need to Collect Millions of Faces for Effective Face\n  Recognition?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition capabilities have recently made extraordinary leaps. Though\nthis progress is at least partially due to ballooning training set sizes --\nhuge numbers of face images downloaded and labeled for identity -- it is not\nclear if the formidable task of collecting so many images is truly necessary.\nWe propose a far more accessible means of increasing training data sizes for\nface recognition systems. Rather than manually harvesting and labeling more\nfaces, we simply synthesize them. We describe novel methods of enriching an\nexisting dataset with important facial appearance variations by manipulating\nthe faces it contains. We further apply this synthesis approach when matching\nquery images represented using a standard convolutional neural network. The\neffect of training and testing with synthesized images is extensively tested on\nthe LFW and IJB-A (verification and identification) benchmarks and Janus CS2.\nThe performances obtained by our approach match state of the art results\nreported by systems trained on millions of downloaded images.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 02:57:15 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 02:25:35 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Masi", "Iacopo", ""], ["Tran", "Anh Tuan", ""], ["Leksut", "Jatuporn Toy", ""], ["Hassner", "Tal", ""], ["Medioni", "Gerard", ""]]}, {"id": "1603.07063", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang and Xiaohui Shen and Jiashi Feng and Liang Lin and\n  Shuicheng Yan", "title": "Semantic Object Parsing with Graph LSTM", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By taking the semantic object parsing task as an exemplar application\nscenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,\nwhich is the generalization of LSTM from sequential data or multi-dimensional\ndata to general graph-structured data. Particularly, instead of evenly and\nfixedly dividing an image to pixels or patches in existing multi-dimensional\nLSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each\narbitrary-shaped superpixel as a semantically consistent node, and adaptively\nconstruct an undirected graph for each image, where the spatial relations of\nthe superpixels are naturally used as edges. Constructed on such an adaptive\ngraph topology, the Graph LSTM is more naturally aligned with the visual\npatterns in the image (e.g., object boundaries or appearance similarities) and\nprovides a more economical information propagation route. Furthermore, for each\noptimization step over Graph LSTM, we propose to use a confidence-driven scheme\nto update the hidden and memory states of nodes progressively till all nodes\nare updated. In addition, for each node, the forgets gates are adaptively\nlearned to capture different degrees of semantic correlation with neighboring\nnodes. Comprehensive evaluations on four diverse semantic object parsing\ndatasets well demonstrate the significant superiority of our Graph LSTM over\nother state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 03:31:02 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Liang", "Xiaodan", ""], ["Shen", "Xiaohui", ""], ["Feng", "Jiashi", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1603.07076", "submitter": "Albert Haque", "authors": "Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, Li\n  Fei-Fei", "title": "Towards Viewpoint Invariant 3D Human Pose Estimation", "comments": "European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a viewpoint invariant model for 3D human pose estimation from a\nsingle depth image. To achieve this, our discriminative model embeds local\nregions into a learned viewpoint invariant feature space. Formulated as a\nmulti-task learning problem, our model is able to selectively predict partial\nposes in the presence of noise and occlusion. Our approach leverages a\nconvolutional and recurrent network architecture with a top-down error feedback\nmechanism to self-correct previous pose estimates in an end-to-end manner. We\nevaluate our model on a previously published depth dataset and a newly\ncollected human pose dataset containing 100K annotated depth images from\nextreme viewpoints. Experiments show that our model achieves competitive\nperformance on frontal views while achieving state-of-the-art performance on\nalternate viewpoints.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 06:24:19 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 01:45:58 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 06:59:37 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Haque", "Albert", ""], ["Peng", "Boya", ""], ["Luo", "Zelun", ""], ["Alahi", "Alexandre", ""], ["Yeung", "Serena", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1603.07120", "submitter": "Amir Shahroudy", "authors": "Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang", "title": "Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single modality action recognition on RGB or depth sequences has been\nextensively explored recently. It is generally accepted that each of these two\nmodalities has different strengths and limitations for the task of action\nrecognition. Therefore, analysis of the RGB+D videos can help us to better\nstudy the complementary properties of these two types of modalities and achieve\nhigher levels of performance. In this paper, we propose a new deep autoencoder\nbased shared-specific feature factorization network to separate input\nmultimodal signals into a hierarchy of components. Further, based on the\nstructure of the features, a structured sparsity learning machine is proposed\nwhich utilizes mixed norms to apply regularization within components and group\nselection between them for better classification performance. Our experimental\nresults show the effectiveness of our cross-modality feature analysis framework\nby achieving state-of-the-art accuracy for action classification on five\nchallenging benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 10:22:12 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 05:31:52 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Shahroudy", "Amir", ""], ["Ng", "Tian-Tsong", ""], ["Gong", "Yihong", ""], ["Wang", "Gang", ""]]}, {"id": "1603.07123", "submitter": "Mohamed Sayed Elahl Dr", "authors": "R. M. Farouk and M. A. SayedElahl", "title": "Robust cDNA microarray image segmentation and analysis technique based\n  on Hough circle transform", "comments": "13 Pages,12 figures,FSP JOURNAL ISSN:1955-2068,Vol.9, Issue.7, part.1", "journal-ref": "HFSP JOURNAL ISSN:1955-2068,Vol.9, Issue.7, part.1 Human Frontier\n  Science Program , HFSP JOURNAL ISSN:1955-2068", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One of the most challenging tasks in microarray image analysis is spot\nsegmentation. A solution to this problem is to provide an algorithm than can be\nused to find any spot within the microarray image. Circular Hough\nTransformation (CHT) is a powerful feature extraction technique used in image\nanalysis, computer vision, and digital image processing. CHT algorithm is\napplied on the cDNA microarray images to develop the accuracy and the\nefficiency of the spots localization, addressing and segmentation process. The\npurpose of the applied technique is to find imperfect instances of spots within\na certain class of circles by applying a voting procedure on the cDNA\nmicroarray images for spots localization, addressing and characterizing the\npixels of each spot into foreground pixels and background simultaneously.\nIntensive experiments on the University of North Carolina (UNC) microarray\ndatabase indicate that the proposed method is superior to the K-means method\nand the Support vector machine (SVM). Keywords: Hough circle transformation,\ncDNA microarray image analysis, cDNA microarray image segmentation, spots\nlocalization and addressing, spots segmentation\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 10:25:20 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Farouk", "R. M.", ""], ["SayedElahl", "M. A.", ""]]}, {"id": "1603.07141", "submitter": "Francesc Moreno-Noguer", "authors": "Arnau Ramisa, Fei Yan, Francesc Moreno-Noguer and Krystian Mikolajczyk", "title": "BreakingNews: Article Annotation by Image and Text Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building upon recent Deep Neural Network architectures, current approaches\nlying in the intersection of computer vision and natural language processing\nhave achieved unprecedented breakthroughs in tasks like automatic captioning or\nimage retrieval. Most of these learning methods, though, rely on large training\nsets of images associated with human annotations that specifically describe the\nvisual content. In this paper we propose to go a step further and explore the\nmore complex cases where textual descriptions are loosely related to the\nimages. We focus on the particular domain of News articles in which the textual\ncontent often expresses connotative and ambiguous relations that are only\nsuggested but not directly inferred from images. We introduce new deep learning\nmethods that address source detection, popularity prediction, article\nillustration and geolocation of articles. An adaptive CNN architecture is\nproposed, that shares most of the structure for all the tasks, and is suitable\nfor multitask and transfer learning. Deep Canonical Correlation Analysis is\ndeployed for article illustration, and a new loss function based on Great\nCircle Distance is proposed for geolocation. Furthermore, we present\nBreakingNews, a novel dataset with approximately 100K news articles including\nimages, text and captions, and enriched with heterogeneous meta-data (such as\nGPS coordinates and popularity metrics). We show this dataset to be appropriate\nto explore all aforementioned problems, for which we provide a baseline\nperformance using various Deep Learning architectures, and different\nrepresentations of the textual and visual features. We report very promising\nresults and bring to light several limitations of current state-of-the-art in\nthis kind of domain, which we hope will help spur progress in the field.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 11:30:24 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Ramisa", "Arnau", ""], ["Yan", "Fei", ""], ["Moreno-Noguer", "Francesc", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1603.07188", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Karteek Alahari, Cordelia Schmid", "title": "Weakly-Supervised Semantic Segmentation using Motion Cues", "comments": "Extended version of our ECCV 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (FCNNs) trained on a large number of\nimages with strong pixel-level annotations have become the new state of the art\nfor the semantic segmentation task. While there have been recent attempts to\nlearn FCNNs from image-level weak annotations, they need additional\nconstraints, such as the size of an object, to obtain reasonable performance.\nTo address this issue, we present motion-CNN (M-CNN), a novel FCNN framework\nwhich incorporates motion cues and is learned from video-level weak\nannotations. Our learning scheme to train the network uses motion segments as\nsoft constraints, thereby handling noisy motion information. When trained on\nweakly-annotated videos, our method outperforms the state-of-the-art EM-Adapt\napproach on the PASCAL VOC 2012 image segmentation benchmark. We also\ndemonstrate that the performance of M-CNN learned with 150 weak video\nannotations is on par with state-of-the-art weakly-supervised methods trained\nwith thousands of images. Finally, M-CNN substantially outperforms recent\napproaches in a related task of video co-localization on the YouTube-Objects\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 14:01:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 12:21:37 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 08:16:06 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1603.07234", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi and Tinne Tuytelaars", "title": "Lightweight Unsupervised Domain Adaptation by Convolutional Filter\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning methods have achieved impressive results in many areas of\ncomputer vision. At the same time, these methods still suffer from a\ndegradation in performance when testing on new datasets that stem from a\ndifferent distribution. This is known as the domain shift effect. Recently\nproposed adaptation methods focus on retraining the network parameters.\nHowever, this requires access to all (labeled) source data, a large amount of\n(unlabeled) target data, and plenty of computational resources. In this work,\nwe propose a lightweight alternative, that allows adapting to the target domain\nbased on a limited number of target samples in a matter of minutes rather than\nhours, days or even weeks. To this end, we first analyze the output of each\nconvolutional layer from a domain adaptation perspective. Surprisingly, we find\nthat already at the very first layer, domain shift effects pop up. We then\npropose a new domain adaptation method, where first layer convolutional filters\nthat are badly affected by the domain shift are reconstructed based on less\naffected ones. This improves the performance of the deep network on various\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:28:29 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1603.07235", "submitter": "Oncel Tuzel", "authors": "Oncel Tuzel, Yuichi Taguchi, and John R. Hershey", "title": "Global-Local Face Upsampling Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face hallucination, which is the task of generating a high-resolution face\nimage from a low-resolution input image, is a well-studied problem that is\nuseful in widespread application areas. Face hallucination is particularly\nchallenging when the input face resolution is very low (e.g., 10 x 12 pixels)\nand/or the image is captured in an uncontrolled setting with large pose and\nillumination variations. In this paper, we revisit the algorithm introduced in\n[1] and present a deep interpretation of this framework that achieves\nstate-of-the-art under such challenging scenarios. In our deep network\narchitecture the global and local constraints that define a face can be\nefficiently modeled and learned end-to-end using training data. Conceptually\nour network design can be partitioned into two sub-networks: the first one\nimplements the holistic face reconstruction according to global constraints,\nand the second one enhances face-specific details and enforces local patch\nstatistics. We optimize the deep network using a new loss function for\nsuper-resolution that combines reconstruction error with a learned face quality\nmeasure in adversarial setting, producing improved visual results. We conduct\nextensive experiments in both controlled and uncontrolled setups and show that\nour algorithm improves the state of the art both numerically and visually.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:29:09 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 15:31:01 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Tuzel", "Oncel", ""], ["Taguchi", "Yuichi", ""], ["Hershey", "John R.", ""]]}, {"id": "1603.07254", "submitter": "Marcel Luethi", "authors": "Marcel L\\\"uthi, Christoph Jud, Thomas Gerig and Thomas Vetter", "title": "Gaussian Process Morphable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape models (SSMs) represent a class of shapes as a normal\ndistribution of point variations, whose parameters are estimated from example\nshapes. Principal component analysis (PCA) is applied to obtain a\nlow-dimensional representation of the shape variation in terms of the leading\nprincipal components. In this paper, we propose a generalization of SSMs,\ncalled Gaussian Process Morphable Models (GPMMs). We model the shape variations\nwith a Gaussian process, which we represent using the leading components of its\nKarhunen-Loeve expansion. To compute the expansion, we make use of an\napproximation scheme based on the Nystrom method. The resulting model can be\nseen as a continuous analogon of an SSM. However, while for SSMs the shape\nvariation is restricted to the span of the example data, with GPMMs we can\ndefine the shape variation using any Gaussian process. For example, we can\nbuild shape models that correspond to classical spline models, and thus do not\nrequire any example data. Furthermore, Gaussian processes make it possible to\ncombine different models. For example, an SSM can be extended with a spline\nmodel, to obtain a model that incorporates learned shape characteristics, but\nis flexible enough to explain shapes that cannot be represented by the SSM. We\nintroduce a simple algorithm for fitting a GPMM to a surface or image. This\nresults in a non-rigid registration approach, whose regularization properties\nare defined by a GPMM. We show how we can obtain different registration\nschemes,including methods for multi-scale, spatially-varying or hybrid\nregistration, by constructing an appropriate GPMM. As our approach strictly\nseparates modelling from the fitting process, this is all achieved without\nchanges to the fitting algorithm. We show the applicability and versatility of\nGPMMs on a clinical use case, where the goal is the model-based segmentation of\n3D forearm images.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 16:19:29 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["L\u00fcthi", "Marcel", ""], ["Jud", "Christoph", ""], ["Gerig", "Thomas", ""], ["Vetter", "Thomas", ""]]}, {"id": "1603.07388", "submitter": "Wael AbdAlmageed", "authors": "Wael AbdAlmageed, Yue Wua, Stephen Rawlsa, Shai Harel, Tal Hassner,\n  Iacopo Masi, Jongmoo Choi, Jatuporn Toy Leksut, Jungyeon Kim, Prem Natarajan,\n  Ram Nevatia, Gerard Medioni", "title": "Face Recognition Using Deep Multi-Pose Representations", "comments": "WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce our method and system for face recognition using multiple\npose-aware deep learning models. In our representation, a face image is\nprocessed by several pose-specific deep convolutional neural network (CNN)\nmodels to generate multiple pose-specific features. 3D rendering is used to\ngenerate multiple face poses from the input image. Sensitivity of the\nrecognition system to pose variations is reduced since we use an ensemble of\npose-specific CNN features. The paper presents extensive experimental results\non the effect of landmark detection, CNN layer selection and pose model\nselection on the performance of the recognition pipeline. Our novel\nrepresentation achieves better results than the state-of-the-art on IARPA's CS2\nand NIST's IJB-A in both verification and identification (i.e. search) tasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 23:16:40 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["AbdAlmageed", "Wael", ""], ["Wua", "Yue", ""], ["Rawlsa", "Stephen", ""], ["Harel", "Shai", ""], ["Hassner", "Tal", ""], ["Masi", "Iacopo", ""], ["Choi", "Jongmoo", ""], ["Leksut", "Jatuporn Toy", ""], ["Kim", "Jungyeon", ""], ["Natarajan", "Prem", ""], ["Nevatia", "Ram", ""], ["Medioni", "Gerard", ""]]}, {"id": "1603.07396", "submitter": "Aniruddha Kembhavi", "authors": "Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh\n  Hajishirzi, Ali Farhadi", "title": "A Diagram Is Worth A Dozen Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagrams are common tools for representing complex concepts, relationships\nand events, often when it would be difficult to portray the same information\nwith natural images. Understanding natural images has been extensively studied\nin computer vision, while diagram understanding has received little attention.\nIn this paper, we study the problem of diagram interpretation and reasoning,\nthe challenging task of identifying the structure of a diagram and the\nsemantics of its constituents and their relationships. We introduce Diagram\nParse Graphs (DPG) as our representation to model the structure of diagrams. We\ndefine syntactic parsing of diagrams as learning to infer DPGs for diagrams and\nstudy semantic interpretation and reasoning of diagrams in the context of\ndiagram question answering. We devise an LSTM-based method for syntactic\nparsing of diagrams and introduce a DPG-based attention model for diagram\nquestion answering. We compile a new dataset of diagrams with exhaustive\nannotations of constituents and relationships for over 5,000 diagrams and\n15,000 questions and answers. Our results show the significance of our models\nfor syntactic parsing and question answering in diagrams using DPGs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 00:02:58 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kembhavi", "Aniruddha", ""], ["Salvato", "Mike", ""], ["Kolve", "Eric", ""], ["Seo", "Minjoon", ""], ["Hajishirzi", "Hannaneh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1603.07415", "submitter": "Jianan Li", "authors": "Jianan Li, Yunchao Wei, Xiaodan Liang, Jian Dong, Tingfa Xu, Jiashi\n  Feng, and Shuicheng Yan", "title": "Attentive Contexts for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network based object detection methods typically classify\ncandidate proposals using their interior features. However, global and local\nsurrounding contexts that are believed to be valuable for object detection are\nnot fully exploited by existing methods yet. In this work, we take a step\ntowards understanding what is a robust practice to extract and utilize\ncontextual information to facilitate object detection in practice.\nSpecifically, we consider the following two questions: \"how to identify useful\nglobal contextual information for detecting a certain object?\" and \"how to\nexploit local context surrounding a proposal for better inferring its\ncontents?\". We provide preliminary answers to these questions through\ndeveloping a novel Attention to Context Convolution Neural Network (AC-CNN)\nbased object detection model. AC-CNN effectively incorporates global and local\ncontextual information into the region-based CNN (e.g. Fast RCNN) detection\nmodel and provides better object detection performance. It consists of one\nattention-based global contextualized (AGC) sub-network and one multi-scale\nlocal contextualized (MLC) sub-network. To capture global context, the AGC\nsub-network recurrently generates an attention map for an input image to\nhighlight useful global contextual locations, through multiple stacked Long\nShort-Term Memory (LSTM) layers. For capturing surrounding local context, the\nMLC sub-network exploits both the inside and outside contextual information of\neach specific proposal at multiple scales. The global and local context are\nthen fused together for making the final decision for detection. Extensive\nexperiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority of\nthe proposed AC-CNN over well-established baselines. In particular, AC-CNN\noutperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 in\nterms of mAP, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 02:18:37 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Li", "Jianan", ""], ["Wei", "Yunchao", ""], ["Liang", "Xiaodan", ""], ["Dong", "Jian", ""], ["Xu", "Tingfa", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1603.07442", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Namil Kim, Sunggyun Park, Anthony S. Paek, In So Kweon", "title": "Pixel-Level Domain Transfer", "comments": "Published in ECCV 2016. Code and dataset available at dgyoo.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image-conditional image generation model. The model transfers\nan input domain to a target domain in semantic level, and generates the target\nimage in pixel level. To generate realistic target images, we employ the\nreal/fake-discriminator as in Generative Adversarial Nets, but also introduce a\nnovel domain-discriminator to make the generated image relevant to the input\nimage. We verify our model through a challenging task of generating a piece of\nclothing from an input image of a dressed person. We present a high quality\nclothing dataset containing the two domains, and succeed in demonstrating\ndecent results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 05:20:59 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 01:20:33 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 13:17:40 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Yoo", "Donggeun", ""], ["Kim", "Namil", ""], ["Park", "Sunggyun", ""], ["Paek", "Anthony S.", ""], ["Kweon", "In So", ""]]}, {"id": "1603.07475", "submitter": "Youngjin Yoon", "authors": "Youngjin Yoon, Gyeongmin Choe, Namil Kim, Joon-Young Lee, In So Kweon", "title": "Fine-scale Surface Normal Estimation using a Single NIR Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present surface normal estimation using a single near infrared (NIR)\nimage. We are focusing on fine-scale surface geometry captured with an\nuncalibrated light source. To tackle this ill-posed problem, we adopt a\ngenerative adversarial network which is effective in recovering a sharp output,\nwhich is also essential for fine-scale surface normal estimation. We\nincorporate angular error and integrability constraint into the objective\nfunction of the network to make estimated normals physically meaningful. We\ntrain and validate our network on a recent NIR dataset, and also evaluate the\ngenerality of our trained model by using new external datasets which are\ncaptured with a different camera under different environment.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 08:43:14 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Yoon", "Youngjin", ""], ["Choe", "Gyeongmin", ""], ["Kim", "Namil", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1603.07485", "submitter": "Anna Khoreva", "authors": "Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt\n  Schiele", "title": "Simple Does It: Weakly Supervised Instance and Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labelling and instance segmentation are two tasks that require\nparticularly costly annotations. Starting from weak supervision in the form of\nbounding box detection annotations, we propose a new approach that does not\nrequire modification of the segmentation training procedure. We show that when\ncarefully designing the input labels from given bounding boxes, even a single\nround of training is enough to improve over previously reported weakly\nsupervised results. Overall, our weak supervision approach reaches ~95% of the\nquality of the fully supervised model, both for semantic labelling and instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 09:04:16 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 09:53:29 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Khoreva", "Anna", ""], ["Benenson", "Rodrigo", ""], ["Hosang", "Jan", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1603.07604", "submitter": "Yan Yan", "authors": "Yan Yan, Hanzi Wang, David Suter", "title": "Multi-Subregion Based Correlation Filter Bank for Robust Face\n  Recognition", "comments": null, "journal-ref": "Pattern Recognition, volume 47, 11, pages3487--3501, (2014)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective feature extraction algorithm, called\nMulti-Subregion based Correlation Filter Bank (MS-CFB), for robust face\nrecognition. MS-CFB combines the benefits of global-based and local-based\nfeature extraction algorithms, where multiple correlation filters correspond-\ning to different face subregions are jointly designed to optimize the overall\ncorrelation outputs. Furthermore, we reduce the computational complexi- ty of\nMS-CFB by designing the correlation filter bank in the spatial domain and\nimprove its generalization capability by capitalizing on the unconstrained form\nduring the filter bank design process. MS-CFB not only takes the d- ifferences\namong face subregions into account, but also effectively exploits the\ndiscriminative information in face subregions. Experimental results on various\npublic face databases demonstrate that the proposed algorithm pro- vides a\nbetter feature representation for classification and achieves higher\nrecognition rates compared with several state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 14:45:51 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Yan", "Yan", ""], ["Wang", "Hanzi", ""], ["Suter", "David", ""]]}, {"id": "1603.07625", "submitter": "Mike Wu", "authors": "Stephen Yu, Mike Wu", "title": "Position and Vector Detection of Blind Spot motion with the Horn-Schunck\n  Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed method uses live image footage which, based on calculations of\npixel motion, decides whether or not an object is in the blind-spot. If found,\nthe driver is notified by a sensory light or noise built into the vehicle's\nCPU. The new technology incorporates optical vectors and flow fields rather\nthan expensive radar-waves, creating cheaper detection systems that retain the\nneeded accuracy while adapting to the current processor speeds.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 15:28:26 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Yu", "Stephen", ""], ["Wu", "Mike", ""]]}, {"id": "1603.07697", "submitter": "Homa Foroughi", "authors": "Homa Foroughi, Nilanjan Ray, Hong Zhang", "title": "Joint Projection and Dictionary Learning using Low-rank Regularization\n  and Graph Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at learning simultaneously a discriminative dictionary\nand a robust projection matrix from noisy data. The joint learning, makes the\nlearned projection and dictionary a better fit for each other, so a more\naccurate classification can be obtained. However, current prevailing joint\ndimensionality reduction and dictionary learning methods, would fail when the\ntraining samples are noisy or heavily corrupted. To address this issue, we\npropose a joint projection and dictionary learning using low-rank\nregularization and graph constraints (JPDL-LR). Specifically, the\ndiscrimination of the dictionary is achieved by imposing Fisher criterion on\nthe coding coefficients. In addition, our method explicitly encodes the local\nstructure of data by incorporating a graph regularization term, that further\nimproves the discriminative ability of the projection matrix. Inspired by\nrecent advances of low-rank representation for removing outliers and noise, we\nenforce a low-rank constraint on sub-dictionaries of all classes to make them\nmore compact and robust to noise. Experimental results on several benchmark\ndatasets verify the effectiveness and robustness of our method for both\ndimensionality reduction and image classification, especially when the data\ncontains considerable noise or variations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 18:35:41 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 00:08:19 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Foroughi", "Homa", ""], ["Ray", "Nilanjan", ""], ["Zhang", "Hong", ""]]}, {"id": "1603.07745", "submitter": "Ganesh Sundaramoorthi", "authors": "Ganesh Sundaramoorthi, Naeemullah Khan, Byung-Woo Hong", "title": "Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a general energy and method for segmentation that is designed to\nhave preference for segmenting the coarse structure over the fine structure of\nthe data, without smoothing across boundaries of regions. The energy is\nformulated by considering data terms at a continuum of scales from the scale\nspace computed from the Heat Equation within regions, and integrating these\nterms over all time. We show that the energy may be approximately optimized\nwithout solving for the entire scale space, but rather solving time-independent\nlinear equations at the native scale of the image, making the method\ncomputationally feasible. We provide a multi-region scheme, and apply our\nmethod to motion segmentation. Experiments on a benchmark dataset shows that\nour method is less sensitive to clutter or other undesirable fine-scale\nstructure, and leads to better performance in motion segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 20:39:24 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Sundaramoorthi", "Ganesh", ""], ["Khan", "Naeemullah", ""], ["Hong", "Byung-Woo", ""]]}, {"id": "1603.07763", "submitter": "Hao Jiang", "authors": "Hao Jiang, Kristen Grauman", "title": "Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the camera wearer's activity is central to egocentric vision,\nyet one key facet of that activity is inherently invisible to the camera--the\nwearer's body pose. Prior work focuses on estimating the pose of hands and arms\nwhen they come into view, but this 1) gives an incomplete view of the full body\nposture, and 2) prevents any pose estimate at all in many frames, since the\nhands are only visible in a fraction of daily life activities. We propose to\ninfer the \"invisible pose\" of a person behind the egocentric camera. Given a\nsingle video, our efficient learning-based approach returns the full body 3D\njoint positions for each frame. Our method exploits cues from the dynamic\nmotion signatures of the surrounding scene--which changes predictably as a\nfunction of body pose--as well as static scene structures that reveal the\nviewpoint (e.g., sitting vs. standing). We further introduce a novel energy\nminimization scheme to infer the pose sequence. It uses soft predictions of the\nposes per time instant together with a non-parametric model of human pose\ndynamics over longer windows. Our method outperforms an array of possible\nalternatives, including deep learning approaches for direct pose regression\nfrom images.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 21:46:49 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Jiang", "Hao", ""], ["Grauman", "Kristen", ""]]}, {"id": "1603.07772", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li\n  Shen, Xiaohui Xie", "title": "Co-occurrence Feature Learning for Skeleton based Action Recognition\n  using Regularized Deep LSTM Networks", "comments": "AAAI 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton based action recognition distinguishes human actions using the\ntrajectories of skeleton joints, which provide a very good representation for\ndescribing actions. Considering that recurrent neural networks (RNNs) with Long\nShort-Term Memory (LSTM) can learn feature representations and model long-term\ntemporal dependencies automatically, we propose an end-to-end fully connected\ndeep LSTM network for skeleton based action recognition. Inspired by the\nobservation that the co-occurrences of the joints intrinsically characterize\nhuman actions, we take the skeleton as the input at each time slot and\nintroduce a novel regularization scheme to learn the co-occurrence features of\nskeleton joints. To train the deep LSTM network effectively, we propose a new\ndropout algorithm which simultaneously operates on the gates, cells, and output\nresponses of the LSTM neurons. Experimental results on three human action\nrecognition datasets consistently demonstrate the effectiveness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 22:43:55 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Zhu", "Wentao", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Li", "Yanghao", ""], ["Shen", "Li", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1603.07797", "submitter": "Yan Yan", "authors": "Yan Yan, Hanzi Wang, Si Chen, Xiaochun Cao, David Zhang", "title": "Quadratic Projection Based Feature Extraction with Its Application to\n  Biometric Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel quadratic projection based feature extraction\nframework, where a set of quadratic matrices is learned to distinguish each\nclass from all other classes. We formulate quadratic matrix learning (QML) as a\nstandard semidefinite programming (SDP) problem. However, the con- ventional\ninterior-point SDP solvers do not scale well to the problem of QML for\nhigh-dimensional data. To solve the scalability of QML, we develop an efficient\nalgorithm, termed DualQML, based on the Lagrange duality theory, to extract\nnonlinear features. To evaluate the feasibility and effectiveness of the\nproposed framework, we conduct extensive experiments on biometric recognition.\nExperimental results on three representative biometric recogni- tion tasks,\nincluding face, palmprint, and ear recognition, demonstrate the superiority of\nthe DualQML-based feature extraction algorithm compared to the current\nstate-of-the-art algorithms\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 01:10:53 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Yan", "Yan", ""], ["Wang", "Hanzi", ""], ["Chen", "Si", ""], ["Cao", "Xiaochun", ""], ["Zhang", "David", ""]]}, {"id": "1603.07800", "submitter": "Yan Yan", "authors": "Yan Yan, Hanzi Wang, Cuihua Li, Chenhui Yang, Bineng Zhong", "title": "An Effective Unconstrained Correlation Filter and Its Kernelization for\n  Face Recognition", "comments": null, "journal-ref": "Neurocomputing, 119, pp.201-211, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an effective unconstrained correlation filter called Uncon-\nstrained Optimal Origin Tradeoff Filter (UOOTF) is presented and applied to\nrobust face recognition. Compared with the conventional correlation filters in\nClass-dependence Feature Analysis (CFA), UOOTF improves the overall performance\nfor unseen patterns by removing the hard constraints on the origin correlation\noutputs during the filter design. To handle non-linearly separable\ndistributions between different classes, we further develop a non- linear\nextension of UOOTF based on the kernel technique. The kernel ex- tension of\nUOOTF allows for higher flexibility of the decision boundary due to a wider\nrange of non-linearity properties. Experimental results demon- strate the\neffectiveness of the proposed unconstrained correlation filter and its\nkernelization in the task of face recognition.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 01:36:41 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Yan", "Yan", ""], ["Wang", "Hanzi", ""], ["Li", "Cuihua", ""], ["Yang", "Chenhui", ""], ["Zhong", "Bineng", ""]]}, {"id": "1603.07807", "submitter": "Xiao Guobao", "authors": "Hanzi Wang, Guobao Xiao, Yan Yan, David Suter", "title": "Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting", "comments": "Proceedings of the IEEE International Conference on Computer Vision,\n  pp. 2902-2910, 2015", "journal-ref": null, "doi": "10.1109/ICCV.2015.332", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel geometric model fitting method, called\nMode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in the\npresence of severe outliers. The proposed method formulates geometric model\nfitting as a mode seeking problem on a hypergraph in which vertices represent\nmodel hypotheses and hyperedges denote data points. MSH intuitively detects\nmodel instances by a simple and effective mode seeking algorithm. In addition\nto the mode seeking algorithm, MSH includes a similarity measure between\nvertices on the hypergraph and a weight-aware sampling technique. The proposed\nmethod not only alleviates sensitivity to the data distribution, but also is\nscalable to large scale problems. Experimental results further demonstrate that\nthe proposed method has significant superiority over the state-of-the-art\nfitting methods on both synthetic data and real images.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 02:29:40 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Hanzi", ""], ["Xiao", "Guobao", ""], ["Yan", "Yan", ""], ["Suter", "David", ""]]}, {"id": "1603.07810", "submitter": "Andreas Veit", "authors": "Andreas Veit, Serge Belongie, Theofanis Karaletsos", "title": "Conditional Similarity Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 02:52:02 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 12:41:01 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 15:18:21 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Karaletsos", "Theofanis", ""]]}, {"id": "1603.07823", "submitter": "Nannan Wang", "authors": "Nannan Wang and Jie Li and Leiyu Sun and Bin Song and Xinbo Gao", "title": "Training-Free Synthesized Face Sketch Recognition Using Image Quality\n  Assessment Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": "Xidian-001", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face sketch synthesis has wide applications ranging from digital\nentertainments to law enforcements. Objective image quality assessment scores\nand face recognition accuracy are two mainly used tools to evaluate the\nsynthesis performance. In this paper, we proposed a synthesized face sketch\nrecognition framework based on full-reference image quality assessment metrics.\nSynthesized sketches generated from four state-of-the-art methods are utilized\nto test the performance of the proposed recognition framework. For the image\nquality assessment metrics, we employed the classical structured similarity\nindex metric and other three prevalent metrics: visual information fidelity,\nfeature similarity index metric and gradient magnitude similarity deviation.\nExtensive experiments compared with baseline methods illustrate the\neffectiveness of the proposed synthesized face sketch recognition framework.\nData and implementation code in this paper are available online at\nwww.ihitworld.com/WNN/IQA_Sketch.zip.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 05:20:08 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Wang", "Nannan", ""], ["Li", "Jie", ""], ["Sun", "Leiyu", ""], ["Song", "Bin", ""], ["Gao", "Xinbo", ""]]}, {"id": "1603.07834", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher\n  Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam,\n  Soumik Sarkar", "title": "An end-to-end convolutional selective autoencoder approach to Soybean\n  Cyst Nematode eggs detection", "comments": "A 10 pages, 8 figures International Conference on Machine\n  Leaning(ICML) Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel selective autoencoder approach within the\nframework of deep convolutional networks. The crux of the idea is to train a\ndeep convolutional autoencoder to suppress undesired parts of an image frame\nwhile allowing the desired parts resulting in efficient object detection. The\nefficacy of the framework is demonstrated on a critical plant science problem.\nIn the United States, approximately $1 billion is lost per annum due to a\nnematode infection on soybean plants. Currently, plant-pathologists rely on\nlabor-intensive and time-consuming identification of Soybean Cyst Nematode\n(SCN) eggs in soil samples via manual microscopy. The proposed framework\nattempts to significantly expedite the process by using a series of manually\nlabeled microscopic images for training followed by automated high-throughput\negg detection. The problem is particularly difficult due to the presence of a\nlarge population of non-egg particles (disturbances) in the image frames that\nare very similar to SCN eggs in shape, pose and illumination. Therefore, the\nselective autoencoder is trained to learn unique features related to the\ninvariant shapes and sizes of the SCN eggs without handcrafting. After that, a\ncomposite non-maximum suppression and differencing is applied at the\npost-processing stage.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 07:12:32 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Lee", "Nigel", ""], ["Chawla", "Vikas", ""], ["Mullaney", "Mark", ""], ["Marett", "Christopher", ""], ["Singh", "Asheesh", ""], ["Singh", "Arti", ""], ["Tylka", "Greg", ""], ["Ganapathysubramaniam", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1603.07839", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar", "title": "Early Detection of Combustion Instabilities using Deep Convolutional\n  Selective Autoencoders on Hi-speed Flame Video", "comments": "A 10 pages, 10 figures submission for Applied Data Science Track of\n  KDD16", "journal-ref": null, "doi": "10.1145/1235", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end convolutional selective autoencoder\napproach for early detection of combustion instabilities using rapidly arriving\nflame image frames. The instabilities arising in combustion processes cause\nsignificant deterioration and safety issues in various human-engineered systems\nsuch as land and air based gas turbine engines. These properties are described\nas self-sustaining, large amplitude pressure oscillations and show varying\nspatial scales periodic coherent vortex structure shedding. However, such\ninstability is extremely difficult to detect before a combustion process\nbecomes completely unstable due to its sudden (bifurcation-type) nature. In\nthis context, an autoencoder is trained to selectively mask stable flame and\nallow unstable flame image frames. In that process, the model learns to\nidentify and extract rich descriptive and explanatory flame shape features.\nWith such a training scheme, the selective autoencoder is shown to be able to\ndetect subtle instability features as a combustion process makes transition\nfrom stable to unstable region. As a consequence, the deep learning tool-chain\ncan perform as an early detection framework for combustion instabilities that\nwill have a transformative impact on the safety and performance of modern\nengines.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:02:41 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Lore", "Kin Gwn", ""], ["Sarkar", "Soumalya", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1603.07886", "submitter": "Shanlin Zhong", "authors": "Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo\n  Zhang", "title": "A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic\n  Extraction of Semantics, Formation of Integrated Concepts and Re-selection\n  Features for Ambiguity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration between biology and information science benefits both fields.\nMany related models have been proposed, such as computational visual cognition\nmodels, computational motor control models, integrations of both and so on. In\ngeneral, the robustness and precision of recognition is one of the key problems\nfor object recognition models.\n  In this paper, inspired by features of human recognition process and their\nbiological mechanisms, a new integrated and dynamic framework is proposed to\nmimic the semantic extraction, concept formation and feature re-selection in\nhuman visual processing. The main contributions of the proposed model are as\nfollows:\n  (1) Semantic feature extraction: Local semantic features are learnt from\nepisodic features that are extracted from raw images through a deep neural\nnetwork;\n  (2) Integrated concept formation: Concepts are formed with local semantic\ninformation and structural information learnt through network.\n  (3) Feature re-selection: When ambiguity is detected during recognition\nprocess, distinctive features according to the difference between ambiguous\ncandidates are re-selected for recognition.\n  Experimental results on hand-written digits and facial shape dataset show\nthat, compared with other methods, the new proposed model exhibits higher\nrobustness and precision for visual recognition, especially in the condition\nwhen input samples are smantic ambiguous. Meanwhile, the introduced biological\nmechanisms further strengthen the interaction between neuroscience and\ninformation science.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:47:16 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Yin", "Peijie", ""], ["Qiao", "Hong", ""], ["Wu", "Wei", ""], ["Qi", "Lu", ""], ["Li", "YinLin", ""], ["Zhong", "Shanlin", ""], ["Zhang", "Bo", ""]]}, {"id": "1603.07957", "submitter": "Fuqiang Liu", "authors": "Fuqiang Liu, Fukun Bi, Liang Chen", "title": "Object Recognition Based on Amounts of Unlabeled Data", "comments": "16 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel semi-supervised method on object recognition.\nFirst, based on Boost Picking, a universal algorithm, Boost Picking Teaching\n(BPT), is proposed to train an effective binary-classifier just using a few\nlabeled data and amounts of unlabeled data. Then, an ensemble strategy is\ndetailed to synthesize multiple BPT-trained binary-classifiers to be a\nhigh-performance multi-classifier. The rationality of the strategy is also\nanalyzed in theory. Finally, the proposed method is tested on two databases,\nCIFAR-10 and CIFAR-100. Using 2% labeled data and 98% unlabeled data, the\naccuracies of the proposed method on the two data sets are 78.39% and 50.77%\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 16:44:35 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Liu", "Fuqiang", ""], ["Bi", "Fukun", ""], ["Chen", "Liang", ""]]}, {"id": "1603.07965", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Isabella Nogues,\n  Jianhua Yao, Ronald Summers", "title": "Unsupervised Category Discovery via Looped Deep Pseudo-Task Optimization\n  Using a Large Scale Radiology Image Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining semantic labels on a large scale radiology image database (215,786\nkey images from 61,845 unique patients) is a prerequisite yet bottleneck to\ntrain highly effective deep convolutional neural network (CNN) models for image\nrecognition. Nevertheless, conventional methods for collecting image labels\n(e.g., Google search followed by crowd-sourcing) are not applicable due to the\nformidable difficulties of medical annotation tasks for those who are not\nclinically trained. This type of image labeling task remains non-trivial even\nfor radiologists due to uncertainty and possible drastic inter-observer\nvariation or inconsistency.\n  In this paper, we present a looped deep pseudo-task optimization procedure\nfor automatic category discovery of visually coherent and clinically semantic\n(concept) clusters. Our system can be initialized by domain-specific (CNN\ntrained on radiology images and text report derived labels) or generic\n(ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks are\nexploited by the looped deep image feature clustering (to refine image labels)\nand deep CNN training/classification using new labels (to obtain more task\nrepresentative deep features). Our method is conceptually simple and based on\nthe hypothesized \"convergence\" of better labels leading to better trained CNN\nmodels which in turn feed more effective deep image features to facilitate more\nmeaningful clustering/labels. We have empirically validated the convergence and\ndemonstrated promising quantitative and qualitative results. Category labels of\nsignificantly higher quality than those in previous work are discovered. This\nallows for further investigation of the hierarchical semantic nature of the\ngiven large-scale radiology image database.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 17:16:00 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Wang", "Xiaosong", ""], ["Lu", "Le", ""], ["Shin", "Hoo-chang", ""], ["Kim", "Lauren", ""], ["Nogues", "Isabella", ""], ["Yao", "Jianhua", ""], ["Summers", "Ronald", ""]]}, {"id": "1603.07998", "submitter": "Hang Zhang", "authors": "Hang Zhang, Kristin Dana and Ko Nishino", "title": "Friction from Reflectance: Deep Reflectance Codes for Predicting\n  Physical Surface Properties from One-Shot In-Field Reflectance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are the standard input for vision algorithms, but one-shot infield\nreflectance measurements are creating new opportunities for recognition and\nscene understanding. In this work, we address the question of what reflectance\ncan reveal about materials in an efficient manner. We go beyond the question of\nrecognition and labeling and ask the question: What intrinsic physical\nproperties of the surface can be estimated using reflectance? We introduce a\nframework that enables prediction of actual friction values for surfaces using\none-shot reflectance measurements. This work is a first of its kind\nvision-based friction estimation. We develop a novel representation for\nreflectance disks that capture partial BRDF measurements instantaneously. Our\nmethod of deep reflectance codes combines CNN features and fisher vector\npooling with optimal binary embedding to create codes that have sufficient\ndiscriminatory power and have important properties of illumination and spatial\ninvariance. The experimental results demonstrate that reflectance can play a\nnew role in deciphering the underlying physical properties of real-world\nscenes.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 19:57:03 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 20:36:37 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Zhang", "Hang", ""], ["Dana", "Kristin", ""], ["Nishino", "Ko", ""]]}, {"id": "1603.08029", "submitter": "Diogo Almeida", "authors": "Sasha Targ, Diogo Almeida, Kevin Lyman", "title": "Resnet in Resnet: Generalizing Residual Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) have recently achieved state-of-the-art on\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\ndual-stream architecture that generalizes ResNets and standard CNNs and is\neasily implemented with no computational overhead. RiR consistently improves\nperformance over ResNets, outperforms architectures with similar amounts of\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:55:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Targ", "Sasha", ""], ["Almeida", "Diogo", ""], ["Lyman", "Kevin", ""]]}, {"id": "1603.08039", "submitter": "Wen-Sheng Chu", "authors": "Zhuo Hui and Wen-Sheng Chu", "title": "An Empirical Study of Dimensional Reduction Techniques for Facial Action\n  Units Detection", "comments": "Report on DR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologically inspired features, such as Gabor filters, result in very high\ndimensional measurement. Does reducing the dimensionality of the feature space\nafford advantages beyond computational efficiency? Do some approaches to\ndimensionality reduction (DR) yield improved action unit detection? To answer\nthese questions, we compared DR approaches in two relatively large databases of\nspontaneous facial behavior (45 participants in total with over 2 minutes of\nFACS-coded video per participant). Facial features were tracked and aligned\nusing active appearance models (AAM). SIFT and Gabor features were extracted\nfrom local facial regions. We compared linear (PCA and KPCA), manifold (LPP and\nLLE), supervised (LDA and KDA) and hybrid approaches (LSDA) to DR with respect\nto AU detection. For further comparison, a no-DR control condition was included\nas well. Linear support vector machine classifiers with independent train and\ntest sets were used for AU detection. AU detection was quantified using area\nunder the ROC curve and F1. Baseline results for PCA with Gabor features were\ncomparable with previous research. With some notable exceptions, DR improved AU\ndetection relative to no-DR. Locality embedding approaches proved vulnerable to\n\\emph{out-of-sample} problems. Gradient-based SIFT lead to better AU detection\nthan the filter-based Gabor features. For area under the curve, few differences\nwere found between linear and other DR approaches. For F1, results were mixed.\nFor both metrics, the pattern of results varied among action units. These\nfindings suggest that action unit detection may be optimized by using specific\nDR for specific action units. PCA and LDA were the most efficient approaches;\nKDA was the least efficient.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:27:31 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Hui", "Zhuo", ""], ["Chu", "Wen-Sheng", ""]]}, {"id": "1603.08067", "submitter": "Bo Li", "authors": "Bo Li and Tianfu Wu and Caiming Xiong and Song-Chun Zhu", "title": "Recognizing Car Fluents from Video", "comments": "Accepted by CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical fluents, a term originally used by Newton [40], refers to\ntime-varying object states in dynamic scenes. In this paper, we are interested\nin inferring the fluents of vehicles from video. For example, a door (hood,\ntrunk) is open or closed through various actions, light is blinking to turn.\nRecognizing these fluents has broad applications, yet have received scant\nattention in the computer vision literature. Car fluent recognition entails a\nunified framework for car detection, car part localization and part status\nrecognition, which is made difficult by large structural and appearance\nvariations, low resolutions and occlusions. This paper learns a\nspatial-temporal And-Or hierarchical model to represent car fluents. The\nlearning of this model is formulated under the latent structural SVM framework.\nSince there are no publicly related dataset, we collect and annotate a car\nfluent dataset consisting of car videos with diverse fluents. In experiments,\nthe proposed method outperforms several highly related baseline methods in\nterms of car fluent recognition and car part localization.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 03:45:00 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Li", "Bo", ""], ["Wu", "Tianfu", ""], ["Xiong", "Caiming", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1603.08070", "submitter": "Sohini Roychowdhury", "authors": "Matthew Bihis, Sohini Roychowdhury", "title": "A generalized flow for multi-class and binary classification tasks: An\n  Azure ML approach", "comments": "10 pages, 7 figures, Conference", "journal-ref": "Big Data (Big Data), 2015 IEEE International Conference on, Santa\n  Clara, CA, 2015, pp. 1728-1737", "doi": "10.1109/BigData.2015.7363944", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constant growth in the present day real-world databases pose\ncomputational challenges for a single computer. Cloud-based platforms, on the\nother hand, are capable of handling large volumes of information manipulation\ntasks, thereby necessitating their use for large real-world data set\ncomputations. This work focuses on creating a novel Generalized Flow within the\ncloud-based computing platform: Microsoft Azure Machine Learning Studio (MAMLS)\nthat accepts multi-class and binary classification data sets alike and\nprocesses them to maximize the overall classification accuracy. First, each\ndata set is split into training and testing data sets, respectively. Then,\nlinear and nonlinear classification model parameters are estimated using the\ntraining data set. Data dimensionality reduction is then performed to maximize\nclassification accuracy. For multi-class data sets, data centric information is\nused to further improve overall classification accuracy by reducing the\nmulti-class classification to a series of hierarchical binary classification\ntasks. Finally, the performance of optimized classification model thus achieved\nis evaluated and scored on the testing data set. The classification\ncharacteristics of the proposed flow are comparatively evaluated on 3 public\ndata sets and a local data set with respect to existing state-of-the-art\nmethods. On the 3 public data sets, the proposed flow achieves 78-97.5%\nclassification accuracy. Also, the local data set, created using the\ninformation regarding presence of Diabetic Retinopathy lesions in fundus\nimages, results in 85.3-95.7% average classification accuracy, which is higher\nthan the existing methods. Thus, the proposed generalized flow can be useful\nfor a wide range of application-oriented \"big data sets\".\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 03:55:53 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Bihis", "Matthew", ""], ["Roychowdhury", "Sohini", ""]]}, {"id": "1603.08071", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury", "title": "Classification of Large-Scale Fundus Image Data Sets: A Cloud-Computing\n  Framework", "comments": "4 pages, 6 figures, [Submitted], 38th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society 2016", "journal-ref": null, "doi": "10.1109/EMBC.2016.7591423", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large medical image data sets with high dimensionality require substantial\namount of computation time for data creation and data processing. This paper\npresents a novel generalized method that finds optimal image-based feature sets\nthat reduce computational time complexity while maximizing overall\nclassification accuracy for detection of diabetic retinopathy (DR). First,\nregion-based and pixel-based features are extracted from fundus images for\nclassification of DR lesions and vessel-like structures. Next, feature ranking\nstrategies are used to distinguish the optimal classification feature sets. DR\nlesion and vessel classification accuracies are computed using the boosted\ndecision tree and decision forest classifiers in the Microsoft Azure Machine\nLearning Studio platform, respectively. For images from the DIARETDB1 data set,\n40 of its highest-ranked features are used to classify four DR lesion types\nwith an average classification accuracy of 90.1% in 792 seconds. Also, for\nclassification of red lesion regions and hemorrhages from microaneurysms,\naccuracies of 85% and 72% are observed, respectively. For images from STARE\ndata set, 40 high-ranked features can classify minor blood vessels with an\naccuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysis\nsystems can significantly enhance the borderline classification performances in\nautomated screening systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 04:07:30 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Roychowdhury", "Sohini", ""]]}, {"id": "1603.08079", "submitter": "Andrei Barbu", "authors": "Yevgeni Berzak and Andrei Barbu and Daniel Harari and Boris Katz and\n  Shimon Ullman", "title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "comments": "EMNLP 2015", "journal-ref": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2015, pages 1477--1487", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding language goes hand in hand with the ability to integrate\ncomplex contextual information obtained via perception. In this work, we\npresent a novel task for grounded language understanding: disambiguating a\nsentence given a visual scene which depicts one of the possible interpretations\nof that sentence. To this end, we introduce a new multimodal corpus containing\nambiguous sentences, representing a wide range of syntactic, semantic and\ndiscourse ambiguities, coupled with videos that visualize the different\ninterpretations for each sentence. We address this task by extending a vision\nmodel which determines if a sentence is depicted by a video. We demonstrate how\nsuch a model can be adjusted to recognize different interpretations of the same\nunderlying sentence, allowing to disambiguate sentences in a unified fashion\nacross the different ambiguity types.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 06:49:33 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Barbu", "Andrei", ""], ["Harari", "Daniel", ""], ["Katz", "Boris", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.08081", "submitter": "Kunal Narayan Chaudhury", "authors": "Sanjay Ghosh and Kunal N. Chaudhury", "title": "On Fast Bilateral Filtering using Fourier Kernels", "comments": "To appear in IEEE Signal Processing Letters (5 pages, 3 figures)", "journal-ref": null, "doi": "10.1109/LSP.2016.2539982", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was demonstrated in earlier work that, by approximating its range kernel\nusing shiftable functions, the non-linear bilateral filter can be computed\nusing a series of fast convolutions. Previous approaches based on shiftable\napproximation have, however, been restricted to Gaussian range kernels. In this\nwork, we propose a novel approximation that can be applied to any range kernel,\nprovided it has a pointwise-convergent Fourier series. More specifically, we\npropose to approximate the Gaussian range kernel of the bilateral filter using\na Fourier basis, where the coefficients of the basis are obtained by solving a\nseries of least-squares problems. The coefficients can be efficiently computed\nusing a recursive form of the QR decomposition. By controlling the cardinality\nof the Fourier basis, we can obtain a good tradeoff between the run-time and\nthe filtering accuracy. In particular, we are able to guarantee sub-pixel\naccuracy for the overall filtering, which is not provided by most existing\nmethods for fast bilateral filtering. We present simulation results to\ndemonstrate the speed and accuracy of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 07:09:58 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ghosh", "Sanjay", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1603.08092", "submitter": "Jianyu Tang", "authors": "Jianyu Tang, Hanzi Wang and Yan Yan", "title": "Learning Hough Regression Models via Bridge Partial Least Squares for\n  Object Detection", "comments": null, "journal-ref": "Neurocomputing, 2015,152(3):236-249", "doi": "10.1016/j.neucom.2014.10.071", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular Hough Transform-based object detection approaches usually construct\nan appearance codebook by clustering local image features. However, how to\nchoose appropriate values for the parameters used in the clustering step\nremains an open problem. Moreover, some popular histogram features extracted\nfrom overlapping image blocks may cause a high degree of redundancy and\nmulticollinearity. In this paper, we propose a novel Hough Transform-based\nobject detection approach. First, to address the above issues, we exploit a\nBridge Partial Least Squares (BPLS) technique to establish context-encoded\nHough Regression Models (HRMs), which are linear regression models that cast\nprobabilistic Hough votes to predict object locations. BPLS is an efficient\nvariant of Partial Least Squares (PLS). PLS-based regression techniques\n(including BPLS) can reduce the redundancy and eliminate the multicollinearity\nof a feature set. And the appropriate value of the only parameter used in PLS\n(i.e., the number of latent components) can be determined by using a\ncross-validation procedure. Second, to efficiently handle object scale changes,\nwe propose a novel multi-scale voting scheme. In this scheme, multiple Hough\nimages corresponding to multiple object scales can be obtained simultaneously.\nThird, an object in a test image may correspond to multiple true and false\npositive hypotheses at different scales. Based on the proposed multi-scale\nvoting scheme, a principled strategy is proposed to fuse hypotheses to reduce\nfalse positives by evaluating normalized pointwise mutual information between\nhypotheses. In the experiments, we also compare the proposed HRM approach with\nits several variants to evaluate the influences of its components on its\nperformance. Experimental results show that the proposed HRM approach has\nachieved desirable performances on popular benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 09:33:30 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Tang", "Jianyu", ""], ["Wang", "Hanzi", ""], ["Yan", "Yan", ""]]}, {"id": "1603.08095", "submitter": "Vania Estrela Dr.", "authors": "Felipe P. do Carmo, Joaquim T. de Assis, Vania V. Estrela, Alessandra\n  M. Coelho", "title": "Blind signal separation and identification of mixtures of images", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/ACSSC.2009", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a fresh procedure to handle image mixtures by means of blind\nsignal separation relying on a combination of second order and higher order\nstatistics techniques are introduced. The problem of blind signal separation is\nreassigned to the wavelet domain. The key idea behind this method is that the\nimage mixture can be decomposed into the sum of uncorrelated and/or independent\nsub-bands using wavelet transform. Initially, the observed image is\npre-whitened in the space domain. Afterwards, an initial separation matrix is\nestimated from the second order statistics de-correlation model in the wavelet\ndomain. Later, this matrix will be used as an initial separation matrix for the\nhigher order statistics stage in order to find the best separation matrix. The\nsuggested algorithm was tested using natural images.Experiments have confirmed\nthat the use of the proposed process provides promising outcomes in identifying\nan image from noisy mixtures of images.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 10:04:41 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Carmo", "Felipe P. do", ""], ["de Assis", "Joaquim T.", ""], ["Estrela", "Vania V.", ""], ["Coelho", "Alessandra M.", ""]]}, {"id": "1603.08105", "submitter": "Ayush Mittal", "authors": "Ayush Mittal, Anant Raj, Vinay P. Namboodiri and Tinne Tuytelaars", "title": "Unsupervised Domain Adaptation in the Wild: Dealing with Asymmetric\n  Label Sets", "comments": "supplementary material:\n  http://home.iitk.ac.in/~ayushmi/supplementary-material-unsupervised.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of domain adaptation is to adapt models learned on a source domain\nto a particular target domain. Most methods for unsupervised domain adaptation\nproposed in the literature to date, assume that the set of classes present in\nthe target domain is identical to the set of classes present in the source\ndomain. This is a restrictive assumption that limits the practical\napplicability of unsupervised domain adaptation techniques in real world\nsettings (\"in the wild\"). Therefore, we relax this constraint and propose a\ntechnique that allows the set of target classes to be a subset of the source\nclasses. This way, large publicly available annotated datasets with a wide\nvariety of classes can be used as source, even if the actual set of classes in\ntarget can be more limited and, maybe most importantly, unknown beforehand.\n  To this end, we propose an algorithm that orders a set of source subspaces\nthat are relevant to the target classification problem. Our method then chooses\na restricted set from this ordered set of source subspaces. As an extension,\neven starting from multiple source datasets with varied sets of categories,\nthis method automatically selects an appropriate subset of source categories\nrelevant to a target dataset. Empirical analysis on a number of source and\ntarget domain datasets shows that restricting the source subspace to only a\nsubset of categories does indeed substantially improve the eventual target\nclassification accuracy over the baseline that considers all source classes.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 13:22:55 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Mittal", "Ayush", ""], ["Raj", "Anant", ""], ["Namboodiri", "Vinay P.", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1603.08108", "submitter": "Yilun Wang", "authors": "Liangtian He, Yilun Wang and Zhaoyin Xiang", "title": "Support Driven Wavelet Frame-based Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet frame systems have been playing an active role in image\nrestoration and many other image processing fields over the past decades, owing\nto the good capability of sparsely approximating piece-wise smooth functions\nsuch as images. In this paper, we propose a novel wavelet frame based sparse\nrecovery model called \\textit{Support Driven Sparse Regularization} (SDSR) for\nimage deblurring, where the partial support information of frame coefficients\nis attained via a self-learning strategy and exploited via the proposed\ntruncated $\\ell_0$ regularization. Moreover, the state-of-the-art image\nrestoration methods can be naturally incorporated into our proposed wavelet\nframe based sparse recovery framework. In particular, in order to achieve\nreliable support estimation of the frame coefficients, we make use of the\nstate-of-the-art image restoration result such as that from the IDD-BM3D method\nas the initial reference image for support estimation. Our extensive\nexperimental results have shown convincing improvements over existing\nstate-of-the-art deblurring methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 13:45:01 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["He", "Liangtian", ""], ["Wang", "Yilun", ""], ["Xiang", "Zhaoyin", ""]]}, {"id": "1603.08109", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury and Swapnil D. Dabhade", "title": "Fast and Provably Accurate Bilateral Filtering", "comments": "To appear in IEEE Transactions on Image Processing (10 pages, 10\n  figures, 4 tables)", "journal-ref": null, "doi": "10.1109/TIP.2016.2548363", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral filter is a non-linear filter that uses a range filter along\nwith a spatial filter to perform edge-preserving smoothing of images. A direct\ncomputation of the bilateral filter requires $O(S)$ operations per pixel, where\n$S$ is the size of the support of the spatial filter. In this paper, we present\na fast and provably accurate algorithm for approximating the bilateral filter\nwhen the range kernel is Gaussian. In particular, for box and Gaussian spatial\nfilters, the proposed algorithm can cut down the complexity to $O(1)$ per pixel\nfor any arbitrary $S$. The algorithm has a simple implementation involving\n$N+1$ spatial filterings, where $N$ is the approximation order. We give a\ndetailed analysis of the filtering accuracy that can be achieved by the\nproposed approximation in relation to the target bilateral filter. This allows\nus to to estimate the order $N$ required to obtain a given accuracy. We also\npresent comprehensive numerical results to demonstrate that the proposed\nalgorithm is competitive with state-of-the-art methods in terms of speed and\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 14:05:34 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chaudhury", "Kunal N.", ""], ["Dabhade", "Swapnil D.", ""]]}, {"id": "1603.08120", "submitter": "Wenbin Li", "authors": "Wenbin Li and Darren Cosker and Zhihan Lv and Matthew Brown", "title": "Nonrigid Optical Flow Ground Truth for Real-World Scenes with\n  Time-Varying Shading Effects", "comments": "preprint of our paper accepted by RA-L'16", "journal-ref": null, "doi": "10.1109/LRA.2016.2592513", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present a dense ground truth dataset of nonrigidly deforming\nreal-world scenes. Our dataset contains both long and short video sequences,\nand enables the quantitatively evaluation for RGB based tracking and\nregistration methods. To construct ground truth for the RGB sequences, we\nsimultaneously capture Near-Infrared (NIR) image sequences where dense markers\n- visible only in NIR - represent ground truth positions. This allows for\ncomparison with automatically tracked RGB positions and the formation of error\nmetrics. Most previous datasets containing nonrigidly deforming sequences are\nbased on synthetic data. Our capture protocol enables us to acquire real-world\ndeforming objects with realistic photometric effects - such as blur and\nillumination change - as well as occlusion and complex deformations. A public\nevaluation website is constructed to allow for ranking of RGB image based\noptical flow and other dense tracking algorithms, with various statistical\nmeasures. Furthermore, we present an RGB-NIR multispectral optical flow model\nallowing for energy optimization by adoptively combining featured information\nfrom both the RGB and the complementary NIR channels. In our experiments we\nevaluate eight existing RGB based optical flow methods on our new dataset. We\nalso evaluate our hybrid optical flow algorithm by comparing to two existing\nmultispectral approaches, as well as varying our input channels across RGB, NIR\nand RGB-NIR.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 16:08:13 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 14:57:38 GMT"}, {"version": "v3", "created": "Fri, 15 Jul 2016 12:39:03 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Li", "Wenbin", ""], ["Cosker", "Darren", ""], ["Lv", "Zhihan", ""], ["Brown", "Matthew", ""]]}, {"id": "1603.08124", "submitter": "Wenbin Li", "authors": "Wenbin Li, Darren Cosker", "title": "Video Interpolation using Optical Flow and Laplacian Smoothness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-rigid video interpolation is a common computer vision task. In this paper\nwe present an optical flow approach which adopts a Laplacian Cotangent Mesh\nconstraint to enhance the local smoothness. Similar to Li et al., our approach\nadopts a mesh to the image with a resolution up to one vertex per pixel and\nuses angle constraints to ensure sensible local deformations between image\npairs. The Laplacian Mesh constraints are expressed wholly inside the optical\nflow optimization, and can be applied in a straightforward manner to a wide\nrange of image tracking and registration problems. We evaluate our approach by\ntesting on several benchmark datasets, including the Middlebury and Garg et al.\ndatasets. In addition, we show application of our method for constructing 3D\nMorphable Facial Models from dynamic 3D data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 17:13:25 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Li", "Wenbin", ""], ["Cosker", "Darren", ""]]}, {"id": "1603.08152", "submitter": "Yair Movshovitz-Attias", "authors": "Yair Movshovitz-Attias, Takeo Kanade, Yaser Sheikh", "title": "How useful is photo-realistic rendering for visual learning?", "comments": "Published in GMDL 2016 In conjunction with ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data seems cheap to get, and in many ways it is, but the process of creating\na high quality labeled dataset from a mass of data is time-consuming and\nexpensive.\n  With the advent of rich 3D repositories, photo-realistic rendering systems\noffer the opportunity to provide nearly limitless data. Yet, their primary\nvalue for visual learning may be the quality of the data they can provide\nrather than the quantity. Rendering engines offer the promise of perfect labels\nin addition to the data: what the precise camera pose is; what the precise\nlighting location, temperature, and distribution is; what the geometry of the\nobject is.\n  In this work we focus on semi-automating dataset creation through use of\nsynthetic data and apply this method to an important task -- object viewpoint\nestimation. Using state-of-the-art rendering software we generate a large\nlabeled dataset of cars rendered densely in viewpoint space. We investigate the\neffect of rendering parameters on estimation performance and show realism is\nimportant. We show that generalizing from synthetic data is not harder than the\ndomain adaptation required between two real-image datasets and that combining\nsynthetic images with a small amount of real data improves estimation accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:56:53 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 03:43:58 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Movshovitz-Attias", "Yair", ""], ["Kanade", "Takeo", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1603.08155", "submitter": "Justin Johnson", "authors": "Justin Johnson, Alexandre Alahi, Li Fei-Fei", "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 01:04:27 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Johnson", "Justin", ""], ["Alahi", "Alexandre", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1603.08161", "submitter": "Matthias Nie{\\ss}ner", "authors": "Matthias Innmann, Michael Zollh\\\"ofer, Matthias Nie{\\ss}ner, Christian\n  Theobalt, Marc Stamminger", "title": "VolumeDeform: Real-time Volumetric Non-rigid Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the reconstruction of dynamic geometric\nshapes using a single hand-held consumer-grade RGB-D sensor at real-time rates.\nOur method does not require a pre-defined shape template to start with and\nbuilds up the scene model from scratch during the scanning process. Geometry\nand motion are parameterized in a unified manner by a volumetric representation\nthat encodes a distance field of the surface geometry as well as the non-rigid\nspace deformation. Motion tracking is based on a set of extracted sparse color\nfeatures in combination with a dense depth-based constraint formulation. This\nenables accurate tracking and drastically reduces drift inherent to standard\nmodel-to-depth alignment. We cast finding the optimal deformation of space as a\nnon-linear regularized variational optimization problem by enforcing local\nsmoothness and proximity to the input constraints. The problem is tackled in\nreal-time at the camera's capture rate using a data-parallel flip-flop\noptimization strategy. Our results demonstrate robust tracking even for fast\nmotion and scenes that lack geometric features.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 02:09:03 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 06:07:24 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Innmann", "Matthias", ""], ["Zollh\u00f6fer", "Michael", ""], ["Nie\u00dfner", "Matthias", ""], ["Theobalt", "Christian", ""], ["Stamminger", "Marc", ""]]}, {"id": "1603.08182", "submitter": "Andy Zeng", "authors": "Andy Zeng, Shuran Song, Matthias Nie{\\ss}ner, Matthew Fisher,\n  Jianxiong Xiao, Thomas Funkhouser", "title": "3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions", "comments": "To appear at the Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2017. Project webpage: http://3dmatch.cs.princeton.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching local geometric features on real-world depth images is a challenging\ntask due to the noisy, low-resolution, and incomplete nature of 3D scan data.\nThese difficulties limit the performance of current state-of-art methods, which\nare typically based on histograms over geometric properties. In this paper, we\npresent 3DMatch, a data-driven model that learns a local volumetric patch\ndescriptor for establishing correspondences between partial 3D data. To amass\ntraining data for our model, we propose a self-supervised feature learning\nmethod that leverages the millions of correspondence labels found in existing\nRGB-D reconstructions. Experiments show that our descriptor is not only able to\nmatch local geometry in new scenes for reconstruction, but also generalize to\ndifferent tasks and spatial scales (e.g. instance-level object model alignment\nfor the Amazon Picking Challenge, and mesh surface correspondence). Results\nshow that 3DMatch consistently outperforms other state-of-the-art approaches by\na significant margin. Code, data, benchmarks, and pre-trained models are\navailable online at http://3dmatch.cs.princeton.edu\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 06:43:52 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 02:17:07 GMT"}, {"version": "v3", "created": "Sun, 9 Apr 2017 19:56:05 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Nie\u00dfner", "Matthias", ""], ["Fisher", "Matthew", ""], ["Xiao", "Jianxiong", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1603.08199", "submitter": "Loris Bazzani", "authors": "Loris Bazzani and Hugo Larochelle and Lorenzo Torresani", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "comments": "ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many computer vision tasks, the relevant information to solve the problem\nat hand is mixed to irrelevant, distracting information. This has motivated\nresearchers to design attentional models that can dynamically focus on parts of\nimages or videos that are salient, e.g., by down-weighting irrelevant pixels.\nIn this work, we propose a spatiotemporal attentional model that learns where\nto look in a video directly from human fixation data. We model visual attention\nwith a mixture of Gaussians at each frame. This distribution is used to express\nthe probability of saliency for each pixel. Time consistency in videos is\nmodeled hierarchically by: 1) deep 3D convolutional features to represent\nspatial and short-term time relations and 2) a long short-term memory network\non top that aggregates the clip-level representation of sequential clips and\ntherefore expands the temporal domain from few frames to seconds. The\nparameters of the proposed model are optimized via maximum likelihood\nestimation using human fixations as training data, without knowledge of the\naction in each video. Our experiments on Hollywood2 show state-of-the-art\nperformance on saliency prediction for video. We also show that our attentional\nmodel trained on Hollywood2 generalizes well to UCF101 and it can be leveraged\nto improve action classification accuracy on both datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 10:34:22 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 14:17:51 GMT"}, {"version": "v3", "created": "Sun, 15 May 2016 11:55:35 GMT"}, {"version": "v4", "created": "Sat, 11 Feb 2017 10:05:06 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Bazzani", "Loris", ""], ["Larochelle", "Hugo", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1603.08212", "submitter": "Ethan Fetaya", "authors": "Ita Lifshitz, Ethan Fetaya and Shimon Ullman", "title": "Human Pose Estimation using Deep Consensus Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of human pose estimation from a single\nstill image. We propose a novel approach where each location in the image votes\nfor the position of each keypoint using a convolutional neural net. The voting\nscheme allows us to utilize information from the whole image, rather than rely\non a sparse set of keypoint locations. Using dense, multi-target votes, not\nonly produces good keypoint predictions, but also enables us to compute\nimage-dependent joint keypoint probabilities by looking at consensus voting.\nThis differs from most previous methods where joint probabilities are learned\nfrom relative keypoint locations and are independent of the image. We finally\ncombine the keypoints votes and joint probabilities in order to identify the\noptimal pose configuration. We show our competitive performance on the MPII\nHuman Pose and Leeds Sports Pose datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 12:45:33 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Lifshitz", "Ita", ""], ["Fetaya", "Ethan", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.08233", "submitter": "Randal Olson", "authors": "Randal S. Olson, Jason H. Moore, Christoph Adami", "title": "Evolution of active categorical image classification via saccadic eye\n  movement", "comments": "10 pages, 5 figures, to appear in PPSN 2016 conference proceedings", "journal-ref": "Lecture Notes in Computer Science 9921 (2016) 581-590", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition and classification is a central concern for modern\ninformation processing systems. In particular, one key challenge to image and\nvideo classification has been that the computational cost of image processing\nscales linearly with the number of pixels in the image or video. Here we\npresent an intelligent machine (the \"active categorical classifier,\" or ACC)\nthat is inspired by the saccadic movements of the eye, and is capable of\nclassifying images by selectively scanning only a portion of the image. We\nharness evolutionary computation to optimize the ACC on the MNIST hand-written\ndigit classification task, and provide a proof-of-concept that the ACC works on\nnoisy multi-class data. We further analyze the ACC and demonstrate its ability\nto classify images after viewing only a fraction of the pixels, and provide\ninsight on future research paths to further improve upon the ACC presented\nhere.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 16:36:43 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 21:00:53 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""], ["Adami", "Christoph", ""]]}, {"id": "1603.08240", "submitter": "Konstantinos Rematas", "authors": "Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario\n  Fritz, Luc Van Gool and Tinne Tuytelaars", "title": "DeLight-Net: Decomposing Reflectance Maps into Specular Materials and\n  Natural Illumination", "comments": "Stamatios Georgoulis and Konstantinos Rematas contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are extracting surface reflectance and natural environmental\nillumination from a reflectance map, i.e. from a single 2D image of a sphere of\none material under one illumination. This is a notoriously difficult problem,\nyet key to various re-rendering applications. With the recent advances in\nestimating reflectance maps from 2D images their further decomposition has\nbecome increasingly relevant.\n  To this end, we propose a Convolutional Neural Network (CNN) architecture to\nreconstruct both material parameters (i.e. Phong) as well as illumination (i.e.\nhigh-resolution spherical illumination maps), that is solely trained on\nsynthetic data. We demonstrate that decomposition of synthetic as well as real\nphotographs of reflectance maps, both in High Dynamic Range (HDR), and, for the\nfirst time, on Low Dynamic Range (LDR) as well. Results are compared to\nprevious approaches quantitatively as well as qualitatively in terms of\nre-renderings where illumination, material, view or shape are changed.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 18:03:28 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Georgoulis", "Stamatios", ""], ["Rematas", "Konstantinos", ""], ["Ritschel", "Tobias", ""], ["Fritz", "Mario", ""], ["Van Gool", "Luc", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1603.08321", "submitter": "Linlin Chao", "authors": "Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li and Zhengqi Wen", "title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on two key problems for audio-visual emotion recognition\nin the video. One is the audio and visual streams temporal alignment for\nfeature level fusion. The other one is locating and re-weighting the perception\nattentions in the whole audio-visual stream for better recognition. The Long\nShort Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main\nclassification architecture. Firstly, soft attention mechanism aligns the audio\nand visual streams. Secondly, seven emotion embedding vectors, which are\ncorresponding to each classification emotion type, are added to locate the\nperception attentions. The locating and re-weighting process is also based on\nthe soft attention mechanism. The experiment results on EmotiW2015 dataset and\nthe qualitative analysis show the efficiency of the proposed two techniques.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 06:06:10 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Chao", "Linlin", ""], ["Tao", "Jianhua", ""], ["Yang", "Minghao", ""], ["Li", "Ya", ""], ["Wen", "Zhengqi", ""]]}, {"id": "1603.08323", "submitter": "{\\L}ukasz Olech Piotr", "authors": "Micha{\\l} Spytkowski, {\\L}ukasz P. Olech, Halina Kwa\\'snicka", "title": "Hierarchy of Groups Evaluation Using Different F-score Variants", "comments": "Presented on ACIIDS2016 conference https://aciids.pwr.edu.pl/. The\n  final publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-662-49381-6_63", "journal-ref": "ACIIDS 2016, Da Nang, Vietnam, March 14-16, 2016, pp. 654\n  (Springer Berlin Heidelberg)", "doi": "10.1007/978-3-662-49381-6_63", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a cursory examination of clustering, focusing on a rarely\nexplored field of hierarchy of clusters. Based on this, a short discussion of\nclustering quality measures is presented and the F-score measure is examined\nmore deeply. As there are no attempts to assess the quality for hierarchies of\nclusters, three variants of the F-Score based index are presented: classic,\nhierarchical and partial order. The partial order index is the authors'\napproach to the subject. Conducted experiments show the properties of the\nconsidered measures. In conclusions, the strong and weak sides of each variant\nare presented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 06:38:56 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Spytkowski", "Micha\u0142", ""], ["Olech", "\u0141ukasz P.", ""], ["Kwa\u015bnicka", "Halina", ""]]}, {"id": "1603.08328", "submitter": "Tatsunori Taniai", "authors": "Tatsunori Taniai, Yasuyuki Matsushita, Yoichi Sato and Takeshi Naemura", "title": "Continuous 3D Label Stereo Matching using Local Expansion Moves", "comments": "14 pages. An extended version of our preliminary conference paper\n  [39], Taniai et al. \"Graph Cut based Continuous Stereo Matching using Locally\n  Shared Labels\" in the proceedings of IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR 2014). Our results were submitted to Middlebury\n  Stereo Benchmark Version 2 on April 22, 2015, and to Version 3 on July 4,\n  2017", "journal-ref": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 11, pp.\n  2725-2739, 2018", "doi": "10.1109/TPAMI.2017.2766072", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an accurate stereo matching method using local expansion moves\nbased on graph cuts. This new move-making scheme is used to efficiently infer\nper-pixel 3D plane labels on a pairwise Markov random field (MRF) that\neffectively combines recently proposed slanted patch matching and curvature\nregularization terms. The local expansion moves are presented as many\nalpha-expansions defined for small grid regions. The local expansion moves\nextend traditional expansion moves by two ways: localization and spatial\npropagation. By localization, we use different candidate alpha-labels according\nto the locations of local alpha-expansions. By spatial propagation, we design\nour local alpha-expansions to propagate currently assigned labels for nearby\nregions. With this localization and spatial propagation, our method can\nefficiently infer MRF models with a continuous label space using randomized\nsearch. Our method has several advantages over previous approaches that are\nbased on fusion moves or belief propagation; it produces submodular moves\nderiving a subproblem optimality; it helps find good, smooth, piecewise linear\ndisparity maps; it is suitable for parallelization; it can use cost-volume\nfiltering techniques for accelerating the matching cost computations. Even\nusing a simple pairwise MRF, our method is shown to have best performance in\nthe Middlebury stereo benchmark V2 and V3.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 07:27:49 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 09:33:03 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 10:31:50 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Taniai", "Tatsunori", ""], ["Matsushita", "Yasuyuki", ""], ["Sato", "Yoichi", ""], ["Naemura", "Takeshi", ""]]}, {"id": "1603.08342", "submitter": "{\\L}ukasz Olech Piotr", "authors": "{\\L}ukasz P. Olech and Mariusz Paradowski", "title": "Hierarchical Gaussian Mixture Model with Objects Attached to Terminal\n  and Non-terminal Dendrogram Nodes", "comments": "This article was presented on CORES2015 conference\n  http://cores.pwr.wroc.pl/ . The final publication is available at Springer\n  via http://dx.doi.org/10.1007/978-3-319-26227-7_18", "journal-ref": "Proceedings of the CORES 2015 conf., pp. 191-201. Springer\n  International Publishing, Cham (2016)", "doi": "10.1007/978-3-319-26227-7_18", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hierarchical clustering algorithm based on Gaussian mixture model is\npresented. The key difference to regular hierarchical mixture models is the\nability to store objects in both terminal and nonterminal nodes. Upper levels\nof the hierarchy contain sparsely distributed objects, while lower levels\ncontain densely represented ones. As it was shown by experiments, this ability\nhelps in noise detection (modelling). Furthermore, compared to regular\nhierarchical mixture model, the presented method generates more compact\ndendrograms with higher quality measured by adopted F-measure.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 08:54:03 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Olech", "\u0141ukasz P.", ""], ["Paradowski", "Mariusz", ""]]}, {"id": "1603.08358", "submitter": "Siddhartha Chandra", "authors": "Siddhartha Chandra and Iasonas Kokkinos", "title": "Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation\n  with Deep Gaussian CRFs", "comments": "Our code is available at https://github.com/siddharthachandra/gcrf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a structured prediction technique that combines the\nvirtues of Gaussian Conditional Random Fields (G-CRF) with Deep Learning: (a)\nour structured prediction task has a unique global optimum that is obtained\nexactly from the solution of a linear system (b) the gradients of our model\nparameters are analytically computed using closed form expressions, in contrast\nto the memory-demanding contemporary deep structured prediction approaches that\nrely on back-propagation-through-time, (c) our pairwise terms do not have to be\nsimple hand-crafted expressions, as in the line of works building on the\nDenseCRF, but can rather be `discovered' from data through deep architectures,\nand (d) out system can trained in an end-to-end manner. Building on standard\ntools from numerical analysis we develop very efficient algorithms for\ninference and learning, as well as a customized technique adapted to the\nsemantic segmentation task. This efficiency allows us to explore more\nsophisticated architectures for structured prediction in deep learning: we\nintroduce multi-resolution architectures to couple information across scales in\na joint optimization framework, yielding systematic improvements. We\ndemonstrate the utility of our approach on the challenging VOC PASCAL 2012\nimage segmentation benchmark, showing substantial improvements over strong\nbaselines. We make all of our code and experiments available at\n{https://github.com/siddharthachandra/gcrf}\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 10:55:20 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 17:50:46 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 10:43:11 GMT"}, {"version": "v4", "created": "Tue, 29 Nov 2016 14:52:20 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chandra", "Siddhartha", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1603.08367", "submitter": "Markus Thom", "authors": "Markus Thom and G\\\"unther Palm", "title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "comments": "See http://jmlr.org/papers/v14/thom13a.html for the authoritative\n  version", "journal-ref": "Journal of Machine Learning Research, vol. 14, pp. 1091-1143, 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparseness is a useful regularizer for learning in a wide range of\napplications, in particular in neural networks. This paper proposes a model\ntargeted at classification tasks, where sparse activity and sparse connectivity\nare used to enhance classification capabilities. The tool for achieving this is\na sparseness-enforcing projection operator which finds the closest vector with\na pre-defined sparseness for any given vector. In the theoretical part of this\npaper, a comprehensive theory for such a projection is developed. In\nconclusion, it is shown that the projection is differentiable almost everywhere\nand can thus be implemented as a smooth neuronal transfer function. The entire\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\non the MNIST database of handwritten digits show that classification\nperformance can be boosted by sparse activity or sparse connectivity. With a\ncombination of both, performance can be significantly better compared to\nclassical non-sparse approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 12:06:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thom", "Markus", ""], ["Palm", "G\u00fcnther", ""]]}, {"id": "1603.08390", "submitter": "Jingbo Zhou", "authors": "Jingbo Zhou, Qi Guo, H. V. Jagadish, Lubo\\v{s} Kr\\v{c}\\'al, Siyuan\n  Liu, Wenhao Luan, Anthony K. H. Tung, Yueji Yang, Yuxin Zheng", "title": "A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report", "comments": "18 pages, technical report for the ICDE 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 14:44:34 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 06:05:25 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 08:49:16 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhou", "Jingbo", ""], ["Guo", "Qi", ""], ["Jagadish", "H. V.", ""], ["Kr\u010d\u00e1l", "Lubo\u0161", ""], ["Liu", "Siyuan", ""], ["Luan", "Wenhao", ""], ["Tung", "Anthony K. H.", ""], ["Yang", "Yueji", ""], ["Zheng", "Yuxin", ""]]}, {"id": "1603.08474", "submitter": "Oswaldo Ludwig", "authors": "Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens", "title": "Deep Embedding for Spatial Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:38:46 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ludwig", "Oswaldo", ""], ["Liu", "Xiao", ""], ["Kordjamshidi", "Parisa", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1603.08486", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao,\n  Ronald M Summers", "title": "Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for\n  Automated Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in automatically describing image contents, their\napplications have been mostly limited to image caption datasets containing\nnatural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep\nlearning model to efficiently detect a disease from an image and annotate its\ncontexts (e.g., location, severity and the affected organs). We employ a\npublicly available radiology dataset of chest x-rays and their reports, and use\nits image annotations to mine disease names to train convolutional neural\nnetworks (CNNs). In doing so, we adopt various regularization techniques to\ncircumvent the large normal-vs-diseased cases bias. Recurrent neural networks\n(RNNs) are then trained to describe the contexts of a detected disease, based\non the deep CNN features. Moreover, we introduce a novel approach to use the\nweights of the already trained pair of CNN/RNN on the domain-specific\nimage/text dataset, to infer the joint image/text contexts for composite image\nlabeling. Significantly improved image annotation results are demonstrated\nusing the recurrent neural cascade model by taking the joint image/text\ncontexts into account.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 19:02:07 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Roberts", "Kirk", ""], ["Lu", "Le", ""], ["Demner-Fushman", "Dina", ""], ["Yao", "Jianhua", ""], ["Summers", "Ronald M", ""]]}, {"id": "1603.08497", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (CMM), Jesus Angulo (CMM), Dominique Jeulin (CMM)", "title": "On distances, paths and connections for hyperspectral image segmentation", "comments": null, "journal-ref": "Proceedings of the 8th International Symposium on Mathematical\n  Morphology: Volume 1, pp.399-410, 2007, 978-85-17-00032-5", "doi": null, "report-no": "N-08/07/MM", "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper introduces the $\\eta$ and {\\eta} connections in order to\nadd regional information on $\\lambda$-flat zones, which only take into account\na local information. A top-down approach is considered. First $\\lambda$-flat\nzones are built in a way leading to a sub-segmentation. Then a finer\nsegmentation is obtained by computing $\\eta$-bounded regions and $\\mu$-geodesic\nballs inside the $\\lambda$-flat zones. The proposed algorithms for the\nconstruction of new partitions are based on queues with an ordered selection of\nseeds using the cumulative distance. $\\eta$-bounded regions offers a control on\nthe variations of amplitude in the class from a point, called center, and\n$\\mu$-geodesic balls controls the \"size\" of the class. These results are\napplied to hyperspectral images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 09:17:06 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Noyel", "Guillaume", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Jeulin", "Dominique", "", "CMM"]]}, {"id": "1603.08507", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue,\n  Bernt Schiele, Trevor Darrell", "title": "Generating Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 19:54:12 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Rohrbach", "Marcus", ""], ["Donahue", "Jeff", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""]]}, {"id": "1603.08511", "submitter": "Richard Zhang", "authors": "Richard Zhang, Phillip Isola, Alexei A. Efros", "title": "Colorful Image Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a \"colorization Turing test,\" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 19:58:19 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 04:11:07 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 17:58:58 GMT"}, {"version": "v4", "created": "Fri, 2 Sep 2016 05:06:19 GMT"}, {"version": "v5", "created": "Wed, 5 Oct 2016 18:01:05 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Zhang", "Richard", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1603.08561", "submitter": "Ishan Misra", "authors": "Ishan Misra and C. Lawrence Zitnick and Martial Hebert", "title": "Shuffle and Learn: Unsupervised Learning using Temporal Order\n  Verification", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for learning a visual representation\nfrom the raw spatiotemporal signals in videos. Our representation is learned\nwithout supervision from semantic labels. We formulate our method as an\nunsupervised sequential verification task, i.e., we determine whether a\nsequence of frames from a video is in the correct temporal order. With this\nsimple task and no semantic labels, we learn a powerful visual representation\nusing a Convolutional Neural Network (CNN). The representation contains\ncomplementary information to that learned from supervised image datasets like\nImageNet. Qualitative results show that our method captures information that is\ntemporally varying, such as human pose. When used as pre-training for action\nrecognition, our method gives significant gains over learning without external\ndata on benchmark datasets like UCF101 and HMDB51. To demonstrate its\nsensitivity to human pose, we show results for pose estimation on the FLIC and\nMPII datasets that are competitive, or better than approaches using\nsignificantly more supervision. Our method can be combined with supervised\nrepresentations to provide an additional boost in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:00:43 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 17:26:01 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Misra", "Ishan", ""], ["Zitnick", "C. Lawrence", ""], ["Hebert", "Martial", ""]]}, {"id": "1603.08564", "submitter": "Bodhisattwa Majumder", "authors": "Satrajit Mukherjee, Bodhisattwa Prasad Majumder, Aritran Piplai, and\n  Swagatam Das", "title": "Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image\n  Segmentation", "comments": "Journal Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel Kernelized image segmentation scheme for noisy\nimages that utilizes the concept of Smallest Univalue Segment Assimilating\nNucleus (SUSAN) and incorporates spatial constraints by computing circular\ncolour map induced weights. Fuzzy damping coefficients are obtained for each\nnucleus or center pixel on the basis of the corresponding weighted SUSAN area\nvalues, the weights being equal to the inverse of the number of horizontal and\nvertical moves required to reach a neighborhood pixel from the center pixel.\nThese weights are used to vary the contributions of the different nuclei in the\nKernel based framework. The paper also presents an edge quality metric obtained\nby fuzzy decision based edge candidate selection and final computation of the\nblurriness of the edges after their selection. The inability of existing\nalgorithms to preserve edge information and structural details in their\nsegmented maps necessitates the computation of the edge quality factor (EQF)\nfor all the competing algorithms. Qualitative and quantitative analysis have\nbeen rendered with respect to state-of-the-art algorithms and for images ridden\nwith varying types of noises. Speckle noise ridden SAR images and Rician noise\nridden Magnetic Resonance Images have also been considered for evaluating the\neffectiveness of the proposed algorithm in extracting important segmentation\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:09:52 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Mukherjee", "Satrajit", ""], ["Majumder", "Bodhisattwa Prasad", ""], ["Piplai", "Aritran", ""], ["Das", "Swagatam", ""]]}, {"id": "1603.08575", "submitter": "Th\\'eophane  Weber", "authors": "S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David\n  Szepesvari, Koray Kavukcuoglu and Geoffrey E. Hinton", "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for efficient inference in structured image models\nthat explicitly reason about objects. We achieve this by performing\nprobabilistic inference using a recurrent neural network that attends to scene\nelements and processes them one at a time. Crucially, the model itself learns\nto choose the appropriate number of inference steps. We use this scheme to\nlearn to perform inference in partially specified 2D models (variable-sized\nvariational auto-encoders) and fully specified 3D models (probabilistic\nrenderers). We show that such models learn to identify multiple objects -\ncounting, locating and classifying the elements of a scene - without any\nsupervision, e.g., decomposing 3D images with various numbers of objects in a\nsingle forward pass of a neural network. We further show that the networks\nproduce accurate inferences when compared to supervised counterparts, and that\ntheir structure leads to improved generalization.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:59:08 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 06:27:26 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 16:05:08 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Eslami", "S. M. Ali", ""], ["Heess", "Nicolas", ""], ["Weber", "Theophane", ""], ["Tassa", "Yuval", ""], ["Szepesvari", "David", ""], ["Kavukcuoglu", "Koray", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1603.08592", "submitter": "Bor-Jeng Chen", "authors": "Bor-Jeng Chen and Gerard Medioni", "title": "Exploring Local Context for Multi-target Tracking in Wide Area Aerial\n  Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking many vehicles in wide coverage aerial imagery is crucial for\nunderstanding events in a large field of view. Most approaches aim to associate\ndetections from frame differencing into tracks. However, slow or stopped\nvehicles result in long-term missing detections and further cause tracking\ndiscontinuities. Relying merely on appearance clue to recover missing\ndetections is difficult as targets are extremely small and in grayscale. In\nthis paper, we address the limitations of detection association methods by\ncoupling it with a local context tracker (LCT), which does not rely on motion\ndetections. On one hand, our LCT learns neighboring spatial relation and tracks\neach target in consecutive frames using graph optimization. It takes the\nadvantage of context constraints to avoid drifting to nearby targets. We\ngenerate hypotheses from sparse and dense flow efficiently to keep solutions\ntractable. On the other hand, we use detection association strategy to extract\nshort tracks in batch processing. We explicitly handle merged detections by\ngenerating additional hypotheses from them. Our evaluation on wide area aerial\nimagery sequences shows significant improvement over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 23:47:25 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Chen", "Bor-Jeng", ""], ["Medioni", "Gerard", ""]]}, {"id": "1603.08597", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin and Rui Zhu and Simon Lucey", "title": "The Conditional Lucas & Kanade Algorithm", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lucas & Kanade (LK) algorithm is the method of choice for efficient dense\nimage and object alignment. The approach is efficient as it attempts to model\nthe connection between appearance and geometric displacement through a linear\nrelationship that assumes independence across pixel coordinates. A drawback of\nthe approach, however, is its generative nature. Specifically, its performance\nis tightly coupled with how well the linear model can synthesize appearance\nfrom geometric displacement, even though the alignment task itself is\nassociated with the inverse problem. In this paper, we present a new approach,\nreferred to as the Conditional LK algorithm, which: (i) directly learns linear\nmodels that predict geometric displacement as a function of appearance, and\n(ii) employs a novel strategy for ensuring that the generative pixel\nindependence assumption can still be taken advantage of. We demonstrate that\nour approach exhibits superior performance to classical generative forms of the\nLK algorithm. Furthermore, we demonstrate its comparable performance to\nstate-of-the-art methods such as the Supervised Descent Method with\nsubstantially less training examples, as well as the unique ability to \"swap\"\ngeometric warp functions without having to retrain from scratch. Finally, from\na theoretical perspective, our approach hints at possible redundancies that\nexist in current state-of-the-art methods for alignment that could be leveraged\nin vision systems of the future.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 00:34:07 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Zhu", "Rui", ""], ["Lucey", "Simon", ""]]}, {"id": "1603.08631", "submitter": "Ghassem Tofighi", "authors": "Saman Sarraf and Ghassem Tofighi", "title": "Classification of Alzheimer's Disease using fMRI Data and Deep Learning\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, machine learning techniques especially predictive\nmodeling and pattern recognition in biomedical sciences from drug delivery\nsystem to medical imaging has become one of the important methods which are\nassisting researchers to have deeper understanding of entire issue and to solve\ncomplex medical problems. Deep learning is power learning machine learning\nalgorithm in classification while extracting high-level features. In this\npaper, we used convolutional neural network to classify Alzheimer's brain from\nnormal healthy brain. The importance of classifying this kind of medical data\nis to potentially develop a predict model or system in order to recognize the\ntype disease from normal subjects or to estimate the stage of the disease.\nClassification of clinical data such as Alzheimer's disease has been always\nchallenging and most problematic part has been always selecting the most\ndiscriminative features. Using Convolutional Neural Network (CNN) and the\nfamous architecture LeNet-5, we successfully classified functional MRI data of\nAlzheimer's subjects from normal controls where the accuracy of test data on\ntrained data reached 96.85%. This experiment suggests us the shift and scale\ninvariant features extracted by CNN followed by deep learning classification is\nmost powerful method to distinguish clinical data from healthy data in fMRI.\nThis approach also enables us to expand our methodology to predict more\ncomplicated systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 04:30:07 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Sarraf", "Saman", ""], ["Tofighi", "Ghassem", ""]]}, {"id": "1603.08637", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, David F. Fouhey, Mikel Rodriguez and Abhinav Gupta", "title": "Learning a Predictable and Generative Vector Representation for Objects", "comments": "To appear in ECCV 2016. Project webpage:\n  rohitgirdhar.github.io/GenerativePredictableVoxels/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is a good vector representation of an object? We believe that it should\nbe generative in 3D, in the sense that it can produce new 3D objects; as well\nas be predictable from 2D, in the sense that it can be perceived from 2D\nimages. We propose a novel architecture, called the TL-embedding network, to\nlearn an embedding space with these properties. The network consists of two\ncomponents: (a) an autoencoder that ensures the representation is generative;\nand (b) a convolutional network that ensures the representation is predictable.\nThis enables tackling a number of tasks including voxel prediction from 2D\nimages and 3D model retrieval. Extensive experimental analysis demonstrates the\nusefulness and versatility of this embedding.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 04:36:18 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 15:39:29 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Girdhar", "Rohit", ""], ["Fouhey", "David F.", ""], ["Rodriguez", "Mikel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1603.08678", "submitter": "Jifeng Dai", "authors": "Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun", "title": "Instance-sensitive Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional networks (FCNs) have been proven very successful for\nsemantic segmentation, but the FCN outputs are unaware of object instances. In\nthis paper, we develop FCNs that are capable of proposing instance-level\nsegment candidates. In contrast to the previous FCN that generates one score\nmap, our FCN is designed to compute a small set of instance-sensitive score\nmaps, each of which is the outcome of a pixel-wise classifier of a relative\nposition to instances. On top of these instance-sensitive score maps, a simple\nassembling module is able to output instance candidate at each position. In\ncontrast to the recent DeepMask method for segmenting instances, our method\ndoes not have any high-dimensional layer related to the mask resolution, but\ninstead exploits image local coherence for estimating instances. We present\ncompetitive results of instance segment proposal on both PASCAL VOC and MS\nCOCO.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 08:37:26 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Dai", "Jifeng", ""], ["He", "Kaiming", ""], ["Li", "Yi", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1603.08695", "submitter": "Piotr Doll\\'ar", "authors": "Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Doll\\`ar", "title": "Learning to Refine Object Segments", "comments": "extended version of ECCV camera-ready (figures 6-9 only in arXiv)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation requires both object-level information and low-level\npixel data. This presents a challenge for feedforward networks: lower layers in\nconvolutional nets capture rich spatial information, while upper layers encode\nobject-level knowledge but are invariant to factors such as pose and\nappearance. In this work we propose to augment feedforward nets for object\nsegmentation with a novel top-down refinement approach. The resulting\nbottom-up/top-down architecture is capable of efficiently generating\nhigh-fidelity object masks. Similarly to skip connections, our approach\nleverages features at all layers of the net. Unlike skip connections, our\napproach does not attempt to output independent predictions at each layer.\nInstead, we first output a coarse `mask encoding' in a feedforward pass, then\nrefine this mask encoding in a top-down pass utilizing features at successively\nlower layers. The approach is simple, fast, and effective. Building on the\nrecent DeepMask network for generating object proposals, we show accuracy\nimprovements of 10-20% in average recall for various setups. Additionally, by\noptimizing the overall network architecture, our approach, which we call\nSharpMask, is 50% faster than the original DeepMask network (under .8s per\nimage).\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 09:33:44 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 20:40:51 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Pinheiro", "Pedro O.", ""], ["Lin", "Tsung-Yi", ""], ["Collobert", "Ronan", ""], ["Doll\u00e0r", "Piotr", ""]]}, {"id": "1603.08720", "submitter": "Qi Wei", "authors": "Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret,\n  Marcus Chen, Simon Godsill", "title": "Multi-Band Image Fusion Based on Spectral Unmixing", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2016.2598784", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-band image fusion algorithm based on unsupervised\nspectral unmixing for combining a high-spatial low-spectral resolution image\nand a low-spatial high-spectral resolution image. The widely used linear\nobservation model (with additive Gaussian noise) is combined with the linear\nspectral mixture model to form the likelihoods of the observations. The\nnon-negativity and sum-to-one constraints resulting from the intrinsic physical\nproperties of the abundances are introduced as prior information to regularize\nthis ill-posed problem. The joint fusion and unmixing problem is then\nformulated as maximizing the joint posterior distribution with respect to the\nendmember signatures and abundance maps, This optimization problem is attacked\nwith an alternating optimization strategy. The two resulting sub-problems are\nconvex and are solved efficiently using the alternating direction method of\nmultipliers. Experiments are conducted for both synthetic and semi-real data.\nSimulation results show that the proposed unmixing based fusion scheme improves\nboth the abundance and endmember estimation comparing with the state-of-the-art\njoint fusion and unmixing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:54:21 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wei", "Qi", ""], ["Bioucas-Dias", "Jose", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["Chen", "Marcus", ""], ["Godsill", "Simon", ""]]}, {"id": "1603.08754", "submitter": "Zeynep Akata PhD", "authors": "Zeynep Akata and Mateusz Malinowski and Mario Fritz and Bernt Schiele", "title": "Multi-Cue Zero-Shot Learning with Strong Supervision", "comments": "2016 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling up visual category recognition to large numbers of classes remains\nchallenging. A promising research direction is zero-shot learning, which does\nnot require any training data to recognize new classes, but rather relies on\nsome form of auxiliary information describing the new classes. Ultimately, this\nmay allow to use textbook knowledge that humans employ to learn about new\nclasses by transferring knowledge from classes they know well. The most\nsuccessful zero-shot learning approaches currently require a particular type of\nauxiliary information -- namely attribute annotations performed by humans --\nthat is not readily available for most classes. Our goal is to circumvent this\nbottleneck by substituting such annotations by extracting multiple pieces of\ninformation from multiple unstructured text sources readily available on the\nweb. To compensate for the weaker form of auxiliary information, we incorporate\nstronger supervision in the form of semantic part annotations on the classes\nfrom which we transfer knowledge. We achieve our goal by a joint embedding\nframework that maps multiple text parts as well as multiple semantic parts into\na common space. Our results consistently and significantly improve on the\nstate-of-the-art in zero-short recognition and retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 13:04:21 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Akata", "Zeynep", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1603.08810", "submitter": "Masakazu Iwamura", "authors": "Masakazu Iwamura, Masataka Konishi and Koichi Kise", "title": "Scalable Solution for Approximate Nearest Subspace Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the nearest subspace is a fundamental problem and influential to many\napplications. In particular, a scalable solution that is fast and accurate for\na large problem has a great impact. The existing methods for the problem are,\nhowever, useless in a large-scale problem with a large number of subspaces and\nhigh dimensionality of the feature space. A cause is that they are designed\nbased on the traditional idea to represent a subspace by a single point. In\nthis paper, we propose a scalable solution for the approximate nearest subspace\nsearch (ANSS) problem. Intuitively, the proposed method represents a subspace\nby multiple points unlike the existing methods. This makes a large-scale ANSS\nproblem tractable. In the experiment with 3036 subspaces in the\n1024-dimensional space, we confirmed that the proposed method was 7.3 times\nfaster than the previous state-of-the-art without loss of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:24:43 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Iwamura", "Masakazu", ""], ["Konishi", "Masataka", ""], ["Kise", "Koichi", ""]]}, {"id": "1603.08895", "submitter": "Zeynep Akata PhD", "authors": "Yongqin Xian and Zeynep Akata and Gaurav Sharma and Quynh Nguyen and\n  Matthias Hein and Bernt Schiele", "title": "Latent Embeddings for Zero-shot Classification", "comments": "2016 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel latent embedding model for learning a compatibility\nfunction between image and class embeddings, in the context of zero-shot\nclassification. The proposed method augments the state-of-the-art bilinear\ncompatibility model by incorporating latent variables. Instead of learning a\nsingle bilinear map, it learns a collection of maps with the selection, of\nwhich map to use, being a latent variable for the current image-class pair. We\ntrain the model with a ranking based objective function which penalizes\nincorrect rankings of the true class for a given image. We empirically\ndemonstrate that our model improves the state-of-the-art for various class\nembeddings consistently on three challenging publicly available datasets for\nthe zero-shot setting. Moreover, our method leads to visually highly\ninterpretable results with clear clusters of different fine-grained object\nproperties that correspond to different latent variable maps.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 19:24:38 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 10:33:02 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Xian", "Yongqin", ""], ["Akata", "Zeynep", ""], ["Sharma", "Gaurav", ""], ["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1603.08907", "submitter": "Punarjay Chakravarty", "authors": "Punarjay Chakravarty and Tinne Tuytelaars", "title": "Cross-modal Supervision for Learning Active Speaker Detection in Video", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how to use audio to supervise the learning of active\nspeaker detection in video. Voice Activity Detection (VAD) guides the learning\nof the vision-based classifier in a weakly supervised manner. The classifier\nuses spatio-temporal features to encode upper body motion - facial expressions\nand gesticulations associated with speaking. We further improve a generic model\nfor active speaker detection by learning person specific models. Finally, we\ndemonstrate the online adaptation of generic models learnt on one dataset, to\npreviously unseen people in a new dataset, again using audio (VAD) for weak\nsupervision. The use of temporal continuity overcomes the lack of clean\ntraining data. We are the first to present an active speaker detection system\nthat learns on one audio-visual dataset and automatically adapts to speakers in\na new dataset. This work can be seen as an example of how the availability of\nmulti-modal data allows us to learn a model without the need for supervision,\nby transferring knowledge from one modality to another.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 19:47:46 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Chakravarty", "Punarjay", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1603.08968", "submitter": "Zhengdong Zhang", "authors": "Zhengdong Zhang, Vivienne Sze", "title": "FAST: A Framework to Accelerate Super-Resolution Processing on\n  Compressed Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art super-resolution (SR) algorithms require significant\ncomputational resources to achieve real-time throughput (e.g., 60Mpixels/s for\nHD video). This paper introduces FAST (Free Adaptive Super-resolution via\nTransfer), a framework to accelerate any SR algorithm applied to compressed\nvideos. FAST exploits the temporal correlation between adjacent frames such\nthat SR is only applied to a subset of frames; SR pixels are then transferred\nto the other frames. The transferring process has negligible computation cost\nas it uses information already embedded in the compressed video (e.g., motion\nvectors and residual). Adaptive processing is used to retain accuracy when the\ntemporal correlation is not present (e.g., occlusions). FAST accelerates\nstate-of-the-art SR algorithms by up to 15x with a visual quality loss of\n0.2dB. FAST is an important step towards real-time SR algorithms for ultra-HD\ndisplays and energy constrained devices (e.g., phones and tablets).\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:07:16 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 18:20:25 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Zhang", "Zhengdong", ""], ["Sze", "Vivienne", ""]]}, {"id": "1603.08984", "submitter": "Aron Monszpart", "authors": "Aron Monszpart, Nils Thuerey, Niloy J. Mitra", "title": "SMASH: Physics-guided Reconstruction of Collisions from Videos", "comments": "SIGGRAPH Asia 2016", "journal-ref": "ACM Trans. Graph. 35, 6, Article 199 (November 2016), 14 pages\n  (2016)", "doi": "10.1145/2980179.2982421", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision sequences are commonly used in games and entertainment to add drama\nand excitement. Authoring even two body collisions in the real world can be\ndifficult, as one has to get timing and the object trajectories to be correctly\nsynchronized. After tedious trial-and-error iterations, when objects can\nactually be made to collide, then they are difficult to capture in 3D. In\ncontrast, synthetically generating plausible collisions is difficult as it\nrequires adjusting different collision parameters (e.g., object mass ratio,\ncoefficient of restitution, etc.) and appropriate initial parameters. We\npresent SMASH to directly read off appropriate collision parameters directly\nfrom raw input video recordings. Technically we enable this by utilizing laws\nof rigid body collision to regularize the problem of lifting 2D trajectories to\na physically valid 3D reconstruction of the collision. The reconstructed\nsequences can then be modified and combined to easily author novel and\nplausible collisions. We evaluate our system on a range of synthetic scenes and\ndemonstrate the effectiveness of our method by accurately reconstructing\nseveral complex real world collision events.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 22:18:29 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 11:09:34 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Monszpart", "Aron", ""], ["Thuerey", "Nils", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1603.09016", "submitter": "Kenneth Tran", "authors": "Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea,\n  Chris Thrasher, Chris Buehler, Chris Sienkiewicz", "title": "Rich Image Captioning in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image caption system that addresses new challenges of\nautomatically describing images in the wild. The challenges include high\nquality caption quality with respect to human judgments, out-of-domain data\nhandling, and low latency required in many applications. Built on top of a\nstate-of-the-art framework, we developed a deep vision model that detects a\nbroad range of visual concepts, an entity recognition model that identifies\ncelebrities and landmarks, and a confidence model for the caption output.\nExperimental results show that our caption engine outperforms previous\nstate-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO)\nand out of-domain datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 01:55:33 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 01:45:31 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Tran", "Kenneth", ""], ["He", "Xiaodong", ""], ["Zhang", "Lei", ""], ["Sun", "Jian", ""], ["Carapcea", "Cornelia", ""], ["Thrasher", "Chris", ""], ["Buehler", "Chris", ""], ["Sienkiewicz", "Chris", ""]]}, {"id": "1603.09027", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and Yao Wang", "title": "Palmprint Recognition Using Deep Scattering Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Palmprint recognition has drawn a lot of attention during the recent years.\nMany algorithms have been proposed for palmprint recognition in the past,\nmajority of them being based on features extracted from the transform domain.\nMany of these transform domain features are not translation or rotation\ninvariant, and therefore a great deal of preprocessing is needed to align the\nimages. In this paper, a powerful image representation, called scattering\nnetwork/transform, is used for palmprint recognition. Scattering network is a\nconvolutional network where its architecture and filters are predefined wavelet\ntransforms. The first layer of scattering network captures similar features to\nSIFT descriptors and the higher-layer features capture higher-frequency content\nof the signal which are lost in SIFT and other similar descriptors. After\nextraction of the scattering features, their dimensionality is reduced by\napplying principal component analysis (PCA) which reduces the computational\ncomplexity of the recognition task. Two different classifiers are used for\nrecognition: multi-class SVM and minimum-distance classifier. The proposed\nscheme has been tested on a well-known palmprint database and achieved accuracy\nrate of 99.95% and 100% using minimum distance classifier and SVM respectively.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 03:09:15 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1603.09037", "submitter": "Vincenzo Liguori", "authors": "Vincenzo Liguori", "title": "Vector Quantization for Machine Vision", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to reduce the computational cost for a variety of common\nmachine vision tasks by operating directly in the compressed domain,\nparticularly in the context of hardware acceleration. Pyramid Vector\nQuantization (PVQ) is the compression technique of choice and its properties\nare exploited to simplify Support Vector Machines (SVM), Convolutional Neural\nNetworks(CNNs), Histogram of Oriented Gradients (HOG) features, interest points\nmatching and other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 04:40:31 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Liguori", "Vincenzo", ""]]}, {"id": "1603.09046", "submitter": "Andrew Shin", "authors": "Andrew Shin, Masataka Yamaguchi, Katsunori Ohnishi and Tatsuya Harada", "title": "Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for\n  Locally Robust Captioning", "comments": "submitted to ECCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workflow of extracting features from images using convolutional neural\nnetworks (CNN) and generating captions with recurrent neural networks (RNN) has\nbecome a de-facto standard for image captioning task. However, since CNN\nfeatures are originally designed for classification task, it is mostly\nconcerned with the main conspicuous element of the image, and often fails to\ncorrectly convey information on local, secondary elements. We propose to\nincorporate coding with vector of locally aggregated descriptors (VLAD) on\nspatial pyramid for CNN features of sub-regions in order to generate image\nrepresentations that better reflect the local information of the images. Our\nresults show that our method of compact VLAD coding can match CNN features with\nas little as 3% of dimensionality and, when combined with spatial pyramid, it\nresults in image captions that more accurately take local elements into\naccount.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 05:48:05 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Shin", "Andrew", ""], ["Yamaguchi", "Masataka", ""], ["Ohnishi", "Katsunori", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1603.09056", "submitter": "Chunhua Shen", "authors": "Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang", "title": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks\n  with Symmetric Skip Connections", "comments": "Accepted to Proc. Advances in Neural Information Processing Systems\n  (NIPS'16). Content of the final version may be slightly different. Extended\n  version is available at http://arxiv.org/abs/1606.08921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a very deep fully convolutional encoding-decoding\nframework for image restoration such as denoising and super-resolution. The\nnetwork is composed of multiple layers of convolution and de-convolution\noperators, learning end-to-end mappings from corrupted images to the original\nones. The convolutional layers act as the feature extractor, which capture the\nabstraction of image contents while eliminating noises/corruptions.\nDe-convolutional layers are then used to recover the image details. We propose\nto symmetrically link convolutional and de-convolutional layers with skip-layer\nconnections, with which the training converges much faster and attains a\nhigher-quality local optimum. First, The skip connections allow the signal to\nbe back-propagated to bottom layers directly, and thus tackles the problem of\ngradient vanishing, making training deep networks easier and achieving\nrestoration performance gains consequently. Second, these skip connections pass\nimage details from convolutional layers to de-convolutional layers, which is\nbeneficial in recovering the original image. Significantly, with the large\ncapacity, we can handle different levels of noises using a single model.\nExperimental results show that our network achieves better performance than all\npreviously reported state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 07:16:05 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 01:15:42 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Mao", "Xiao-Jiao", ""], ["Shen", "Chunhua", ""], ["Yang", "Yu-Bin", ""]]}, {"id": "1603.09065", "submitter": "Xiao Chu", "authors": "Xiao Chu, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang", "title": "Structured Feature Learning for Pose Estimation", "comments": "Accepted by CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a structured feature learning framework to reason\nthe correlations among body joints at the feature level in human pose\nestimation. Different from existing approaches of modelling structures on score\nmaps or predicted labels, feature maps preserve substantially richer\ndescriptions of body joints. The relationships between feature maps of joints\nare captured with the introduced geometrical transform kernels, which can be\neasily implemented with a convolution layer. Features and their relationships\nare jointly learned in an end-to-end learning system. A bi-directional tree\nstructured model is proposed, so that the feature channels at a body joint can\nwell receive information from other joints. The proposed framework improves\nfeature learning substantially. With very simple post processing, it reaches\nthe best mean PCP on the LSP and FLIC datasets. Compared with the baseline of\nlearning features at each joint separately with ConvNet, the mean PCP has been\nimproved by 18% on FLIC. The code is released to the public.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 07:52:22 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Chu", "Xiao", ""], ["Ouyang", "Wanli", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1603.09095", "submitter": "Nenad Marku\\v{s}", "authors": "Nenad Marku\\v{s}, Igor S. Pand\\v{z}i\\'c, J\\\"orgen Ahlberg", "title": "Learning Local Descriptors by Optimizing the Keypoint-Correspondence\n  Criterion: Applications to Face Matching, Learning from Unlabeled Videos and\n  3D-Shape Retrieval", "comments": "This version has been accepted for publication in IEEE Transactions\n  on Image Processing (presents methodological and experimental improvements of\n  our ICPR2016 paper)", "journal-ref": null, "doi": "10.1109/TIP.2018.2867270", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current best local descriptors are learned on a large dataset of matching and\nnon-matching keypoint pairs. However, data of this kind is not always available\nsince detailed keypoint correspondences can be hard to establish. On the other\nhand, we can often obtain labels for pairs of keypoint bags. For example,\nkeypoint bags extracted from two images of the same object under different\nviews form a matching pair, and keypoint bags extracted from images of\ndifferent objects form a non-matching pair. On average, matching pairs should\ncontain more corresponding keypoints than non-matching pairs. We describe an\nend-to-end differentiable architecture that enables the learning of local\nkeypoint descriptors from such weakly-labeled data. Additionally, we discuss\nhow to improve the method by incorporating the procedure of mining hard\nnegatives. We also show how can our approach be used to learn convolutional\nfeatures from unlabeled video signals and 3D models.\n  Our implementation is available at https://github.com/nenadmarkus/wlrn\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 09:24:40 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 08:46:34 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 20:15:06 GMT"}, {"version": "v4", "created": "Mon, 21 Aug 2017 16:52:34 GMT"}, {"version": "v5", "created": "Tue, 22 May 2018 12:07:00 GMT"}, {"version": "v6", "created": "Tue, 7 May 2019 11:41:01 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Marku\u0161", "Nenad", ""], ["Pand\u017ei\u0107", "Igor S.", ""], ["Ahlberg", "J\u00f6rgen", ""]]}, {"id": "1603.09114", "submitter": "Eduard Trulls", "authors": "Kwang Moo Yi and Eduard Trulls and Vincent Lepetit and Pascal Fua", "title": "LIFT: Learned Invariant Feature Transform", "comments": "Accepted to ECCV 2016 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel Deep Network architecture that implements the full\nfeature point handling pipeline, that is, detection, orientation estimation,\nand feature description. While previous works have successfully tackled each\none of these problems individually, we show how to learn to do all three in a\nunified manner while preserving end-to-end differentiability. We then\ndemonstrate that our Deep pipeline outperforms state-of-the-art methods on a\nnumber of benchmark datasets, without the need of retraining.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 10:33:18 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 15:29:39 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Yi", "Kwang Moo", ""], ["Trulls", "Eduard", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}, {"id": "1603.09129", "submitter": "Matthew Day", "authors": "Matthew Day", "title": "Exploiting Facial Landmarks for Emotion Recognition in the Wild", "comments": "4 pages, ICMI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an entry to the third Emotion Recognition in the\nWild Challenge, EmotiW2015. We detail the associated experiments and show that,\nthrough more accurately locating the facial landmarks, and considering only the\ndistances between them, we can achieve a surprising level of performance. The\nresulting system is not only more accurate than the challenge baseline, but\nalso much simpler.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 11:11:29 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Day", "Matthew", ""]]}, {"id": "1603.09188", "submitter": "Spandana Gella", "authors": "Spandana Gella, Mirella Lapata, Frank Keller", "title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal\n  Embeddings", "comments": "11 pages, NAACL-HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, visual sense disambiguation for verbs: given an\nimage and a verb, assign the correct sense of the verb, i.e., the one that\ndescribes the action depicted in the image. Just as textual word sense\ndisambiguation is useful for a wide range of NLP tasks, visual sense\ndisambiguation can be useful for multimodal tasks such as image retrieval,\nimage description, and text illustration. We introduce VerSe, a new dataset\nthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.\nWe propose an unsupervised algorithm based on Lesk which performs visual sense\ndisambiguation using textual, visual, or multimodal embeddings. We find that\ntextual embeddings perform well when gold-standard textual annotations (object\nlabels and image descriptions) are available, while multimodal embeddings\nperform well on unannotated images. We also verify our findings by using the\ntextual and multimodal embeddings as features in a supervised setting and\nanalyse the performance of visual sense disambiguation task. VerSe is made\npublicly available and can be downloaded at:\nhttps://github.com/spandanagella/verse.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:43:38 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Gella", "Spandana", ""], ["Lapata", "Mirella", ""], ["Keller", "Frank", ""]]}, {"id": "1603.09200", "submitter": "Alejandro Betancourt", "authors": "Alejandro Betancourt, Natalia D\\'iaz-Rodr\\'iguez, Emilia Barakova,\n  Lucio Marcenaro, Matthias Rauterberg, Carlo Regazzoni", "title": "Unsupervised Understanding of Location and Illumination Changes in\n  Egocentric Videos", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras stand out as one of the most promising devices for the\nupcoming years, and as a consequence, the demand of computer algorithms to\nautomatically understand the videos recorded with them is increasing quickly.\nAn automatic understanding of these videos is not an easy task, and its mobile\nnature implies important challenges to be faced, such as the changing light\nconditions and the unrestricted locations recorded. This paper proposes an\nunsupervised strategy based on global features and manifold learning to endow\nwearable cameras with contextual information regarding the light conditions and\nthe location captured. Results show that non-linear manifold methods can\ncapture contextual patterns from global features without compromising large\ncomputational resources. The proposed strategy is used, as an application case,\nas a switching mechanism to improve the hand-detection problem in egocentric\nvideos.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 14:03:18 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 22:27:43 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Betancourt", "Alejandro", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Barakova", "Emilia", ""], ["Marcenaro", "Lucio", ""], ["Rauterberg", "Matthias", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1603.09240", "submitter": "Afshin Dehghan", "authors": "Afshin Dehghan and Mubarak Shah", "title": "Binary Quadratic Programing for Online Tracking of Hundreds of People in\n  Extremely Crowded Scenes", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2687462", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking has been studied for decades. However, when it comes to\ntracking pedestrians in extremely crowded scenes, we are limited to only few\nworks. This is an important problem which gives rise to several challenges.\nPre-trained object detectors fail to localize targets in crowded sequences.\nThis consequently limits the use of data-association based multi-target\ntracking methods which rely on the outcome of an object detector. Additionally,\nthe small apparent target size makes it challenging to extract features to\ndiscriminate targets from their surroundings. Finally, the large number of\ntargets greatly increases computational complexity which in turn makes it hard\nto extend existing multi-target tracking approaches to high-density crowd\nscenarios. In this paper, we propose a tracker that addresses the\naforementioned problems and is capable of tracking hundreds of people\nefficiently. We formulate online crowd tracking as Binary Quadratic Programing.\nOur formulation employs target's individual information in the form of\nappearance and motion as well as contextual cues in the form of neighborhood\nmotion, spatial proximity and grouping constraints, and solves detection and\ndata association simultaneously. In order to solve the proposed quadratic\noptimization efficiently, where state-of art commercial quadratic programing\nsolvers fail to find the answer in a reasonable amount of time, we propose to\nuse the most recent version of the Modified Frank Wolfe algorithm, which takes\nadvantage of SWAP-steps to speed up the optimization. We show that the proposed\nformulation can track hundreds of targets efficiently and improves state-of-art\nresults by significant margins on eleven challenging high density crowd\nsequences.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 15:11:38 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Dehghan", "Afshin", ""], ["Shah", "Mubarak", ""]]}, {"id": "1603.09246", "submitter": "Mehdi Noroozi", "authors": "Mehdi Noroozi and Paolo Favaro", "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw\n  Puzzles", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of image representation learning without\nhuman annotation. By following the principles of self-supervision, we build a\nconvolutional neural network (CNN) that can be trained to solve Jigsaw puzzles\nas a pretext task, which requires no manual labeling, and then later repurposed\nto solve object classification and detection. To maintain the compatibility\nacross tasks we introduce the context-free network (CFN), a siamese-ennead CNN.\nThe CFN takes image tiles as input and explicitly limits the receptive field\n(or context) of its early processing units to one tile at a time. We show that\nthe CFN includes fewer parameters than AlexNet while preserving the same\nsemantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we\nlearn both a feature mapping of object parts as well as their correct spatial\narrangement. Our experimental evaluations show that the learned features\ncapture semantically relevant content. Our proposed method for learning visual\nrepresentations outperforms state of the art methods in several transfer\nlearning benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 15:27:37 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 23:43:32 GMT"}, {"version": "v3", "created": "Tue, 22 Aug 2017 17:32:19 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Noroozi", "Mehdi", ""], ["Favaro", "Paolo", ""]]}, {"id": "1603.09302", "submitter": "Valsamis Ntouskos", "authors": "Valsamis Ntouskos, Fiora Pirri", "title": "Confidence driven TGV fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel model for spatially varying variational data fusion,\ndriven by point-wise confidence values. The proposed model allows for the joint\nestimation of the data and the confidence values based on the spatial coherence\nof the data. We discuss the main properties of the introduced model as well as\nsuitable algorithms for estimating the solution of the corresponding biconvex\nminimization problem and their convergence. The performance of the proposed\nmodel is evaluated considering the problem of depth image fusion by using both\nsynthetic and real data from publicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 18:27:22 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 17:25:58 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Ntouskos", "Valsamis", ""], ["Pirri", "Fiora", ""]]}, {"id": "1603.09320", "submitter": "Yury Malkov A", "authors": "Yu. A. Malkov, D. A. Yashunin", "title": "Efficient and robust approximate nearest neighbor search using\n  Hierarchical Navigable Small World graphs", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for the approximate K-nearest neighbor search based\non navigable small world graphs with controllable hierarchy (Hierarchical NSW,\nHNSW). The proposed solution is fully graph-based, without any need for\nadditional search structures, which are typically used at the coarse search\nstage of the most proximity graph techniques. Hierarchical NSW incrementally\nbuilds a multi-layer structure consisting from hierarchical set of proximity\ngraphs (layers) for nested subsets of the stored elements. The maximum layer in\nwhich an element is present is selected randomly with an exponentially decaying\nprobability distribution. This allows producing graphs similar to the\npreviously studied Navigable Small World (NSW) structures while additionally\nhaving the links separated by their characteristic distance scales. Starting\nsearch from the upper layer together with utilizing the scale separation boosts\nthe performance compared to NSW and allows a logarithmic complexity scaling.\nAdditional employment of a heuristic for selecting proximity graph neighbors\nsignificantly increases performance at high recall and in case of highly\nclustered data. Performance evaluation has demonstrated that the proposed\ngeneral metric space search index is able to strongly outperform previous\nopensource state-of-the-art vector-only approaches. Similarity of the algorithm\nto the skip list structure allows straightforward balanced distributed\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 19:29:44 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 07:27:25 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 12:07:54 GMT"}, {"version": "v4", "created": "Tue, 14 Aug 2018 19:29:07 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Malkov", "Yu. A.", ""], ["Yashunin", "D. A.", ""]]}, {"id": "1603.09335", "submitter": "Stephen Marsland", "authors": "Stephen Marsland and Robert McLachlan", "title": "M\\\"obius Invariants of Shapes and Images", "comments": null, "journal-ref": "SIGMA 12 (2016), 080, 29 pages", "doi": "10.3842/SIGMA.2016.080", "report-no": null, "categories": "cs.CV math.MG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Identifying when different images are of the same object despite changes\ncaused by imaging technologies, or processes such as growth, has many\napplications in fields such as computer vision and biological image analysis.\nOne approach to this problem is to identify the group of possible\ntransformations of the object and to find invariants to the action of that\ngroup, meaning that the object has the same values of the invariants despite\nthe action of the group. In this paper we study the invariants of planar shapes\nand images under the M\\\"obius group $\\mathrm{PSL}(2,\\mathbb{C})$, which arises\nin the conformal camera model of vision and may also correspond to neurological\naspects of vision, such as grouping of lines and circles. We survey properties\nof invariants that are important in applications, and the known M\\\"obius\ninvariants, and then develop an algorithm by which shapes can be recognised\nthat is M\\\"obius- and reparametrization-invariant, numerically stable, and\nrobust to noise. We demonstrate the efficacy of this new invariant approach on\nsets of curves, and then develop a M\\\"obius-invariant signature of grey-scale\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 05:13:59 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 05:36:45 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Marsland", "Stephen", ""], ["McLachlan", "Robert", ""]]}, {"id": "1603.09364", "submitter": "Upal Mahbub", "authors": "Upal Mahbub, Vishal M. Patel, Deepak Chandra, Brandon Barbello, Rama\n  Chellappa", "title": "Partial Face Detection for Continuous Authentication", "comments": null, "journal-ref": "2016 IEEE International Conference on Image Processing (ICIP),\n  Phoenix, AZ, USA, 2016, pp. 2991-2995", "doi": "10.1109/ICIP.2016.7532908", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a part-based technique for real time detection of users' faces\non mobile devices is proposed. This method is specifically designed for\ndetecting partially cropped and occluded faces captured using a smartphone's\nfront-facing camera for continuous authentication. The key idea is to detect\nfacial segments in the frame and cluster the results to obtain the region which\nis most likely to contain a face. Extensive experimentation on a mobile dataset\nof 50 users shows that our method performs better than many state-of-the-art\nface detection methods in terms of accuracy and processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:15:08 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Mahbub", "Upal", ""], ["Patel", "Vishal M.", ""], ["Chandra", "Deepak", ""], ["Barbello", "Brandon", ""], ["Chellappa", "Rama", ""]]}, {"id": "1603.09382", "submitter": "Yu Sun", "authors": "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger", "title": "Deep Networks with Stochastic Depth", "comments": "first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:58:07 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 18:42:37 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 23:24:16 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Huang", "Gao", ""], ["Sun", "Yu", ""], ["Liu", "Zhuang", ""], ["Sedra", "Daniel", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1603.09423", "submitter": "Weilin Huang", "authors": "Tong He, Weilin Huang, Yu Qiao and Jian Yao", "title": "Accurate Text Localization in Natural Image with Cascaded Convolutional\n  Text Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new top-down pipeline for scene text detection. We propose a\nnovel Cascaded Convolutional Text Network (CCTN) that joints two customized\nconvolutional networks for coarse-to-fine text localization. The CCTN fast\ndetects text regions roughly from a low-resolution image, and then accurately\nlocalizes text lines from each enlarged region. We cast previous character\nbased detection into direct text region estimation, avoiding multiple bottom-\nup post-processing steps. It exhibits surprising robustness and discriminative\npower by considering whole text region as detection object which provides\nstrong semantic information. We customize convolutional network by develop- ing\nrectangle convolutions and multiple in-network fusions. This enables it to\nhandle multi-shape and multi-scale text efficiently. Furthermore, the CCTN is\ncomputationally efficient by sharing convolutional computations, and high-level\nproperty allows it to be invariant to various languages and multiple\norientations. It achieves 0.84 and 0.86 F-measures on the ICDAR 2011 and ICDAR\n2013, delivering substantial improvements over state-of-the-art results [23,\n1].\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 00:16:31 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["He", "Tong", ""], ["Huang", "Weilin", ""], ["Qiao", "Yu", ""], ["Yao", "Jian", ""]]}, {"id": "1603.09439", "submitter": "Phuc Nguyen X", "authors": "Phuc Xuan Nguyen, Gregory Rogez, Charless Fowlkes, Deva Ramanan", "title": "The Open World of Micro-Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-videos are six-second videos popular on social media networks with\nseveral unique properties. Firstly, because of the authoring process, they\ncontain significantly more diversity and narrative structure than existing\ncollections of video \"snippets\". Secondly, because they are often captured by\nhand-held mobile cameras, they contain specialized viewpoints including\nthird-person, egocentric, and self-facing views seldom seen in traditional\nproduced video. Thirdly, due to to their continuous production and publication\non social networks, aggregate micro-video content contains interesting\nopen-world dynamics that reflects the temporal evolution of tag topics. These\naspects make micro-videos an appealing well of visual data for developing\nlarge-scale models for video understanding. We analyze a novel dataset of\nmicro-videos labeled with 58 thousand tags. To analyze this data, we introduce\nviewpoint-specific and temporally-evolving models for video understanding,\ndefined over state-of-the-art motion and deep visual features. We conclude that\nour dataset opens up new research opportunities for large-scale video analysis,\nnovel viewpoints, and open-world dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 02:19:53 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 01:53:32 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Nguyen", "Phuc Xuan", ""], ["Rogez", "Gregory", ""], ["Fowlkes", "Charless", ""], ["Ramanan", "Deva", ""]]}, {"id": "1603.09446", "submitter": "Wei Shen", "authors": "Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang, Xiang Bai", "title": "Object Skeleton Extraction in Natural Images by Fusing Scale-associated\n  Deep Side Outputs", "comments": "Accepted by CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object skeleton is a useful cue for object detection, complementary to the\nobject contour, as it provides a structural representation to describe the\nrelationship among object parts. While object skeleton extraction in natural\nimages is a very challenging problem, as it requires the extractor to be able\nto capture both local and global image context to determine the intrinsic scale\nof each skeleton pixel. Existing methods rely on per-pixel based multi-scale\nfeature computation, which results in difficult modeling and high time\nconsumption. In this paper, we present a fully convolutional network with\nmultiple scale-associated side outputs to address this problem. By observing\nthe relationship between the receptive field sizes of the sequential stages in\nthe network and the skeleton scales they can capture, we introduce a\nscale-associated side output to each stage. We impose supervision to different\nstages by guiding the scale-associated side outputs toward groundtruth\nskeletons of different scales. The responses of the multiple scale-associated\nside outputs are then fused in a scale-specific way to localize skeleton pixels\nwith multiple scales effectively. Our method achieves promising results on two\nskeleton extraction datasets, and significantly outperforms other competitors.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 03:21:33 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 05:51:33 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Shen", "Wei", ""], ["Zhao", "Kai", ""], ["Jiang", "Yuan", ""], ["Wang", "Yan", ""], ["Zhang", "Zhijiang", ""], ["Bai", "Xiang", ""]]}, {"id": "1603.09454", "submitter": "Wenxi Liu", "authors": "Wenxi Liu, Rynson W.H. Lau, Xiaogang Wang, Dinesh Manocha", "title": "Exemplar-AMMs: Recognizing Crowd Movements from Pedestrian Trajectories", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2016.2598091", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method to recognize the types of crowd\nmovement from crowd trajectories using agent-based motion models (AMMs). Our\nidea is to apply a number of AMMs, referred to as exemplar-AMMs, to describe\nthe crowd movement. Specifically, we propose an optimization framework that\nfilters out the unknown noise in the crowd trajectories and measures their\nsimilarity to the exemplar-AMMs to produce a crowd motion feature. We then\naddress our real-world crowd movement recognition problem as a multi-label\nclassification problem. Our experiments show that the proposed feature\noutperforms the state-of-the-art methods in recognizing both simulated and\nreal-world crowd movements from their trajectories. Finally, we have created a\nsynthetic dataset, SynCrowd, which contains 2D crowd trajectories in various\nscenarios, generated by various crowd simulators. This dataset can serve as a\ntraining set or benchmark for crowd analysis work.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 04:58:25 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Wenxi", ""], ["Lau", "Rynson W. H.", ""], ["Wang", "Xiaogang", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1603.09462", "submitter": "Hyunsuk Ko", "authors": "Hyunsuk Ko, Han Suk Shim, Ouk Choi, C.-C. Jay Kuo", "title": "Robust Uncalibrated Stereo Rectification with Constrained Geometric\n  Distortions (USR-CGD)", "comments": null, "journal-ref": "Image and Vision Computing, 2017", "doi": "10.1016/j.imavis.2017.01.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm for uncalibrated stereo image-pair rectification under the\nconstraint of geometric distortion, called USR-CGD, is presented in this work.\nAlthough it is straightforward to define a rectifying transformation (or\nhomography) given the epipolar geometry, many existing algorithms have unwanted\ngeometric distortions as a side effect. To obtain rectified images with reduced\ngeometric distortions while maintaining a small rectification error, we\nparameterize the homography by considering the influence of various kinds of\ngeometric distortions. Next, we define several geometric measures and\nincorporate them into a new cost function for parameter optimization. Finally,\nwe propose a constrained adaptive optimization scheme to allow a balanced\nperformance between the rectification error and the geometric error. Extensive\nexperimental results are provided to demonstrate the superb performance of the\nproposed USR-CGD method, which outperforms existing algorithms by a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 06:02:23 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Ko", "Hyunsuk", ""], ["Shim", "Han Suk", ""], ["Choi", "Ouk", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1603.09469", "submitter": "Hyunsuk Ko", "authors": "Hyunsuk Ko, Rui Song, C.-C. Jay Kuo", "title": "A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation, 2017", "doi": "10.1016/j.jvcir.2017.02.014", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of stereoscopic image quality assessment, which finds\napplications in 3D visual content delivery such as 3DTV, is investigated in\nthis work. Specifically, we propose a new ParaBoost (parallel-boosting)\nstereoscopic image quality assessment (PBSIQA) system. The system consists of\ntwo stages. In the first stage, various distortions are classified into a few\ntypes, and individual quality scorers targeting at a specific distortion type\nare developed. These scorers offer complementary performance in face of a\ndatabase consisting of heterogeneous distortion types. In the second stage,\nscores from multiple quality scorers are fused to achieve the best overall\nperformance, where the fuser is designed based on the parallel boosting idea\nborrowed from machine learning. Extensive experimental results are conducted to\ncompare the performance of the proposed PBSIQA system with those of existing\nstereo image quality assessment (SIQA) metrics. The developed quality metric\ncan serve as an objective function to optimize the performance of a 3D content\ndelivery system.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 06:55:25 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Ko", "Hyunsuk", ""], ["Song", "Rui", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1603.09473", "submitter": "Ruining He", "authors": "Ruining He and Charles Packer and Julian McAuley", "title": "Learning Compatibility Across Categories for Heterogeneous Item\n  Recommendation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying relationships between items is a key task of an online\nrecommender system, in order to help users discover items that are functionally\ncomplementary or visually compatible. In domains like clothing recommendation,\nthis task is particularly challenging since a successful system should be\ncapable of handling a large corpus of items, a huge amount of relationships\namong them, as well as the high-dimensional and semantically complicated\nfeatures involved. Furthermore, the human notion of \"compatibility\" to capture\ngoes beyond mere similarity: For two items to be compatible---whether jeans and\na t-shirt, or a laptop and a charger---they should be similar in some ways, but\nsystematically different in others.\n  In this paper we propose a novel method, Monomer, to learn complicated and\nheterogeneous relationships between items in product recommendation settings.\nRecently, scalable methods have been developed that address this task by\nlearning similarity metrics on top of the content of the products involved.\nHere our method relaxes the metricity assumption inherent in previous work and\nmodels multiple localized notions of 'relatedness,' so as to uncover ways in\nwhich related items should be systematically similar, and systematically\ndifferent. Quantitatively, we show that our system achieves state-of-the-art\nperformance on large-scale compatibility prediction tasks, especially in cases\nwhere there is substantial heterogeneity between related items. Qualitatively,\nwe demonstrate that richer notions of compatibility can be learned that go\nbeyond similarity, and that our model can make effective recommendations of\nheterogeneous content.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 07:22:30 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 07:25:36 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 00:43:21 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["He", "Ruining", ""], ["Packer", "Charles", ""], ["McAuley", "Julian", ""]]}, {"id": "1603.09558", "submitter": "Vania Estrela Dr.", "authors": "R. L. B. Breder, Vania V. Estrela, J. T. de Assis", "title": "Sub-pixel accuracy edge fitting by means of B-spline", "comments": "5 pages, Proceedings of the MMSP '09. IEEE International Workshop on\n  Multimedia Signal Processing, ISBN 978-1-4244-4463-2", "journal-ref": null, "doi": "10.1109/MMSP.2009.5293265", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local perturbations around contours strongly disturb the final result of\ncomputer vision tasks. It is common to introduce a priori information in the\nestimation process. Improvement can be achieved via a deformable model such as\nthe snake model. In recent works, the deformable contour is modeled by means of\nB-spline snakes which allows local control, concise representation, and the use\nof fewer parameters. The estimation of the sub-pixel edges using a global\nB-spline model relies on the contour global determination according to a\nmaximum likelihood framework and using the observed data likelihood. This\nprocedure guarantees that the noisiest data will be filtered out. The data\nlikelihood is computed as a consequence of the observation model which includes\nboth orientation and position information. Comparative experiments of this\nalgorithm and the classical spline interpolation have shown that the proposed\nalgorithm outperforms the classical approach for Gaussian and Salt & Pepper\nnoise.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 12:37:58 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Breder", "R. L. B.", ""], ["Estrela", "Vania V.", ""], ["de Assis", "J. T.", ""]]}, {"id": "1603.09599", "submitter": "Vania Estrela Dr.", "authors": "Vania V. Estrela, Hermes Aguiar Magalhaes, Osamu Saotome", "title": "Total Variation Applications in Computer Vision", "comments": "24 pages, Book Title: Handbook of Research on Emerging Perspectives\n  in Intelligent Pattern Recognition, Analysis, and Image Processing, Editor\n  Narendra Kumar Kamila, IGI Global, 2016,\n  http://www.igi-global.com/chapter/total-variation-applications-in-computer-vision/141626", "journal-ref": null, "doi": "10.4018/978-1-4666-8654-0.ch002", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objectives of this chapter are: (i) to introduce a concise overview of\nregularization; (ii) to define and to explain the role of a particular type of\nregularization called total variation norm (TV-norm) in computer vision tasks;\n(iii) to set up a brief discussion on the mathematical background of TV\nmethods; and (iv) to establish a relationship between models and a few existing\nmethods to solve problems cast as TV-norm. For the most part, image-processing\nalgorithms blur the edges of the estimated images, however TV regularization\npreserves the edges with no prior information on the observed and the original\nimages. The regularization scalar parameter {\\lambda} controls the amount of\nregularization allowed and it is an essential to obtain a high-quality\nregularized output. A wide-ranging review of several ways to put into practice\nTV regularization as well as its advantages and limitations are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 14:08:53 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Estrela", "Vania V.", ""], ["Magalhaes", "Hermes Aguiar", ""], ["Saotome", "Osamu", ""]]}, {"id": "1603.09687", "submitter": "Claudio Gennaro", "authors": "Claudio Gennaro", "title": "Large Scale Deep Convolutional Neural Network Features Search with\n  Lucene", "comments": "This paper has been withdrawn by the author due to many errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an approach to index Deep Convolutional Neural\nNetwork Features to support efficient content-based retrieval on large image\ndatabases. To this aim, we have converted the these features into a textual\nform, to index them into an inverted index by means of Lucene. In this way, we\nwere able to set up a robust retrieval system that combines full-text search\nwith content-based image retrieval capabilities. We evaluated different\nstrategies of textual representation in order to optimize the index occupation\nand the query response time. In order to show that our approach is able to\nhandle large datasets, we have developed a web-based prototype that provides an\ninterface for combined textual and visual searching into a dataset of about 100\nmillion of images.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 17:11:43 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 09:43:48 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 15:02:51 GMT"}, {"version": "v4", "created": "Wed, 20 Jul 2016 09:29:57 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Gennaro", "Claudio", ""]]}, {"id": "1603.09712", "submitter": "Nihar Athreyas", "authors": "Nihar Athreyas, Zhiguo Lai, Jai Gupta and Dev Gupta", "title": "Analog Signal Processing Approach for Coarse and Fine Depth Estimation", "comments": "appears in Signal & Image Processing : An International Journal\n  (SIPIJ) Vol.5, No.6, December 2014. arXiv admin note: substantial text\n  overlap with arXiv:1411.3929", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging and Image sensors is a field that is continuously evolving. There are\nnew products coming into the market every day. Some of these have very severe\nSize, Weight and Power constraints whereas other devices have to handle very\nhigh computational loads. Some require both these conditions to be met\nsimultaneously. Current imaging architectures and digital image processing\nsolutions will not be able to meet these ever increasing demands. There is a\nneed to develop novel imaging architectures and image processing solutions to\naddress these requirements. In this work we propose analog signal processing as\na solution to this problem. The analog processor is not suggested as a\nreplacement to a digital processor but it will be used as an augmentation\ndevice which works in parallel with the digital processor, making the system\nfaster and more efficient. In order to show the merits of analog processing two\nstereo correspondence algorithms are implemented. We propose novel\nmodifications to the algorithms and new imaging architectures which,\nsignificantly reduces the computation time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 15:07:41 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Athreyas", "Nihar", ""], ["Lai", "Zhiguo", ""], ["Gupta", "Jai", ""], ["Gupta", "Dev", ""]]}, {"id": "1603.09725", "submitter": "Radu Horaud P", "authors": "Israel D. Gebru, Sil\\`eye Ba, Xiaofei Li and Radu Horaud", "title": "Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion", "comments": "14 pages, 6 figures, 5 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  40(6), 1086 - 1099, 2018", "doi": "10.1109/TPAMI.2017.2648793", "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization consists of assigning speech signals to people engaged in\na dialogue. An audio-visual spatiotemporal diarization model is proposed. The\nmodel is well suited for challenging scenarios that consist of several\nparticipants engaged in multi-party interaction while they move around and turn\ntheir heads towards the other participants rather than facing the cameras and\nthe microphones. Multiple-person visual tracking is combined with multiple\nspeech-source localization in order to tackle the speech-to-person association\nproblem. The latter is solved within a novel audio-visual fusion method on the\nfollowing grounds: binaural spectral features are first extracted from a\nmicrophone pair, then a supervised audio-visual alignment technique maps these\nfeatures onto an image, and finally a semi-supervised clustering method assigns\nbinaural spectral features to visible persons. The main advantage of this\nmethod over previous work is that it processes in a principled way speech\nsignals uttered simultaneously by multiple persons. The diarization itself is\ncast into a latent-variable temporal graphical model that infers speaker\nidentities and speech turns, based on the output of an audio-visual association\nprocess, executed at each time slice, and on the dynamics of the diarization\nvariable itself. The proposed formulation yields an efficient exact inference\nprocedure. A novel dataset, that contains audio-visual training data as well as\na number of scenarios involving several participants engaged in formal and\ninformal dialogue, is introduced. The proposed method is thoroughly tested and\nbenchmarked with respect to several state-of-the art diarization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:15:01 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 16:40:24 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Gebru", "Israel D.", ""], ["Ba", "Sil\u00e8ye", ""], ["Li", "Xiaofei", ""], ["Horaud", "Radu", ""]]}, {"id": "1603.09732", "submitter": "Radu Horaud P", "authors": "Vincent Drouard, Radu Horaud, Antoine Deleforge, Sil\\`eye Ba and\n  Georgios Evangelidis", "title": "Robust Head-Pose Estimation Based on Partially-Latent Mixture of Linear\n  Regressions", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": "IEEE Transactions on Image Processing, volume 26, Issue 3,\n  1428-1440, 2017", "doi": "10.1109/TIP.2017.2654165", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head-pose estimation has many applications, such as social event analysis,\nhuman-robot and human-computer interaction, driving assistance, and so forth.\nHead-pose estimation is challenging because it must cope with changing\nillumination conditions, variabilities in face orientation and in appearance,\npartial occlusions of facial landmarks, as well as bounding-box-to-face\nalignment errors. We propose tu use a mixture of linear regressions with\npartially-latent output. This regression method learns to map high-dimensional\nfeature vectors (extracted from bounding boxes of faces) onto the joint space\nof head-pose angles and bounding-box shifts, such that they are robustly\npredicted in the presence of unobservable phenomena. We describe in detail the\nmapping method that combines the merits of unsupervised manifold learning\ntechniques and of mixtures of regressions. We validate our method with three\npublicly available datasets and we thoroughly benchmark four variants of the\nproposed algorithm with several state-of-the-art head-pose estimation methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:32:52 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 09:10:36 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 11:18:47 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Drouard", "Vincent", ""], ["Horaud", "Radu", ""], ["Deleforge", "Antoine", ""], ["Ba", "Sil\u00e8ye", ""], ["Evangelidis", "Georgios", ""]]}, {"id": "1603.09742", "submitter": "Qin Huang", "authors": "Qin Huang, Chunyang Xia, Wenchao Zheng, Yuhang Song, Hao Xu and C.-C.\n  Jay Kuo", "title": "Object Boundary Guided Semantic Segmentation", "comments": "The results in the first version of this paper are mistaken due to\n  overlapping validation data and incorrect benchmark methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:51:05 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 15:56:14 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 23:14:00 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 23:51:40 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Huang", "Qin", ""], ["Xia", "Chunyang", ""], ["Zheng", "Wenchao", ""], ["Song", "Yuhang", ""], ["Xu", "Hao", ""], ["Kuo", "C. -C. Jay", ""]]}]