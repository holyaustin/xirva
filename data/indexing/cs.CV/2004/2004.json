[{"id": "2004.00006", "submitter": "Yiqin Zhao", "authors": "Yiqin Zhao, Tian Guo", "title": "PointAR: Efficient Lighting Estimation for Mobile Augmented Reality", "comments": "Accepted to 16th European Conference On Computer Vision (ECCV'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient lighting estimation pipeline that is suitable to run\non modern mobile devices, with comparable resource complexities to\nstate-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a\nsingle RGB-D image captured from the mobile camera and a 2D location in that\nimage, and estimates 2nd order spherical harmonics coefficients. This estimated\nspherical harmonics coefficients can be directly utilized by rendering engines\nfor supporting spatially variant indoor lighting, in the context of augmented\nreality. Our key insight is to formulate the lighting estimation as a point\ncloud-based learning problem directly from point clouds, which is in part\ninspired by the Monte Carlo integration leveraged by real-time spherical\nharmonics lighting. While existing approaches estimate lighting information\nwith complex deep learning pipelines, our method focuses on reducing the\ncomputational complexity. Through both quantitative and qualitative\nexperiments, we demonstrate that PointAR achieves lower lighting estimation\nerrors compared to state-of-the-art methods. Further, our method requires an\norder of magnitude lower resource, comparable to that of mobile-specific DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:13:26 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 00:24:54 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 21:07:11 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 20:13:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhao", "Yiqin", ""], ["Guo", "Tian", ""]]}, {"id": "2004.00038", "submitter": "Kayhan Ghafoor", "authors": "Halgurd S. Maghdid, Aras T. Asaad, Kayhan Zrar Ghafoor, Ali Safaa\n  Sadiq, and Muhammad Khurram Khan", "title": "Diagnosing COVID-19 Pneumonia from X-Ray and CT Images using Deep\n  Learning and Transfer Learning Algorithms", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  COVID-19 (also known as 2019 Novel Coronavirus) first emerged in Wuhan, China\nand spread across the globe with unprecedented effect and has now become the\ngreatest crisis of the modern era. The COVID-19 has proved much more pervasive\ndemands for diagnosis that has driven researchers to develop more intelligent,\nhighly responsive and efficient detection methods. In this work, we focus on\nproposing AI tools that can be used by radiologists or healthcare professionals\nto diagnose COVID-19 cases in a quick and accurate manner. However, the lack of\na publicly available dataset of X-ray and CT images makes the design of such AI\ntools a challenging task. To this end, this study aims to build a comprehensive\ndataset of X-rays and CT scan images from multiple sources as well as provides\na simple but an effective COVID-19 detection technique using deep learning and\ntransfer learning algorithms. In this vein, a simple convolution neural network\n(CNN) and modified pre-trained AlexNet model are applied on the prepared X-rays\nand CT scan images dataset. The result of the experiments shows that the\nutilized models can provide accuracy up to 98 % via pre-trained network and\n94.1 % accuracy by using the modified CNN.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:10:10 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Maghdid", "Halgurd S.", ""], ["Asaad", "Aras T.", ""], ["Ghafoor", "Kayhan Zrar", ""], ["Sadiq", "Ali Safaa", ""], ["Khan", "Muhammad Khurram", ""]]}, {"id": "2004.00049", "submitter": "Yujun Shen", "authors": "Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou", "title": "In-Domain GAN Inversion for Real Image Editing", "comments": "ECCV 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that a variety of semantics emerge in the latent space\nof Generative Adversarial Networks (GANs) when being trained to synthesize\nimages. However, it is difficult to use these learned semantics for real image\nediting. A common practice of feeding a real image to a trained GAN generator\nis to invert it back to a latent code. However, existing inversion methods\ntypically focus on reconstructing the target image by pixel values yet fail to\nland the inverted code in the semantic domain of the original latent space. As\na result, the reconstructed image cannot well support semantic editing through\nvarying the inverted code. To solve this problem, we propose an in-domain GAN\ninversion approach, which not only faithfully reconstructs the input image but\nalso ensures the inverted code to be semantically meaningful for editing. We\nfirst learn a novel domain-guided encoder to project a given image to the\nnative latent space of GANs. We then propose domain-regularized optimization by\ninvolving the encoder as a regularizer to fine-tune the code produced by the\nencoder and better recover the target image. Extensive experiments suggest that\nour inversion method achieves satisfying real image reconstruction and more\nimportantly facilitates various image editing tasks, significantly\noutperforming start-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:20:18 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 20:16:32 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 09:47:36 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhu", "Jiapeng", ""], ["Shen", "Yujun", ""], ["Zhao", "Deli", ""], ["Zhou", "Bolei", ""]]}, {"id": "2004.00060", "submitter": "Bardia Doosti", "authors": "Bardia Doosti, Shujon Naha, Majid Mirbagheri, David Crandall", "title": "HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand-object pose estimation (HOPE) aims to jointly detect the poses of both a\nhand and of a held object. In this paper, we propose a lightweight model called\nHOPE-Net which jointly estimates hand and object pose in 2D and 3D in\nreal-time. Our network uses a cascade of two adaptive graph convolutional\nneural networks, one to estimate 2D coordinates of the hand joints and object\ncorners, followed by another to convert 2D coordinates to 3D. Our experiments\nshow that through end-to-end training of the full network, we achieve better\naccuracy for both the 2D and 3D coordinate estimation problems. The proposed 2D\nto 3D graph convolution-based model could be applied to other 3D landmark\ndetection problems, where it is possible to first predict the 2D keypoints and\nthen transform them to 3D.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:01:42 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Doosti", "Bardia", ""], ["Naha", "Shujon", ""], ["Mirbagheri", "Majid", ""], ["Crandall", "David", ""]]}, {"id": "2004.00070", "submitter": "Davide Abati", "authors": "Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara,\n  Rita Cucchiara, Babak Ehteshami Bejnordi", "title": "Conditional Channel Gated Networks for Task-Aware Continual Learning", "comments": "CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks experience catastrophic forgetting when\noptimized on a sequence of learning problems: as they meet the objective of the\ncurrent training examples, their performance on previous tasks drops\ndrastically. In this work, we introduce a novel framework to tackle this\nproblem with conditional computation. We equip each convolutional layer with\ntask-specific gating modules, selecting which filters to apply on the given\ninput. This way, we achieve two appealing properties. Firstly, the execution\npatterns of the gates allow to identify and protect important filters, ensuring\nno loss in the performance of the model for previously learned tasks. Secondly,\nby using a sparsity objective, we can promote the selection of a limited set of\nkernels, allowing to retain sufficient model capacity to digest new\ntasks.Existing solutions require, at test time, awareness of the task to which\neach example belongs to. This knowledge, however, may not be available in many\npractical scenarios. Therefore, we additionally introduce a task classifier\nthat predicts the task label of each example, to deal with settings in which a\ntask oracle is not available. We validate our proposal on four continual\nlearning datasets. Results show that our model consistently outperforms\nexisting methods both in the presence and the absence of a task oracle.\nNotably, on Split SVHN and Imagenet-50 datasets, our model yields up to 23.98%\nand 17.42% improvement in accuracy w.r.t. competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:35:07 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Abati", "Davide", ""], ["Tomczak", "Jakub", ""], ["Blankevoort", "Tijmen", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""], ["Bejnordi", "Babak Ehteshami", ""]]}, {"id": "2004.00074", "submitter": "Karthik Gopinath", "authors": "Karthik Gopinath, Christian Desrosiers, and Herve Lombaert", "title": "Graph Domain Adaptation for Alignment-Invariant Brain Surface\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The varying cortical geometry of the brain creates numerous challenges for\nits analysis. Recent developments have enabled learning surface data directly\nacross multiple brain surfaces via graph convolutions on cortical data.\nHowever, current graph learning algorithms do fail when brain surface data are\nmisaligned across subjects, thereby affecting their ability to deal with data\nfrom multiple domains. Adversarial training is widely used for domain\nadaptation to improve the segmentation performance across domains. In this\npaper, adversarial training is exploited to learn surface data across\ninconsistent graph alignments. This novel approach comprises a segmentator that\nuses a set of graph convolution layers to enable parcellation directly across\nbrain surfaces in a source domain, and a discriminator that predicts a graph\ndomain from segmentations. More precisely, the proposed adversarial network\nlearns to generalize a parcellation across both, source and target domains. We\ndemonstrate an 8% mean improvement in performance over a non-adversarial\ntraining strategy applied on multiple target domains extracted from MindBoggle,\nthe largest publicly available manually-labeled brain surface dataset.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:43:59 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gopinath", "Karthik", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "2004.00121", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard,\n  Hans-Peter Seidel, Patrick P\\'erez, Michael Zollh\\\"ofer, Christian Theobalt", "title": "StyleRig: Rigging StyleGAN for 3D Control over Portrait Images", "comments": "CVPR 2020 (Oral). Project page:\n  https://gvv.mpi-inf.mpg.de/projects/StyleRig/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StyleGAN generates photorealistic portrait images of faces with eyes, teeth,\nhair and context (neck, shoulders, background), but lacks a rig-like control\nover semantic face parameters that are interpretable in 3D, such as face pose,\nexpressions, and scene illumination. Three-dimensional morphable face models\n(3DMMs) on the other hand offer control over the semantic parameters, but lack\nphotorealism when rendered and only model the face interior, not other parts of\na portrait image (hair, mouth interior, background). We present the first\nmethod to provide a face rig-like control over a pretrained and fixed StyleGAN\nvia a 3DMM. A new rigging network, RigNet is trained between the 3DMM's\nsemantic parameters and StyleGAN's input. The network is trained in a\nself-supervised manner, without the need for manual annotations. At test time,\nour method generates portrait images with the photorealism of StyleGAN and\nprovides explicit control over the 3D semantic parameters of the face.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:20:34 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 09:40:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tewari", "Ayush", ""], ["Elgharib", "Mohamed", ""], ["Bharaj", "Gaurav", ""], ["Bernard", "Florian", ""], ["Seidel", "Hans-Peter", ""], ["P\u00e9rez", "Patrick", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "2004.00123", "submitter": "Longfei Zeng", "authors": "Longfei Zeng and Mohammed Sabah", "title": "EOLO: Embedded Object Segmentation only Look Once", "comments": "7 pages, 5 figures, 2 tables, 25 conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an anchor-free and single-shot instance\nsegmentation method, which is conceptually simple with 3 independent branches,\nfully convolutional and can be used by easily embedding it into mobile and\nembedded devices.\n  Our method, refer as EOLO, reformulates the instance segmentation problem as\npredicting semantic segmentation and distinguishing overlapping objects\nproblem, through instance center classification and 4D distance regression on\neach pixel. Moreover, we propose one effective loss function to deal with\nsampling a high-quality center of gravity examples and optimization for 4D\ndistance regression, which can significantly improve the mAP performance.\nWithout any bells and whistles, EOLO achieves 27.7$\\%$ in mask mAP under IoU50\nand reaches 30 FPS on 1080Ti GPU, with a single-model and single-scale\ntraining/testing on the challenging COCO2017 dataset.\n  For the first time, we show the different comprehension of instance\nsegmentation in recent methods, in terms of both up-bottom, down-up, and\ndirect-predict paradigms. Then we illustrate our model and present related\nexperiments and results. We hope that the proposed EOLO framework can serve as\na fundamental baseline for a single-shot instance segmentation task in\nReal-time Industrial Scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:22:05 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zeng", "Longfei", ""], ["Sabah", "Mohammed", ""]]}, {"id": "2004.00137", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Ximeng Sun, Eric Tzeng, Abir Das, Kate Saenko, Trevor\n  Darrell", "title": "Revisiting Few-shot Activity Detection with Class Similarity Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting events in the real world are rare making preannotated\nmachine learning ready videos a rarity in consequence. Thus, temporal activity\ndetection models that are able to learn from a few examples are desirable. In\nthis paper, we present a conceptually simple and general yet novel framework\nfor few-shot temporal activity detection based on proposal regression which\ndetects the start and end time of the activities in untrimmed videos. Our model\nis end-to-end trainable, takes into account the frame rate differences between\nfew-shot activities and untrimmed test videos, and can benefit from additional\nfew-shot examples. We experiment on three large scale benchmarks for temporal\nactivity detection (ActivityNet1.2, ActivityNet1.3 and THUMOS14 datasets) in a\nfew-shot setting. We also study the effect on performance of different amount\nof overlap with activities used to pretrain the video classification backbone\nand propose corrective measures for future works in this domain. Our code will\nbe made available.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:02:38 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Xu", "Huijuan", ""], ["Sun", "Ximeng", ""], ["Tzeng", "Eric", ""], ["Das", "Abir", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "2004.00140", "submitter": "Ligong Han", "authors": "Ligong Han, Robert F. Murphy, and Deva Ramanan", "title": "Learning Generative Models of Tissue Organization with Supervised GANs", "comments": "Accepted at WACV-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A key step in understanding the spatial organization of cells and tissues is\nthe ability to construct generative models that accurately reflect that\norganization. In this paper, we focus on building generative models of electron\nmicroscope (EM) images in which the positions of cell membranes and\nmitochondria have been densely annotated, and propose a two-stage procedure\nthat produces realistic images using Generative Adversarial Networks (or GANs)\nin a supervised way. In the first stage, we synthesize a label \"image\" given a\nnoise \"image\" as input, which then provides supervision for EM image synthesis\nin the second stage. The full model naturally generates label-image pairs. We\nshow that accurate synthetic EM images are produced using assessment via (1)\nshape features and global statistics, (2) segmentation accuracies, and (3) user\nstudies. We also demonstrate further improvements by enforcing a reconstruction\nloss on intermediate synthetic labels and thus unifying the two stages into one\nsingle end-to-end framework.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:22:58 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Han", "Ligong", ""], ["Murphy", "Robert F.", ""], ["Ramanan", "Deva", ""]]}, {"id": "2004.00144", "submitter": "Yun-Chun Chen", "authors": "Yun-Chun Chen, Po-Hsiang Huang, Li-Yu Yu, Jia-Bin Huang, Ming-Hsuan\n  Yang, Yen-Yu Lin", "title": "Deep Semantic Matching with Foreground Detection and Cycle-Consistency", "comments": "ACCV 2018. PAMI 2020 extension: arXiv:1906.05857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing dense semantic correspondences between object instances remains\na challenging problem due to background clutter, significant scale and pose\ndifferences, and large intra-class variations. In this paper, we address weakly\nsupervised semantic matching based on a deep network where only image pairs\nwithout manual keypoint correspondence annotations are provided. To facilitate\nnetwork training with this weaker form of supervision, we 1) explicitly\nestimate the foreground regions to suppress the effect of background clutter\nand 2) develop cycle-consistent losses to enforce the predicted transformations\nacross multiple images to be geometrically plausible and consistent. We train\nthe proposed model using the PF-PASCAL dataset and evaluate the performance on\nthe PF-PASCAL, PF-WILLOW, and TSS datasets. Extensive experimental results show\nthat the proposed approach performs favorably against the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:38:09 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Huang", "Po-Hsiang", ""], ["Yu", "Li-Yu", ""], ["Huang", "Jia-Bin", ""], ["Yang", "Ming-Hsuan", ""], ["Lin", "Yen-Yu", ""]]}, {"id": "2004.00161", "submitter": "Victor Schmidt", "authors": "Victor Schmidt, Makesh Narsimhan Sreedhar, Mostafa ElAraby, Irina Rish", "title": "Towards Lifelong Self-Supervision For Unpaired Image-to-Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unpaired Image-to-Image Translation (I2IT) tasks often suffer from lack of\ndata, a problem which self-supervised learning (SSL) has recently been very\npopular and successful at tackling. Leveraging auxiliary tasks such as rotation\nprediction or generative colorization, SSL can produce better and more robust\nrepresentations in a low data regime. Training such tasks along an I2IT task is\nhowever computationally intractable as model size and the number of task grow.\nOn the other hand, learning sequentially could incur catastrophic forgetting of\npreviously learned tasks. To alleviate this, we introduce Lifelong\nSelf-Supervision (LiSS) as a way to pre-train an I2IT model (e.g., CycleGAN) on\na set of self-supervised auxiliary tasks. By keeping an exponential moving\naverage of past encoders and distilling the accumulated knowledge, we are able\nto maintain the network's validation performance on a number of tasks without\nany form of replay, parameter isolation or retraining techniques typically used\nin continual learning. We show that models trained with LiSS perform better on\npast tasks, while also being more robust than the CycleGAN baseline to color\nbias and entity entanglement (when two entities are very close).\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 23:23:51 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Schmidt", "Victor", ""], ["Sreedhar", "Makesh Narsimhan", ""], ["ElAraby", "Mostafa", ""], ["Rish", "Irina", ""]]}, {"id": "2004.00163", "submitter": "Zhekun Luo", "authors": "Zhekun Luo, Devin Guillory, Baifeng Shi, Wei Ke, Fang Wan, Trevor\n  Darrell, Huijuan Xu", "title": "Weakly-Supervised Action Localization with Expectation-Maximization\n  Multi-Instance Learning", "comments": "Accepted at European Conference on Computer Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised action localization requires training a model to localize\nthe action segments in the video given only video level action label. It can be\nsolved under the Multiple Instance Learning (MIL) framework, where a bag\n(video) contains multiple instances (action segments). Since only the bag's\nlabel is known, the main challenge is assigning which key instances within the\nbag to trigger the bag's label. Most previous models use attention-based\napproaches applying attentions to generate the bag's representation from\ninstances, and then train it via the bag's classification. These models,\nhowever, implicitly violate the MIL assumption that instances in negative bags\nshould be uniformly negative. In this work, we explicitly model the key\ninstances assignment as a hidden variable and adopt an Expectation-Maximization\n(EM) framework. We derive two pseudo-label generation schemes to model the E\nand M process and iteratively optimize the likelihood lower bound. We show that\nour EM-MIL approach more accurately models both the learning objective and the\nMIL assumptions. It achieves state-of-the-art performance on two standard\nbenchmarks, THUMOS14 and ActivityNet1.2.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 23:36:04 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 19:26:17 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Luo", "Zhekun", ""], ["Guillory", "Devin", ""], ["Shi", "Baifeng", ""], ["Ke", "Wei", ""], ["Wan", "Fang", ""], ["Darrell", "Trevor", ""], ["Xu", "Huijuan", ""]]}, {"id": "2004.00171", "submitter": "Shengjie Zhu", "authors": "Shengjie Zhu, Garrick Brazil, Xiaoming Liu", "title": "The Edge of Depth: Explicit Constraints between Segmentation and Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the mutual benefits of two common computer vision\ntasks, self-supervised depth estimation and semantic segmentation from images.\nFor example, to help unsupervised monocular depth estimation, constraints from\nsemantic segmentation has been explored implicitly such as sharing and\ntransforming features. In contrast, we propose to explicitly measure the border\nconsistency between segmentation and depth and minimize it in a greedy manner\nby iteratively supervising the network towards a locally optimal solution.\nPartially this is motivated by our observation that semantic segmentation even\ntrained with limited ground truth (200 images of KITTI) can offer more accurate\nborder than that of any (monocular or stereo) image-based depth estimation.\nThrough extensive experiments, our proposed approach advances the state of the\nart on unsupervised monocular depth estimation in the KITTI.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 00:03:20 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhu", "Shengjie", ""], ["Brazil", "Garrick", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2004.00173", "submitter": "Benoit Anctil-Robitaille", "authors": "Benoit Anctil-Robitaille, Christian Desrosiers, Herve Lombaert", "title": "Manifold-Aware CycleGAN for High-Resolution Structural-to-DTI Synthesis", "comments": "Accepted at MICCAI 2020 International Workshop on Computational\n  Diffusion MRI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired image-to-image translation has been applied successfully to natural\nimages but has received very little attention for manifold-valued data such as\nin diffusion tensor imaging (DTI). The non-Euclidean nature of DTI prevents\ncurrent generative adversarial networks (GANs) from generating plausible images\nand has mainly limited their application to diffusion MRI scalar maps, such as\nfractional anisotropy (FA) or mean diffusivity (MD). Even if these scalar maps\nare clinically useful, they mostly ignore fiber orientations and therefore have\nlimited applications for analyzing brain fibers. Here, we propose a\nmanifold-aware CycleGAN that learns the generation of high-resolution DTI from\nunpaired T1w images. We formulate the objective as a Wasserstein distance\nminimization problem of data distributions on a Riemannian manifold of\nsymmetric positive definite 3x3 matrices SPD(3), using adversarial and\ncycle-consistency losses. To ensure that the generated diffusion tensors lie on\nthe SPD(3) manifold, we exploit the theoretical properties of the exponential\nand logarithm maps of the Log-Euclidean metric. We demonstrate that, unlike\nstandard GANs, our method is able to generate realistic high-resolution DTI\nthat can be used to compute diffusion-based metrics and potentially run fiber\ntractography algorithms. To evaluate our model's performance, we compute the\ncosine similarity between the generated tensors principal orientation and their\nground-truth orientation, the mean squared error (MSE) of their derived FA\nvalues and the Log-Euclidean distance between the tensors. We demonstrate that\nour method produces 2.5 times better FA MSE than a standard CycleGAN and up to\n30% better cosine similarity than a manifold-aware Wasserstein GAN while\nsynthesizing sharp high-resolution DTI.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 00:08:14 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 18:16:51 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 00:19:36 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Anctil-Robitaille", "Benoit", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "2004.00176", "submitter": "Long Zhao", "authors": "Long Zhao, Xi Peng, Yuxiao Chen, Mubbasir Kapadia, Dimitris N. Metaxas", "title": "Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets\n  without Superior Knowledge", "comments": "In CVPR 2020. (15 pages including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal knowledge distillation deals with transferring knowledge from a\nmodel trained with superior modalities (Teacher) to another model trained with\nweak modalities (Student). Existing approaches require paired training examples\nexist in both modalities. However, accessing the data from superior modalities\nmay not always be feasible. For example, in the case of 3D hand pose\nestimation, depth maps, point clouds, or stereo images usually capture better\nhand structures than RGB images, but most of them are expensive to be\ncollected. In this paper, we propose a novel scheme to train the Student in a\nTarget dataset where the Teacher is unavailable. Our key idea is to generalize\nthe distilled cross-modal knowledge learned from a Source dataset, which\ncontains paired examples from both modalities, to the Target dataset by\nmodeling knowledge as priors on parameters of the Student. We name our method\n\"Cross-Modal Knowledge Generalization\" and demonstrate that our scheme results\nin competitive performance for 3D hand pose estimation on standard benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 00:28:15 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhao", "Long", ""], ["Peng", "Xi", ""], ["Chen", "Yuxiao", ""], ["Kapadia", "Mubbasir", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "2004.00180", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Lizhi Yang, Stan Sclaroff, Kate Saenko, Trevor Darrell", "title": "Spatio-Temporal Action Detection with Multi-Object Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal action detection in videos requires localizing the action\nboth spatially and temporally in the form of an \"action tube\". Nowadays, most\nspatio-temporal action detection datasets (e.g. UCF101-24, AVA, DALY) are\nannotated with action tubes that contain a single person performing the action,\nthus the predominant action detection models simply employ a person detection\nand tracking pipeline for localization. However, when the action is defined as\nan interaction between multiple objects, such methods may fail since each\nbounding box in the action tube contains multiple objects instead of one\nperson. In this paper, we study the spatio-temporal action detection problem\nwith multi-object interaction. We introduce a new dataset that is annotated\nwith action tubes containing multi-object interactions. Moreover, we propose an\nend-to-end spatio-temporal action detection model that performs both spatial\nand temporal regression simultaneously. Our spatial regression may enclose\nmultiple objects participating in the action. During test time, we simply\nconnect the regressed bounding boxes within the predicted temporal duration\nusing a simple heuristic. We report the baseline results of our proposed model\non this new dataset, and also show competitive results on the standard\nbenchmark UCF101-24 using only RGB input.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 00:54:56 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Xu", "Huijuan", ""], ["Yang", "Lizhi", ""], ["Sclaroff", "Stan", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "2004.00184", "submitter": "Michel Besserve", "authors": "Michel Besserve, R\\'emy Sun, Dominik Janzing and Bernhard Sch\\\"olkopf", "title": "A theory of independent mechanisms for extrapolation in generative\n  models", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models reproduce complex empirical data but cannot\nextrapolate to novel environments. An intuitive idea to promote extrapolation\ncapabilities is to enforce the architecture to have the modular structure of a\ncausal graphical model, where one can intervene on each module independently of\nthe others in the graph. We develop a framework to formalize this intuition,\nusing the principle of Independent Causal Mechanisms, and show how\nover-parameterization of generative neural networks can hinder extrapolation\ncapabilities. Our experiments on the generation of human faces shows successive\nlayers of a generator architecture implement independent mechanisms to some\nextent, allowing meaningful extrapolations. Finally, we illustrate that\nindependence of mechanisms may be enforced during training to improve\nextrapolation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 01:01:43 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Besserve", "Michel", ""], ["Sun", "R\u00e9my", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2004.00186", "submitter": "Guodong Xu", "authors": "Guodong Xu, Wenxiao Wang, Zili Liu, Liang Xie, Zheng Yang, Haifeng\n  Liu, Deng Cai", "title": "Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object\n  Detection from Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection based on point clouds has become more and more popular.\nSome methods propose localizing 3D objects directly from raw point clouds to\navoid information loss. However, these methods come with complex structures and\nsignificant computational overhead, limiting its broader application in\nreal-time scenarios. Some methods choose to transform the point cloud data into\ncompact tensors first and leverage off-the-shelf 2D detectors to propose 3D\nobjects, which is much faster and achieves state-of-the-art results. However,\nbecause of the inconsistency between 2D and 3D data, we argue that the\nperformance of compact tensor-based 3D detectors is restricted if we use 2D\ndetectors without corresponding modification. Specifically, the distribution of\npoint clouds is uneven, with most points gather on the boundary of objects,\nwhile detectors for 2D data always extract features evenly. Motivated by this\nobservation, we propose DENse Feature Indicator (DENFI), a universal module\nthat helps 3D detectors focus on the densest region of the point clouds in a\nboundary-aware manner. Moreover, DENFI is lightweight and guarantees real-time\nspeed when applied to 3D object detectors. Experiments on KITTI dataset show\nthat DENFI improves the performance of the baseline single-stage detector\nremarkably, which achieves new state-of-the-art performance among previous 3D\ndetectors, including both two-stage and multi-sensor fusion methods, in terms\nof mAP with a 34FPS detection speed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 01:21:23 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Xu", "Guodong", ""], ["Wang", "Wenxiao", ""], ["Liu", "Zili", ""], ["Xie", "Liang", ""], ["Yang", "Zheng", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "2004.00191", "submitter": "Yanglan  Ou", "authors": "Yanglan Ou, Yuan Xue, Ye Yuan, Tao Xu, Vincent Pisztora, Jia Li,\n  Xiaolei Huang", "title": "Semi-Supervised Cervical Dysplasia Classification With Learnable Graph\n  Convolutional Network", "comments": "ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical cancer is the second most prevalent cancer affecting women today. As\nthe early detection of cervical carcinoma relies heavily upon screening and\npre-clinical testing, digital cervicography has great potential as a primary or\nauxiliary screening tool, especially in low-resource regions due to its low\ncost and easy access. Although an automated cervical dysplasia detection system\nhas been desirable, traditional fully-supervised training of such systems\nrequires large amounts of annotated data which are often labor-intensive to\ncollect. To alleviate the need for much manual annotation, we propose a novel\ngraph convolutional network (GCN) based semi-supervised classification model\nthat can be trained with fewer annotations. In existing GCNs, graphs are\nconstructed with fixed features and can not be updated during the learning\nprocess. This limits their ability to exploit new features learned during graph\nconvolution. In this paper, we propose a novel and more flexible GCN model with\na feature encoder that adaptively updates the adjacency matrix during learning\nand demonstrate that this model design leads to improved performance. Our\nexperimental results on a cervical dysplasia classification dataset show that\nthe proposed framework outperforms previous methods under a semi-supervised\nsetting, especially when the labeled samples are scarce.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 01:53:26 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Ou", "Yanglan", ""], ["Xue", "Yuan", ""], ["Yuan", "Ye", ""], ["Xu", "Tao", ""], ["Pisztora", "Vincent", ""], ["Li", "Jia", ""], ["Huang", "Xiaolei", ""]]}, {"id": "2004.00202", "submitter": "Chiho Choi", "authors": "Chiho Choi, Joon Hee Choi, Srikanth Malla, Jiachen Li", "title": "Shared Cross-Modal Trajectory Prediction for Autonomous Driving", "comments": "CVPR 2021 [Oral]. arXiv admin note: substantial text overlap with\n  arXiv:2011.08436", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Predicting future trajectories of traffic agents in highly interactive\nenvironments is an essential and challenging problem for the safe operation of\nautonomous driving systems. On the basis of the fact that self-driving vehicles\nare equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,\nradar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit\nfrom the use of multiple input modalities. At training time, our model learns\nto embed a set of complementary features in a shared latent space by jointly\noptimizing the objective functions across different types of input data. At\ntest time, a single input modality (e.g., LiDAR data) is required to generate\npredictions from the input perspective (i.e., in the LiDAR space), while taking\nadvantages from the model trained with multiple sensor modalities. An extensive\nevaluation is conducted to show the efficacy of the proposed framework using\ntwo benchmark driving datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 02:44:30 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 20:14:25 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 21:24:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Choi", "Chiho", ""], ["Choi", "Joon Hee", ""], ["Malla", "Srikanth", ""], ["Li", "Jiachen", ""]]}, {"id": "2004.00207", "submitter": "Xin Yang", "authors": "Chaoyu Chen, Xin Yang, Ruobing Huang, Wenlong Shi, Shengfeng Liu,\n  Mingrong Lin, Yuhao Huang, Yong Yang, Yuanji Zhang, Huanjia Luo, Yankai\n  Huang, Yi Xiong, Dong Ni", "title": "Region Proposal Network with Graph Prior and IoU-Balance Loss for\n  Landmark Detection in 3D Ultrasound", "comments": "IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D ultrasound (US) can facilitate detailed prenatal examinations for fetal\ngrowth monitoring. To analyze a 3D US volume, it is fundamental to identify\nanatomical landmarks of the evaluated organs accurately. Typical deep learning\nmethods usually regress the coordinates directly or involve heatmap-matching.\nHowever, these methods struggle to deal with volumes with large sizes and the\nhighly-varying positions and orientations of fetuses. In this work, we exploit\nan object detection framework to detect landmarks in 3D fetal facial US\nvolumes. By regressing multiple parameters of the landmark-centered bounding\nbox (B-box) with a strict criteria, the proposed model is able to pinpoint the\nexact location of the targeted landmarks. Specifically, the model uses a 3D\nregion proposal network (RPN) to generate 3D candidate regions, followed by\nseveral 3D classification branches to select the best candidate. It also adopts\nan IoU-balance loss to improve communications between branches that benefits\nthe learning process. Furthermore, it leverages a distance-based graph prior to\nregularize the training and helps to reduce false positive predictions. The\nperformance of the proposed framework is evaluated on a 3D US dataset to detect\nfive key fetal facial landmarks. Results showed the proposed method outperforms\nsome of the state-of-the-art methods in efficacy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:00:03 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Chen", "Chaoyu", ""], ["Yang", "Xin", ""], ["Huang", "Ruobing", ""], ["Shi", "Wenlong", ""], ["Liu", "Shengfeng", ""], ["Lin", "Mingrong", ""], ["Huang", "Yuhao", ""], ["Yang", "Yong", ""], ["Zhang", "Yuanji", ""], ["Luo", "Huanjia", ""], ["Huang", "Yankai", ""], ["Xiong", "Yi", ""], ["Ni", "Dong", ""]]}, {"id": "2004.00214", "submitter": "Boyi Jiang", "authors": "Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu and Hujun\n  Bao", "title": "BCNet: Learning Body and Cloth Shape from A Single Image", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem to automatically reconstruct garment\nand body shapes from a single near-front view RGB image. To this end, we\npropose a layered garment representation on top of SMPL and novelly make the\nskinning weight of garment independent of the body mesh, which significantly\nimproves the expression ability of our garment model. Compared with existing\nmethods, our method can support more garment categories and recover more\naccurate geometry. To train our model, we construct two large scale datasets\nwith ground truth body and garment geometries as well as paired color images.\nCompared with single mesh or non-parametric representation, our method can\nachieve more flexible control with separate meshes, makes applications like\nre-pose, garment transfer, and garment texture mapping possible. Code and some\ndata is available at https://github.com/jby1993/BCNet.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:41:36 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 10:03:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Jiang", "Boyi", ""], ["Zhang", "Juyong", ""], ["Hong", "Yang", ""], ["Luo", "Jinhao", ""], ["Liu", "Ligang", ""], ["Bao", "Hujun", ""]]}, {"id": "2004.00218", "submitter": "Sukrit Gupta", "authors": "Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman\n  Padmanabhan and Bal\\'azs Guly\\'as", "title": "3D Deep Learning on Medical Images: A Review", "comments": "Published in Sensors Journal\n  (https://www.mdpi.com/1424-8220/20/18/5097)", "journal-ref": "Sensors 2020, 20, 5097", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The rapid advancements in machine learning, graphics processing technologies\nand the availability of medical imaging data have led to a rapid increase in\nthe use of deep learning models in the medical domain. This was exacerbated by\nthe rapid advancements in convolutional neural network (CNN) based\narchitectures, which were adopted by the medical imaging community to assist\nclinicians in disease diagnosis. Since the grand success of AlexNet in 2012,\nCNNs have been increasingly used in medical image analysis to improve the\nefficiency of human clinicians. In recent years, three-dimensional (3D) CNNs\nhave been employed for the analysis of medical images. In this paper, we trace\nthe history of how the 3D CNN was developed from its machine learning roots, we\nprovide a brief mathematical description of 3D CNN and provide the\npreprocessing steps required for medical images before feeding them to 3D CNNs.\nWe review the significant research in the field of 3D medical imaging analysis\nusing 3D CNNs (and its variants) in different medical areas such as\nclassification, segmentation, detection and localization. We conclude by\ndiscussing the challenges associated with the use of 3D CNNs in the medical\nimaging domain (and the use of deep learning models in general) and possible\nfuture trends in the field.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:56:48 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 05:26:19 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 04:28:29 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 08:38:19 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Singh", "Satya P.", ""], ["Wang", "Lipo", ""], ["Gupta", "Sukrit", ""], ["Goli", "Haveesh", ""], ["Padmanabhan", "Parasuraman", ""], ["Guly\u00e1s", "Bal\u00e1zs", ""]]}, {"id": "2004.00221", "submitter": "Alvin Wan", "authors": "Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Henry Jin,\n  Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez", "title": "NBDT: Neural-Backed Decision Trees", "comments": "8 pages, 7 figures, accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications such as finance and medicine demand accurate\nand justifiable predictions, barring most deep learning methods from use. In\nresponse, previous work combines decision trees with deep learning, yielding\nmodels that (1) sacrifice interpretability for accuracy or (2) sacrifice\naccuracy for interpretability. We forgo this dilemma by jointly improving\naccuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs\nreplace a neural network's final linear layer with a differentiable sequence of\ndecisions and a surrogate loss. This forces the model to learn high-level\nconcepts and lessens reliance on highly-uncertain decisions, yielding (1)\naccuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet\nand better generalize to unseen classes by up to 16%. Furthermore, our\nsurrogate loss improves the original model's accuracy by up to 2%. NBDTs also\nafford (2) interpretability: improving human trustby clearly identifying model\nmistakes and assisting in dataset debugging. Code and pretrained NBDTs are at\nhttps://github.com/alvinwan/neural-backed-decision-trees.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:04:03 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 22:33:20 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 03:06:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wan", "Alvin", ""], ["Dunlap", "Lisa", ""], ["Ho", "Daniel", ""], ["Yin", "Jihan", ""], ["Lee", "Scott", ""], ["Jin", "Henry", ""], ["Petryk", "Suzanne", ""], ["Bargal", "Sarah Adel", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2004.00222", "submitter": "Chen Chen", "authors": "Sijie Zhu, Chen Chen, and Waqas Sultani", "title": "Video Anomaly Detection for Smart Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern intelligent video surveillance systems, automatic anomaly detection\nthrough computer vision analytics plays a pivotal role which not only\nsignificantly increases monitoring efficiency but also reduces the burden on\nlive monitoring. Anomalies in videos are broadly defined as events or\nactivities that are unusual and signify irregular behavior. The goal of anomaly\ndetection is to temporally or spatially localize the anomaly events in video\nsequences. Temporal localization (i.e. indicating the start and end frames of\nthe anomaly event in a video) is referred to as frame-level detection. Spatial\nlocalization, which is more challenging, means to identify the pixels within\neach anomaly frame that correspond to the anomaly event. This setting is\nusually referred to as pixel-level detection. In this paper, we provide a brief\noverview of the recent research progress on video anomaly detection and\nhighlight a few future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:13:55 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:54:39 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 19:07:05 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Sultani", "Waqas", ""]]}, {"id": "2004.00225", "submitter": "Wenqian Ronny Huang", "authors": "W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, Tom Goldstein", "title": "MetaPoison: Practical General-purpose Clean-label Data Poisoning", "comments": "Conference paper at NeurIPS 2020. First two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning -- the process by which an attacker takes control of a model\nby making imperceptible changes to a subset of the training data -- is an\nemerging threat in the context of neural networks. Existing attacks for data\npoisoning neural networks have relied on hand-crafted heuristics, because\nsolving the poisoning problem directly via bilevel optimization is generally\nthought of as intractable for deep models. We propose MetaPoison, a first-order\nmethod that approximates the bilevel problem via meta-learning and crafts\npoisons that fool neural networks. MetaPoison is effective: it outperforms\nprevious clean-label poisoning methods by a large margin. MetaPoison is robust:\npoisoned data made for one model transfer to a variety of victim models with\nunknown training settings and architectures. MetaPoison is general-purpose, it\nworks not only in fine-tuning scenarios, but also for end-to-end training from\nscratch, which till now hasn't been feasible for clean-label attacks with deep\nnets. MetaPoison can achieve arbitrary adversary goals -- like using poisons of\none class to make a target image don the label of another arbitrarily chosen\nclass. Finally, MetaPoison works in the real-world. We demonstrate for the\nfirst time successful data poisoning of models trained on the black-box Google\nCloud AutoML API. Code and premade poisons are provided at\nhttps://github.com/wronnyhuang/metapoison\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:23:20 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 02:40:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Huang", "W. Ronny", ""], ["Geiping", "Jonas", ""], ["Fowl", "Liam", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "2004.00226", "submitter": "Xin Yang", "authors": "Jiamin Liang, Xin Yang, Haoming Li, Yi Wang, Manh The Van, Haoran Dou,\n  Chaoyu Chen, Jinghui Fang, Xiaowen Liang, Zixin Mai, Guowen Zhu, Zhiyi Chen,\n  Dong Ni", "title": "Synthesis and Edition of Ultrasound Images via Sketch Guided Progressive\n  Growing GANs", "comments": "IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is widely accepted in clinic for anatomical structure\ninspection. However, lacking in resources to practice US scan, novices often\nstruggle to learn the operation skills. Also, in the deep learning era,\nautomated US image analysis is limited by the lack of annotated samples.\nEfficiently synthesizing realistic, editable and high resolution US images can\nsolve the problems. The task is challenging and previous methods can only\npartially complete it. In this paper, we devise a new framework for US image\nsynthesis. Particularly, we firstly adopt a sketch generative adversarial\nnetworks (Sgan) to introduce background sketch upon object mask in a\nconditioned generative adversarial network. With enriched sketch cues, Sgan can\ngenerate realistic US images with editable and fine-grained structure details.\nAlthough effective, Sgan is hard to generate high resolution US images. To\nachieve this, we further implant the Sgan into a progressive growing scheme\n(PGSgan). By smoothly growing both generator and discriminator, PGSgan can\ngradually synthesize US images from low to high resolution. By synthesizing\novary and follicle US images, our extensive perceptual evaluation, user study\nand segmentation results prove the promising efficacy and efficiency of the\nproposed PGSgan.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:24:01 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Liang", "Jiamin", ""], ["Yang", "Xin", ""], ["Li", "Haoming", ""], ["Wang", "Yi", ""], ["Van", "Manh The", ""], ["Dou", "Haoran", ""], ["Chen", "Chaoyu", ""], ["Fang", "Jinghui", ""], ["Liang", "Xiaowen", ""], ["Mai", "Zixin", ""], ["Zhu", "Guowen", ""], ["Chen", "Zhiyi", ""], ["Ni", "Dong", ""]]}, {"id": "2004.00230", "submitter": "Shang Gao", "authors": "Shang Gao, Jingya Wang, Huchuan Lu, Zimo Liu", "title": "Pose-guided Visible Part Matching for Occluded Person ReID", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occluded person re-identification is a challenging task as the appearance\nvaries substantially with various obstacles, especially in the crowd scenario.\nTo address this issue, we propose a Pose-guided Visible Part Matching (PVPM)\nmethod that jointly learns the discriminative features with pose-guided\nattention and self-mines the part visibility in an end-to-end framework.\nSpecifically, the proposed PVPM includes two key components: 1) pose-guided\nattention (PGA) method for part feature pooling that exploits more\ndiscriminative local features; 2) pose-guided visibility predictor (PVP) that\nestimates whether a part suffers the occlusion or not. As there are no ground\ntruth training annotations for the occluded part, we turn to utilize the\ncharacteristic of part correspondence in positive pairs and self-mining the\ncorrespondence scores via graph matching. The generated correspondence scores\nare then utilized as pseudo-labels for visibility predictor (PVP). Experimental\nresults on three reported occluded benchmarks show that the proposed method\nachieves competitive performance to state-of-the-art methods. The source codes\nare available at https://github.com/hh23333/PVPM\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 04:36:51 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gao", "Shang", ""], ["Wang", "Jingya", ""], ["Lu", "Huchuan", ""], ["Liu", "Zimo", ""]]}, {"id": "2004.00251", "submitter": "Hong-Gyu Jung", "authors": "Jin-Woo Seo, Hong-Gyu Jung, Seong-Whan Lee", "title": "Self-Augmentation: Generalizing Deep Networks to Unseen Classes for\n  Few-Shot Learning", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": "10.1016/j.neunet.2021.02.007", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to classify unseen classes with a few training\nexamples. While recent works have shown that standard mini-batch training with\na carefully designed training strategy can improve generalization ability for\nunseen classes, well-known problems in deep networks such as memorizing\ntraining statistics have been less explored for few-shot learning. To tackle\nthis issue, we propose self-augmentation that consolidates self-mix and\nself-distillation. Specifically, we exploit a regional dropout technique called\nself-mix, in which a patch of an image is substituted into other values in the\nsame image. Then, we employ a backbone network that has auxiliary branches with\nits own classifier to enforce knowledge sharing. Lastly, we present a local\nrepresentation learner to further exploit a few training examples for unseen\nclasses. Experimental results show that the proposed method outperforms the\nstate-of-the-art methods for prevalent few-shot benchmarks and improves the\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 06:39:08 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 04:53:01 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 08:37:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Seo", "Jin-Woo", ""], ["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2004.00255", "submitter": "Xi Li", "authors": "Weichao Li, Xi Li, Omar Elfarouk Bourahla, Fuxian Huang, Fei Wu, Wei\n  Liu, Zhiheng Wang, and Hongmin Liu", "title": "Progressive Multi-Stage Learning for Discriminative Tracking", "comments": "accepted to IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is typically solved as a discriminative learning problem that\nusually requires high-quality samples for online model adaptation. It is a\ncritical and challenging problem to evaluate the training samples collected\nfrom previous predictions and employ sample selection by their quality to train\nthe model.\n  To tackle the above problem, we propose a joint discriminative learning\nscheme with the progressive multi-stage optimization policy of sample selection\nfor robust visual tracking. The proposed scheme presents a novel time-weighted\nand detection-guided self-paced learning strategy for easy-to-hard sample\nselection, which is capable of tolerating relatively large intra-class\nvariations while maintaining inter-class separability. Such a self-paced\nlearning strategy is jointly optimized in conjunction with the discriminative\ntracking process, resulting in robust tracking results. Experiments on the\nbenchmark datasets demonstrate the effectiveness of the proposed learning\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 07:01:30 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Li", "Weichao", ""], ["Li", "Xi", ""], ["Bourahla", "Omar Elfarouk", ""], ["Huang", "Fuxian", ""], ["Wu", "Fei", ""], ["Liu", "Wei", ""], ["Wang", "Zhiheng", ""], ["Liu", "Hongmin", ""]]}, {"id": "2004.00272", "submitter": "Lei Zhao", "authors": "Lei Zhao, Xiaohui Wang, and Lei Huang", "title": "An Efficient Agreement Mechanism in CapsNets By Pairwise Product", "comments": "Accepted to ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks (CapsNets) are capable of modeling visual hierarchical\nrelationships, which is achieved by the \"routing-by-agreement\" mechanism. This\npaper proposes a pairwise agreement mechanism to build capsules, inspired by\nthe feature interactions of factorization machines (FMs). The proposed method\nhas a much lower computation complexity. We further proposed a new CapsNet\narchitecture that combines the strengths of residual networks in representing\nlow-level visual features and CapsNets in modeling the relationships of parts\nto wholes. We conduct comprehensive experiments to compare the routing\nalgorithms, including dynamic routing, EM routing, and our proposed FM\nagreement, based on both architectures of original CapsNet and our proposed\none, and the results show that our method achieves both excellent performance\nand efficiency under a variety of situations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:09:23 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhao", "Lei", ""], ["Wang", "Xiaohui", ""], ["Huang", "Lei", ""]]}, {"id": "2004.00277", "submitter": "Chunxiao Liu", "authors": "Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang,\n  Yongdong Zhang", "title": "Graph Structured Network for Image-Text Matching", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching has received growing interest since it bridges vision and\nlanguage. The key challenge lies in how to learn correspondence between image\nand text. Existing works learn coarse correspondence based on object\nco-occurrence statistics, while failing to learn fine-grained phrase\ncorrespondence. In this paper, we present a novel Graph Structured Matching\nNetwork (GSMN) to learn fine-grained correspondence. The GSMN explicitly models\nobject, relation and attribute as a structured phrase, which not only allows to\nlearn correspondence of object, relation and attribute separately, but also\nbenefits to learn fine-grained correspondence of structured phrase. This is\nachieved by node-level matching and structure-level matching. The node-level\nmatching associates each node with its relevant nodes from another modality,\nwhere the node can be object, relation or attribute. The associated nodes then\njointly infer fine-grained correspondence by fusing neighborhood associations\nat structure-level matching. Comprehensive experiments show that GSMN\noutperforms state-of-the-art methods on benchmarks, with relative Recall@1\nimprovements of nearly 7% and 2% on Flickr30K and MSCOCO, respectively. Code\nwill be released at: https://github.com/CrossmodalGroup/GSMN.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:20:42 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Liu", "Chunxiao", ""], ["Mao", "Zhendong", ""], ["Zhang", "Tianzhu", ""], ["Xie", "Hongtao", ""], ["Wang", "Bin", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2004.00280", "submitter": "Hengtong Hu", "authors": "Hengtong Hu, Lingxi Xie, Richang Hong, Qi Tian", "title": "Creating Something from Nothing: Unsupervised Knowledge Distillation for\n  Cross-Modal Hashing", "comments": "This paper has been accepted for CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, cross-modal hashing (CMH) has attracted increasing\nattentions, mainly because its potential ability of mapping contents from\ndifferent modalities, especially in vision and language, into the same space,\nso that it becomes efficient in cross-modal data retrieval. There are two main\nframeworks for CMH, differing from each other in whether semantic supervision\nis required. Compared to the unsupervised methods, the supervised methods often\nenjoy more accurate results, but require much heavier labors in data\nannotation. In this paper, we propose a novel approach that enables guiding a\nsupervised method using outputs produced by an unsupervised method.\nSpecifically, we make use of teacher-student optimization for propagating\nknowledge. Experiments are performed on two popular CMH benchmarks, i.e., the\nMIRFlickr and NUS-WIDE datasets. Our approach outperforms all existing\nunsupervised methods by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:32:15 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Hu", "Hengtong", ""], ["Xie", "Lingxi", ""], ["Hong", "Richang", ""], ["Tian", "Qi", ""]]}, {"id": "2004.00288", "submitter": "Yuge Huang", "authors": "Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen,\n  Shaoxin Li, Jilin Li, Feiyue Huang", "title": "CurricularFace: Adaptive Curriculum Learning Loss for Deep Face\n  Recognition", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging topic in face recognition, designing margin-based loss\nfunctions can increase the feature margin between different classes for\nenhanced discriminability. More recently, the idea of mining-based strategies\nis adopted to emphasize the misclassified samples, achieving promising results.\nHowever, during the entire training process, the prior methods either do not\nexplicitly emphasize the sample based on its importance that renders the hard\nsamples not fully exploited; or explicitly emphasize the effects of\nsemi-hard/hard samples even at the early training stage that may lead to\nconvergence issue. In this work, we propose a novel Adaptive Curriculum\nLearning loss (CurricularFace) that embeds the idea of curriculum learning into\nthe loss function to achieve a novel training strategy for deep face\nrecognition, which mainly addresses easy samples in the early training stage\nand hard ones in the later stage. Specifically, our CurricularFace adaptively\nadjusts the relative importance of easy and hard samples during different\ntraining stages. In each stage, different samples are assigned with different\nimportance according to their corresponding difficultness. Extensive\nexperimental results on popular benchmarks demonstrate the superiority of our\nCurricularFace over the state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:43:10 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Huang", "Yuge", ""], ["Wang", "Yuhan", ""], ["Tai", "Ying", ""], ["Liu", "Xiaoming", ""], ["Shen", "Pengcheng", ""], ["Li", "Shaoxin", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2004.00292", "submitter": "Christoffer Rasmussen", "authors": "Christoffer B{\\o}gelund Rasmussen and Thomas B. Moeslund", "title": "Evaluation of Model Selection for Kernel Fragment Recognition in Corn\n  Silage", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection when designing deep learning systems for specific use-cases\ncan be a challenging task as many options exist and it can be difficult to know\nthe trade-off between them. Therefore, we investigate a number of state of the\nart CNN models for the task of measuring kernel fragmentation in harvested corn\nsilage. The models are evaluated across a number of feature extractors and\nimage sizes in order to determine optimal model design choices based upon the\ntrade-off between model complexity, accuracy and speed. We show that accuracy\nimprovements can be made with more complex meta-architectures and speed can be\noptimised by decreasing the image size with only slight losses in accuracy.\nAdditionally, we show improvements in Average Precision at an Intersection over\nUnion of 0.5 of up to 20 percentage points while also decreasing inference time\nin comparison to previously published work. This result for better model\nselection enables opportunities for creating systems that can aid farmers in\nimproving their silage quality while harvesting.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:56:01 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Rasmussen", "Christoffer B\u00f8gelund", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2004.00303", "submitter": "Dan Halbersberg", "authors": "Dan Halbersberg, Aharon Bar Hillel, Shon Mendelson, Daniel Koster,\n  Lena Karol, and Boaz Lerner", "title": "Transfer Learning of Photometric Phenotypes in Agriculture Using\n  Metadata", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of photometric plant phenotypes (e.g., hue, shine, chroma) in\nfield conditions is important for decisions on the expected yield quality,\nfruit ripeness, and need for further breeding. Estimating these from images is\ndifficult due to large variances in lighting conditions, shadows, and sensor\nproperties. We combine the image and metadata regarding capturing conditions\nembedded into a network, enabling more accurate estimation and transfer between\ndifferent conditions. Compared to a state-of-the-art deep CNN and a human\nexpert, metadata embedding improves the estimation of the tomato's hue and\nchroma.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:24:34 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Halbersberg", "Dan", ""], ["Hillel", "Aharon Bar", ""], ["Mendelson", "Shon", ""], ["Koster", "Daniel", ""], ["Karol", "Lena", ""], ["Lerner", "Boaz", ""]]}, {"id": "2004.00305", "submitter": "Kenan Dai", "authors": "Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li, Huchuan Lu, and\n  Xiaoyun Yang", "title": "High-Performance Long-Term Tracking with Meta-Updater", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term visual tracking has drawn increasing attention because it is much\ncloser to practical applications than short-term tracking. Most top-ranked\nlong-term trackers adopt the offline-trained Siamese architectures, thus, they\ncannot benefit from great progress of short-term trackers with online update.\nHowever, it is quite risky to straightforwardly introduce online-update-based\ntrackers to solve the long-term problem, due to long-term uncertain and noisy\nobservations. In this work, we propose a novel offline-trained Meta-Updater to\naddress an important but unsolved problem: Is the tracker ready for updating in\nthe current frame? The proposed meta-updater can effectively integrate\ngeometric, discriminative, and appearance cues in a sequential manner, and then\nmine the sequential information with a designed cascaded LSTM module. Our\nmeta-updater learns a binary output to guide the tracker's update and can be\neasily embedded into different trackers. This work also introduces a long-term\ntracking framework consisting of an online local tracker, an online verifier, a\nSiamRPN-based re-detector, and our meta-updater. Numerous experimental results\non the VOT2018LT, VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our\ntracker performs remarkably better than other competing algorithms. Our project\nis available on the website: https://github.com/Daikenan/LTMU.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:29:23 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Dai", "Kenan", ""], ["Zhang", "Yunhua", ""], ["Wang", "Dong", ""], ["Li", "Jianhua", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2004.00306", "submitter": "Sravanti Addepalli", "authors": "Sravanti Addepalli, Vivek B.S., Arya Baburaj, Gaurang Sriramanan, R.\n  Venkatesh Babu", "title": "Towards Achieving Adversarial Robustness by Enforcing Feature\n  Consistency Across Bit Planes", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans, we inherently perceive images based on their predominant features,\nand ignore noise embedded within lower bit planes. On the contrary, Deep Neural\nNetworks are known to confidently misclassify images corrupted with\nmeticulously crafted perturbations that are nearly imperceptible to the human\neye. In this work, we attempt to address this problem by training networks to\nform coarse impressions based on the information in higher bit planes, and use\nthe lower bit planes only to refine their prediction. We demonstrate that, by\nimposing consistency on the representations learned across differently\nquantized images, the adversarial robustness of networks improves significantly\nwhen compared to a normally trained model. Present state-of-the-art defenses\nagainst adversarial attacks require the networks to be explicitly trained using\nadversarial samples that are computationally expensive to generate. While such\nmethods that use adversarial training continue to achieve the best results,\nthis work paves the way towards achieving robustness without having to\nexplicitly train on adversarial samples. The proposed approach is therefore\nfaster, and also closer to the natural learning process in humans.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:31:10 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Addepalli", "Sravanti", ""], ["S.", "Vivek B.", ""], ["Baburaj", "Arya", ""], ["Sriramanan", "Gaurang", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2004.00315", "submitter": "Linjun Zhou", "authors": "Linjun Zhou, Peng Cui, Xu Jia, Shiqiang Yang, Qi Tian", "title": "Learning to Select Base Classes for Few-shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning has attracted intensive research attention in recent years.\nMany methods have been proposed to generalize a model learned from provided\nbase classes to novel classes, but no previous work studies how to select base\nclasses, or even whether different base classes will result in different\ngeneralization performance of the learned model. In this paper, we utilize a\nsimple yet effective measure, the Similarity Ratio, as an indicator for the\ngeneralization performance of a few-shot model. We then formulate the base\nclass selection problem as a submodular optimization problem over Similarity\nRatio. We further provide theoretical analysis on the optimization lower bound\nof different optimization methods, which could be used to identify the most\nappropriate algorithm for different experimental settings. The extensive\nexperiments on ImageNet, Caltech256 and CUB-200-2011 demonstrate that our\nproposed method is effective in selecting a better base dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:55:18 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhou", "Linjun", ""], ["Cui", "Peng", ""], ["Jia", "Xu", ""], ["Yang", "Shiqiang", ""], ["Tian", "Qi", ""]]}, {"id": "2004.00326", "submitter": "Dan Casas", "authors": "Igor Santesteban, Elena Garces, Miguel A. Otaduy, Dan Casas", "title": "SoftSMPL: Data-driven Modeling of Nonlinear Soft-tissue Dynamics for\n  Parametric Humans", "comments": "Accepted at Eurographics 2020. Project website:\n  http://dancasas.github.io/projects/SoftSMPL", "journal-ref": null, "doi": "10.1111/cgf.13912", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SoftSMPL, a learning-based method to model realistic soft-tissue\ndynamics as a function of body shape and motion. Datasets to learn such task\nare scarce and expensive to generate, which makes training models prone to\noverfitting. At the core of our method there are three key contributions that\nenable us to model highly realistic dynamics and better generalization\ncapabilities than state-of-the-art methods, while training on the same data.\nFirst, a novel motion descriptor that disentangles the standard pose\nrepresentation by removing subject-specific features; second, a\nneural-network-based recurrent regressor that generalizes to unseen shapes and\nmotions; and third, a highly efficient nonlinear deformation subspace capable\nof representing soft-tissue deformations of arbitrary shapes. We demonstrate\nqualitative and quantitative improvements over existing methods and,\nadditionally, we show the robustness of our method on a variety of motion\ncapture databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 10:35:06 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Santesteban", "Igor", ""], ["Garces", "Elena", ""], ["Otaduy", "Miguel A.", ""], ["Casas", "Dan", ""]]}, {"id": "2004.00329", "submitter": "Matteo Fabbri Ing.", "authors": "Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Alletto, Rita\n  Cucchiara", "title": "Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel approach for bottom-up multi-person 3D human\npose estimation from monocular RGB images. We propose to use high resolution\nvolumetric heatmaps to model joint locations, devising a simple and effective\ncompression method to drastically reduce the size of this representation. At\nthe core of the proposed method lies our Volumetric Heatmap Autoencoder, a\nfully-convolutional network tasked with the compression of ground-truth\nheatmaps into a dense intermediate representation. A second model, the Code\nPredictor, is then trained to predict these codes, which can be decompressed at\ntest time to re-obtain the original representation. Our experimental evaluation\nshows that our method performs favorably when compared to state of the art on\nboth multi-person and single-person 3D human pose estimation datasets and,\nthanks to our novel compression strategy, can process full-HD images at the\nconstant runtime of 8 fps regardless of the number of subjects in the scene.\nCode and models available at https://github.com/fabbrimatteo/LoCO .\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 10:37:39 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Fabbri", "Matteo", ""], ["Lanzi", "Fabio", ""], ["Calderara", "Simone", ""], ["Alletto", "Stefano", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2004.00331", "submitter": "Kajol Gupta", "authors": "Kajol Gupta", "title": "Digit Recognition Using Convolution Neural Network", "comments": "7 pages, 4 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pattern recognition, digit recognition has always been a very challenging\ntask. This paper aims to extracting a correct feature so that it can achieve\nbetter accuracy for recognition of digits. The applications of digit\nrecognition such as in password, bank check process, etc. to recognize the\nvalid user identification. Earlier, several researchers have used various\ndifferent machine learning algorithms in pattern recognition i.e. KNN, SVM,\nRFC. The main objective of this work is to obtain highest accuracy 99.15% by\nusing convolution neural network (CNN) to recognize the digit without doing too\nmuch pre-processing of dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 10:41:57 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gupta", "Kajol", ""]]}, {"id": "2004.00347", "submitter": "Liyuan Pan Miss", "authors": "Liyuan Pan, Miaomiao Liu and Richard Hartley", "title": "Single Image Optical Flow Estimation with an Event Camera", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired sensors that asynchronously report intensity\nchanges in microsecond resolution. DAVIS can capture high dynamics of a scene\nand simultaneously output high temporal resolution events and low frame-rate\nintensity images. In this paper, we propose a single image (potentially\nblurred) and events based optical flow estimation approach. First, we\ndemonstrate how events can be used to improve flow estimates. To this end, we\nencode the relation between flow and events effectively by presenting an\nevent-based photometric consistency formulation. Then, we consider the special\ncase of image blur caused by high dynamics in the visual environments and show\nthat including the blur formation in our model further constrains flow\nestimation. This is in sharp contrast to existing works that ignore the blurred\nimages while our formulation can naturally handle either blurred or sharp\nimages to achieve accurate flow estimation. Finally, we reduce flow estimation,\nas well as image deblurring, to an alternative optimization problem of an\nobjective function using the primal-dual algorithm. Experimental results on\nboth synthetic and real data (with blurred and non-blurred images) show the\nsuperiority of our model in comparison to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 11:28:30 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Pan", "Liyuan", ""], ["Liu", "Miaomiao", ""], ["Hartley", "Richard", ""]]}, {"id": "2004.00390", "submitter": "Yuanen Zhou", "authors": "Yuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, Hanwang Zhang", "title": "More Grounded Image Captioning by Distilling Image-Text Matching Model", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention not only improves the performance of image captioners, but\nalso serves as a visual interpretation to qualitatively measure the caption\nrationality and model transparency. Specifically, we expect that a captioner\ncan fix its attentive gaze on the correct objects while generating the\ncorresponding words. This ability is also known as grounded image captioning.\nHowever, the grounding accuracy of existing captioners is far from\nsatisfactory. To improve the grounding accuracy while retaining the captioning\nquality, it is expensive to collect the word-region alignment as strong\nsupervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text\nmatching model (SCAN \\cite{lee2018stacked}): POS-SCAN, as the effective\nknowledge distillation for more grounded image captioning. The benefits are\ntwo-fold: 1) given a sentence and an image, POS-SCAN can ground the objects\nmore accurately than SCAN; 2) POS-SCAN serves as a word-region alignment\nregularization for the captioner's visual attention module. By showing\nbenchmark experimental results, we demonstrate that conventional image\ncaptioners equipped with POS-SCAN can significantly improve the grounding\naccuracy without strong supervision. Last but not the least, we explore the\nindispensable Self-Critical Sequence Training (SCST) \\cite{Rennie_2017_CVPR} in\nthe context of grounded image captioning and show that the image-text matching\nscore can serve as a reward for more grounded captioning\n\\footnote{https://github.com/YuanEZhou/Grounded-Image-Captioning}.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:42:06 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhou", "Yuanen", ""], ["Wang", "Meng", ""], ["Liu", "Daqing", ""], ["Hu", "Zhenzhen", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2004.00403", "submitter": "Mark Boss", "authors": "Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, Jan Kautz", "title": "Two-shot Spatially-varying BRDF and Shape Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00404", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the shape and spatially-varying appearance (SVBRDF) of an object\nfrom images is a challenging task that has applications in both computer vision\nand graphics. Traditional optimization-based approaches often need a large\nnumber of images taken from multiple views in a controlled environment. Newer\ndeep learning-based approaches require only a few input images, but the\nreconstruction quality is not on par with optimization techniques. We propose a\nnovel deep learning architecture with a stage-wise estimation of shape and\nSVBRDF. The previous predictions guide each estimation, and a joint refinement\nnetwork later refines both SVBRDF and shape. We follow a practical mobile image\ncapture setting and use unaligned two-shot flash and no-flash images as input.\nBoth our two-shot image capture and network inference can run on mobile\nhardware. We also create a large-scale synthetic training dataset with\ndomain-randomized geometry and realistic materials. Extensive experiments on\nboth synthetic and real-world datasets show that our network trained on a\nsynthetic dataset can generalize well to real-world images. Comparisons with\nrecent approaches demonstrate the superior performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:56:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Boss", "Mark", ""], ["Jampani", "Varun", ""], ["Kim", "Kihwan", ""], ["Lensch", "Hendrik P. A.", ""], ["Kautz", "Jan", ""]]}, {"id": "2004.00406", "submitter": "Bolun Zheng", "authors": "Bolun Zheng, Shanxin Yuan, Gregory Slabaugh, Ales Leonardis", "title": "Image Demoireing with Learnable Bandpass Filters", "comments": "Accepted by CVPR2020. Code is available at\n  https://github.com/zhenngbolun/Learnbale_Bandpass_Filter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image demoireing is a multi-faceted image restoration task involving both\ntexture and color restoration. In this paper, we propose a novel multiscale\nbandpass convolutional neural network (MBCNN) to address this problem. As an\nend-to-end solution, MBCNN respectively solves the two sub-problems. For\ntexture restoration, we propose a learnable bandpass filter (LBF) to learn the\nfrequency prior for moire texture removal. For color restoration, we propose a\ntwo-step tone mapping strategy, which first applies a global tone mapping to\ncorrect for a global color shift, and then performs local fine tuning of the\ncolor per pixel. Through an ablation study, we demonstrate the effectiveness of\nthe different components of MBCNN. Experimental results on two public datasets\nshow that our method outperforms state-of-the-art methods by a large margin\n(more than 2dB in terms of PSNR).\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:57:26 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zheng", "Bolun", ""], ["Yuan", "Shanxin", ""], ["Slabaugh", "Gregory", ""], ["Leonardis", "Ales", ""]]}, {"id": "2004.00431", "submitter": "Jaehyung Kim", "authors": "Jaehyung Kim, Jongheon Jeong, Jinwoo Shin", "title": "M2m: Imbalanced Classification via Major-to-minor Translation", "comments": "12 pages; CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real-world scenarios, labeled training datasets are highly\nclass-imbalanced, where deep neural networks suffer from generalizing to a\nbalanced testing criterion. In this paper, we explore a novel yet simple way to\nalleviate this issue by augmenting less-frequent classes via translating\nsamples (e.g., images) from more-frequent classes. This simple approach enables\na classifier to learn more generalizable features of minority classes, by\ntransferring and leveraging the diversity of the majority information. Our\nexperimental results on a variety of class-imbalanced datasets show that the\nproposed method improves the generalization on minority classes significantly\ncompared to other existing re-sampling or re-weighting methods. The performance\nof our method even surpasses those of previous state-of-the-art methods for the\nimbalanced classification.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:21:17 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 10:48:34 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kim", "Jaehyung", ""], ["Jeong", "Jongheon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2004.00436", "submitter": "Sherif Abdelkarim Mr.", "authors": "Sherif Abdelkarim, Aniket Agarwal, Panos Achlioptas, Jun Chen, Jiaji\n  Huang, Boyang Li, Kenneth Church, Mohamed Elhoseiny", "title": "Long Tail Visual Relationship Recognition with Hubless Regularized\n  Relmix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches have been proposed in recent literature to alleviate the\nlong-tail problem, mostly in the object classification task. We propose to\nstudy the task of Long-Tail Visual Relationship Recognition (LTVRR), which aims\nat generalizing on the structured long-tail distribution of visual\nrelationships (e.g., \"rabbit grazing on grass\"). In this setup, subject,\nrelation, and object classes individually follow a long-tail distribution. We\nfirst introduce two large-scale long-tail visual relationship recognition\nbenchmarks to study this task, dubbed as VG8K-LT (5330 objects, 2000\nrelationships) and GQA-LT (1703 objects, 310 relations). VG8K-LT and GQA-LT are\nbuilt upon the widely used Visual Genome and GQA datasets. In contrast to\nexisting benchmarks, some classes appear at a very low frequency ($1-14$\nexamples). We use these benchmarks to study the performance of several\nstate-of-the-art long-tail models on LTVRR setup. We developed a\nvisiolinguistic hubless (ViLHub) loss that consistently encourages visual\nclassifiers to be more predictive of tail classes while being accurate on the\nhead. We also propose relationship Mixup augmentation, dubbed as RelMix, to\nimprove performance on the tail on VG8K-LT and GQA-LT benchmarks with the best\nperformance achieved when combined with ViLHub loss. Benchmarks and code will\nbe made available.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 19:03:29 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 20:13:01 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 21:36:30 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 09:53:50 GMT"}, {"version": "v5", "created": "Wed, 24 Feb 2021 02:52:17 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Abdelkarim", "Sherif", ""], ["Agarwal", "Aniket", ""], ["Achlioptas", "Panos", ""], ["Chen", "Jun", ""], ["Huang", "Jiaji", ""], ["Li", "Boyang", ""], ["Church", "Kenneth", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2004.00440", "submitter": "Lu Yu", "authors": "Lu Yu, Bart{\\l}omiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang,\n  Yongmei Cheng, Shangling Jui, Joost van de Weijer", "title": "Semantic Drift Compensation for Class-Incremental Learning", "comments": "Accepted at CVPR2020, Code available at\n  \\url{https://github.com/yulu0724/SDC-IL}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Class-incremental learning of deep networks sequentially increases the number\nof classes to be classified. During training, the network has only access to\ndata of one task at a time, where each task contains several classes. In this\nsetting, networks suffer from catastrophic forgetting which refers to the\ndrastic drop in performance on previous tasks. The vast majority of methods\nhave studied this scenario for classification networks, where for each new task\nthe classification layer of the network must be augmented with additional\nweights to make room for the newly added classes. Embedding networks have the\nadvantage that new classes can be naturally included into the network without\nadding new weights. Therefore, we study incremental learning for embedding\nnetworks. In addition, we propose a new method to estimate the drift, called\nsemantic drift, of features and compensate for it without the need of any\nexemplars. We approximate the drift of previous tasks based on the drift that\nis experienced by current task data. We perform experiments on fine-grained\ndatasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks\nsuffer significantly less from catastrophic forgetting. We outperform existing\nmethods which do not require exemplars and obtain competitive results compared\nto methods which store exemplars. Furthermore, we show that our proposed SDC\nwhen combined with existing methods to prevent forgetting consistently improves\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:31:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yu", "Lu", ""], ["Twardowski", "Bart\u0142omiej", ""], ["Liu", "Xialei", ""], ["Herranz", "Luis", ""], ["Wang", "Kai", ""], ["Cheng", "Yongmei", ""], ["Jui", "Shangling", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2004.00445", "submitter": "Lei Yang", "authors": "Lei Yang, Dapeng Chen, Xiaohang Zhan, Rui Zhao, Chen Change Loy, Dahua\n  Lin", "title": "Learning to Cluster Faces via Confidence and Connectivity Estimation", "comments": "8 pages, 6 figures, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face clustering is an essential tool for exploiting the unlabeled face data,\nand has a wide range of applications including face annotation and retrieval.\nRecent works show that supervised clustering can result in noticeable\nperformance gain. However, they usually involve heuristic steps and require\nnumerous overlapped subgraphs, severely restricting their accuracy and\nefficiency. In this paper, we propose a fully learnable clustering framework\nwithout requiring a large number of overlapped subgraphs. Instead, we transform\nthe clustering problem into two sub-problems. Specifically, two graph\nconvolutional networks, named GCN-V and GCN-E, are designed to estimate the\nconfidence of vertices and the connectivity of edges, respectively. With the\nvertex confidence and edge connectivity, we can naturally organize more\nrelevant vertices on the affinity graph and group them into clusters.\nExperiments on two large-scale benchmarks show that our method significantly\nimproves clustering accuracy and thus performance of the recognition models\ntrained on top, yet it is an order of magnitude more efficient than existing\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:39:37 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 05:38:44 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yang", "Lei", ""], ["Chen", "Dapeng", ""], ["Zhan", "Xiaohang", ""], ["Zhao", "Rui", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.00448", "submitter": "Namhyuk Ahn", "authors": "Jaejun Yoo, Namhyuk Ahn, Kyung-Ah Sohn", "title": "Rethinking Data Augmentation for Image Super-resolution: A Comprehensive\n  Analysis and a New Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an effective way to improve the performance of deep\nnetworks. Unfortunately, current methods are mostly developed for high-level\nvision tasks (e.g., classification) and few are studied for low-level vision\ntasks (e.g., image restoration). In this paper, we provide a comprehensive\nanalysis of the existing augmentation methods applied to the super-resolution\ntask. We find that the methods discarding or manipulating the pixels or\nfeatures too much hamper the image restoration, where the spatial relationship\nis very important. Based on our analyses, we propose CutBlur that cuts a\nlow-resolution patch and pastes it to the corresponding high-resolution image\nregion and vice versa. The key intuition of CutBlur is to enable a model to\nlearn not only \"how\" but also \"where\" to super-resolve an image. By doing so,\nthe model can understand \"how much\", instead of blindly learning to apply\nsuper-resolution to every given pixel. Our method consistently and\nsignificantly improves the performance across various scenarios, especially\nwhen the model size is big and the data is collected under real-world\nenvironments. We also show that our method improves other low-level vision\ntasks, such as denoising and compression artifact removal.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:49:38 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 08:28:10 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yoo", "Jaejun", ""], ["Ahn", "Namhyuk", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "2004.00451", "submitter": "Daniel Cores", "authors": "Daniel Cores, V\\'ictor M. Brea and Manuel Mucientes", "title": "Spatio-temporal Tubelet Feature Aggregation and Object Linking in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of how to exploit spatio-temporal\ninformation available in videos to improve the object detection precision. We\npropose a two stage object detector called FANet based on short-term\nspatio-temporal feature aggregation to give a first detection set, and\nlong-term object linking to refine these detections. Firstly, we generate a set\nof short tubelet proposals containing the object in $N$ consecutive frames.\nThen, we aggregate RoI pooled deep features through the tubelet using a\ntemporal pooling operator that summarizes the information with a fixed size\noutput independent of the number of input frames. On top of that, we define a\ndouble head implementation that we feed with spatio-temporal aggregated\ninformation for spatio-temporal object classification, and with spatial\ninformation extracted from the current frame for object localization and\nspatial classification. Furthermore, we also specialize each head branch\narchitecture to better perform in each task taking into account the input data.\nFinally, a long-term linking method builds long tubes using the previously\ncalculated short tubelets to overcome detection errors. We have evaluated our\nmodel in the widely used ImageNet VID dataset achieving a 80.9% mAP, which is\nthe new state-of-the-art result for single models. Also, in the challenging\nsmall object detection dataset USC-GRAD-STDdb, our proposal outperforms the\nsingle frame baseline by 5.4% mAP.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:52:03 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 12:17:33 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Cores", "Daniel", ""], ["Brea", "V\u00edctor M.", ""], ["Mucientes", "Manuel", ""]]}, {"id": "2004.00452", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo", "title": "PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution\n  3D Human Digitization", "comments": "project page: https://shunsukesaito.github.io/PIFuHD", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image-based 3D human shape estimation have been driven by\nthe significant improvement in representation power afforded by deep neural\nnetworks. Although current approaches have demonstrated the potential in real\nworld settings, they still fail to produce reconstructions with the level of\ndetail often present in the input images. We argue that this limitation stems\nprimarily form two conflicting requirements; accurate predictions require large\ncontext, but precise predictions require high resolution. Due to memory\nlimitations in current hardware, previous approaches tend to take low\nresolution images as input to cover large spatial context, and produce less\nprecise (or low resolution) 3D estimates as a result. We address this\nlimitation by formulating a multi-level architecture that is end-to-end\ntrainable. A coarse level observes the whole image at lower resolution and\nfocuses on holistic reasoning. This provides context to an fine level which\nestimates highly detailed geometry by observing higher-resolution images. We\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art techniques on single image human shape reconstruction by fully\nleveraging 1k-resolution input images.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:52:53 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Saito", "Shunsuke", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Joo", "Hanbyul", ""]]}, {"id": "2004.00482", "submitter": "Amelia Jim\\'enez-S\\'anchez", "authors": "Amelia Jim\\'enez-S\\'anchez, Diana Mateus, Sonja Kirchhoff, Chlodwig\n  Kirchhoff, Peter Biberthaler, Nassir Navab, Miguel A. Gonz\\'alez Ballester,\n  Gemma Piella", "title": "Medical-based Deep Curriculum Learning for Improved Fracture\n  Classification", "comments": "MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_77", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep-learning based methods do not easily integrate to clinical\nprotocols, neither take full advantage of medical knowledge. In this work, we\npropose and compare several strategies relying on curriculum learning, to\nsupport the classification of proximal femur fracture from X-ray images, a\nchallenging problem as reflected by existing intra- and inter-expert\ndisagreement. Our strategies are derived from knowledge such as medical\ndecision trees and inconsistencies in the annotations of multiple experts,\nwhich allows us to assign a degree of difficulty to each training sample. We\ndemonstrate that if we start learning \"easy\" examples and move towards \"hard\",\nthe model can reach a better performance, even with fewer data. The evaluation\nis performed on the classification of a clinical dataset of about 1000 X-ray\nimages. Our results show that, compared to class-uniform and random strategies,\nthe proposed medical knowledge-based curriculum, performs up to 15% better in\nterms of accuracy, achieving the performance of experienced trauma surgeons.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:56:43 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Amelia", ""], ["Mateus", "Diana", ""], ["Kirchhoff", "Sonja", ""], ["Kirchhoff", "Chlodwig", ""], ["Biberthaler", "Peter", ""], ["Navab", "Nassir", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""], ["Piella", "Gemma", ""]]}, {"id": "2004.00542", "submitter": "Yue Wu", "authors": "Yue Wu, Rongrong Gao, Jaesik Park, Qifeng Chen", "title": "Future Video Synthesis with Object Motion Prediction", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to predict future video frames given a sequence of\ncontinuous video frames in the past. Instead of synthesizing images directly,\nour approach is designed to understand the complex scene dynamics by decoupling\nthe background scene and moving objects. The appearance of the scene components\nin the future is predicted by non-rigid deformation of the background and\naffine transformation of moving objects. The anticipated appearances are\ncombined to create a reasonable video in the future. With this procedure, our\nmethod exhibits much less tearing or distortion artifact compared to other\napproaches. Experimental results on the Cityscapes and KITTI datasets show that\nour model outperforms the state-of-the-art in terms of visual quality and\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:09:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 10:55:42 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Wu", "Yue", ""], ["Gao", "Rongrong", ""], ["Park", "Jaesik", ""], ["Chen", "Qifeng", ""]]}, {"id": "2004.00543", "submitter": "James Tu", "authors": "James Tu, Mengye Ren, Siva Manivasagam, Ming Liang, Bin Yang, Richard\n  Du, Frank Cheng, Raquel Urtasun", "title": "Physically Realizable Adversarial Examples for LiDAR Object Detection", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern autonomous driving systems rely heavily on deep learning models to\nprocess point cloud sensory data; meanwhile, deep models have been shown to be\nsusceptible to adversarial attacks with visually imperceptible perturbations.\nDespite the fact that this poses a security concern for the self-driving\nindustry, there has been very little exploration in terms of 3D perception, as\nmost adversarial attacks have only been applied to 2D flat images. In this\npaper, we address this issue and present a method to generate universal 3D\nadversarial objects to fool LiDAR detectors. In particular, we demonstrate that\nplacing an adversarial object on the rooftop of any target vehicle to hide the\nvehicle entirely from LiDAR detectors with a success rate of 80%. We report\nattack results on a suite of detectors using various input representation of\npoint clouds. We also conduct a pilot study on adversarial defense using data\naugmentation. This is one step closer towards safer self-driving under unseen\nconditions from limited training data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:11:04 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 16:02:41 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Tu", "James", ""], ["Ren", "Mengye", ""], ["Manivasagam", "Siva", ""], ["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Du", "Richard", ""], ["Cheng", "Frank", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2004.00554", "submitter": "Tao Lu", "authors": "Bin Wang, Tao Lu, Yanduo Zhang", "title": "Feature-Driven Super-Resolution for Object Detection", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although some convolutional neural networks (CNNs) based super-resolution\n(SR) algorithms yield good visual performances on single images recently. Most\nof them focus on perfect perceptual quality but ignore specific needs of\nsubsequent detection task. This paper proposes a simple but powerful\nfeature-driven super-resolution (FDSR) to improve the detection performance of\nlow-resolution (LR) images. First, the proposed method uses feature-domain\nprior which extracts from an existing detector backbone to guide the HR image\nreconstruction. Then, with the aligned features, FDSR update SR parameters for\nbetter detection performance. Comparing with some state-of-the-art SR\nalgorithms with 4$\\times$ scale factor, FDSR outperforms the detection\nperformance mAP on MS COCO validation, VOC2007 databases with good\ngeneralization to other detection networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:33:07 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wang", "Bin", ""], ["Lu", "Tao", ""], ["Zhang", "Yanduo", ""]]}, {"id": "2004.00562", "submitter": "Debapriya Roy", "authors": "Debapriya Roy, Sanchayan Santra, and Bhabatosh Chanda", "title": "LGVTON: A Landmark Guided Approach to Virtual Try-On", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of image based virtual try-on (VTON), where the goal\nis to synthesize an image of a person wearing the cloth of a model. An\nessential requirement for generating a perceptually convincing VTON result is\npreserving the characteristics of the cloth and the person. Keeping this in\nmind we propose \\textit{LGVTON}, a novel self-supervised landmark guided\napproach to image based virtual try-on. The incorporation of self-supervision\ntackles the problem of lack of paired training data in model to person VTON\nscenario. LGVTON uses two types of landmarks to warp the model cloth according\nto the shape and pose of the person, one, human landmarks, the locations of\nanatomical keypoints of human, two, fashion landmarks, the structural keypoints\nof cloth. We introduce an unique way of using landmarks for warping which is\nmore efficient and effective compared to existing warping based methods in\ncurrent problem scenario. In addition to that, to make the method robust in\ncases of noisy landmark estimates that causes inaccurate warping, we propose a\nmask generator module that attempts to predict the true segmentation mask of\nthe model cloth on the person, which in turn guides our image synthesizer\nmodule in tackling warping issues. Experimental results show the effectiveness\nof our method in comparison to the state-of-the-art VTON methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 16:49:57 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Roy", "Debapriya", ""], ["Santra", "Sanchayan", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "2004.00583", "submitter": "Alan JiaXiang Guo", "authors": "Alan J.X. Guo and Fei Zhu", "title": "Improving Deep Hyperspectral Image Classification Performance with\n  Spectral Unmixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural networks have made great progress in the\nhyperspectral image (HSI) classification. However, the overfitting effect,\nwhich is mainly caused by complicated model structure and small training set,\nremains a major concern. Reducing the complexity of the neural networks could\nprevent overfitting to some extent, but also declines the networks' ability to\nexpress more abstract features. Enlarging the training set is also difficult,\nfor the high expense of acquisition and manual labeling. In this paper, we\npropose an abundance-based multi-HSI classification method. Firstly, we convert\nevery HSI from the spectral domain to the abundance domain by a\ndataset-specific autoencoder. Secondly, the abundance representations from\nmultiple HSIs are collected to form an enlarged dataset. Lastly, we train an\nabundance-based classifier and employ the classifier to predict over all the\ninvolved HSI datasets. Different from the spectra that are usually highly\nmixed, the abundance features are more representative in reduced dimension with\nless noise. This benefits the proposed method to employ simple classifiers and\nenlarged training data, and to expect less overfitting issues. The\neffectiveness of the proposed method is verified by the ablation study and the\ncomparative experiments.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:14:05 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 02:52:54 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 16:09:51 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2020 05:10:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Guo", "Alan J. X.", ""], ["Zhu", "Fei", ""]]}, {"id": "2004.00587", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu", "title": "Symmetry and Group in Attribute-Object Compositions", "comments": "Accepted to CVPR 2020, supplementary materials included, code\n  available:https://github.com/DirtyHarryLYL/SymNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes and objects can compose diverse compositions. To model the\ncompositional nature of these general concepts, it is a good choice to learn\nthem through transformations, such as coupling and decoupling. However, complex\ntransformations need to satisfy specific principles to guarantee the\nrationality. In this paper, we first propose a previously ignored principle of\nattribute-object transformation: Symmetry. For example, coupling peeled-apple\nwith attribute peeled should result in peeled-apple, and decoupling peeled from\napple should still output apple. Incorporating the symmetry principle, a\ntransformation framework inspired by group theory is built, i.e. SymNet. SymNet\nconsists of two modules, Coupling Network and Decoupling Network. With the\ngroup axioms and symmetry property as objectives, we adopt Deep Neural Networks\nto implement SymNet and train it in an end-to-end paradigm. Moreover, we\npropose a Relative Moving Distance (RMD) based recognition method to utilize\nthe attribute change instead of the attribute pattern itself to classify\nattributes. Our symmetry learning can be utilized for the Compositional\nZero-Shot Learning task and outperforms the state-of-the-art on widely-used\nbenchmarks. Code is available at https://github.com/DirtyHarryLYL/SymNet.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:16:57 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Li", "Yong-Lu", ""], ["Xu", "Yue", ""], ["Mao", "Xiaohan", ""], ["Lu", "Cewu", ""]]}, {"id": "2004.00588", "submitter": "Kayo Yin", "authors": "Kayo Yin and Jesse Read", "title": "Better Sign Language Translation with STMC-Transformer", "comments": "Proceedings of the 28th International Conference on Computational\n  Linguistics (COLING'2020)", "journal-ref": "28th International Conference on Computational Linguistics 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\nsystem to extract sign language glosses from videos. Then, a translation system\ngenerates spoken language translations from the sign language glosses. This\npaper focuses on the translation system and introduces the STMC-Transformer\nwhich improves on the current state-of-the-art by over 5 and 7 BLEU\nrespectively on gloss-to-text and video-to-text translation of the\nPHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\nof over 16 BLEU.\n  We also demonstrate the problem in current methods that rely on gloss\nsupervision. The video-to-text translation of our STMC-Transformer outperforms\ntranslation of GT glosses. This contradicts previous claims that GT gloss\ntranslation acts as an upper bound for SLT performance and reveals that glosses\nare an inefficient representation of sign language. For future SLT research, we\ntherefore suggest an end-to-end training of the recognition and translation\nmodels, or using a different sign language annotation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:20:04 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:59:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yin", "Kayo", ""], ["Read", "Jesse", ""]]}, {"id": "2004.00589", "submitter": "Leon Bungert", "authors": "Leon Bungert, Matthias J. Ehrhardt", "title": "Robust Image Reconstruction with Misaligned Structural Information", "comments": null, "journal-ref": "IEEE Access, vol. 8, pp. 222944-222955, 2020,", "doi": "10.1109/ACCESS.2020.3043638", "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality (or multi-channel) imaging is becoming increasingly important\nand more widely available, e.g. hyperspectral imaging in remote sensing,\nspectral CT in material sciences as well as multi-contrast MRI and PET-MR in\nmedicine. Research in the last decades resulted in a plethora of mathematical\nmethods to combine data from several modalities. State-of-the-art methods,\noften formulated as variational regularization, have shown to significantly\nimprove image reconstruction both quantitatively and qualitatively. Almost all\nof these models rely on the assumption that the modalities are perfectly\nregistered, which is not the case in most real world applications. We propose a\nvariational framework which jointly performs reconstruction and registration,\nthereby overcoming this hurdle. Our approach is the first to achieve this for\ndifferent modalities and outranks established approaches in terms of accuracy\nof both reconstruction and registration. Numerical results on simulated and\nreal data show the potential of the proposed strategy for various applications\nin multi-contrast MRI, PET-MR, and hyperspectral imaging: typical misalignments\nbetween modalities such as rotations, translations, zooms can be effectively\ncorrected during the reconstruction process. Therefore the proposed framework\nallows the robust exploitation of shared information across multiple modalities\nunder real conditions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:21:25 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 14:03:49 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 12:11:40 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Bungert", "Leon", ""], ["Ehrhardt", "Matthias J.", ""]]}, {"id": "2004.00605", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Daniel Barath, Jiri Matas", "title": "EPOS: Estimating 6D Pose of Objects with Symmetries", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for estimating the 6D pose of rigid objects with\navailable 3D models from a single RGB input image. The method is applicable to\na broad range of objects, including challenging ones with global or partial\nsymmetries. An object is represented by compact surface fragments which allow\nhandling symmetries in a systematic manner. Correspondences between densely\nsampled pixels and the fragments are predicted using an encoder-decoder\nnetwork. At each pixel, the network predicts: (i) the probability of each\nobject's presence, (ii) the probability of the fragments given the object's\npresence, and (iii) the precise 3D location on each fragment. A data-dependent\nnumber of corresponding 3D locations is selected per pixel, and poses of\npossibly multiple object instances are estimated using a robust and efficient\nvariant of the PnP-RANSAC algorithm. In the BOP Challenge 2019, the method\noutperforms all RGB and most RGB-D and D methods on the T-LESS and LM-O\ndatasets. On the YCB-V dataset, it is superior to all competitors, with a large\nmargin over the second-best RGB method. Source code is at:\ncmp.felk.cvut.cz/epos.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:41:08 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Hodan", "Tomas", ""], ["Barath", "Daniel", ""], ["Matas", "Jiri", ""]]}, {"id": "2004.00614", "submitter": "Nilesh Kulkarni", "authors": "Nilesh Kulkarni, Abhinav Gupta, David F. Fouhey, Shubham Tulsiani", "title": "Articulation-aware Canonical Surface Mapping", "comments": "To appear at CVPR 2020, project page\n  https://nileshkulkarni.github.io/acsm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the tasks of: 1) predicting a Canonical Surface Mapping (CSM) that\nindicates the mapping from 2D pixels to corresponding points on a canonical\ntemplate shape, and 2) inferring the articulation and pose of the template\ncorresponding to the input image. While previous approaches rely on keypoint\nsupervision for learning, we present an approach that can learn without such\nannotations. Our key insight is that these tasks are geometrically related, and\nwe can obtain supervisory signal via enforcing consistency among the\npredictions. We present results across a diverse set of animal object\ncategories, showing that our method can learn articulation and CSM prediction\nfrom image collections using only foreground mask labels for training. We\nempirically show that allowing articulation helps learn more accurate CSM\nprediction, and that enforcing the consistency with predicted CSM is similarly\ncritical for learning meaningful articulation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:56:45 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 17:14:05 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 22:22:44 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kulkarni", "Nilesh", ""], ["Gupta", "Abhinav", ""], ["Fouhey", "David F.", ""], ["Tulsiani", "Shubham", ""]]}, {"id": "2004.00622", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, Hany Farid", "title": "Evading Deepfake-Image Detectors with White- and Black-Box Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now possible to synthesize highly realistic images of people who don't\nexist. Such content has, for example, been implicated in the creation of\nfraudulent social-media profiles responsible for dis-information campaigns.\nSignificant efforts are, therefore, being deployed to detect\nsynthetically-generated content. One popular forensic approach trains a neural\nnetwork to distinguish real from synthetic content.\n  We show that such forensic classifiers are vulnerable to a range of attacks\nthat reduce the classifier to near-0% accuracy. We develop five attack case\nstudies on a state-of-the-art classifier that achieves an area under the ROC\ncurve (AUC) of 0.95 on almost all existing image generators, when only trained\non one generator. With full access to the classifier, we can flip the lowest\nbit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb\n1% of the image area to reduce the classifier's AUC to 0.08; or add a single\nnoise pattern in the synthesizer's latent space to reduce the classifier's AUC\nto 0.17. We also develop a black-box attack that, with no access to the target\nclassifier, reduces the AUC to 0.22. These attacks reveal significant\nvulnerabilities of certain image-forensic classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:59:59 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Carlini", "Nicholas", ""], ["Farid", "Hany", ""]]}, {"id": "2004.00626", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steve Seitz, and Ira\n  Kemelmacher-Shlizerman", "title": "Background Matting: The World is Your Green Screen", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for creating a matte -- the per-pixel foreground color\nand alpha -- of a person by taking photos or videos in an everyday setting with\na handheld camera. Most existing matting methods require a green screen\nbackground or a manually created trimap to produce a good matte. Automatic,\ntrimap-free methods are appearing, but are not of comparable quality. In our\ntrimap free approach, we ask the user to take an additional photo of the\nbackground without the subject at the time of capture. This step requires a\nsmall amount of foresight but is far less time-consuming than creating a\ntrimap. We train a deep network with an adversarial loss to predict the matte.\nWe first train a matting network with supervised loss on ground truth data with\nsynthetic composites. To bridge the domain gap to real imagery with no\nlabeling, we train another matting network guided by the first network and by a\ndiscriminator that judges the quality of composites. We demonstrate results on\na wide variety of photos and videos and show significant improvement over the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:38:55 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 03:31:38 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Jayaram", "Vivek", ""], ["Curless", "Brian", ""], ["Seitz", "Steve", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2004.00642", "submitter": "Paul Henderson", "authors": "Titas Anciukevicius, Christoph H. Lampert, Paul Henderson", "title": "Object-Centric Image Generation with Factored Depths, Locations, and\n  Appearances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative model of images that explicitly reasons over the set\nof objects they show. Our model learns a structured latent representation that\nseparates objects from each other and from the background; unlike prior works,\nit explicitly represents the 2D position and depth of each object, as well as\nan embedding of its segmentation mask and appearance. The model can be trained\nfrom images alone in a purely unsupervised fashion without the need for object\nmasks or depth information. Moreover, it always generates complete objects,\neven though a significant fraction of training images contain occlusions.\nFinally, we show that our model can infer decompositions of novel images into\ntheir constituent objects, including accurate prediction of depth ordering and\nsegmentation of occluded parts.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:00:11 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Anciukevicius", "Titas", ""], ["Lampert", "Christoph H.", ""], ["Henderson", "Paul", ""]]}, {"id": "2004.00663", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Michael Arbel, Umut \\c{S}im\\c{s}ekli, and Leonidas\n  Guibas", "title": "Synchronizing Probability Measures on Rotations via Optimal Transport", "comments": "Accepted for publication at CVPR 2020, includes supplementary\n  material. Project website: https://github.com/SynchInVision/probsync", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm, $\\textit{measure synchronization}$, for\nsynchronizing graphs with measure-valued edges. We formulate this problem as\nmaximization of the cycle-consistency in the space of probability measures over\nrelative rotations. In particular, we aim at estimating marginal distributions\nof absolute orientations by synchronizing the $\\textit{conditional}$ ones,\nwhich are defined on the Riemannian manifold of quaternions. Such graph\noptimization on distributions-on-manifolds enables a natural treatment of\nmultimodal hypotheses, ambiguities and uncertainties arising in many computer\nvision applications such as SLAM, SfM, and object pose estimation. We first\nformally define the problem as a generalization of the classical rotation graph\nsynchronization, where in our case the vertices denote probability measures\nover rotations. We then measure the quality of the synchronization by using\nSinkhorn divergences, which reduces to other popular metrics such as\nWasserstein distance or the maximum mean discrepancy as limit cases. We propose\na nonparametric Riemannian particle optimization approach to solve the problem.\nEven though the problem is non-convex, by drawing a connection to the recently\nproposed sparse optimization methods, we show that the proposed algorithm\nconverges to the global optimum in a special case of the problem under certain\nconditions. Our qualitative and quantitative experiments show the validity of\nour approach and we bring in new perspectives to the study of synchronization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:44:18 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Birdal", "Tolga", ""], ["Arbel", "Michael", ""], ["\u015eim\u015fekli", "Umut", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2004.00666", "submitter": "Rohit Keshari", "authors": "Rohit Keshari, Richa Singh, Mayank Vatsa", "title": "Generalized Zero-Shot Learning Via Over-Complete Distribution", "comments": "9 pages, 5 figures, Accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well trained and generalized deep neural network (DNN) should be robust to\nboth seen and unseen classes. However, the performance of most of the existing\nsupervised DNN algorithms degrade for classes which are unseen in the training\nset. To learn a discriminative classifier which yields good performance in\nZero-Shot Learning (ZSL) settings, we propose to generate an Over-Complete\nDistribution (OCD) using Conditional Variational Autoencoder (CVAE) of both\nseen and unseen classes. In order to enforce the separability between classes\nand reduce the class scatter, we propose the use of Online Batch Triplet Loss\n(OBTL) and Center Loss (CL) on the generated OCD. The effectiveness of the\nframework is evaluated using both Zero-Shot Learning and Generalized Zero-Shot\nLearning protocols on three publicly available benchmark databases, SUN, CUB\nand AWA2. The results show that generating over-complete distributions and\nenforcing the classifier to learn a transform function from overlapping to\nnon-overlapping distributions can improve the performance on both seen and\nunseen classes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 19:05:28 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Keshari", "Rohit", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "2004.00698", "submitter": "Erik Quintanilla", "authors": "Erik Quintanilla, Yogesh Rawat, Andrey Sakryukin, Mubarak Shah, Mohan\n  Kankanhalli", "title": "Adversarial Learning for Personalized Tag Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently seen great progress in image classification due to the\nsuccess of deep convolutional neural networks and the availability of\nlarge-scale datasets. Most of the existing work focuses on single-label image\nclassification. However, there are usually multiple tags associated with an\nimage. The existing works on multi-label classification are mainly based on lab\ncurated labels. Humans assign tags to their images differently, which is mainly\nbased on their interests and personal tagging behavior. In this paper, we\naddress the problem of personalized tag recommendation and propose an\nend-to-end deep network which can be trained on large-scale datasets. The\nuser-preference is learned within the network in an unsupervised way where the\nnetwork performs joint optimization for user-preference and visual encoding. A\njoint training of user-preference and visual encoding allows the network to\nefficiently integrate the visual preference with tagging behavior for a better\nuser recommendation. In addition, we propose the use of adversarial learning,\nwhich enforces the network to predict tags resembling user-generated tags. We\ndemonstrate the effectiveness of the proposed model on two different\nlarge-scale and publicly available datasets, YFCC100M and NUS-WIDE. The\nproposed method achieves significantly better performance on both the datasets\nwhen compared to the baselines and other state-of-the-art methods. The code is\npublicly available at https://github.com/vyzuer/ALTReco.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 20:41:41 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Quintanilla", "Erik", ""], ["Rawat", "Yogesh", ""], ["Sakryukin", "Andrey", ""], ["Shah", "Mubarak", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2004.00705", "submitter": "Luming Tang", "authors": "Luming Tang, Davis Wertheimer, Bharath Hariharan", "title": "Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot, fine-grained classification requires a model to learn subtle,\nfine-grained distinctions between different classes (e.g., birds) based on a\nfew images alone. This requires a remarkable degree of invariance to pose,\narticulation and background. A solution is to use pose-normalized\nrepresentations: first localize semantic parts in each image, and then describe\nimages by characterizing the appearance of each part. While such\nrepresentations are out of favor for fully supervised classification, we show\nthat they are extremely effective for few-shot fine-grained classification.\nWith a minimal increase in model capacity, pose normalization improves accuracy\nbetween 10 and 20 percentage points for shallow and deep architectures,\ngeneralizes better to new domains, and is effective for multiple few-shot\nalgorithms and network backbones. Code is available at\nhttps://github.com/Tsingularity/PoseNorm_Fewshot\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 21:00:06 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Tang", "Luming", ""], ["Wertheimer", "Davis", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2004.00713", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, Cordelia Schmid", "title": "Memory-Efficient Incremental Learning Through Feature Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for incremental learning that preserves feature\ndescriptors of training images from previously learned classes, instead of the\nimages themselves, unlike most existing work. Keeping the much\nlower-dimensional feature embeddings of images reduces the memory footprint\nsignificantly. We assume that the model is updated incrementally for new\nclasses as new data becomes available sequentially.This requires adapting the\npreviously stored feature vectors to the updated feature space without having\naccess to the corresponding original training images. Feature adaptation is\nlearned with a multi-layer perceptron, which is trained on feature pairs\ncorresponding to the outputs of the original and updated network on a training\nimage. We validate experimentally that such a transformation generalizes well\nto the features of the previous set of classes, and maps features to a\ndiscriminative subspace in the feature space. As a result, the classifier is\noptimized jointly over new and old classes without requiring old class images.\nExperimental results show that our method achieves state-of-the-art\nclassification accuracy in incremental learning benchmarks, while having at\nleast an order of magnitude lower memory footprint compared to image-preserving\nstrategies.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 21:16:05 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 21:44:38 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Iscen", "Ahmet", ""], ["Zhang", "Jeffrey", ""], ["Lazebnik", "Svetlana", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2004.00732", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Robust Single Rotation Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for single rotation averaging using the Weiszfeld\nalgorithm. Our contribution is threefold: First, we propose a robust\ninitialization based on the elementwise median of the input rotation matrices.\nOur initial solution is more accurate and robust than the commonly used chordal\n$L_2$-mean. Second, we propose an outlier rejection scheme that can be\nincorporated in the Weiszfeld algorithm to improve the robustness of $L_1$\nrotation averaging. Third, we propose a method for approximating the chordal\n$L_1$-mean using the Weiszfeld algorithm. An extensive evaluation shows that\nboth our method and the state of the art perform equally well with the proposed\noutlier rejection scheme, but ours is $2-4$ times faster.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 23:06:57 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 20:43:16 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 19:58:38 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2020 00:17:44 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "2004.00740", "submitter": "Huai Yu", "authors": "Huai Yu, Weikun Zhen, Wen Yang, Ji Zhang, Sebastian Scherer", "title": "Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line\n  Correspondences", "comments": "accepted by IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-weight camera localization in existing maps is essential for\nvision-based navigation. Currently, visual and visual-inertial odometry\n(VO\\&VIO) techniques are well-developed for state estimation but with\ninevitable accumulated drifts and pose jumps upon loop closure. To overcome\nthese problems, we propose an efficient monocular camera localization method in\nprior LiDAR maps using direct 2D-3D line correspondences. To handle the\nappearance differences and modality gaps between LiDAR point clouds and images,\ngeometric 3D lines are extracted offline from LiDAR maps while robust 2D lines\nare extracted online from video sequences. With the pose prediction from VIO,\nwe can efficiently obtain coarse 2D-3D line correspondences. Then the camera\nposes and 2D-3D correspondences are iteratively optimized by minimizing the\nprojection error of correspondences and rejecting outliers. Experimental\nresults on the EurocMav dataset and our collected dataset demonstrate that the\nproposed method can efficiently estimate camera poses without accumulated\ndrifts or pose jumps in structured environments.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 23:36:30 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 17:22:35 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yu", "Huai", ""], ["Zhen", "Weikun", ""], ["Yang", "Wen", ""], ["Zhang", "Ji", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2004.00753", "submitter": "Ping Yang", "authors": "Yanwei Zhao, Ping Yang, Qiu Guan, Jianwei Zheng, Wanliang Wang", "title": "Image Denoising Using Sparsifying Transform Learning and Weighted\n  Singular Values Minimization", "comments": "17 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image denoising (IDN) processing, the low-rank property is usually\nconsidered as an important image prior. As a convex relaxation approximation of\nlow rank, nuclear norm based algorithms and their variants have attracted\nsignificant attention. These algorithms can be collectively called image domain\nbased methods, whose common drawback is the requirement of great number of\niterations for some acceptable solution. Meanwhile, the sparsity of images in a\ncertain transform domain has also been exploited in image denoising problems.\nSparsity transform learning algorithms can achieve extremely fast computations\nas well as desirable performance. By taking both advantages of image domain and\ntransform domain in a general framework, we propose a sparsity transform\nlearning and weighted singular values minimization method (STLWSM) for IDN\nproblems. The proposed method can make full use of the preponderance of both\ndomains. For solving the non-convex cost function, we also present an efficient\nalternative solution for acceleration. Experimental results show that the\nproposed STLWSM achieves improvement both visually and quantitatively with a\nlarge margin over state-of-the-art approaches based on an alternatively single\ndomain. It also needs much less iteration than all the image domain algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 00:30:29 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Zhao", "Yanwei", ""], ["Yang", "Ping", ""], ["Guan", "Qiu", ""], ["Zheng", "Jianwei", ""], ["Wang", "Wanliang", ""]]}, {"id": "2004.00760", "submitter": "Bicheng Xu", "authors": "Bicheng Xu, Leonid Sigal", "title": "Consistent Multiple Sequence Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence decoding is one of the core components of most visual-lingual\nmodels. However, typical neural decoders when faced with decoding multiple,\npossibly correlated, sequences of tokens resort to simple independent decoding\nschemes. In this paper, we introduce a consistent multiple sequence decoding\narchitecture, which is while relatively simple, is general and allows for\nconsistent and simultaneous decoding of an arbitrary number of sequences. Our\nformulation utilizes a consistency fusion mechanism, implemented using message\npassing in a Graph Neural Network (GNN), to aggregate context from related\ndecoders. This context is then utilized as a secondary input, in addition to\npreviously generated output, to make a prediction at a given step of decoding.\nSelf-attention, in the GNN, is used to modulate the fusion mechanism locally at\neach node and each step in the decoding process. We show the efficacy of our\nconsistent multiple sequence decoder on the task of dense relational image\ncaptioning and illustrate state-of-the-art performance (+ 5.2% in mAP) on the\ntask. More importantly, we illustrate that the decoded sentences, for the same\nregions, are more consistent (improvement of 9.5%), while across images and\nregions maintain diversity.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 00:43:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 21:19:16 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Xu", "Bicheng", ""], ["Sigal", "Leonid", ""]]}, {"id": "2004.00779", "submitter": "Myungsub Choi", "authors": "Myungsub Choi, Janghoon Choi, Sungyong Baik, Tae Hyun Kim, Kyoung Mu\n  Lee", "title": "Scene-Adaptive Video Frame Interpolation via Meta-Learning", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation is a challenging problem because there are\ndifferent scenarios for each video depending on the variety of foreground and\nbackground motion, frame rate, and occlusion. It is therefore difficult for a\nsingle network with fixed parameters to generalize across different videos.\nIdeally, one could have a different network for each scenario, but this is\ncomputationally infeasible for practical applications. In this work, we propose\nto adapt the model to each video by making use of additional information that\nis readily available at test time and yet has not been exploited in previous\nworks. We first show the benefits of `test-time adaptation' through simple\nfine-tuning of a network, then we greatly improve its efficiency by\nincorporating meta-learning. We obtain significant performance gains with only\na single gradient update without any additional parameters. Finally, we show\nthat our meta-learning framework can be easily employed to any video frame\ninterpolation network and can consistently improve its performance on multiple\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 02:46:44 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Choi", "Myungsub", ""], ["Choi", "Janghoon", ""], ["Baik", "Sungyong", ""], ["Kim", "Tae Hyun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2004.00786", "submitter": "David Alejandro Jimenez Sierra David Alejandro Jimenez-Sierra", "authors": "David Alejandro Jimenez Sierra, Hern\\'an Dar\\'io Ben\\'itez Restrepo,\n  Hern\\'an Dar\\'io Vargas Cardonay, Jocelyn Chanussot", "title": "Graph-based fusion for change detection in multi-spectral images", "comments": "Four pages conference paper, four figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of change detection in multi-spectral\nimages by proposing a data-driven framework of graph-based data fusion. The\nmain steps of the proposed approach are: (i) The generation of a multi-temporal\npixel based graph, by the fusion of intra-graphs of each temporal data; (ii)\nthe use of Nystr\\\"om extension to obtain the eigenvalues and eigenvectors of\nthe fused graph, and the selection of the final change map. We validated our\napproach in two real cases of remote sensing according to both qualitative and\nquantitative analyses. The results confirm the potential of the proposed\ngraph-based change detection algorithm outperforming state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 02:59:00 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Sierra", "David Alejandro Jimenez", ""], ["Restrepo", "Hern\u00e1n Dar\u00edo Ben\u00edtez", ""], ["Cardonay", "Hern\u00e1n Dar\u00edo Vargas", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2004.00794", "submitter": "Zhonghao Wang", "authors": "Zhonghao Wang, Yunchao Wei, Rogerior Feris, Jinjun Xiong, Wen-Mei Hwu,\n  Thomas S. Huang, Humphrey Shi", "title": "Alleviating Semantic-level Shift: A Semi-supervised Domain Adaptation\n  Method for Semantic Segmentation", "comments": "CVPRW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning segmentation from synthetic data and adapting to real data can\nsignificantly relieve human efforts in labelling pixel-level masks. A key\nchallenge of this task is how to alleviate the data distribution discrepancy\nbetween the source and target domains, i.e. reducing domain shift. The common\napproach to this problem is to minimize the discrepancy between feature\ndistributions from different domains through adversarial training. However,\ndirectly aligning the feature distribution globally cannot guarantee\nconsistency from a local view (i.e. semantic-level), which prevents certain\nsemantic knowledge learned on the source domain from being applied to the\ntarget domain. To tackle this issue, we propose a semi-supervised approach\nnamed Alleviating Semantic-level Shift (ASS), which can successfully promote\nthe distribution consistency from both global and local views. Specifically,\nleveraging a small number of labeled data from the target domain, we directly\nextract semantic-level feature representations from both the source and the\ntarget domains by averaging the features corresponding to same categories\nadvised by pixel-level masks. We then feed the produced features to the\ndiscriminator to conduct semantic-level adversarial learning, which\ncollaborates with the adversarial learning from the global view to better\nalleviate the domain shift. We apply our ASS to two domain adaptation tasks,\nfrom GTA5 to Cityscapes and from Synthia to Cityscapes. Extensive experiments\ndemonstrate that: (1) ASS can significantly outperform the current unsupervised\nstate-of-the-arts by employing a small number of annotated samples from the\ntarget domain; (2) ASS can beat the oracle model trained on the whole target\ndataset by over 3 points by augmenting the synthetic source data with annotated\nsamples from the target domain without suffering from the prevalent problem of\noverfitting to the source domain.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:25:05 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 22:38:27 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wang", "Zhonghao", ""], ["Wei", "Yunchao", ""], ["Feris", "Rogerior", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-Mei", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "2004.00797", "submitter": "Umar Asif", "authors": "Umar Asif, Stefan Von Cavallar, Jianbin Tang, and Stefan Harrer", "title": "SSHFD: Single Shot Human Fall Detection with Occluded Joints Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falling can have fatal consequences for elderly people especially if the\nfallen person is unable to call for help due to loss of consciousness or any\ninjury. Automatic fall detection systems can assist through prompt fall alarms\nand by minimizing the fear of falling when living independently at home.\nExisting vision-based fall detection systems lack generalization to unseen\nenvironments due to challenges such as variations in physical appearances,\ndifferent camera viewpoints, occlusions, and background clutter. In this paper,\nwe explore ways to overcome the above challenges and present Single Shot Human\nFall Detector (SSHFD), a deep learning based framework for automatic fall\ndetection from a single image. This is achieved through two key innovations.\nFirst, we present a human pose based fall representation which is invariant to\nappearance characteristics. Second, we present neural network models for 3d\npose estimation and fall recognition which are resilient to missing joints due\nto occluded body parts. Experiments on public fall datasets show that our\nframework successfully transfers knowledge of 3d pose estimation and fall\nrecognition learnt purely from synthetic data to unseen real-world data,\nshowcasing its generalization capability for accurate fall detection in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:41:39 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 02:45:33 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Asif", "Umar", ""], ["Von Cavallar", "Stefan", ""], ["Tang", "Jianbin", ""], ["Harrer", "Stefan", ""]]}, {"id": "2004.00830", "submitter": "Chong Luo", "authors": "Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong and Wenjun Zeng", "title": "Tracking by Instance Detection: A Meta-Learning Approach", "comments": "This paper has been accepted by CVPR'20 as an oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the tracking problem as a special type of object detection\nproblem, which we call instance detection. With proper initialization, a\ndetector can be quickly converted into a tracker by learning the new instance\nfrom a single image. We find that model-agnostic meta-learning (MAML) offers a\nstrategy to initialize the detector that satisfies our needs. We propose a\nprincipled three-step approach to build a high-performance tracker. First, pick\nany modern object detector trained with gradient descent. Second, conduct\noffline training (or initialization) with MAML. Third, perform domain\nadaptation using the initial frame. We follow this procedure to build two\ntrackers, named Retina-MAML and FCOS-MAML, based on two modern detectors\nRetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are\ncompetitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves\nthe highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the\nleader board with an AUC of 0.757 and the normalized precision of 0.822. Both\ntrackers run in real-time at 40 FPS.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 05:55:06 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wang", "Guangting", ""], ["Luo", "Chong", ""], ["Sun", "Xiaoyan", ""], ["Xiong", "Zhiwei", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2004.00831", "submitter": "Shuyang Cheng", "authors": "Shuyang Cheng, Zhaoqi Leng, Ekin Dogus Cubuk, Barret Zoph, Chunyan\n  Bai, Jiquan Ngiam, Yang Song, Benjamin Caine, Vijay Vasudevan, Congcong Li,\n  Quoc V. Le, Jonathon Shlens, Dragomir Anguelov", "title": "Improving 3D Object Detection through Progressive Population Based\n  Augmentation", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has been widely adopted for object detection in 3D point\nclouds. However, all previous related efforts have focused on manually\ndesigning specific data augmentation methods for individual architectures. In\nthis work, we present the first attempt to automate the design of data\naugmentation policies for 3D object detection. We introduce the Progressive\nPopulation Based Augmentation (PPBA) algorithm, which learns to optimize\naugmentation strategies by narrowing down the search space and adopting the\nbest parameters discovered in previous iterations. On the KITTI 3D detection\ntest set, PPBA improves the StarNet detector by substantial margins on the\nmoderate difficulty category of cars, pedestrians, and cyclists, outperforming\nall current state-of-the-art single-stage detection models. Additional\nexperiments on the Waymo Open Dataset indicate that PPBA continues to\neffectively improve the StarNet and PointPillars detectors on a 20x larger\ndataset compared to KITTI. The magnitude of the improvements may be comparable\nto advances in 3D perception architectures and the gains come without an\nincurred cost at inference time. In subsequent experiments, we find that PPBA\nmay be up to 10x more data efficient than baseline 3D detection models without\naugmentation, highlighting that 3D detection models may achieve competitive\naccuracy with far fewer labeled examples.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 05:57:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 19:07:15 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Cheng", "Shuyang", ""], ["Leng", "Zhaoqi", ""], ["Cubuk", "Ekin Dogus", ""], ["Zoph", "Barret", ""], ["Bai", "Chunyan", ""], ["Ngiam", "Jiquan", ""], ["Song", "Yang", ""], ["Caine", "Benjamin", ""], ["Vasudevan", "Vijay", ""], ["Li", "Congcong", ""], ["Le", "Quoc V.", ""], ["Shlens", "Jonathon", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2004.00843", "submitter": "Marija Vella", "authors": "Marija Vella and Jo\\~ao F. C. Mota", "title": "Robust Single-Image Super-Resolution via CNNs and TV-TV Minimization", "comments": "Under peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution is the process of increasing the resolution of\nan image, obtaining a high-resolution (HR) image from a low-resolution (LR)\none. By leveraging large training datasets, convolutional neural networks\n(CNNs) currently achieve the state-of-the-art performance in this task. Yet,\nduring testing/deployment, they fail to enforce consistency between the HR and\nLR images: if we downsample the output HR image, it never matches its LR input.\nBased on this observation, we propose to post-process the CNN outputs with an\noptimization problem that we call TV-TV minimization, which enforces\nconsistency. As our extensive experiments show, such post-processing not only\nimproves the quality of the images, in terms of PSNR and SSIM, but also makes\nthe super-resolution task robust to operator mismatch, i.e., when the true\ndownsampling operator is different from the one used to create the training\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:06:55 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Vella", "Marija", ""], ["Mota", "Jo\u00e3o F. C.", ""]]}, {"id": "2004.00845", "submitter": "Xiaoxiao Long", "authors": "Xiaoxiao Long, Lingjie Liu, Christian Theobalt, and Wenping Wang", "title": "Occlusion-Aware Depth Estimation with Adaptive Normal Constraints", "comments": "ECCV 2020", "journal-ref": "ECCV 2020. Lecture Notes in Computer Science, vol 12354. Springer,\n  Cham", "doi": "10.1007/978-3-030-58545-7_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new learning-based method for multi-frame depth estimation from\na color video, which is a fundamental problem in scene understanding, robot\nnavigation or handheld 3D reconstruction. While recent learning-based methods\nestimate depth at high accuracy, 3D point clouds exported from their depth maps\noften fail to preserve important geometric feature (e.g., corners, edges,\nplanes) of man-made scenes. Widely-used pixel-wise depth errors do not\nspecifically penalize inconsistency on these features. These inaccuracies are\nparticularly severe when subsequent depth reconstructions are accumulated in an\nattempt to scan a full environment with man-made objects with this kind of\nfeatures. Our depth estimation algorithm therefore introduces a Combined Normal\nMap (CNM) constraint, which is designed to better preserve high-curvature\nfeatures and global planar regions. In order to further improve the depth\nestimation accuracy, we introduce a new occlusion-aware strategy that\naggregates initial depth predictions from multiple adjacent views into one\nfinal depth map and one occlusion probability map for the current reference\nview. Our method outperforms the state-of-the-art in terms of depth estimation\naccuracy, and preserves essential geometric features of man-made indoor scenes\nmuch better than other algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:10:45 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 15:17:14 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 17:34:48 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 16:23:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Long", "Xiaoxiao", ""], ["Liu", "Lingjie", ""], ["Theobalt", "Christian", ""], ["Wang", "Wenping", ""]]}, {"id": "2004.00849", "submitter": "Bei Liu", "authors": "Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Pixel-BERT to align image pixels with text by deep multi-modal\ntransformers that jointly learn visual and language embedding in a unified\nend-to-end framework. We aim to build a more accurate and thorough connection\nbetween image pixels and language semantics directly from image and sentence\npairs instead of using region-based image features as the most recent vision\nand language tasks. Our Pixel-BERT which aligns semantic connection in pixel\nand text level solves the limitation of task-specific visual representation for\nvision and language tasks. It also relieves the cost of bounding box\nannotations and overcomes the unbalance between semantic labels in visual task\nand language semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence pairs\nfrom Visual Genome dataset and MS-COCO dataset. We propose to use a random\npixel sampling mechanism to enhance the robustness of visual representation and\nto apply the Masked Language Model and Image-Text Matching as pre-training\ntasks. Extensive experiments on downstream tasks with our pre-trained model\nshow that our approach makes the most state-of-the-arts in downstream tasks,\nincluding Visual Question Answering (VQA), image-text retrieval, Natural\nLanguage for Visual Reasoning for Real (NLVR). Particularly, we boost the\nperformance of a single model in VQA task by 2.17 points compared with SOTA\nunder fair comparison.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:39:28 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 09:09:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Zhicheng", ""], ["Zeng", "Zhaoyang", ""], ["Liu", "Bei", ""], ["Fu", "Dongmei", ""], ["Fu", "Jianlong", ""]]}, {"id": "2004.00871", "submitter": "Yoni Kasten", "authors": "Yoni Kasten, Daniel Doktofsky and Ilya Kovler", "title": "End-To-End Convolutional Neural Network for 3D Reconstruction of Knee\n  Bones From Bi-Planar X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end Convolutional Neural Network (CNN) approach for 3D\nreconstruction of knee bones directly from two bi-planar X-ray images.\nClinically, capturing the 3D models of the bones is crucial for surgical\nplanning, implant fitting, and postoperative evaluation. X-ray imaging\nsignificantly reduces the exposure of patients to ionizing radiation compared\nto Computer Tomography (CT) imaging, and is much more common and inexpensive\ncompared to Magnetic Resonance Imaging (MRI) scanners. However, retrieving 3D\nmodels from such 2D scans is extremely challenging. In contrast to the common\napproach of statistically modeling the shape of each bone, our deep network\nlearns the distribution of the bones' shapes directly from the training images.\nWe train our model with both supervised and unsupervised losses using Digitally\nReconstructed Radiograph (DRR) images generated from CT scans. To apply our\nmodel to X-Ray data, we use style transfer to transform between X-Ray and DRR\nmodalities. As a result, at test time, without further optimization, our\nsolution directly outputs a 3D reconstruction from a pair of bi-planar X-ray\nimages, while preserving geometric constraints. Our results indicate that our\ndeep learning model is very efficient, generalizes well and produces high\nquality reconstructions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:37:11 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 17:20:56 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kasten", "Yoni", ""], ["Doktofsky", "Daniel", ""], ["Kovler", "Ilya", ""]]}, {"id": "2004.00899", "submitter": "Florian Tschopp", "authors": "Kenneth Blomqvist, Michel Breyer, Andrei Cramariuc, Julian F\\\"orster,\n  Margarita Grinvald, Florian Tschopp, Jen Jen Chung, Lionel Ott, Juan Nieto,\n  Roland Siegwart", "title": "Go Fetch: Mobile Manipulation in Unstructured Environments", "comments": "Kenneth Blomqvist, Michel Breyer, Andrei Cramariuc, Julian F\\\"orster,\n  Margarita Grinvald, and Florian Tschopp contributed equally to this work", "journal-ref": "ICRA 2020 Workshop on Perception, Action, Learning: From\n  Metric-Semantic Scene Understanding to High-level Task Execution, 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With humankind facing new and increasingly large-scale challenges in the\nmedical and domestic spheres, automation of the service sector carries a\ntremendous potential for improved efficiency, quality, and safety of\noperations. Mobile robotics can offer solutions with a high degree of mobility\nand dexterity, however these complex systems require a multitude of\nheterogeneous components to be carefully integrated into one consistent\nframework. This work presents a mobile manipulation system that combines\nperception, localization, navigation, motion planning and grasping skills into\none common workflow for fetch and carry applications in unstructured indoor\nenvironments. The tight integration across the various modules is\nexperimentally demonstrated on the task of finding a commonly available object\nin an office environment, grasping it, and delivering it to a desired drop-off\nlocation. The accompanying video is available at https://youtu.be/e89_Xg1sLnY.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 09:33:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Blomqvist", "Kenneth", ""], ["Breyer", "Michel", ""], ["Cramariuc", "Andrei", ""], ["F\u00f6rster", "Julian", ""], ["Grinvald", "Margarita", ""], ["Tschopp", "Florian", ""], ["Chung", "Jen Jen", ""], ["Ott", "Lionel", ""], ["Nieto", "Juan", ""], ["Siegwart", "Roland", ""]]}, {"id": "2004.00900", "submitter": "Xinting Hu", "authors": "Xinting Hu, Yi Jiang, Kaihua Tang, Jingyuan Chen, Chunyan Miao,\n  Hanwang Zhang", "title": "Learning to Segment the Tail", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world visual recognition requires handling the extreme sample imbalance\nin large-scale long-tailed data. We propose a \"divide&conquer\" strategy for the\nchallenging LVIS task: divide the whole data into balanced parts and then apply\nincremental learning to conquer each one. This derives a novel learning\nparadigm: class-incremental few-shot learning, which is especially effective\nfor the challenge evolving over time: 1) the class imbalance among the\nold-class knowledge review and 2) the few-shot data in new-class learning. We\ncall our approach Learning to Segment the Tail (LST). In particular, we design\nan instance-level balanced replay scheme, which is a memory-efficient\napproximation to balance the instance-level samples from the old-class images.\nWe also propose to use a meta-module for new-class learning, where the module\nparameters are shared across incremental phases, gaining the learning-to-learn\nknowledge incrementally, from the data-rich head to the data-poor tail. We\nempirically show that: at the expense of a little sacrifice of head-class\nforgetting, we can gain a significant 8.3% AP improvement for the tail classes\nwith less than 10 instances, achieving an overall 2.0% AP boost for the whole\n1,230 classes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 09:39:08 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 03:27:08 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hu", "Xinting", ""], ["Jiang", "Yi", ""], ["Tang", "Kaihua", ""], ["Chen", "Jingyuan", ""], ["Miao", "Chunyan", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2004.00909", "submitter": "Ankit Dhall", "authors": "Ankit Dhall", "title": "Learning Representations For Images With Hierarchical Labels", "comments": "Master thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has been studied extensively but there has been limited\nwork in the direction of using non-conventional, external guidance other than\ntraditional image-label pairs to train such models. In this thesis we present a\nset of methods to leverage information about the semantic hierarchy induced by\nclass labels. In the first part of the thesis, we inject label-hierarchy\nknowledge to an arbitrary classifier and empirically show that availability of\nsuch external semantic information in conjunction with the visual semantics\nfrom images boosts overall performance. Taking a step further in this\ndirection, we model more explicitly the label-label and label-image\ninteractions by using order-preserving embedding-based models, prevalent in\nnatural language, and tailor them to the domain of computer vision to perform\nimage classification. Although, contrasting in nature, both the CNN-classifiers\ninjected with hierarchical information, and the embedding-based models\noutperform a hierarchy-agnostic model on the newly presented, real-world ETH\nEntomological Collection image dataset\nhttps://www.research-collection.ethz.ch/handle/20.500.11850/365379.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 09:56:03 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 17:51:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Dhall", "Ankit", ""]]}, {"id": "2004.00917", "submitter": "Lei Huang", "authors": "Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, Ling Shao", "title": "Controllable Orthogonalization in Training DNNs", "comments": "Accepted to CVPR 2020. The Code is available at\n  https://github.com/huangleiBuaa/ONI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonality is widely used for training deep neural networks (DNNs) due to\nits ability to maintain all singular values of the Jacobian close to 1 and\nreduce redundancy in representation. This paper proposes a computationally\nefficient and numerically stable orthogonalization method using Newton's\niteration (ONI), to learn a layer-wise orthogonal weight matrix in DNNs. ONI\nworks by iteratively stretching the singular values of a weight matrix towards\n1. This property enables it to control the orthogonality of a weight matrix by\nits number of iterations. We show that our method improves the performance of\nimage classification networks by effectively controlling the orthogonality to\nprovide an optimal tradeoff between optimization benefits and representational\ncapacity reduction. We also show that ONI stabilizes the training of generative\nadversarial networks (GANs) by maintaining the Lipschitz continuity of a\nnetwork, similar to spectral normalization (SN), and further outperforms SN by\nproviding controllable orthogonality.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 10:14:27 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Huang", "Lei", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Wan", "Diwen", ""], ["Yuan", "Zehuan", ""], ["Li", "Bo", ""], ["Shao", "Ling", ""]]}, {"id": "2004.00945", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang,\n  Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu", "title": "PaStaNet: Toward Human Activity Knowledge Engine", "comments": "Accepted to CVPR 2020, supplementary materials included, code\n  available: http://hake-mvig.cn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image-based activity understanding methods mainly adopt direct\nmapping, i.e. from image to activity concepts, which may encounter performance\nbottleneck since the huge gap. In light of this, we propose a new path: infer\nhuman part states first and then reason out the activities based on part-level\nsemantics. Human Body Part States (PaSta) are fine-grained action semantic\ntokens, e.g. <hand, hold, something>, which can compose the activities and help\nus step toward human activity knowledge engine. To fully utilize the power of\nPaSta, we build a large-scale knowledge base PaStaNet, which contains 7M+ PaSta\nannotations. And two corresponding models are proposed: first, we design a\nmodel named Activity2Vec to extract PaSta features, which aim to be general\nrepresentations for various activities. Second, we use a PaSta-based Reasoning\nmethod to infer activities. Promoted by PaStaNet, our method achieves\nsignificant improvements, e.g. 6.4 and 13.9 mAP on full and one-shot sets of\nHICO in supervised learning, and 3.2 and 4.2 mAP on V-COCO and images-based AVA\nin transfer learning. Code and data are available at http://hake-mvig.cn/.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 11:35:59 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 11:55:17 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Yong-Lu", ""], ["Xu", "Liang", ""], ["Liu", "Xinpeng", ""], ["Huang", "Xijie", ""], ["Xu", "Yue", ""], ["Wang", "Shiyi", ""], ["Fang", "Hao-Shu", ""], ["Ma", "Ze", ""], ["Chen", "Mingyang", ""], ["Lu", "Cewu", ""]]}, {"id": "2004.00974", "submitter": "Sourya Dey", "authors": "Sourya Dey, Saikrishna C. Kanala, Keith M. Chugg, Peter A. Beerel", "title": "Deep-n-Cheap: An Automated Search Framework for Low Complexity Deep\n  Learning", "comments": "Accepted as a conference paper at ACML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep-n-Cheap -- an open-source AutoML framework to search for deep\nlearning models. This search includes both architecture and training\nhyperparameters, and supports convolutional neural networks and multi-layer\nperceptrons. Our framework is targeted for deployment on both benchmark and\ncustom datasets, and as a result, offers a greater degree of search space\ncustomizability as compared to a more limited search over only pre-existing\nmodels from literature. We also introduce the technique of 'search transfer',\nwhich demonstrates the generalization capabilities of the models found by our\nframework to multiple datasets.\n  Deep-n-Cheap includes a user-customizable complexity penalty which trades off\nperformance with training time or number of parameters. Specifically, our\nframework results in models offering performance comparable to state-of-the-art\nwhile taking 1-2 orders of magnitude less time to train than models from other\nAutoML and model search frameworks. Additionally, this work investigates and\ndevelops various insights regarding the search process. In particular, we show\nthe superiority of a greedy strategy and justify our choice of Bayesian\noptimization as the primary search methodology over random / grid search.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:00:21 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 01:29:44 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 21:24:04 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dey", "Sourya", ""], ["Kanala", "Saikrishna C.", ""], ["Chugg", "Keith M.", ""], ["Beerel", "Peter A.", ""]]}, {"id": "2004.01002", "submitter": "Jonas Schult", "authors": "Jonas Schult, Francis Engelmann, Theodora Kontogianni, Bastian Leibe", "title": "DualConvMesh-Net: Joint Geodesic and Euclidean Convolutions on 3D Meshes", "comments": "CVPR 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DualConvMesh-Nets (DCM-Net) a family of deep hierarchical\nconvolutional networks over 3D geometric data that combines two types of\nconvolutions. The first type, geodesic convolutions, defines the kernel weights\nover mesh surfaces or graphs. That is, the convolutional kernel weights are\nmapped to the local surface of a given mesh. The second type, Euclidean\nconvolutions, is independent of any underlying mesh structure. The\nconvolutional kernel is applied on a neighborhood obtained from a local\naffinity representation based on the Euclidean distance between 3D points.\nIntuitively, geodesic convolutions can easily separate objects that are\nspatially close but have disconnected surfaces, while Euclidean convolutions\ncan represent interactions between nearby objects better, as they are oblivious\nto object surfaces. To realize a multi-resolution architecture, we borrow\nwell-established mesh simplification methods from the geometry processing\ndomain and adapt them to define mesh-preserving pooling and unpooling\noperations. We experimentally show that combining both types of convolutions in\nour architecture leads to significant performance gains for 3D semantic\nsegmentation, and we report competitive results on three scene segmentation\nbenchmarks. Our models and code are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 13:52:00 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Schult", "Jonas", ""], ["Engelmann", "Francis", ""], ["Kontogianni", "Theodora", ""], ["Leibe", "Bastian", ""]]}, {"id": "2004.01019", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Jan Niklas Kolf, Naser Damer, Florian\n  Kirchbuchner, Arjan Kuijper", "title": "Face Quality Estimation and Its Correlation to Demographic and\n  Non-Demographic Bias in Face Recognition", "comments": "Accepted at IJCB2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face quality assessment aims at estimating the utility of a face image for\nthe purpose of recognition. It is a key factor to achieve high face recognition\nperformances. Currently, the high performance of these face recognition systems\ncome with the cost of a strong bias against demographic and non-demographic\nsub-groups. Recent work has shown that face quality assessment algorithms\nshould adapt to the deployed face recognition system, in order to achieve\nhighly accurate and robust quality estimations. However, this could lead to a\nbias transfer towards the face quality assessment leading to discriminatory\neffects e.g. during enrolment. In this work, we present an in-depth analysis of\nthe correlation between bias in face recognition and face quality assessment.\nExperiments were conducted on two publicly available datasets captured under\ncontrolled and uncontrolled circumstances with two popular face embeddings. We\nevaluated four state-of-the-art solutions for face quality assessment towards\nbiases to pose, ethnicity, and age. The experiments showed that the face\nquality assessment solutions assign significantly lower quality values towards\nsubgroups affected by the recognition bias demonstrating that these approaches\nare biased as well. This raises ethical questions towards fairness and\ndiscrimination which future works have to address.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 14:19:12 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 12:44:28 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 11:24:37 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["Kolf", "Jan Niklas", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2004.01023", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Andrew Lindley, Anahid Jalali, Martin Boyer,\n  Sergiu Gordea, Ross King", "title": "Multi-Modal Video Forensic Platform for Investigating Post-Terrorist\n  Attack Scenarios", "comments": null, "journal-ref": "In Proceedings of the 11th ACM Multimedia Systems Conference\n  (MMSys2020), June 06-11, 2020, Istanbul, Turkey", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.CY cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forensic investigation of a terrorist attack poses a significant\nchallenge to the investigative authorities, as often several thousand hours of\nvideo footage must be viewed. Large scale Video Analytic Platforms (VAP) assist\nlaw enforcement agencies (LEA) in identifying suspects and securing evidence.\nCurrent platforms focus primarily on the integration of different computer\nvision methods and thus are restricted to a single modality. We present a video\nanalytic platform that integrates visual and audio analytic modules and fuses\ninformation from surveillance cameras and video uploads from eyewitnesses.\nVideos are analyzed according their acoustic and visual content. Specifically,\nAudio Event Detection is applied to index the content according to\nattack-specific acoustic concepts. Audio similarity search is utilized to\nidentify similar video sequences recorded from different perspectives. Visual\nobject detection and tracking are used to index the content according to\nrelevant concepts. Innovative user-interface concepts are introduced to harness\nthe full potential of the heterogeneous results of the analytical modules,\nallowing investigators to more quickly follow-up on leads and eyewitness\nreports.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 14:29:27 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Schindler", "Alexander", ""], ["Lindley", "Andrew", ""], ["Jalali", "Anahid", ""], ["Boyer", "Martin", ""], ["Gordea", "Sergiu", ""], ["King", "Ross", ""]]}, {"id": "2004.01029", "submitter": "Titas De", "authors": "Titas De", "title": "Introducing Anisotropic Minkowski Functionals for Local Structure\n  Analysis and Prediction of Biomechanical Strength of Proximal Femur Specimens", "comments": null, "journal-ref": "Master's Thesis - 2013", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone fragility and fracture caused by osteoporosis or injury are prevalent in\nadults over the age of 50 and can reduce their quality of life. Hence,\npredicting the biomechanical bone strength, specifically of the proximal femur,\nthrough non-invasive imaging-based methods is an important goal for the\ndiagnosis of Osteoporosis as well as estimating fracture risk. Dual X-ray\nabsorptiometry (DXA) has been used as a standard clinical procedure for\nassessment and diagnosis of bone strength and osteoporosis through bone mineral\ndensity (BMD) measurements. However, previous studies have shown that\nquantitative computer tomography (QCT) can be more sensitive and specific to\ntrabecular bone characterization because it reduces the overlap effects and\ninterferences from the surrounding soft tissue and cortical shell.\n  This study proposes a new method to predict the bone strength of proximal\nfemur specimens from quantitative multi-detector computer tomography (MDCT)\nimages. Texture analysis methods such as conventional statistical moments (BMD\nmean), Isotropic Minkowski Functionals (IMF) and Anisotropic Minkowski\nFunctionals (AMF) are used to quantify BMD properties of the trabecular bone\nmicro-architecture. Combinations of these extracted features are then used to\npredict the biomechanical strength of the femur specimens using sophisticated\nmachine learning techniques such as multiregression (MultiReg) and support\nvector regression with linear kernel (SVRlin). The prediction performance\nachieved with these feature sets is compared to the standard approach that uses\nthe mean BMD of the specimens and multiregression models using root mean square\nerror (RMSE).\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 14:33:03 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["De", "Titas", ""]]}, {"id": "2004.01030", "submitter": "Lachlan Kermode", "authors": "Lachlan Kermode, Jan Freyberg, Alican Akturk, Robert Trafford, Denis\n  Kochetkov, Rafael Pardinas, Eyal Weizman, and Julien Cornebise", "title": "Objects of violence: synthetic data for practical ML in human rights\n  investigations", "comments": "Presented at NeurIPS 2019 in the AI for Social Good track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a machine learning workflow to search for, identify, and\nmeaningfully triage videos and images of munitions, weapons, and military\nequipment, even when limited training data exists for the object of interest.\nThis workflow is designed to expedite the work of OSINT (\"open source\nintelligence\") researchers in human rights investigations. It consists of three\ncomponents: automatic rendering and annotating of synthetic datasets that make\nup for a lack of training data; training image classifiers from combined sets\nof photographic and synthetic data; and mtriage, an open source software that\norchestrates these classifiers' deployment to triage public domain media, and\nvisualise predictions in a web interface. We show that synthetic data helps to\ntrain classifiers more effectively, and that certain approaches yield better\nresults for different architectures. We then demonstrate our workflow in two\nreal-world human rights investigations: the use of the Triple-Chaser tear gas\ngrenade against civilians, and the verification of allegations of military\npresence in Ukraine in 2014.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:50:43 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Kermode", "Lachlan", ""], ["Freyberg", "Jan", ""], ["Akturk", "Alican", ""], ["Trafford", "Robert", ""], ["Kochetkov", "Denis", ""], ["Pardinas", "Rafael", ""], ["Weizman", "Eyal", ""], ["Cornebise", "Julien", ""]]}, {"id": "2004.01059", "submitter": "Aybora Koksal", "authors": "Aybora Koksal, Kutalmis Gokalp Ince, A. Aydin Alatan", "title": "Effect of Annotation Errors on Drone Detection with YOLOv3", "comments": "Best Paper Award at The 1st Anti-UAV Workshop & Challenge - CVPR\n  Workshops, 2020", "journal-ref": null, "doi": "10.1109/CVPRW50498.2020.00523", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent advances in deep networks, object detection and tracking\nalgorithms with deep learning backbones have been improved significantly;\nhowever, this rapid development resulted in the necessity of large amounts of\nannotated labels. Even if the details of such semi-automatic annotation\nprocesses for most of these datasets are not known precisely, especially for\nthe video annotations, some automated labeling processes are usually employed.\nUnfortunately, such approaches might result with erroneous annotations. In this\nwork, different types of annotation errors for object detection problem are\nsimulated and the performance of a popular state-of-the-art object detector,\nYOLOv3, with erroneous annotations during training and testing stages is\nexamined. Moreover, some inevitable annotation errors in CVPR-2020 Anti-UAV\nChallenge dataset is also examined in this manner, while proposing a solution\nto correct such annotation errors of this valuable data set.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:06:14 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 17:18:25 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 19:30:01 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 14:54:00 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Koksal", "Aybora", ""], ["Ince", "Kutalmis Gokalp", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "2004.01071", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati, Pietro Cerri, Raoul de Charette", "title": "Model-based occlusion disentanglement for image-to-image translation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-to-image translation is affected by entanglement phenomena, which may\noccur in case of target data encompassing occlusions such as raindrops, dirt,\netc. Our unsupervised model-based learning disentangles scene and occlusions,\nwhile benefiting from an adversarial pipeline to regress physical parameters of\nthe occlusion model. The experiments demonstrate our method is able to handle\nvarying types of occlusions and generate highly realistic translations,\nqualitatively and quantitatively outperforming the state-of-the-art on multiple\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:24:41 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 09:27:54 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pizzati", "Fabio", ""], ["Cerri", "Pietro", ""], ["de Charette", "Raoul", ""]]}, {"id": "2004.01091", "submitter": "Lijie Fan", "authors": "Lijie Fan, Tianhong Li, Rongyao Fang, Rumen Hristov, Yuan Yuan, Dina\n  Katabi", "title": "Learning Longterm Representations for Person Re-Identification Using\n  Radio Signals", "comments": "CVPR 2020. The first three authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (ReID) aims to recognize a person-of-interest across\ndifferent places and times. Existing ReID methods rely on images or videos\ncollected using RGB cameras. They extract appearance features like clothes,\nshoes, hair, etc. Such features, however, can change drastically from one day\nto the next, leading to inability to identify people over extended time\nperiods. In this paper, we introduce RF-ReID, a novel approach that harnesses\nradio frequency (RF) signals for longterm person ReID. RF signals traverse\nclothes and reflect off the human body; thus they can be used to extract more\npersistent human-identifying features like body size and shape. We evaluate the\nperformance of RF-ReID on longitudinal datasets that span days and weeks, where\nthe person may wear different clothes across days. Our experiments demonstrate\nthat RF-ReID outperforms state-of-the-art RGB-based ReID approaches for long\nterm person ReID. Our results also reveal two interesting features: First since\nRF signals work in the presence of occlusions and poor lighting, RF-ReID allows\nfor person ReID in such scenarios. Second, unlike photos and videos which\nreveal personal and private information, RF signals are more\nprivacy-preserving, and hence can help extend person ReID to privacy-concerned\ndomains, like healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:50:42 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Fan", "Lijie", ""], ["Li", "Tianhong", ""], ["Fang", "Rongyao", ""], ["Hristov", "Rumen", ""], ["Yuan", "Yuan", ""], ["Katabi", "Dina", ""]]}, {"id": "2004.01095", "submitter": "Han Fu", "authors": "Han Fu, Rui Wu, Chenghao Liu, Jianling Sun", "title": "MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images\n  with Latent Variable Model", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, driven by the increasing concern on diet and health, food computing\nhas attracted enormous attention from both industry and research community. One\nof the most popular research topics in this domain is Food Retrieval, due to\nits profound influence on health-oriented applications. In this paper, we focus\non the task of cross-modal retrieval between food images and cooking recipes.\nWe present Modality-Consistent Embedding Network (MCEN) that learns\nmodality-invariant representations by projecting images and texts to the same\nembedding space. To capture the latent alignments between modalities, we\nincorporate stochastic latent variables to explicitly exploit the interactions\nbetween textual and visual features. Importantly, our method learns the\ncross-modal alignments during training but computes embeddings of different\nmodalities independently at inference time for the sake of efficiency.\nExtensive experimental results clearly demonstrate that the proposed MCEN\noutperforms all existing approaches on the benchmark Recipe1M dataset and\nrequires less computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:00:10 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Fu", "Han", ""], ["Wu", "Rui", ""], ["Liu", "Chenghao", ""], ["Sun", "Jianling", ""]]}, {"id": "2004.01101", "submitter": "Xiaoliang Wang", "authors": "Xiaoliang Wang, Yeqiang Qian, Chunxiang Wang, and Ming Yang", "title": "Map-Enhanced Ego-Lane Detection in the Missing Feature Scenarios", "comments": "There has some mistake in experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important tasks in autonomous driving systems, ego-lane\ndetection has been extensively studied and has achieved impressive results in\nmany scenarios. However, ego-lane detection in the missing feature scenarios is\nstill an unsolved problem. To address this problem, previous methods have been\ndevoted to proposing more complicated feature extraction algorithms, but they\nare very time-consuming and cannot deal with extreme scenarios. Different from\nothers, this paper exploits prior knowledge contained in digital maps, which\nhas a strong capability to enhance the performance of detection algorithms.\nSpecifically, we employ the road shape extracted from OpenStreetMap as lane\nmodel, which is highly consistent with the real lane shape and irrelevant to\nlane features. In this way, only a few lane features are needed to eliminate\nthe position error between the road shape and the real lane, and a search-based\noptimization algorithm is proposed. Experiments show that the proposed method\ncan be applied to various scenarios and can run in real-time at a frequency of\n20 Hz. At the same time, we evaluated the proposed method on the public KITTI\nLane dataset where it achieves state-of-the-art performance. Moreover, our code\nwill be open source after publication.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:06:48 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 10:50:24 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Xiaoliang", ""], ["Qian", "Yeqiang", ""], ["Wang", "Chunxiang", ""], ["Yang", "Ming", ""]]}, {"id": "2004.01110", "submitter": "Ehsan Yaghoubi", "authors": "Ehsan Yaghoubi, Diana Borza, Jo\\~ao Neves, Aruna Kumar, Hugo\n  Proen\\c{c}a", "title": "An Attention-Based Deep Learning Model for Multiple Pedestrian\n  Attributes Recognition", "comments": "Submitted to Image and Vision Computing journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic characterization of pedestrians in surveillance footage is a\ntough challenge, particularly when the data is extremely diverse with cluttered\nbackgrounds, and subjects are captured from varying distances, under multiple\nposes, with partial occlusion. Having observed that the state-of-the-art\nperformance is still unsatisfactory, this paper provides a novel solution to\nthe problem, with two-fold contributions: 1) considering the strong semantic\ncorrelation between the different full-body attributes, we propose a multi-task\ndeep model that uses an element-wise multiplication layer to extract more\ncomprehensive feature representations. In practice, this layer serves as a\nfilter to remove irrelevant background features, and is particularly important\nto handle complex, cluttered data; and 2) we introduce a weighted-sum term to\nthe loss function that not only relativizes the contribution of each task (kind\nof attributed) but also is crucial for performance improvement in\nmultiple-attribute inference settings. Our experiments were performed on two\nwell-known datasets (RAP and PETA) and point for the superiority of the\nproposed method with respect to the state-of-the-art. The code is available at\nhttps://github.com/Ehsan-Yaghoubi/MAN-PAR-.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:21:14 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Yaghoubi", "Ehsan", ""], ["Borza", "Diana", ""], ["Neves", "Jo\u00e3o", ""], ["Kumar", "Aruna", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2004.01113", "submitter": "Eu Wern Teh", "authors": "Eu Wern Teh, Terrance DeVries, Graham W. Taylor", "title": "ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component\n  Analysis", "comments": "To appear in the European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distance metric learning (DML), where the task is\nto learn an effective similarity measure between images. We revisit ProxyNCA\nand incorporate several enhancements. We find that low temperature scaling is a\nperformance-critical component and explain why it works. Besides, we also\ndiscover that Global Max Pooling works better in general when compared to\nGlobal Average Pooling. Additionally, our proposed fast moving proxies also\naddresses small gradient issue of proxies, and this component synergizes well\nwith low temperature scaling and Global Max Pooling. Our enhanced model, called\nProxyNCA++, achieves a 22.9 percentage point average improvement of Recall@1\nacross four different zero-shot retrieval datasets compared to the original\nProxyNCA algorithm. Furthermore, we achieve state-of-the-art results on the\nCUB200, Cars196, Sop, and InShop datasets, achieving Recall@1 scores of 72.2,\n90.1, 81.4, and 90.9, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:24:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 14:21:13 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Teh", "Eu Wern", ""], ["DeVries", "Terrance", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2004.01130", "submitter": "Maxime Bucher", "authors": "Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick P\\'erez", "title": "Handling new target classes in semantic segmentation with domain\n  adaptation", "comments": "Under review at CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we define and address a novel domain adaptation (DA) problem in\nsemantic scene segmentation, where the target domain not only exhibits a data\ndistribution shift w.r.t. the source domain, but also includes novel classes\nthat do not exist in the latter. Different to \"open-set\" and \"universal domain\nadaptation\", which both regard all objects from new classes as \"unknown\", we\naim at explicit test-time prediction for these new classes. To reach this goal,\nwe propose a framework that leverages domain adaptation and zero-shot learning\ntechniques to enable \"boundless\" adaptation in the target domain. It relies on\na novel architecture, along with a dedicated learning scheme, to bridge the\nsource-target domain gap while learning how to map new classes' labels to\nrelevant visual representations. The performance is further improved using\nself-training on target-domain pseudo-labels. For validation, we consider\ndifferent domain adaptation set-ups, namely synthetic-2-real, country-2-country\nand dataset-2-dataset. Our framework outperforms the baselines by significant\nmargins, setting competitive standards on all benchmarks for the new task. Code\nand models are available at https://github.com/valeoai/buda.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:59:57 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 13:08:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Bucher", "Maxime", ""], ["Vu", "Tuan-Hung", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2004.01166", "submitter": "Henry M. Clever", "authors": "Henry M. Clever, Zackory Erickson, Ariel Kapusta, Greg Turk, C. Karen\n  Liu, and Charles C. Kemp", "title": "Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image\n  using Synthetic Data", "comments": "18 pages, 18 figures, 5 tables. Accepted for oral presentation at\n  CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People spend a substantial part of their lives at rest in bed. 3D human pose\nand shape estimation for this activity would have numerous beneficial\napplications, yet line-of-sight perception is complicated by occlusion from\nbedding. Pressure sensing mats are a promising alternative, but training data\nis challenging to collect at scale. We describe a physics-based method that\nsimulates human bodies at rest in a bed with a pressure sensing mat, and\npresent PressurePose, a synthetic dataset with 206K pressure images with 3D\nhuman poses and shapes. We also present PressureNet, a deep learning model that\nestimates human pose and shape given a pressure image and gender. PressureNet\nincorporates a pressure map reconstruction (PMR) network that models pressure\nimage generation to promote consistency between estimated 3D body models and\npressure image input. In our evaluations, PressureNet performed well with real\ndata from participants in diverse poses, even though it had only been trained\nwith synthetic data. When we ablated the PMR network, performance dropped\nsubstantially.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:44:58 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Clever", "Henry M.", ""], ["Erickson", "Zackory", ""], ["Kapusta", "Ariel", ""], ["Turk", "Greg", ""], ["Liu", "C. Karen", ""], ["Kemp", "Charles C.", ""]]}, {"id": "2004.01170", "submitter": "Mahyar Najibi", "authors": "Mahyar Najibi, Guangda Lai, Abhijit Kundu, Zhichao Lu, Vivek Rathod,\n  Thomas Funkhouser, Caroline Pantofaru, David Ross, Larry S. Davis, Alireza\n  Fathi", "title": "DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DOPS, a fast single-stage 3D object detection method for LIDAR\ndata. Previous methods often make domain-specific design decisions, for example\nprojecting points into a bird-eye view image in autonomous driving scenarios.\nIn contrast, we propose a general-purpose method that works on both indoor and\noutdoor scenes. The core novelty of our method is a fast, single-pass\narchitecture that both detects objects in 3D and estimates their shapes. 3D\nbounding box parameters are estimated in one pass for every point, aggregated\nthrough graph convolutions, and fed into a branch of the network that predicts\nlatent codes representing the shape of each detected object. The latent shape\nspace and shape decoder are learned on a synthetic dataset and then used as\nsupervision for the end-to-end training of the 3D object detection pipeline.\nThus our model is able to extract shapes without access to ground-truth shape\ninformation in the target dataset. During experiments, we find that our\nproposed method achieves state-of-the-art results by ~5% on object detection in\nScanNet scenes, and it gets top results by 3.4% in the Waymo Open Dataset,\nwhile reproducing the shapes of detected cars.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:48:50 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 00:40:57 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Najibi", "Mahyar", ""], ["Lai", "Guangda", ""], ["Kundu", "Abhijit", ""], ["Lu", "Zhichao", ""], ["Rathod", "Vivek", ""], ["Funkhouser", "Thomas", ""], ["Pantofaru", "Caroline", ""], ["Ross", "David", ""], ["Davis", "Larry S.", ""], ["Fathi", "Alireza", ""]]}, {"id": "2004.01176", "submitter": "Despoina Paschalidou", "authors": "Despoina Paschalidou, Luc van Gool, and Andreas Geiger", "title": "Learning Unsupervised Hierarchical Part Decomposition of 3D Objects from\n  a Single RGB Image", "comments": "To appear at CVPR 2020, project page\n  https://github.com/paschalidoud/hierarchical_primitives", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perceive the 3D world as a set of distinct objects that are\ncharacterized by various low-level (geometry, reflectance) and high-level\n(connectivity, adjacency, symmetry) properties. Recent methods based on\nconvolutional neural networks (CNNs) demonstrated impressive progress in 3D\nreconstruction, even when using a single 2D image as input. However, the\nmajority of these methods focuses on recovering the local 3D geometry of an\nobject without considering its part-based decomposition or relations between\nparts. We address this challenging problem by proposing a novel formulation\nthat allows to jointly recover the geometry of a 3D object as a set of\nprimitives as well as their latent hierarchical structure without part-level\nsupervision. Our model recovers the higher level structural decomposition of\nvarious objects in the form of a binary tree of primitives, where simple parts\nare represented with fewer primitives and more complex parts are modeled with\nmore components. Our experiments on the ShapeNet and D-FAUST datasets\ndemonstrate that considering the organization of parts indeed facilitates\nreasoning about 3D geometry.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:58:05 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Paschalidou", "Despoina", ""], ["van Gool", "Luc", ""], ["Geiger", "Andreas", ""]]}, {"id": "2004.01177", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Vladlen Koltun, Philipp Kr\\\"ahenb\\\"uhl", "title": "Tracking Objects as Points", "comments": "ECCV 2020 Camera-ready version. Updated track rebirth results. Code\n  available at https://github.com/xingyizhou/CenterTrack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking has traditionally been the art of following interest points through\nspace and time. This changed with the rise of powerful deep networks. Nowadays,\ntracking is dominated by pipelines that perform object detection followed by\ntemporal association, also known as tracking-by-detection. In this paper, we\npresent a simultaneous detection and tracking algorithm that is simpler,\nfaster, and more accurate than the state of the art. Our tracker, CenterTrack,\napplies a detection model to a pair of images and detections from the prior\nframe. Given this minimal input, CenterTrack localizes objects and predicts\ntheir associations with the previous frame. That's it. CenterTrack is simple,\nonline (no peeking into the future), and real-time. It achieves 67.3% MOTA on\nthe MOT17 challenge at 22 FPS and 89.4% MOTA on the KITTI tracking benchmark at\n15 FPS, setting a new state of the art on both datasets. CenterTrack is easily\nextended to monocular 3D tracking by regressing additional 3D attributes. Using\nmonocular video input, it achieves 28.3% AMOTA@0.2 on the newly released\nnuScenes 3D tracking benchmark, substantially outperforming the monocular\nbaseline on this benchmark while running at 28 FPS.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:58:40 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 16:28:05 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zhou", "Xingyi", ""], ["Koltun", "Vladlen", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2004.01178", "submitter": "Yawei Li", "authors": "Yunxuan Wei, Shuhang Gu, Yawei Li, Longcun Jin", "title": "Unsupervised Real-world Image Super Resolution via Domain-distance Aware\n  Training", "comments": "Code will be available at https://github.com/ShuhangGu/DASR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These days, unsupervised super-resolution (SR) has been soaring due to its\npractical and promising potential in real scenarios. The philosophy of\noff-the-shelf approaches lies in the augmentation of unpaired data, i.e. first\ngenerating synthetic low-resolution (LR) images $\\mathcal{Y}^g$ corresponding\nto real-world high-resolution (HR) images $\\mathcal{X}^r$ in the real-world LR\ndomain $\\mathcal{Y}^r$, and then utilizing the pseudo pairs $\\{\\mathcal{Y}^g,\n\\mathcal{X}^r\\}$ for training in a supervised manner. Unfortunately, since\nimage translation itself is an extremely challenging task, the SR performance\nof these approaches are severely limited by the domain gap between generated\nsynthetic LR images and real LR images. In this paper, we propose a novel\ndomain-distance aware super-resolution (DASR) approach for unsupervised\nreal-world image SR. The domain gap between training data (e.g.\n$\\mathcal{Y}^g$) and testing data (e.g. $\\mathcal{Y}^r$) is addressed with our\n\\textbf{domain-gap aware training} and \\textbf{domain-distance weighted\nsupervision} strategies. Domain-gap aware training takes additional benefit\nfrom real data in the target domain while domain-distance weighted supervision\nbrings forward the more rational use of labeled source domain data. The\nproposed method is validated on synthetic and real datasets and the\nexperimental results show that DASR consistently outperforms state-of-the-art\nunsupervised SR approaches in generating SR outputs with more realistic and\nnatural textures.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:59:03 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wei", "Yunxuan", ""], ["Gu", "Shuhang", ""], ["Li", "Yawei", ""], ["Jin", "Longcun", ""]]}, {"id": "2004.01179", "submitter": "Jia-Bin Huang", "authors": "Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan\n  Yang, Yung-Yu Chuang, and Jia-Bin Huang", "title": "Single-Image HDR Reconstruction by Learning to Reverse the Camera\n  Pipeline", "comments": "CVPR 2020. Project page:\n  https://www.cmlab.csie.ntu.edu.tw/~yulunliu/SingleHDR Code:\n  https://github.com/alex04072000/SingleHDR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a high dynamic range (HDR) image from a single low dynamic range\n(LDR) input image is challenging due to missing details in under-/over-exposed\nregions caused by quantization and saturation of camera sensors. In contrast to\nexisting learning-based methods, our core idea is to incorporate the domain\nknowledge of the LDR image formation pipeline into our model. We model the\nHDRto-LDR image formation pipeline as the (1) dynamic range clipping, (2)\nnon-linear mapping from a camera response function, and (3) quantization. We\nthen propose to learn three specialized CNNs to reverse these steps. By\ndecomposing the problem into specific sub-tasks, we impose effective physical\nconstraints to facilitate the training of individual sub-networks. Finally, we\njointly fine-tune the entire model end-to-end to reduce error accumulation.\nWith extensive quantitative and qualitative experiments on diverse image\ndatasets, we demonstrate that the proposed method performs favorably against\nstate-of-the-art single-image HDR reconstruction algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:59:04 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Yu-Lun", ""], ["Lai", "Wei-Sheng", ""], ["Chen", "Yu-Sheng", ""], ["Kao", "Yi-Lung", ""], ["Yang", "Ming-Hsuan", ""], ["Chuang", "Yung-Yu", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2004.01180", "submitter": "Jia-Bin Huang", "authors": "Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin\n  Huang", "title": "Learning to See Through Obstructions", "comments": "CVPR 2020. Project page:\n  https://www.cmlab.csie.ntu.edu.tw/~yulunliu/ObstructionRemoval Code:\n  https://github.com/alex04072000/ObstructionRemoval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based approach for removing unwanted obstructions, such\nas window reflections, fence occlusions or raindrops, from a short sequence of\nimages captured by a moving camera. Our method leverages the motion differences\nbetween the background and the obstructing elements to recover both layers.\nSpecifically, we alternate between estimating dense optical flow fields of the\ntwo layers and reconstructing each layer from the flow-warped images via a deep\nconvolutional neural network. The learning-based layer reconstruction allows us\nto accommodate potential errors in the flow estimation and brittle assumptions\nsuch as brightness consistency. We show that training on synthetically\ngenerated data transfers well to real images. Our results on numerous\nchallenging scenarios of reflection and fence removal demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 17:59:12 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Yu-Lun", ""], ["Lai", "Wei-Sheng", ""], ["Yang", "Ming-Hsuan", ""], ["Chuang", "Yung-Yu", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2004.01181", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Simon Alford, Vijay Gadepally, Michael Jones, Lauren\n  Milechin, Albert Reuther, Ryan Robinett, Sid Samsi", "title": "GraphChallenge.org Sparse Deep Neural Network Performance", "comments": "7 pages, 7 figures, 80 references, to be submitted to IEEE HPEC 2020.\n  This work reports new updated results on prior work reported in\n  arXiv:1909.05631. arXiv admin note: substantial text overlap with\n  arXiv:1807.03165, arXiv:1708.02937. arXiv admin note: text overlap with\n  arXiv:2003.09269", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286253", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to\ndeveloping new solutions for analyzing graphs and sparse data. Sparse AI\nanalytics present unique scalability difficulties. The Sparse Deep Neural\nNetwork (DNN) Challenge draws upon prior challenges from machine learning, high\nperformance computing, and visual analytics to create a challenge that is\nreflective of emerging sparse AI systems. The sparse DNN challenge is based on\na mathematically well-defined DNN inference computation and can be implemented\nin any programming environment. In 2019 several sparse DNN challenge\nsubmissions were received from a wide range of authors and organizations. This\npaper presents a performance analysis of the best performers of these\nsubmissions. These submissions show that their state-of-the-art sparse DNN\nexecution time, $T_{\\rm DNN}$, is a strong function of the number of DNN\noperations performed, $N_{\\rm op}$. The sparse DNN challenge provides a clear\npicture of current sparse DNN systems and underscores the need for new\ninnovations to achieve high performance on very large sparse DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 00:29:12 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 02:38:52 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kepner", "Jeremy", ""], ["Alford", "Simon", ""], ["Gadepally", "Vijay", ""], ["Jones", "Michael", ""], ["Milechin", "Lauren", ""], ["Reuther", "Albert", ""], ["Robinett", "Ryan", ""], ["Samsi", "Sid", ""]]}, {"id": "2004.01184", "submitter": "Aboul Ella Hassanien Abo", "authors": "Nour Eldeen M. Khalifa, Mohamed Hamed N. Taha, Aboul Ella Hassanien,\n  Sally Elghamrawy", "title": "Detection of Coronavirus (COVID-19) Associated Pneumonia based on\n  Generative Adversarial Networks and a Fine-Tuned Deep Transfer Learning Model\n  using Chest X-ray Dataset", "comments": "15 pages, 3 Tables and 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 coronavirus is one of the devastating viruses according to the\nworld health organization. This novel virus leads to pneumonia, which is an\ninfection that inflames the lungs' air sacs of a human. One of the methods to\ndetect those inflames is by using x-rays for the chest. In this paper, a\npneumonia chest x-ray detection based on generative adversarial networks (GAN)\nwith a fine-tuned deep transfer learning for a limited dataset will be\npresented. The use of GAN positively affects the proposed model robustness and\nmade it immune to the overfitting problem and helps in generating more images\nfrom the dataset. The dataset used in this research consists of 5863 X-ray\nimages with two categories: Normal and Pneumonia. This research uses only 10%\nof the dataset for training data and generates 90% of images using GAN to prove\nthe efficiency of the proposed model. Through the paper, AlexNet, GoogLeNet,\nSqueeznet, and Resnet18 are selected as deep transfer learning models to detect\nthe pneumonia from chest x-rays. Those models are selected based on their small\nnumber of layers on their architectures, which will reflect in reducing the\ncomplexity of the models and the consumed memory and time. Using a combination\nof GAN and deep transfer models proved it is efficiency according to testing\naccuracy measurement. The research concludes that the Resnet18 is the most\nappropriate deep transfer model according to testing accuracy measurement and\nachieved 99% with the other performance metrics such as precision, recall, and\nF1 score while using GAN as an image augmenter. Finally, a comparison result\nwas carried out at the end of the research with related work which used the\nsame dataset except that this research used only 10% of original dataset. The\npresented work achieved a superior result than the related work in terms of\ntesting accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:14:37 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Khalifa", "Nour Eldeen M.", ""], ["Taha", "Mohamed Hamed N.", ""], ["Hassanien", "Aboul Ella", ""], ["Elghamrawy", "Sally", ""]]}, {"id": "2004.01185", "submitter": "Titas De", "authors": "Axel Wismueller, Titas De, Eva Lochmueller, Felix Eckstein, Mahesh B.\n  Nagarajan", "title": "Introducing Anisotropic Minkowski Functionals and Quantitative\n  Anisotropy Measures for Local Structure Analysis in Biomedical Imaging", "comments": "SPIE Medical Imaging 2013. arXiv admin note: text overlap with\n  arXiv:2002.07156", "journal-ref": null, "doi": "10.1117/86720I-86720I-8", "report-no": null, "categories": "eess.IV cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of Minkowski Functionals to characterize local structure in\ndifferent biological tissue types has been demonstrated in a variety of medical\nimage processing tasks. We introduce anisotropic Minkowski Functionals (AMFs)\nas a novel variant that captures the inherent anisotropy of the underlying\ngray-level structures. To quantify the anisotropy characterized by our\napproach, we further introduce a method to compute a quantitative measure\nmotivated by a technique utilized in MR diffusion tensor imaging, namely\nfractional anisotropy. We showcase the applicability of our method in the\nresearch context of characterizing the local structure properties of trabecular\nbone micro-architecture in the proximal femur as visualized on multi-detector\nCT. To this end, AMFs were computed locally for each pixel of ROIs extracted\nfrom the head, neck and trochanter regions. Fractional anisotropy was then used\nto quantify the local anisotropy of the trabecular structures found in these\nROIs and to compare its distribution in different anatomical regions. Our\nresults suggest a significantly greater concentration of anisotropic trabecular\nstructures in the head and neck regions when compared to the trochanter region\n(p < 10-4). We also evaluated the ability of such AMFs to predict bone strength\nin the femoral head of proximal femur specimens obtained from 50 donors. Our\nresults suggest that such AMFs, when used in conjunction with multi-regression\nmodels, can outperform more conventional features such as BMD in predicting\nfailure load. We conclude that such anisotropic Minkowski Functionals can\ncapture valuable information regarding directional attributes of local\nstructure, which may be useful in a wide scope of biomedical imaging\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 14:13:50 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wismueller", "Axel", ""], ["De", "Titas", ""], ["Lochmueller", "Eva", ""], ["Eckstein", "Felix", ""], ["Nagarajan", "Mahesh B.", ""]]}, {"id": "2004.01225", "submitter": "O\\u{g}ulcan \\\"Ozdemir", "authors": "Ahmet Alp K{\\i}nd{\\i}ro\\u{g}lu, O\\u{g}ulcan \\\"Ozdemir and Lale Akarun", "title": "Temporal Accumulative Features for Sign Language Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a set of features called temporal accumulative\nfeatures (TAF) for representing and recognizing isolated sign language\ngestures. By incorporating sign language specific constructs to better\nrepresent the unique linguistic characteristic of sign language videos, we have\ndevised an efficient and fast SLR method for recognizing isolated sign language\ngestures. The proposed method is an HSV based accumulative video representation\nwhere keyframes based on the linguistic movement-hold model are represented by\ndifferent colors. We also incorporate hand shape information and using a small\nscale convolutional neural network, demonstrate that sequential modeling of\naccumulative features for linguistic subunits improves upon baseline\nclassification results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 19:03:40 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["K\u0131nd\u0131ro\u011flu", "Ahmet Alp", ""], ["\u00d6zdemir", "O\u011fulcan", ""], ["Akarun", "Lale", ""]]}, {"id": "2004.01228", "submitter": "Mikaela Angelina Uy", "authors": "Mikaela Angelina Uy and Jingwei Huang and Minhyuk Sung and Tolga\n  Birdal and Leonidas Guibas", "title": "Deformation-Aware 3D Model Embedding and Retrieval", "comments": "Accepted for publication at ECCV 2020. Project page under\n  https://deformscan2cad.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new problem of retrieving 3D models that are deformable to a\ngiven query shape and present a novel deep deformation-aware embedding to solve\nthis retrieval task. 3D model retrieval is a fundamental operation for\nrecovering a clean and complete 3D model from a noisy and partial 3D scan.\nHowever, given a finite collection of 3D shapes, even the closest model to a\nquery may not be satisfactory. This motivates us to apply 3D model deformation\ntechniques to adapt the retrieved model so as to better fit the query. Yet,\ncertain restrictions are enforced in most 3D deformation techniques to preserve\nimportant features of the original model that prevent a perfect fitting of the\ndeformed model to the query. This gap between the deformed model and the query\ninduces asymmetric relationships among the models, which cannot be handled by\ntypical metric learning techniques. Thus, to retrieve the best models for\nfitting, we propose a novel deep embedding approach that learns the asymmetric\nrelationships by leveraging location-dependent egocentric distance fields. We\nalso propose two strategies for training the embedding network. We demonstrate\nthat both of these approaches outperform other baselines in our experiments\nwith both synthetic and real data. Our project page can be found at\nhttps://deformscan2cad.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 19:10:57 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 02:38:26 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 05:10:25 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Uy", "Mikaela Angelina", ""], ["Huang", "Jingwei", ""], ["Sung", "Minhyuk", ""], ["Birdal", "Tolga", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2004.01241", "submitter": "Md Jahidul Islam", "authors": "Md Jahidul Islam, Chelsey Edge, Yuyang Xiao, Peigen Luo, Muntaqim\n  Mehtaz, Christopher Morse, Sadman Sakib Enan and Junaed Sattar", "title": "Semantic Segmentation of Underwater Imagery: Dataset and Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the first large-scale dataset for semantic\nSegmentation of Underwater IMagery (SUIM). It contains over 1500 images with\npixel annotations for eight object categories: fish (vertebrates), reefs\n(invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and\nsea-floor. The images have been rigorously collected during oceanic\nexplorations and human-robot collaborative experiments, and annotated by human\nparticipants. We also present a benchmark evaluation of state-of-the-art\nsemantic segmentation approaches based on standard performance metrics. In\naddition, we present SUIM-Net, a fully-convolutional encoder-decoder model that\nbalances the trade-off between performance and computational efficiency. It\noffers competitive performance while ensuring fast end-to-end inference, which\nis essential for its use in the autonomy pipeline of visually-guided underwater\nrobots. In particular, we demonstrate its usability benefits for visual\nservoing, saliency prediction, and detailed scene understanding. With a variety\nof use cases, the proposed model and benchmark dataset open up promising\nopportunities for future research in underwater robot vision.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 19:53:14 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 17:51:50 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 23:47:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Islam", "Md Jahidul", ""], ["Edge", "Chelsey", ""], ["Xiao", "Yuyang", ""], ["Luo", "Peigen", ""], ["Mehtaz", "Muntaqim", ""], ["Morse", "Christopher", ""], ["Enan", "Sadman Sakib", ""], ["Sattar", "Junaed", ""]]}, {"id": "2004.01255", "submitter": "Yifan Xu", "authors": "Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max\n  Welling, Zhuowen Tu", "title": "Guided Variational Autoencoder for Disentanglement Learning", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm, guided variational autoencoder (Guided-VAE), that is\nable to learn a controllable generative model by performing latent\nrepresentation disentanglement learning. The learning objective is achieved by\nproviding signals to the latent encoding/embedding in VAE without changing its\nmain backbone architecture, hence retaining the desirable properties of the\nVAE. We design an unsupervised strategy and a supervised strategy in Guided-VAE\nand observe enhanced modeling and controlling capability over the vanilla VAE.\nIn the unsupervised strategy, we guide the VAE learning by introducing a\nlightweight decoder that learns latent geometric transformation and principal\ncomponents; in the supervised strategy, we use an adversarial excitation and\ninhibition mechanism to encourage the disentanglement of the latent variables.\nGuided-VAE enjoys its transparency and simplicity for the general\nrepresentation learning task, as well as disentanglement learning. On a number\nof experiments for representation learning, improved synthesis/sampling, better\ndisentanglement for classification, and reduced classification errors in\nmeta-learning have been observed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 20:49:15 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ding", "Zheng", ""], ["Xu", "Yifan", ""], ["Xu", "Weijian", ""], ["Parmar", "Gaurav", ""], ["Yang", "Yang", ""], ["Welling", "Max", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2004.01278", "submitter": "Juan-Manuel Perez-Rua", "authors": "Juan-Manuel Perez-Rua and Brais Martinez and Xiatian Zhu and Antoine\n  Toisoul and Victor Escorcia and Tao Xiang", "title": "Knowing What, Where and When to Look: Efficient Video Action Modeling\n  with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Attentive video modeling is essential for action recognition in unconstrained\nvideos due to their rich yet redundant information over space and time.\nHowever, introducing attention in a deep neural network for action recognition\nis challenging for two reasons. First, an effective attention module needs to\nlearn what (objects and their local motion patterns), where (spatially), and\nwhen (temporally) to focus on. Second, a video attention module must be\nefficient because existing action recognition models already suffer from high\ncomputational cost. To address both challenges, a novel What-Where-When (W3)\nvideo attention module is proposed. Departing from existing alternatives, our\nW3 module models all three facets of video attention jointly. Crucially, it is\nextremely efficient by factorizing the high-dimensional video feature data into\nlow-dimensional meaningful spaces (1D channel vector for `what' and 2D spatial\ntensors for `where'), followed by lightweight temporal attention reasoning.\nExtensive experiments show that our attention model brings significant\nimprovements to existing action recognition models, achieving new\nstate-of-the-art performance on a number of benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 21:48:11 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Perez-Rua", "Juan-Manuel", ""], ["Martinez", "Brais", ""], ["Zhu", "Xiatian", ""], ["Toisoul", "Antoine", ""], ["Escorcia", "Victor", ""], ["Xiang", "Tao", ""]]}, {"id": "2004.01279", "submitter": "Sasa Grbic", "authors": "Shikha Chaganti, Abishek Balachandran, Guillaume Chabin, Stuart Cohen,\n  Thomas Flohr, Bogdan Georgescu, Philippe Grenier, Sasa Grbic, Siqi Liu,\n  Fran\\c{c}ois Mellot, Nicolas Murray, Savvas Nicolaou, William Parker, Thomas\n  Re, Pina Sanelli, Alexander W. Sauter, Zhoubing Xu, Youngjin Yoo, Valentin\n  Ziebandt, Dorin Comaniciu", "title": "Automated Quantification of CT Patterns Associated with COVID-19 from\n  Chest CT", "comments": null, "journal-ref": "Radiology: Artificial Intelligence, Vol. 2, No. 4, 2020", "doi": "10.1148/ryai.2020200048#", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To present a method that automatically segments and quantifies\nabnormal CT patterns commonly present in coronavirus disease 2019 (COVID-19),\nnamely ground glass opacities and consolidations. Materials and Methods: In\nthis retrospective study, the proposed method takes as input a non-contrasted\nchest CT and segments the lesions, lungs, and lobes in three dimensions, based\non a dataset of 9749 chest CT volumes. The method outputs two combined measures\nof the severity of lung and lobe involvement, quantifying both the extent of\nCOVID-19 abnormalities and presence of high opacities, based on deep learning\nand deep reinforcement learning. The first measure of (PO, PHO) is global,\nwhile the second of (LSS, LHOS) is lobewise. Evaluation of the algorithm is\nreported on CTs of 200 participants (100 COVID-19 confirmed patients and 100\nhealthy controls) from institutions from Canada, Europe and the United States\ncollected between 2002-Present (April, 2020). Ground truth is established by\nmanual annotations of lesions, lungs, and lobes. Correlation and regression\nanalyses were performed to compare the prediction to the ground truth. Results:\nPearson correlation coefficient between method prediction and ground truth for\nCOVID-19 cases was calculated as 0.92 for PO (P < .001), 0.97 for PHO(P <\n.001), 0.91 for LSS (P < .001), 0.90 for LHOS (P < .001). 98 of 100 healthy\ncontrols had a predicted PO of less than 1%, 2 had between 1-2%. Automated\nprocessing time to compute the severity scores was 10 seconds per case compared\nto 30 minutes required for manual annotations. Conclusion: A new method\nsegments regions of CT abnormalities associated with COVID-19 and computes (PO,\nPHO), as well as (LSS, LHOS) severity scores.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 21:49:14 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:54:07 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 01:53:01 GMT"}, {"version": "v4", "created": "Tue, 26 May 2020 18:09:16 GMT"}, {"version": "v5", "created": "Thu, 4 Jun 2020 15:34:36 GMT"}, {"version": "v6", "created": "Mon, 20 Jul 2020 11:59:37 GMT"}, {"version": "v7", "created": "Wed, 18 Nov 2020 21:17:32 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chaganti", "Shikha", ""], ["Balachandran", "Abishek", ""], ["Chabin", "Guillaume", ""], ["Cohen", "Stuart", ""], ["Flohr", "Thomas", ""], ["Georgescu", "Bogdan", ""], ["Grenier", "Philippe", ""], ["Grbic", "Sasa", ""], ["Liu", "Siqi", ""], ["Mellot", "Fran\u00e7ois", ""], ["Murray", "Nicolas", ""], ["Nicolaou", "Savvas", ""], ["Parker", "William", ""], ["Re", "Thomas", ""], ["Sanelli", "Pina", ""], ["Sauter", "Alexander W.", ""], ["Xu", "Zhoubing", ""], ["Yoo", "Youngjin", ""], ["Ziebandt", "Valentin", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2004.01283", "submitter": "O\\u{g}ulcan \\\"Ozdemir", "authors": "O\\u{g}ulcan \\\"Ozdemir, Ahmet Alp K{\\i}nd{\\i}ro\\u{g}lu, Necati Cihan\n  Camg\\\"oz and Lale Akarun", "title": "BosphorusSign22k Sign Language Recognition Dataset", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language Recognition is a challenging research domain. It has recently\nseen several advancements with the increased availability of data. In this\npaper, we introduce the BosphorusSign22k, a publicly available large scale sign\nlanguage dataset aimed at computer vision, video recognition and deep learning\nresearch communities. The primary objective of this dataset is to serve as a\nnew benchmark in Turkish Sign Language Recognition for its vast lexicon, the\nhigh number of repetitions by native signers, high recording quality, and the\nunique syntactic properties of the signs it encompasses. We also provide\nstate-of-the-art human pose estimates to encourage other tasks such as Sign\nLanguage Production. We survey other publicly available datasets and expand on\nhow BosphorusSign22k can contribute to future research that is being made\npossible through the widespread availability of similar Sign Language\nresources. We have conducted extensive experiments and present baseline results\nto underpin future research on our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 22:15:38 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 14:07:10 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["\u00d6zdemir", "O\u011fulcan", ""], ["K\u0131nd\u0131ro\u011flu", "Ahmet Alp", ""], ["Camg\u00f6z", "Necati Cihan", ""], ["Akarun", "Lale", ""]]}, {"id": "2004.01288", "submitter": "Dominik Notz", "authors": "Dominik Notz, Felix Becker, Thomas K\\\"uhbeck, Daniel Watzenig", "title": "Extraction and Assessment of Naturalistic Human Driving Trajectories\n  from Infrastructure Camera and Radar Sensors", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting realistic driving trajectories is crucial for training machine\nlearning models that imitate human driving behavior. Most of today's autonomous\ndriving datasets contain only a few trajectories per location and are recorded\nwith test vehicles that are cautiously driven by trained drivers. In particular\nin interactive scenarios such as highway merges, the test driver's behavior\nsignificantly influences other vehicles. This influence prevents recording the\nwhole traffic space of human driving behavior. In this work, we present a novel\nmethodology to extract trajectories of traffic objects using infrastructure\nsensors. Infrastructure sensors allow us to record a lot of data for one\nlocation and take the test drivers out of the loop. We develop both a hardware\nsetup consisting of a camera and a traffic surveillance radar and a trajectory\nextraction algorithm. Our vision pipeline accurately detects objects, fuses\ncamera and radar detections and tracks them over time. We improve a\nstate-of-the-art object tracker by combining the tracking in image coordinates\nwith a Kalman filter in road coordinates. We show that our sensor fusion\napproach successfully combines the advantages of camera and radar detections\nand outperforms either single sensor. Finally, we also evaluate the accuracy of\nour trajectory extraction pipeline. For that, we equip our test vehicle with a\ndifferential GPS sensor and use it to collect ground truth trajectories. With\nthis data we compute the measurement errors. While we use the mean error to\nde-bias the trajectories, the error standard deviation is in the magnitude of\nthe ground truth data inaccuracy. Hence, the extracted trajectories are not\nonly naturalistic but also highly accurate and prove the potential of using\ninfrastructure sensors to extract real-world trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 22:28:29 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Notz", "Dominik", ""], ["Becker", "Felix", ""], ["K\u00fchbeck", "Thomas", ""], ["Watzenig", "Daniel", ""]]}, {"id": "2004.01294", "submitter": "Jae Shin Yoon", "authors": "Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz", "title": "Novel View Synthesis of Dynamic Scenes with Globally Coherent Depths\n  from a Monocular Camera", "comments": "This paper is accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to synthesize an image from arbitrary views\nand times given a collection of images of a dynamic scene. A key challenge for\nthe novel view synthesis arises from dynamic scene reconstruction where\nepipolar geometry does not apply to the local motion of dynamic contents. To\naddress this challenge, we propose to combine the depth from single view (DSV)\nand the depth from multi-view stereo (DMV), where DSV is complete, i.e., a\ndepth is assigned to every pixel, yet view-variant in its scale, while DMV is\nview-invariant yet incomplete. Our insight is that although its scale and\nquality are inconsistent with other views, the depth estimation from a single\nview can be used to reason about the globally coherent geometry of dynamic\ncontents. We cast this problem as learning to correct the scale of DSV, and to\nrefine each depth with locally consistent motions between views to form a\ncoherent depth estimation. We integrate these tasks into a depth fusion network\nin a self-supervised fashion. Given the fused depth maps, we synthesize a\nphotorealistic virtual view in a specific location and time with our deep\nblending network that completes the scene and renders the virtual view. We\nevaluate our method of depth estimation and view synthesis on diverse\nreal-world dynamic scenes and show the outstanding performance over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 22:45:53 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yoon", "Jae Shin", ""], ["Kim", "Kihwan", ""], ["Gallo", "Orazio", ""], ["Park", "Hyun Soo", ""], ["Kautz", "Jan", ""]]}, {"id": "2004.01301", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Yifei Xu, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu", "title": "Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets\n  for 3D Generation, Reconstruction and Classification", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model of unordered point sets, such as point clouds,\nin the form of an energy-based model, where the energy function is\nparameterized by an input-permutation-invariant bottom-up neural network. The\nenergy function learns a coordinate encoding of each point and then aggregates\nall individual point features into an energy for the whole point cloud. We call\nour model the Generative PointNet because it can be derived from the\ndiscriminative PointNet. Our model can be trained by MCMC-based maximum\nlikelihood learning (as well as its variants), without the help of any\nassisting networks like those in GANs and VAEs. Unlike most point cloud\ngenerators that rely on hand-crafted distance metrics, our model does not\nrequire any hand-crafted distance metric for the point cloud generation,\nbecause it synthesizes point clouds by matching observed examples in terms of\nstatistical properties defined by the energy function. Furthermore, we can\nlearn a short-run MCMC toward the energy-based model as a flow-like generator\nfor point cloud reconstruction and interpolation. The learned point cloud\nrepresentation can be useful for point cloud classification. Experiments\ndemonstrate the advantages of the proposed generative model of point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 23:08:10 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:37:08 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Xie", "Jianwen", ""], ["Xu", "Yifei", ""], ["Zheng", "Zilong", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2004.01307", "submitter": "Md Selim", "authors": "Md Selim, Jie Zhang, Baowei Fei, Guo-Qiang Zhang and Jin Chen", "title": "STAN-CT: Standardizing CT Image using Generative Adversarial Network", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed tomography (CT) plays an important role in lung malignancy\ndiagnostics and therapy assessment and facilitating precision medicine\ndelivery. However, the use of personalized imaging protocols poses a challenge\nin large-scale cross-center CT image radiomic studies. We present an end-to-end\nsolution called STAN-CT for CT image standardization and normalization, which\neffectively reduces discrepancies in image features caused by using different\nimaging protocols or using different CT scanners with the same imaging\nprotocol. STAN-CT consists of two components: 1) a novel Generative Adversarial\nNetworks (GAN) model that is capable of effectively learning the data\ndistribution of a standard imaging protocol with only a few rounds of generator\ntraining, and 2) an automatic DICOM reconstruction pipeline with systematic\nimage quality control that ensure the generation of high-quality standard DICOM\nimages. Experimental results indicate that the training efficiency and model\nperformance of STAN-CT have been significantly improved compared to the\nstate-of-the-art CT image standardization and normalization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 23:43:06 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Selim", "Md", ""], ["Zhang", "Jie", ""], ["Fei", "Baowei", ""], ["Zhang", "Guo-Qiang", ""], ["Chen", "Jin", ""]]}, {"id": "2004.01314", "submitter": "Wang Zhao", "authors": "Wang Zhao, Shaohui Liu, Yezhi Shu, Yong-Jin Liu", "title": "Towards Better Generalization: Joint Depth-Pose Learning without PoseNet", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the essential problem of scale inconsistency for\nself-supervised joint depth-pose learning. Most existing methods assume that a\nconsistent scale of depth and pose can be learned across all input samples,\nwhich makes the learning problem harder, resulting in degraded performance and\nlimited generalization in indoor environments and long-sequence visual odometry\napplication. To address this issue, we propose a novel system that explicitly\ndisentangles scale from the network estimation. Instead of relying on PoseNet\narchitecture, our method recovers relative pose by directly solving fundamental\nmatrix from dense optical flow correspondence and makes use of a two-view\ntriangulation module to recover an up-to-scale 3D structure. Then, we align the\nscale of the depth prediction with the triangulated point cloud and use the\ntransformed depth map for depth error computation and dense reprojection check.\nOur whole system can be jointly trained end-to-end. Extensive experiments show\nthat our system not only reaches state-of-the-art performance on KITTI depth\nand flow estimation, but also significantly improves the generalization ability\nof existing self-supervised depth-pose learning methods under a variety of\nchallenging scenarios, and achieves state-of-the-art results among\nself-supervised learning-based methods on KITTI Odometry and NYUv2 dataset.\nFurthermore, we present some interesting findings on the limitation of\nPoseNet-based relative pose estimation methods in terms of generalization\nability. Code is available at https://github.com/B1ueber2y/TrianFlow.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 00:28:09 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zhao", "Wang", ""], ["Liu", "Shaohui", ""], ["Shu", "Yezhi", ""], ["Liu", "Yong-Jin", ""]]}, {"id": "2004.01316", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Kristen Grauman", "title": "From Paris to Berlin: Discovering Fashion Style Influences Around the\n  World", "comments": "CVPR 2020. Project page:\n  https://www.cs.utexas.edu/~ziad/fashion_influence.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of clothing styles and their migration across the world is\nintriguing, yet difficult to describe quantitatively. We propose to discover\nand quantify fashion influences from everyday images of people wearing clothes.\nWe introduce an approach that detects which cities influence which other cities\nin terms of propagating their styles. We then leverage the discovered influence\npatterns to inform a forecasting model that predicts the popularity of any\ngiven style at any given city into the future. Demonstrating our idea with\nGeoStyle---a large-scale dataset of 7.7M images covering 44 major world cities,\nwe present the discovered influence relationships, revealing how cities exert\nand receive fashion influence for an array of 50 observed visual styles.\nFurthermore, the proposed forecasting model achieves state-of-the-art results\nfor a challenging style forecasting task, showing the advantage of grounding\nvisual style evolution both spatially and temporally.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 00:54:23 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 02:34:18 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2004.01317", "submitter": "Ricardo Batista Das Neves Junior", "authors": "Ricardo Batista das Neves Junior, Luiz Felipe Ver\\c{c}osa, David\n  Mac\\^edo, Byron Leite Dantas Bezerra, Cleber Zanchettin", "title": "A Fast Fully Octave Convolutional Neural Network for Document Image\n  Segmentation", "comments": "This paper was accepted for IJCNN 2020 Conference", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9206711", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwide\npractices to online customer identification based on personal identification\ndocuments, similarity and liveness checking, and proof of address. To answer\nthe basic regulation question: are you whom you say you are? The customer needs\nto upload valid identification documents (ID). This task imposes some\ncomputational challenges since these documents are diverse, may present\ndifferent and complex backgrounds, some occlusion, partial rotation, poor\nquality, or damage. Advanced text and document segmentation algorithms were\nused to process the ID images. In this context, we investigated a method based\non U-Net to detect the document edges and text regions in ID images. Besides\nthe promising results on image segmentation, the U-Net based approach is\ncomputationally expensive for a real application, since the image segmentation\nis a customer device task. We propose a model optimization based on Octave\nConvolutions to qualify the method to situations where storage, processing, and\ntime resources are limited, such as in mobile and robotic applications. We\nconducted the evaluation experiments in two new datasets CDPhotoDataset and\nDTDDataset, which are composed of real ID images of Brazilian documents. Our\nresults showed that the proposed models are efficient to document segmentation\ntasks and portable.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 00:57:33 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Junior", "Ricardo Batista das Neves", ""], ["Ver\u00e7osa", "Luiz Felipe", ""], ["Mac\u00eado", "David", ""], ["Bezerra", "Byron Leite Dantas", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "2004.01347", "submitter": "Bo Peng", "authors": "Bo Peng, Wei Wang, Jing Dong and Tieniu Tan", "title": "Learning Pose-invariant 3D Object Reconstruction from Single-view Images", "comments": "under review, code available at https://github.com/bomb2peng/learn3D", "journal-ref": "Neurocomputing Volume 423, 29 January 2021, Pages 407-418", "doi": "10.1016/j.neucom.2020.10.089", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to reconstruct 3D shapes using 2D images is an active research\ntopic, with benefits of not requiring expensive 3D data. However, most work in\nthis direction requires multi-view images for each object instance as training\nsupervision, which oftentimes does not apply in practice. In this paper, we\nrelax the common multi-view assumption and explore a more challenging yet more\nrealistic setup of learning 3D shape from only single-view images. The major\ndifficulty lies in insufficient constraints that can be provided by single view\nimages, which leads to the problem of pose entanglement in learned shape space.\nAs a result, reconstructed shapes vary along input pose and have poor accuracy.\nWe address this problem by taking a novel domain adaptation perspective, and\npropose an effective adversarial domain confusion method to learn\npose-disentangled compact shape space. Experiments on single-view\nreconstruction show effectiveness in solving pose entanglement, and the\nproposed method achieves on-par reconstruction accuracy with state-of-the-art\nwith higher efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 02:47:35 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 08:11:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Tan", "Tieniu", ""]]}, {"id": "2004.01351", "submitter": "Younkwan Lee", "authors": "Younkwan Lee, Jihyo Jeon, Jongmin Yu, Moongu Jeon", "title": "Context-Aware Multi-Task Learning for Traffic Scene Recognition in\n  Autonomous Vehicles", "comments": "Accepted to IV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic scene recognition, which requires various visual classification\ntasks, is a critical ingredient in autonomous vehicles. However, most existing\napproaches treat each relevant task independently from one another, never\nconsidering the entire system as a whole. Because of this, they are limited to\nutilizing a task-specific set of features for all possible tasks of\ninference-time, which ignores the capability to leverage common task-invariant\ncontextual knowledge for the task at hand. To address this problem, we propose\nan algorithm to jointly learn the task-specific and shared representations by\nadopting a multi-task learning network. Specifically, we present a lower bound\nfor the mutual information constraint between shared feature embedding and\ninput that is considered to be able to extract common contextual information\nacross tasks while preserving essential information of each task jointly. The\nlearned representations capture richer contextual information without\nadditional task-specific network. Extensive experiments on the large-scale\ndataset HSD demonstrate the effectiveness and superiority of our network over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 03:09:26 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Lee", "Younkwan", ""], ["Jeon", "Jihyo", ""], ["Yu", "Jongmin", ""], ["Jeon", "Moongu", ""]]}, {"id": "2004.01354", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi and Michael S. Brown", "title": "Deep White-Balance Editing", "comments": "Accepted as Oral at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning approach to realistically edit an sRGB image's\nwhite balance. Cameras capture sensor images that are rendered by their\nintegrated signal processor (ISP) to a standard RGB (sRGB) color space\nencoding. The ISP rendering begins with a white-balance procedure that is used\nto remove the color cast of the scene's illumination. The ISP then applies a\nseries of nonlinear color manipulations to enhance the visual quality of the\nfinal sRGB image. Recent work by [3] showed that sRGB images that were rendered\nwith the incorrect white balance cannot be easily corrected due to the ISP's\nnonlinear rendering. The work in [3] proposed a k-nearest neighbor (KNN)\nsolution based on tens of thousands of image pairs. We propose to solve this\nproblem with a deep neural network (DNN) architecture trained in an end-to-end\nmanner to learn the correct white balance. Our DNN maps an input image to two\nadditional white-balance settings corresponding to indoor and outdoor\nilluminations. Our solution not only is more accurate than the KNN approach in\nterms of correcting a wrong white-balance setting but also provides the user\nthe freedom to edit the white balance in the sRGB image to other illumination\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 03:18:42 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Brown", "Michael S.", ""]]}, {"id": "2004.01355", "submitter": "Vishnu Suresh Lokhande", "authors": "Vishnu Suresh Lokhande, Aditya Kumar Akash, Sathya N. Ravi and Vikas\n  Singh", "title": "FairALM: Augmented Lagrangian Method for Training Fair Models with\n  Little Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic decision making based on computer vision and machine learning\ntechnologies continue to permeate our lives. But issues related to biases of\nthese models and the extent to which they treat certain segments of the\npopulation unfairly, have led to concern in the general public. It is now\naccepted that because of biases in the datasets we present to the models, a\nfairness-oblivious training will lead to unfair models. An interesting topic is\nthe study of mechanisms via which the de novo design or training of the model\ncan be informed by fairness measures. Here, we study mechanisms that impose\nfairness concurrently while training the model. While existing fairness based\napproaches in vision have largely relied on training adversarial modules\ntogether with the primary classification/regression task, in an effort to\nremove the influence of the protected attribute or variable, we show how ideas\nbased on well-known optimization concepts can provide a simpler alternative. In\nour proposed scheme, imposing fairness just requires specifying the protected\nattribute and utilizing our optimization routine. We provide a detailed\ntechnical analysis and present experiments demonstrating that various fairness\nmeasures from the literature can be reliably imposed on a number of training\ntasks in vision in a manner that is interpretable.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 03:18:53 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 00:17:37 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Lokhande", "Vishnu Suresh", ""], ["Akash", "Aditya Kumar", ""], ["Ravi", "Sathya N.", ""], ["Singh", "Vikas", ""]]}, {"id": "2004.01374", "submitter": "Alexander Carballo", "authors": "Alexander Carballo, Abraham Monrroy, David Wong, Patiphon Narksri,\n  Jacob Lambert, Yuki Kitsukawa, Eijiro Takeuchi, Shinpei Kato, and Kazuya\n  Takeda", "title": "Characterization of Multiple 3D LiDARs for Localization and Mapping\n  using Normal Distributions Transform", "comments": "Submitted to IEEE International Conference on Intelligent\n  Transportation Systems(ITSC) 2020 LIBRE dataset is available at\n  https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a detailed comparison of ten different 3D LiDAR\nsensors, covering a range of manufacturers, models, and laser configurations,\nfor the tasks of mapping and vehicle localization, using as common reference\nthe Normal Distributions Transform (NDT) algorithm implemented in the\nself-driving open source platform Autoware. LiDAR data used in this study is a\nsubset of our LiDAR Benchmarking and Reference (LIBRE) dataset, captured\nindependently from each sensor, from a vehicle driven on public urban roads\nmultiple times, at different times of the day. In this study, we analyze the\nperformance and characteristics of each LiDAR for the tasks of (1) 3D mapping\nincluding an assessment map quality based on mean map entropy, and (2) 6-DOF\nlocalization using a ground truth reference map.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 05:05:36 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Carballo", "Alexander", ""], ["Monrroy", "Abraham", ""], ["Wong", "David", ""], ["Narksri", "Patiphon", ""], ["Lambert", "Jacob", ""], ["Kitsukawa", "Yuki", ""], ["Takeuchi", "Eijiro", ""], ["Kato", "Shinpei", ""], ["Takeda", "Kazuya", ""]]}, {"id": "2004.01377", "submitter": "Da Li", "authors": "Da Li, Yongxin Yang, Yi-Zhe Song and Timothy Hospedales", "title": "Sequential Learning for Domain Generalization", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a sequential learning framework for Domain\nGeneralization (DG), the problem of training a model that is robust to domain\nshift by design. Various DG approaches have been proposed with different\nmotivating intuitions, but they typically optimize for a single step of domain\ngeneralization -- training on one set of domains and generalizing to one other.\nOur sequential learning is inspired by the idea lifelong learning, where\naccumulated experience means that learning the $n^{th}$ thing becomes easier\nthan the $1^{st}$ thing. In DG this means encountering a sequence of domains\nand at each step training to maximise performance on the next domain. The\nperformance at domain $n$ then depends on the previous $n-1$ learning problems.\nThus backpropagating through the sequence means optimizing performance not just\nfor the next domain, but all following domains. Training on all such sequences\nof domains provides dramatically more `practice' for a base DG learner compared\nto existing approaches, thus improving performance on a true testing domain.\nThis strategy can be instantiated for different base DG algorithms, but we\nfocus on its application to the recently proposed Meta-Learning Domain\ngeneralization (MLDG). We show that for MLDG it leads to a simple to implement\nand fast algorithm that provides consistent performance improvement on a\nvariety of DG benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 05:10:33 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Li", "Da", ""], ["Yang", "Yongxin", ""], ["Song", "Yi-Zhe", ""], ["Hospedales", "Timothy", ""]]}, {"id": "2004.01382", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei,\n  Kamal Nasrollahi, Thomas B. Moeslund", "title": "Effective Fusion of Deep Multitasking Representations for Robust Visual\n  Tracking", "comments": "2020 Springer. Personal use of this material is permitted. Permission\n  from Springer must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking remains an active research field in computer vision\ndue to persisting challenges with various problem-specific factors in\nreal-world scenes. Many existing tracking methods based on discriminative\ncorrelation filters (DCFs) employ feature extraction networks (FENs) to model\nthe target appearance during the learning process. However, using deep feature\nmaps extracted from FENs based on different residual neural networks (ResNets)\nhas not previously been investigated. This paper aims to evaluate the\nperformance of twelve state-of-the-art ResNet-based FENs in a DCF-based\nframework to determine the best for visual tracking purposes. First, it ranks\ntheir best feature maps and explores the generalized adoption of the best\nResNet-based FEN into another DCF-based method. Then, the proposed method\nextracts deep semantic information from a fully convolutional FEN and fuses it\nwith the best ResNet-based feature maps to strengthen the target representation\nin the learning process of continuous convolution filters. Finally, it\nintroduces a new and efficient semantic weighting method (using semantic\nsegmentation feature maps on each video frame) to reduce the drift problem.\nExtensive experimental results on the well-known OTB-2013, OTB-2015, TC-128 and\nVOT-2018 visual tracking datasets demonstrate that the proposed method\neffectively outperforms state-of-the-art methods in terms of precision and\nrobustness of visual tracking.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 05:33:59 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""], ["Nasrollahi", "Kamal", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2004.01389", "submitter": "Junbo Yin", "authors": "Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, Ruigang Yang", "title": "LiDAR-based Online 3D Video Object Detection with Graph-based Message\n  Passing and Spatiotemporal Transformer Attention", "comments": "Accepted to CVPR 2020. Code: https://github.com/yinjunbo/3DVID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing LiDAR-based 3D object detectors usually focus on the single-frame\ndetection, while ignoring the spatiotemporal information in consecutive point\ncloud frames. In this paper, we propose an end-to-end online 3D video object\ndetector that operates on point cloud sequences. The proposed model comprises a\nspatial feature encoding component and a spatiotemporal feature aggregation\ncomponent. In the former component, a novel Pillar Message Passing Network\n(PMPNet) is proposed to encode each discrete point cloud frame. It adaptively\ncollects information for a pillar node from its neighbors by iterative message\npassing, which effectively enlarges the receptive field of the pillar feature.\nIn the latter component, we propose an Attentive Spatiotemporal Transformer GRU\n(AST-GRU) to aggregate the spatiotemporal information, which enhances the\nconventional ConvGRU with an attentive memory gating mechanism. AST-GRU\ncontains a Spatial Transformer Attention (STA) module and a Temporal\nTransformer Attention (TTA) module, which can emphasize the foreground objects\nand align the dynamic objects, respectively. Experimental results demonstrate\nthat the proposed 3D video object detector achieves state-of-the-art\nperformance on the large-scale nuScenes benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 06:06:52 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yin", "Junbo", ""], ["Shen", "Jianbing", ""], ["Guan", "Chenye", ""], ["Zhou", "Dingfu", ""], ["Yang", "Ruigang", ""]]}, {"id": "2004.01397", "submitter": "Yinghuan Shi", "authors": "Qian Yu, Yinghuan Shi, Yefeng Zheng, Yang Gao, Jianbing Zhu, Yakang\n  Dai", "title": "Crossover-Net: Leveraging the Vertical-Horizontal Crossover Relation for\n  Robust Segmentation", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust segmentation for non-elongated tissues in medical images is hard to\nrealize due to the large variation of the shape, size, and appearance of these\ntissues in different patients. In this paper, we present an end-to-end\ntrainable deep segmentation model termed Crossover-Net for robust segmentation\nin medical images. Our proposed model is inspired by an insightful observation:\nduring segmentation, the representation from the horizontal and vertical\ndirections can provide different local appearance and orthogonality context\ninformation, which helps enhance the discrimination between different tissues\nby simultaneously learning from these two directions. Specifically, by\nconverting the segmentation task to a pixel/voxel-wise prediction problem,\nfirstly, we originally propose a cross-shaped patch, namely crossover-patch,\nwhich consists of a pair of (orthogonal and overlapped) vertical and horizontal\npatches, to capture the orthogonal vertical and horizontal relation. Then, we\ndevelop the Crossover-Net to learn the vertical-horizontal crossover relation\ncaptured by our crossover-patches. To achieve this goal, for learning the\nrepresentation on a typical crossover-patch, we design a novel loss function to\n(1) impose the consistency on the overlap region of the vertical and horizontal\npatches and (2) preserve the diversity on their non-overlap regions. We have\nextensively evaluated our method on CT kidney tumor, MR cardiac, and X-ray\nbreast mass segmentation tasks. Promising results are achieved according to our\nextensive evaluation and comparison with the state-of-the-art segmentation\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 06:49:31 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yu", "Qian", ""], ["Shi", "Yinghuan", ""], ["Zheng", "Yefeng", ""], ["Gao", "Yang", ""], ["Zhu", "Jianbing", ""], ["Dai", "Yakang", ""]]}, {"id": "2004.01398", "submitter": "Yan Li", "authors": "Yan Li and Bin Ji and Xintian Shi and Jianguo Zhang and Bin Kang and\n  Limin Wang", "title": "TEA: Temporal Excitation and Aggregation for Action Recognition", "comments": "CVPR2020. The code is available at\n  https://github.com/Phoenix1327/tea-action-recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal modeling is key for action recognition in videos. It normally\nconsiders both short-range motions and long-range aggregations. In this paper,\nwe propose a Temporal Excitation and Aggregation (TEA) block, including a\nmotion excitation (ME) module and a multiple temporal aggregation (MTA) module,\nspecifically designed to capture both short- and long-range temporal evolution.\nIn particular, for short-range motion modeling, the ME module calculates the\nfeature-level temporal differences from spatiotemporal features. It then\nutilizes the differences to excite the motion-sensitive channels of the\nfeatures. The long-range temporal aggregations in previous works are typically\nachieved by stacking a large number of local temporal convolutions. Each\nconvolution processes a local temporal window at a time. In contrast, the MTA\nmodule proposes to deform the local convolution to a group of sub-convolutions,\nforming a hierarchical residual architecture. Without introducing additional\nparameters, the features will be processed with a series of sub-convolutions,\nand each frame could complete multiple temporal aggregations with\nneighborhoods. The final equivalent receptive field of temporal dimension is\naccordingly enlarged, which is capable of modeling the long-range temporal\nrelationship over distant frames. The two components of the TEA block are\ncomplementary in temporal modeling. Finally, our approach achieves impressive\nresults at low FLOPs on several action recognition benchmarks, such as\nKinetics, Something-Something, HMDB51, and UCF101, which confirms its\neffectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 06:53:30 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Li", "Yan", ""], ["Ji", "Bin", ""], ["Shi", "Xintian", ""], ["Zhang", "Jianguo", ""], ["Kang", "Bin", ""], ["Wang", "Limin", ""]]}, {"id": "2004.01418", "submitter": "Pawel Drozdowski", "authors": "P. Drozdowski, B. Prommegger, G. Wimmer, R. Schraml, C. Rathgeb, A.\n  Uhl, C. Busch", "title": "Demographic Bias: A Challenge for Fingervein Recognition Systems?", "comments": "5 pages, 2 figures, 8 tables. Submitted to European Signal Processing\n  Conference (EUSIPCO) -- special session on bias in biometrics", "journal-ref": null, "doi": "10.23919/Eusipco47968.2020.9287722", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, concerns regarding potential biases in the underlying algorithms of\nmany automated systems (including biometrics) have been raised. In this\ncontext, a biased algorithm produces statistically different outcomes for\ndifferent groups of individuals based on certain (often protected by\nanti-discrimination legislation) attributes such as sex and age. While several\npreliminary studies investigating this matter for facial recognition algorithms\ndo exist, said topic has not yet been addressed for vascular biometric\ncharacteristics. Accordingly, in this paper, several popular types of\nrecognition algorithms are benchmarked to ascertain the matter for fingervein\nrecognition. The experimental evaluation suggests lack of bias for the tested\nalgorithms, although future works with larger datasets are needed to validate\nand confirm those preliminary results.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 07:53:11 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Drozdowski", "P.", ""], ["Prommegger", "B.", ""], ["Wimmer", "G.", ""], ["Schraml", "R.", ""], ["Rathgeb", "C.", ""], ["Uhl", "A.", ""], ["Busch", "C.", ""]]}, {"id": "2004.01426", "submitter": "Zunlei Feng", "authors": "Zunlei Feng, Xinchao Wang, Yongming He, Yike Yuan, Xin Gao, Mingli\n  Song", "title": "Disassembling Object Representations without Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new representation-learning task, which we termed\nas disassembling object representations. Given an image featuring multiple\nobjects, the goal of disassembling is to acquire a latent representation, of\nwhich each part corresponds to one category of objects. Disassembling thus\nfinds its application in a wide domain such as image editing and few- or\nzero-shot learning, as it enables category-specific modularity in the learned\nrepresentations. To this end, we propose an unsupervised approach to achieving\ndisassembling, named Unsupervised Disassembling Object Representation (UDOR).\nUDOR follows a double auto-encoder architecture, in which a fuzzy\nclassification and an object-removing operation are imposed. The fuzzy\nclassification constrains each part of the latent representation to encode\nfeatures of up to one object category, while the object-removing, combined with\na generative adversarial network, enforces the modularity of the\nrepresentations and integrity of the reconstructed image. Furthermore, we\ndevise two metrics to respectively measure the modularity of disassembled\nrepresentations and the visual integrity of reconstructed images. Experimental\nresults demonstrate that the proposed UDOR, despited unsupervised, achieves\ntruly encouraging results on par with those of supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 08:23:09 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Feng", "Zunlei", ""], ["Wang", "Xinchao", ""], ["He", "Yongming", ""], ["Yuan", "Yike", ""], ["Gao", "Xin", ""], ["Song", "Mingli", ""]]}, {"id": "2004.01459", "submitter": "Shijie Ai", "authors": "Lili Pan, Shijie Ai, Yazhou Ren and Zenglin Xu", "title": "Self-Paced Deep Regression Forests with Consideration on\n  Underrepresented Examples", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep discriminative models (e.g. deep regression forests, deep neural\ndecision forests) have achieved remarkable success recently to solve problems\nsuch as facial age estimation and head pose estimation. Most existing methods\npursue robust and unbiased solutions either through learning discriminative\nfeatures, or reweighting samples. We argue what is more desirable is learning\ngradually to discriminate like our human beings, and hence we resort to\nself-paced learning (SPL). Then, a natural question arises: can self-paced\nregime lead deep discriminative models to achieve more robust and less biased\nsolutions? To this end, this paper proposes a new deep discriminative\nmodel--self-paced deep regression forests with consideration on\nunderrepresented examples (SPUDRFs). It tackles the fundamental ranking and\nselecting problem in SPL from a new perspective: fairness. This paradigm is\nfundamental and could be easily combined with a variety of deep discriminative\nmodels (DDMs). Extensive experiments on two computer vision tasks, i.e., facial\nage estimation and head pose estimation, demonstrate the efficacy of SPUDRFs,\nwhere state-of-the-art performances are achieved.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 10:18:05 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:50:39 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 09:54:55 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 01:06:32 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Pan", "Lili", ""], ["Ai", "Shijie", ""], ["Ren", "Yazhou", ""], ["Xu", "Zenglin", ""]]}, {"id": "2004.01461", "submitter": "Hongwei Yong", "authors": "Hongwei Yong, Jianqiang Huang, Xiansheng Hua and Lei Zhang", "title": "Gradient Centralization: A New Optimization Technique for Deep Neural\n  Networks", "comments": "20 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization techniques are of great importance to effectively and\nefficiently train a deep neural network (DNN). It has been shown that using the\nfirst and second order statistics (e.g., mean and variance) to perform Z-score\nstandardization on network activations or weight vectors, such as batch\nnormalization (BN) and weight standardization (WS), can improve the training\nperformance. Different from these existing methods that mostly operate on\nactivations or weights, we present a new optimization technique, namely\ngradient centralization (GC), which operates directly on gradients by\ncentralizing the gradient vectors to have zero mean. GC can be viewed as a\nprojected gradient descent method with a constrained loss function. We show\nthat GC can regularize both the weight space and output feature space so that\nit can boost the generalization performance of DNNs. Moreover, GC improves the\nLipschitzness of the loss function and its gradient so that the training\nprocess becomes more efficient and stable. GC is very simple to implement and\ncan be easily embedded into existing gradient based DNN optimizers with only\none line of code. It can also be directly used to fine-tune the pre-trained\nDNNs. Our experiments on various applications, including general image\nclassification, fine-grained image classification, detection and segmentation,\ndemonstrate that GC can consistently improve the performance of DNN learning.\nThe code of GC can be found at\nhttps://github.com/Yonghongwei/Gradient-Centralization.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 10:25:00 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:40:44 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Yong", "Hongwei", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xiansheng", ""], ["Zhang", "Lei", ""]]}, {"id": "2004.01486", "submitter": "Tim Scherr", "authors": "Tim Scherr, Katharina L\\\"offler, Moritz B\\\"ohland, Ralf Mikut", "title": "Cell Segmentation and Tracking using CNN-Based Distance Predictions and\n  a Graph-Based Matching Strategy", "comments": "25 pages, 14 figures, methods of the team KIT-Sch-GE for the IEEE\n  ISBI 2020 Cell Tracking Challenge", "journal-ref": "PLoS ONE (2020): e0243219", "doi": "10.1371/journal.pone.0243219", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate segmentation and tracking of cells in microscopy image sequences\nis an important task in biomedical research, e.g., for studying the development\nof tissues, organs or entire organisms. However, the segmentation of touching\ncells in images with a low signal-to-noise-ratio is still a challenging\nproblem. In this paper, we present a method for the segmentation of touching\ncells in microscopy images. By using a novel representation of cell borders,\ninspired by distance maps, our method is capable to utilize not only touching\ncells but also close cells in the training process. Furthermore, this\nrepresentation is notably robust to annotation errors and shows promising\nresults for the segmentation of microscopy images containing in the training\ndata underrepresented or not included cell types. For the prediction of the\nproposed neighbor distances, an adapted U-Net convolutional neural network\n(CNN) with two decoder paths is used. In addition, we adapt a graph-based cell\ntracking algorithm to evaluate our proposed method on the task of cell\ntracking. The adapted tracking algorithm includes a movement estimation in the\ncost function to re-link tracks with missing segmentation masks over a short\nsequence of frames. Our combined tracking by detection method has proven its\npotential in the IEEE ISBI 2020 Cell Tracking Challenge\n(http://celltrackingchallenge.net/) where we achieved as team KIT-Sch-GE\nmultiple top three rankings including two top performances using a single\nsegmentation model for the diverse data sets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 11:55:28 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 13:22:34 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 14:16:33 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 14:51:01 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Scherr", "Tim", ""], ["L\u00f6ffler", "Katharina", ""], ["B\u00f6hland", "Moritz", ""], ["Mikut", "Ralf", ""]]}, {"id": "2004.01494", "submitter": "Gurkirt Singh", "authors": "Suman Saha, Gurkirt Singh and Fabio Cuzzolin", "title": "Two-Stream AMTnet for Action Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Two-Stream AMTnet, which leverages recent advances\nin video-based action representation[1] and incremental action tube\ngeneration[2]. Majority of the present action detectors follow a frame-based\nrepresentation, a late-fusion followed by an offline action tube building\nsteps. These are sub-optimal as: frame-based features barely encode the\ntemporal relations; late-fusion restricts the network to learn robust\nspatiotemporal features; and finally, an offline action tube generation is not\nsuitable for many real-world problems such as autonomous driving, human-robot\ninteraction to name a few. The key contributions of this work are: (1)\ncombining AMTnet's 3D proposal architecture with an online action tube\ngeneration technique which allows the model to learn stronger temporal features\nneeded for accurate action detection and facilitates running inference online;\n(2) an efficient fusion technique allowing the deep network to learn strong\nspatiotemporal action representations. This is achieved by augmenting the\nprevious Action Micro-Tube (AMTnet) action detection framework in three\ndistinct ways: by adding a parallel motion stIn this paper, we propose a new\ndeep neural network architecture for online action detection, termed ream to\nthe original appearance one in AMTnet; (2) in opposition to state-of-the-art\naction detectors which train appearance and motion streams separately, and use\na test time late fusion scheme to fuse RGB and flow cues, by jointly training\nboth streams in an end-to-end fashion and merging RGB and optical flow features\nat training time; (3) by introducing an online action tube generation algorithm\nwhich works at video-level, and in real-time (when exploiting only appearance\nfeatures). Two-Stream AMTnet exhibits superior action detection performance\nover state-of-the-art approaches on the standard action detection benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 12:16:45 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Saha", "Suman", ""], ["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2004.01526", "submitter": "Xi Shen", "authors": "Xi Shen, Fran\\c{c}ois Darmon, Alexei A. Efros, Mathieu Aubry", "title": "RANSAC-Flow: generic two-stage image alignment", "comments": "Accepted to ECCV 2020 as a spotlight. Project page:\n  http://imagine.enpc.fr/~shenx/RANSAC-Flow/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the generic problem of dense alignment between two\nimages, whether they be two frames of a video, two widely different views of a\nscene, two paintings depicting similar content, etc. Whereas each such task is\ntypically addressed with a domain-specific solution, we show that a simple\nunsupervised approach performs surprisingly well across a range of tasks. Our\nmain insight is that parametric and non-parametric alignment methods have\ncomplementary strengths. We propose a two-stage process: first, a feature-based\nparametric coarse alignment using one or more homographies, followed by\nnon-parametric fine pixel-wise alignment. Coarse alignment is performed using\nRANSAC on off-the-shelf deep features. Fine alignment is learned in an\nunsupervised way by a deep network which optimizes a standard structural\nsimilarity metric (SSIM) between the two images, plus cycle-consistency.\nDespite its simplicity, our method shows competitive results on a range of\ntasks and datasets, including unsupervised optical flow on KITTI, dense\ncorrespondences on Hpatches, two-view geometry estimation on YFCC100M,\nlocalization on Aachen Day-Night, and, for the first time, fine alignment of\nartworks on the Brughel dataset. Our code and data are available at\nhttp://imagine.enpc.fr/~shenx/RANSAC-Flow/\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 12:37:58 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 13:51:18 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Shen", "Xi", ""], ["Darmon", "Fran\u00e7ois", ""], ["Efros", "Alexei A.", ""], ["Aubry", "Mathieu", ""]]}, {"id": "2004.01536", "submitter": "Ylva Jansson", "authors": "Ylva Jansson and Tony Lindeberg", "title": "Exploring the ability of CNNs to generalise to previously unseen scales\n  over wide scale ranges", "comments": "14 pages, 6 figures, 3 tables", "journal-ref": "Shortened version in International Conference on Pattern\n  Recognition (ICPR 2020), pages 1181-1188, Jan 2021", "doi": "10.1109/ICPR48806.2021.9413276", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to handle large scale variations is crucial for many real world\nvisual tasks. A straightforward approach for handling scale in a deep network\nis to process an image at several scales simultaneously in a set of scale\nchannels. Scale invariance can then, in principle, be achieved by using weight\nsharing between the scale channels together with max or average pooling over\nthe outputs from the scale channels. The ability of such scale channel networks\nto generalise to scales not present in the training set over significant scale\nranges has, however, not previously been explored. We, therefore, present a\ntheoretical analysis of invariance and covariance properties of scale channel\nnetworks and perform an experimental evaluation of the ability of different\ntypes of scale channel networks to generalise to previously unseen scales. We\nidentify limitations of previous approaches and propose a new type of foveated\nscale channel architecture, where the scale channels process increasingly\nlarger parts of the image with decreasing resolution. Our proposed FovMax and\nFovAvg networks perform almost identically over a scale range of 8, also when\ntraining on single scale training data, and do also give improvements in the\nsmall sample regime.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:00:35 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 19:04:47 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 10:14:16 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 13:06:41 GMT"}, {"version": "v5", "created": "Tue, 15 Sep 2020 12:32:33 GMT"}, {"version": "v6", "created": "Mon, 12 Apr 2021 09:07:54 GMT"}, {"version": "v7", "created": "Tue, 18 May 2021 09:27:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jansson", "Ylva", ""], ["Lindeberg", "Tony", ""]]}, {"id": "2004.01547", "submitter": "Changqian Yu", "authors": "Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, Nong\n  Sang", "title": "Context Prior for Scene Segmentation", "comments": "Accepted to CVPR 2020. Code is available at\n  https://git.io/ContextPrior", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have widely explored the contextual dependencies to achieve more\naccurate segmentation results. However, most approaches rarely distinguish\ndifferent types of contextual dependencies, which may pollute the scene\nunderstanding. In this work, we directly supervise the feature aggregation to\ndistinguish the intra-class and inter-class context clearly. Specifically, we\ndevelop a Context Prior with the supervision of the Affinity Loss. Given an\ninput image and corresponding ground truth, Affinity Loss constructs an ideal\naffinity map to supervise the learning of Context Prior. The learned Context\nPrior extracts the pixels belonging to the same category, while the reversed\nprior focuses on the pixels of different classes. Embedded into a conventional\ndeep CNN, the proposed Context Prior Layer can selectively capture the\nintra-class and inter-class contextual dependencies, leading to robust feature\nrepresentation. To validate the effectiveness, we design an effective Context\nPrior Network (CPNet). Extensive quantitative and qualitative evaluations\ndemonstrate that the proposed model performs favorably against state-of-the-art\nsemantic segmentation approaches. More specifically, our algorithm achieves\n46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on\nCityscapes. Code is available at https://git.io/ContextPrior.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:16:32 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yu", "Changqian", ""], ["Wang", "Jingbo", ""], ["Gao", "Changxin", ""], ["Yu", "Gang", ""], ["Shen", "Chunhua", ""], ["Sang", "Nong", ""]]}, {"id": "2004.01551", "submitter": "Niladri Puhan", "authors": "Kalyan S Dash, N B Puhan, G Panda", "title": "Sparse Concept Coded Tetrolet Transform for Unconstrained Odia Character\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature representation in the form of spatio-spectral decomposition is one of\nthe robust techniques adopted in automatic handwritten character recognition\nsystems. In this regard, we propose a new image representation approach for\nunconstrained handwritten alphanumeric characters using sparse concept coded\nTetrolets. Tetrolets, which does not use fixed dyadic square blocks for\nspectral decomposition like conventional wavelets, preserve the localized\nvariations in handwritings by adopting tetrominoes those capture the shape\ngeometry. The sparse concept coding of low entropy Tetrolet representation is\nfound to extract the important hidden information (concept) for superior\npattern discrimination. Large scale experimentation using ten databases in six\ndifferent scripts (Bangla, Devanagari, Odia, English, Arabic and Telugu) has\nbeen performed. The proposed feature representation along with standard\nclassifiers such as random forest, support vector machine (SVM), nearest\nneighbor and modified quadratic discriminant function (MQDF) is found to\nachieve state-of-the-art recognition performance in all the databases, viz.\n99.40% (MNIST); 98.72% and 93.24% (IITBBS); 99.38% and 99.22% (ISI Kolkata).\nThe proposed OCR system is shown to perform better than other sparse based\ntechniques such as PCA, SparsePCA and SparseLDA, as well as better than\nexisting transforms (Wavelet, Slantlet and Stockwell).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:20:12 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Dash", "Kalyan S", ""], ["Puhan", "N B", ""], ["Panda", "G", ""]]}, {"id": "2004.01573", "submitter": "Sina Mohammadi", "authors": "Mehrdad Noori, Sina Mohammadi, Sina Ghofrani Majelan, Ali Bahri,\n  Mohammad Havaei", "title": "DFNet: Discriminative feature extraction and integration network for\n  salient object detection", "comments": "Accepted by Engineering Applications of Artificial Intelligence. 22\n  pages, 8 figures", "journal-ref": "Engineering Applications of Artificial Intelligence, Volume 89,\n  2020, 103419, ISSN 0952-1976", "doi": "10.1016/j.engappai.2019.103419", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the powerful feature extraction capability of Convolutional Neural\nNetworks, there are still some challenges in saliency detection. In this paper,\nwe focus on two aspects of challenges: i) Since salient objects appear in\nvarious sizes, using single-scale convolution would not capture the right size.\nMoreover, using multi-scale convolutions without considering their importance\nmay confuse the model. ii) Employing multi-level features helps the model use\nboth local and global context. However, treating all features equally results\nin information redundancy. Therefore, there needs to be a mechanism to\nintelligently select which features in different levels are useful. To address\nthe first challenge, we propose a Multi-scale Attention Guided Module. This\nmodule not only extracts multi-scale features effectively but also gives more\nattention to more discriminative feature maps corresponding to the scale of the\nsalient object. To address the second challenge, we propose an Attention-based\nMulti-level Integrator Module to give the model the ability to assign different\nweights to multi-level feature maps. Furthermore, our Sharpening Loss function\nguides our network to output saliency maps with higher certainty and less\nblurry salient objects, and it has far better performance than the\nCross-entropy loss. For the first time, we adopt four different backbones to\nshow the generalization of our method. Experiments on five challenging datasets\nprove that our method achieves the state-of-the-art performance. Our approach\nis fast as well and can run at a real-time speed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:56:41 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Noori", "Mehrdad", ""], ["Mohammadi", "Sina", ""], ["Majelan", "Sina Ghofrani", ""], ["Bahri", "Ali", ""], ["Havaei", "Mohammad", ""]]}, {"id": "2004.01582", "submitter": "Qilei Chen", "authors": "Alexander Ding, Qilei Chen, Yu Cao, Benyuan Liu", "title": "Retinopathy of Prematurity Stage Diagnosis Using Object Segmentation and\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinopathy of Prematurity (ROP) is an eye disorder primarily affecting\npremature infants with lower weights. It causes proliferation of vessels in the\nretina and could result in vision loss and, eventually, retinal detachment,\nleading to blindness. While human experts can easily identify severe stages of\nROP, the diagnosis of earlier stages, which are the most relevant to\ndetermining treatment choice, are much more affected by variability in\nsubjective interpretations of human experts. In recent years, there has been a\nsignificant effort to automate the diagnosis using deep learning. This paper\nbuilds upon the success of previous models and develops a novel architecture,\nwhich combines object segmentation and convolutional neural networks (CNN) to\nconstruct an effective classifier of ROP stages 1-3 based on neonatal retinal\nimages. Motivated by the fact that the formation and shape of a demarcation\nline in the retina is the distinguishing feature between earlier ROP stages,\nour proposed system first trains an object segmentation model to identify the\ndemarcation line at a pixel level and adds the resulting mask as an additional\n\"color\" channel in the original image. Then, the system trains a CNN classifier\nbased on the processed images to leverage information from both the original\nimage and the mask, which helps direct the model's attention to the demarcation\nline. In a number of careful experiments comparing its performance to previous\nobject segmentation systems and CNN-only systems trained on our dataset, our\nnovel architecture significantly outperforms previous systems in accuracy,\ndemonstrating the effectiveness of our proposed pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:07:41 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ding", "Alexander", ""], ["Chen", "Qilei", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "2004.01588", "submitter": "Vladislav Golyanik", "authors": "Jameel Malik, Ibrahim Abdelaziz, Ahmed Elhayek, Soshi Shimada, Sk Aziz\n  Ali, Vladislav Golyanik, Christian Theobalt, Didier Stricker", "title": "HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose\n  Estimation from a Single Depth Map", "comments": "10 pages, 8 figures, 5 tables, CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand shape and pose estimation from a single depth map is a new and\nchallenging computer vision problem with many applications. The\nstate-of-the-art methods directly regress 3D hand meshes from 2D depth images\nvia 2D convolutional neural networks, which leads to artefacts in the\nestimations due to perspective distortions in the images. In contrast, we\npropose a novel architecture with 3D convolutions trained in a\nweakly-supervised manner. The input to our method is a 3D voxelized depth map,\nand we rely on two hand shape representations. The first one is the 3D\nvoxelized grid of the shape which is accurate but does not preserve the mesh\ntopology and the number of mesh vertices. The second representation is the 3D\nhand surface which is less accurate but does not suffer from the limitations of\nthe first representation. We combine the advantages of these two\nrepresentations by registering the hand surface to the voxelized hand shape. In\nthe extensive experiments, the proposed approach improves over the state of the\nart by 47.8% on the SynHand5M dataset. Moreover, our augmentation policy for\nvoxelized depth maps further enhances the accuracy of 3D hand pose estimation\non real data. Our method produces visually more reasonable and realistic hand\nshapes on NYU and BigHand2.2M datasets compared to the existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:27:16 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Malik", "Jameel", ""], ["Abdelaziz", "Ibrahim", ""], ["Elhayek", "Ahmed", ""], ["Shimada", "Soshi", ""], ["Ali", "Sk Aziz", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "2004.01589", "submitter": "Kimmo Kartasalo", "authors": "Peter Str\\\"om (1), Kimmo Kartasalo (1,2), Pekka Ruusuvuori (2,3),\n  Henrik Gr\\\"onberg (1,4), Hemamali Samaratunga (5), Brett Delahunt (6),\n  Toyonori Tsuzuki (7), Lars Egevad (8), Martin Eklund (1) ((1) Department of\n  Medical Epidemiology and Biostatistics, Karolinska Institutet, Stockholm,\n  Sweden, (2) Faculty of Medicine and Health Technology, Tampere University,\n  Tampere, Finland, (3) Institute of Biomedicine, University of Turku, Turku,\n  Finland, (4) Department of Oncology, St G\\\"oran Hospital, Stockholm, Sweden,\n  (5) Aquesta Uropathology and University of Queensland, Brisbane, Qld,\n  Australia, (6) Department of Pathology and Molecular Medicine, Wellington\n  School of Medicine and Health Sciences, University of Otago, Wellington, New\n  Zealand, (7) Department of Surgical Pathology, School of Medicine, Aichi\n  Medical University, Nagoya, Japan, (8) Department of Oncology and Pathology,\n  Karolinska Institutet, Stockholm, Sweden)", "title": "Detection of Perineural Invasion in Prostate Needle Biopsies with Deep\n  Neural Networks", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The detection of perineural invasion (PNI) by carcinoma in\nprostate biopsies has been shown to be associated with poor prognosis. The\nassessment and quantification of PNI is; however, labor intensive. In the study\nwe aimed to develop an algorithm based on deep neural networks to aid\npathologists in this task.\n  Methods: We collected, digitized and pixel-wise annotated the PNI findings in\neach of the approximately 80,000 biopsy cores from the 7,406 men who underwent\nbiopsy in the prospective and diagnostic STHLM3 trial between 2012 and 2014. In\ntotal, 485 biopsy cores showed PNI. We also digitized more than 10% (n=8,318)\nof the PNI negative biopsy cores. Digitized biopsies from a random selection of\n80% of the men were used to build deep neural networks, and the remaining 20%\nwere used to evaluate the performance of the algorithm.\n  Results: For the detection of PNI in prostate biopsy cores the network had an\nestimated area under the receiver operating characteristics curve of 0.98 (95%\nCI 0.97-0.99) based on 106 PNI positive cores and 1,652 PNI negative cores in\nthe independent test set. For the pre-specified operating point this translates\nto sensitivity of 0.87 and specificity of 0.97. The corresponding positive and\nnegative predictive values were 0.67 and 0.99, respectively. For localizing the\nregions of PNI within a slide we estimated an average intersection over union\nof 0.50 (CI: 0.46-0.55).\n  Conclusion: We have developed an algorithm based on deep neural networks for\ndetecting PNI in prostate biopsies with apparently acceptable diagnostic\nproperties. These algorithms have the potential to aid pathologists in the\nday-to-day work by drastically reducing the number of biopsy cores that need to\nbe assessed for PNI and by highlighting regions of diagnostic interest.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:27:53 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Str\u00f6m", "Peter", ""], ["Kartasalo", "Kimmo", ""], ["Ruusuvuori", "Pekka", ""], ["Gr\u00f6nberg", "Henrik", ""], ["Samaratunga", "Hemamali", ""], ["Delahunt", "Brett", ""], ["Tsuzuki", "Toyonori", ""], ["Egevad", "Lars", ""], ["Eklund", "Martin", ""]]}, {"id": "2004.01607", "submitter": "Filip Lux", "authors": "Filip Lux, Petr Matula", "title": "Cell Segmentation by Combining Marker-Controlled Watershed and Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a cell segmentation method for analyzing images of densely\nclustered cells. The method combines the strengths of marker-controlled\nwatershed transformation and a convolutional neural network (CNN). We\ndemonstrate the method universality and high performance on three Cell Tracking\nChallenge (CTC) datasets of clustered cells captured by different acquisition\ntechniques. For all tested datasets, our method reached the top performance in\nboth cell detection and segmentation. Based on a series of experiments, we\nobserved: (1) Predicting both watershed marker function and segmentation\nfunction significantly improves the accuracy of the segmentation. (2) Both\nfunctions can be learned independently. (3) Training data augmentation by\nscaling and rigid geometric transformations is superior to augmentation that\ninvolves elastic transformations. Our method is simple to use, and it\ngeneralizes well for various data with state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:51:43 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Lux", "Filip", ""], ["Matula", "Petr", ""]]}, {"id": "2004.01610", "submitter": "David Major", "authors": "David Major, Dimitrios Lenis, Maria Wimmer, Gert Sluiter, Astrid Berg,\n  and Katja B\\\"uhler", "title": "Interpreting Medical Image Classifiers by Optimization Based\n  Counterfactual Impact Analysis", "comments": "Accepted for publication at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical applicability of automated decision support systems depends on a\nrobust, well-understood classification interpretation. Artificial neural\nnetworks while achieving class-leading scores fall short in this regard.\nTherefore, numerous approaches have been proposed that map a salient region of\nan image to a diagnostic classification. Utilizing heuristic methodology, like\nblurring and noise, they tend to produce diffuse, sometimes misleading results,\nhindering their general adoption. In this work we overcome these issues by\npresenting a model agnostic saliency mapping framework tailored to medical\nimaging. We replace heuristic techniques with a strong neighborhood conditioned\ninpainting approach, which avoids anatomically implausible artefacts. We\nformulate saliency attribution as a map-quality optimization task, enforcing\nconstrained and focused attributions. Experiments on public mammography data\nshow quantitatively and qualitatively more precise localization and clearer\nconveying results than existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:59:08 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Major", "David", ""], ["Lenis", "Dimitrios", ""], ["Wimmer", "Maria", ""], ["Sluiter", "Gert", ""], ["Berg", "Astrid", ""], ["B\u00fchler", "Katja", ""]]}, {"id": "2004.01613", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul, Jian Kang, Beg\\\"um Demir", "title": "Deep Learning for Image Search and Retrieval in Large Remote Sensing\n  Archives", "comments": "To appear as a book chapter in \"Deep Learning for the Earth\n  Sciences\", John Wiley & Sons, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter presents recent advances in content based image search and\nretrieval (CBIR) systems in remote sensing (RS) for fast and accurate\ninformation discovery from massive data archives. Initially, we analyze the\nlimitations of the traditional CBIR systems that rely on the hand-crafted RS\nimage descriptors. Then, we focus our attention on the advances in RS CBIR\nsystems for which deep learning (DL) models are at the forefront. In\nparticular, we present the theoretical properties of the most recent DL based\nCBIR systems for the characterization of the complex semantic content of RS\nimages. After discussing their strengths and limitations, we present the deep\nhashing based CBIR systems that have high time-efficient search capability\nwithin huge data archives. Finally, the most promising research directions in\nRS CBIR are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 15:03:41 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 08:52:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Sumbul", "Gencer", ""], ["Kang", "Jian", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "2004.01614", "submitter": "Srinath Jayachandran", "authors": "Srinath Jayachandran, Ashlin Ghosh", "title": "Deep Transfer Learning for Texture Classification in Colorectal Cancer\n  Histology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopic examination of tissues or histopathology is one of the diagnostic\nprocedures for detecting colorectal cancer. The pathologist involved in such an\nexamination usually identifies tissue type based on texture analysis,\nespecially focusing on tumour-stroma ratio. In this work, we automate the task\nof tissue classification within colorectal cancer histology samples using deep\ntransfer learning. We use discriminative fine-tuning with one-cycle-policy and\napply structure-preserving colour normalization to boost our results. We also\nprovide visual explanations of the deep neural network's decision on texture\nclassification. With achieving state-of-the-art test accuracy of 96.2% we also\nembark on using deployment friendly architecture called SqueezeNet for\nmemory-limited hardware.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 15:05:36 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Jayachandran", "Srinath", ""], ["Ghosh", "Ashlin", ""]]}, {"id": "2004.01643", "submitter": "Martin Hahner", "authors": "Martin Hahner, Dengxin Dai, Alexander Liniger, and Luc Van Gool", "title": "Quantifying Data Augmentation for LiDAR based 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we shed light on different data augmentation techniques\ncommonly used in Light Detection and Ranging (LiDAR) based 3D Object Detection.\nWe, therefore, utilize a state of the art voxel-based 3D Object Detection\npipeline called PointPillars and carry out our experiments on the well\nestablished KITTI dataset. We investigate a variety of global and local\naugmentation techniques, where global augmentation techniques are applied to\nthe entire point cloud of a scene and local augmentation techniques are only\napplied to points belonging to individual objects in the scene. Our findings\nshow that both types of data augmentation can lead to performance increases,\nbut it also turns out, that some augmentation techniques, such as individual\nobject translation, for example, can be counterproductive and can hurt overall\nperformance. We show that when we apply our findings to the data augmentation\npolicy of PointPillars we can easily increase its performance by up to 2%. In\norder to provide reproducibility, our code will be publicly available at\nwww.trace.ethz.ch/3D_Object_Detection.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:09:14 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hahner", "Martin", ""], ["Dai", "Dengxin", ""], ["Liniger", "Alexander", ""], ["Van Gool", "Luc", ""]]}, {"id": "2004.01648", "submitter": "Manikanta Srikar Yellapragada", "authors": "Manikanta Srikar Yellapragada, Yiting Xie, Benedikt Graf, David\n  Richmond, Arun Krishnan, Arkadiusz Sitek", "title": "Deep Learning based detection of Acute Aortic Syndrome in contrast CT\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute aortic syndrome (AAS) is a group of life threatening conditions of the\naorta. We have developed an end-to-end automatic approach to detect AAS in\ncomputed tomography (CT) images. Our approach consists of two steps. At first,\nwe extract N cross sections along the segmented aorta centerline for each CT\nscan. These cross sections are stacked together to form a new volume which is\nthen classified using two different classifiers, a 3D convolutional neural\nnetwork (3D CNN) and a multiple instance learning (MIL). We trained, validated,\nand compared two models on 2291 contrast CT volumes. We tested on a set aside\ncohort of 230 normal and 50 positive CT volumes. Our models detected AAS with\nan Area under Receiver Operating Characteristic curve (AUC) of 0.965 and 0.985\nusing 3DCNN and MIL, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:12:04 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yellapragada", "Manikanta Srikar", ""], ["Xie", "Yiting", ""], ["Graf", "Benedikt", ""], ["Richmond", "David", ""], ["Krishnan", "Arun", ""], ["Sitek", "Arkadiusz", ""]]}, {"id": "2004.01658", "submitter": "Li Jiang", "authors": "Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya\n  Jia", "title": "PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is an important task for scene understanding. Compared\nto the fully-developed 2D, 3D instance segmentation for point clouds have much\nroom to improve. In this paper, we present PointGroup, a new end-to-end\nbottom-up architecture, specifically focused on better grouping the points by\nexploring the void space between objects. We design a two-branch network to\nextract point features and predict semantic labels and offsets, for shifting\neach point towards its respective instance centroid. A clustering component is\nfollowed to utilize both the original and offset-shifted point coordinate sets,\ntaking advantage of their complementary strength. Further, we formulate the\nScoreNet to evaluate the candidate instances, followed by the Non-Maximum\nSuppression (NMS) to remove duplicates. We conduct extensive experiments on two\nchallenging datasets, ScanNet v2 and S3DIS, on which our method achieves the\nhighest performance, 63.6% and 64.0%, compared to 54.9% and 54.4% achieved by\nformer best solutions in terms of mAP with IoU threshold 0.5.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:26:37 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Jiang", "Li", ""], ["Zhao", "Hengshuang", ""], ["Shi", "Shaoshuai", ""], ["Liu", "Shu", ""], ["Fu", "Chi-Wing", ""], ["Jia", "Jiaya", ""]]}, {"id": "2004.01661", "submitter": "Marie-Julie Rakotosaona", "authors": "Marie-Julie Rakotosaona, Maks Ovsjanikov", "title": "Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for interpolating and manipulating 3D\nshapes represented as point clouds, that is explicitly designed to preserve\nintrinsic shape properties. Our approach is based on constructing a dual\nencoding space that enables shape synthesis and, at the same time, provides\nlinks to the intrinsic shape information, which is typically not available on\npoint cloud data. Our method works in a single pass and avoids expensive\noptimization, employed by existing techniques. Furthermore, the strong\nregularization provided by our dual latent space approach also helps to improve\nshape recovery in challenging settings from noisy point clouds across different\ndatasets. Extensive experiments show that our method results in more realistic\nand smoother interpolations compared to baselines.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:28:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rakotosaona", "Marie-Julie", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2004.01673", "submitter": "Hugo Germain", "authors": "Hugo Germain, Guillaume Bourmaud, Vincent Lepetit", "title": "S2DNet: Learning Accurate Correspondences for Sparse-to-Dense Feature\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing robust and accurate correspondences is a fundamental backbone to\nmany computer vision algorithms. While recent learning-based feature matching\nmethods have shown promising results in providing robust correspondences under\nchallenging conditions, they are often limited in terms of precision. In this\npaper, we introduce S2DNet, a novel feature matching pipeline, designed and\ntrained to efficiently establish both robust and accurate correspondences. By\nleveraging a sparse-to-dense matching paradigm, we cast the correspondence\nlearning problem as a supervised classification task to learn to output highly\npeaked correspondence maps. We show that S2DNet achieves state-of-the-art\nresults on the HPatches benchmark, as well as on several long-term visual\nlocalization datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:04:34 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Germain", "Hugo", ""], ["Bourmaud", "Guillaume", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2004.01689", "submitter": "Fernando Cladera Ojeda", "authors": "Anthony Bisulco, Fernando Cladera Ojeda, Volkan Isler, Daniel D. Lee", "title": "Near-chip Dynamic Vision Filtering for Low-Bandwidth Pedestrian\n  Detection", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel end-to-end system for pedestrian detection using\nDynamic Vision Sensors (DVSs). We target applications where multiple sensors\ntransmit data to a local processing unit, which executes a detection algorithm.\nOur system is composed of (i) a near-chip event filter that compresses and\ndenoises the event stream from the DVS, and (ii) a Binary Neural Network (BNN)\ndetection module that runs on a low-computation edge computing device (in our\ncase a STM32F4 microcontroller). We present the system architecture and provide\nan end-to-end implementation for pedestrian detection in an office environment.\nOur implementation reduces transmission size by up to 99.6% compared to\ntransmitting the raw event stream. The average packet size in our system is\nonly 1397 bits, while 307.2 kb are required to send an uncompressed DVS time\nwindow. Our detector is able to perform a detection every 450 ms, with an\noverall testing F1 score of 83%. The low bandwidth and energy properties of our\nsystem make it ideal for IoT applications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:36:26 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Bisulco", "Anthony", ""], ["Ojeda", "Fernando Cladera", ""], ["Isler", "Volkan", ""], ["Lee", "Daniel D.", ""]]}, {"id": "2004.01704", "submitter": "Qiwei Ye", "authors": "Yuxuan Song, Qiwei Ye, Minkai Xu, Tie-Yan Liu", "title": "Discriminator Contrastive Divergence: Semi-Amortized Generative Modeling\n  by Exploring Energy of the Discriminator", "comments": "17 pages, 9 figures, pre-submmited to cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown great promise in modeling\nhigh dimensional data. The learning objective of GANs usually minimizes some\nmeasure discrepancy, \\textit{e.g.}, $f$-divergence~($f$-GANs) or Integral\nProbability Metric~(Wasserstein GANs). With $f$-divergence as the objective\nfunction, the discriminator essentially estimates the density ratio, and the\nestimated ratio proves useful in further improving the sample quality of the\ngenerator. However, how to leverage the information contained in the\ndiscriminator of Wasserstein GANs (WGAN) is less explored. In this paper, we\nintroduce the Discriminator Contrastive Divergence, which is well motivated by\nthe property of WGAN's discriminator and the relationship between WGAN and\nenergy-based model. Compared to standard GANs, where the generator is directly\nutilized to obtain new samples, our method proposes a semi-amortized generation\nprocedure where the samples are produced with the generator's output as an\ninitial state. Then several steps of Langevin dynamics are conducted using the\ngradient of the discriminator. We demonstrate the benefits of significant\nimproved generation on both synthetic data and several real-world image\ngeneration benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 01:50:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Song", "Yuxuan", ""], ["Ye", "Qiwei", ""], ["Xu", "Minkai", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2004.01735", "submitter": "Yuhong Guo", "authors": "Kevin Hua, Yuhong Guo", "title": "Unsupervised Domain Adaptation with Progressive Domain Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to exploit a label-rich source domain for learning\nclassifiers in a different label-scarce target domain. It is particularly\nchallenging when there are significant divergences between the two domains. In\nthe paper, we propose a novel unsupervised domain adaptation method based on\nprogressive domain augmentation. The proposed method generates virtual\nintermediate domains via domain interpolation, progressively augments the\nsource domain and bridges the source-target domain divergence by conducting\nmultiple subspace alignment on the Grassmann manifold. We conduct experiments\non multiple domain adaptation tasks and the results shows the proposed method\nachieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 18:45:39 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 01:45:24 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Hua", "Kevin", ""], ["Guo", "Yuhong", ""]]}, {"id": "2004.01738", "submitter": "Elizabeth Cole", "authors": "Elizabeth K. Cole, Joseph Y. Cheng, John M. Pauly, and Shreyas S.\n  Vasanawala", "title": "Analysis of Deep Complex-Valued Convolutional Neural Networks for MRI\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world signal sources are complex-valued, having real and imaginary\ncomponents. However, the vast majority of existing deep learning platforms and\nnetwork architectures do not support the use of complex-valued data. MRI data\nis inherently complex-valued, so existing approaches discard the richer\nalgebraic structure of the complex data. In this work, we investigate\nend-to-end complex-valued convolutional neural networks - specifically, for\nimage reconstruction in lieu of two-channel real-valued networks. We apply this\nto magnetic resonance imaging reconstruction for the purpose of accelerating\nscan times and determine the performance of various promising complex-valued\nactivation functions. We find that complex-valued CNNs with complex-valued\nconvolutions provide superior reconstructions compared to real-valued\nconvolutions with the same number of trainable parameters, over a variety of\nnetwork architectures and datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 19:00:23 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 00:41:29 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 04:02:38 GMT"}, {"version": "v4", "created": "Tue, 12 May 2020 01:21:36 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Cole", "Elizabeth K.", ""], ["Cheng", "Joseph Y.", ""], ["Pauly", "John M.", ""], ["Vasanawala", "Shreyas S.", ""]]}, {"id": "2004.01743", "submitter": "Zitao Chen", "authors": "Zitao Chen, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik\n  Pattabiraman and Nathan DeBardeleben", "title": "TensorFI: A Flexible Fault Injection Framework for TensorFlow\n  Applications", "comments": "A preliminary version of this work was published in a workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning (ML) has seen increasing adoption in safety-critical\ndomains (e.g., autonomous vehicles), the reliability of ML systems has also\ngrown in importance. While prior studies have proposed techniques to enable\nefficient error-resilience techniques (e.g., selective instruction\nduplication), a fundamental requirement for realizing these techniques is a\ndetailed understanding of the application's resilience.\n  In this work, we present TensorFI, a high-level fault injection (FI)\nframework for TensorFlow-based applications. TensorFI is able to inject both\nhardware and software faults in general TensorFlow programs. TensorFI is a\nconfigurable FI tool that is flexible, easy to use, and portable. It can be\nintegrated into existing TensorFlow programs to assess their resilience for\ndifferent fault types (e.g., faults in particular operators). We use TensorFI\nto evaluate the resilience of 12 ML programs, including DNNs used in the\nautonomous vehicle domain. Our tool is publicly available at\nhttps://github.com/DependableSystemsLab/TensorFI.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 19:26:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chen", "Zitao", ""], ["Narayanan", "Niranjhana", ""], ["Fang", "Bo", ""], ["Li", "Guanpeng", ""], ["Pattabiraman", "Karthik", ""], ["DeBardeleben", "Nathan", ""]]}, {"id": "2004.01792", "submitter": "Aayush Chaudhary", "authors": "Aayush K Chaudhary and Jeff B. Pelz", "title": "Privacy-Preserving Eye Videos using Rubber Sheet Model", "comments": "Will be published in ETRA 20 Short Papers, June 2-5, 2020, Stuttgart,\n  Germany Copyright 2020 Association for Computing Machinery", "journal-ref": null, "doi": "10.1145/3379156.3391375", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based eye trackers estimate gaze based on eye images/videos. As\nsecurity and privacy concerns loom over technological advancements, tackling\nsuch challenges is crucial. We present a new approach to handle privacy issues\nin eye videos by replacing the current identifiable iris texture with a\ndifferent iris template in the video capture pipeline based on the Rubber Sheet\nModel. We extend to image blending and median-value representations to\ndemonstrate that videos can be manipulated without significantly degrading\nsegmentation and pupil detection accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 21:59:38 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chaudhary", "Aayush K", ""], ["Pelz", "Jeff B.", ""]]}, {"id": "2004.01793", "submitter": "Siva Karthik Mustikovela", "authors": "Siva Karthik Mustikovela, Varun Jampani, Shalini De Mello, Sifei Liu,\n  Umar Iqbal, Carsten Rother, Jan Kautz", "title": "Self-Supervised Viewpoint Learning From Image Collections", "comments": "Accepted at CVPR 20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks to estimate the viewpoint of objects requires\nlarge labeled training datasets. However, manually labeling viewpoints is\nnotoriously hard, error-prone, and time-consuming. On the other hand, it is\nrelatively easy to mine many unlabelled images of an object category from the\ninternet, e.g., of cars or faces. We seek to answer the research question of\nwhether such unlabeled collections of in-the-wild images can be successfully\nutilized to train viewpoint estimation networks for general object categories\npurely via self-supervision. Self-supervision here refers to the fact that the\nonly true supervisory signal that the network has is the input image itself. We\npropose a novel learning framework which incorporates an analysis-by-synthesis\nparadigm to reconstruct images in a viewpoint aware manner with a generative\nnetwork, along with symmetry and adversarial constraints to successfully\nsupervise our viewpoint estimation network. We show that our approach performs\ncompetitively to fully-supervised approaches for several object categories like\nhuman faces, cars, buses, and trains. Our work opens up further research in\nself-supervised viewpoint learning and serves as a robust baseline for it. We\nopen-source our code at https://github.com/NVlabs/SSV.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 22:01:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mustikovela", "Siva Karthik", ""], ["Jampani", "Varun", ""], ["De Mello", "Shalini", ""], ["Liu", "Sifei", ""], ["Iqbal", "Umar", ""], ["Rother", "Carsten", ""], ["Kautz", "Jan", ""]]}, {"id": "2004.01800", "submitter": "Ping Hu", "authors": "Ping Hu, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Stan Sclaroff and\n  Federico Perazzi", "title": "Temporally Distributed Networks for Fast Video Semantic Segmentation", "comments": "[CVPR2020] Project: https://github.com/feinanshan/TDNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TDNet, a temporally distributed network designed for fast and\naccurate video semantic segmentation. We observe that features extracted from a\ncertain high-level layer of a deep CNN can be approximated by composing\nfeatures extracted from several shallower sub-networks. Leveraging the inherent\ntemporal continuity in videos, we distribute these sub-networks over sequential\nframes. Therefore, at each time step, we only need to perform a lightweight\ncomputation to extract a sub-features group from a single sub-network. The full\nfeatures used for segmentation are then recomposed by application of a novel\nattention propagation module that compensates for geometry deformation between\nframes. A grouped knowledge distillation loss is also introduced to further\nimprove the representation power at both full and sub-feature levels.\nExperiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method\nachieves state-of-the-art accuracy with significantly faster speed and lower\nlatency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 22:43:32 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 00:44:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hu", "Ping", ""], ["Heilbron", "Fabian Caba", ""], ["Wang", "Oliver", ""], ["Lin", "Zhe", ""], ["Sclaroff", "Stan", ""], ["Perazzi", "Federico", ""]]}, {"id": "2004.01803", "submitter": "Chenfeng Xu", "authors": "Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt\n  Keutzer, Masayoshi Tomizuka", "title": "SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud\n  Segmentation", "comments": "Accepted by ECCV 2020. Code and data are available at:\n  https://github.com/chenfengxu714/SqueezeSegV3.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR point-cloud segmentation is an important problem for many applications.\nFor large-scale point cloud segmentation, the \\textit{de facto} method is to\nproject a 3D point cloud to get a 2D LiDAR image and use convolutions to\nprocess it. Despite the similarity between regular RGB and LiDAR images, we\ndiscover that the feature distribution of LiDAR images changes drastically at\ndifferent image locations. Using standard convolutions to process such LiDAR\nimages is problematic, as convolution filters pick up local features that are\nonly active in specific regions in the image. As a result, the capacity of the\nnetwork is under-utilized and the segmentation performance decreases. To fix\nthis, we propose Spatially-Adaptive Convolution (SAC) to adopt different\nfilters for different locations according to the input image. SAC can be\ncomputed efficiently since it can be implemented as a series of element-wise\nmultiplications, im2col, and standard convolution. It is a general framework\nsuch that several previous methods can be seen as special cases of SAC. Using\nSAC, we build SqueezeSegV3 for LiDAR point-cloud segmentation and outperform\nall previous published methods by at least 3.7% mIoU on the SemanticKITTI\nbenchmark with comparable inference speed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 22:47:56 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 09:42:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Chenfeng", ""], ["Wu", "Bichen", ""], ["Wang", "Zining", ""], ["Zhan", "Wei", ""], ["Vajda", "Peter", ""], ["Keutzer", "Kurt", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2004.01804", "submitter": "Andre Araujo", "authors": "Tobias Weyand, Andre Araujo, Bingyi Cao, Jack Sim", "title": "Google Landmarks Dataset v2 -- A Large-Scale Benchmark for\n  Instance-Level Recognition and Retrieval", "comments": "CVPR20 camera-ready (oral) + appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While image retrieval and instance recognition techniques are progressing\nrapidly, there is a need for challenging datasets to accurately measure their\nperformance -- while posing novel challenges that are relevant for practical\napplications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new\nbenchmark for large-scale, fine-grained instance recognition and image\nretrieval in the domain of human-made and natural landmarks. GLDv2 is the\nlargest such dataset to date by a large margin, including over 5M images and\n200k distinct instance labels. Its test set consists of 118k images with ground\ntruth annotations for both the retrieval and recognition tasks. The ground\ntruth construction involved over 800 hours of human annotator work. Our new\ndataset has several challenging properties inspired by real world applications\nthat previous datasets did not consider: An extremely long-tailed class\ndistribution, a large fraction of out-of-domain test photos and large\nintra-class variability. The dataset is sourced from Wikimedia Commons, the\nworld's largest crowdsourced collection of landmark photos. We provide baseline\nresults for both recognition and retrieval tasks based on state-of-the-art\nmethods as well as competitive results from a public challenge. We further\ndemonstrate the suitability of the dataset for transfer learning by showing\nthat image embeddings trained on it achieve competitive retrieval performance\non independent datasets. The dataset images, ground-truth and metric scoring\ncode are available at https://github.com/cvdfoundation/google-landmark.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 22:52:17 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 18:30:45 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Weyand", "Tobias", ""], ["Araujo", "Andre", ""], ["Cao", "Bingyi", ""], ["Sim", "Jack", ""]]}, {"id": "2004.01808", "submitter": "Noureldien Hussein", "authors": "Noureldien Hussein, Mihir Jain, Babak Ehteshami Bejnordi", "title": "TimeGate: Conditional Gating of Segments in Long-range Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When recognizing a long-range activity, exploring the entire video is\nexhaustive and computationally expensive, as it can span up to a few minutes.\nThus, it is of great importance to sample only the salient parts of the video.\nWe propose TimeGate, along with a novel conditional gating module, for sampling\nthe most representative segments from the long-range activity. TimeGate has two\nnovelties that address the shortcomings of previous sampling methods, as\nSCSampler. First, it enables a differentiable sampling of segments. Thus,\nTimeGate can be fitted with modern CNNs and trained end-to-end as a single and\nunified model.Second, the sampling is conditioned on both the segments and\ntheir context. Consequently, TimeGate is better suited for long-range\nactivities, where the importance of a segment heavily depends on the video\ncontext.TimeGate reduces the computation of existing CNNs on three benchmarks\nfor long-range activities: Charades, Breakfast and MultiThumos. In particular,\nTimeGate reduces the computation of I3D by 50% while maintaining the\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 23:14:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hussein", "Noureldien", ""], ["Jain", "Mihir", ""], ["Bejnordi", "Babak Ehteshami", ""]]}, {"id": "2004.01817", "submitter": "Xuelu Li", "authors": "Xuelu Li and Vishal Monga", "title": "Group Based Deep Shared Feature Learning for Fine-grained Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification has emerged as a significant challenge\nbecause objects in such images have small inter-class visual differences but\nwith large variations in pose, lighting, and viewpoints, etc. Most existing\nwork focuses on highly customized feature extraction via deep network\narchitectures which have been shown to deliver state of the art performance.\nGiven that images from distinct classes in fine-grained classification share\nsignificant features of interest, we present a new deep network architecture\nthat explicitly models shared features and removes their effect to achieve\nenhanced classification results. Our modeling of shared features is based on a\nnew group based learning wherein existing classes are divided into groups and\nmultiple shared feature patterns are discovered (learned). We call this\nframework Group based deep Shared Feature Learning (GSFL) and the resulting\nlearned network as GSFL-Net. Specifically, the proposed GSFL-Net develops a\nspecially designed autoencoder which is constrained by a newly proposed Feature\nExpression Loss to decompose a set of features into their constituent shared\nand discriminative components. During inference, only the discriminative\nfeature component is used to accomplish the classification task. A key benefit\nof our specialized autoencoder is that it is versatile and can be combined with\nstate-of-the-art fine-grained feature extraction models and trained together\nwith them to improve their performance directly. Experiments on benchmark\ndatasets show that GSFL-Net can enhance classification accuracy over the state\nof the art with a more interpretable architecture.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 00:01:11 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Xuelu", ""], ["Monga", "Vishal", ""]]}, {"id": "2004.01823", "submitter": "Andr\\'es Mu\\~noz Garza", "authors": "Andres Munoz, Mohammadreza Zolfaghari, Max Argus and Thomas Brox", "title": "Temporal Shift GAN for Large Scale Video Generation", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video generation models have become increasingly popular in the last few\nyears, however the standard 2D architectures used today lack natural\nspatio-temporal modelling capabilities. In this paper, we present a network\narchitecture for video generation that models spatio-temporal consistency\nwithout resorting to costly 3D architectures. The architecture facilitates\ninformation exchange between neighboring time points, which improves the\ntemporal consistency of both the high level structure as well as the low-level\ndetails of the generated frames. The approach achieves state-of-the-art\nquantitative performance, as measured by the inception score on the UCF-101\ndataset as well as better qualitative results. We also introduce a new\nquantitative measure (S3) that uses downstream tasks for evaluation. Moreover,\nwe present a new multi-label dataset MaisToy, which enables us to evaluate the\ngeneralization of the model.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 00:40:52 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 19:46:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Munoz", "Andres", ""], ["Zolfaghari", "Mohammadreza", ""], ["Argus", "Max", ""], ["Brox", "Thomas", ""]]}, {"id": "2004.01849", "submitter": "Haochen Wang", "authors": "Haochen Wang, Ruotian Luo, Michael Maire, Greg Shakhnarovich", "title": "Pixel Consensus Voting for Panoptic Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core of our approach, Pixel Consensus Voting, is a framework for instance\nsegmentation based on the Generalized Hough transform. Pixels cast discretized,\nprobabilistic votes for the likely regions that contain instance centroids. At\nthe detected peaks that emerge in the voting heatmap, backprojection is applied\nto collect pixels and produce instance masks. Unlike a sliding window detector\nthat densely enumerates object proposals, our method detects instances as a\nresult of the consensus among pixel-wise votes. We implement vote aggregation\nand backprojection using native operators of a convolutional neural network.\nThe discretization of centroid voting reduces the training of instance\nsegmentation to pixel labeling, analogous and complementary to FCN-style\nsemantic segmentation, leading to an efficient and unified architecture that\njointly models things and stuff. We demonstrate the effectiveness of our\npipeline on COCO and Cityscapes Panoptic Segmentation and obtain competitive\nresults. Code will be open-sourced.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 04:33:45 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Haochen", ""], ["Luo", "Ruotian", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Greg", ""]]}, {"id": "2004.01857", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Milad Sikaroudi, H.R. Tizhoosh, Fakhri Karray, Mark\n  Crowley", "title": "Weighted Fisher Discriminant Analysis in the Input and Feature Spaces", "comments": "Accepted (to appear) in International Conference on Image Analysis\n  and Recognition (ICIAR) 2020, Springer", "journal-ref": "International Conference on Image Analysis and Recognition, vol 2,\n  pp. 3-15. Springer, Cham, 2020", "doi": "10.1007/978-3-030-50516-5_1", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher Discriminant Analysis (FDA) is a subspace learning method which\nminimizes and maximizes the intra- and inter-class scatters of data,\nrespectively. Although, in FDA, all the pairs of classes are treated the same\nway, some classes are closer than the others. Weighted FDA assigns weights to\nthe pairs of classes to address this shortcoming of FDA. In this paper, we\npropose a cosine-weighted FDA as well as an automatically weighted FDA in which\nweights are found automatically. We also propose a weighted FDA in the feature\nspace to establish a weighted kernel FDA for both existing and newly proposed\nweights. Our experiments on the ORL face recognition dataset show the\neffectiveness of the proposed weighting schemes.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:17:53 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Sikaroudi", "Milad", ""], ["Tizhoosh", "H. R.", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2004.01860", "submitter": "Kaihao Zhang", "authors": "Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu,\n  Hongdong Li", "title": "Deblurring by Realistic Blurring", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning methods for image deblurring typically train models\nusing pairs of sharp images and their blurred counterparts. However,\nsynthetically blurring images do not necessarily model the genuine blurring\nprocess in real-world scenarios with sufficient accuracy. To address this\nproblem, we propose a new method which combines two GAN models, i.e., a\nlearning-to-Blur GAN (BGAN) and learning-to-DeBlur GAN (DBGAN), in order to\nlearn a better model for image deblurring by primarily learning how to blur\nimages. The first model, BGAN, learns how to blur sharp images with unpaired\nsharp and blurry image sets, and then guides the second model, DBGAN, to learn\nhow to correctly deblur such images. In order to reduce the discrepancy between\nreal blur and synthesized blur, a relativistic blur loss is leveraged. As an\nadditional contribution, this paper also introduces a Real-World Blurred Image\n(RWBI) dataset including diverse blurry images. Our experiments show that the\nproposed method achieves consistently superior quantitative performance as well\nas higher perceptual quality on both the newly proposed dataset and the public\nGOPRO dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:25:15 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 03:34:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhang", "Kaihao", ""], ["Luo", "Wenhan", ""], ["Zhong", "Yiran", ""], ["Ma", "Lin", ""], ["Stenger", "Bjorn", ""], ["Liu", "Wei", ""], ["Li", "Hongdong", ""]]}, {"id": "2004.01864", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Theoretical Insights into the Use of Structural Similarity Index In\n  Generative Models and Inferential Autoencoders", "comments": "Accepted (to appear) in International Conference on Image Analysis\n  and Recognition (ICIAR) 2020, Springer", "journal-ref": "International Conference on Image Analysis and Recognition, vol 2,\n  pp. 112-117. Springer, Cham, 2020", "doi": "10.1007/978-3-030-50516-5_10", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models and inferential autoencoders mostly make use of $\\ell_2$\nnorm in their optimization objectives. In order to generate perceptually better\nimages, this short paper theoretically discusses how to use Structural\nSimilarity Index (SSIM) in generative models and inferential autoencoders. We\nfirst review SSIM, SSIM distance metrics, and SSIM kernel. We show that the\nSSIM kernel is a universal kernel and thus can be used in unconditional and\nconditional generated moment matching networks. Then, we explain how to use\nSSIM distance in variational and adversarial autoencoders and unconditional and\nconditional Generative Adversarial Networks (GANs). Finally, we propose to use\nSSIM distance rather than $\\ell_2$ norm in least squares GAN.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:39:15 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2004.01888", "submitter": "Xinggang Wang", "authors": "Yifu Zhang and Chunyu Wang and Xinggang Wang and Wenjun Zeng and Wenyu\n  Liu", "title": "FairMOT: On the Fairness of Detection and Re-Identification in Multiple\n  Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been remarkable progress on object detection and re-identification\n(re-ID) in recent years which are the key components of multi-object tracking.\nHowever, little attention has been focused on jointly accomplishing the two\ntasks in a single network. Our study shows that the previous attempts ended up\nwith degraded accuracy mainly because the re-ID task is not fairly learned\nwhich causes many identity switches. The unfairness lies in two-fold: (1) they\ntreat re-ID as a secondary task whose accuracy heavily depends on the primary\ndetection task. So training is largely biased to the detection task but ignores\nthe re-ID task; (2) they use ROI-Align to extract re-ID features which is\ndirectly borrowed from object detection. However, this introduces a lot of\nambiguity in characterizing objects because many sampling points may belong to\ndisturbing instances or background. To solve the problems, we present a simple\napproach \\emph{FairMOT} which consists of two homogeneous branches to predict\npixel-wise objectness scores and re-ID features. The achieved fairness between\nthe tasks allows \\emph{FairMOT} to obtain high levels of detection and tracking\naccuracy and outperform previous state-of-the-arts by a large margin on several\npublic datasets. The source code and pre-trained models are released at\nhttps://github.com/ifzhang/FairMOT.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 08:18:00 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 03:19:16 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 01:15:16 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 09:25:09 GMT"}, {"version": "v5", "created": "Wed, 9 Sep 2020 09:00:13 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Zhang", "Yifu", ""], ["Wang", "Chunyu", ""], ["Wang", "Xinggang", ""], ["Zeng", "Wenjun", ""], ["Liu", "Wenyu", ""]]}, {"id": "2004.01903", "submitter": "Zuowen Wang", "authors": "Zuowen Wang and Leo Horne", "title": "Understanding (Non-)Robust Feature Disentanglement and the Relationship\n  Between Low- and High-Dimensional Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has put forth the hypothesis that adversarial vulnerabilities in\nneural networks are due to them overusing \"non-robust features\" inherent in the\ntraining data. We show empirically that for PGD-attacks, there is a training\nstage where neural networks start heavily relying on non-robust features to\nboost natural accuracy. We also propose a mechanism reducing vulnerability to\nPGD-style attacks consisting of mixing in a certain amount of images\ncontain-ing mostly \"robust features\" into each training batch, and then show\nthat robust accuracy is improved, while natural accuracy is not substantially\nhurt. We show that training on \"robust features\" provides boosts in robust\naccuracy across various architectures and for different attacks. Finally, we\ndemonstrate empirically that these \"robust features\" do not induce spatial\ninvariance.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 10:38:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Zuowen", ""], ["Horne", "Leo", ""]]}, {"id": "2004.01905", "submitter": "Aashish Sharma Mr", "authors": "Wending Yan, Aashish Sharma, Robby T. Tan", "title": "Optical Flow in Dense Foggy Scenes using Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dense foggy scenes, existing optical flow methods are erroneous. This is\ndue to the degradation caused by dense fog particles that break the optical\nflow basic assumptions such as brightness and gradient constancy. To address\nthe problem, we introduce a semi-supervised deep learning technique that\nemploys real fog images without optical flow ground-truths in the training\nprocess. Our network integrates the domain transformation and optical flow\nnetworks in one framework. Initially, given a pair of synthetic fog images, its\ncorresponding clean images and optical flow ground-truths, in one training\nbatch we train our network in a supervised manner. Subsequently, given a pair\nof real fog images and a pair of clean images that are not corresponding to\neach other (unpaired), in the next training batch, we train our network in an\nunsupervised manner. We then alternate the training of synthetic and real data\niteratively. We use real data without ground-truths, since to have\nground-truths in such conditions is intractable, and also to avoid the\noverfitting problem of synthetic data training, where the knowledge learned on\nsynthetic data cannot be generalized to real data testing. Together with the\nnetwork architecture design, we propose a new training strategy that combines\nsupervised synthetic-data training and unsupervised real-data training.\nExperimental results show that our method is effective and outperforms the\nstate-of-the-art methods in estimating optical flow in dense foggy scenes.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 10:44:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yan", "Wending", ""], ["Sharma", "Aashish", ""], ["Tan", "Robby T.", ""]]}, {"id": "2004.01929", "submitter": "Sharad Joshi", "authors": "Sharad Joshi, Pawel Korus, Nitin Khanna, Nasir Memon", "title": "Empirical Evaluation of PRNU Fingerprint Variation for Mismatched\n  Imaging Pipelines", "comments": "6 pages and 3 pages supplemental file", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the variability of PRNU-based camera fingerprints with mismatched\nimaging pipelines (e.g., different camera ISP or digital darkroom software). We\nshow that camera fingerprints exhibit non-negligible variations in this setup,\nwhich may lead to unexpected degradation of detection statistics in real-world\nuse-cases. We tested 13 different pipelines, including standard digital\ndarkroom software and recent neural-networks. We observed that correlation\nbetween fingerprints from mismatched pipelines drops on average to 0.38 and the\nPCE detection statistic drops by over 40%. The degradation in error rates is\nthe strongest for small patches commonly used in photo manipulation detection,\nand when neural networks are used for photo development. At a fixed 0.5% FPR\nsetting, the TPR drops by 17 ppt (percentage points) for 128 px and 256 px\npatches.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 13:09:50 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 18:29:16 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Joshi", "Sharad", ""], ["Korus", "Pawel", ""], ["Khanna", "Nitin", ""], ["Memon", "Nasir", ""]]}, {"id": "2004.01946", "submitter": "Dominik Kulon", "authors": "Dominik Kulon, Riza Alp G\\\"uler, Iasonas Kokkinos, Michael Bronstein,\n  Stefanos Zafeiriou", "title": "Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020). Additional resources: https://arielai.com/mesh_hands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and effective network architecture for monocular 3D\nhand pose estimation consisting of an image encoder followed by a mesh\nconvolutional decoder that is trained through a direct 3D hand mesh\nreconstruction loss. We train our network by gathering a large-scale dataset of\nhand action in YouTube videos and use it as a source of weak supervision. Our\nweakly-supervised mesh convolutions-based system largely outperforms\nstate-of-the-art methods, even halving the errors on the in the wild benchmark.\nThe dataset and additional resources are available at\nhttps://arielai.com/mesh_hands.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:35:37 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kulon", "Dominik", ""], ["G\u00fcler", "Riza Alp", ""], ["Kokkinos", "Iasonas", ""], ["Bronstein", "Michael", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2004.01959", "submitter": "Guoqing Wang", "authors": "Guoqing Wang, Hu Han, Shiguang Shan and Xilin Chen", "title": "Cross-domain Face Presentation Attack Detection via Multi-domain\n  Disentangled Representation Learning", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face presentation attack detection (PAD) has been an urgent problem to be\nsolved in the face recognition systems. Conventional approaches usually assume\nthe testing and training are within the same domain; as a result, they may not\ngeneralize well into unseen scenarios because the representations learned for\nPAD may overfit to the subjects in the training set. In light of this, we\npropose an efficient disentangled representation learning for cross-domain face\nPAD. Our approach consists of disentangled representation learning (DR-Net) and\nmulti-domain learning (MD-Net). DR-Net learns a pair of encoders via generative\nmodels that can disentangle PAD informative features from subject\ndiscriminative features. The disentangled features from different domains are\nfed to MD-Net which learns domain-independent features for the final\ncross-domain face PAD task. Extensive experiments on several public datasets\nvalidate the effectiveness of the proposed approach for cross-domain PAD.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 15:45:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Guoqing", ""], ["Han", "Hu", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2004.01961", "submitter": "Yingwei Li", "authors": "Yingwei Li, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie Yang, Cihang\n  Xie, Qihang Yu, Yuyin Zhou, Song Bai, Alan Yuille", "title": "Neural Architecture Search for Lightweight Non-Local Networks", "comments": "CVPR 2020. Project page: https://github.com/LiYingwei/AutoNL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Local (NL) blocks have been widely studied in various vision tasks.\nHowever, it has been rarely explored to embed the NL blocks in mobile neural\nnetworks, mainly due to the following challenges: 1) NL blocks generally have\nheavy computation cost which makes it difficult to be applied in applications\nwhere computational resources are limited, and 2) it is an open problem to\ndiscover an optimal configuration to embed NL blocks into mobile neural\nnetworks. We propose AutoNL to overcome the above two obstacles. Firstly, we\npropose a Lightweight Non-Local (LightNL) block by squeezing the transformation\noperations and incorporating compact features. With the novel design choices,\nthe proposed LightNL block is 400x computationally cheaper} than its\nconventional counterpart without sacrificing the performance. Secondly, by\nrelaxing the structure of the LightNL block to be differentiable during\ntraining, we propose an efficient neural architecture search algorithm to learn\nan optimal configuration of LightNL blocks in an end-to-end manner. Notably,\nusing only 32 GPU hours, the searched AutoNL model achieves 77.7% top-1\naccuracy on ImageNet under a typical mobile setting (350M FLOPs), significantly\noutperforming previous mobile models including MobileNetV2 (+5.7%), FBNet\n(+2.8%) and MnasNet (+2.1%). Code and models are available at\nhttps://github.com/LiYingwei/AutoNL.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 15:46:39 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Yingwei", ""], ["Jin", "Xiaojie", ""], ["Mei", "Jieru", ""], ["Lian", "Xiaochen", ""], ["Yang", "Linjie", ""], ["Xie", "Cihang", ""], ["Yu", "Qihang", ""], ["Zhou", "Yuyin", ""], ["Bai", "Song", ""], ["Yuille", "Alan", ""]]}, {"id": "2004.01963", "submitter": "Yawogan Jean Eudes Gbodjo", "authors": "Yawogan Jean Eudes Gbodjo, Dino Ienco, Louise Leroux, Roberto\n  Interdonato, Raffaelle Gaetano", "title": "Fine grained classification for multi-source land cover mapping", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Nowadays, there is a general agreement on the need to better characterize\nagricultural monitoring systems in response to the global changes. Timely and\naccurate land use/land cover mapping can support this vision by providing\nuseful information at fine scale. Here, a deep learning approach is proposed to\ndeal with multi-source land cover mapping at object level. The approach is\nbased on an extension of Recurrent Neural Network enriched via an attention\nmechanism dedicated to multi-temporal data context. Moreover, a new\nhierarchical pretraining strategy designed to exploit specific domain knowledge\navailable under hierarchical relationships within land cover classes is\nintroduced. Experiments carried out on the Reunion island - a french overseas\ndepartment - demonstrate the significance of the proposal compared to remote\nsensing standard approaches for land cover mapping.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 15:49:37 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gbodjo", "Yawogan Jean Eudes", ""], ["Ienco", "Dino", ""], ["Leroux", "Louise", ""], ["Interdonato", "Roberto", ""], ["Gaetano", "Raffaelle", ""]]}, {"id": "2004.01981", "submitter": "Ze Yang", "authors": "Ze Yang, Wei Wu, Huang Hu, Can Xu, Wei Wang, Zhoujun Li", "title": "Open Domain Dialogue Generation with Latent Images", "comments": "AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider grounding open domain dialogues with images. Existing work\nassumes that both an image and a textual context are available, but\nimage-grounded dialogues by nature are more difficult to obtain than textual\ndialogues. Thus, we propose learning a response generation model with both\nimage-grounded dialogues and textual dialogues by assuming that the visual\nscene information at the time of a conversation can be represented by an image,\nand trying to recover the latent images of the textual dialogues through\ntext-to-image generation techniques. The likelihood of the two types of\ndialogues is then formulated by a response generator and an image reconstructor\nthat are learned within a conditional variational auto-encoding framework.\nEmpirical studies are conducted in both image-grounded conversation and\ntext-based conversation. In the first scenario, image-grounded dialogues,\nespecially under a low-resource setting, can be effectively augmented by\ntextual dialogues with latent images; while in the second scenario, latent\nimages can enrich the content of responses and at the same time keep them\nrelevant to contexts.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:32:46 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 07:43:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yang", "Ze", ""], ["Wu", "Wei", ""], ["Hu", "Huang", ""], ["Xu", "Can", ""], ["Wang", "Wei", ""], ["Li", "Zhoujun", ""]]}, {"id": "2004.01997", "submitter": "Xudong Wang", "authors": "Xudong Wang, Shizhong Han, Yunqiang Chen, Dashan Gao, and Nuno\n  Vasconcelos", "title": "Volumetric Attention for 3D Medical Image Segmentation and Detection", "comments": "Accepted by MICCAI 2019", "journal-ref": "In International Conference on Medical Image Computing and\n  Computer-Assisted Intervention, pp. 175-184. Springer, Cham, 2019", "doi": "10.1007/978-3-030-32226-7_20", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A volumetric attention(VA) module for 3D medical image segmentation and\ndetection is proposed. VA attention is inspired by recent advances in video\nprocessing, enables 2.5D networks to leverage context information along the z\ndirection, and allows the use of pretrained 2D detection models when training\ndata is limited, as is often the case for medical applications. Its integration\nin the Mask R-CNN is shown to enable state-of-the-art performance on the Liver\nTumor Segmentation (LiTS) Challenge, outperforming the previous challenge\nwinner by 3.9 points and achieving top performance on the LiTS leader board at\nthe time of paper submission. Detection experiments on the DeepLesion dataset\nalso show that the addition of VA to existing object detectors enables a 69.1\nsensitivity at 0.5 false positive per image, outperforming the best published\nresults by 6.6 points.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 18:55:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Xudong", ""], ["Han", "Shizhong", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2004.02009", "submitter": "Mehrdad Noori", "authors": "Mehrdad Noori, Ali Bahri and Karim Mohammadi", "title": "Attention-Guided Version of 2D UNet for Automatic Brain Tumor\n  Segmentation", "comments": "7 pages, 5 figures, 4 tables, Accepted by ICCKE 2019", "journal-ref": "2019 9th International Conference on Computer and Knowledge\n  Engineering (ICCKE), Mashhad, Iran, 2019, pp. 269-275", "doi": "10.1109/ICCKE48569.2019.8964956", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gliomas are the most common and aggressive among brain tumors, which cause a\nshort life expectancy in their highest grade. Therefore, treatment assessment\nis a key stage to enhance the quality of the patients' lives. Recently, deep\nconvolutional neural networks (DCNNs) have achieved a remarkable performance in\nbrain tumor segmentation, but this task is still difficult owing to high\nvarying intensity and appearance of gliomas. Most of the existing methods,\nespecially UNet-based networks, integrate low-level and high-level features in\na naive way, which may result in confusion for the model. Moreover, most\napproaches employ 3D architectures to benefit from 3D contextual information of\ninput images. These architectures contain more parameters and computational\ncomplexity than 2D architectures. On the other hand, using 2D models causes not\nto benefit from 3D contextual information of input images. In order to address\nthe mentioned issues, we design a low-parameter network based on 2D UNet in\nwhich we employ two techniques. The first technique is an attention mechanism,\nwhich is adopted after concatenation of low-level and high-level features. This\ntechnique prevents confusion for the model by weighting each of the channels\nadaptively. The second technique is the Multi-View Fusion. By adopting this\ntechnique, we can benefit from 3D contextual information of input images\ndespite using a 2D model. Experimental results demonstrate that our method\nperforms favorably against 2017 and 2018 state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:09:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Noori", "Mehrdad", ""], ["Bahri", "Ali", ""], ["Mohammadi", "Karim", ""]]}, {"id": "2004.02021", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Yongyi Lu, Wei Shen, Elliot K. Fishman, Alan L. Yuille", "title": "Segmentation for Classification of Screening Pancreatic Neuroendocrine\n  Tumors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents comprehensive results to detect in the early stage the\npancreatic neuroendocrine tumors (PNETs), a group of endocrine tumors arising\nin the pancreas, which are the second common type of pancreatic cancer, by\nchecking the abdominal CT scans. To the best of our knowledge, this task has\nnot been studied before as a computational task. To provide radiologists with\ntumor locations, we adopt a segmentation framework to classify CT volumes by\nchecking if at least a sufficient number of voxels is segmented as tumors. To\nquantitatively analyze our method, we collect and voxelwisely label a new\nabdominal CT dataset containing $376$ cases with both arterial and venous\nphases available for each case, in which $228$ cases were diagnosed with PNETs\nwhile the remaining $148$ cases are normal, which is currently the largest\ndataset for PNETs to the best of our knowledge. In order to incorporate rich\nknowledge of radiologists to our framework, we annotate dilated pancreatic duct\nas well, which is regarded as the sign of high risk for pancreatic cancer.\nQuantitatively, our approach outperforms state-of-the-art segmentation networks\nand achieves a sensitivity of $89.47\\%$ at a specificity of $81.08\\%$, which\nindicates a potential direction to achieve a clinical impact related to cancer\ndiagnosis by earlier tumor detection.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:21:44 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Lu", "Yongyi", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2004.02022", "submitter": "Junwei Liang", "authors": "Junwei Liang, Lu Jiang, Alexander Hauptmann", "title": "SimAug: Learning Robust Representations from Simulation for Trajectory\n  Prediction", "comments": "Accepted by ECCV 2020. Project website:\n  https://next.cs.cmu.edu/simaug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of predicting future trajectories of people in\nunseen cameras of novel scenarios and views. We approach this problem through\nthe real-data-free setting in which the model is trained only on 3D simulation\ndata and applied out-of-the-box to a wide variety of real cameras. We propose a\nnovel approach to learn robust representation through augmenting the simulation\ntraining data such that the representation can better generalize to unseen\nreal-world test data. The key idea is to mix the feature of the hardest camera\nview with the adversarial feature of the original view. We refer to our method\nas SimAug. We show that SimAug achieves promising results on three real-world\nbenchmarks using zero real training data, and state-of-the-art performance in\nthe Stanford Drone and the VIRAT/ActEV dataset when using in-domain training\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:22:01 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 00:58:16 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 20:43:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liang", "Junwei", ""], ["Jiang", "Lu", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2004.02025", "submitter": "Karttikeya Mangalam", "authors": "Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal, Kuan-Hui Lee,\n  Ehsan Adeli, Jitendra Malik, Adrien Gaidon", "title": "It Is Not the Journey but the Destination: Endpoint Conditioned\n  Trajectory Prediction", "comments": "Accepted at ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trajectory forecasting with multiple socially interacting agents is of\ncritical importance for autonomous navigation in human environments, e.g., for\nself-driving cars and social robots. In this work, we present Predicted\nEndpoint Conditioned Network (PECNet) for flexible human trajectory prediction.\nPECNet infers distant trajectory endpoints to assist in long-range multi-modal\ntrajectory prediction. A novel non-local social pooling layer enables PECNet to\ninfer diverse yet socially compliant trajectories. Additionally, we present a\nsimple \"truncation-trick\" for improving few-shot multi-modal trajectory\nprediction performance. We show that PECNet improves state-of-the-art\nperformance on the Stanford Drone trajectory prediction benchmark by ~20.9% and\non the ETH/UCY benchmark by ~40.8%. Project homepage:\nhttps://karttikeya.github.io/publication/htf/\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:27:13 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 10:08:04 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 21:33:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mangalam", "Karttikeya", ""], ["Girase", "Harshayu", ""], ["Agarwal", "Shreyas", ""], ["Lee", "Kuan-Hui", ""], ["Adeli", "Ehsan", ""], ["Malik", "Jitendra", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2004.02032", "submitter": "Hammad Ayyubi", "authors": "Hammad A. Ayyubi, Md. Mehrab Tanjim, Julian J. McAuley, and Garrison\n  W. Cottrell", "title": "Generating Rationales in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in Visual QuestionAnswering (VQA), it remains a\nchallenge todetermine how much success can be attributedto sound reasoning and\ncomprehension ability.We seek to investigate this question by propos-ing a new\ntask ofrationale generation. Es-sentially, we task a VQA model with generat-ing\nrationales for the answers it predicts. Weuse data from the Visual Commonsense\nRea-soning (VCR) task, as it contains ground-truthrationales along with visual\nquestions and an-swers. We first investigate commonsense un-derstanding in one\nof the leading VCR mod-els, ViLBERT, by generating rationales frompretrained\nweights using a state-of-the-art lan-guage model, GPT-2. Next, we seek to\njointlytrain ViLBERT with GPT-2 in an end-to-endfashion with the dual task of\npredicting the an-swer in VQA and generating rationales. Weshow that this kind\nof training injects com-monsense understanding in the VQA modelthrough\nquantitative and qualitative evaluationmetrics\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:15:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ayyubi", "Hammad A.", ""], ["Tanjim", "Md. Mehrab", ""], ["McAuley", "Julian J.", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "2004.02034", "submitter": "Subramanyam Natarajan", "authors": "Arvind Srinivasan, Aprameya Bharadwaj, Manasa Sathyan, S Natarajan", "title": "Optimization of Image Embeddings for Few Shot Learning", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we improve the image embeddings generated in the graph neural\nnetwork solution for few shot learning. We propose alternate architectures for\nexisting networks such as Inception-Net, U-Net, Attention U-Net, and\nSqueeze-Net to generate embeddings and increase the accuracy of the models. We\nimprove the quality of embeddings created at the cost of the time taken to\ngenerate them. The proposed implementations outperform the existing state of\nthe art methods for 1-shot and 5-shot learning on the Omniglot dataset. The\nexperiments involved a testing set and training set which had no common classes\nbetween them. The results for 5-way and 10-way/20-way tests have been\ntabulated.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:17:08 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Srinivasan", "Arvind", ""], ["Bharadwaj", "Aprameya", ""], ["Sathyan", "Manasa", ""], ["Natarajan", "S", ""]]}, {"id": "2004.02038", "submitter": "Ahmed Shahin", "authors": "Ahmed H. Shahin, Prateek Munjal, Ling Shao, Shadab Khan", "title": "FAIRS -- Soft Focus Generator and Attention for Robust Object\n  Segmentation from Extreme Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation from user inputs has been actively studied to\nfacilitate interactive segmentation for data annotation and other applications.\nRecent studies have shown that extreme points can be effectively used to encode\nuser inputs. A heat map generated from the extreme points can be appended to\nthe RGB image and input to the model for training. In this study, we present\nFAIRS -- a new approach to generate object segmentation from user inputs in the\nform of extreme points and corrective clicks. We propose a novel approach for\neffectively encoding the user input from extreme points and corrective clicks,\nin a novel and scalable manner that allows the network to work with a variable\nnumber of clicks, including corrective clicks for output refinement. We also\nintegrate a dual attention module with our approach to increase the efficacy of\nthe model in preferentially attending to the objects. We demonstrate that these\nadditions help achieve significant improvements over state-of-the-art in dense\nobject segmentation from user inputs, on multiple large-scale datasets. Through\nexperiments, we demonstrate our method's ability to generate high-quality\ntraining data as well as its scalability in incorporating extreme points,\nguiding clicks, and corrective clicks in a principled manner.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:25:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Shahin", "Ahmed H.", ""], ["Munjal", "Prateek", ""], ["Shao", "Ling", ""], ["Khan", "Shadab", ""]]}, {"id": "2004.02042", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "ObjectNet Dataset: Reanalysis and Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Barbu et al introduced a dataset called ObjectNet which includes\nobjects in daily life situations. They showed a dramatic performance drop of\nthe state of the art object recognition models on this dataset. Due to the\nimportance and implications of their results regarding generalization ability\nof deep models, we take a second look at their findings. We highlight a major\nproblem with their work which is applying object recognizers to the scenes\ncontaining multiple objects rather than isolated objects. The latter results in\naround 20-30% performance gain using our code. Compared with the results\nreported in the ObjectNet paper, we observe that around 10-15 % of the\nperformance loss can be recovered, without any test time data augmentation. In\naccordance with Barbu et al.'s conclusions, however, we also conclude that deep\nmodels suffer drastically on this dataset. Thus, we believe that ObjectNet\nremains a challenging dataset for testing the generalization power of models\nbeyond datasets on which they have been trained.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:45:57 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2004.02052", "submitter": "Victor Fragoso", "authors": "Victor Fragoso, Joseph DeGol, Gang Hua", "title": "gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity\n  Priors", "comments": null, "journal-ref": "IEEE/CVF CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications in augmented reality (AR), 3D mapping, and\nrobotics require both fast and accurate estimation of camera poses and scales\nfrom multiple images captured by multiple cameras or a single moving camera.\nAchieving high speed and maintaining high accuracy in a pose-and-scale\nestimator are often conflicting goals. To simultaneously achieve both, we\nexploit a priori knowledge about the solution space. We present gDLS*, a\ngeneralized-camera-model pose-and-scale estimator that utilizes rotation and\nscale priors. gDLS* allows an application to flexibly weigh the contribution of\neach prior, which is important since priors often come from noisy sensors.\nCompared to state-of-the-art generalized-pose-and-scale estimators (e.g.,\ngDLS), our experiments on both synthetic and real data consistently demonstrate\nthat gDLS* accelerates the estimation process and improves scale and pose\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 00:01:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Fragoso", "Victor", ""], ["DeGol", "Joseph", ""], ["Hua", "Gang", ""]]}, {"id": "2004.02060", "submitter": "Rahul Paul", "authors": "Lawrence O. Hall, Rahul Paul, Dmitry B. Goldgof, and Gregory M.\n  Goldgof", "title": "Finding Covid-19 from Chest X-rays using Deep Learning on a Small\n  Dataset", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing for COVID-19 has been unable to keep up with the demand. Further, the\nfalse negative rate is projected to be as high as 30% and test results can take\nsome time to obtain. X-ray machines are widely available and provide images for\ndiagnosis quickly. This paper explores how useful chest X-ray images can be in\ndiagnosing COVID-19 disease. We have obtained 122 chest X-rays of COVID-19 and\nover 4,000 chest X-rays of viral and bacterial pneumonia. A pretrained deep\nconvolutional neural network has been tuned on 102 COVID-19 cases and 102 other\npneumonia cases in a 10-fold cross validation. The results were all 102\nCOVID-19 cases were correctly classified and there were 8 false positives\nresulting in an AUC of 0.997. On a test set of 20 unseen COVID-19 cases all\nwere correctly classified and more than 95% of 4171 other pneumonia examples\nwere correctly classified. This study has flaws, most critically a lack of\ninformation about where in the disease process the COVID-19 cases were and the\nsmall data set size. More COVID-19 case images will enable a better answer to\nthe question of how useful chest X-rays can be for diagnosing COVID-19 (so\nplease send them).\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 00:58:54 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 18:08:17 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 12:29:26 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 19:12:46 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hall", "Lawrence O.", ""], ["Paul", "Rahul", ""], ["Goldgof", "Dmitry B.", ""], ["Goldgof", "Gregory M.", ""]]}, {"id": "2004.02070", "submitter": "Qi Song", "authors": "Qi Song, Qianyi Jiang, Nan Li, Rui Zhang and Xiaolin Wei", "title": "ReADS: A Rectified Attentional Double Supervised Network for Scene Text\n  Recognition", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, scene text recognition is always regarded as a\nsequence-to-sequence problem. Connectionist Temporal Classification (CTC) and\nAttentional sequence recognition (Attn) are two very prevailing approaches to\ntackle this problem while they may fail in some scenarios respectively. CTC\nconcentrates more on every individual character but is weak in text semantic\ndependency modeling. Attn based methods have better context semantic modeling\nability while tends to overfit on limited training data. In this paper, we\nelaborately design a Rectified Attentional Double Supervised Network (ReADS)\nfor general scene text recognition. To overcome the weakness of CTC and Attn,\nboth of them are applied in our method but with different modules in two\nsupervised branches which can make a complementary to each other. Moreover,\neffective spatial and channel attention mechanisms are introduced to eliminate\nbackground noise and extract valid foreground information. Finally, a simple\nrectified network is implemented to rectify irregular text. The ReADS can be\ntrained end-to-end and only word-level annotations are required. Extensive\nexperiments on various benchmarks verify the effectiveness of ReADS which\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 02:05:35 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 01:44:17 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Song", "Qi", ""], ["Jiang", "Qianyi", ""], ["Li", "Nan", ""], ["Zhang", "Rui", ""], ["Wei", "Xiaolin", ""]]}, {"id": "2004.02072", "submitter": "Keval Doshi", "authors": "Keval Doshi, Yasin Yilmaz", "title": "Any-Shot Sequential Anomaly Detection in Surveillance Videos", "comments": "Accepted to CVPR 2020: Workshop on Continual Learning in Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in surveillance videos has been recently gaining attention.\nEven though the performance of state-of-the-art methods on publicly available\ndata sets has been competitive, they demand a massive amount of training data.\nAlso, they lack a concrete approach for continuously updating the trained model\nonce new data is available. Furthermore, online decision making is an important\nbut mostly neglected factor in this domain. Motivated by these research gaps,\nwe propose an online anomaly detection method for surveillance videos using\ntransfer learning and any-shot learning, which in turn significantly reduces\nthe training complexity and provides a mechanism that can detect anomalies\nusing only a few labeled nominal examples. Our proposed algorithm leverages the\nfeature extraction power of neural network-based models for transfer learning\nand the any-shot learning capability of statistical detection methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 02:15:45 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2004.02086", "submitter": "Chuan Tan", "authors": "Chuan Tan (1), Jin Zhu (1), Pietro Lio' (1) ((1) University of\n  Cambridge)", "title": "Arbitrary Scale Super-Resolution for Brain MRI Images", "comments": "12 pages, 8 figures, 1 table, to appear as a full paper with oral\n  contribution in AIAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent attempts at Super-Resolution for medical images used deep learning\ntechniques such as Generative Adversarial Networks (GANs) to achieve\nperceptually realistic single image Super-Resolution. Yet, they are constrained\nby their inability to generalise to different scale factors. This involves high\nstorage and energy costs as every integer scale factor involves a separate\nneural network. A recent paper has proposed a novel meta-learning technique\nthat uses a Weight Prediction Network to enable Super-Resolution on arbitrary\nscale factors using only a single neural network. In this paper, we propose a\nnew network that combines that technique with SRGAN, a state-of-the-art\nGAN-based architecture, to achieve arbitrary scale, high fidelity\nSuper-Resolution for medical images. By using this network to perform arbitrary\nscale magnifications on images from the Multimodal Brain Tumor Segmentation\nChallenge (BraTS) dataset, we demonstrate that it is able to outperform\ntraditional interpolation methods by up to 20$\\%$ on SSIM scores whilst\nretaining generalisability on brain MRI images. We show that performance across\nscales is not compromised, and that it is able to achieve competitive results\nwith other state-of-the-art methods such as EDSR whilst being fifty times\nsmaller than them. Combining efficiency, performance, and generalisability,\nthis can hopefully become a new foundation for tackling Super-Resolution on\nmedical images.\n  Check out the webapp here: https://metasrgan.herokuapp.com/ Check out the\ngithub tutorial here: https://github.com/pancakewaffles/metasrgan-tutorial\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 03:53:28 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 02:34:44 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tan", "Chuan", ""], ["Zhu", "Jin", ""], ["Lio'", "Pietro", ""]]}, {"id": "2004.02088", "submitter": "Chunyuan Li", "authors": "Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, Changyou Chen", "title": "Feature Quantization Improves GAN Training", "comments": "The first two authors contributed equally to this manuscript. ICML\n  2020. Code: https://github.com/YangNaruto/FQ-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instability in GAN training has been a long-standing problem despite\nremarkable research efforts. We identify that instability issues stem from\ndifficulties of performing feature matching with mini-batch statistics, due to\na fragile balance between the fixed target distribution and the progressively\ngenerated distribution. In this work, we propose Feature Quantization (FQ) for\nthe discriminator, to embed both true and fake data samples into a shared\ndiscrete space. The quantized values of FQ are constructed as an evolving\ndictionary, which is consistent with feature statistics of the recent\ndistribution history. Hence, FQ implicitly enables robust feature matching in a\ncompact space. Our method can be easily plugged into existing GAN models, with\nlittle computational overhead in training. We apply FQ to 3 representative GAN\nmodels on 9 benchmarks: BigGAN for image generation, StyleGAN for face\nsynthesis, and U-GAT-IT for unsupervised image-to-image translation. Extensive\nexperimental results show that the proposed FQ-GAN can improve the FID scores\nof baseline methods by a large margin on a variety of tasks, achieving new\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 04:06:50 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 00:06:52 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhao", "Yang", ""], ["Li", "Chunyuan", ""], ["Yu", "Ping", ""], ["Gao", "Jianfeng", ""], ["Chen", "Changyou", ""]]}, {"id": "2004.02093", "submitter": "Minghao Fu", "authors": "Minghao Fu, Zhenshan Xie, Wen Li, Lixin Duan", "title": "Deeply Aligned Adaptation for Cross-domain Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain object detection has recently attracted more and more attention\nfor real-world applications, since it helps build robust detectors adapting\nwell to new environments. In this work, we propose an end-to-end solution based\non Faster R-CNN, where ground-truth annotations are available for source images\n(e.g., cartoon) but not for target ones (e.g., watercolor) during training.\nMotivated by the observation that the transferabilities of different neural\nnetwork layers differ from each other, we propose to apply a number of domain\nalignment strategies to different layers of Faster R-CNN, where the alignment\nstrength is gradually reduced from low to higher layers. Moreover, after\nobtaining region proposals in our network, we develop a foreground-background\naware alignment module to further reduce the domain mismatch by separately\naligning features of the foreground and background regions from the source and\ntarget domains. Extensive experiments on benchmark datasets demonstrate the\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 04:41:45 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 01:39:11 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Fu", "Minghao", ""], ["Xie", "Zhenshan", ""], ["Li", "Wen", ""], ["Duan", "Lixin", ""]]}, {"id": "2004.02097", "submitter": "Jian Wang", "authors": "Jian Wang, Miaomiao Zhang", "title": "DeepFLASH: An Efficient Network for Learning-based Medical Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents DeepFLASH, a novel network with efficient training and\ninference for learning-based medical image registration. In contrast to\nexisting approaches that learn spatial transformations from training data in\nthe high dimensional imaging space, we develop a new registration network\nentirely in a low dimensional bandlimited space. This dramatically reduces the\ncomputational cost and memory footprint of an expensive training and inference.\nTo achieve this goal, we first introduce complex-valued operations and\nrepresentations of neural architectures that provide key components for\nlearning-based registration models. We then construct an explicit loss function\nof transformation fields fully characterized in a bandlimited space with much\nfewer parameterizations. Experimental results show that our method is\nsignificantly faster than the state-of-the-art deep learning based image\nregistration methods, while producing equally accurate alignment. We\ndemonstrate our algorithm in two different applications of image registration:\n2D synthetic data and 3D real brain magnetic resonance (MR) images. Our code is\navailable at https://github.com/jw4hv/deepflash.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 05:17:07 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Jian", ""], ["Zhang", "Miaomiao", ""]]}, {"id": "2004.02099", "submitter": "Conrad M Albrecht", "authors": "Conrad M Albrecht, Chris Fisher, Marcus Freitag, Hendrik F Hamann,\n  Sharathchandra Pankanti, Florencia Pezzutti, Francesca Rossi", "title": "Learning and Recognizing Archeological Features from LiDAR Data", "comments": null, "journal-ref": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9005548", "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a remote sensing pipeline that processes LiDAR (Light Detection\nAnd Ranging) data through machine & deep learning for the application of\narcheological feature detection on big geo-spatial data platforms such as e.g.\nIBM PAIRS Geoscope.\n  Today, archeologists get overwhelmed by the task of visually surveying huge\namounts of (raw) LiDAR data in order to identify areas of interest for\ninspection on the ground. We showcase a software system pipeline that results\nin significant savings in terms of expert productivity while missing only a\nsmall fraction of the artifacts.\n  Our work employs artificial neural networks in conjunction with an efficient\nspatial segmentation procedure based on domain knowledge. Data processing is\nconstraint by a limited amount of training labels and noisy LiDAR signals due\nto vegetation cover and decay of ancient structures. We aim at identifying\ngeo-spatial areas with archeological artifacts in a supervised fashion allowing\nthe domain expert to flexibly tune parameters based on her needs.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 05:36:37 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Albrecht", "Conrad M", ""], ["Fisher", "Chris", ""], ["Freitag", "Marcus", ""], ["Hamann", "Hendrik F", ""], ["Pankanti", "Sharathchandra", ""], ["Pezzutti", "Florencia", ""], ["Rossi", "Francesca", ""]]}, {"id": "2004.02108", "submitter": "Shi Yin", "authors": "Shi Yin, Shangfei Wang, Xiaoping Chen, Enhong Chen", "title": "Attentive One-Dimensional Heatmap Regression for Facial Landmark\n  Detection and Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although heatmap regression is considered a state-of-the-art method to locate\nfacial landmarks, it suffers from huge spatial complexity and is prone to\nquantization error. To address this, we propose a novel attentive\none-dimensional heatmap regression method for facial landmark localization.\nFirst, we predict two groups of 1D heatmaps to represent the marginal\ndistributions of the x and y coordinates. These 1D heatmaps reduce spatial\ncomplexity significantly compared to current heatmap regression methods, which\nuse 2D heatmaps to represent the joint distributions of x and y coordinates.\nWith much lower spatial complexity, the proposed method can output\nhigh-resolution 1D heatmaps despite limited GPU memory, significantly\nalleviating the quantization error. Second, a co-attention mechanism is adopted\nto model the inherent spatial patterns existing in x and y coordinates, and\ntherefore the joint distributions on the x and y axes are also captured. Third,\nbased on the 1D heatmap structures, we propose a facial landmark detector\ncapturing spatial patterns for landmark detection on an image; and a tracker\nfurther capturing temporal patterns with a temporal refinement mechanism for\nlandmark tracking. Experimental results on four benchmark databases demonstrate\nthe superiority of our method.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 06:51:22 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 01:22:18 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 14:21:21 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 08:52:39 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2020 03:19:39 GMT"}, {"version": "v6", "created": "Fri, 21 Aug 2020 02:07:19 GMT"}, {"version": "v7", "created": "Thu, 27 Aug 2020 13:54:22 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yin", "Shi", ""], ["Wang", "Shangfei", ""], ["Chen", "Xiaoping", ""], ["Chen", "Enhong", ""]]}, {"id": "2004.02113", "submitter": "Gwenaelle Cunha Sergio", "authors": "Gwenaelle Cunha Sergio and Minho Lee", "title": "Emotional Video to Audio Transformation Using Deep Recurrent Neural\n  Networks and a Neuro-Fuzzy System", "comments": "Published (https://www.hindawi.com/journals/mpe/2020/8478527/)", "journal-ref": "Mathematical Problems in Engineering 2020 (2020) 1-15", "doi": "10.1155/2020/8478527", "report-no": null, "categories": "cs.SD cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating music with emotion similar to that of an input video is a very\nrelevant issue nowadays. Video content creators and automatic movie directors\nbenefit from maintaining their viewers engaged, which can be facilitated by\nproducing novel material eliciting stronger emotions in them. Moreover, there's\ncurrently a demand for more empathetic computers to aid humans in applications\nsuch as augmenting the perception ability of visually and/or hearing impaired\npeople. Current approaches overlook the video's emotional characteristics in\nthe music generation step, only consider static images instead of videos, are\nunable to generate novel music, and require a high level of human effort and\nskills. In this study, we propose a novel hybrid deep neural network that uses\nan Adaptive Neuro-Fuzzy Inference System to predict a video's emotion from its\nvisual features and a deep Long Short-Term Memory Recurrent Neural Network to\ngenerate its corresponding audio signals with similar emotional inkling. The\nformer is able to appropriately model emotions due to its fuzzy properties, and\nthe latter is able to model data with dynamic time properties well due to the\navailability of the previous hidden state information. The novelty of our\nproposed method lies in the extraction of visual emotional features in order to\ntransform them into audio signals with corresponding emotional aspects for\nusers. Quantitative experiments show low mean absolute errors of 0.217 and\n0.255 in the Lindsey and DEAP datasets respectively, and similar global\nfeatures in the spectrograms. This indicates that our model is able to\nappropriately perform domain transformation between visual and audio features.\nBased on experimental results, our model can effectively generate audio that\nmatches the scene eliciting a similar emotion from the viewer in both datasets,\nand music generated by our model is also chosen more often.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 07:18:28 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sergio", "Gwenaelle Cunha", ""], ["Lee", "Minho", ""]]}, {"id": "2004.02122", "submitter": "Jie Li", "authors": "Jie Li, Kai Han, Peng Wang, Yu Liu, Xia Yuan", "title": "Anisotropic Convolutional Networks for 3D Semantic Scene Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As a voxel-wise labeling task, semantic scene completion (SSC) tries to\nsimultaneously infer the occupancy and semantic labels for a scene from a\nsingle depth and/or RGB image. The key challenge for SSC is how to effectively\ntake advantage of the 3D context to model various objects or stuffs with severe\nvariations in shapes, layouts and visibility. To handle such variations, we\npropose a novel module called anisotropic convolution, which properties with\nflexibility and power impossible for the competing methods such as standard 3D\nconvolution and some of its variations. In contrast to the standard 3D\nconvolution that is limited to a fixed 3D receptive field, our module is\ncapable of modeling the dimensional anisotropy voxel-wisely. The basic idea is\nto enable anisotropic 3D receptive field by decomposing a 3D convolution into\nthree consecutive 1D convolutions, and the kernel size for each such 1D\nconvolution is adaptively determined on the fly. By stacking multiple such\nanisotropic convolution modules, the voxel-wise modeling capability can be\nfurther enhanced while maintaining a controllable amount of model parameters.\nExtensive experiments on two SSC benchmarks, NYU-Depth-v2 and NYUCAD, show the\nsuperior performance of the proposed method. Our code is available at\nhttps://waterljwant.github.io/SSC/\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 07:57:02 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Jie", ""], ["Han", "Kai", ""], ["Wang", "Peng", ""], ["Liu", "Yu", ""], ["Yuan", "Xia", ""]]}, {"id": "2004.02132", "submitter": "Hoang Le", "authors": "Hoang Le, Feng Liu, Shu Zhang, and Aseem Agarwala", "title": "Deep Homography Estimation for Dynamic Scenes", "comments": "CVPR 2020, https://github.com/lcmhoang/hmg-dynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homography estimation is an important step in many computer vision problems.\nRecently, deep neural network methods have shown to be favorable for this\nproblem when compared to traditional methods. However, these new methods do not\nconsider dynamic content in input images. They train neural networks with only\nimage pairs that can be perfectly aligned using homographies. This paper\ninvestigates and discusses how to design and train a deep neural network that\nhandles dynamic scenes. We first collect a large video dataset with dynamic\ncontent. We then develop a multi-scale neural network and show that when\nproperly trained using our new dataset, this neural network can already handle\ndynamic scenes to some extent. To estimate a homography of a dynamic scene in a\nmore principled way, we need to identify the dynamic content. Since dynamic\ncontent detection and homography estimation are two tightly coupled tasks, we\nfollow the multi-task learning principles and augment our multi-scale network\nsuch that it jointly estimates the dynamics masks and homographies. Our\nexperiments show that our method can robustly estimate homography for\nchallenging scenarios with dynamic scenes, blur artifacts, or lack of textures.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:07:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Le", "Hoang", ""], ["Liu", "Feng", ""], ["Zhang", "Shu", ""], ["Agarwala", "Aseem", ""]]}, {"id": "2004.02133", "submitter": "Junyu Gao", "authors": "Qi Wang, Tao Han, Junyu Gao, Yuan Yuan", "title": "Neuron Linear Transformation: Modeling the Domain Shift for Crowd\n  Counting", "comments": "accepted by IEEE T-NNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2021.3051371", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain crowd counting (CDCC) is a hot topic due to its importance in\npublic safety. The purpose of CDCC is to alleviate the domain shift between the\nsource and target domain. Recently, typical methods attempt to extract\ndomain-invariant features via image translation and adversarial learning. When\nit comes to specific tasks, we find that the domain shifts are reflected on\nmodel parameters' differences. To describe the domain gap directly at the\nparameter-level, we propose a Neuron Linear Transformation (NLT) method,\nexploiting domain factor and bias weights to learn the domain shift.\nSpecifically, for a specific neuron of a source model, NLT exploits few labeled\ntarget data to learn domain shift parameters. Finally, the target neuron is\ngenerated via a linear transformation. Extensive experiments and analysis on\nsix real-world datasets validate that NLT achieves top performance compared\nwith other domain adaptation methods. An ablation study also shows that the NLT\nis robust and more effective than supervised and fine-tune training. Code is\navailable at: \\url{https://github.com/taohan10200/NLT}.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:15:47 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:51:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Qi", ""], ["Han", "Tao", ""], ["Gao", "Junyu", ""], ["Yuan", "Yuan", ""]]}, {"id": "2004.02134", "submitter": "Jialin Peng", "authors": "Jiajin Yi, Zhimin Yuan, Jialin Peng", "title": "Adversarial-Prediction Guided Multi-task Adaptation for Semantic\n  Segmentation of Electron Microscopy Images", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing 14.6 (2020):\n  1199-1209", "doi": "10.1109/JSTSP.2020.3005317", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an essential step for electron microscopy (EM) image\nanalysis. Although supervised models have achieved significant progress, the\nneed for labor intensive pixel-wise annotation is a major limitation. To\ncomplicate matters further, supervised learning models may not generalize well\non a novel dataset due to domain shift. In this study, we introduce an\nadversarial-prediction guided multi-task network to learn the adaptation of a\nwell-trained model for use on a novel unlabeled target domain. Since no label\nis available on target domain, we learn an encoding representation not only for\nthe supervised segmentation on source domain but also for unsupervised\nreconstruction of the target data. To improve the discriminative ability with\ngeometrical cues, we further guide the representation learning by multi-level\nadversarial learning in semantic prediction space. Comparisons and ablation\nstudy on public benchmark demonstrated state-of-the-art performance and\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:18:11 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Yi", "Jiajin", ""], ["Yuan", "Zhimin", ""], ["Peng", "Jialin", ""]]}, {"id": "2004.02135", "submitter": "Peng Jin", "authors": "Xingyuan Chen, Ping Cai, Peng Jin, Hongjun Wang, Xinyu Dai, Jiajun\n  Chen", "title": "Adding A Filter Based on The Discriminator to Improve Unconditional Text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoregressive language model (ALM) trained with maximum likelihood\nestimation (MLE) is widely used in unconditional text generation. Due to\nexposure bias, the generated texts still suffer from low quality and diversity.\nThis presents statistically as a discrepancy between the real text and\ngenerated text. Some research shows a discriminator can detect this\ndiscrepancy. Because the discriminator can encode more information than the\ngenerator, discriminator has the potentiality to improve generator. To\nalleviate the exposure bias, generative adversarial networks (GAN) use the\ndiscriminator to update the generator's parameters directly, but they fail by\nbeing evaluated precisely. A critical reason for the failure is the difference\nbetween the discriminator input and the ALM input. We propose a novel mechanism\nby adding a filter which has the same input as the discriminator. First,\ndiscriminator detects the discrepancy signals and passes to filter directly (or\nby learning). Then, we use the filter to reject some generated samples with a\nsampling-based method. Thus, the original generative distribution is revised to\nreduce the discrepancy. Two ALMs, RNN-based and Transformer-based, are\nexperimented. Evaluated precisely by three metrics, our mechanism consistently\noutperforms the ALMs and all kinds of GANs across two benchmark data sets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:34:52 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:41:21 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 02:51:41 GMT"}, {"version": "v4", "created": "Tue, 19 May 2020 23:16:52 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2020 04:59:43 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chen", "Xingyuan", ""], ["Cai", "Ping", ""], ["Jin", "Peng", ""], ["Wang", "Hongjun", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "2004.02138", "submitter": "Pengpeng Liu", "authors": "Pengpeng Liu and Irwin King and Michael Lyu and Jia Xu", "title": "Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and\n  Stereo Matching", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified method to jointly learn optical flow and\nstereo matching. Our first intuition is stereo matching can be modeled as a\nspecial case of optical flow, and we can leverage 3D geometry behind\nstereoscopic videos to guide the learning of these two forms of\ncorrespondences. We then enroll this knowledge into the state-of-the-art\nself-supervised learning framework, and train one single network to estimate\nboth flow and stereo. Second, we unveil the bottlenecks in prior\nself-supervised learning approaches, and propose to create a new set of\nchallenging proxy tasks to boost performance. These two insights yield a single\nmodel that achieves the highest accuracy among all existing unsupervised flow\nand stereo methods on KITTI 2012 and 2015 benchmarks. More remarkably, our\nself-supervised method even outperforms several state-of-the-art fully\nsupervised methods, including PWC-Net and FlowNet2 on KITTI 2012.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:52:04 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Pengpeng", ""], ["King", "Irwin", ""], ["Lyu", "Michael", ""], ["Xu", "Jia", ""]]}, {"id": "2004.02147", "submitter": "Changqian Yu", "authors": "Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, Nong\n  Sang", "title": "BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time\n  Semantic Segmentation", "comments": "16 pages, 10 figures, 9 tables. Code is available at\n  https://git.io/BiSeNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-level details and high-level semantics are both essential to the\nsemantic segmentation task. However, to speed up the model inference, current\napproaches almost always sacrifice the low-level details, which leads to a\nconsiderable accuracy decrease. We propose to treat these spatial details and\ncategorical semantics separately to achieve high accuracy and high efficiency\nfor realtime semantic segmentation. To this end, we propose an efficient and\neffective architecture with a good trade-off between speed and accuracy, termed\nBilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a\nDetail Branch, with wide channels and shallow layers to capture low-level\ndetails and generate high-resolution feature representation; (ii) a Semantic\nBranch, with narrow channels and deep layers to obtain high-level semantic\ncontext. The Semantic Branch is lightweight due to reducing the channel\ncapacity and a fast-downsampling strategy. Furthermore, we design a Guided\nAggregation Layer to enhance mutual connections and fuse both types of feature\nrepresentation. Besides, a booster training strategy is designed to improve the\nsegmentation performance without any extra inference cost. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed\narchitecture performs favourably against a few state-of-the-art real-time\nsemantic segmentation approaches. Specifically, for a 2,048x1,024 input, we\nachieve 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on\none NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than\nexisting methods, yet we achieve better segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 10:26:38 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yu", "Changqian", ""], ["Gao", "Changxin", ""], ["Wang", "Jingbo", ""], ["Yu", "Gang", ""], ["Shen", "Chunhua", ""], ["Sang", "Nong", ""]]}, {"id": "2004.02164", "submitter": "Tianchen Zhao", "authors": "Xuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu Wang, Huazhong\n  Yang", "title": "DSA: More Efficient Budgeted Pruning via Differentiable Sparsity\n  Allocation", "comments": "The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Budgeted pruning is the problem of pruning under resource constraints. In\nbudgeted pruning, how to distribute the resources across layers (i.e., sparsity\nallocation) is the key problem. Traditional methods solve it by discretely\nsearching for the layer-wise pruning ratios, which lacks efficiency. In this\npaper, we propose Differentiable Sparsity Allocation (DSA), an efficient\nend-to-end budgeted pruning flow. Utilizing a novel differentiable pruning\nprocess, DSA finds the layer-wise pruning ratios with gradient-based\noptimization. It allocates sparsity in continuous space, which is more\nefficient than methods based on discrete evaluation and search. Furthermore,\nDSA could work in a pruning-from-scratch manner, whereas traditional budgeted\npruning methods are applied to pre-trained models. Experimental results on\nCIFAR-10 and ImageNet show that DSA could achieve superior performance than\ncurrent iterative budgeted pruning methods, and shorten the time cost of the\noverall pruning process by at least 1.5x in the meantime.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 11:28:39 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:23:05 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 11:05:53 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2020 08:13:41 GMT"}, {"version": "v5", "created": "Mon, 20 Jul 2020 15:26:14 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ning", "Xuefei", ""], ["Zhao", "Tianchen", ""], ["Li", "Wenshuo", ""], ["Lei", "Peng", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "2004.02168", "submitter": "Dipesh Gyawali", "authors": "Dipesh Gyawali, Alok Regmi, Aatish Shakya, Ashish Gautam, Surendra\n  Shrestha", "title": "Comparative Analysis of Multiple Deep CNN Models for Waste\n  Classification", "comments": "6 pages, 13 figures", "journal-ref": "5th International Conference on Advanced Engineering and\n  ICT-Convergence 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Waste is a wealth in a wrong place. Our research focuses on analyzing\npossibilities for automatic waste sorting and collecting in such a way that\nhelps it for further recycling process. Various approaches are being practiced\nmanaging waste but not efficient and require human intervention. The automatic\nwaste segregation would fit in to fill the gap. The project tested well known\nDeep Learning Network architectures for waste classification with dataset\ncombined from own endeavors and Trash Net. The convolutional neural network is\nused for image classification. The hardware built in the form of dustbin is\nused to segregate those wastes into different compartments. Without the human\nexercise in segregating those waste products, the study would save the precious\ntime and would introduce the automation in the area of waste management.\nMunicipal solid waste is a huge, renewable source of energy. The situation is\nwin-win for both government, society and industrialists. Because of fine-tuning\nof the ResNet18 Network, the best validation accuracy was found to be 87.8%.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 11:50:27 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 09:19:12 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Gyawali", "Dipesh", ""], ["Regmi", "Alok", ""], ["Shakya", "Aatish", ""], ["Gautam", "Ashish", ""], ["Shrestha", "Surendra", ""]]}, {"id": "2004.02183", "submitter": "Jay Nandy", "authors": "Jay Nandy, Wynne Hsu, Mong Li Lee", "title": "Approximate Manifold Defense Against Multiple Adversarial Perturbations", "comments": "Workshop on Machine Learning with Guarantees, NeurIPS 2019. IJCNN,\n  2020 (full paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing defenses against adversarial attacks are typically tailored to a\nspecific perturbation type. Using adversarial training to defend against\nmultiple types of perturbation requires expensive adversarial examples from\ndifferent perturbation types at each training step. In contrast, manifold-based\ndefense incorporates a generative network to project an input sample onto the\nclean data manifold. This approach eliminates the need to generate expensive\nadversarial examples while achieving robustness against multiple perturbation\ntypes. However, the success of this approach relies on whether the generative\nnetwork can capture the complete clean data manifold, which remains an open\nproblem for complex input domain. In this work, we devise an approximate\nmanifold defense mechanism, called RBF-CNN, for image classification. Instead\nof capturing the complete data manifold, we use an RBF layer to learn the\ndensity of small image patches. RBF-CNN also utilizes a reconstruction layer\nthat mitigates any minor adversarial perturbations. Further, incorporating our\nproposed reconstruction process for training improves the adversarial\nrobustness of our RBF-CNN models. Experiment results on MNIST and CIFAR-10\ndatasets indicate that RBF-CNN offers robustness for multiple perturbations\nwithout the need for expensive adversarial training.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 12:36:08 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 08:08:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Nandy", "Jay", ""], ["Hsu", "Wynne", ""], ["Lee", "Mong Li", ""]]}, {"id": "2004.02186", "submitter": "Edoardo Remelli", "authors": "Edoardo Remelli, Shangchen Han, Sina Honari, Pascal Fua, Robert Wang", "title": "Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled\n  Representation", "comments": null, "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 6040-6049", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a lightweight solution to recover 3D pose from multi-view images\ncaptured with spatially calibrated cameras. Building upon recent advances in\ninterpretable representation learning, we exploit 3D geometry to fuse input\nimages into a unified latent representation of pose, which is disentangled from\ncamera view-points. This allows us to reason effectively about 3D pose across\ndifferent views without using compute-intensive volumetric grids. Our\narchitecture then conditions the learned representation on camera projection\noperators to produce accurate per-view 2d detections, that can be simply lifted\nto 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do\nit efficiently, we propose a novel implementation of DLT that is orders of\nmagnitude faster on GPU architectures than standard SVD-based triangulation\nmethods. We evaluate our approach on two large-scale human pose datasets (H36M\nand Total Capture): our method outperforms or performs comparably to the\nstate-of-the-art volumetric methods, while, unlike them, yielding real-time\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 12:52:29 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 08:35:57 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Remelli", "Edoardo", ""], ["Han", "Shangchen", ""], ["Honari", "Sina", ""], ["Fua", "Pascal", ""], ["Wang", "Robert", ""]]}, {"id": "2004.02194", "submitter": "Dan Guo", "authors": "Dan Guo, Hui Wang, Hanwang Zhang, Zheng-Jun Zha, Meng Wang", "title": "Iterative Context-Aware Graph Inference for Visual Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog is a challenging task that requires the comprehension of the\nsemantic dependencies among implicit visual and textual contexts. This task can\nrefer to the relation inference in a graphical model with sparse contexts and\nunknown graph structure (relation descriptor), and how to model the underlying\ncontext-aware relation inference is critical. To this end, we propose a novel\nContext-Aware Graph (CAG) neural network. Each node in the graph corresponds to\na joint semantic feature, including both object-based (visual) and\nhistory-related (textual) context representations. The graph structure\n(relations in dialog) is iteratively updated using an adaptive top-$K$ message\npassing mechanism. Specifically, in every message passing step, each node\nselects the most $K$ relevant nodes, and only receives messages from them.\nThen, after the update, we impose graph attention on all the nodes to get the\nfinal graph embedding and infer the answer. In CAG, each node has dynamic\nrelations in the graph (different related $K$ neighbor nodes), and only the\nmost relevant nodes are attributive to the context-aware relational graph\ninference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG\noutperforms comparative methods. Visualization results further validate the\ninterpretability of our method.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:09:37 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Guo", "Dan", ""], ["Wang", "Hui", ""], ["Zhang", "Hanwang", ""], ["Zha", "Zheng-Jun", ""], ["Wang", "Meng", ""]]}, {"id": "2004.02195", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Makarand Tapaswi, M. Saquib Sarfraz, Rainer Stiefelhagen", "title": "Clustering based Contrastive Learning for Improving Face Representations", "comments": "To appear at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A good clustering algorithm can discover natural groupings in data. These\ngroupings, if used wisely, provide a form of weak supervision for learning\nrepresentations. In this work, we present Clustering-based Contrastive Learning\n(CCL), a new clustering-based representation learning approach that uses labels\nobtained from clustering along with video constraints to learn discriminative\nface features. We demonstrate our method on the challenging task of learning\nrepresentations for video face clustering. Through several ablation studies, we\nanalyze the impact of creating pair-wise positive and negative labels from\ndifferent sources. Experiments on three challenging video face clustering\ndatasets: BBT-0101, BF-0502, and ACCIO show that CCL achieves a new\nstate-of-the-art on all datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:11:44 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sharma", "Vivek", ""], ["Tapaswi", "Makarand", ""], ["Sarfraz", "M. Saquib", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2004.02200", "submitter": "Seong Tae Kim", "authors": "Seong Tae Kim, Farrukh Mushtaq, Nassir Navab", "title": "Confident Coreset for Active Learning in Medical Image Analysis", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have resulted in great successes in various\napplications. Although semi-supervised or unsupervised learning methods have\nbeen widely investigated, the performance of deep neural networks highly\ndepends on the annotated data. The problem is that the budget for annotation is\nusually limited due to the annotation time and expensive annotation cost in\nmedical data. Active learning is one of the solutions to this problem where an\nactive learner is designed to indicate which samples need to be annotated to\neffectively train a target model. In this paper, we propose a novel active\nlearning method, confident coreset, which considers both uncertainty and\ndistribution for effectively selecting informative samples. By comparative\nexperiments on two medical image analysis tasks, we show that our method\noutperforms other active learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 13:46:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kim", "Seong Tae", ""], ["Mushtaq", "Farrukh", ""], ["Navab", "Nassir", ""]]}, {"id": "2004.02205", "submitter": "Vivek Sharma", "authors": "Vivek Sharma and Makarand Tapaswi and Rainer Stiefelhagen", "title": "Deep Multimodal Feature Encoding for Video Ordering", "comments": "IEEE International Conference on Computer Vision (ICCV) Workshop on\n  Large Scale Holistic Video Understanding. The datasets and code are available\n  at https://github.com/vivoutlaw/tcbp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  True understanding of videos comes from a joint analysis of all its\nmodalities: the video frames, the audio track, and any accompanying text such\nas closed captions. We present a way to learn a compact multimodal feature\nrepresentation that encodes all these modalities. Our model parameters are\nlearned through a proxy task of inferring the temporal ordering of a set of\nunordered videos in a timeline. To this end, we create a new multimodal dataset\nfor temporal ordering that consists of approximately 30K scenes (2-6 clips per\nscene) based on the \"Large Scale Movie Description Challenge\". We analyze and\nevaluate the individual and joint modalities on three challenging tasks: (i)\ninferring the temporal ordering of a set of videos; and (ii) action\nrecognition. We demonstrate empirically that multimodal representations are\nindeed complementary, and can play a key role in improving the performance of\nmany applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:02:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sharma", "Vivek", ""], ["Tapaswi", "Makarand", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2004.02215", "submitter": "Jing Jin", "authors": "Jing Jin and Junhui Hou and Jie Chen and Sam Kwong", "title": "Light Field Spatial Super-resolution via Deep Combinatorial Geometry\n  Embedding and Structural Consistency Regularization", "comments": "This paper was accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field (LF) images acquired by hand-held devices usually suffer from low\nspatial resolution as the limited sampling resources have to be shared with the\nangular dimension. LF spatial super-resolution (SR) thus becomes an\nindispensable part of the LF camera processing pipeline. The\nhigh-dimensionality characteristic and complex geometrical structure of LF\nimages make the problem more challenging than traditional single-image SR. The\nperformance of existing methods is still limited as they fail to thoroughly\nexplore the coherence among LF views and are insufficient in accurately\npreserving the parallax structure of the scene. In this paper, we propose a\nnovel learning-based LF spatial SR framework, in which each view of an LF image\nis first individually super-resolved by exploring the complementary information\namong views with combinatorial geometry embedding. For accurate preservation of\nthe parallax structure among the reconstructed views, a regularization network\ntrained over a structure-aware loss function is subsequently appended to\nenforce correct parallax relationships over the intermediate estimation. Our\nproposed approach is evaluated over datasets with a large number of testing\nimages including both synthetic and real-world scenes. Experimental results\ndemonstrate the advantage of our approach over state-of-the-art methods, i.e.,\nour method not only improves the average PSNR by more than 1.0 dB but also\npreserves more accurate parallax details, at a lower computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:39:57 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jin", "Jing", ""], ["Hou", "Junhui", ""], ["Chen", "Jie", ""], ["Kwong", "Sam", ""]]}, {"id": "2004.02222", "submitter": "Sagie Benaim", "authors": "Sagie Benaim, Ron Mokady, Amit Bermano, Daniel Cohen-Or, Lior Wolf", "title": "Structural-analogy from a Single Image Pair", "comments": "Published in 'Computer Graphics Forum'", "journal-ref": null, "doi": "10.1111/cgf.14186", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of unsupervised image-to-image translation has seen substantial\nadvancements in recent years through the use of deep neural networks.\nTypically, the proposed solutions learn the characterizing distribution of two\nlarge, unpaired collections of images, and are able to alter the appearance of\na given image, while keeping its geometry intact. In this paper, we explore the\ncapabilities of neural networks to understand image structure given only a\nsingle pair of images, A and B. We seek to generate images that are\nstructurally aligned: that is, to generate an image that keeps the appearance\nand style of B, but has a structural arrangement that corresponds to A. The key\nidea is to map between image patches at different scales. This enables\ncontrolling the granularity at which analogies are produced, which determines\nthe conceptual distinction between style and content. In addition to structural\nalignment, our method can be used to generate high quality imagery in other\nconditional generation tasks utilizing images A and B only: guided image\nsynthesis, style and texture transfer, text translation as well as video\ntranslation. Our code and additional results are available in\nhttps://github.com/rmokady/structural-analogy/.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 14:51:10 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 16:36:49 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 16:57:44 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Benaim", "Sagie", ""], ["Mokady", "Ron", ""], ["Bermano", "Amit", ""], ["Cohen-Or", "Daniel", ""], ["Wolf", "Lior", ""]]}, {"id": "2004.02234", "submitter": "Wei Jing", "authors": "Wei Jing, Feng Tian, Jizhong Zhang, Kuo-Ming Chao, Zhenxin Hong, Xu\n  Liu", "title": "Feature Super-Resolution Based Facial Expression Recognition for\n  Multi-scale Low-Resolution Faces", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Expressions Recognition(FER) on low-resolution images is necessary for\napplications like group expression recognition in crowd scenarios(station,\nclassroom etc.). Classifying a small size facial image into the right\nexpression category is still a challenging task. The main cause of this problem\nis the loss of discriminative feature due to reduced resolution.\nSuper-resolution method is often used to enhance low-resolution images, but the\nperformance on FER task is limited when on images of very low resolution. In\nthis work, inspired by feature super-resolution methods for object detection,\nwe proposed a novel generative adversary network-based feature level\nsuper-resolution method for robust facial expression recognition(FSR-FER). In\nparticular, a pre-trained FER model was employed as feature extractor, and a\ngenerator network G and a discriminator network D are trained with features\nextracted from images of low resolution and original high resolution. Generator\nnetwork G tries to transform features of low-resolution images to more\ndiscriminative ones by making them closer to the ones of corresponding\nhigh-resolution images. For better classification performance, we also proposed\nan effective classification-aware loss re-weighting strategy based on the\nclassification probability calculated by a fixed FER model to make our model\nfocus more on samples that are easily misclassified. Experiment results on\nReal-World Affective Faces (RAF) Database demonstrate that our method achieves\nsatisfying results on various down-sample factors with a single model and has\nbetter performance on low-resolution images compared with methods using image\nsuper-resolution and expression recognition separately.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 15:38:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jing", "Wei", ""], ["Tian", "Feng", ""], ["Zhang", "Jizhong", ""], ["Chao", "Kuo-Ming", ""], ["Hong", "Zhenxin", ""], ["Liu", "Xu", ""]]}, {"id": "2004.02235", "submitter": "Dvir Samuel", "authors": "Dvir Samuel, Yuval Atzmon and Gal Chechik", "title": "From Generalized zero-shot learning to long-tail with class descriptors", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data is predominantly unbalanced and long-tailed, but deep models\nstruggle to recognize rare classes in the presence of frequent classes. Often,\nclasses can be accompanied by side information like textual descriptions, but\nit is not fully clear how to use them for learning with unbalanced long-tail\ndata. Such descriptions have been mostly used in (Generalized) Zero-shot\nlearning (ZSL), suggesting that ZSL with class descriptions may also be useful\nfor long-tail distributions. We describe DRAGON, a late-fusion architecture for\nlong-tail learning with class descriptors. It learns to (1) correct the bias\ntowards head classes on a sample-by-sample basis; and (2) fuse information from\nclass-descriptions to improve the tail-class accuracy. We also introduce new\nbenchmarks CUB-LT, SUN-LT, AWA-LT for long-tail learning with\nclass-descriptions, building on existing learning-with-attributes datasets and\na version of Imagenet-LT with class descriptors. DRAGON outperforms\nstate-of-the-art models on the new benchmark. It is also a new SoTA on existing\nbenchmarks for GFSL with class descriptors (GFSL-d) and standard (vision-only)\nlong-tailed learning ImageNet-LT, CIFAR-10, 100, and Places365.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 15:51:31 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 21:41:26 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 13:12:28 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 15:17:56 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Samuel", "Dvir", ""], ["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""]]}, {"id": "2004.02249", "submitter": "S. M. Kamrul Hasan", "authors": "S. M. Kamrul Hasan and Cristian A. Linte", "title": "CondenseUNet: A Memory-Efficient Condensely-Connected Architecture for\n  Bi-ventricular Blood Pool and Myocardium Segmentation", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of Cardiac Cine Magnetic Resonance (CMR) Imaging, there has\nbeen a paradigm shift in medical technology, thanks to its capability of\nimaging different structures within the heart without ionizing radiation.\nHowever, it is very challenging to conduct pre-operative planning of minimally\ninvasive cardiac procedures without accurate segmentation and identification of\nthe left ventricle (LV), right ventricle (RV) blood-pool, and LV-myocardium.\nManual segmentation of those structures, nevertheless, is time-consuming and\noften prone to error and biased outcomes. Hence, automatic and computationally\nefficient segmentation techniques are paramount. In this work, we propose a\nnovel memory-efficient Convolutional Neural Network (CNN) architecture as a\nmodification of both CondenseNet, as well as DenseNet for ventricular\nblood-pool segmentation by introducing a bottleneck block and an upsampling\npath. Our experiments show that the proposed architecture runs on the Automated\nCardiac Diagnosis Challenge (ACDC) dataset using half (50%) the memory\nrequirement of DenseNet and one-twelfth (~ 8%) of the memory requirements of\nU-Net, while still maintaining excellent accuracy of cardiac segmentation. We\nvalidated the framework on the ACDC dataset featuring one healthy and four\npathology groups whose heart images were acquired throughout the cardiac cycle\nand achieved the mean dice scores of 96.78% (LV blood-pool), 93.46% (RV\nblood-pool) and 90.1% (LV-Myocardium). These results are promising and promote\nthe proposed methods as a competitive tool for cardiac image segmentation and\nclinical parameter estimation that has the potential to provide fast and\naccurate results, as needed for pre-procedural planning and/or pre-operative\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 16:34:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hasan", "S. M. Kamrul", ""], ["Linte", "Cristian A.", ""]]}, {"id": "2004.02270", "submitter": "Mingrui Yang", "authors": "Mingrui Yang, Yun Jiang, Dan Ma, Bhairav B. Mehta, Mark A. Griswold", "title": "Game of Learning Bloch Equation Simulations for MR Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This work proposes a novel approach to efficiently generate MR\nfingerprints for MR fingerprinting (MRF) problems based on the unsupervised\ndeep learning model generative adversarial networks (GAN). Methods: The GAN\nmodel is adopted and modified for better convergence and performance, resulting\nin an MRF specific model named GAN-MRF. The GAN-MRF model is trained,\nvalidated, and tested using different MRF fingerprints simulated from the Bloch\nequations with certain MRF sequence. The performance and robustness of the\nmodel are further tested by using in vivo data collected on a 3 Tesla scanner\nfrom a healthy volunteer together with MRF dictionaries with different sizes.\nT1, T2 maps are generated and compared quantitatively. Results: The validation\nand testing curves for the GAN-MRF model show no evidence of high bias or high\nvariance problems. The sample MRF fingerprints generated from the trained\nGAN-MRF model agree well with the benchmark fingerprints simulated from the\nBloch equations. The in vivo T1, T2 maps generated from the GAN-MRF\nfingerprints are in good agreement with those generated from the Bloch\nsimulated fingerprints, showing good performance and robustness of the proposed\nGAN-MRF model. Moreover, the MRF dictionary generation time is reduced from\nhours to sub-second for the testing dictionary. Conclusion: The GAN-MRF model\nenables a fast and accurate generation of the MRF fingerprints. It\nsignificantly reduces the MRF dictionary generation process and opens the door\nfor real-time applications and sequence optimization problems.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 18:15:52 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yang", "Mingrui", ""], ["Jiang", "Yun", ""], ["Ma", "Dan", ""], ["Mehta", "Bhairav B.", ""], ["Griswold", "Mark A.", ""]]}, {"id": "2004.02273", "submitter": "Riccardo La Grassa", "authors": "Riccardo La Grassa, Ignazio Gallo, Nicola Landro", "title": "Dynamic Decision Boundary for One-class Classifiers applied to\n  non-uniformly Sampled Data", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A typical issue in Pattern Recognition is the non-uniformly sampled data,\nwhich modifies the general performance and capability of machine learning\nalgorithms to make accurate predictions. Generally, the data is considered\nnon-uniformly sampled when in a specific area of data space, they are not\nenough, leading us to misclassification problems. This issue cut down the goal\nof the one-class classifiers decreasing their performance. In this paper, we\npropose a one-class classifier based on the minimum spanning tree with a\ndynamic decision boundary (OCdmst) to make good prediction also in the case we\nhave non-uniformly sampled data. To prove the effectiveness and robustness of\nour approach we compare with the most recent one-class classifier reaching the\nstate-of-the-art in most of them.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 18:29:36 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["La Grassa", "Riccardo", ""], ["Gallo", "Ignazio", ""], ["Landro", "Nicola", ""]]}, {"id": "2004.02307", "submitter": "Abhinav Valada", "authors": "Rohit Mohan, Abhinav Valada", "title": "EfficientPS: Efficient Panoptic Segmentation", "comments": "Ranked # 1 on Cityscapes panoptic segmentation benchmark, ranked # 2\n  among the published methods on Cityscapes semantic segmentation benchmark,\n  and ranked # 2 among the published methods on Cityscapes instance\n  segmentation benchmark. Demo, code and models are available at\n  https://rl.uni-freiburg.de/research/panoptic", "journal-ref": "International Journal of Computer Vision (IJCV), 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the scene in which an autonomous robot operates is critical for\nits competent functioning. Such scene comprehension necessitates recognizing\ninstances of traffic participants along with general scene semantics which can\nbe effectively addressed by the panoptic segmentation task. In this paper, we\nintroduce the Efficient Panoptic Segmentation (EfficientPS) architecture that\nconsists of a shared backbone which efficiently encodes and fuses semantically\nrich multi-scale features. We incorporate a new semantic head that aggregates\nfine and contextual features coherently and a new variant of Mask R-CNN as the\ninstance head. We also propose a novel panoptic fusion module that congruously\nintegrates the output logits from both the heads of our EfficientPS\narchitecture to yield the final panoptic segmentation output. Additionally, we\nintroduce the KITTI panoptic segmentation dataset that contains panoptic\nannotations for the popularly challenging KITTI benchmark. Extensive\nevaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset\ndemonstrate that our proposed architecture consistently sets the new\nstate-of-the-art on all these four benchmarks while being the most efficient\nand fast panoptic segmentation architecture to date.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 20:15:59 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 00:23:26 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 09:33:18 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Mohan", "Rohit", ""], ["Valada", "Abhinav", ""]]}, {"id": "2004.02317", "submitter": "Maria A. Zuluaga", "authors": "Maria A. Zuluaga and M. Jorge Cardoso and S\\'ebastien Ourselin", "title": "Automatic Right Ventricle Segmentation using Multi-Label Fusion in\n  Cardiac MRI", "comments": null, "journal-ref": "Workshop on RV Segmentation Challenge in Cardiac MRI in\n  conjunction with Medical Image Computing and Computer-Assisted Intervention\n  2012", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the right ventricle (RV) is a crucial step in the\nassessment of the ventricular structure and function. Yet, due to its complex\nanatomy and motion segmentation of the RV has not been as largely studied as\nthe left ventricle. This paper presents a fully automatic method for the\nsegmentation of the RV in cardiac magnetic resonance images (MRI). The method\nuses a coarse-to-fine segmentation strategy in combination with a multi-atlas\npropagation segmentation framework. Based on a cross correlation metric, our\nmethod selects the best atlases for propagation allowing the refinement of the\nsegmentation at each iteration of the propagation. The proposed method was\nevaluated on 32 cardiac MRI datasets provided by the RV Segmentation Challenge\nin Cardiac MRI.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 21:06:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zuluaga", "Maria A.", ""], ["Cardoso", "M. Jorge", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2004.02325", "submitter": "Dmitrii Shadrin Mr", "authors": "Dmitrii Shadrin (1), Mariia Pukalchik (1), Anastasia Uryasheva (2 and\n  3), Evgeny Tsykunov (2), Grigoriy Yashin (2), Nikita Rodichenko (3), Dzmitry\n  Tsetserukou (2) ((1) Center for Computational and Data-Intensive Science and\n  Engineering, Skolkovo Institute of Science and Technology, (2) Space Center,\n  Skolkovo Institute of Science and Technology, (3) Tsuru Robotics (tsapk\n  llc.))", "title": "Hyper-spectral NIR and MIR data and optimal wavebands for detection of\n  apple tree diseases", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant diseases can lead to dramatic losses in yield and quality of food,\nbecoming a problem of high priority for farmers. Apple scab, moniliasis, and\npowdery mildew are the most significant apple tree diseases worldwide and may\ncause between 50% and 60% in yield losses annually; they are controlled by\nfungicide use with huge financial and time expenses. This research proposes a\nmodern approach for analyzing the spectral data in Near-Infrared and\nMid-Infrared ranges of the apple tree diseases at different stages. Using the\nobtained spectra, we found optimal spectral bands for detecting particular\ndisease and discriminating it from other diseases and healthy trees. The\nproposed instrument will provide farmers with accurate, real-time information\non different stages of apple tree diseases, enabling more effective timing, and\nselecting the fungicide application, resulting in better control and increasing\nyield. The obtained dataset, as well as scripts in Matlab for processing data\nand finding optimal spectral bands, are available via the link:\nhttps://yadi.sk/d/ZqfGaNlYVR3TUA\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 21:51:13 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 09:07:47 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 07:42:03 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Shadrin", "Dmitrii", "", "2 and\n  3"], ["Pukalchik", "Mariia", "", "2 and\n  3"], ["Uryasheva", "Anastasia", "", "2 and\n  3"], ["Tsykunov", "Evgeny", ""], ["Yashin", "Grigoriy", ""], ["Rodichenko", "Nikita", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "2004.02331", "submitter": "Simon Jenni", "authors": "Simon Jenni, Hailin Jin, Paolo Favaro", "title": "Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics", "comments": "CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel principle for self-supervised feature learning based on\nthe discrimination of specific transformations of an image. We argue that the\ngeneralization capability of learned features depends on what image\nneighborhood size is sufficient to discriminate different image\ntransformations: The larger the required neighborhood size and the more global\nthe image statistics that the feature can describe. An accurate description of\nglobal image statistics allows to better represent the shape and configuration\nof objects and their context, which ultimately generalizes better to new tasks\nsuch as object classification and detection. This suggests a criterion to\nchoose and design image transformations. Based on this criterion, we introduce\na novel image transformation that we call limited context inpainting (LCI).\nThis transformation inpaints an image patch conditioned only on a small\nrectangular pixel boundary (the limited context). Because of the limited\nboundary information, the inpainter can learn to match local pixel statistics,\nbut is unlikely to match the global statistics of the image. We claim that the\nsame principle can be used to justify the performance of transformations such\nas image rotations and warping. Indeed, we demonstrate experimentally that\nlearning to discriminate transformations such as LCI, image warping and\nrotations, yields features with state of the art generalization capabilities on\nseveral datasets such as Pascal VOC, STL-10, CelebA, and ImageNet. Remarkably,\nour trained features achieve a performance on Places on par with features\ntrained through supervised learning with ImageNet labels.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:09:08 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jenni", "Simon", ""], ["Jin", "Hailin", ""], ["Favaro", "Paolo", ""]]}, {"id": "2004.02377", "submitter": "Julia Gong", "authors": "Julia Gong (1), Yannick Hold-Geoffroy (2), Jingwan Lu (2) ((1)\n  Stanford University, (2) Adobe Research)", "title": "AutoToon: Automatic Geometric Warping for Face Cartoon Generation", "comments": "Accepted and presented at WACV 2020; to appear in proceedings of 2020\n  IEEE Winter Conference on Applications of Computer Vision (WACV). Completed\n  during Julia Gong's internship at Adobe Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature, a type of exaggerated artistic portrait, amplifies the\ndistinctive, yet nuanced traits of human faces. This task is typically left to\nartists, as it has proven difficult to capture subjects' unique characteristics\nwell using automated methods. Recent development of deep end-to-end methods has\nachieved promising results in capturing style and higher-level exaggerations.\nHowever, a key part of caricatures, face warping, has remained challenging for\nthese systems. In this work, we propose AutoToon, the first supervised deep\nlearning method that yields high-quality warps for the warping component of\ncaricatures. Completely disentangled from style, it can be paired with any\nstylization method to create diverse caricatures. In contrast to prior art, we\nleverage an SENet and spatial transformer module and train directly on artist\nwarping fields, applying losses both prior to and after warping. As shown by\nour user studies, we achieve appealing exaggerations that amplify\ndistinguishing features of the face while preserving facial detail.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 02:27:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gong", "Julia", ""], ["Hold-Geoffroy", "Yannick", ""], ["Lu", "Jingwan", ""]]}, {"id": "2004.02428", "submitter": "Zhou Huang", "authors": "Zhou Huang, Huai-Xin Chen, Tao Zhou, Yun-Zhi Yang, Chang-Yin Wang and\n  Bi-Yuan Liu", "title": "Contrast-weighted Dictionary Learning Based Saliency Detection for\n  Remote Sensing Images", "comments": null, "journal-ref": "Pattern Recognition 2020", "doi": "10.1016/j.patcog.2020.107757", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important task in remote sensing image analysis. To\nreduce the computational complexity of redundant information and improve the\nefficiency of image processing, visual saliency models have been widely applied\nin this field. In this paper, a novel saliency detection model based on\nContrast-weighted Dictionary Learning (CDL) is proposed for remote sensing\nimages. Specifically, the proposed CDL learns salient and non-salient atoms\nfrom positive and negative samples to construct a discriminant dictionary, in\nwhich a contrast-weighted term is proposed to encourage the contrast-weighted\npatterns to be present in the learned salient dictionary while discouraging\nthem from being present in the non-salient dictionary. Then, we measure the\nsaliency by combining the coefficients of the sparse representation (SR) and\nreconstruction errors. Furthermore, by using the proposed joint saliency\nmeasure, a variety of saliency maps are generated based on the discriminant\ndictionary. Finally, a fusion method based on global gradient optimization is\nproposed to integrate multiple saliency maps. Experimental results on four\ndatasets demonstrate that the proposed model outperforms other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 06:49:05 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 07:10:02 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Huang", "Zhou", ""], ["Chen", "Huai-Xin", ""], ["Zhou", "Tao", ""], ["Yang", "Yun-Zhi", ""], ["Wang", "Chang-Yin", ""], ["Liu", "Bi-Yuan", ""]]}, {"id": "2004.02432", "submitter": "Jaeyeon Kang", "authors": "Jaeyeon Kang, Younghyun Jo, Seoung Wug Oh, Peter Vajda, and Seon Joo\n  Kim", "title": "Deep Space-Time Video Upsampling Networks", "comments": "ECCV2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) and frame interpolation (FI) are traditional\ncomputer vision problems, and the performance have been improving by\nincorporating deep learning recently. In this paper, we investigate the problem\nof jointly upsampling videos both in space and time, which is becoming more\nimportant with advances in display systems. One solution for this is to run VSR\nand FI, one by one, independently. This is highly inefficient as heavy deep\nneural networks (DNN) are involved in each solution. To this end, we propose an\nend-to-end DNN framework for the space-time video upsampling by efficiently\nmerging VSR and FI into a joint framework. In our framework, a novel weighting\nscheme is proposed to fuse input frames effectively without explicit motion\ncompensation for efficient processing of videos. The results show better\nresults both quantitatively and qualitatively, while reducing the computation\ntime (x7 faster) and the number of parameters (30%) compared to baselines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:04:21 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 02:37:53 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kang", "Jaeyeon", ""], ["Jo", "Younghyun", ""], ["Oh", "Seoung Wug", ""], ["Vajda", "Peter", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2004.02434", "submitter": "Dimity Miller", "authors": "Dimity Miller, Niko S\\\"underhauf, Michael Milford, Feras Dayoub", "title": "Class Anchor Clustering: a Loss for Distance-based Open Set Recognition", "comments": "Published at 2021 IEEE Winter Conference on Applications of Computer\n  Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open set recognition, deep neural networks encounter object classes that\nwere unknown during training. Existing open set classifiers distinguish between\nknown and unknown classes by measuring distance in a network's logit space,\nassuming that known classes cluster closer to the training data than unknown\nclasses. However, this approach is applied post-hoc to networks trained with\ncross-entropy loss, which does not guarantee this clustering behaviour. To\novercome this limitation, we introduce the Class Anchor Clustering (CAC) loss.\nCAC is a distance-based loss that explicitly trains known classes to form tight\nclusters around anchored class-dependent centres in the logit space. We show\nthat training with CAC achieves state-of-the-art performance for distance-based\nopen set classifiers on all six standard benchmark datasets, with a 15.2% AUROC\nincrease on the challenging TinyImageNet, without sacrificing classification\naccuracy. We also show that our anchored class centres achieve higher open set\nperformance than learnt class centres, particularly on object-based datasets\nand large numbers of training classes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:06:18 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 02:23:40 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 23:33:24 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Miller", "Dimity", ""], ["S\u00fcnderhauf", "Niko", ""], ["Milford", "Michael", ""], ["Dayoub", "Feras", ""]]}, {"id": "2004.02460", "submitter": "Zhe Li", "authors": "Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu", "title": "Robust 3D Self-portraits in Seconds", "comments": "CVPR 2020, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient method for robust 3D self-portraits\nusing a single RGBD camera. Benefiting from the proposed PIFusion and\nlightweight bundle adjustment algorithm, our method can generate detailed 3D\nself-portraits in seconds and shows the ability to handle subjects wearing\nextremely loose clothes. To achieve highly efficient and robust reconstruction,\nwe propose PIFusion, which combines learning-based 3D recovery with volumetric\nnon-rigid fusion to generate accurate sparse partial scans of the subject.\nMoreover, a non-rigid volumetric deformation method is proposed to continuously\nrefine the learned shape prior. Finally, a lightweight bundle adjustment\nalgorithm is proposed to guarantee that all the partial scans can not only\n\"loop\" with each other but also remain consistent with the selected live key\nobservations. The results and experiments show that the proposed method\nachieves more robust and efficient 3D self-portraits compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:02:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Zhe", ""], ["Yu", "Tao", ""], ["Pan", "Chuanyu", ""], ["Zheng", "Zerong", ""], ["Liu", "Yebin", ""]]}, {"id": "2004.02478", "submitter": "Kai Chen", "authors": "Kai Chen, Jian Yao, Jingmin Tu, Yahui Liu, Yinxuan Li and Li Li", "title": "Vanishing Point Guided Natural Image Stitching", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, works on improving the naturalness of stitching images gain more\nand more extensive attention. Previous methods suffer the failures of severe\nprojective distortion and unnatural rotation, especially when the number of\ninvolved images is large or images cover a very wide field of view. In this\npaper, we propose a novel natural image stitching method, which takes into\naccount the guidance of vanishing points to tackle the mentioned failures.\nInspired by a vital observation that mutually orthogonal vanishing points in\nManhattan world can provide really useful orientation clues, we design a scheme\nto effectively estimate prior of image similarity. Given such estimated prior\nas global similarity constraints, we feed it into a popular mesh deformation\nframework to achieve impressive natural stitching performances. Compared with\nother existing methods, including APAP, SPHP, AANAP, and GSP, our method\nachieves state-of-the-art performance in both quantitative and qualitative\nexperiments on natural image stitching.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:29:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chen", "Kai", ""], ["Yao", "Jian", ""], ["Tu", "Jingmin", ""], ["Liu", "Yahui", ""], ["Li", "Yinxuan", ""], ["Li", "Li", ""]]}, {"id": "2004.02489", "submitter": "Dhruval Jain", "authors": "Dhruval Jain, DP Mohanty, Sanjeev Roy, Naresh Purre, Sukumar Moharana", "title": "On-device Filtering of Social Media Images for Efficient Storage", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206933", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificially crafted images such as memes, seasonal greetings, etc are\nflooding the social media platforms today. These eventually start occupying a\nlot of internal memory of smartphones and it gets cumbersome for the user to go\nthrough hundreds of images and delete these synthetic images. To address this,\nwe propose a novel method based on Convolutional Neural Networks (CNNs) for the\non-device filtering of social media images by classifying these synthetic\nimages and allowing the user to delete them in one go. The custom model uses\ndepthwise separable convolution layers to achieve low inference time on\nsmartphones. We have done an extensive evaluation of our model on various\ncamera image datasets to cover most aspects of images captured by a camera.\nVarious sorts of synthetic social media images have also been tested. The\nproposed solution achieves an accuracy of 98.25% on the Places-365 dataset and\n95.81% on the Synthetic image dataset that we have prepared containing 30K\ninstances.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:49:29 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 20:40:49 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jain", "Dhruval", ""], ["Mohanty", "DP", ""], ["Roy", "Sanjeev", ""], ["Purre", "Naresh", ""], ["Moharana", "Sukumar", ""]]}, {"id": "2004.02493", "submitter": "Lukas Liebel", "authors": "Lukas Liebel, Ksenia Bittner, Marco K\\\"orner", "title": "A Generalized Multi-Task Learning Approach to Stereo DSM Filtering in\n  Urban Areas", "comments": "This paper was accepted for publication in the ISPRS Journal of\n  Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  City models and height maps of urban areas serve as a valuable data source\nfor numerous applications, such as disaster management or city planning. While\nthis information is not globally available, it can be substituted by digital\nsurface models (DSMs), automatically produced from inexpensive satellite\nimagery. However, stereo DSMs often suffer from noise and blur. Furthermore,\nthey are heavily distorted by vegetation, which is of lesser relevance for most\napplications. Such basic models can be filtered by convolutional neural\nnetworks (CNNs), trained on labels derived from digital elevation models (DEMs)\nand 3D city models, in order to obtain a refined DSM. We propose a modular\nmulti-task learning concept that consolidates existing approaches into a\ngeneralized framework. Our encoder-decoder models with shared encoders and\nmultiple task-specific decoders leverage roof type classification as a\nsecondary task and multiple objectives including a conditional adversarial\nterm. The contributing single-objective losses are automatically weighted in\nthe final multi-task loss function based on learned uncertainty estimates. We\nevaluated the performance of specific instances of this family of network\narchitectures. Our method consistently outperforms the state of the art on\ncommon data, both quantitatively and qualitatively, and generalizes well to a\nnew dataset of an independent study area.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:54:22 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 15:33:46 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Liebel", "Lukas", ""], ["Bittner", "Ksenia", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "2004.02498", "submitter": "Dipesh Tamboli", "authors": "Mukesh Kumar Vishal, Dipesh Tamboli, Abhijeet Patil, Rohit Saluja,\n  Biplab Banerjee, Amit Sethi, Dhandapani Raju, Sudhir Kumar, R N Sahoo,\n  Viswanathan Chinnusamy, J Adinarayana", "title": "Image-based phenotyping of diverse Rice (Oryza Sativa L.) Genotypes", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of either drought-resistant or drought-tolerant varieties in rice\n(Oryza sativa L.), especially for high yield in the context of climate change,\nis a crucial task across the world. The need for high yielding rice varieties\nis a prime concern for developing nations like India, China, and other\nAsian-African countries where rice is a primary staple food. The present\ninvestigation is carried out for discriminating drought tolerant, and\nsusceptible genotypes. A total of 150 genotypes were grown under controlled\nconditions to evaluate at High Throughput Plant Phenomics facility, Nanaji\nDeshmukh Plant Phenomics Centre, Indian Council of Agricultural Research-Indian\nAgricultural Research Institute, New Delhi. A subset of 10 genotypes is taken\nout of 150 for the current investigation. To discriminate against the\ngenotypes, we considered features such as the number of leaves per plant, the\nconvex hull and convex hull area of a plant-convex hull formed by joining the\ntips of the leaves, the number of leaves per unit convex hull of a plant,\ncanopy spread - vertical spread, and horizontal spread of a plant. We trained\nYou Only Look Once (YOLO) deep learning algorithm for leaves tips detection and\nto estimate the number of leaves in a rice plant. With this proposed framework,\nwe screened the genotypes based on selected traits. These genotypes were\nfurther grouped among different groupings of drought-tolerant and drought\nsusceptible genotypes using the Ward method of clustering.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:04:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Vishal", "Mukesh Kumar", ""], ["Tamboli", "Dipesh", ""], ["Patil", "Abhijeet", ""], ["Saluja", "Rohit", ""], ["Banerjee", "Biplab", ""], ["Sethi", "Amit", ""], ["Raju", "Dhandapani", ""], ["Kumar", "Sudhir", ""], ["Sahoo", "R N", ""], ["Chinnusamy", "Viswanathan", ""], ["Adinarayana", "J", ""]]}, {"id": "2004.02501", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Haoran Bai, Jinhui Tang", "title": "Cascaded Deep Video Deblurring Using Temporal Sharpness Prior", "comments": "CVPR 2020. The code is available at https://github.com/csbhr/CDVD-TSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective deep convolutional neural network (CNN)\nmodel for video deblurring. The proposed algorithm mainly consists of optical\nflow estimation from intermediate latent frames and latent frame restoration\nsteps. It first develops a deep CNN model to estimate optical flow from\nintermediate latent frames and then restores the latent frames based on the\nestimated optical flow. To better explore the temporal information from videos,\nwe develop a temporal sharpness prior to constrain the deep CNN model to help\nthe latent frame restoration. We develop an effective cascaded training\napproach and jointly train the proposed CNN model in an end-to-end manner. We\nshow that exploring the domain knowledge of video deblurring is able to make\nthe deep CNN model more compact and efficient. Extensive experimental results\nshow that the proposed algorithm performs favorably against state-of-the-art\nmethods on the benchmark datasets as well as real-world videos.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:13:49 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pan", "Jinshan", ""], ["Bai", "Haoran", ""], ["Tang", "Jinhui", ""]]}, {"id": "2004.02540", "submitter": "Shasha Guo", "authors": "Shasha Guo, Lianhua Qu, Lei Wang, Shuo Tian, Shiming Li, Weixia Xu", "title": "Exploration of Input Patterns for Enhancing the Performance of Liquid\n  State Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNN) have gained increasing attention for its low\npower consumption. But training SNN is challenging. Liquid State Machine (LSM),\nas a major type of Reservoir computing, has been widely recognized for its low\ntraining cost among SNNs. The exploration of LSM topology for enhancing\nperformance often requires hyper-parameter search, which is both\nresource-expensive and time-consuming. We explore the influence of input scale\nreduction on LSM instead. There are two main reasons for studying input\nreduction of LSM. One is that the input dimension of large images requires\nefficient processing. Another one is that input exploration is generally more\neconomic than architecture search. To mitigate the difficulty in effectively\ndealing with huge input spaces of LSM, and to find that whether input reduction\ncan enhance LSM performance, we explore several input patterns, namely\nfullscale, scanline, chessboard, and patch. Several datasets have been used to\nevaluate the performance of the proposed input patterns, including two spatio\nimage datasets and one spatio-temporal image database. The experimental results\nshow that the reduced input under chessboard pattern improves the accuracy by\nup to 5%, and reduces execution time by up to 50% with up to 75\\% less input\nstorage than the fullscale input pattern for LSM.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:20:39 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 17:59:15 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Guo", "Shasha", ""], ["Qu", "Lianhua", ""], ["Wang", "Lei", ""], ["Tian", "Shuo", ""], ["Li", "Shiming", ""], ["Xu", "Weixia", ""]]}, {"id": "2004.02541", "submitter": "Daniel Michelsanti", "authors": "Daniel Michelsanti, Olga Slizovskaia, Gloria Haro, Emilia G\\'omez,\n  Zheng-Hua Tan, Jesper Jensen", "title": "Vocoder-Based Speech Synthesis from Silent Videos", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both acoustic and visual information influence human perception of speech.\nFor this reason, the lack of audio in a video sequence determines an extremely\nlow speech intelligibility for untrained lip readers. In this paper, we present\na way to synthesise speech from the silent video of a talker using deep\nlearning. The system learns a mapping function from raw video frames to\nacoustic features and reconstructs the speech with a vocoder synthesis\nalgorithm. To improve speech reconstruction performance, our model is also\ntrained to predict text information in a multi-task learning fashion and it is\nable to simultaneously reconstruct and recognise speech in real time. The\nresults in terms of estimated speech quality and intelligibility show the\neffectiveness of our method, which exhibits an improvement over existing\nvideo-to-speech approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:22:04 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 22:00:42 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Michelsanti", "Daniel", ""], ["Slizovskaia", "Olga", ""], ["Haro", "Gloria", ""], ["G\u00f3mez", "Emilia", ""], ["Tan", "Zheng-Hua", ""], ["Jensen", "Jesper", ""]]}, {"id": "2004.02546", "submitter": "Erik H\\\"ark\\\"onen", "authors": "Erik H\\\"ark\\\"onen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris", "title": "GANSpace: Discovering Interpretable GAN Controls", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple technique to analyze Generative Adversarial\nNetworks (GANs) and create interpretable controls for image synthesis, such as\nchange of viewpoint, aging, lighting, and time of day. We identify important\nlatent directions based on Principal Components Analysis (PCA) applied either\nin latent space or feature space. Then, we show that a large number of\ninterpretable controls can be defined by layer-wise perturbation along the\nprincipal directions. Moreover, we show that BigGAN can be controlled with\nlayer-wise inputs in a StyleGAN-like manner. We show results on different GANs\ntrained on various datasets, and demonstrate good qualitative matches to edit\ndirections found through earlier supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:41:44 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 11:10:27 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 10:13:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["H\u00e4rk\u00f6nen", "Erik", ""], ["Hertzmann", "Aaron", ""], ["Lehtinen", "Jaakko", ""], ["Paris", "Sylvain", ""]]}, {"id": "2004.02574", "submitter": "Evann Courdier", "authors": "Evann Courdier and Francois Fleuret", "title": "Fair Latency-Aware Metric for real-time video segmentation networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As supervised semantic segmentation is reaching satisfying results, many\nrecent papers focused on making segmentation network architectures faster,\nsmaller and more efficient. In particular, studies often aim to reach the stage\nto which they can claim to be \"real-time\". Achieving this goal is especially\nrelevant in the context of real-time video operations for autonomous vehicles\nand robots, or medical imaging during surgery.\n  The common metric used for assessing these methods is so far the same as the\nones used for image segmentation without time constraint: mean Intersection\nover Union (mIoU). In this paper, we argue that this metric is not relevant\nenough for real-time video as it does not take into account the processing time\n(latency) of the network. We propose a similar but more relevant metric called\nFLAME for video-segmentation networks, that compares the output segmentation of\nthe network with the ground truth segmentation of the current video frame at\nthe time when the network finishes the processing.\n  We perform experiments to compare a few networks using this metric and\npropose a simple addition to network training to enhance results according to\nthat metric.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:41:31 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Courdier", "Evann", ""], ["Fleuret", "Francois", ""]]}, {"id": "2004.02640", "submitter": "Ophir Gozes", "authors": "Ophir Gozes, Maayan Frid-Adar, Nimrod Sagie, Huangqi Zhang, Wenbin Ji,\n  and Hayit Greenspan", "title": "Coronavirus Detection and Analysis on Chest CT with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of the novel coronavirus, officially declared a global pandemic,\nhas a severe impact on our daily lives. As of this writing there are\napproximately 197,188 confirmed cases of which 80,881 are in \"Mainland China\"\nwith 7,949 deaths, a mortality rate of 3.4%. In order to support radiologists\nin this overwhelming challenge, we develop a deep learning based algorithm that\ncan detect, localize and quantify severity of COVID-19 manifestation from chest\nCT scans. The algorithm is comprised of a pipeline of image processing\nalgorithms which includes lung segmentation, 2D slice classification and fine\ngrain localization. In order to further understand the manifestations of the\ndisease, we perform unsupervised clustering of abnormal slices. We present our\nresults on a dataset comprised of 110 confirmed COVID-19 patients from Zhejiang\nprovince, China.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:05:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gozes", "Ophir", ""], ["Frid-Adar", "Maayan", ""], ["Sagie", "Nimrod", ""], ["Zhang", "Huangqi", ""], ["Ji", "Wenbin", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2004.02658", "submitter": "Shunwang Gong", "authors": "Shunwang Gong, Mehdi Bahri, Michael M. Bronstein, Stefanos Zafeiriou", "title": "Geometrically Principled Connections in Graph Neural Networks", "comments": "Presented at Computer Vision and Pattern Recognition (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolution operators bring the advantages of deep learning to a\nvariety of graph and mesh processing tasks previously deemed out of reach. With\ntheir continued success comes the desire to design more powerful architectures,\noften by adapting existing deep learning techniques to non-Euclidean data. In\nthis paper, we argue geometry should remain the primary driving force behind\ninnovation in the emerging field of geometric deep learning. We relate graph\nneural networks to widely successful computer graphics and data approximation\nmodels: radial basis functions (RBFs). We conjecture that, like RBFs, graph\nconvolution layers would benefit from the addition of simple functions to the\npowerful convolution kernels. We introduce affine skip connections, a novel\nbuilding block formed by combining a fully connected layer with any graph\nconvolution operator. We experimentally demonstrate the effectiveness of our\ntechnique and show the improved performance is the consequence of more than the\nincreased number of parameters. Operators equipped with the affine skip\nconnection markedly outperform their base performance on every task we\nevaluated, i.e., shape reconstruction, dense shape correspondence, and graph\nclassification. We hope our simple and effective approach will serve as a solid\nbaseline and help ease future research in graph neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:25:46 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gong", "Shunwang", ""], ["Bahri", "Mehdi", ""], ["Bronstein", "Michael M.", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2004.02673", "submitter": "Michal Nazarczuk", "authors": "Michal Nazarczuk and Krystian Mikolajczyk", "title": "SHOP-VRB: A Visual Reasoning Benchmark for Object Perception", "comments": "International Conference on Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach and a benchmark for visual reasoning in\nrobotics applications, in particular small object grasping and manipulation.\nThe approach and benchmark are focused on inferring object properties from\nvisual and text data. It concerns small household objects with their\nproperties, functionality, natural language descriptions as well as\nquestion-answer pairs for visual reasoning queries along with their\ncorresponding scene semantic representations. We also present a method for\ngenerating synthetic data which allows to extend the benchmark to other objects\nor scenes and propose an evaluation protocol that is more challenging than in\nthe existing datasets. We propose a reasoning system based on symbolic program\nexecution. A disentangled representation of the visual and textual inputs is\nobtained and used to execute symbolic programs that represent a 'reasoning\nprocess' of the algorithm. We perform a set of experiments on the proposed\nbenchmark and compare to results for the state of the art methods. These\nresults expose the shortcomings of the existing benchmarks that may lead to\nmisleading conclusions on the actual performance of the visual reasoning\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:46:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nazarczuk", "Michal", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2004.02677", "submitter": "Charles-Olivier Dufresne Camaro", "authors": "Charles-Olivier Dufresne Camaro, Morteza Rezanejad, Stavros Tsogkas,\n  Kaleem Siddiqi, Sven Dickinson", "title": "Appearance Shock Grammar for Fast Medial Axis Extraction from Real\n  Images", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine ideas from shock graph theory with more recent appearance-based\nmethods for medial axis extraction from complex natural scenes, improving upon\nthe present best unsupervised method, in terms of efficiency and performance.\nWe make the following specific contributions: i) we extend the shock graph\nrepresentation to the domain of real images, by generalizing the shock type\ndefinitions using local, appearance-based criteria; ii) we then use the rules\nof a Shock Grammar to guide our search for medial points, drastically reducing\nrun time when compared to other methods, which exhaustively consider all points\nin the input image;iii) we remove the need for typical post-processing steps\nincluding thinning, non-maximum suppression, and grouping, by adhering to the\nShock Grammar rules while deriving the medial axis solution; iv) finally, we\nraise some fundamental concerns with the evaluation scheme used in previous\nwork and propose a more appropriate alternative for assessing the performance\nof medial axis extraction from scenes. Our experiments on the BMAX500 and\nSK-LARGE datasets demonstrate the effectiveness of our approach. We outperform\nthe present state-of-the-art, excelling particularly in the high-precision\nregime, while running an order of magnitude faster and requiring no\npost-processing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:57:27 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Camaro", "Charles-Olivier Dufresne", ""], ["Rezanejad", "Morteza", ""], ["Tsogkas", "Stavros", ""], ["Siddiqi", "Kaleem", ""], ["Dickinson", "Sven", ""]]}, {"id": "2004.02678", "submitter": "Anyi Rao", "authors": "Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou,\n  Dahua Lin", "title": "A Local-to-Global Approach to Multi-modal Movie Scene Segmentation", "comments": "CVPR2020. Project page: https://anyirao.com/projects/SceneSeg.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene, as the crucial unit of storytelling in movies, contains complex\nactivities of actors and their interactions in a physical environment.\nIdentifying the composition of scenes serves as a critical step towards\nsemantic understanding of movies. This is very challenging -- compared to the\nvideos studied in conventional vision problems, e.g. action recognition, as\nscenes in movies usually contain much richer temporal structures and more\ncomplex semantic information. Towards this goal, we scale up the scene\nsegmentation task by building a large-scale video dataset MovieScenes, which\ncontains 21K annotated scene segments from 150 movies. We further propose a\nlocal-to-global scene segmentation framework, which integrates multi-modal\ninformation across three levels, i.e. clip, segment, and movie. This framework\nis able to distill complex semantics from hierarchical temporal structures over\na long movie, providing top-down guidance for scene segmentation. Our\nexperiments show that the proposed network is able to segment a movie into\nscenes with high accuracy, consistently outperforming previous methods. We also\nfound that pretraining on our MovieScenes can bring significant improvements to\nthe existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:58:08 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:54:57 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 14:30:05 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Rao", "Anyi", ""], ["Xu", "Linning", ""], ["Xiong", "Yu", ""], ["Xu", "Guodong", ""], ["Huang", "Qingqiu", ""], ["Zhou", "Bolei", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.02684", "submitter": "Hao Li", "authors": "Hao Li, Xiaopeng Zhang, Hongkai Xiong, Qi Tian", "title": "Attribute Mix: Semantic Data Augmentation for Fine Grained Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting fine-grained labels usually requires expert-level domain knowledge\nand is prohibitive to scale up. In this paper, we propose Attribute Mix, a data\naugmentation strategy at attribute level to expand the fine-grained samples.\nThe principle lies in that attribute features are shared among fine-grained\nsub-categories, and can be seamlessly transferred among images. Toward this\ngoal, we propose an automatic attribute mining approach to discover attributes\nthat belong to the same super-category, and Attribute Mix is operated by mixing\nsemantically meaningful attribute features from two images. Attribute Mix is a\nsimple but effective data augmentation strategy that can significantly improve\nthe recognition performance without increasing the inference budgets.\nFurthermore, since attributes can be shared among images from the same\nsuper-category, we further enrich the training samples with attribute level\nlabels using images from the generic domain. Experiments on widely used\nfine-grained benchmarks demonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:06:47 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 09:52:23 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Li", "Hao", ""], ["Zhang", "Xiaopeng", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "2004.02688", "submitter": "Davide Mazzini", "authors": "Alessio Elmi, Davide Mazzini and Pietro Tortella", "title": "Light3DPose: Real-time Multi-Person 3D PoseEstimation from Multiple\n  Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to perform 3D pose estimation of multiple people from\na few calibrated camera views. Our architecture, leveraging the recently\nproposed unprojection layer, aggregates feature-maps from a 2D pose estimator\nbackbone into a comprehensive representation of the 3D scene. Such intermediate\nrepresentation is then elaborated by a fully-convolutional volumetric network\nand a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our\nmethod achieves state of the art MPJPE on the CMU Panoptic dataset using a few\nunseen views and obtains competitive results even with a single input view. We\nalso assess the transfer learning capabilities of the model by testing it\nagainst the publicly available Shelf dataset obtaining good performance\nmetrics. The proposed method is inherently efficient: as a pure bottom-up\napproach, it is computationally independent of the number of people in the\nscene. Furthermore, even though the computational burden of the 2D part scales\nlinearly with the number of input views, the overall architecture is able to\nexploit a very lightweight 2D backbone which is orders of magnitude faster than\nthe volumetric counterpart, resulting in fast inference time. The system can\nrun at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:12:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Elmi", "Alessio", ""], ["Mazzini", "Davide", ""], ["Tortella", "Pietro", ""]]}, {"id": "2004.02693", "submitter": "David Griffiths Mr", "authors": "David Griffiths, Jan Boehm, Tobias Ritschel", "title": "Finding Your (3D) Center: 3D Object Detection Using a Learned Loss", "comments": "19 pages, 8 figures, Accepted ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive semantically labeled datasets are readily available for 2D images,\nhowever, are much harder to achieve for 3D scenes. Objects in 3D repositories\nlike ShapeNet are labeled, but regrettably only in isolation, so without\ncontext. 3D scenes can be acquired by range scanners on city-level scale, but\nmuch fewer with semantic labels. Addressing this disparity, we introduce a new\noptimization procedure, which allows training for 3D detection with raw 3D\nscans while using as little as 5% of the object labels and still achieve\ncomparable performance. Our optimization uses two networks. A scene network\nmaps an entire 3D scene to a set of 3D object centers. As we assume the scene\nnot to be labeled by centers, no classic loss, such as Chamfer can be used to\ntrain it. Instead, we use another network to emulate the loss. This loss\nnetwork is trained on a small labeled subset and maps a non centered 3D object\nin the presence of distractions to its own center. This function is very\nsimilar - and hence can be used instead of - the gradient the supervised loss\nwould provide. Our evaluation documents competitive fidelity at a much lower\nlevel of supervision, respectively higher quality at comparable supervision.\nSupplementary material can be found at: https://dgriffiths3.github.io.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:17:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:39:27 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Griffiths", "David", ""], ["Boehm", "Jan", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2004.02696", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Shahin Heidarian, Farnoosh Naderkhani, Anastasia\n  Oikonomou, Konstantinos N. Plataniotis, and Arash Mohammadi", "title": "COVID-CAPS: A Capsule Network-based Framework for Identification of\n  COVID-19 cases from X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel Coronavirus disease (COVID-19) has abruptly and undoubtedly changed the\nworld as we know it at the end of the 2nd decade of the 21st century. COVID-19\nis extremely contagious and quickly spreading globally making its early\ndiagnosis of paramount importance. Early diagnosis of COVID-19 enables health\ncare professionals and government authorities to break the chain of transition\nand flatten the epidemic curve. The common type of COVID-19 diagnosis test,\nhowever, requires specific equipment and has relatively low sensitivity.\nComputed tomography (CT) scans and X-ray images, on the other hand, reveal\nspecific manifestations associated with this disease. Overlap with other lung\ninfections makes human-centered diagnosis of COVID-19 challenging.\nConsequently, there has been an urgent surge of interest to develop Deep Neural\nNetwork (DNN)-based diagnosis solutions, mainly based on Convolutional Neural\nNetworks (CNNs), to facilitate identification of positive COVID-19 cases. CNNs,\nhowever, are prone to lose spatial information between image instances and\nrequire large datasets. The paper presents an alternative modeling framework\nbased on Capsule Networks, referred to as the COVID-CAPS, being capable of\nhandling small datasets, which is of significant importance due to sudden and\nrapid emergence of COVID-19. Our results based on a dataset of X-ray images\nshow that COVID-CAPS has advantage over previous CNN-based models. COVID-CAPS\nachieved an Accuracy of 95.7%, Sensitivity of 90%, Specificity of 95.8%, and\nArea Under the Curve (AUC) of 0.97, while having far less number of trainable\nparameters in comparison to its counterparts. To further improve diagnosis\ncapabilities of the COVID-CAPS, pre-training based on a new dataset constructed\nfrom an external dataset of X-ray images. Pre-training with a dataset of\nsimilar nature further improved accuracy to 98.3% and specificity to 98.6%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:20:47 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 22:05:31 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Afshar", "Parnian", ""], ["Heidarian", "Shahin", ""], ["Naderkhani", "Farnoosh", ""], ["Oikonomou", "Anastasia", ""], ["Plataniotis", "Konstantinos N.", ""], ["Mohammadi", "Arash", ""]]}, {"id": "2004.02707", "submitter": "Yicong Hong", "authors": "Yicong Hong, Cristian Rodriguez-Opazo, Qi Wu, Stephen Gould", "title": "Sub-Instruction Aware Vision-and-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language navigation requires an agent to navigate through a real\n3D environment following natural language instructions. Despite significant\nadvances, few previous works are able to fully utilize the strong\ncorrespondence between the visual and textual sequences. Meanwhile, due to the\nlack of intermediate supervision, the agent's performance at following each\npart of the instruction cannot be assessed during navigation. In this work, we\nfocus on the granularity of the visual and language sequences as well as the\ntraceability of agents through the completion of an instruction. We provide\nagents with fine-grained annotations during training and find that they are\nable to follow the instruction better and have a higher chance of reaching the\ntarget at test time. We enrich the benchmark dataset Room-to-Room (R2R) with\nsub-instructions and their corresponding paths. To make use of this data, we\npropose effective sub-instruction attention and shifting modules that select\nand attend to a single sub-instruction at each time-step. We implement our\nsub-instruction modules in four state-of-the-art agents, compare with their\nbaseline models, and show that our proposed method improves the performance of\nall four agents.\n  We release the Fine-Grained R2R dataset (FGR2R) and the code at\nhttps://github.com/YicongHong/Fine-Grained-R2R.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:44:53 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 05:14:29 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hong", "Yicong", ""], ["Rodriguez-Opazo", "Cristian", ""], ["Wu", "Qi", ""], ["Gould", "Stephen", ""]]}, {"id": "2004.02711", "submitter": "Bernhard Egger", "authors": "William A.P. Smith, Alassane Seck, Hannah Dee, Bernard Tiddeman,\n  Joshua Tenenbaum, Bernhard Egger", "title": "A Morphable Face Albedo Model", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we bring together two divergent strands of research:\nphotometric face capture and statistical 3D face appearance modelling. We\npropose a novel lightstage capture and processing pipeline for acquiring\near-to-ear, truly intrinsic diffuse and specular albedo maps that fully factor\nout the effects of illumination, camera and geometry. Using this pipeline, we\ncapture a dataset of 50 scans and combine them with the only existing publicly\navailable albedo dataset (3DRFE) of 23 scans. This allows us to build the first\nmorphable face albedo model. We believe this is the first statistical analysis\nof the variability of facial specular albedo maps. This model can be used as a\nplug in replacement for the texture model of the Basel Face Model (BFM) or\nFLAME and we make the model publicly available. We ensure careful spectral\ncalibration such that our model is built in a linear sRGB space, suitable for\ninverse rendering of images taken by typical cameras. We demonstrate our model\nin a state of the art analysis-by-synthesis 3DMM fitting pipeline, are the\nfirst to integrate specular map estimation and outperform the BFM in albedo\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:49:40 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 13:39:22 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Smith", "William A. P.", ""], ["Seck", "Alassane", ""], ["Dee", "Hannah", ""], ["Tiddeman", "Bernard", ""], ["Tenenbaum", "Joshua", ""], ["Egger", "Bernhard", ""]]}, {"id": "2004.02724", "submitter": "Tai Wang", "authors": "Tai Wang, Xinge Zhu, Dahua Lin", "title": "Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds", "comments": "Conference on Robot Learning (CoRL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  LiDAR is an important method for autonomous driving systems to sense the\nenvironment. The point clouds obtained by LiDAR typically exhibit sparse and\nirregular distribution, thus posing great challenges to the detection of 3D\nobjects, especially those that are small and distant. To tackle this\ndifficulty, we propose Reconfigurable Voxels, a new approach to constructing\nrepresentations from 3D point clouds. Specifically, we devise a biased random\nwalk scheme, which adaptively covers each neighborhood with a fixed number of\nvoxels based on the local spatial distribution and produces a representation by\nintegrating the points in the chosen neighbors. We found empirically that this\napproach effectively improves the stability of voxel features, especially for\nsparse regions. Experimental results on multiple benchmarks, including\nnuScenes, Lyft, and KITTI, show that this new representation can remarkably\nimprove the detection performance for small and distant objects, without\nincurring noticeable overhead costs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:07:16 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 11:16:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wang", "Tai", ""], ["Zhu", "Xinge", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.02731", "submitter": "Feng Shi", "authors": "Feng Shi, Jun Wang, Jun Shi, Ziyan Wu, Qian Wang, Zhenyu Tang, Kelei\n  He, Yinghuan Shi, Dinggang Shen", "title": "Review of Artificial Intelligence Techniques in Imaging Data\n  Acquisition, Segmentation and Diagnosis for COVID-19", "comments": "Added journal submission info", "journal-ref": "IEEE Reviews in Biomedical Engineering (2020)", "doi": "10.1109/RBME.2020.2987975", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (This paper was submitted as an invited paper to IEEE Reviews in Biomedical\nEngineering on April 6, 2020.) The pandemic of coronavirus disease 2019\n(COVID-19) is spreading all over the world. Medical imaging such as X-ray and\ncomputed tomography (CT) plays an essential role in the global fight against\nCOVID-19, whereas the recently emerging artificial intelligence (AI)\ntechnologies further strengthen the power of the imaging tools and help medical\nspecialists. We hereby review the rapid responses in the community of medical\nimaging (empowered by AI) toward COVID-19. For example, AI-empowered image\nacquisition can significantly help automate the scanning procedure and also\nreshape the workflow with minimal contact to patients, providing the best\nprotection to the imaging technicians. Also, AI can improve work efficiency by\naccurate delination of infections in X-ray and CT images, facilitating\nsubsequent quantification. Moreover, the computer-aided platforms help\nradiologists make clinical decisions, i.e., for disease diagnosis, tracking,\nand prognosis. In this review paper, we thus cover the entire pipeline of\nmedical imaging and analysis techniques involved with COVID-19, including image\nacquisition, segmentation, diagnosis, and follow-up. We particularly focus on\nthe integration of AI with X-ray and CT, both of which are widely used in the\nfrontline hospitals, in order to depict the latest progress of medical imaging\nand radiology fighting against COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:21:34 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 08:18:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Shi", "Feng", ""], ["Wang", "Jun", ""], ["Shi", "Jun", ""], ["Wu", "Ziyan", ""], ["Wang", "Qian", ""], ["Tang", "Zhenyu", ""], ["He", "Kelei", ""], ["Shi", "Yinghuan", ""], ["Shen", "Dinggang", ""]]}, {"id": "2004.02747", "submitter": "Frank Mancolo", "authors": "Frank Mancolo", "title": "Eisen: a python package for solid deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eisen is an open source python package making the implementation of deep\nlearning methods easy. It is specifically tailored to medical image analysis\nand computer vision tasks, but its flexibility allows extension to any\napplication. Eisen is based on PyTorch and it follows the same architecture of\nother packages belonging to the PyTorch ecosystem. This simplifies its use and\nallows it to be compatible with modules provided by other packages. Eisen\nimplements multiple dataset loading methods, I/O for various data formats, data\nmanipulation and transformation, full implementation of training, validation\nand test loops, implementation of losses and network architectures, automatic\nexport of training artifacts, summaries and logs, visual experiment building,\ncommand line interface and more. Furthermore, it is open to user contributions\nby the community. Documentation, examples and code can be downloaded from\nhttp://eisen.ai.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:07:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mancolo", "Frank", ""]]}, {"id": "2004.02748", "submitter": "Shreya Roy", "authors": "Shreya Roy and Anirban Chakraborty", "title": "Semantic Segmentation of highly class imbalanced fully labelled 3D\n  volumetric biomedical images and unsupervised Domain Adaptation of the\n  pre-trained Segmentation Network to segment another fully unlabelled\n  Biomedical 3D Image stack", "comments": "6 pages and 6 figures. Submitting to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our work is to perform pixel label semantic segmentation on 3D\nbiomedical volumetric data. Manual annotation is always difficult for a large\nbio-medical dataset. So, we consider two cases where one dataset is fully\nlabeled and the other dataset is assumed to be fully unlabelled. We first\nperform Semantic Segmentation on the fully labeled isotropic biomedical source\ndata (FIBSEM) and try to incorporate the the trained model for segmenting the\ntarget unlabelled dataset(SNEMI3D)which shares some similarities with the\nsource dataset in the context of different types of cellular bodies and other\ncellular components. Although, the cellular components vary in size and shape.\nSo in this paper, we have proposed a novel approach in the context of\nunsupervised domain adaptation while classifying each pixel of the target\nvolumetric data into cell boundary and cell body. Also, we have proposed a\nnovel approach to giving non-uniform weights to different pixels in the\ntraining images while performing the pixel-level semantic segmentation in the\npresence of the corresponding pixel-wise label map along with the training\noriginal images in the source domain. We have used the Entropy Map or a\nDistance Transform matrix retrieved from the given ground truth label map which\nhas helped to overcome the class imbalance problem in the medical image data\nwhere the cell boundaries are extremely thin and hence, extremely prone to be\nmisclassified as non-boundary.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 06:01:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Roy", "Shreya", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2004.02753", "submitter": "Joshua Knights Mr", "authors": "Joshua Knights, Ben Harwood, Daniel Ward, Anthony Vanderkop, Olivia\n  Mackenzie-Ross, Peyman Moghadam", "title": "Temporally Coherent Embeddings for Self-Supervised Video Representation\n  Learning", "comments": "Accepted at ICPR 2020. Project page:\n  https://csiro-robotics.github.io/TCE-Webpage/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TCE: Temporally Coherent Embeddings for self-supervised\nvideo representation learning. The proposed method exploits inherent structure\nof unlabeled video data to explicitly enforce temporal coherency in the\nembedding space, rather than indirectly learning it through ranking or\npredictive proxy tasks. In the same way that high-level visual information in\nthe world changes smoothly, we believe that nearby frames in learned\nrepresentations will benefit from demonstrating similar properties. Using this\nassumption, we train our TCE model to encode videos such that adjacent frames\nexist close to each other and videos are separated from one another. Using TCE\nwe learn robust representations from large quantities of unlabeled video data.\nWe thoroughly analyse and evaluate our self-supervised learned TCE models on a\ndownstream task of video action recognition using multiple challenging\nbenchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN\nbackbone and only RGB stream inputs, TCE pre-trained representations outperform\nall previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code\nand pre-trained models for this paper can be downloaded at:\nhttps://github.com/csiro-robotics/TCE\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 12:25:50 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 00:24:07 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 09:03:55 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 05:48:04 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 04:21:35 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Knights", "Joshua", ""], ["Harwood", "Ben", ""], ["Ward", "Daniel", ""], ["Vanderkop", "Anthony", ""], ["Mackenzie-Ross", "Olivia", ""], ["Moghadam", "Peyman", ""]]}, {"id": "2004.02755", "submitter": "Dingkang Wang", "authors": "Dingkang Wang, Lucas Magee, Bing-Xing Huo, Samik Banerjee, Xu Li,\n  Jaikishan Jayakumar, Meng Kuan Lin, Keerthi Ram, Suyi Wang, Yusu Wang, Partha\n  P. Mitra", "title": "Detection and skeletonization of single neurons and tracer injections\n  using topological methods", "comments": "20 pages (14 pages main-text and 6 pages supplementary information).\n  5 main-text figures. 5 supplementary figures. 2 supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neuroscientific data analysis has traditionally relied on linear algebra and\nstochastic process theory. However, the tree-like shapes of neurons cannot be\ndescribed easily as points in a vector space (the subtraction of two neuronal\nshapes is not a meaningful operation), and methods from computational topology\nare better suited to their analysis. Here we introduce methods from Discrete\nMorse (DM) Theory to extract the tree-skeletons of individual neurons from\nvolumetric brain image data, and to summarize collections of neurons labelled\nby tracer injections. Since individual neurons are topologically trees, it is\nsensible to summarize the collection of neurons using a consensus tree-shape\nthat provides a richer information summary than the traditional regional\n'connectivity matrix' approach. The conceptually elegant DM approach lacks\nhand-tuned parameters and captures global properties of the data as opposed to\nprevious approaches which are inherently local. For individual skeletonization\nof sparsely labelled neurons we obtain substantial performance gains over\nstate-of-the-art non-topological methods (over 10% improvements in precision\nand faster proofreading). The consensus-tree summary of tracer injections\nincorporates the regional connectivity matrix information, but in addition\ncaptures the collective collateral branching patterns of the set of neurons\nconnected to the injection site, and provides a bridge between single-neuron\nmorphology and tracer-injection data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 20:58:38 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Dingkang", ""], ["Magee", "Lucas", ""], ["Huo", "Bing-Xing", ""], ["Banerjee", "Samik", ""], ["Li", "Xu", ""], ["Jayakumar", "Jaikishan", ""], ["Lin", "Meng Kuan", ""], ["Ram", "Keerthi", ""], ["Wang", "Suyi", ""], ["Wang", "Yusu", ""], ["Mitra", "Partha P.", ""]]}, {"id": "2004.02756", "submitter": "Qinkai Zheng", "authors": "Qinkai Zheng, Han Qiu, Gerard Memmi, Isabelle Bloch", "title": "Investigating Image Applications Based on Spatial-Frequency Transform\n  and Deep Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the report for the PRIM project in Telecom Paris. This report is\nabout applications based on spatial-frequency transform and deep learning\ntechniques. In this report, there are two main works. The first work is about\nthe enhanced JPEG compression method based on deep learning. we propose a novel\nmethod to highly enhance the JPEG compression by transmitting fewer image data\nat the sender's end. At the receiver's end, we propose a DC recovery algorithm\ntogether with the deep residual learning framework to recover images with high\nquality. The second work is about adversarial examples defenses based on signal\nprocessing. We propose the wavelet extension method to extend image data\nfeatures, which makes it more difficult to generate adversarial examples. We\nfurther adopt wavelet denoising to reduce the influence of the adversarial\nperturbations. With intensive experiments, we demonstrate that both works are\neffective in their application scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 17:34:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zheng", "Qinkai", ""], ["Qiu", "Han", ""], ["Memmi", "Gerard", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2004.02757", "submitter": "Yuanhan Mo", "authors": "Yuanhan Mo and Shuo Wang and Chengliang Dai and Rui Zhou and Zhongzhao\n  Teng and Wenjia Bai and Yike Guo", "title": "Efficient Deep Representation Learning by Adaptive Latent Space Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning requires a large amount of training samples with\nannotations (e.g. label class for classification task, pixel- or voxel-wised\nlabel map for segmentation tasks), which are expensive and time-consuming to\nobtain. During the training of a deep neural network, the annotated samples are\nfed into the network in a mini-batch way, where they are often regarded of\nequal importance. However, some of the samples may become less informative\nduring training, as the magnitude of the gradient start to vanish for these\nsamples. In the meantime, other samples of higher utility or hardness may be\nmore demanded for the training process to proceed and require more\nexploitation. To address the challenges of expensive annotations and loss of\nsample informativeness, here we propose a novel training framework which\nadaptively selects informative samples that are fed to the training process.\nThe adaptive selection or sampling is performed based on a hardness-aware\nstrategy in the latent space constructed by a generative model. To evaluate the\nproposed training framework, we perform experiments on three different\ndatasets, including MNIST and CIFAR-10 for image classification task and a\nmedical image dataset IVUS for biophysical simulation task. On all three\ndatasets, the proposed framework outperforms a random sampling method, which\ndemonstrates the effectiveness of proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 22:17:02 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 18:25:55 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Mo", "Yuanhan", ""], ["Wang", "Shuo", ""], ["Dai", "Chengliang", ""], ["Zhou", "Rui", ""], ["Teng", "Zhongzhao", ""], ["Bai", "Wenjia", ""], ["Guo", "Yike", ""]]}, {"id": "2004.02758", "submitter": "Anthony Griffin", "authors": "Farah Sarwar, Anthony Griffin, Saeed Ur Rehman, and Timotius Pasang", "title": "Towards Detection of Sheep Onboard a UAV", "comments": "This was accepted for publication and presentation at the Embedded AI\n  for Real-time Machine Vision 2019 in conjunction with the British Machine\n  Vision Conference (BMVC) 2019. It was presented on 12 September 2019 in\n  Cardiff, Wales. 10 pages, 3 figures, and 1 table, note that this is a\n  web-friendly format as used at BMVC, so the pages are about A5 size (but not\n  exactly!)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the task of detecting sheep onboard an unmanned\naerial vehicle (UAV) flying at an altitude of 80 m. At this height, the sheep\nare relatively small, only about 15 pixels across. Although deep learning\nstrategies have gained enormous popularity in the last decade and are now\nextensively used for object detection in many fields, state-of-the-art\ndetectors perform poorly in the case of smaller objects. We develop a novel\ndataset of UAV imagery of sheep and consider a variety of object detectors to\ndetermine which is the most suitable for our task in terms of both accuracy and\nspeed. Our findings indicate that a UNet detector using the weighted Hausdorff\ndistance as a loss function during training is an excellent option for\ndetection of sheep onboard a UAV.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 00:40:48 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sarwar", "Farah", ""], ["Griffin", "Anthony", ""], ["Rehman", "Saeed Ur", ""], ["Pasang", "Timotius", ""]]}, {"id": "2004.02760", "submitter": "Lam Huynh", "authors": "Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, Janne Heikkila", "title": "Guiding Monocular Depth Estimation Using Depth-Attention Volume", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the scene depth from a single image is an ill-posed problem that\nrequires additional priors, often referred to as monocular depth cues, to\ndisambiguate different 3D interpretations. In recent works, those priors have\nbeen learned in an end-to-end manner from large datasets by using deep neural\nnetworks. In this paper, we propose guiding depth estimation to favor planar\nstructures that are ubiquitous especially in indoor environments. This is\nachieved by incorporating a non-local coplanarity constraint to the network\nwith a novel attention mechanism called depth-attention volume (DAV).\nExperiments on two popular indoor datasets, namely NYU-Depth-v2 and ScanNet,\nshow that our method achieves state-of-the-art depth estimation results while\nusing only a fraction of the number of parameters needed by the competing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:45:52 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 16:22:27 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Huynh", "Lam", ""], ["Nguyen-Ha", "Phong", ""], ["Matas", "Jiri", ""], ["Rahtu", "Esa", ""], ["Heikkila", "Janne", ""]]}, {"id": "2004.02767", "submitter": "Zhengsu Chen", "authors": "Zhengsu Chen, Jianwei Niu, Lingxi Xie, Xuefeng Liu, Longhui Wei, Qi\n  Tian", "title": "Network Adjustment: Channel Search Guided by FLOPs Utilization Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic designing computationally efficient neural networks has received\nmuch attention in recent years. Existing approaches either utilize network\npruning or leverage the network architecture search methods. This paper\npresents a new framework named network adjustment, which considers network\naccuracy as a function of FLOPs, so that under each network configuration, one\ncan estimate the FLOPs utilization ratio (FUR) for each layer and use it to\ndetermine whether to increase or decrease the number of channels on the layer.\nNote that FUR, like the gradient of a non-linear function, is accurate only in\na small neighborhood of the current network. Hence, we design an iterative\nmechanism so that the initial network undergoes a number of steps, each of\nwhich has a small `adjusting rate' to control the changes to the network. The\ncomputational overhead of the entire search process is reasonable, i.e.,\ncomparable to that of re-training the final model from scratch. Experiments on\nstandard image classification datasets and a wide range of base networks\ndemonstrate the effectiveness of our approach, which consistently outperforms\nthe pruning counterpart. The code is available at\nhttps://github.com/danczs/NetworkAdjustment.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 15:51:00 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chen", "Zhengsu", ""], ["Niu", "Jianwei", ""], ["Xie", "Lingxi", ""], ["Liu", "Xuefeng", ""], ["Wei", "Longhui", ""], ["Tian", "Qi", ""]]}, {"id": "2004.02774", "submitter": "Xinge Zhu", "authors": "Xinge Zhu, Yuexin Ma, Tai Wang, Yan Xu, Jianping Shi, Dahua Lin", "title": "SSN: Shape Signature Networks for Multi-class Object Detection from\n  Point Clouds", "comments": "Code is available at https://github.com/xinge008/SSN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-class 3D object detection aims to localize and classify objects of\nmultiple categories from point clouds. Due to the nature of point clouds, i.e.\nunstructured, sparse and noisy, some features benefit-ting multi-class\ndiscrimination are underexploited, such as shape information. In this paper, we\npropose a novel 3D shape signature to explore the shape information from point\nclouds. By incorporating operations of symmetry, convex hull and chebyshev\nfitting, the proposed shape sig-nature is not only compact and effective but\nalso robust to the noise, which serves as a soft constraint to improve the\nfeature capability of multi-class discrimination. Based on the proposed shape\nsignature, we develop the shape signature networks (SSN) for 3D object\ndetection, which consist of pyramid feature encoding part, shape-aware grouping\nheads and explicit shape encoding objective. Experiments show that the proposed\nmethod performs remarkably better than existing methods on two large-scale\ndatasets. Furthermore, our shape signature can act as a plug-and-play component\nand ablation study shows its effectiveness and good scalability\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:01:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhu", "Xinge", ""], ["Ma", "Yuexin", ""], ["Wang", "Tai", ""], ["Xu", "Yan", ""], ["Shi", "Jianping", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.02782", "submitter": "Hugo Proen\\c{c}a", "authors": "S.V. Aruna Kumar, Ehsan Yaghoubi, Abhijit Das, B.S. Harish and Hugo\n  Proen\\c{c}a", "title": "The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection,\n  Tracking, Re-Identification and Search from Aerial Devices", "comments": "11 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, the world has been witnessing growing threats to the\nsecurity in urban spaces, which has augmented the relevance given to visual\nsurveillance solutions able to detect, track and identify persons of interest\nin crowds. In particular, unmanned aerial vehicles (UAVs) are a potential tool\nfor this kind of analysis, as they provide a cheap way for data collection,\ncover large and difficult-to-reach areas, while reducing human staff demands.\nIn this context, all the available datasets are exclusively suitable for the\npedestrian re-identification problem, in which the multi-camera views per ID\nare taken on a single day, and allows the use of clothing appearance features\nfor identification purposes. Accordingly, the main contributions of this paper\nare two-fold: 1) we announce the UAV-based P-DESTRE dataset, which is the first\nof its kind to provide consistent ID annotations across multiple days, making\nit suitable for the extremely challenging problem of person search, i.e., where\nno clothing information can be reliably used. Apart this feature, the P-DESTRE\nannotations enable the research on UAV-based pedestrian detection, tracking,\nre-identification and soft biometric solutions; and 2) we compare the results\nattained by state-of-the-art pedestrian detection, tracking, reidentification\nand search techniques in well-known surveillance datasets, to the effectiveness\nobtained by the same techniques in the P-DESTRE data. Such comparison enables\nto identify the most problematic data degradation factors of UAV-based data for\neach task, and can be used as baselines for subsequent advances in this kind of\ntechnology. The dataset and the full details of the empirical evaluation\ncarried out are freely available at http://p-destre.di.ubi.pt/.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:17:32 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kumar", "S. V. Aruna", ""], ["Yaghoubi", "Ehsan", ""], ["Das", "Abhijit", ""], ["Harish", "B. S.", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2004.02786", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Adaptive Partial Scanning Transmission Electron Microscopy with\n  Reinforcement Learning", "comments": "13 pages, 3 figures + 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed sensing can decrease scanning transmission electron microscopy\nelectron dose and scan time with minimal information loss. Traditionally,\nsparse scans used in compressed sensing sample a static set of probing\nlocations. However, dynamic scans that adapt to specimens are expected to be\nable to match or surpass the performance of static scans as static scans are a\nsubset of possible dynamic scans. Thus, we present a prototype for a contiguous\nsparse scan system that piecewise adapts scan paths to specimens as they are\nscanned. Sampling directions for scan segments are chosen by a recurrent neural\nnetwork based on previously observed scan segments. The recurrent neural\nnetwork is trained by reinforcement learning to cooperate with a feedforward\nconvolutional neural network that completes the sparse scans. This paper\npresents our learning policy, experiments, and example partial scans, and\ndiscusses future research directions. Source code, pretrained models, and\ntraining data is openly accessible at\nhttps://github.com/Jeffrey-Ede/adaptive-scans\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:25:38 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 13:05:52 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 16:06:00 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 14:26:08 GMT"}, {"version": "v5", "created": "Mon, 1 Mar 2021 11:11:30 GMT"}, {"version": "v6", "created": "Wed, 3 Mar 2021 12:15:56 GMT"}, {"version": "v7", "created": "Mon, 8 Mar 2021 18:09:48 GMT"}, {"version": "v8", "created": "Thu, 11 Mar 2021 16:55:09 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "2004.02788", "submitter": "Xiaohang Zhan", "authors": "Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, Chen Change\n  Loy", "title": "Self-Supervised Scene De-occlusion", "comments": "Accepted to CVPR 2020 as oral. Project page:\n  https://xiaohangzhan.github.io/projects/deocclusion/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural scene understanding is a challenging task, particularly when\nencountering images of multiple objects that are partially occluded. This\nobstacle is given rise by varying object ordering and positioning. Existing\nscene understanding paradigms are able to parse only the visible parts,\nresulting in incomplete and unstructured scene interpretation. In this paper,\nwe investigate the problem of scene de-occlusion, which aims to recover the\nunderlying occlusion ordering and complete the invisible parts of occluded\nobjects. We make the first attempt to address the problem through a novel and\nunified framework that recovers hidden scene structures without ordering and\namodal annotations as supervisions. This is achieved via Partial Completion\nNetwork (PCNet)-mask (M) and -content (C), that learn to recover fractions of\nobject masks and contents, respectively, in a self-supervised manner. Based on\nPCNet-M and PCNet-C, we devise a novel inference scheme to accomplish scene\nde-occlusion, via progressive ordering recovery, amodal completion and content\ncompletion. Extensive experiments on real-world scenes demonstrate the superior\nperformance of our approach to other alternatives. Remarkably, our approach\nthat is trained in a self-supervised manner achieves comparable results to\nfully-supervised methods. The proposed scene de-occlusion framework benefits\nmany applications, including high-quality and controllable image manipulation\nand scene recomposition (see Fig. 1), as well as the conversion of existing\nmodal mask annotations to amodal mask annotations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:31:11 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhan", "Xiaohang", ""], ["Pan", "Xingang", ""], ["Dai", "Bo", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""]]}, {"id": "2004.02803", "submitter": "Yingqian Wang", "authors": "Xinyi Ying, Longguang Wang, Yingqian Wang, Weidong Sheng, Wei An,\n  Yulan Guo", "title": "Deformable 3D Convolution for Video Super-Resolution", "comments": "Accepted by IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.3013518", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatio-temporal information among video sequences is significant for\nvideo super-resolution (SR). However, the spatio-temporal information cannot be\nfully used by existing video SR methods since spatial feature extraction and\ntemporal motion compensation are usually performed sequentially. In this paper,\nwe propose a deformable 3D convolution network (D3Dnet) to incorporate\nspatio-temporal information from both spatial and temporal dimensions for video\nSR. Specifically, we introduce deformable 3D convolution (D3D) to integrate\ndeformable convolution with 3D convolution, obtaining both superior\nspatio-temporal modeling capability and motion-aware modeling flexibility.\nExtensive experiments have demonstrated the effectiveness of D3D in exploiting\nspatio-temporal information. Comparative results show that our network achieves\nstate-of-the-art SR performance. Code is available at:\nhttps://github.com/XinyiYing/D3Dnet.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:49:48 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 14:08:03 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 14:18:25 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 11:58:50 GMT"}, {"version": "v5", "created": "Sat, 15 Aug 2020 14:45:44 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ying", "Xinyi", ""], ["Wang", "Longguang", ""], ["Wang", "Yingqian", ""], ["Sheng", "Weidong", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2004.02804", "submitter": "Akrem Sellami", "authors": "Akrem Sellami (QARMA, LIS, INT), Fran\\c{c}ois-Xavier Dup\\'e (QARMA,\n  LIS), Bastien Cagna (INT), Hachem Kadri (QARMA, LIS), St\\'ephane Ayache\n  (QARMA, LIS), Thierry Arti\\`eres (QARMA, LIS, ECM), Sylvain Takerkart (INT)", "title": "Mapping individual differences in cortical architecture using multi-view\n  representation learning", "comments": null, "journal-ref": "IJCNN 2020 - International Joint Conference on Neural Networks,\n  Jul 2020, Glasgow, United Kingdom", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, understanding inter-individual differences has recently\nemerged as a major challenge, for which functional magnetic resonance imaging\n(fMRI) has proven invaluable. For this, neuroscientists rely on basic methods\nsuch as univariate linear correlations between single brain features and a\nscore that quantifies either the severity of a disease or the subject's\nperformance in a cognitive task. However, to this date, task-fMRI and\nresting-state fMRI have been exploited separately for this question, because of\nthe lack of methods to effectively combine them. In this paper, we introduce a\nnovel machine learning method which allows combining the activation-and\nconnectivity-based information respectively measured through these two fMRI\nprotocols to identify markers of individual differences in the functional\norganization of the brain. It combines a multi-view deep autoencoder which is\ndesigned to fuse the two fMRI modalities into a joint representation space\nwithin which a predictive model is trained to guess a scalar score that\ncharacterizes the patient. Our experimental results demonstrate the ability of\nthe proposed method to outperform competitive approaches and to produce\ninterpretable and biologically plausible results.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:01:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sellami", "Akrem", "", "QARMA, LIS, INT"], ["Dup\u00e9", "Fran\u00e7ois-Xavier", "", "QARMA,\n  LIS"], ["Cagna", "Bastien", "", "INT"], ["Kadri", "Hachem", "", "QARMA, LIS"], ["Ayache", "St\u00e9phane", "", "QARMA, LIS"], ["Arti\u00e8res", "Thierry", "", "QARMA, LIS, ECM"], ["Takerkart", "Sylvain", "", "INT"]]}, {"id": "2004.02805", "submitter": "Weiya Fan", "authors": "Rui Nie (2), Huan Yang (1), Hejuan Peng (2), Wenbin Luo (2), Weiya Fan\n  (2), Jie Zhang (2), Jing Liao (2), Fang Huang (2), Yufeng Xiao (1) ((1)\n  Depatment of Gastroenterology, Second Affiliated Hospital, Army Medical\n  University (Third Military Medical University), Chongqing, China. (2)\n  Chongqing Jinshan Science & Technology (Group) Co., Ltd., Chongqing, China.)", "title": "Application of Structural Similarity Analysis of Visually Salient Areas\n  and Hierarchical Clustering in the Screening of Similar Wireless Capsule\n  Endoscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small intestinal capsule endoscopy is the mainstream method for inspecting\nsmall intestinal lesions,but a single small intestinal capsule endoscopy will\nproduce 60,000 - 120,000 images, the majority of which are similar and have no\ndiagnostic value. It takes 2 - 3 hours for doctors to identify lesions from\nthese images. This is time-consuming and increase the probability of\nmisdiagnosis and missed diagnosis since doctors are likely to experience visual\nfatigue while focusing on a large number of similar images for an extended\nperiod of time.In order to solve these problems, we proposed a similar wireless\ncapsule endoscope (WCE) image screening method based on structural similarity\nanalysis and the hierarchical clustering of visually salient sub-image blocks.\nThe similarity clustering of images was automatically identified by\nhierarchical clustering based on the hue,saturation,value (HSV) spatial color\ncharacteristics of the images,and the keyframe images were extracted based on\nthe structural similarity of the visually salient sub-image blocks, in order to\naccurately identify and screen out similar small intestinal capsule endoscopic\nimages. Subsequently, the proposed method was applied to the capsule endoscope\nimaging workstation. After screening out similar images in the complete data\ngathered by the Type I OMOM Small Intestinal Capsule Endoscope from 52 cases\ncovering 17 common types of small intestinal lesions, we obtained a lesion\nrecall of 100% and an average similar image reduction ratio of 76%. With\nsimilar images screened out, the average play time of the OMOM image\nworkstation was 18 minutes, which greatly reduced the time spent by doctors\nviewing the images.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 09:03:33 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Nie", "Rui", ""], ["Yang", "Huan", ""], ["Peng", "Hejuan", ""], ["Luo", "Wenbin", ""], ["Fan", "Weiya", ""], ["Zhang", "Jie", ""], ["Liao", "Jing", ""], ["Huang", "Fang", ""], ["Xiao", "Yufeng", ""]]}, {"id": "2004.02806", "submitter": "Zewen Li", "authors": "Zewen Li, Wenjie Yang, Shouheng Peng, Fan Liu", "title": "A Survey of Convolutional Neural Networks: Analysis, Applications, and\n  Prospects", "comments": "21 pages, 33 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) is one of the most significant networks in\nthe deep learning field. Since CNN made impressive achievements in many areas,\nincluding but not limited to computer vision and natural language processing,\nit attracted much attention both of industry and academia in the past few\nyears. The existing reviews mainly focus on the applications of CNN in\ndifferent scenarios without considering CNN from a general perspective, and\nsome novel ideas proposed recently are not covered. In this review, we aim to\nprovide novel ideas and prospects in this fast-growing field as much as\npossible. Besides, not only two-dimensional convolution but also\none-dimensional and multi-dimensional ones are involved. First, this review\nstarts with a brief introduction to the history of CNN. Second, we provide an\noverview of CNN. Third, classic and advanced CNN models are introduced,\nespecially those key points making them reach state-of-the-art results. Fourth,\nthrough experimental analysis, we draw some conclusions and provide several\nrules of thumb for function selection. Fifth, the applications of\none-dimensional, two-dimensional, and multi-dimensional convolution are\ncovered. Finally, some open issues and promising directions for CNN are\ndiscussed to serve as guidelines for future work.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 14:04:10 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Zewen", ""], ["Yang", "Wenjie", ""], ["Peng", "Shouheng", ""], ["Liu", "Fan", ""]]}, {"id": "2004.02808", "submitter": "Hongwei Lin", "authors": "Chenkai Xu, Hongwei Lin", "title": "High-Dimensional Data Set Simplification by Laplace-Beltrami Operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the Internet and other digital technologies, the\nspeed of data generation has become considerably faster than the speed of data\nprocessing. Because big data typically contain massive redundant information,\nit is possible to significantly simplify a big data set while maintaining the\nkey information it contains. In this paper, we develop a big data\nsimplification method based on the eigenvalues and eigenfunctions of the\nLaplace-Beltrami operator (LBO). Specifically, given a data set that can be\nconsidered as an unorganized data point set in high-dimensional space, a\ndiscrete LBO defined on the big data set is constructed and its eigenvalues and\neigenvectors are calculated. Then, the local extremum and the saddle points of\nthe eigenfunctions are proposed to be the feature points of a data set in\nhigh-dimensional space, constituting a simplified data set. Moreover, we\ndevelop feature point detection methods for the functions defined on an\nunorganized data point set in high-dimensional space, and devise metrics for\nmeasuring the fidelity of the simplified data set to the original set. Finally,\nexamples and applications are demonstrated to validate the efficiency and\neffectiveness of the proposed methods, demonstrating that data set\nsimplification is a method for processing a maximum-sized data set using a\nlimited data processing capability.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 13:52:58 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Xu", "Chenkai", ""], ["Lin", "Hongwei", ""]]}, {"id": "2004.02809", "submitter": "Edgar Rojas-Mu\\~noz", "authors": "Edgar Rojas-Mu\\~noz, Kyle Couperus and Juan Wachs", "title": "DAISI: Database for AI Surgical Instruction", "comments": "10 pages, 4 figures, to access database, see\n  https://engineering.purdue.edu/starproj/_daisi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Telementoring surgeons as they perform surgery can be essential in the\ntreatment of patients when in situ expertise is not available. Nonetheless,\nexpert mentors are often unavailable to provide trainees with real-time medical\nguidance. When mentors are unavailable, a fallback autonomous mechanism should\nprovide medical practitioners with the required guidance. However,\nAI/autonomous mentoring in medicine has been limited by the availability of\ngeneralizable prediction models, and surgical procedures datasets to train\nthose models with. This work presents the initial steps towards the development\nof an intelligent artificial system for autonomous medical mentoring.\nSpecifically, we present the first Database for AI Surgical Instruction\n(DAISI). DAISI leverages on images and instructions to provide step-by-step\ndemonstrations of how to perform procedures from various medical disciplines.\nThe dataset was acquired from real surgical procedures and data from academic\ntextbooks. We used DAISI to train an encoder-decoder neural network capable of\npredicting medical instructions given a current view of the surgery.\nAfterwards, the instructions predicted by the network were evaluated using\ncumulative BLEU scores and input from expert physicians. According to the BLEU\nscores, the predicted and ground truth instructions were as high as 67%\nsimilar. Additionally, expert physicians subjectively assessed the algorithm\nusing Likert scale, and considered that the predicted descriptions were related\nto the images. This work provides a baseline for AI algorithms to assist in\nautonomous medical mentoring.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 22:07:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rojas-Mu\u00f1oz", "Edgar", ""], ["Couperus", "Kyle", ""], ["Wachs", "Juan", ""]]}, {"id": "2004.02810", "submitter": "Benson Babu", "authors": "Jasmin Hundall, Benson A. Babu", "title": "Computer Vision and Abnormal Patient Gait Assessment a Comparison of\n  Machine Learning Models", "comments": "2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal gait, its associated falls and complications have high patient\nmorbidity, mortality. Computer vision detects, predicts patient gait\nabnormalities, assesses fall risk and serves as clinical decision support tool\nfor physicians. This paper performs a systematic review of how computer vision,\nmachine learning models perform an abnormal patient's gait assessment. Computer\nvision is beneficial in gait analysis, it helps capture the patient posture.\nSeveral literature suggests the use of different machine learning algorithms\nsuch as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the\nclassification on the features extracted to study patient gait abnormalities.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 02:00:15 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hundall", "Jasmin", ""], ["Babu", "Benson A.", ""]]}, {"id": "2004.02822", "submitter": "Madhumitha Harishankar", "authors": "Madhumitha Harishankar, Jun Han, Sai Vineeth Kalluru Srinivas, Faisal\n  Alqarni, Shi Su, Shijia Pan, Hae Young Noh, Pei Zhang, Marco Gruteser,\n  Patrick Tague", "title": "LaNet: Real-time Lane Identification by Learning Road\n  SurfaceCharacteristics from Accelerometer Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resolution of GPS measurements, especially in urban areas, is\ninsufficient for identifying a vehicle's lane. In this work, we develop a deep\nLSTM neural network model LaNet that determines the lane vehicles are on by\nperiodically classifying accelerometer samples collected by vehicles as they\ndrive in real time. Our key finding is that even adjacent patches of road\nsurfaces contain characteristics that are sufficiently unique to differentiate\nbetween lanes, i.e., roads inherently exhibit differing bumps, cracks,\npotholes, and surface unevenness. Cars can capture this road surface\ninformation as they drive using inexpensive, easy-to-install accelerometers\nthat increasingly come fitted in cars and can be accessed via the CAN-bus. We\ncollect an aggregate of 60 km driving data and synthesize more based on this\nthat capture factors such as variable driving speed, vehicle suspensions, and\naccelerometer noise. Our formulated LSTM-based deep learning model, LaNet,\nlearns lane-specific sequences of road surface events (bumps, cracks etc.) and\nyields 100% lane classification accuracy with 200 meters of driving data,\nachieving over 90% with just 100 m (correspondingly to roughly one minute of\ndriving). We design the LaNet model to be practical for use in real-time lane\nclassification and show with extensive experiments that LaNet yields high\nclassification accuracy even on smooth roads, on large multi-lane roads, and on\ndrives with frequent lane changes. Since different road surfaces have different\ninherent characteristics or entropy, we excavate our neural network model and\ndiscover a mechanism to easily characterize the achievable classification\naccuracies in a road over various driving distances by training the model just\nonce. We present LaNet as a low-cost, easily deployable and highly accurate way\nto achieve fine-grained lane identification.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:09:50 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Harishankar", "Madhumitha", ""], ["Han", "Jun", ""], ["Srinivas", "Sai Vineeth Kalluru", ""], ["Alqarni", "Faisal", ""], ["Su", "Shi", ""], ["Pan", "Shijia", ""], ["Noh", "Hae Young", ""], ["Zhang", "Pei", ""], ["Gruteser", "Marco", ""], ["Tague", "Patrick", ""]]}, {"id": "2004.02853", "submitter": "Junhwa Hur", "authors": "Junhwa Hur, Stefan Roth", "title": "Optical Flow Estimation in the Deep Learning Age", "comments": "To appear as a book chapter in Modelling Human Motion, N. Noceti, A.\n  Sciutti and F. Rea, Eds., Springer, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Akin to many subareas of computer vision, the recent advances in deep\nlearning have also significantly influenced the literature on optical flow.\nPreviously, the literature had been dominated by classical energy-based models,\nwhich formulate optical flow estimation as an energy minimization problem.\nHowever, as the practical benefits of Convolutional Neural Networks (CNNs) over\nconventional methods have become apparent in numerous areas of computer vision\nand beyond, they have also seen increased adoption in the context of motion\nestimation to the point where the current state of the art in terms of accuracy\nis set by CNN approaches. We first review this transition as well as the\ndevelopments from early work to the current state of CNNs for optical flow\nestimation. Alongside, we discuss some of their technical details and compare\nthem to recapitulate which technical contribution led to the most significant\naccuracy improvements. Then we provide an overview of the various optical flow\napproaches introduced in the deep learning age, including those based on\nalternative learning paradigms (e.g., unsupervised and semi-supervised methods)\nas well as the extension to the multi-frame case, which is able to yield\nfurther accuracy improvements.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:45:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "2004.02857", "submitter": "Jacob Krantz", "authors": "Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee", "title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a language-guided navigation task set in a continuous 3D\nenvironment where agents must execute low-level actions to follow natural\nlanguage navigation directions. By being situated in continuous environments,\nthis setting lifts a number of assumptions implicit in prior work that\nrepresents environments as a sparse graph of panoramas with edges corresponding\nto navigability. Specifically, our setting drops the presumptions of known\nenvironment topologies, short-range oracle navigation, and perfect agent\nlocalization. To contextualize this new task, we develop models that mirror\nmany of the advances made in prior settings as well as single-modality\nbaselines. While some of these techniques transfer, we find significantly lower\nabsolute performance in the continuous setting -- suggesting that performance\nin prior `navigation-graph' settings may be inflated by the strong implicit\nassumptions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:49:12 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 18:06:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Krantz", "Jacob", ""], ["Wijmans", "Erik", ""], ["Majumdar", "Arjun", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "2004.02866", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, Andrea Vedaldi", "title": "There and Back Again: Revisiting Backpropagation Saliency Methods", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency methods seek to explain the predictions of a model by producing an\nimportance map across each input sample. A popular class of such methods is\nbased on backpropagating a signal and analyzing the resulting gradient. Despite\nmuch research on such methods, relatively little work has been done to clarify\nthe differences between such methods as well as the desiderata of these\ntechniques. Thus, there is a need for rigorously understanding the\nrelationships between different methods as well as their failure modes. In this\nwork, we conduct a thorough analysis of backpropagation-based saliency methods\nand propose a single framework under which several such methods can be unified.\nAs a result of our study, we make three additional contributions. First, we use\nour framework to propose NormGrad, a novel saliency method based on the spatial\ncontribution of gradients of convolutional weights. Second, we combine saliency\nmaps at different layers to test the ability of saliency methods to extract\ncomplementary information at different network levels (e.g.~trading off spatial\nresolution and distinctiveness) and we explain why some methods fail at\nspecific layers (e.g., Grad-CAM anywhere besides the last convolutional layer).\nThird, we introduce a class-sensitivity metric and a meta-learning inspired\nparadigm applicable to any saliency method for improving sensitivity to the\noutput class being explained.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:58:08 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Fong", "Ruth", ""], ["Ji", "Xu", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2004.02867", "submitter": "Dongdong Chen", "authors": "Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming\n  He, Lu Yuan, Nenghai Yu", "title": "Rethinking Spatially-Adaptive Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-adaptive normalization is remarkably successful recently in\nconditional semantic image synthesis, which modulates the normalized activation\nwith spatially-varying transformations learned from semantic layouts, to\npreserve the semantic information from being washed away. Despite its\nimpressive performance, a more thorough understanding of the true advantages\ninside the box is still highly demanded, to help reduce the significant\ncomputation and parameter overheads introduced by these new structures. In this\npaper, from a return-on-investment point of view, we present a deep analysis of\nthe effectiveness of SPADE and observe that its advantages actually come mainly\nfrom its semantic-awareness rather than the spatial-adaptiveness. Inspired by\nthis point, we propose class-adaptive normalization (CLADE), a lightweight\nvariant that is not adaptive to spatial positions or layouts. Benefited from\nthis design, CLADE greatly reduces the computation cost while still being able\nto preserve the semantic information during the generation. Extensive\nexperiments on multiple challenging datasets demonstrate that while the\nresulting fidelity is on par with SPADE, its overhead is much cheaper than\nSPADE. Take the generator for ADE20k dataset as an example, the extra parameter\nand computation cost introduced by CLADE are only 4.57% and 0.07% while that of\nSPADE are 39.21% and 234.73% respectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:58:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Tan", "Zhentao", ""], ["Chen", "Dongdong", ""], ["Chu", "Qi", ""], ["Chai", "Menglei", ""], ["Liao", "Jing", ""], ["He", "Mingming", ""], ["Yuan", "Lu", ""], ["Yu", "Nenghai", ""]]}, {"id": "2004.02869", "submitter": "Zekun Hao", "authors": "Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie", "title": "DualSDF: Semantic Shape Manipulation using a Two-Level Representation", "comments": "Published in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are seeing a Cambrian explosion of 3D shape representations for use in\nmachine learning. Some representations seek high expressive power in capturing\nhigh-resolution detail. Other approaches seek to represent shapes as\ncompositions of simple parts, which are intuitive for people to understand and\neasy to edit and manipulate. However, it is difficult to achieve both fidelity\nand interpretability in the same representation. We propose DualSDF, a\nrepresentation expressing shapes at two levels of granularity, one capturing\nfine details and the other representing an abstracted proxy shape using simple\nand semantically consistent shape primitives. To achieve a tight coupling\nbetween the two representations, we use a variational objective over a shared\nlatent space. Our two-level model gives rise to a new shape manipulation\ntechnique in which a user can interactively manipulate the coarse proxy shape\nand see the changes instantly mirrored in the high-resolution shape. Moreover,\nour model actively augments and guides the manipulation towards producing\nsemantically meaningful shapes, making complex manipulations possible with\nminimal user input.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:59:15 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Hao", "Zekun", ""], ["Averbuch-Elor", "Hadar", ""], ["Snavely", "Noah", ""], ["Belongie", "Serge", ""]]}, {"id": "2004.02872", "submitter": "Sheng Cao", "authors": "Sheng Cao, Chao-Yuan Wu, Philipp Kr\\\"ahenb\\\"uhl", "title": "Lossless Image Compression through Super-Resolution", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and efficient lossless image compression algorithm. We\nstore a low resolution version of an image as raw pixels, followed by several\niterations of lossless super-resolution. For lossless super-resolution, we\npredict the probability of a high-resolution image, conditioned on the\nlow-resolution input, and use entropy coding to compress this super-resolution\noperator. Super-Resolution based Compression (SReC) is able to achieve\nstate-of-the-art compression rates with practical runtimes on large datasets.\nCode is available online at https://github.com/caoscott/SReC.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:59:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Cao", "Sheng", ""], ["Wu", "Chao-Yuan", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2004.02877", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Empirical Upper Bound, Error Diagnosis and Invariance Analysis of Modern\n  Object Detectors", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.12451", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection remains as one of the most notorious open problems in\ncomputer vision. Despite large strides in accuracy in recent years, modern\nobject detectors have started to saturate on popular benchmarks raising the\nquestion of how far we can reach with deep learning tools and tricks. Here, by\nemploying 2 state-of-the-art object detection benchmarks, and analyzing more\nthan 15 models over 4 large scale datasets, we I) carefully determine the upper\nbound in AP, which is 91.6% on VOC (test2007), 78.2% on COCO (val2017), and\n58.9% on OpenImages V4 (validation), regardless of the IOU threshold. These\nnumbers are much better than the mAP of the best model (47.9% on VOC, and 46.9%\non COCO; IOUs=.5:.05:.95), II) characterize the sources of errors in object\ndetectors, in a novel and intuitive way, and find that classification error\n(confusion with other classes and misses) explains the largest fraction of\nerrors and weighs more than localization and duplicate errors, and III) analyze\nthe invariance properties of models when surrounding context of an object is\nremoved, when an object is placed in an incongruent background, and when images\nare blurred or flipped vertically. We find that models generate a lot of boxes\non empty regions and that context is more important for detecting small objects\nthan larger ones. Our work taps into the tight relationship between object\ndetection and object recognition and offers insights for building better\nmodels. Our code is publicly available at\nhttps://github.com/aliborji/Deetctionupper bound.git.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 06:19:43 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2004.02932", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei", "title": "Beyond Background-Aware Correlation Filters: Adaptive Context Modeling\n  by Hand-Crafted and Deep RGB Features for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the background-aware correlation filters have achie-ved a\nlot of research interest in the visual target tracking. However, these methods\ncannot suitably model the target appearance due to the exploitation of\nhand-crafted features. On the other hand, the recent deep learning-based visual\ntracking methods have provided a competitive performance along with extensive\ncomputations. In this paper, an adaptive background-aware correlation\nfilter-based tracker is proposed that effectively models the target appearance\nby using either the histogram of oriented gradients (HOG) or convolutional\nneural network (CNN) feature maps. The proposed method exploits the fast 2D\nnon-maximum suppression (NMS) algorithm and the semantic information comparison\nto detect challenging situations. When the HOG-based response map is not\nreliable, or the context region has a low semantic similarity with prior\nregions, the proposed method constructs the CNN context model to improve the\ntarget region estimation. Furthermore, the rejection option allows the proposed\nmethod to update the CNN context model only on valid regions. Comprehensive\nexperimental results demonstrate that the proposed adaptive method clearly\noutperforms the accuracy and robustness of visual target tracking compared to\nthe state-of-the-art methods on the OTB-50, OTB-100, TC-128, UAV-123, and\nVOT-2015 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 18:48:39 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2004.02933", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei", "title": "Efficient Scale Estimation Methods using Lightweight Deep Convolutional\n  Neural Networks for Visual Tracking", "comments": "Accepted Manuscript in Neural Computing and Applications (NCAA),\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, visual tracking methods that are based on discriminative\ncorrelation filters (DCF) have been very promising. However, most of these\nmethods suffer from a lack of robust scale estimation skills. Although a wide\nrange of recent DCF-based methods exploit the features that are extracted from\ndeep convolutional neural networks (CNNs) in their translation model, the scale\nof the visual target is still estimated by hand-crafted features. Whereas the\nexploitation of CNNs imposes a high computational burden, this paper exploits\npre-trained lightweight CNNs models to propose two efficient scale estimation\nmethods, which not only improve the visual tracking performance but also\nprovide acceptable tracking speeds. The proposed methods are formulated based\non either holistic or region representation of convolutional feature maps to\nefficiently integrate into DCF formulations to learn a robust scale model in\nthe frequency domain. Moreover, against the conventional scale estimation\nmethods with iterative feature extraction of different target regions, the\nproposed methods exploit proposed one-pass feature extraction processes that\nsignificantly improve the computational efficiency. Comprehensive experimental\nresults on the OTB-50, OTB-100, TC-128 and VOT-2018 visual tracking datasets\ndemonstrate that the proposed visual tracking methods outperform the\nstate-of-the-art methods, effectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 18:49:37 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 08:22:24 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2004.02941", "submitter": "Steven Grosz Mr.", "authors": "Steven A. Grosz, Tarang Chugh, Anil K. Jain", "title": "Fingerprint Presentation Attack Detection: A Sensor and Material\n  Agnostic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of automated fingerprint recognition systems to\npresentation attacks (PA), i.e., spoof or altered fingers, has been a growing\nconcern, warranting the development of accurate and efficient presentation\nattack detection (PAD) methods. However, one major limitation of the existing\nPAD solutions is their poor generalization to new PA materials and fingerprint\nsensors, not used in training. In this study, we propose a robust PAD solution\nwith improved cross-material and cross-sensor generalization. Specifically, we\nbuild on top of any CNN-based architecture trained for fingerprint spoof\ndetection combined with cross-material spoof generalization using a style\ntransfer network wrapper. We also incorporate adversarial representation\nlearning (ARL) in deep neural networks (DNN) to learn sensor and material\ninvariant representations for PAD. Experimental results on LivDet 2015 and 2017\npublic domain datasets exhibit the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:03:05 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Grosz", "Steven A.", ""], ["Chugh", "Tarang", ""], ["Jain", "Anil K.", ""]]}, {"id": "2004.02945", "submitter": "Yinan Zhao", "authors": "Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari", "title": "Objectness-Aware Few-Shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot semantic segmentation models aim to segment images after learning\nfrom only a few annotated examples. A key challenge for them is overfitting.\nPrior works usually limit the overall model capacity to alleviate overfitting,\nbut the limited capacity also hampers the segmentation accuracy. We instead\npropose a method that increases the overall model capacity by supplementing\nclass-specific features with objectness, which is class-agnostic and so not\nprone to overfitting. Extensive experiments demonstrate the versatility of our\nmethod with multiple backbone models (ResNet-50, ResNet-101 and HRNetV2-W48)\nand existing base architectures (DENet and PFENet). Given only one annotated\nexample of an unseen category, experiments show that our method outperforms\nstate-of-art methods with respect to mIoU by at least 4.7% and 1.5% on\nPASCAL-5i and COCO-20i respectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:12:08 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 05:43:02 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhao", "Yinan", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Gurari", "Danna", ""]]}, {"id": "2004.02956", "submitter": "Raanan Fattal", "authors": "Adam Kaufman and Raanan Fattal", "title": "Deblurring using Analysis-Synthesis Networks Pair", "comments": null, "journal-ref": "Computer Vision and Pattern Recognition (CVPR) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring remains a challenging problem for modern artificial\nneural networks. Unlike other image restoration problems, deblurring networks\nfail behind the performance of existing deblurring algorithms in case of\nuniform and 3D blur models. This follows from the diverse and profound effect\nthat the unknown blur-kernel has on the deblurring operator.\n  We propose a new architecture which breaks the deblurring network into an\nanalysis network which estimates the blur, and a synthesis network that uses\nthis kernel to deblur the image. Unlike existing deblurring networks, this\ndesign allows us to explicitly incorporate the blur-kernel in the network's\ntraining. In addition, we introduce new cross-correlation layers that allow\nbetter blur estimations, as well as unique components that allow the estimate\nblur to control the action of the synthesis deblurring action.\n  Evaluating the new approach over established benchmark datasets shows its\nability to achieve state-of-the-art deblurring accuracy on various tests, as\nwell as offer a major speedup in runtime.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:32:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kaufman", "Adam", ""], ["Fattal", "Raanan", ""]]}, {"id": "2004.02967", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Andrew Brock, Karen Simonyan, Quoc V. Le", "title": "Evolving Normalization-Activation Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization layers and activation functions are fundamental components in\ndeep networks and typically co-locate with each other. Here we propose to\ndesign them using an automated approach. Instead of designing them separately,\nwe unify them into a single tensor-to-tensor computation graph, and evolve its\nstructure starting from basic mathematical functions. Examples of such\nmathematical functions are addition, multiplication and statistical moments.\nThe use of low-level mathematical functions, in contrast to the use of\nhigh-level modules in mainstream NAS, leads to a highly sparse and large search\nspace which can be challenging for search methods. To address the challenge, we\ndevelop efficient rejection protocols to quickly filter out candidate layers\nthat do not work well. We also use multi-objective evolution to optimize each\nlayer's performance across many architectures to prevent overfitting. Our\nmethod leads to the discovery of EvoNorms, a set of new\nnormalization-activation layers with novel, and sometimes surprising structures\nthat go beyond existing design patterns. For example, some EvoNorms do not\nassume that normalization and activation functions must be applied\nsequentially, nor need to center the feature maps, nor require explicit\nactivation functions. Our experiments show that EvoNorms work well on image\nclassification models including ResNets, MobileNets and EfficientNets but also\ntransfer well to Mask R-CNN with FPN/SpineNet for instance segmentation and to\nBigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers\nin many cases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:52:48 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 02:58:37 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 16:29:08 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 22:59:31 GMT"}, {"version": "v5", "created": "Fri, 17 Jul 2020 04:42:59 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Liu", "Hanxiao", ""], ["Brock", "Andrew", ""], ["Simonyan", "Karen", ""], ["Le", "Quoc V.", ""]]}, {"id": "2004.02980", "submitter": "Abhinav Kumar", "authors": "Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones,\n  Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, Chen Feng", "title": "LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and\n  Visibility Likelihood", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern face alignment methods have become quite accurate at predicting the\nlocations of facial landmarks, but they do not typically estimate the\nuncertainty of their predicted locations nor predict whether landmarks are\nvisible. In this paper, we present a novel framework for jointly predicting\nlandmark locations, associated uncertainties of these predicted locations, and\nlandmark visibilities. We model these as mixed random variables and estimate\nthem using a deep network trained with our proposed Location, Uncertainty, and\nVisibility Likelihood (LUVLi) loss. In addition, we release an entirely new\nlabeling of a large face alignment dataset with over 19,000 face images in a\nfull range of head poses. Each face is manually labeled with the ground-truth\nlocations of 68 landmarks, with the additional information of whether each\nlandmark is unoccluded, self-occluded (due to extreme head poses), or\nexternally occluded. Not only does our joint estimation yield accurate\nestimates of the uncertainty of predicted landmark locations, but it also\nyields state-of-the-art estimates for the landmark locations themselves on\nmultiple standard face alignment datasets. Our method's estimates of the\nuncertainty of predicted landmark locations could be used to automatically\nidentify input images on which face alignment fails, which can be critical for\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:17:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kumar", "Abhinav", ""], ["Marks", "Tim K.", ""], ["Mou", "Wenxuan", ""], ["Wang", "Ye", ""], ["Jones", "Michael", ""], ["Cherian", "Anoop", ""], ["Koike-Akino", "Toshiaki", ""], ["Liu", "Xiaoming", ""], ["Feng", "Chen", ""]]}, {"id": "2004.03015", "submitter": "Qiuyu Chen", "authors": "Qiuyu Chen, Wei Zhang, Ning Zhou, Peng Lei, Yi Xu, Yu Zheng, Jianping\n  Fan", "title": "Adaptive Fractional Dilated Convolution Network for Image Aesthetics\n  Assessment", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To leverage deep learning for image aesthetics assessment, one critical but\nunsolved issue is how to seamlessly incorporate the information of image aspect\nratios to learn more robust models. In this paper, an adaptive fractional\ndilated convolution (AFDC), which is aspect-ratio-embedded,\ncomposition-preserving and parameter-free, is developed to tackle this issue\nnatively in convolutional kernel level. Specifically, the fractional dilated\nkernel is adaptively constructed according to the image aspect ratios, where\nthe interpolation of nearest two integers dilated kernels is used to cope with\nthe misalignment of fractional sampling. Moreover, we provide a concise\nformulation for mini-batch training and utilize a grouping strategy to reduce\ncomputational overhead. As a result, it can be easily implemented by common\ndeep learning libraries and plugged into popular CNN architectures in a\ncomputation-efficient manner. Our experimental results demonstrate that our\nproposed method achieves state-of-the-art performance on image aesthetics\nassessment over the AVA dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 21:56:29 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Chen", "Qiuyu", ""], ["Zhang", "Wei", ""], ["Zhou", "Ning", ""], ["Lei", "Peng", ""], ["Xu", "Yi", ""], ["Zheng", "Yu", ""], ["Fan", "Jianping", ""]]}, {"id": "2004.03023", "submitter": "Hannah Kerner", "authors": "Hannah Kerner, Catherine Nakalembe, Inbal Becker-Reshef", "title": "Field-Level Crop Type Classification with k Nearest Neighbors: A\n  Baseline for a New Kenya Smallholder Dataset", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate crop type maps provide critical information for ensuring food\nsecurity, yet there has been limited research on crop type classification for\nsmallholder agriculture, particularly in sub-Saharan Africa where risk of food\ninsecurity is highest. Publicly-available ground-truth data such as the\nnewly-released training dataset of crop types in Kenya (Radiant MLHub) are\ncatalyzing this research, but it is important to understand the context of\nwhen, where, and how these datasets were obtained when evaluating\nclassification performance and using them as a benchmark across methods. In\nthis paper, we provide context for the new western Kenya dataset which was\ncollected during an atypical 2019 main growing season and demonstrate\nclassification accuracy up to 64% for maize and 70% for cassava using k Nearest\nNeighbors--a fast, interpretable, and scalable method that can serve as a\nbaseline for future work.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:24:44 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kerner", "Hannah", ""], ["Nakalembe", "Catherine", ""], ["Becker-Reshef", "Inbal", ""]]}, {"id": "2004.03028", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Giorgio Gori, Duygu Ceylan, Radomir Mech, Nathan\n  Carr, Tamy Boubekeur, Rui Wang, Subhransu Maji", "title": "Learning Generative Models of Shape Handles", "comments": "11 pages, 11 figures, accepted do CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative model to synthesize 3D shapes as sets of handles --\nlightweight proxies that approximate the original 3D shape -- for applications\nin interactive editing, shape parsing, and building compact 3D representations.\nOur model can generate handle sets with varying cardinality and different types\nof handles (Figure 1). Key to our approach is a deep architecture that predicts\nboth the parameters and existence of shape handles, and a novel similarity\nmeasure that can easily accommodate different types of handles, such as cuboids\nor sphere-meshes. We leverage the recent advances in semantic 3D annotation as\nwell as automatic shape summarizing techniques to supervise our approach. We\nshow that the resulting shape representations are intuitive and achieve\nsuperior quality than previous state-of-the-art. Finally, we demonstrate how\nour method can be used in applications such as interactive shape editing,\ncompletion, and interpolation, leveraging the latent space learned by our model\nto guide these tasks. Project page: http://mgadelha.me/shapehandles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:35:55 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Gadelha", "Matheus", ""], ["Gori", "Giorgio", ""], ["Ceylan", "Duygu", ""], ["Mech", "Radomir", ""], ["Carr", "Nathan", ""], ["Boubekeur", "Tamy", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "2004.03037", "submitter": "Simon Graham Mr", "authors": "Simon Graham, David Epstein and Nasir Rajpoot", "title": "Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in\n  Histology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histology images are inherently symmetric under rotation, where each\norientation is equally as likely to appear. However, this rotational symmetry\nis not widely utilised as prior knowledge in modern Convolutional Neural\nNetworks (CNNs), resulting in data hungry models that learn independent\nfeatures at each orientation. Allowing CNNs to be rotation-equivariant removes\nthe necessity to learn this set of transformations from the data and instead\nfrees up model capacity, allowing more discriminative features to be learned.\nThis reduction in the number of required parameters also reduces the risk of\noverfitting. In this paper, we propose Dense Steerable Filter CNNs (DSF-CNNs)\nthat use group convolutions with multiple rotated copies of each filter in a\ndensely connected framework. Each filter is defined as a linear combination of\nsteerable basis filters, enabling exact rotation and decreasing the number of\ntrainable parameters compared to standard filters. We also provide the first\nin-depth comparison of different rotation-equivariant CNNs for histology image\nanalysis and demonstrate the advantage of encoding rotational symmetry into\nmodern architectures. We show that DSF-CNNs achieve state-of-the-art\nperformance, with significantly fewer parameters, when applied to three\ndifferent tasks in the area of computational pathology: breast tumour\nclassification, colon gland segmentation and multi-tissue nuclear segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 23:12:31 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:22:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Graham", "Simon", ""], ["Epstein", "David", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2004.03042", "submitter": "Xin Li", "authors": "Xin Li, Chengyin Li, Dongxiao Zhu", "title": "COVID-MobileXpert: On-Device COVID-19 Patient Triage and Follow-up using\n  Chest X-rays", "comments": "COVID-19, SARS-CoV-2, On-device Machine Learning, Chest X-Ray (CXR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the COVID-19 pandemic, there has been an emerging need for rapid,\ndedicated, and point-of-care COVID-19 patient disposition techniques to\noptimize resource utilization and clinical workflow. In view of this need, we\npresent COVID-MobileXpert: a lightweight deep neural network (DNN) based mobile\napp that can use chest X-ray (CXR) for COVID-19 case screening and radiological\ntrajectory prediction. We design and implement a novel three-player knowledge\ntransfer and distillation (KTD) framework including a pre-trained attending\nphysician (AP) network that extracts CXR imaging features from a large scale of\nlung disease CXR images, a fine-tuned resident fellow (RF) network that learns\nthe essential CXR imaging features to discriminate COVID-19 from pneumonia\nand/or normal cases with a small amount of COVID-19 cases, and a trained\nlightweight medical student (MS) network to perform on-device COVID-19 patient\ntriage and follow-up. To tackle the challenge of vastly similar and dominant\nfore- and background in medical images, we employ novel loss functions and\ntraining schemes for the MS network to learn the robust features. We\ndemonstrate the significant potential of COVID-MobileXpert for rapid deployment\nvia extensive experiments with diverse MS architecture and tuning parameter\nsettings. The source codes for cloud and mobile based models are available from\nthe following url: https://github.com/xinli0928/COVID-Xray.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 23:43:58 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 04:24:31 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 05:56:14 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Xin", ""], ["Li", "Chengyin", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "2004.03044", "submitter": "Yu Yao", "authors": "Yu Yao, Xizi Wang, Mingze Xu, Zelin Pu, Ella Atkins, David Crandall", "title": "When, Where, and What? A New Dataset for Anomaly Detection in Driving\n  Videos", "comments": "23 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection (VAD) has been extensively studied. However, research\non egocentric traffic videos with dynamic scenes lacks large-scale benchmark\ndatasets as well as effective evaluation metrics. This paper proposes traffic\nanomaly detection with a \\textit{when-where-what} pipeline to detect, localize,\nand recognize anomalous events from egocentric videos. We introduce a new\ndataset called Detection of Traffic Anomaly (DoTA) containing 4,677 videos with\ntemporal, spatial, and categorical annotations. A new spatial-temporal area\nunder curve (STAUC) evaluation metric is proposed and used with DoTA.\nState-of-the-art methods are benchmarked for two VAD-related tasks.Experimental\nresults show STAUC is an effective VAD metric. To our knowledge, DoTA is the\nlargest traffic anomaly dataset to-date and is the first supporting traffic\nanomaly studies across when-where-what perspectives. Our code and dataset can\nbe found in: https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 23:58:59 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Yao", "Yu", ""], ["Wang", "Xizi", ""], ["Xu", "Mingze", ""], ["Pu", "Zelin", ""], ["Atkins", "Ella", ""], ["Crandall", "David", ""]]}, {"id": "2004.03046", "submitter": "Sukesh Adiga Vasudeva", "authors": "Sukesh Adiga V, Jose Dolz, Herve Lombaert", "title": "Manifold-driven Attention Maps for Weakly Supervised Segmentation", "comments": "Paper is submitted to MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation using deep learning has shown promising directions in medical\nimaging as it aids in the analysis and diagnosis of diseases. Nevertheless, a\nmain drawback of deep models is that they require a large amount of pixel-level\nlabels, which are laborious and expensive to obtain. To mitigate this problem,\nweakly supervised learning has emerged as an efficient alternative, which\nemploys image-level labels, scribbles, points, or bounding boxes as\nsupervision. Among these, image-level labels are easier to obtain. However,\nsince this type of annotation only contains object category information, the\nsegmentation task under this learning paradigm is a challenging problem. To\naddress this issue, visual salient regions derived from trained classification\nnetworks are typically used. Despite their success to identify important\nregions on classification tasks, these saliency regions only focus on the most\ndiscriminant areas of an image, limiting their use in semantic segmentation. In\nthis work, we propose a manifold driven attention-based network to enhance\nvisual salient regions, thereby improving segmentation accuracy in a weakly\nsupervised setting. Our method generates superior attention maps directly\nduring inference without the need of extra computations. We evaluate the\nbenefits of our approach in the task of segmentation using a public benchmark\non skin lesion images. Results demonstrate that our method outperforms the\nstate-of-the-art GradCAM by a margin of ~22% in terms of Dice score.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:03:28 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Adiga", "Sukesh", "V"], ["Dolz", "Jose", ""], ["Lombaert", "Herve", ""]]}, {"id": "2004.03048", "submitter": "Kai Zhang", "authors": "Kai Zhang, Jiaxin Xie, Noah Snavely, Qifeng Chen", "title": "Depth Sensing Beyond LiDAR Range", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth sensing is a critical component of autonomous driving technologies, but\ntoday's LiDAR- or stereo camera-based solutions have limited range. We seek to\nincrease the maximum range of self-driving vehicles' depth perception modules\nfor the sake of better safety. To that end, we propose a novel three-camera\nsystem that utilizes small field of view cameras. Our system, along with our\nnovel algorithm for computing metric depth, does not require full\npre-calibration and can output dense depth maps with practically acceptable\naccuracy for scenes and objects at long distances not well covered by most\ncommercial LiDARs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:09:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhang", "Kai", ""], ["Xie", "Jiaxin", ""], ["Snavely", "Noah", ""], ["Chen", "Qifeng", ""]]}, {"id": "2004.03064", "submitter": "Jichao Zhang", "authors": "Jingjing Chen, Jichao Zhang, Enver Sangineto, Jiayuan Fan, Tao Chen,\n  Nicu Sebe", "title": "Coarse-to-Fine Gaze Redirection with Numerical and Pictorial Guidance", "comments": "12 pages, accepted by WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze redirection aims at manipulating the gaze of a given face image with\nrespect to a desired direction (i.e., a reference angle) and it can be applied\nto many real life scenarios, such as video-conferencing or taking group photos.\nHowever, previous work on this topic mainly suffers of two limitations: (1)\nLow-quality image generation and (2) Low redirection precision. In this paper,\nwe propose to alleviate these problems by means of a novel gaze redirection\nframework which exploits both a numerical and a pictorial direction guidance,\njointly with a coarse-to-fine learning strategy. Specifically, the coarse\nbranch learns the spatial transformation which warps input image according to\ndesired gaze. On the other hand, the fine-grained branch consists of a\ngenerator network with conditional residual image learning and a multi-task\ndiscriminator. This second branch reduces the gap between the previously warped\nimage and the ground-truth image and recovers finer texture details. Moreover,\nwe propose a numerical and pictorial guidance module~(NPG) which uses a\npictorial gazemap description and numerical angles as an extra guide to further\nimprove the precision of gaze redirection. Extensive experiments on a benchmark\ndataset show that the proposed method outperforms the state-of-the-art\napproaches in terms of both image quality and redirection precision. The code\nis available at https://github.com/jingjingchen777/CFGR\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:17:27 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 19:31:08 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 22:37:18 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2020 06:17:15 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chen", "Jingjing", ""], ["Zhang", "Jichao", ""], ["Sangineto", "Enver", ""], ["Fan", "Jiayuan", ""], ["Chen", "Tao", ""], ["Sebe", "Nicu", ""]]}, {"id": "2004.03074", "submitter": "Kai Zhang", "authors": "Kai Zhang, V\\'itor Albiero and Kevin W. Bowyer", "title": "A Method for Curation of Web-Scraped Face Image Datasets", "comments": "This paper will appear at IWBF 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-scraped, in-the-wild datasets have become the norm in face recognition\nresearch. The numbers of subjects and images acquired in web-scraped datasets\nare usually very large, with number of images on the millions scale. A variety\nof issues occur when collecting a dataset in-the-wild, including images with\nthe wrong identity label, duplicate images, duplicate subjects and variation in\nquality. With the number of images being in the millions, a manual cleaning\nprocedure is not feasible. But fully automated methods used to date result in a\nless-than-ideal level of clean dataset. We propose a semi-automated method,\nwhere the goal is to have a clean dataset for testing face recognition methods,\nwith similar quality across men and women, to support comparison of accuracy\nacross gender. Our approach removes near-duplicate images, merges duplicate\nsubjects, corrects mislabeled images, and removes images outside a defined\nrange of pose and quality. We conduct the curation on the Asian Face Dataset\n(AFD) and VGGFace2 test dataset. The experiments show that a state-of-the-art\nmethod achieves a much higher accuracy on the datasets after they are curated.\nFinally, we release our cleaned versions of both datasets to the research\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 01:57:32 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhang", "Kai", ""], ["Albiero", "V\u00edtor", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2004.03080", "submitter": "Wei-Lun Chao", "authors": "Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge Belongie, Bharath\n  Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao", "title": "End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection", "comments": "Accepted to 2020 Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliable and accurate 3D object detection is a necessity for safe autonomous\ndriving. Although LiDAR sensors can provide accurate 3D point cloud estimates\nof the environment, they are also prohibitively expensive for many settings.\nRecently, the introduction of pseudo-LiDAR (PL) has led to a drastic reduction\nin the accuracy gap between methods based on LiDAR sensors and those based on\ncheap stereo cameras. PL combines state-of-the-art deep neural networks for 3D\ndepth estimation with those for 3D object detection by converting 2D depth map\noutputs to 3D point cloud inputs. However, so far these two networks have to be\ntrained separately. In this paper, we introduce a new framework based on\ndifferentiable Change of Representation (CoR) modules that allow the entire PL\npipeline to be trained end-to-end. The resulting framework is compatible with\nmost state-of-the-art networks for both tasks and in combination with PointRCNN\nimproves over PL consistently across all benchmarks -- yielding the highest\nentry on the KITTI image-based 3D object detection leaderboard at the time of\nsubmission. Our code will be made available at\nhttps://github.com/mileyan/pseudo-LiDAR_e2e.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:18:38 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:39:42 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Qian", "Rui", ""], ["Garg", "Divyansh", ""], ["Wang", "Yan", ""], ["You", "Yurong", ""], ["Belongie", "Serge", ""], ["Hariharan", "Bharath", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2004.03104", "submitter": "Qinghai Zheng", "authors": "Qinghai Zheng, Jihua Zhu, Haoyu Tang, Xinyuan Liu, Zhongyu Li, and\n  Huimin Lu", "title": "Generalized Label Enhancement with Sample Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, label distribution learning (LDL) has drawn much attention in\nmachine learning, where LDL model is learned from labelel instances. Different\nfrom single-label and multi-label annotations, label distributions describe the\ninstance by multiple labels with different intensities and accommodate to more\ngeneral scenes. Since most existing machine learning datasets merely provide\nlogical labels, label distributions are unavailable in many real-world\napplications. To handle this problem, we propose two novel label enhancement\nmethods, i.e., Label Enhancement with Sample Correlations (LESC) and\ngeneralized Label Enhancement with Sample Correlations (gLESC). More\nspecifically, LESC employs a low-rank representation of samples in the feature\nspace, and gLESC leverages a tensor multi-rank minimization to further\ninvestigate the sample correlations in both the feature space and label space.\nBenefitting from the sample correlations, the proposed methods can boost the\nperformance of label enhancement. Extensive experiments on 14 benchmark\ndatasets demonstrate the effectiveness and superiority of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:32:36 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:17:26 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 02:47:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zheng", "Qinghai", ""], ["Zhu", "Jihua", ""], ["Tang", "Haoyu", ""], ["Liu", "Xinyuan", ""], ["Li", "Zhongyu", ""], ["Lu", "Huimin", ""]]}, {"id": "2004.03109", "submitter": "Yuxia Geng", "authors": "Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Zhiquan Ye, Zonggang Yuan, Yantao\n  Jia, Huajun Chen", "title": "Generative Adversarial Zero-shot Learning via Knowledge Graphs", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is to handle the prediction of those unseen classes\nthat have no labeled training data. Recently, generative methods like\nGenerative Adversarial Networks (GANs) are being widely investigated for ZSL\ndue to their high accuracy, generalization capability and so on. However, the\nside information of classes used now is limited to text descriptions and\nattribute annotations, which are in short of semantics of the classes. In this\npaper, we introduce a new generative ZSL method named KG-GAN by incorporating\nrich semantics in a knowledge graph (KG) into GANs. Specifically, we build upon\nGraph Neural Networks and encode KG from two views: class view and attribute\nview considering the different semantics of KG. With well-learned semantic\nembeddings for each node (representing a visual category), we leverage GANs to\nsynthesize compelling visual features for unseen classes. According to our\nevaluation with multiple image classification datasets, KG-GAN can achieve\nbetter performance than the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:55:26 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Geng", "Yuxia", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Zhuo", ""], ["Ye", "Zhiquan", ""], ["Yuan", "Zonggang", ""], ["Jia", "Yantao", ""], ["Chen", "Huajun", ""]]}, {"id": "2004.03132", "submitter": "Jun Ling", "authors": "Jun Ling, Han Xue, Li Song, Shuhui Yang, Rong Xie, Xiao Gu", "title": "Toward Fine-grained Facial Expression Manipulation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58604-1_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression manipulation aims at editing facial expression with a given\ncondition. Previous methods edit an input image under the guidance of a\ndiscrete emotion label or absolute condition (e.g., facial action units) to\npossess the desired expression. However, these methods either suffer from\nchanging condition-irrelevant regions or are inefficient for fine-grained\nediting. In this study, we take these two objectives into consideration and\npropose a novel method. First, we replace continuous absolute condition with\nrelative condition, specifically, relative action units. With relative action\nunits, the generator learns to only transform regions of interest which are\nspecified by non-zero-valued relative AUs. Second, our generator is built on\nU-Net but strengthened by Multi-Scale Feature Fusion (MSF) mechanism for\nhigh-quality expression editing purposes. Extensive experiments on both\nquantitative and qualitative evaluation demonstrate the improvements of our\nproposed approach compared to the state-of-the-art expression editing methods.\nCode is available at \\url{https://github.com/junleen/Expression-manipulator}.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 05:14:15 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 08:19:55 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ling", "Jun", ""], ["Xue", "Han", ""], ["Song", "Li", ""], ["Yang", "Shuhui", ""], ["Xie", "Rong", ""], ["Gu", "Xiao", ""]]}, {"id": "2004.03142", "submitter": "Jian Ren", "authors": "Jian Ren, Menglei Chai, Sergey Tulyakov, Chen Fang, Xiaohui Shen,\n  Jianchao Yang", "title": "Human Motion Transfer from Poses in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of human motion transfer, where we\nsynthesize novel motion video for a target person that imitates the movement\nfrom a reference video. It is a video-to-video translation task in which the\nestimated poses are used to bridge two domains. Despite substantial progress on\nthe topic, there exist several problems with the previous methods. First, there\nis a domain gap between training and testing pose sequences--the model is\ntested on poses it has not seen during training, such as difficult dancing\nmoves. Furthermore, pose detection errors are inevitable, making the job of the\ngenerator harder. Finally, generating realistic pixels from sparse poses is\nchallenging in a single step. To address these challenges, we introduce a novel\npose-to-video translation framework for generating high-quality videos that are\ntemporally coherent even for in-the-wild pose sequences unseen during training.\nWe propose a pose augmentation method to minimize the training-test gap, a\nunified paired and unpaired learning strategy to improve the robustness to\ndetection errors, and two-stage network architecture to achieve superior\ntexture quality. To further boost research on the topic, we build two human\nmotion datasets. Finally, we show the superiority of our approach over the\nstate-of-the-art studies through extensive experiments and evaluations on\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 05:59:53 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Ren", "Jian", ""], ["Chai", "Menglei", ""], ["Tulyakov", "Sergey", ""], ["Fang", "Chen", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jianchao", ""]]}, {"id": "2004.03143", "submitter": "Zhe Wang", "authors": "Zhe Wang, Daeyun Shin, Charless C. Fowlkes", "title": "Predicting Camera Viewpoint Improves Cross-dataset Generalization for 3D\n  Human Pose Estimation", "comments": "http://wangzheallen.github.io/cross-dataset-generalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular estimation of 3d human pose has attracted increased attention with\nthe availability of large ground-truth motion capture datasets. However, the\ndiversity of training data available is limited and it is not clear to what\nextent methods generalize outside the specific datasets they are trained on. In\nthis work we carry out a systematic study of the diversity and biases present\nin specific datasets and its effect on cross-dataset generalization across a\ncompendium of 5 pose datasets. We specifically focus on systematic differences\nin the distribution of camera viewpoints relative to a body-centered coordinate\nframe. Based on this observation, we propose an auxiliary task of predicting\nthe camera viewpoint in addition to pose. We find that models trained to\njointly predict viewpoint and pose systematically show significantly improved\ncross-dataset generalization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:06:20 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wang", "Zhe", ""], ["Shin", "Daeyun", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "2004.03145", "submitter": "Ruturaj Gavaskar", "authors": "Ruturaj G. Gavaskar and Kunal N. Chaudhury", "title": "Plug-and-play ISTA converges with kernel denoisers", "comments": "5 pages, Accepted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.2986643", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-play (PnP) method is a recent paradigm for image regularization,\nwhere the proximal operator (associated with some given regularizer) in an\niterative algorithm is replaced with a powerful denoiser. Algorithmically, this\ninvolves repeated inversion (of the forward model) and denoising until\nconvergence. Remarkably, PnP regularization produces promising results for\nseveral restoration applications. However, a fundamental question in this\nregard is the theoretical convergence of the PnP iterations, since the\nalgorithm is not strictly derived from an optimization framework. This question\nhas been investigated in recent works, but there are still many unresolved\nproblems. For example, it is not known if convergence can be guaranteed if we\nuse generic kernel denoisers (e.g. nonlocal means) within the ISTA framework\n(PnP-ISTA). We prove that, under reasonable assumptions, fixed-point\nconvergence of PnP-ISTA is indeed guaranteed for linear inverse problems such\nas deblurring, inpainting and superresolution (the assumptions are verifiable\nfor inpainting). We compare our theoretical findings with existing results,\nvalidate them numerically, and explain their practical relevance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:25:34 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 14:24:53 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Gavaskar", "Ruturaj G.", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "2004.03150", "submitter": "Yang Zhang", "authors": "Yang Zhang, Changhui Hu, and Xiaobo Lu", "title": "Deep Attentive Generative Adversarial Network for Photo-Realistic Image\n  De-Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of current display devices are with eight or higher bit-depth. However,\nthe quality of most multimedia tools cannot achieve this bit-depth standard for\nthe generating images. De-quantization can improve the visual quality of low\nbit-depth image to display on high bit-depth screen. This paper proposes DAGAN\nalgorithm to perform super-resolution on image intensity resolution, which is\northogonal to the spatial resolution, realizing photo-realistic de-quantization\nvia an end-to-end learning pattern. Until now, this is the first attempt to\napply Generative Adversarial Network (GAN) framework for image de-quantization.\nSpecifically, we propose the Dense Residual Self-attention (DenseResAtt)\nmodule, which is consisted of dense residual blocks armed with self-attention\nmechanism, to pay more attention on high-frequency information. Moreover, the\nseries connection of sequential DenseResAtt modules forms deep attentive\nnetwork with superior discriminative learning ability in image de-quantization,\nmodeling representative feature maps to recover as much useful information as\npossible. In addition, due to the adversarial learning framework can reliably\nproduce high quality natural images, the specified content loss as well as the\nadversarial loss are back-propagated to optimize the training of model. Above\nall, DAGAN is able to generate the photo-realistic high bit-depth image without\nbanding artifacts. Experiment results on several public benchmarks prove that\nthe DAGAN algorithm possesses ability to achieve excellent visual effect and\nsatisfied quantitative performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:45:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhang", "Yang", ""], ["Hu", "Changhui", ""], ["Lu", "Xiaobo", ""]]}, {"id": "2004.03153", "submitter": "Yang Zhang", "authors": "Yang Zhang, Changhui Hu, Xiaobo Lu", "title": "Adaptive Multiscale Illumination-Invariant Feature Representation for\n  Undersampled Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an novel illumination-invariant feature representation\napproach used to eliminate the varying illumination affection in undersampled\nface recognition. Firstly, a new illumination level classification technique\nbased on Singular Value Decomposition (SVD) is proposed to judge the\nillumination level of input image. Secondly, we construct the logarithm\nedgemaps feature (LEF) based on lambertian model and local near neighbor\nfeature of the face image, applying to local region within multiple scales.\nThen, the illumination level is referenced to construct the high performance\nLEF as well realize adaptive fusion for multiple scales LEFs for the face\nimage, performing JLEF-feature. In addition, the constrain operation is used to\nremove the useless high-frequency interference, disentangling useful facial\nfeature edges and constructing AJLEF-face. Finally, the effects of the our\nmethods and other state-of-the-art algorithms including deep learning methods\nare tested on Extended Yale B, CMU PIE, AR as well as our Self-build Driver\ndatabase (SDB). The experimental results demonstrate that the JLEF-feature and\nAJLEF-face outperform other related approaches for undersampled face\nrecognition under varying illumination.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:48:44 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhang", "Yang", ""], ["Hu", "Changhui", ""], ["Lu", "Xiaobo", ""]]}, {"id": "2004.03156", "submitter": "Alessio Quaglino PhD", "authors": "Giorgio Giannone, Asha Anoosheh, Alessio Quaglino, Pierluca D'Oro,\n  Marco Gallieri, Jonathan Masci", "title": "Real-time Classification from Short Event-Camera Streams using\n  Input-filtering Neural ODEs", "comments": "Submitted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are novel, efficient sensors inspired by the human vision\nsystem, generating an asynchronous, pixel-wise stream of data. Learning from\nsuch data is generally performed through heavy preprocessing and event\nintegration into images. This requires buffering of possibly long sequences and\ncan limit the response time of the inference system. In this work, we instead\npropose to directly use events from a DVS camera, a stream of intensity changes\nand their spatial coordinates. This sequence is used as the input for a novel\n\\emph{asynchronous} RNN-like architecture, the Input-filtering Neural ODEs\n(INODE). This is inspired by the dynamical systems and filtering literature.\nINODE is an extension of Neural ODEs (NODE) that allows for input signals to be\ncontinuously fed to the network, like in filtering. The approach naturally\nhandles batches of time series with irregular time-stamps by implementing a\nbatch forward Euler solver. INODE is trained like a standard RNN, it learns to\ndiscriminate short event sequences and to perform event-by-event online\ninference. We demonstrate our approach on a series of classification tasks,\ncomparing against a set of LSTM baselines. We show that, independently of the\ncamera resolution, INODE can outperform the baselines by a large margin on the\nASL task and it's on par with a much larger LSTM for the NCALTECH task.\nFinally, we show that INODE is accurate even when provided with very few\nevents.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 06:58:38 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Giannone", "Giorgio", ""], ["Anoosheh", "Asha", ""], ["Quaglino", "Alessio", ""], ["D'Oro", "Pierluca", ""], ["Gallieri", "Marco", ""], ["Masci", "Jonathan", ""]]}, {"id": "2004.03164", "submitter": "Haitian Zeng", "authors": "Haitian Zeng, Haizhou Ai, Zijie Zhuang, Long Chen", "title": "Multi-Task Learning via Co-Attentive Sharing for Pedestrian Attribute\n  Recognition", "comments": "2020 IEEE International Conference on Multimedia & Expo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict multiple attributes of a pedestrian is a multi-task\nlearning problem. To share feature representation between two individual task\nnetworks, conventional methods like Cross-Stitch and Sluice network learn a\nlinear combination of features or feature subspaces. However, linear\ncombination rules out the complex interdependency between channels. Moreover,\nspatial information exchanging is less-considered. In this paper, we propose a\nnovel Co-Attentive Sharing (CAS) module which extracts discriminative channels\nand spatial regions for more effective feature sharing in multi-task learning.\nThe module consists of three branches, which leverage different channels for\nbetween-task feature fusing, attention generation and task-specific feature\nenhancing, respectively. Experiments on two pedestrian attribute recognition\ndatasets show that our module outperforms the conventional sharing units and\nachieves superior results compared to the state-of-the-art approaches using\nmany metrics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:24:22 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zeng", "Haitian", ""], ["Ai", "Haizhou", ""], ["Zhuang", "Zijie", ""], ["Chen", "Long", ""]]}, {"id": "2004.03179", "submitter": "Seiichi Uchida", "authors": "Takuro Karamatsu, Gibran Benitez-Garcia, Keiji Yanai, Seiichi Uchida", "title": "Iconify: Converting Photographs into Icons", "comments": "to appear at 2020 Joint Workshop on Multimedia Artworks Analysis and\n  Attractiveness Computing in Multimedia (MMArt-ACM'20)", "journal-ref": null, "doi": "10.1145/3379173.3393708", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle a challenging domain conversion task between photo\nand icon images. Although icons often originate from real object images (i.e.,\nphotographs), severe abstractions and simplifications are applied to generate\nicon images by professional graphic designers. Moreover, there is no one-to-one\ncorrespondence between the two domains, for this reason we cannot use it as the\nground-truth for learning a direct conversion function. Since generative\nadversarial networks (GAN) can undertake the problem of domain conversion\nwithout any correspondence, we test CycleGAN and UNIT to generate icons from\nobjects segmented from photo images. Our experiments with several image\ndatasets prove that CycleGAN learns sufficient abstraction and simplification\nability to generate icon-like images.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:01:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Karamatsu", "Takuro", ""], ["Benitez-Garcia", "Gibran", ""], ["Yanai", "Keiji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2004.03212", "submitter": "Lisai Zhang", "authors": "Lisai Zhang, Qingcai Chen, Baotian Hu, and Shuoran Jiang", "title": "Text-Guided Neural Image Inpainting", "comments": "ACM MM'2020 (Oral). 9 pages, 4 tables, 7 figures", "journal-ref": null, "doi": "10.1145/3394171.3414017", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting task requires filling the corrupted image with contents\ncoherent with the context. This research field has achieved promising progress\nby using neural image inpainting methods. Nevertheless, there is still a\ncritical challenge in guessing the missed content with only the context pixels.\nThe goal of this paper is to fill the semantic information in corrupted images\naccording to the provided descriptive text. Unique from existing text-guided\nimage generation works, the inpainting models are required to compare the\nsemantic content of the given text and the remaining part of the image, then\nfind out the semantic content that should be filled for missing part. To\nfulfill such a task, we propose a novel inpainting model named Text-Guided Dual\nAttention Inpainting Network (TDANet). Firstly, a dual multimodal attention\nmechanism is designed to extract the explicit semantic information about the\ncorrupted regions, which is done by comparing the descriptive text and\ncomplementary image areas through reciprocal attention. Secondly, an image-text\nmatching loss is applied to maximize the semantic similarity of the generated\nimage and the text. Experiments are conducted on two open datasets. Results\nshow that the proposed TDANet model reaches new state-of-the-art on both\nquantitative and qualitative measures. Result analysis suggests that the\ngenerated images are consistent with the guidance text, enabling the generation\nof various results by providing different descriptions. Codes are available at\nhttps://github.com/idealwhite/TDANet\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:04:43 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 03:40:11 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 12:23:53 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 08:12:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Lisai", ""], ["Chen", "Qingcai", ""], ["Hu", "Baotian", ""], ["Jiang", "Shuoran", ""]]}, {"id": "2004.03222", "submitter": "Mohammad Mahdi Kazemi Moghaddam", "authors": "Mahdi Kazemi Moghaddam, Qi Wu, Ehsan Abbasnejad and Javen Qinfeng Shi", "title": "Optimistic Agent: Accurate Graph-Based Value Estimation for More\n  Successful Visual Navigation", "comments": "Accepted for publication at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We humans can impeccably search for a target object, given its name only,\neven in an unseen environment. We argue that this ability is largely due to\nthree main reasons: the incorporation of prior knowledge (or experience), the\nadaptation of it to the new environment using the observed visual cues and most\nimportantly optimistically searching without giving up early. This is currently\nmissing in the state-of-the-art visual navigation methods based on\nReinforcement Learning (RL). In this paper, we propose to use externally\nlearned prior knowledge of the relative object locations and integrate it into\nour model by constructing a neural graph. In order to efficiently incorporate\nthe graph without increasing the state-space complexity, we propose our\nGraph-based Value Estimation (GVE) module. GVE provides a more accurate\nbaseline for estimating the Advantage function in actor-critic RL algorithm.\nThis results in reduced value estimation error and, consequently, convergence\nto a more optimal policy. Through empirical studies, we show that our agent,\ndubbed as the optimistic agent, has a more realistic estimate of the state\nvalue during a navigation episode which leads to a higher success rate. Our\nextensive ablation studies show the efficacy of our simple method which\nachieves the state-of-the-art results measured by the conventional visual\nnavigation metrics, e.g. Success Rate (SR) and Success weighted by Path Length\n(SPL), in AI2THOR environment.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:31:07 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 11:30:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Moghaddam", "Mahdi Kazemi", ""], ["Wu", "Qi", ""], ["Abbasnejad", "Ehsan", ""], ["Shi", "Javen Qinfeng", ""]]}, {"id": "2004.03234", "submitter": "Aliaksandr Siarohin", "authors": "Aliaksandr Siarohin, Subhankar Roy, St\\'ephane Lathuili\\`ere, Sergey\n  Tulyakov, Elisa Ricci and Nicu Sebe", "title": "Motion-supervised Co-Part Segmentation", "comments": null, "journal-ref": "ICPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent co-part segmentation methods mostly operate in a supervised learning\nsetting, which requires a large amount of annotated data for training. To\novercome this limitation, we propose a self-supervised deep learning method for\nco-part segmentation. Differently from previous works, our approach develops\nthe idea that motion information inferred from videos can be leveraged to\ndiscover meaningful object parts. To this end, our method relies on pairs of\nframes sampled from the same video. The network learns to predict part segments\ntogether with a representation of the motion between two frames, which permits\nreconstruction of the target image. Through extensive experimental evaluation\non publicly available video sequences we demonstrate that our approach can\nproduce improved segmentation maps with respect to previous self-supervised\nco-part segmentation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:56:45 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 20:13:39 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Roy", "Subhankar", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Tulyakov", "Sergey", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "2004.03249", "submitter": "Yaoyi Li", "authors": "Yaoyi Li, Qingyao Xu, Hongtao Lu", "title": "Hierarchical Opacity Propagation for Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural image matting is a fundamental problem in computational photography\nand computer vision. Deep neural networks have seen the surge of successful\nmethods in natural image matting in recent years. In contrast to traditional\npropagation-based matting methods, some top-tier deep image matting approaches\ntend to perform propagation in the neural network implicitly. A novel structure\nfor more direct alpha matte propagation between pixels is in demand. To this\nend, this paper presents a hierarchical opacity propagation (HOP) matting\nmethod, where the opacity information is propagated in the neighborhood of each\npoint at different semantic levels. The hierarchical structure is based on one\nglobal and multiple local propagation blocks. With the HOP structure, every\nfeature point pair in high-resolution feature maps will be connected based on\nthe appearance of input image. We further propose a scale-insensitive\npositional encoding tailored for image matting to deal with the unfixed size of\ninput image and introduce the random interpolation augmentation into image\nmatting. Extensive experiments and ablation study show that HOP matting is\ncapable of outperforming state-of-the-art matting methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:39:55 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Li", "Yaoyi", ""], ["Xu", "Qingyao", ""], ["Lu", "Hongtao", ""]]}, {"id": "2004.03259", "submitter": "Lei Shi", "authors": "Lei Shi, Yifan Zhang, Jian Cheng and Hanqing Lu", "title": "What and Where: Modeling Skeletons from Semantic and Spatial\n  Perspectives for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Skeleton data, which consists of only the 2D/3D coordinates of the human\njoints, has been widely studied for human action recognition. Existing methods\ntake the semantics as prior knowledge to group human joints and draw\ncorrelations according to their spatial locations, which we call the semantic\nperspective for skeleton modeling. In this paper, in contrast to previous\napproaches, we propose to model skeletons from a novel spatial perspective,\nfrom which the model takes the spatial location as prior knowledge to group\nhuman joints and mines the discriminative patterns of local areas in a\nhierarchical manner. The two perspectives are orthogonal and complementary to\neach other; and by fusing them in a unified framework, our method achieves a\nmore comprehensive understanding of the skeleton data. Besides, we customized\ntwo networks for the two perspectives. From the semantic perspective, we\npropose a Transformer-like network that is expert in modeling joint\ncorrelations, and present three effective techniques to adapt it for skeleton\ndata. From the spatial perspective, we transform the skeleton data into the\nsparse format for efficient feature extraction and present two types of sparse\nconvolutional networks for sparse skeleton modeling. Extensive experiments are\nconducted on three challenging datasets for skeleton-based human action/gesture\nrecognition, namely, NTU-60, NTU-120 and SHREC, where our method achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:53:45 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 12:31:40 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Shi", "Lei", ""], ["Zhang", "Yifan", ""], ["Cheng", "Jian", ""], ["Lu", "Hanqing", ""]]}, {"id": "2004.03264", "submitter": "Geon Heo", "authors": "Geon Heo, Yuji Roh, Seonghyeon Hwang, Dayun Lee, Steven Euijong Whang", "title": "Inspector Gadget: A Data Programming-based Labeling System for\n  Industrial Images", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning for images becomes democratized in the Software 2.0 era,\none of the serious bottlenecks is securing enough labeled data for training.\nThis problem is especially critical in a manufacturing setting where smart\nfactories rely on machine learning for product quality control by analyzing\nindustrial images. Such images are typically large and may only need to be\npartially analyzed where only a small portion is problematic (e.g., identifying\ndefects on a surface). Since manual labeling these images is expensive, weak\nsupervision is an attractive alternative where the idea is to generate weak\nlabels that are not perfect, but can be produced at scale. Data programming is\na recent paradigm in this category where it uses human knowledge in the form of\nlabeling functions and combines them into a generative model. Data programming\nhas been successful in applications based on text or structured data and can\nalso be applied to images usually if one can find a way to convert them into\nstructured data. In this work, we expand the horizon of data programming by\ndirectly applying it to images without this conversion, which is a common\nscenario for industrial applications. We propose Inspector Gadget, an image\nlabeling system that combines crowdsourcing, data augmentation, and data\nprogramming to produce weak labels at scale for image classification. We\nperform experiments on real industrial image datasets and show that Inspector\nGadget obtains better performance than other weak-labeling techniques: Snuba,\nGOGGLES, and self-learning baselines using convolutional neural networks (CNNs)\nwithout pre-training.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:00:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 05:45:21 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 04:12:15 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Heo", "Geon", ""], ["Roh", "Yuji", ""], ["Hwang", "Seonghyeon", ""], ["Lee", "Dayun", ""], ["Whang", "Steven Euijong", ""]]}, {"id": "2004.03271", "submitter": "Christoph Baur", "authors": "Christoph Baur, Stefan Denner, Benedikt Wiestler, Shadi Albarqouni and\n  Nassir Navab", "title": "Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A\n  Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep unsupervised representation learning has recently led to new approaches\nin the field of Unsupervised Anomaly Detection (UAD) in brain MRI. The main\nprinciple behind these works is to learn a model of normal anatomy by learning\nto compress and recover healthy data. This allows to spot abnormal structures\nfrom erroneous recoveries of compressed, potentially anomalous samples. The\nconcept is of great interest to the medical image analysis community as it i)\nrelieves from the need of vast amounts of manually segmented training data---a\nnecessity for and pitfall of current supervised Deep Learning---and ii)\ntheoretically allows to detect arbitrary, even rare pathologies which\nsupervised approaches might fail to find. To date, the experimental design of\nmost works hinders a valid comparison, because i) they are evaluated against\ndifferent datasets and different pathologies, ii) use different image\nresolutions and iii) different model architectures with varying complexity. The\nintent of this work is to establish comparability among recent methods by\nutilizing a single architecture, a single resolution and the same dataset(s).\nBesides providing a ranking of the methods, we also try to answer questions\nlike i) how many healthy training subjects are needed to model normality and\nii) if the reviewed approaches are also sensitive to domain shift. Further, we\nidentify open challenges and provide suggestions for future community efforts\nand research directions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:12:07 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 08:04:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Baur", "Christoph", ""], ["Denner", "Stefan", ""], ["Wiestler", "Benedikt", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "2004.03272", "submitter": "Tong Zheng", "authors": "Tong ZHENG, Hirohisa ODA, Takayasu MORIYA, Takaaki SUGINO, Shota\n  NAKAMURA, Masahiro ODA, Masaki MORI, Hirotsugu TAKABATAKE, Hiroshi NATORI,\n  Kensaku MORI", "title": "Super-resolution of clinical CT volumes with modified CycleGAN using\n  micro CT volumes", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a super-resolution (SR) method with unpaired training\ndataset of clinical CT and micro CT volumes. For obtaining very detailed\ninformation such as cancer invasion from pre-operative clinical CT volumes of\nlung cancer patients, SR of clinical CT volumes to $\\m$}CT level is desired.\nWhile most SR methods require paired low- and high- resolution images for\ntraining, it is infeasible to obtain paired clinical CT and {\\mu}CT volumes. We\npropose a SR approach based on CycleGAN, which could perform SR on clinical CT\ninto $\\mu$CT level. We proposed new loss functions to keep cycle consistency,\nwhile training without paired volumes. Experimental results demonstrated that\nour proposed method successfully performed SR of clinical CT volume of lung\ncancer patients into $\\mu$CT level.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:12:24 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["ZHENG", "Tong", ""], ["ODA", "Hirohisa", ""], ["MORIYA", "Takayasu", ""], ["SUGINO", "Takaaki", ""], ["NAKAMURA", "Shota", ""], ["ODA", "Masahiro", ""], ["MORI", "Masaki", ""], ["TAKABATAKE", "Hirotsugu", ""], ["NATORI", "Hiroshi", ""], ["MORI", "Kensaku", ""]]}, {"id": "2004.03281", "submitter": "Shaiq Munir Malik", "authors": "Shaiq Munir Malik, Mohbat Tharani, and Murtaza Taj", "title": "Teacher-Class Network: A Neural Network Compression Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the problem of the overwhelming size of Deep Neural Networks (DNN)\nseveral compression schemes have been proposed, one of them is teacher-student.\nTeacher-student tries to transfer knowledge from a complex teacher network to a\nsimple student network. In this paper, we propose a novel method called a\nteacher-class network consisting of a single teacher and multiple student\nnetworks (i.e. class of students). Instead of transferring knowledge to one\nstudent only, the proposed method transfers a chunk of knowledge about the\nentire solution to each student. Our students are not trained for\nproblem-specific logits, they are trained to mimic knowledge (dense\nrepresentation) learned by the teacher network. Thus unlike the logits-based\nsingle student approach, the combined knowledge learned by the class of\nstudents can be used to solve other problems as well. These students can be\ndesigned to satisfy a given budget, e.g. for comparative purposes we kept the\ncollective parameters of all the students less than or equivalent to that of a\nsingle student in the teacher-student approach . These small student networks\nare trained independently, making it possible to train and deploy models on\nmemory deficient devices as well as on parallel processing systems such as data\ncenters. The proposed teacher-class architecture is evaluated on several\nbenchmark datasets including MNIST, FashionMNIST, IMDB Movie Reviews and CAMVid\non multiple tasks including classification, sentiment classification and\nsegmentation. Our approach outperforms the state-of-the-art single student\napproach in terms of accuracy as well as computational cost and in many cases\nit achieves an accuracy equivalent to the teacher network while having 10-30\ntimes fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:31:20 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 14:50:33 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Malik", "Shaiq Munir", ""], ["Tharani", "Mohbat", ""], ["Taj", "Murtaza", ""]]}, {"id": "2004.03302", "submitter": "Xukun Zhang", "authors": "Xukun Zhang and Wenxin Hu and Wen Wu", "title": "Pyramid Focusing Network for mutation prediction and classification in\n  CT images", "comments": "The carelessness of writing the paper led to the incorrect\n  experimental results shown in the paper.In addition, the experimental design\n  shown in the paper is incomplete, and I need to revise it substantially\n  before submitting it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the mutation status of genes in tumors is of great clinical\nsignificance. Recent studies have suggested that certain mutations may be\nnoninvasively predicted by studying image features of the tumors from Computed\nTomography (CT) data. Currently, this kind of image feature identification\nmethod mainly relies on manual processing to extract generalized image features\nalone or machine processing without considering the morphological differences\nof the tumor itself, which makes it difficult to achieve further breakthroughs.\nIn this paper, we propose a pyramid focusing network (PFNet) for mutation\nprediction and classification based on CT images. Firstly, we use Space Pyramid\nPooling to collect semantic cues in feature maps from multiple scales according\nto the observation that the shape and size of the tumors are varied.Secondly,\nwe improve the loss function based on the consideration that the features\nrequired for proper mutation detection are often not obvious in cross-sections\nof tumor edges, which raises more attention to these hard examples in the\nnetwork. Finally, we devise a training scheme based on data augmentation to\nenhance the generalization ability of networks. Extensively verified on\nclinical gastric CT datasets of 20 testing volumes with 63648 CT images, our\nmethod achieves the accuracy of 94.90% in predicting the HER-2 genes mutation\nstatus of at the CT image.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 12:14:05 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 06:39:59 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Xukun", ""], ["Hu", "Wenxin", ""], ["Wu", "Wen", ""]]}, {"id": "2004.03303", "submitter": "Huikai Shao", "authors": "Huikai Shao, Dexing Zhong and Xuefeng Du", "title": "Towards Efficient Unconstrained Palmprint Recognition via Deep\n  Distillation Hashing", "comments": "13 pages, 8 figures, to access database, see\n  http://gr.xjtu.edu.cn/web/bell/resource", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep palmprint recognition has become an emerging issue with great potential\nfor personal authentication on handheld and wearable consumer devices. Previous\nstudies of palmprint recognition are mainly based on constrained datasets\ncollected by dedicated devices in controlled environments, which has to reduce\nthe flexibility and convenience. In addition, general deep palmprint\nrecognition algorithms are often too heavy to meet the real-time requirements\nof embedded system. In this paper, a new palmprint benchmark is established,\nwhich consists of more than 20,000 images collected by 5 brands of smart phones\nin an unconstrained manner. Each image has been manually labeled with 14 key\npoints for region of interest (ROI) extraction. Further, the approach called\nDeep Distillation Hashing (DDH) is proposed as benchmark for efficient deep\npalmprint recognition. Palmprint images are converted to binary codes to\nimprove the efficiency of feature matching. Derived from knowledge\ndistillation, novel distillation loss functions are constructed to compress\ndeep model to further improve the efficiency of feature extraction on light\nnetwork. Comprehensive experiments are conducted on both constrained and\nunconstrained palmprint databases. Using DDH, the accuracy of palmprint\nidentification can be increased by up to 11.37%, and the Equal Error Rate (EER)\nof palmprint verification can be reduced by up to 3.11%. The results indicate\nthe feasibility of our database, and DDH can outperform other baselines to\nachieve the state-of-the-art performance. The collected dataset and related\nsource codes are publicly available at http://gr.xjtu.edu.cn/web/bell/resource.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 12:15:04 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Shao", "Huikai", ""], ["Zhong", "Dexing", ""], ["Du", "Xuefeng", ""]]}, {"id": "2004.03327", "submitter": "Xiaogang Wang", "authors": "Xiaogang Wang, Marcelo H Ang Jr, Gim Hee Lee", "title": "Cascaded Refinement Network for Point Cloud Completion", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are often sparse and incomplete. Existing shape completion\nmethods are incapable of generating details of objects or learning the complex\npoint distributions. To this end, we propose a cascaded refinement network\ntogether with a coarse-to-fine strategy to synthesize the detailed object\nshapes. Considering the local details of partial input with the global shape\ninformation together, we can preserve the existing details in the incomplete\npoint set and generate the missing parts with high fidelity. We also design a\npatch discriminator that guarantees every local area has the same pattern with\nthe ground truth to learn the complicated point distribution. Quantitative and\nqualitative experiments on different datasets show that our method achieves\nsuperior results compared to existing state-of-the-art approaches on the 3D\npoint cloud completion task. Our source code is available at\nhttps://github.com/xiaogangw/cascaded-point-completion.git.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:03:29 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 05:35:27 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 14:45:49 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Wang", "Xiaogang", ""], ["Ang", "Marcelo H", "Jr"], ["Lee", "Gim Hee", ""]]}, {"id": "2004.03332", "submitter": "Micha{\\l} Koziarski", "authors": "Micha{\\l} Koziarski", "title": "Two-Stage Resampling for Convolutional Neural Network Training in the\n  Imbalanced Colorectal Cancer Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data imbalance remains one of the open challenges in the contemporary machine\nlearning. It is especially prevalent in case of medical data, such as\nhistopathological images. Traditional data-level approaches for dealing with\ndata imbalance are ill-suited for image data: oversampling methods such as\nSMOTE and its derivatives lead to creation of unrealistic synthetic\nobservations, whereas undersampling reduces the amount of available data,\ncritical for successful training of convolutional neural networks. To alleviate\nthe problems associated with over- and undersampling we propose a novel\ntwo-stage resampling methodology, in which we initially use the oversampling\ntechniques in the image space to leverage a large amount of data for training\nof a convolutional neural network, and afterwards apply undersampling in the\nfeature space to fine-tune the last layers of the network. Experiments\nconducted on a colorectal cancer image dataset indicate the usefulness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:11:17 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 13:44:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Koziarski", "Micha\u0142", ""]]}, {"id": "2004.03333", "submitter": "Haotong Qin", "authors": "Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu\n  Sebe", "title": "Binary Neural Networks: A Survey", "comments": null, "journal-ref": "Pattern Recognition (2020) 107281", "doi": "10.1016/j.patcog.2020.107281", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary neural network, largely saving the storage and computation, serves\nas a promising technique for deploying deep models on resource-limited devices.\nHowever, the binarization inevitably causes severe information loss, and even\nworse, its discontinuity brings difficulty to the optimization of the deep\nnetwork. To address these issues, a variety of algorithms have been proposed,\nand achieved satisfying progress in recent years. In this paper, we present a\ncomprehensive survey of these algorithms, mainly categorized into the native\nsolutions directly conducting binarization, and the optimized ones using\ntechniques like minimizing the quantization error, improving the network loss\nfunction, and reducing the gradient error. We also investigate other practical\naspects of binary neural networks such as the hardware-friendly design and the\ntraining tricks. Then, we give the evaluation and discussions on different\ntasks, including image classification, object detection and semantic\nsegmentation. Finally, the challenges that may be faced in future research are\nprospected.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 16:47:20 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Qin", "Haotong", ""], ["Gong", "Ruihao", ""], ["Liu", "Xianglong", ""], ["Bai", "Xiao", ""], ["Song", "Jingkuan", ""], ["Sebe", "Nicu", ""]]}, {"id": "2004.03334", "submitter": "Sergey Tarasenko", "authors": "Sergey Tarasenko and Fumihiko Takahashi", "title": "Streaming Networks: Increase Noise Robustness and Filter Diversity via\n  Hard-wired and Input-induced Sparsity", "comments": "17 pages, 37 figures. arXiv admin note: text overlap with\n  arXiv:1910.11107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CNNs have achieved a state-of-the-art performance in many applications.\nRecent studies illustrate that CNN's recognition accuracy drops drastically if\nimages are noise corrupted. We focus on the problem of robust recognition\naccuracy of noise-corrupted images. We introduce a novel network architecture\ncalled Streaming Networks. Each stream is taking a certain intensity slice of\nthe original image as an input, and stream parameters are trained\nindependently. We use network capacity, hard-wired and input-induced sparsity\nas the dimensions for experiments. The results indicate that only the presence\nof both hard-wired and input-induces sparsity enables robust noisy image\nrecognition. Streaming Nets is the only architecture which has both types of\nsparsity and exhibits higher robustness to noise. Finally, to illustrate\nincrease in filter diversity we illustrate that a distribution of filter\nweights of the first conv layer gradually approaches uniform distribution as\nthe degree of hard-wired and domain-induced sparsity and capacities increases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:58:23 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 03:50:07 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Tarasenko", "Sergey", ""], ["Takahashi", "Fumihiko", ""]]}, {"id": "2004.03335", "submitter": "Chuan-Yung Tsai", "authors": "Zachary Polizzi, Chuan-Yung Tsai", "title": "FusedProp: Towards Efficient Training of Generative Adversarial Networks", "comments": "source code available at https://github.com/zplizzi/fusedprop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are capable of generating strikingly\nrealistic samples but state-of-the-art GANs can be extremely computationally\nexpensive to train. In this paper, we propose the fused propagation (FusedProp)\nalgorithm which can be used to efficiently train the discriminator and the\ngenerator of common GANs simultaneously using only one forward and one backward\npropagation. We show that FusedProp achieves 1.49 times the training speed\ncompared to the conventional training of GANs, although further studies are\nrequired to improve its stability. By reporting our preliminary results and\nopen-sourcing our implementation, we hope to accelerate future research on the\ntraining of GANs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 06:46:29 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Polizzi", "Zachary", ""], ["Tsai", "Chuan-Yung", ""]]}, {"id": "2004.03336", "submitter": "Ciro Javier Diaz Penedo", "authors": "Ciro Javier Diaz Penedo", "title": "Predict the model of a camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of predicting the model of a camera based\non the content of their photographs. We use two set of features, one set\nconsist in properties extracted from a Discrete Wavelet Domain (DWD) obtained\nby applying a 4 level Fast Wavelet Decomposition of the images, and a second\nset are Local Binary Patterns (LBP) features from the after filter noise of\nimages. The algorithms used for classification were Logistic regression, K-NN\nand Artificial Neural Networks\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 02:08:11 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Penedo", "Ciro Javier Diaz", ""]]}, {"id": "2004.03337", "submitter": "Andre G Hochuli", "authors": "Andre G. Hochuli, Alceu S. Britto Jr., Jean P. Barddal, Luiz E. S.\n  Oliveira, Robert Sabourin", "title": "An End-to-End Approach for Recognition of Modern and Historical\n  Handwritten Numeral Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An end-to-end solution for handwritten numeral string recognition is\nproposed, in which the numeral string is considered as composed of objects\nautomatically detected and recognized by a YoLo-based model. The main\ncontribution of this paper is to avoid heuristic-based methods for string\npreprocessing and segmentation, the need for task-oriented classifiers, and\nalso the use of specific constraints related to the string length. A robust\nexperimental protocol based on several numeral string datasets, including one\ncomposed of historical documents, has shown that the proposed method is a\nfeasible end-to-end solution for numeral string recognition. Besides, it\nreduces the complexity of the string recognition task considerably since it\ndrops out classical steps, in special preprocessing, segmentation, and a set of\nclassifiers devoted to strings with a specific length.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 16:51:00 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hochuli", "Andre G.", ""], ["Britto", "Alceu S.", "Jr."], ["Barddal", "Jean P.", ""], ["Oliveira", "Luiz E. S.", ""], ["Sabourin", "Robert", ""]]}, {"id": "2004.03338", "submitter": "Bo Huang", "authors": "Fenxi Xiao, Jie Zhang, Bo Huang, Xia Wu", "title": "Multiform Fonts-to-Fonts Translation via Style and Content Disentangled\n  Representations of Chinese Character", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly discusses the generation of personalized fonts as the\nproblem of image style transfer. The main purpose of this paper is to design a\nnetwork framework that can extract and recombine the content and style of the\ncharacters. These attempts can be used to synthesize the entire set of fonts\nwith only a small amount of characters. The paper combines various depth\nnetworks such as Convolutional Neural Network, Multi-layer Perceptron and\nResidual Network to find the optimal model to extract the features of the fonts\ncharacter. The result shows that those characters we have generated is very\nclose to real characters, using Structural Similarity index and Peak\nSignal-to-Noise Ratio evaluation criterions.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 04:30:00 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Xiao", "Fenxi", ""], ["Zhang", "Jie", ""], ["Huang", "Bo", ""], ["Wu", "Xia", ""]]}, {"id": "2004.03339", "submitter": "Bo Huang", "authors": "Fenxi Xiao, Bo Huang, Xia Wu", "title": "Automatic Generation of Chinese Handwriting via Fonts Style\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and end-to-end deep Chinese font generation system.\nThis system can generate new style fonts by interpolation of latent\nstyle-related embeding variables that could achieve smooth transition between\ndifferent style. Our method is simpler and more effective than other methods,\nwhich will help to improve the font design efficiency\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 23:34:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Xiao", "Fenxi", ""], ["Huang", "Bo", ""], ["Wu", "Xia", ""]]}, {"id": "2004.03351", "submitter": "Patrick Wspanialy", "authors": "Patrick Wspanialy, Justin Brooks, Medhat Moussa", "title": "An Image Labeling Tool and Agricultural Dataset for Deep Learning", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a labeling tool and dataset aimed to facilitate computer vision\nresearch in agriculture. The annotation tool introduces novel methods for\nlabeling with a variety of manual, semi-automatic, and fully-automatic tools.\nThe dataset includes original images collected from commercial greenhouses,\nimages from PlantVillage, and images from Google Images. Images were annotated\nwith segmentations for foreground leaf, fruit, and stem instances, and diseased\nleaf area. Labels were in an extended COCO format. In total the dataset\ncontained 10k tomatoes, 7k leaves, 2k stems, and 2k diseased leaf annotations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 13:38:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wspanialy", "Patrick", ""], ["Brooks", "Justin", ""], ["Moussa", "Medhat", ""]]}, {"id": "2004.03355", "submitter": "Ning Yu", "authors": "Ning Yu, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, Mario Fritz", "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models", "comments": "Accepted to ECCV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have brought about rapid progress\ntowards generating photorealistic images. Yet the equitable allocation of their\nmodeling capacity among subgroups has received less attention, which could lead\nto potential biases against underrepresented minorities if left uncontrolled.\nIn this work, we first formalize the problem of minority inclusion as one of\ndata coverage, and then propose to improve data coverage by harmonizing\nadversarial training with reconstructive generation. The experiments show that\nour method outperforms the existing state-of-the-art methods in terms of data\ncoverage on both seen and unseen data. We develop an extension that allows\nexplicit control over the minority subgroups that the model should ensure to\ninclude, and validate its effectiveness at little compromise from the overall\nperformance on the entire dataset. Code, models, and supplemental videos are\navailable at GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:31:33 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 15:39:33 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 01:10:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yu", "Ning", ""], ["Li", "Ke", ""], ["Zhou", "Peng", ""], ["Malik", "Jitendra", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "2004.03357", "submitter": "Chairi Kiourt", "authors": "Chairi Kiourt, George Pavlidis, Stella Markantonatou", "title": "Deep learning approaches in food recognition", "comments": "26 pages, 10 figures, book chapter for Machine Learning Paradigms -\n  Advances in Theory and Applications of Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic image-based food recognition is a particularly challenging task.\nTraditional image analysis approaches have achieved low classification accuracy\nin the past, whereas deep learning approaches enabled the identification of\nfood types and their ingredients. The contents of food dishes are typically\ndeformable objects, usually including complex semantics, which makes the task\nof defining their structure very difficult. Deep learning methods have already\nshown very promising results in such challenges, so this chapter focuses on the\npresentation of some popular approaches and techniques applied in image-based\nfood recognition. The three main lines of solutions, namely the design from\nscratch, the transfer learning and the platform-based approaches, are outlined,\nparticularly for the task at hand, and are tested and compared to reveal the\ninherent strengths and weaknesses. The chapter is complemented with basic\nbackground material, a section devoted to the relevant datasets that are\ncrucial in light of the empirical approaches adopted, and some concluding\nremarks that underline the future directions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:22:16 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 08:44:48 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Kiourt", "Chairi", ""], ["Pavlidis", "George", ""], ["Markantonatou", "Stella", ""]]}, {"id": "2004.03360", "submitter": "Tan Le", "authors": "Abrar Zahin, Le Thanh Tan, and Rose Qingyang Hu", "title": "A Machine Learning Based Framework for the Smart Healthcare Monitoring", "comments": null, "journal-ref": "2020 Intermountain Engineering, Technology and Computing (IETC)", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework for the smart healthcare system,\nwhere we employ the compressed sensing (CS) and the combination of the\nstate-of-the-art machine learning based denoiser as well as the alternating\ndirection of method of multipliers (ADMM) structure. This integration\nsignificantly simplifies the software implementation for the lowcomplexity\nencoder, thanks to the modular structure of ADMM. Furthermore, we focus on\ndetecting fall down actions from image streams. Thus, teh primary purpose of\nthus study is to reconstruct the image as visibly clear as possible and hence\nit helps the detection step at the trained classifier. For this efficient smart\nhealth monitoring framework, we employ the trained binary convolutional neural\nnetwork (CNN) classifier for the fall-action classifier, because this scheme is\na part of surveillance scenario. In this scenario, we deal with the fallimages,\nthus, we compress, transmit and reconstruct the fallimages. Experimental\nresults demonstrate the impacts of network parameters and the significant\nperformance gain of the proposal compared to traditional methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:41:28 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zahin", "Abrar", ""], ["Tan", "Le Thanh", ""], ["Hu", "Rose Qingyang", ""]]}, {"id": "2004.03364", "submitter": "S\\'andor K\\'onya Dr.", "authors": "Sandor Konya, Sai Natarajan T R, Hassan Allouch, Kais Abu Nahleh,\n  Omneya Yakout Dogheim, Heinrich Boehm", "title": "Convolutional Neural Networks based automated segmentation and labelling\n  of the lumbar spine X-ray", "comments": "Submitted to Medical & Biological Engineering & Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is to investigate the segmentation accuracies of\ndifferent segmentation networks trained on 730 manually annotated lateral\nlumbar spine X-rays. Instance segmentation networks were compared to semantic\nsegmentation networks. The study cohort comprised diseased spines and\npostoperative images with metallic implants. The average mean accuracy and mean\nintersection over union (IoU) was up to 3 percent better for the best\nperforming instance segmentation model, the average pixel accuracy and weighted\nIoU were slightly better for the best performing semantic segmentation model.\nMoreover, the inferences of the instance segmentation models are easier to\nimplement for further processing pipelines in clinical decision support.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:15:03 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Konya", "Sandor", ""], ["R", "Sai Natarajan T", ""], ["Allouch", "Hassan", ""], ["Nahleh", "Kais Abu", ""], ["Dogheim", "Omneya Yakout", ""], ["Boehm", "Heinrich", ""]]}, {"id": "2004.03366", "submitter": "David Noever", "authors": "David A. Noever, Sam E. Miller Noever", "title": "Knife and Threat Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite rapid advances in image-based machine learning, the threat\nidentification of a knife wielding attacker has not garnered substantial\nacademic attention. This relative research gap appears less understandable\ngiven the high knife assault rate (>100,000 annually) and the increasing\navailability of public video surveillance to analyze and forensically document.\nWe present three complementary methods for scoring automated threat\nidentification using multiple knife image datasets, each with the goal of\nnarrowing down possible assault intentions while minimizing misidentifying\nfalse positives and risky false negatives. To alert an observer to the\nknife-wielding threat, we test and deploy classification built around MobileNet\nin a sparse and pruned neural network with a small memory requirement (< 2.2\nmegabytes) and 95% test accuracy. We secondly train a detection algorithm\n(MaskRCNN) to segment the hand from the knife in a single image and assign\nprobable certainty to their relative location. This segmentation accomplishes\nboth localization with bounding boxes but also relative positions to infer\noverhand threats. A final model built on the PoseNet architecture assigns\nanatomical waypoints or skeletal features to narrow the threat characteristics\nand reduce misunderstood intentions. We further identify and supplement\nexisting data gaps that might blind a deployed knife threat detector such as\ncollecting innocuous hand and fist images as important negative training sets.\nWhen automated on commodity hardware and software solutions one original\nresearch contribution is this systematic survey of timely and readily available\nimage-based alerts to task and prioritize crime prevention countermeasures\nprior to a tragic outcome.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 12:41:28 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 14:36:29 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Noever", "David A.", ""], ["Noever", "Sam E. Miller", ""]]}, {"id": "2004.03370", "submitter": "Victor Lorena de Farias Souza", "authors": "Victor L. F. Souza, Adriano L. I. Oliveira, Rafael M. O. Cruz, Robert\n  Sabourin", "title": "A white-box analysis on the writer-independent dichotomy transformation\n  applied to offline handwritten signature verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High number of writers, small number of training samples per writer with high\nintra-class variability and heavily imbalanced class distributions are among\nthe challenges and difficulties of the offline Handwritten Signature\nVerification (HSV) problem. A good alternative to tackle these issues is to use\na writer-independent (WI) framework. In WI systems, a single model is trained\nto perform signature verification for all writers from a dissimilarity space\ngenerated by the dichotomy transformation. Among the advantages of this\nframework is its scalability to deal with some of these challenges and its ease\nin managing new writers, and hence of being used in a transfer learning\ncontext. In this work, we present a white-box analysis of this approach\nhighlighting how it handles the challenges, the dynamic selection of references\nthrough fusion function, and its application for transfer learning. All the\nanalyses are carried out at the instance level using the instance hardness (IH)\nmeasure. The experimental results show that, using the IH analysis, we were\nable to characterize \"good\" and \"bad\" quality skilled forgeries as well as the\nfrontier region between positive and negative samples. This enables futures\ninvestigations on methods for improving discrimination between genuine\nsignatures and skilled forgeries by considering these characterizations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 19:59:34 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 17:51:28 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Souza", "Victor L. F.", ""], ["Oliveira", "Adriano L. I.", ""], ["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""]]}, {"id": "2004.03373", "submitter": "Victor Lorena de Farias Souza", "authors": "Victor L. F. Souza, Adriano L. I. Oliveira, Rafael M. O. Cruz, Robert\n  Sabourin", "title": "Improving BPSO-based feature selection applied to offline WI handwritten\n  signature verification through overfitting control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the presence of overfitting when using Binary\nParticle Swarm Optimization (BPSO) to perform the feature selection in a\ncontext of Handwritten Signature Verification (HSV). SigNet is a state of the\nart Deep CNN model for feature representation in the HSV context and contains\n2048 dimensions. Some of these dimensions may include redundant information in\nthe dissimilarity representation space generated by the dichotomy\ntransformation (DT) used by the writer-independent (WI) approach. The analysis\nis carried out on the GPDS-960 dataset. Experiments demonstrate that the\nproposed method is able to control overfitting during the search for the most\ndiscriminant representation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:42:29 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 11:18:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Souza", "Victor L. F.", ""], ["Oliveira", "Adriano L. I.", ""], ["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""]]}, {"id": "2004.03374", "submitter": "Gal Oren", "authors": "Re'em Harel, Matan Rusanovsky, Yehonatan Fridman, Assaf Shimony, Gal\n  Oren", "title": "Complete CVDL Methodology for Investigating Hydrodynamic Instabilities", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CV cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fluid dynamics, one of the most important research fields is hydrodynamic\ninstabilities and their evolution in different flow regimes. The investigation\nof said instabilities is concerned with the highly non-linear dynamics.\nCurrently, three main methods are used for understanding of such phenomenon -\nnamely analytical models, experiments and simulations - and all of them are\nprimarily investigated and correlated using human expertise. In this work we\nclaim and demonstrate that a major portion of this research effort could and\nshould be analysed using recent breakthrough advancements in the field of\nComputer Vision with Deep Learning (CVDL, or Deep Computer-Vision).\nSpecifically, we target and evaluate specific state-of-the-art techniques -\nsuch as Image Retrieval, Template Matching, Parameters Regression and\nSpatiotemporal Prediction - for the quantitative and qualitative benefits they\nprovide. In order to do so we focus in this research on one of the most\nrepresentative instabilities, the Rayleigh-Taylor one, simulate its behaviour\nand create an open-sourced state-of-the-art annotated database (RayleAI).\nFinally, we use adjusted experimental results and novel physical loss\nmethodologies to validate the correspondence of the predicted results to actual\nphysical reality to prove the models efficiency. The techniques which were\ndeveloped and proved in this work can be served as essential tools for\nphysicists in the field of hydrodynamics for investigating a variety of\nphysical systems, and also could be used via Transfer Learning to other\ninstabilities research. A part of the techniques can be easily applied on\nalready exist simulation results. All models as well as the data-set that was\ncreated for this work, are publicly available at:\nhttps://github.com/scientific-computing-nrcn/SimulAI.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:52:04 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 06:52:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Harel", "Re'em", ""], ["Rusanovsky", "Matan", ""], ["Fridman", "Yehonatan", ""], ["Shimony", "Assaf", ""], ["Oren", "Gal", ""]]}, {"id": "2004.03375", "submitter": "Dario Sitnik", "authors": "Dario Sitnik and Ivica Kopriva", "title": "Robust Self-Supervised Convolutional Neural Network for Subspace\n  Clustering and Classification", "comments": "15 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insufficient capability of existing subspace clustering methods to handle\ndata coming from nonlinear manifolds, data corruptions, and out-of-sample data\nhinders their applicability to address real-world clustering and classification\nproblems. This paper proposes the robust formulation of the self-supervised\nconvolutional subspace clustering network ($S^2$ConvSCN) that incorporates the\nfully connected (FC) layer and, thus, it is capable for handling out-of-sample\ndata by classifying them using a softmax classifier. $S^2$ConvSCN clusters data\ncoming from nonlinear manifolds by learning the linear self-representation\nmodel in the feature space. Robustness to data corruptions is achieved by using\nthe correntropy induced metric (CIM) of the error. Furthermore, the\nblock-diagonal (BD) structure of the representation matrix is enforced\nexplicitly through BD regularization. In a truly unsupervised training\nenvironment, Robust $S^2$ConvSCN outperforms its baseline version by a\nsignificant amount for both seen and unseen data on four well-known datasets.\nArguably, such an ablation study has not been reported before.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:07:58 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sitnik", "Dario", ""], ["Kopriva", "Ivica", ""]]}, {"id": "2004.03376", "submitter": "Kaveena Persand", "authors": "Kaveena Persand, Andrew Anderson, David Gregg", "title": "Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle", "comments": null, "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)", "doi": "10.1109/SSCI47803.2020.9308157", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation and memory needed for Convolutional Neural Network (CNN)\ninference can be reduced by pruning weights from the trained network. Pruning\nis guided by a pruning saliency, which heuristically approximates the change in\nthe loss function associated with the removal of specific weights. Many pruning\nsignals have been proposed, but the performance of each heuristic depends on\nthe particular trained network. This leaves the data scientist with a difficult\nchoice. When using any one saliency metric for the entire pruning process, we\nrun the risk of the metric assumptions being invalidated, leading to poor\ndecisions being made by the metric. Ideally we could combine the best aspects\nof different saliency metrics. However, despite an extensive literature review,\nwe are unable to find any prior work on composing different saliency metrics.\nThe chief difficulty lies in combining the numerical output of different\nsaliency metrics, which are not directly comparable.\n  We propose a method to compose several primitive pruning saliencies, to\nexploit the cases where each saliency measure does well. Our experiments show\nthat the composition of saliencies avoids many poor pruning choices identified\nby individual saliencies. In most cases our method finds better selections than\neven the best individual pruning saliency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 11:29:41 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 12:56:50 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Persand", "Kaveena", ""], ["Anderson", "Andrew", ""], ["Gregg", "David", ""]]}, {"id": "2004.03378", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani, Veeru Talreja, Matthew C. Valenti, Nasser M.\n  Nasrabadi", "title": "Error-Corrected Margin-Based Deep Cross-Modal Hashing for Facial Image\n  Retrieval", "comments": "IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal hashing facilitates mapping of heterogeneous multimedia data into\na common Hamming space, which can beutilized for fast and flexible retrieval\nacross different modalities. In this paper, we propose a novel cross-modal\nhashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which\nuses a binary vector specifying the presence of certainfacial attributes as an\ninput query to retrieve relevant face images from a database. The DNDCMH\nnetwork consists of two separatecomponents: an attribute-based deep cross-modal\nhashing (ADCMH) module, which uses a margin (m)-based loss function\ntoefficiently learn compact binary codes to preserve similarity between\nmodalities in the Hamming space, and a neural error correctingdecoder (NECD),\nwhich is an error correcting decoder implemented with a neural network. The\ngoal of NECD network in DNDCMH isto error correct the hash codes generated by\nADCMH to improve the retrieval efficiency. The NECD network is trained such\nthat it hasan error correcting capability greater than or equal to the margin\n(m) of the margin-based loss function. This results in NECD cancorrect the\ncorrupted hash codes generated by ADCMH up to the Hamming distance of m. We\nhave evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing\nmethods on standard datasets to demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 08:20:08 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Talreja", "Veeru", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2004.03383", "submitter": "Subhashini Venugopalan", "authors": "Shawn Xu, Subhashini Venugopalan, Mukund Sundararajan", "title": "Attribution in Scale and Space", "comments": "CVPR 2020 camera-ready. Code is available at\n  https://github.com/PAIR-code/saliency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the attribution problem [28] for deep networks applied to perception\ntasks. For vision tasks, attribution techniques attribute the prediction of a\nnetwork to the pixels of the input image. We propose a new technique called\n\\emph{Blur Integrated Gradients}. This technique has several advantages over\nother methods. First, it can tell at what scale a network recognizes an object.\nIt produces scores in the scale/frequency dimension, that we find captures\ninteresting phenomena. Second, it satisfies the scale-space axioms [14], which\nimply that it employs perturbations that are free of artifact. We therefore\nproduce explanations that are cleaner and consistent with the operation of deep\nnetworks. Third, it eliminates the need for a 'baseline' parameter for\nIntegrated Gradients [31] for perception tasks. This is desirable because the\nchoice of baseline has a significant effect on the explanations. We compare the\nproposed technique against previous techniques and demonstrate application on\nthree tasks: ImageNet object recognition, Diabetic Retinopathy prediction, and\nAudioSet audio event identification.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:04:16 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 16:41:12 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Xu", "Shawn", ""], ["Venugopalan", "Subhashini", ""], ["Sundararajan", "Mukund", ""]]}, {"id": "2004.03385", "submitter": "Mat\\'ias  Di Martino", "authors": "J. Matias Di Martino, Fernando Suzacq, Mauricio Delbracio, Qiang Qiu,\n  and Guillermo Sapiro", "title": "Differential 3D Facial Recognition: Adding 3D to Your State-of-the-Art\n  2D Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Active illumination is a prominent complement to enhance 2D face recognition\nand make it more robust, e.g., to spoofing attacks and low-light conditions. In\nthe present work we show that it is possible to adopt active illumination to\nenhance state-of-the-art 2D face recognition approaches with 3D features, while\nbypassing the complicated task of 3D reconstruction. The key idea is to project\nover the test face a high spatial frequency pattern, which allows us to\nsimultaneously recover real 3D information plus a standard 2D facial image.\nTherefore, state-of-the-art 2D face recognition solution can be transparently\napplied, while from the high frequency component of the input image,\ncomplementary 3D facial features are extracted. Experimental results on ND-2006\ndataset show that the proposed ideas can significantly boost face recognition\nperformance and dramatically improve the robustness to spoofing attacks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 20:17:14 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Di Martino", "J. Matias", ""], ["Suzacq", "Fernando", ""], ["Delbracio", "Mauricio", ""], ["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "2004.03399", "submitter": "Karim Hammoudi", "authors": "Karim Hammoudi and Halim Benhabiles and Mahmoud Melkemi and Fadi\n  Dornaika and Ignacio Arganda-Carreras and Dominique Collard and Arnaud\n  Scherpereel", "title": "Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia\n  Cases at the Era of COVID-19", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is an infectious disease with first\nsymptoms similar to the flu. COVID-19 appeared first in China and very quickly\nspreads to the rest of the world, causing then the 2019-20 coronavirus\npandemic. In many cases, this disease causes pneumonia. Since pulmonary\ninfections can be observed through radiography images, this paper investigates\ndeep learning methods for automatically analyzing query chest X-ray images with\nthe hope to bring precision tools to health professionals towards screening the\nCOVID-19 and diagnosing confirmed patients. In this context, training datasets,\ndeep learning architectures and analysis strategies have been experimented from\npublicly open sets of chest X-ray images. Tailored deep learning models are\nproposed to detect pneumonia infection cases, notably viral cases. It is\nassumed that viral pneumonia cases detected during an epidemic COVID-19 context\nhave a high probability to presume COVID-19 infections. Moreover, easy-to-apply\nhealth indicators are proposed for estimating infection status and predicting\npatient status from the detected pneumonia cases. Experimental results show\npossibilities of training deep learning models over publicly open sets of chest\nX-ray images towards screening viral pneumonia. Chest X-ray test images of\nCOVID-19 infected patients are successfully diagnosed through detection models\nretained for their performances. The efficiency of proposed health indicators\nis highlighted through simulated scenarios of patients presenting infections\nand health problems by combining real and synthetic health data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 21:30:54 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hammoudi", "Karim", ""], ["Benhabiles", "Halim", ""], ["Melkemi", "Mahmoud", ""], ["Dornaika", "Fadi", ""], ["Arganda-Carreras", "Ignacio", ""], ["Collard", "Dominique", ""], ["Scherpereel", "Arnaud", ""]]}, {"id": "2004.03401", "submitter": "Yang Zheng", "authors": "Yang Zheng, Izzat H. Izzat, Sanling Song", "title": "MNEW: Multi-domain Neighborhood Embedding and Weighting for Sparse Point\n  Clouds Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds have been widely adopted in 3D semantic scene understanding.\nHowever, point clouds for typical tasks such as 3D shape segmentation or indoor\nscenario parsing are much denser than outdoor LiDAR sweeps for the application\nof autonomous driving perception. Due to the spatial property disparity, many\nsuccessful methods designed for dense point clouds behave depreciated\neffectiveness on the sparse data. In this paper, we focus on the semantic\nsegmentation task of sparse outdoor point clouds. We propose a new method\ncalled MNEW, including multi-domain neighborhood embedding, and attention\nweighting based on their geometry distance, feature similarity, and\nneighborhood sparsity. The network architecture inherits PointNet which\ndirectly process point clouds to capture pointwise details and global\nsemantics, and is improved by involving multi-scale local neighborhoods in\nstatic geometry domain and dynamic feature space. The distance/similarity\nattention and sparsity-adapted weighting mechanism of MNEW enable its\ncapability for a wide range of data sparsity distribution. With experiments\nconducted on virtual and real KITTI semantic datasets, MNEW achieves the top\nperformance for sparse point clouds, which is important to the application of\nLiDAR-based automated driving perception.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 18:02:07 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zheng", "Yang", ""], ["Izzat", "Izzat H.", ""], ["Song", "Sanling", ""]]}, {"id": "2004.03402", "submitter": "Yifang Deng", "authors": "Vic Patrangenaru and Yifang Deng", "title": "Nonparametric Data Analysis on the Space of Perceived Colors", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Moving around in a 3D world, requires the visual system of a living\nindividual to rely on three channels of image recognition, which is done\nthrough three types of retinal cones. Newton, Grasmann, Helmholz and\nSchr$\\ddot{o}$dinger laid down the basic assumptions needed to understand\ncolored vision. Such concepts were furthered by Resnikoff, who imagined the\nspace of perceived colors as a 3D homogeneous space.\n  This article is concerned with perceived colors regarded as random objects on\na Resnikoff 3D homogeneous space model. Two applications to color\ndifferentiation in machine vision are illustrated for the proposed statistical\nmethodology, applied to the Euclidean model for perceived colors.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 17:43:33 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Patrangenaru", "Vic", ""], ["Deng", "Yifang", ""]]}, {"id": "2004.03408", "submitter": "Misgina Tsighe Hagos", "authors": "Misgina Tsighe Hagos, Shri Kant, Surayya Ado Bala", "title": "Automated Smartphone based System for Diagnosis of Diabetic Retinopathy", "comments": "12 pages, 4 figures, 4 tables, 1 appendix. Copyright \\copyright 2019,\n  IEEE. Published in: 2019 International Conference on Computing,\n  Communication, and Intelligent Systems (ICCCIS)", "journal-ref": "International Conference on Computing, Communication, and\n  Intelligent Systems (ICCCIS), Greater Noida, India, 2019, pp. 256-261", "doi": "10.1109/ICCCIS48478.2019.8974492", "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of diabetic retinopathy for treatment of the disease has been\nfailing to reach diabetic people living in rural areas. Shortage of trained\nophthalmologists, limited availability of healthcare centers, and expensiveness\nof diagnostic equipment are among the reasons. Although many deep\nlearning-based automatic diagnosis of diabetic retinopathy techniques have been\nimplemented in the literature, these methods still fail to provide a\npoint-of-care diagnosis. This raises the need for an independent diagnostic of\ndiabetic retinopathy that can be used by a non-expert. Recently the usage of\nsmartphones has been increasing across the world. Automated diagnoses of\ndiabetic retinopathy can be deployed on smartphones in order to provide an\ninstant diagnosis to diabetic people residing in remote areas. In this paper,\ninception based convolutional neural network and binary decision tree-based\nensemble of classifiers have been proposed and implemented to detect and\nclassify diabetic retinopathy. The proposed method was further imported into a\nsmartphone application for mobile-based classification, which provides an\noffline and automatic system for diagnosis of diabetic retinopathy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:01:36 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hagos", "Misgina Tsighe", ""], ["Kant", "Shri", ""], ["Bala", "Surayya Ado", ""]]}, {"id": "2004.03431", "submitter": "Nilanjan Dey", "authors": "V. Rajinikanth, Nilanjan Dey, Alex Noel Joseph Raj, Aboul Ella\n  Hassanien, K.C. Santosh, N. Sri Madhava Raja", "title": "Harmony-Search and Otsu based System for Coronavirus Disease (COVID-19)\n  Detection using Lung CT Scan Images", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pneumonia is one of the foremost lung diseases and untreated pneumonia will\nlead to serious threats for all age groups. The proposed work aims to extract\nand evaluate the Coronavirus disease (COVID-19) caused pneumonia infection in\nlung using CT scans. We propose an image-assisted system to extract COVID-19\ninfected sections from lung CT scans (coronal view). It includes following\nsteps: (i) Threshold filter to extract the lung region by eliminating possible\nartifacts; (ii) Image enhancement using Harmony-Search-Optimization and Otsu\nthresholding; (iii) Image segmentation to extract infected region(s); and (iv)\nRegion-of-interest (ROI) extraction (features) from binary image to compute\nlevel of severity. The features that are extracted from ROI are then employed\nto identify the pixel ratio between the lung and infection sections to identify\ninfection level of severity. The primary objective of the tool is to assist the\npulmonologist not only to detect but also to help plan treatment process. As a\nconsequence, for mass screening processing, it will help prevent diagnostic\nburden.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:55:22 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Rajinikanth", "V.", ""], ["Dey", "Nilanjan", ""], ["Raj", "Alex Noel Joseph", ""], ["Hassanien", "Aboul Ella", ""], ["Santosh", "K. C.", ""], ["Raja", "N. Sri Madhava", ""]]}, {"id": "2004.03449", "submitter": "Farzan Erlik Nowruzi", "authors": "Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Fahed Al\n  Hassanat, Elnaz Jahani Heravi, Robert Laganiere, Julien Rebut, Waqas Malik", "title": "Deep Open Space Segmentation using Automotive Radar", "comments": "IEEE MTT-S International Conference on Microwaves for Intelligent\n  Mobility (ICMIM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the use of radar with advanced deep segmentation\nmodels to identify open space in parking scenarios. A publically available\ndataset of radar observations called SCORP was collected. Deep models are\nevaluated with various radar input representations. Our proposed approach\nachieves low memory usage and real-time processing speeds, and is thus very\nwell suited for embedded deployment.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:49:29 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nowruzi", "Farzan Erlik", ""], ["Kolhatkar", "Dhanvin", ""], ["Kapoor", "Prince", ""], ["Hassanat", "Fahed Al", ""], ["Heravi", "Elnaz Jahani", ""], ["Laganiere", "Robert", ""], ["Rebut", "Julien", ""], ["Malik", "Waqas", ""]]}, {"id": "2004.03450", "submitter": "Chenming Wu", "authors": "Chenming Wu, Yong-Jin Liu, Charlie C.L. Wang", "title": "Learning to Accelerate Decomposition for Multi-Directional 3D Printing", "comments": "8 pages, accepted by IEEE Robotics and Automation Letters 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-directional 3D printing has the capability of decreasing or eliminating\nthe need for support structures. Recent work proposed a beam-guided search\nalgorithm to find an optimized sequence of plane-clipping, which gives volume\ndecomposition of a given 3D model. Different printing directions are employed\nin different regions to fabricate a model with tremendously less support (or\neven no support in many cases).To obtain optimized decomposition, a large beam\nwidth needs to be used in the search algorithm, leading to a very\ntime-consuming computation. In this paper, we propose a learning framework that\ncan accelerate the beam-guided search by using a smaller number of the original\nbeam width to obtain results with similar quality. Specifically, we use the\nresults of beam-guided search with large beam width to train a scoring function\nfor candidate clipping planes based on six newly proposed feature metrics. With\nthe help of these feature metrics, both the current and the sequence-dependent\ninformation are captured by the neural network to score candidates of clipping.\nAs a result, we can achieve around 3x computational speed. We test and\ndemonstrate our accelerated decomposition on a large dataset of models for 3D\nprinting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 18:37:44 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 06:00:49 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 04:50:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wu", "Chenming", ""], ["Liu", "Yong-Jin", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "2004.03451", "submitter": "Matthew Gadd", "authors": "Prannay Kaul, Daniele De Martini, Matthew Gadd, Paul Newman", "title": "RSS-Net: Weakly-Supervised Multi-Class Semantic Segmentation with FMCW\n  Radar", "comments": "submitted to IEEE Intelligent Vehicles Symposium (IV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient annotation procedure and an application\nthereof to end-to-end, rich semantic segmentation of the sensed environment\nusing FMCW scanning radar. We advocate radar over the traditional sensors used\nfor this task as it operates at longer ranges and is substantially more robust\nto adverse weather and illumination conditions. We avoid laborious manual\nlabelling by exploiting the largest radar-focused urban autonomy dataset\ncollected to date, correlating radar scans with RGB cameras and LiDAR sensors,\nfor which semantic segmentation is an already consolidated procedure. The\ntraining procedure leverages a state-of-the-art natural image segmentation\nsystem which is publicly available and as such, in contrast to previous\napproaches, allows for the production of copious labels for the radar stream by\nincorporating four camera and two LiDAR streams. Additionally, the losses are\ncomputed taking into account labels to the radar sensor horizon by accumulating\nLiDAR returns along a pose-chain ahead and behind of the current vehicle\nposition. Finally, we present the network with multi-channel radar scan inputs\nin order to deal with ephemeral and dynamic scene objects.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 11:40:26 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kaul", "Prannay", ""], ["De Martini", "Daniele", ""], ["Gadd", "Matthew", ""], ["Newman", "Paul", ""]]}, {"id": "2004.03452", "submitter": "Jason Stock", "authors": "Jason Stock, Andy Dolan, and Tom Cavey", "title": "Strategies for Robust Image Classification", "comments": "15 pages, and 39 figure (with Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we evaluate the impact of digitally altered images on the\nperformance of artificial neural networks. We explore factors that negatively\naffect the ability of an image classification model to produce consistent and\naccurate results. A model's ability to classify is negatively influenced by\nalterations to images as a result of digital abnormalities or changes in the\nphysical environment. The focus of this paper is to discover and replicate\nscenarios that modify the appearance of an image and evaluate them on\nstate-of-the-art machine learning models. Our contributions present various\ntraining techniques that enhance a model's ability to generalize and improve\nrobustness against these alterations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 21:22:39 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 16:50:35 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Stock", "Jason", ""], ["Dolan", "Andy", ""], ["Cavey", "Tom", ""]]}, {"id": "2004.03453", "submitter": "Brian Reily", "authors": "Brian Reily, Qingzhao Zhu, Christopher Reardon, and Hao Zhang", "title": "Simultaneous Learning from Human Pose and Object Cues for Real-Time\n  Activity Recognition", "comments": "Accepted to International Conference on Robotics and Automation\n  (ICRA) 2020, IEEE copyright", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time human activity recognition plays an essential role in real-world\nhuman-centered robotics applications, such as assisted living and human-robot\ncollaboration. Although previous methods based on skeletal data to encode human\nposes showed promising results on real-time activity recognition, they lacked\nthe capability to consider the context provided by objects within the scene and\nin use by the humans, which can provide a further discriminant between human\nactivity categories. In this paper, we propose a novel approach to real-time\nhuman activity recognition, through simultaneously learning from observations\nof both human poses and objects involved in the human activity. We formulate\nhuman activity recognition as a joint optimization problem under a unified\nmathematical framework, which uses a regression-like loss function to integrate\nhuman pose and object cues and defines structured sparsity-inducing norms to\nidentify discriminative body joints and object attributes. To evaluate our\nmethod, we perform extensive experiments on two benchmark datasets and a\nphysical robot in a home assistance setting. Experimental results have shown\nthat our method outperforms previous methods and obtains real-time performance\nfor human activity recognition with a processing speed of 10^4 Hz.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 22:04:37 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Reily", "Brian", ""], ["Zhu", "Qingzhao", ""], ["Reardon", "Christopher", ""], ["Zhang", "Hao", ""]]}, {"id": "2004.03458", "submitter": "Yajie Zhao", "authors": "Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara, Owen Ingraham,\n  Pengda Xiang, Xinglei Ren, Pratusha Prasad, Bipin Kishore, Jun Xing, Hao Li", "title": "Learning Formation of Physically-Based Face Attributes", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a combined data set of 4000 high resolution facial scans, we\nintroduce a non-linear morphable face model, capable of producing multifarious\nface geometry of pore-level resolution, coupled with material attributes for\nuse in physically-based rendering. We aim to maximize the variety of face\nidentities, while increasing the robustness of correspondence between unique\ncomponents, including middle-frequency geometry, albedo maps, specular\nintensity maps and high-frequency displacement details. Our deep learning based\ngenerative model learns to correlate albedo and geometry, which ensures the\nanatomical correctness of the generated assets. We demonstrate potential use of\nour generative model for novel identity generation, model fitting,\ninterpolation, animation, high fidelity data visualization, and low-to-high\nresolution data domain transferring. We hope the release of this generative\nmodel will encourage further cooperation between all graphics, vision, and data\nfocused professionals while demonstrating the cumulative value of every\nindividual's complete biometric profile.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 07:01:30 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 03:29:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Li", "Ruilong", ""], ["Bladin", "Karl", ""], ["Zhao", "Yajie", ""], ["Chinara", "Chinmay", ""], ["Ingraham", "Owen", ""], ["Xiang", "Pengda", ""], ["Ren", "Xinglei", ""], ["Prasad", "Pratusha", ""], ["Kishore", "Bipin", ""], ["Xing", "Jun", ""], ["Li", "Hao", ""]]}, {"id": "2004.03459", "submitter": "Ankit Dhall", "authors": "Ankit Dhall, Anastasia Makarova, Octavian Ganea, Dario Pavllo, Michael\n  Greeff, Andreas Krause", "title": "Hierarchical Image Classification using Entailment Cone Embeddings", "comments": "Accepted in the CVPR 2020 Workshop on Differential Geometry in\n  Computer Vision and Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has been studied extensively, but there has been limited\nwork in using unconventional, external guidance other than traditional\nimage-label pairs for training. We present a set of methods for leveraging\ninformation about the semantic hierarchy embedded in class labels. We first\ninject label-hierarchy knowledge into an arbitrary CNN-based classifier and\nempirically show that availability of such external semantic information in\nconjunction with the visual semantics from images boosts overall performance.\nTaking a step further in this direction, we model more explicitly the\nlabel-label and label-image interactions using order-preserving embeddings\ngoverned by both Euclidean and hyperbolic geometries, prevalent in natural\nlanguage, and tailor them to hierarchical image classification and\nrepresentation learning. We empirically validate all the models on the\nhierarchical ETHEC dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 10:22:02 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 12:56:07 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dhall", "Ankit", ""], ["Makarova", "Anastasia", ""], ["Ganea", "Octavian", ""], ["Pavllo", "Dario", ""], ["Greeff", "Michael", ""], ["Krause", "Andreas", ""]]}, {"id": "2004.03466", "submitter": "Shuhang Wang", "authors": "Shuhang Wang, Szu-Yeu Hu, Eugene Cheah, Xiaohong Wang, Jingchao Wang,\n  Lei Chen, Masoud Baikpour, Arinc Ozturk, Qian Li, Shinn-Huey Chou, Constance\n  D. Lehman, Viksit Kumar, Anthony Samir", "title": "U-Net Using Stacked Dilated Convolutions for Medical Image Segmentation", "comments": "8 pages MICCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel U-Net variant using stacked dilated convolutions\nfor medical image segmentation (SDU-Net). SDU-Net adopts the architecture of\nvanilla U-Net with modifications in the encoder and decoder operations (an\noperation indicates all the processing for feature maps of the same\nresolution). Unlike vanilla U-Net which incorporates two standard convolutions\nin each encoder/decoder operation, SDU-Net uses one standard convolution\nfollowed by multiple dilated convolutions and concatenates all dilated\nconvolution outputs as input to the next operation. Experiments showed that\nSDU-Net outperformed vanilla U-Net, attention U-Net (AttU-Net), and recurrent\nresidual U-Net (R2U-Net) in all four tested segmentation tasks while using\nparameters around 40% of vanilla U-Net's, 17% of AttU-Net's, and 15% of\nR2U-Net's.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:06:20 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 13:27:16 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Wang", "Shuhang", ""], ["Hu", "Szu-Yeu", ""], ["Cheah", "Eugene", ""], ["Wang", "Xiaohong", ""], ["Wang", "Jingchao", ""], ["Chen", "Lei", ""], ["Baikpour", "Masoud", ""], ["Ozturk", "Arinc", ""], ["Li", "Qian", ""], ["Chou", "Shinn-Huey", ""], ["Lehman", "Constance D.", ""], ["Kumar", "Viksit", ""], ["Samir", "Anthony", ""]]}, {"id": "2004.03468", "submitter": "Ivan Matvienko", "authors": "Ivan Matvienko, Mikhail Gasanov, Anna Petrovskaia, Raghavendra Belur\n  Jana, Maria Pukalchik, Ivan Oseledets", "title": "Bayesian aggregation improves traditional single image crop\n  classification approaches", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) methods and neural networks (NN) are widely implemented\nfor crop types recognition and classification based on satellite images.\nHowever, most of these studies use several multi-temporal images which could be\ninapplicable for cloudy regions. We present a comparison between the classical\nML approaches and U-Net NN for classifying crops with a single satellite image.\nThe results show the advantages of using field-wise classification over\npixel-wise approach. We first used a Bayesian aggregation for field-wise\nclassification and improved on 1.5% results between majority voting\naggregation. The best result for single satellite image crop classification is\nachieved for gradient boosting with an overall accuracy of 77.4% and macro\nF1-score 0.66.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:14:03 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Matvienko", "Ivan", ""], ["Gasanov", "Mikhail", ""], ["Petrovskaia", "Anna", ""], ["Jana", "Raghavendra Belur", ""], ["Pukalchik", "Maria", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2004.03531", "submitter": "Mar\\'ia J. G\\'omez-Silva", "authors": "Mar\\'ia J. G\\'omez-Silva", "title": "Deep Multi-Shot Network for modelling Appearance Similarity in\n  Multi-Person Tracking applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatization of Multi-Object Tracking becomes a demanding task in real\nunconstrained scenarios, where the algorithms have to deal with crowds,\ncrossing people, occlusions, disappearances and the presence of visually\nsimilar individuals. In those circumstances, the data association between the\nincoming detections and their corresponding identities could miss some tracks\nor produce identity switches. In order to reduce these tracking errors, and\neven their propagation in further frames, this article presents a Deep\nMulti-Shot neural model for measuring the Degree of Appearance Similarity\n(MS-DoAS) between person observations. This model provides temporal consistency\nto the individuals' appearance representation, and provides an affinity metric\nto perform frame-by-frame data association, allowing online tracking. The model\nhas been deliberately trained to be able to manage the presence of previous\nidentity switches and missed observations in the handled tracks. With that\npurpose, a novel data generation tool has been designed to create training\ntracklets that simulate such situations. The model has demonstrated a high\ncapacity to discern when a new observation corresponds to a certain track,\nachieving a classification accuracy of 97\\% in a hard test that simulates\ntracks with previous mistakes. Moreover, the tracking efficiency of the model\nin a Surveillance application has been demonstrated by integrating that into\nthe frame-by-frame association of a Tracking-by-Detection algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 16:43:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["G\u00f3mez-Silva", "Mar\u00eda J.", ""]]}, {"id": "2004.03545", "submitter": "Mingkui Tan", "authors": "Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan,\n  Chuang Gan", "title": "Dense Regression Network for Video Grounding", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of video grounding from natural language queries. The\nkey challenge in this task is that one training video might only contain a few\nannotated starting/ending frames that can be used as positive examples for\nmodel training. Most conventional approaches directly train a binary classifier\nusing such imbalance data, thus achieving inferior results. The key idea of\nthis paper is to use the distances between the frame within the ground truth\nand the starting (ending) frame as dense supervisions to improve the video\ngrounding accuracy. Specifically, we design a novel dense regression network\n(DRN) to regress the distances from each frame to the starting (ending) frame\nof the video segment described by the query. We also propose a simple but\neffective IoU regression head module to explicitly consider the localization\nquality of the grounding results (i.e., the IoU between the predicted location\nand the ground truth). Experimental results show that our approach\nsignificantly outperforms state-of-the-arts on three datasets (i.e.,\nCharades-STA, ActivityNet-Captions, and TACoS).\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:15:37 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zeng", "Runhao", ""], ["Xu", "Haoming", ""], ["Huang", "Wenbing", ""], ["Chen", "Peihao", ""], ["Tan", "Mingkui", ""], ["Gan", "Chuang", ""]]}, {"id": "2004.03547", "submitter": "Yutian Lin", "authors": "Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, Qi Tian", "title": "Unsupervised Person Re-identification via Softened Similarity Learning", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is an important topic in computer vision.\nThis paper studies the unsupervised setting of re-ID, which does not require\nany labeled information and thus is freely deployed to new scenarios. There are\nvery few studies under this setting, and one of the best approach till now used\niterative clustering and classification, so that unlabeled images are clustered\ninto pseudo classes for a classifier to get trained, and the updated features\nare used for clustering and so on. This approach suffers two problems, namely,\nthe difficulty of determining the number of clusters, and the hard quantization\nloss in clustering. In this paper, we follow the iterative training mechanism\nbut discard clustering, since it incurs loss from hard quantization, yet its\nonly product, image-level similarity, can be easily replaced by pairwise\ncomputation and a softened classification task. With these improvements, our\napproach becomes more elegant and is more robust to hyper-parameter changes.\nExperiments on two image-based and video-based datasets demonstrate\nstate-of-the-art performance under the unsupervised re-ID setting.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:16:41 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Lin", "Yutian", ""], ["Xie", "Lingxi", ""], ["Wu", "Yu", ""], ["Yan", "Chenggang", ""], ["Tian", "Qi", ""]]}, {"id": "2004.03548", "submitter": "Ceyuan Yang", "authors": "Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, Bolei Zhou", "title": "Temporal Pyramid Network for Action Recognition", "comments": "To appear in CVPR 2020. Code is available at\n  https://github.com/decisionforce/TPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tempo characterizes the dynamics and the temporal scale of an action.\nModeling such visual tempos of different actions facilitates their recognition.\nPrevious works often capture the visual tempo through sampling raw videos at\nmultiple rates and constructing an input-level frame pyramid, which usually\nrequires a costly multi-branch network to handle. In this work we propose a\ngeneric Temporal Pyramid Network (TPN) at the feature-level, which can be\nflexibly integrated into 2D or 3D backbone networks in a plug-and-play manner.\nTwo essential components of TPN, the source of features and the fusion of\nfeatures, form a feature hierarchy for the backbone so that it can capture\naction instances at various tempos. TPN also shows consistent improvements over\nother challenging baselines on several action recognition datasets.\nSpecifically, when equipped with TPN, the 3D ResNet-50 with dense sampling\nobtains a 2% gain on the validation set of Kinetics-400. A further analysis\nalso reveals that TPN gains most of its improvements on action classes that\nhave large variances in their visual tempos, validating the effectiveness of\nTPN.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:17:23 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 02:05:13 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yang", "Ceyuan", ""], ["Xu", "Yinghao", ""], ["Shi", "Jianping", ""], ["Dai", "Bo", ""], ["Zhou", "Bolei", ""]]}, {"id": "2004.03572", "submitter": "Jiaming Sun", "authors": "Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang,\n  Xiaowei Zhou, Hujun Bao", "title": "Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance\n  Disparity Estimation", "comments": "Accepted to CVPR 2020. Code is available at\n  https://github.com/zju3dv/disprcnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel system named Disp R-CNN for 3D object\ndetection from stereo images. Many recent works solve this problem by first\nrecovering a point cloud with disparity estimation and then apply a 3D\ndetector. The disparity map is computed for the entire image, which is costly\nand fails to leverage category-specific prior. In contrast, we design an\ninstance disparity estimation network (iDispNet) that predicts disparity only\nfor pixels on objects of interest and learns a category-specific shape prior\nfor more accurate disparity estimation. To address the challenge from scarcity\nof disparity annotation in training, we propose to use a statistical shape\nmodel to generate dense disparity pseudo-ground-truth without the need of LiDAR\npoint clouds, which makes our system more widely applicable. Experiments on the\nKITTI dataset show that, even when LiDAR ground-truth is not available at\ntraining time, Disp R-CNN achieves competitive performance and outperforms\nprevious state-of-the-art methods by 20% in terms of average precision.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:48:45 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sun", "Jiaming", ""], ["Chen", "Linghao", ""], ["Xie", "Yiming", ""], ["Zhang", "Siyu", ""], ["Jiang", "Qinhong", ""], ["Zhou", "Xiaowei", ""], ["Bao", "Hujun", ""]]}, {"id": "2004.03577", "submitter": "Anastasios Angelopoulos", "authors": "Anastasios N. Angelopoulos, Julien N.P. Martel, Amit P.S. Kohli, Jorg\n  Conradt, Gordon Wetzstein", "title": "Event Based, Near Eye Gaze Tracking Beyond 10,000Hz", "comments": "IEEEVR oral/TVCG paper Dataset at\n  https://github.com/aangelopoulos/event_based_gaze_tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cameras in modern gaze-tracking systems suffer from fundamental bandwidth\nand power limitations, constraining data acquisition speed to 300 Hz\nrealistically. This obstructs the use of mobile eye trackers to perform, e.g.,\nlow latency predictive rendering, or to study quick and subtle eye motions like\nmicrosaccades using head-mounted devices in the wild. Here, we propose a hybrid\nframe-event-based near-eye gaze tracking system offering update rates beyond\n10,000 Hz with an accuracy that matches that of high-end desktop-mounted\ncommercial trackers when evaluated in the same conditions. Our system builds on\nemerging event cameras that simultaneously acquire regularly sampled frames and\nadaptively sampled events. We develop an online 2D pupil fitting method that\nupdates a parametric model every one or few events. Moreover, we propose a\npolynomial regressor for estimating the point of gaze from the parametric pupil\nmodel in real time. Using the first event-based gaze dataset, available at\nhttps://github.com/aangelopoulos/event_based_gaze_tracking , we demonstrate\nthat our system achieves accuracies of 0.45 degrees--1.75 degrees for fields of\nview from 45 degrees to 98 degrees. With this technology, we hope to enable a\nnew generation of ultra-low-latency gaze-contingent rendering and display\ntechniques for virtual and augmented reality.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:57:18 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 00:39:24 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Angelopoulos", "Anastasios N.", ""], ["Martel", "Julien N. P.", ""], ["Kohli", "Amit P. S.", ""], ["Conradt", "Jorg", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2004.03580", "submitter": "Christoph Feichtenhofer", "authors": "Kai Chen, Yuhang Cao, Chen Change Loy, Dahua Lin, Christoph\n  Feichtenhofer", "title": "Feature Pyramid Grids", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramid networks have been widely adopted in the object detection\nliterature to improve feature representations for better handling of variations\nin scale. In this paper, we present Feature Pyramid Grids (FPG), a deep\nmulti-pathway feature pyramid, that represents the feature scale-space as a\nregular grid of parallel bottom-up pathways which are fused by\nmulti-directional lateral connections. FPG can improve single-pathway feature\npyramid networks by significantly increasing its performance at similar\ncomputation cost, highlighting importance of deep pyramid representations. In\naddition to its general and uniform structure, over complicated structures that\nhave been found with neural architecture search, it also compares favorably\nagainst such approaches without relying on search. We hope that FPG with its\nuniform and effective nature can serve as a strong component for future work in\nobject recognition.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:59:52 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Chen", "Kai", ""], ["Cao", "Yuhang", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2004.03590", "submitter": "Ke Li", "authors": "Ke Li, Shichong Peng, Tianhao Zhang, Jitendra Malik", "title": "Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood\n  Estimation", "comments": "To appear in International Journal of Computer Vision (IJCV). arXiv\n  admin note: text overlap with arXiv:1811.12373", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in computer vision and graphics fall within the framework of\nconditional image synthesis. In recent years, generative adversarial nets\n(GANs) have delivered impressive advances in quality of synthesized images.\nHowever, it remains a challenge to generate both diverse and plausible images\nfor the same input, due to the problem of mode collapse. In this paper, we\ndevelop a new generic multimodal conditional image synthesis method based on\nImplicit Maximum Likelihood Estimation (IMLE) and demonstrate improved\nmultimodal image synthesis performance on two tasks, single image\nsuper-resolution and image synthesis from scene layouts. We make our\nimplementation publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:06:55 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Zhang", "Tianhao", ""], ["Malik", "Jitendra", ""]]}, {"id": "2004.03597", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi, Rajeev Yasarla, Vishal M. Patel", "title": "JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method", "comments": "Accepted at T-PAMI 2020. The dataset can be downloaded from\n  http://www.crowd-counting.com. arXiv admin note: substantial text overlap\n  with arXiv:1910.12384", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to its variety of applications in the real-world, the task of single\nimage-based crowd counting has received a lot of interest in the recent years.\nRecently, several approaches have been proposed to address various problems\nencountered in crowd counting. These approaches are essentially based on\nconvolutional neural networks that require large amounts of data to train the\nnetwork parameters. Considering this, we introduce a new large scale\nunconstrained crowd counting dataset (JHU-CROWD++) that contains \"4,372\" images\nwith \"1.51 million\" annotations. In comparison to existing datasets, the\nproposed dataset is collected under a variety of diverse scenarios and\nenvironmental conditions. Specifically, the dataset includes several images\nwith weather-based degradations and illumination variations, making it a very\nchallenging dataset. Additionally, the dataset consists of a rich set of\nannotations at both image-level and head-level. Several recent methods are\nevaluated and compared on this dataset. The dataset can be downloaded from\nhttp://www.crowd-counting.com .\n  Furthermore, we propose a novel crowd counting network that progressively\ngenerates crowd density maps via residual error estimation. The proposed method\nuses VGG16 as the backbone network and employs density map generated by the\nfinal layer as a coarse prediction to refine and generate finer density maps in\na progressive fashion using residual learning. Additionally, the residual\nlearning is guided by an uncertainty-based confidence weighting mechanism that\npermits the flow of only high-confidence residuals in the refinement path. The\nproposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is\nevaluated on recent complex datasets, and it achieves significant improvements\nin errors.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:59:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 17:52:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Yasarla", "Rajeev", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2004.03623", "submitter": "Kamal Gupta", "authors": "Kamal Gupta, Saurabh Singh, Abhinav Shrivastava", "title": "PatchVAE: Learning Local Latent Codes for Recognition", "comments": "To appear at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning holds the promise of exploiting large\namounts of unlabeled data to learn general representations. A promising\ntechnique for unsupervised learning is the framework of Variational\nAuto-encoders (VAEs). However, unsupervised representations learned by VAEs are\nsignificantly outperformed by those learned by supervised learning for\nrecognition. Our hypothesis is that to learn useful representations for\nrecognition the model needs to be encouraged to learn about repeating and\nconsistent patterns in data. Drawing inspiration from the mid-level\nrepresentation discovery work, we propose PatchVAE, that reasons about images\nat patch level. Our key contribution is a bottleneck formulation that\nencourages mid-level style representations in the VAE framework. Our\nexperiments demonstrate that representations learned by our method perform much\nbetter on the recognition tasks compared to those learned by vanilla VAEs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:01:26 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Gupta", "Kamal", ""], ["Singh", "Saurabh", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2004.03624", "submitter": "Ajna Ram", "authors": "Ajna Ram, Constantino Carlos Reyes-Aldasoro", "title": "The relationship between Fully Connected Layers and number of classes\n  for the analysis of retinal images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper experiments with the number of fully-connected layers in a deep\nconvolutional neural network as applied to the classification of fundus retinal\nimages. The images analysed corresponded to the ODIR 2019 (Peking University\nInternational Competition on Ocular Disease Intelligent Recognition) [9], which\nincluded images of various eye diseases (cataract, glaucoma, myopia, diabetic\nretinopathy, age-related macular degeneration (AMD), hypertension) as well as\nnormal cases. This work focused on the classification of Normal, Cataract, AMD\nand Myopia. The feature extraction (convolutional) part of the neural network\nis kept the same while the feature mapping (linear) part of the network is\nchanged. Different data sets are also explored on these neural nets. Each data\nset differs from another by the number of classes it has. This paper hence aims\nto find the relationship between number of classes and number of\nfully-connected layers. It was found out that the effect of increasing the\nnumber of fully-connected layers of a neural networks depends on the type of\ndata set being used. For simple, linearly separable data sets, addition of\nfully-connected layer is something that should be explored and that could\nresult in better training accuracy, but a direct correlation was not found.\nHowever as complexity of the data set goes up(more overlapping classes),\nincreasing the number of fully-connected layers causes the neural network to\nstop learning. This phenomenon happens quicker the more complex the data set\nis.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:03:02 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 15:50:05 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ram", "Ajna", ""], ["Reyes-Aldasoro", "Constantino Carlos", ""]]}, {"id": "2004.03627", "submitter": "Aythami Morales", "authors": "Alejandro Acien, John V. Monaco, Aythami Morales, Ruben\n  Vera-Rodriguez, and Julian Fierrez", "title": "TypeNet: Scaling up Keystroke Biometrics", "comments": null, "journal-ref": "IAPR/IEEE International Joint Conference on Biometrics (IJCB),\n  Houston, USA, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the suitability of keystroke dynamics to authenticate 100K users\ntyping free-text. For this, we first analyze to what extent our method based on\na Siamese Recurrent Neural Network (RNN) is able to authenticate users when the\namount of data per user is scarce, a common scenario in free-text keystroke\nauthentication. With 1K users for testing the network, a population size\ncomparable to previous works, TypeNet obtains an equal error rate of 4.8% using\nonly 5 enrollment sequences and 1 test sequence per user with 50 keystrokes per\nsequence. Using the same amount of data per user, as the number of test users\nis scaled up to 100K, the performance in comparison to 1K decays relatively by\nless than 5%, demonstrating the potential of TypeNet to scale well at large\nscale number of users. Our experiments are conducted with the Aalto University\nkeystroke database. To the best of our knowledge, this is the largest free-text\nkeystroke database captured with more than 136M keystrokes from 168K users.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:05:33 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 09:01:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Acien", "Alejandro", ""], ["Monaco", "John V.", ""], ["Morales", "Aythami", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""]]}, {"id": "2004.03661", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang and Marcel Worring", "title": "Query-controllable Video Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When video collections become huge, how to explore both within and across\nvideos efficiently is challenging. Video summarization is one of the ways to\ntackle this issue. Traditional summarization approaches limit the effectiveness\nof video exploration because they only generate one fixed video summary for a\ngiven input video independent of the information need of the user. In this\nwork, we introduce a method which takes a text-based query as input and\ngenerates a video summary corresponding to it. We do so by modeling video\nsummarization as a supervised learning problem and propose an end-to-end deep\nlearning based method for query-controllable video summarization to generate a\nquery-dependent video summary. Our proposed method consists of a video summary\ncontroller, video summary generator, and video summary output module. To foster\nthe research of query-controllable video summarization and conduct our\nexperiments, we introduce a dataset that contains frame-based relevance score\nlabels. Based on our experimental result, it shows that the text-based query\nhelps control the video summary. It also shows the text-based query improves\nour model performance. Our code and dataset:\nhttps://github.com/Jhhuangkay/Query-controllable-Video-Summarization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:35:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Worring", "Marcel", ""]]}, {"id": "2004.03669", "submitter": "Mohammad Shifat-E-Rabbi", "authors": "Mohammad Shifat-E-Rabbi, Xuwang Yin, Abu Hasnat Mohammad Rubaiyat,\n  Shiying Li, Soheil Kolouri, Akram Aldroubi, Jonathan M. Nichols, and Gustavo\n  K. Rohde", "title": "Radon cumulative distribution transform subspace modeling for image\n  classification", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new supervised image classification method applicable to a broad\nclass of image deformation models. The method makes use of the previously\ndescribed Radon Cumulative Distribution Transform (R-CDT) for image data, whose\nmathematical properties are exploited to express the image data in a form that\nis more suitable for machine learning. While certain operations such as\ntranslation, scaling, and higher-order transformations are challenging to model\nin native image space, we show the R-CDT can capture some of these variations\nand thus render the associated image classification problems easier to solve.\nThe method -- utilizing a nearest-subspace algorithm in R-CDT space -- is\nsimple to implement, non-iterative, has no hyper-parameters to tune, is\ncomputationally efficient, label efficient, and provides competitive accuracies\nto state-of-the-art neural networks for many types of classification problems.\nIn addition to the test accuracy performances, we show improvements (with\nrespect to neural network-based methods) in terms of computational efficiency\n(it can be implemented without the use of GPUs), number of training samples\nneeded for training, as well as out-of-distribution generalization. The Python\ncode for reproducing our results is available at\nhttps://github.com/rohdelab/rcdt_ns_classifier.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:47:26 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 18:35:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shifat-E-Rabbi", "Mohammad", ""], ["Yin", "Xuwang", ""], ["Rubaiyat", "Abu Hasnat Mohammad", ""], ["Li", "Shiying", ""], ["Kolouri", "Soheil", ""], ["Aldroubi", "Akram", ""], ["Nichols", "Jonathan M.", ""], ["Rohde", "Gustavo K.", ""]]}, {"id": "2004.03675", "submitter": "Seong Tae Kim", "authors": "Stefan Denner, Ashkan Khakzar, Moiz Sajid, Mahdi Saleh, Ziga Spiclin,\n  Seong Tae Kim, Nassir Navab", "title": "Spatio-temporal Learning from Longitudinal Data for Multiple Sclerosis\n  Lesion Segmentation", "comments": "Accepted at BrainLes Workshop in MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of Multiple Sclerosis (MS) lesions in longitudinal brain MR\nscans is performed for monitoring the progression of MS lesions. We hypothesize\nthat the spatio-temporal cues in longitudinal data can aid the segmentation\nalgorithm. Therefore, we propose a multi-task learning approach by defining an\nauxiliary self-supervised task of deformable registration between two\ntime-points to guide the neural network toward learning from spatio-temporal\nchanges. We show the efficacy of our method on a clinical dataset comprised of\n70 patients with one follow-up study for each patient. Our results show that\nspatio-temporal information in longitudinal data is a beneficial cue for\nimproving segmentation. We improve the result of current state-of-the-art by\n2.6% in terms of overall score (p<0.05). Code is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:57:24 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 19:42:07 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Denner", "Stefan", ""], ["Khakzar", "Ashkan", ""], ["Sajid", "Moiz", ""], ["Saleh", "Mahdi", ""], ["Spiclin", "Ziga", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "2004.03677", "submitter": "Helisa Dhamo", "authors": "Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D.\n  Hager, Federico Tombari, Christian Rupprecht", "title": "Semantic Image Manipulation Using Scene Graphs", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:02:49 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Dhamo", "Helisa", ""], ["Farshad", "Azade", ""], ["Laina", "Iro", ""], ["Navab", "Nassir", ""], ["Hager", "Gregory D.", ""], ["Tombari", "Federico", ""], ["Rupprecht", "Christian", ""]]}, {"id": "2004.03686", "submitter": "Hanbyul Joo", "authors": "Hanbyul Joo, Natalia Neverova, Andrea Vedaldi", "title": "Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D\n  Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Exemplar Fine-Tuning (EFT), a new method to fit a 3D parametric\nhuman model to a single RGB input image cropped around a person with 2D\nkeypoint annotations. While existing parametric human model fitting approaches,\nsuch as SMPLify, rely on the \"view-agnostic\" human pose priors to enforce the\noutput in a plausible 3D pose space, EFT exploits the pose prior that comes\nfrom the specific 2D input observations by leveraging a fully-trained 3D pose\nregressor. We thoroughly compare our EFT with SMPLify, and demonstrate that EFT\nproduces more reliable and accurate 3D human fitting outputs on the same\ninputs. Especially, we use our EFT to augment a large scale in-the-wild 2D\nkeypoint datasets, such as COCO and MPII, with plausible and convincing 3D pose\nfitting outputs. We demonstrate that the pseudo ground-truth 3D pose data by\nEFT can supervise a strong 3D pose estimator that outperforms the previous\nstate-of-the-art in the standard outdoor benchmark (3DPW), even without using\nany ground-truth 3D human pose datasets such as Human3.6M. Our code and data\nare available at https://github.com/facebookresearch/eft.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:21:18 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 23:05:36 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Joo", "Hanbyul", ""], ["Neverova", "Natalia", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2004.03696", "submitter": "Changlu Guo", "authors": "Changlu Guo, M\\'arton Szemenyei, Yugen Yi, Wenle Wang, Buer Chen,\n  Changqi Fan", "title": "SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise segmentation of retinal blood vessels is of great significance\nfor early diagnosis of eye-related diseases such as diabetes and hypertension.\nIn this work, we propose a lightweight network named Spatial Attention U-Net\n(SA-UNet) that does not require thousands of annotated training samples and can\nbe utilized in a data augmentation manner to use the available annotated\nsamples more efficiently. SA-UNet introduces a spatial attention module which\ninfers the attention map along the spatial dimension, and multiplies the\nattention map by the input feature map for adaptive feature refinement. In\naddition, the proposed network employs structured dropout convolutional blocks\ninstead of the original convolutional blocks of U-Net to prevent the network\nfrom overfitting. We evaluate SA-UNet based on two benchmark retinal datasets:\nthe Vascular Extraction (DRIVE) dataset and the Child Heart and Health Study\n(CHASE_DB1) dataset. The results show that the proposed SA-UNet achieves\nstate-of-the-art performance on both datasets.The implementation and the\ntrained networks are available on Github1.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:41:12 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 20:55:22 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 19:46:11 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Guo", "Changlu", ""], ["Szemenyei", "M\u00e1rton", ""], ["Yi", "Yugen", ""], ["Wang", "Wenle", ""], ["Chen", "Buer", ""], ["Fan", "Changqi", ""]]}, {"id": "2004.03697", "submitter": "Changlu Guo", "authors": "Changlu Guo, M\\'arton Szemenyei, Yugen Yi, Ying Xue, Wei Zhou,\n  Yangyuan Li", "title": "Dense Residual Network for Retinal Vessel Segmentation", "comments": "Accepted by IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation plays an imaportant role in the field of retinal\nimage analysis because changes in retinal vascular structure can aid in the\ndiagnosis of diseases such as hypertension and diabetes. In recent research,\nnumerous successful segmentation methods for fundus images have been proposed.\nBut for other retinal imaging modalities, more research is needed to explore\nvascular extraction. In this work, we propose an efficient method to segment\nblood vessels in Scanning Laser Ophthalmoscopy (SLO) retinal images. Inspired\nby U-Net, \"feature map reuse\" and residual learning, we propose a deep dense\nresidual network structure called DRNet. In DRNet, feature maps of previous\nblocks are adaptively aggregated into subsequent layers as input, which not\nonly facilitates spatial reconstruction, but also learns more efficiently due\nto more stable gradients. Furthermore, we introduce DropBlock to alleviate the\noverfitting problem of the network. We train and test this model on the recent\nSLO public dataset. The results show that our method achieves the\nstate-of-the-art performance even without data augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:42:13 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Guo", "Changlu", ""], ["Szemenyei", "M\u00e1rton", ""], ["Yi", "Yugen", ""], ["Xue", "Ying", ""], ["Zhou", "Wei", ""], ["Li", "Yangyuan", ""]]}, {"id": "2004.03698", "submitter": "Umut \\\"Ozkaya", "authors": "Umut Ozkaya, Saban Ozturk, Mucahid Barstugan", "title": "Coronavirus (COVID-19) Classification using Deep Features Fusion and\n  Ranking Technique", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus (COVID-19) emerged towards the end of 2019. World Health\nOrganization (WHO) was identified it as a global epidemic. Consensus occurred\nin the opinion that using Computerized Tomography (CT) techniques for early\ndiagnosis of pandemic disease gives both fast and accurate results. It was\nstated by expert radiologists that COVID-19 displays different behaviours in CT\nimages. In this study, a novel method was proposed as fusing and ranking deep\nfeatures to detect COVID-19 in early phase. 16x16 (Subset-1) and 32x32\n(Subset-2) patches were obtained from 150 CT images to generate sub-datasets.\nWithin the scope of the proposed method, 3000 patch images have been labelled\nas CoVID-19 and No finding for using in training and testing phase. Feature\nfusion and ranking method have been applied in order to increase the\nperformance of the proposed method. Then, the processed data was classified\nwith a Support Vector Machine (SVM). According to other pre-trained\nConvolutional Neural Network (CNN) models used in transfer learning, the\nproposed method shows high performance on Subset-2 with 98.27% accuracy, 98.93%\nsensitivity, 97.60% specificity, 97.63% precision, 98.28% F1-score and 96.54%\nMatthews Correlation Coefficient (MCC) metrics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:43:44 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Ozkaya", "Umut", ""], ["Ozturk", "Saban", ""], ["Barstugan", "Mucahid", ""]]}, {"id": "2004.03702", "submitter": "Changlu Guo", "authors": "Changlu Guo, M\\'arton Szemenyei, Yangtao Hu, Wenle Wang, Wei Zhou,\n  Yugen Yi", "title": "Channel Attention Residual U-Net for Retinal Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is a vital step for the diagnosis of many early\neye-related diseases. In this work, we propose a new deep learning model,\nnamely Channel Attention Residual U-Net (CAR-UNet), to accurately segment\nretinal vascular and non-vascular pixels. In this model, we introduced a novel\nModified Efficient Channel Attention (MECA) to enhance the discriminative\nability of the network by considering the interdependence between feature maps.\nOn the one hand, we apply MECA to the \"skip connections\" in the traditional\nU-shaped networks, instead of simply copying the feature maps of the\ncontracting path to the corresponding expansive path. On the other hand, we\npropose a Channel Attention Double Residual Block (CADRB), which integrates\nMECA into a residual structure as a core structure to construct the proposed\nCAR-UNet. The results show that our proposed CAR-UNet has reached the\nstate-of-the-art performance on three publicly available retinal vessel\ndatasets: DRIVE, CHASE DB1 and STARE.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:47:40 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 20:50:45 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 18:26:12 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 21:22:24 GMT"}, {"version": "v5", "created": "Tue, 20 Oct 2020 19:48:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Guo", "Changlu", ""], ["Szemenyei", "M\u00e1rton", ""], ["Hu", "Yangtao", ""], ["Wang", "Wenle", ""], ["Zhou", "Wei", ""], ["Yi", "Yugen", ""]]}, {"id": "2004.03706", "submitter": "Saurabh Sharma", "authors": "Saurabh Sharma, Ning Yu, Mario Fritz, Bernt Schiele", "title": "Long-Tailed Recognition Using Class-Balanced Experts", "comments": "Accepted and presented at 42nd German Conference on Pattern\n  Recognition (DAGM-GCPR 2020), T\\\"ubingen, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning enables impressive performance in image recognition using\nlarge-scale artificially-balanced datasets. However, real-world datasets\nexhibit highly class-imbalanced distributions, yielding two main challenges:\nrelative imbalance amongst the classes and data scarcity for mediumshot or\nfewshot classes. In this work, we address the problem of long-tailed\nrecognition wherein the training set is highly imbalanced and the test set is\nkept balanced. Differently from existing paradigms relying on data-resampling,\ncost-sensitive learning, online hard example mining, loss objective reshaping,\nand/or memory-based modeling, we propose an ensemble of class-balanced experts\nthat combines the strength of diverse classifiers. Our ensemble of\nclass-balanced experts reaches results close to state-of-the-art and an\nextended ensemble establishes a new state-of-the-art on two benchmarks for\nlong-tailed recognition. We conduct extensive experiments to analyse the\nperformance of the ensembles, and discover that in modern large-scale datasets,\nrelative imbalance is a harder problem than data scarcity. The training and\nevaluation code is available at\nhttps://github.com/ssfootball04/class-balanced-experts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:57:44 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 11:22:02 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sharma", "Saurabh", ""], ["Yu", "Ning", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "2004.03708", "submitter": "Zhuowan Li", "authors": "Zhuowan Li, Quan Tran, Long Mai, Zhe Lin, Alan Yuille", "title": "Context-Aware Group Captioning via Self-Attention and Contrastive\n  Features", "comments": "To appear in CVPR 2020; Project page:\n  https://lizw14.github.io/project/groupcap", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While image captioning has progressed rapidly, existing works focus mainly on\ndescribing single images. In this paper, we introduce a new task, context-aware\ngroup captioning, which aims to describe a group of target images in the\ncontext of another group of related reference images. Context-aware group\ncaptioning requires not only summarizing information from both the target and\nreference image group but also contrasting between them. To solve this problem,\nwe propose a framework combining self-attention mechanism with contrastive\nfeature construction to effectively summarize common information from each\nimage group while capturing discriminative information between them. To build\nthe dataset for this task, we propose to group the images and generate the\ngroup captions based on single image captions using scene graphs matching. Our\ndatasets are constructed on top of the public Conceptual Captions dataset and\nour new Stock Captions dataset. Experiments on the two datasets show the\neffectiveness of our method on this new task. Related Datasets and code are\nreleased at https://lizw14.github.io/project/groupcap .\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:59:53 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Zhuowan", ""], ["Tran", "Quan", ""], ["Mai", "Long", ""], ["Lin", "Zhe", ""], ["Yuille", "Alan", ""]]}, {"id": "2004.03718", "submitter": "Dina Machuve", "authors": "Sophia Sanga, Victor Mero, Dina Machuve and Davis Mwanganda", "title": "Mobile-Based Deep Learning Models for Banana Diseases Detection", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smallholder farmers in Tanzania are challenged on the lack of tools for early\ndetection of banana diseases. This study aimed at developing a mobile\napplication for early detection of Fusarium wilt race 1 and black Sigatoka\nbanana diseases using deep learning. We used a dataset of 3000 banana leaves\nimages. We pre-trained our model on Resnet152 and Inceptionv3 Convolution\nNeural Network architectures. The Resnet152 achieved an accuracy of 99.2% and\nInceptionv3 an accuracy of 95.41%. On deployment using Android mobile phones,\nwe chose Inceptionv3 since it has lower memory requirements compared to\nResnet152. The mobile application on real environment detected the two diseases\nwith a confidence level of 99% of the captured leaf area. This result indicates\nthe potential in improving the yield of bananas by smallholder farmers using a\ntool for early detection of diseases.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 21:17:45 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Sanga", "Sophia", ""], ["Mero", "Victor", ""], ["Machuve", "Dina", ""], ["Mwanganda", "Davis", ""]]}, {"id": "2004.03737", "submitter": "Zhecan Wang", "authors": "Zhecan Wang, Jian Zhao, Cheng Lu, Han Huang, Fan Yang, Lianji Li,\n  Yandong Guo", "title": "Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation\n  in the Wild", "comments": "2020 Winter Conference on Applications of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained remote gaze estimation remains challenging mostly due to its\nvulnerability to the large variability in head-pose. Prior solutions struggle\nto maintain reliable accuracy in unconstrained remote gaze tracking. Among\nthem, appearance-based solutions demonstrate tremendous potential in improving\ngaze accuracy. However, existing works still suffer from head movement and are\nnot robust enough to handle real-world scenarios. Especially most of them study\ngaze estimation under controlled scenarios where the collected datasets often\ncover limited ranges of both head-pose and gaze which introduces further bias.\nIn this paper, we propose novel end-to-end appearance-based gaze estimation\nmethods that could more robustly incorporate different levels of head-pose\nrepresentations into gaze estimation. Our method could generalize to real-world\nscenarios with low image quality, different lightings and scenarios where\ndirect head-pose information is not available. To better demonstrate the\nadvantage of our methods, we further propose a new benchmark dataset with the\nmost rich distribution of head-gaze combination reflecting real-world\nscenarios. Extensive evaluations on several public datasets and our own dataset\ndemonstrate that our method consistently outperforms the state-of-the-art by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 22:38:49 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wang", "Zhecan", ""], ["Zhao", "Jian", ""], ["Lu", "Cheng", ""], ["Huang", "Han", ""], ["Yang", "Fan", ""], ["Li", "Lianji", ""], ["Guo", "Yandong", ""]]}, {"id": "2004.03744", "submitter": "Virginie Do", "authors": "Virginie Do, Oana-Maria Camburu, Zeynep Akata and Thomas Lukasiewicz", "title": "e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language\n  Explanations", "comments": null, "journal-ref": "IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer\n  Vision, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed SNLI-VE corpus for recognising visual-textual\nentailment is a large, real-world dataset for fine-grained multimodal\nreasoning. However, the automatic way in which SNLI-VE has been assembled (via\ncombining parts of two related datasets) gives rise to a large number of errors\nin the labels of this corpus. In this paper, we first present a data collection\neffort to correct the class with the highest error rate in SNLI-VE. Secondly,\nwe re-evaluate an existing model on the corrected corpus, which we call\nSNLI-VE-2.0, and provide a quantitative comparison with its performance on the\nnon-corrected corpus. Thirdly, we introduce e-SNLI-VE-2.0, which appends\nhuman-written natural language explanations to SNLI-VE-2.0. Finally, we train\nmodels that learn from these explanations at training time, and output such\nexplanations at testing time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 23:12:51 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:26:19 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Do", "Virginie", ""], ["Camburu", "Oana-Maria", ""], ["Akata", "Zeynep", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2004.03747", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, M M Shaifur Rahman, Mst Shamima Nasrin, Tarek M.\n  Taha, and Vijayan K. Asari", "title": "COVID_MTNet: COVID-19 Detection with Multi-Task Deep Learning Approaches", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 is currently one the most life-threatening problems around the\nworld. The fast and accurate detection of the COVID-19 infection is essential\nto identify, take better decisions and ensure treatment for the patients which\nwill help save their lives. In this paper, we propose a fast and efficient way\nto identify COVID-19 patients with multi-task deep learning (DL) methods. Both\nX-ray and CT scan images are considered to evaluate the proposed technique. We\nemploy our Inception Residual Recurrent Convolutional Neural Network with\nTransfer Learning (TL) approach for COVID-19 detection and our NABLA-N network\nmodel for segmenting the regions infected by COVID-19. The detection model\nshows around 84.67% testing accuracy from X-ray images and 98.78% accuracy in\nCT-images. A novel quantitative analysis strategy is also proposed in this\npaper to determine the percentage of infected regions in X-ray and CT images.\nThe qualitative and quantitative results demonstrate promising results for\nCOVID-19 detection and infected region localization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 23:19:59 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 02:26:05 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 19:01:10 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Rahman", "M M Shaifur", ""], ["Nasrin", "Mst Shamima", ""], ["Taha", "Tarek M.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "2004.03755", "submitter": "Goonmeet Bajaj", "authors": "Goonmeet Bajaj, Bortik Bandyopadhyay, Daniel Schmidt, Pranav\n  Maneriker, Christopher Myers, Srinivasan Parthasarathy", "title": "Understanding Knowledge Gaps in Visual Question Answering: Implications\n  for Gap Identification and Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) systems are tasked with answering natural\nlanguage questions corresponding to a presented image. Traditional VQA datasets\ntypically contain questions related to the spatial information of objects,\nobject attributes, or general scene questions. Recently, researchers have\nrecognized the need to improve the balance of such datasets to reduce the\nsystem's dependency on memorized linguistic features and statistical biases,\nwhile aiming for enhanced visual understanding. However, it is unclear whether\nany latent patterns exist to quantify and explain these failures. As an initial\nstep towards better quantifying our understanding of the performance of VQA\nmodels, we use a taxonomy of Knowledge Gaps (KGs) to tag questions with one or\nmore types of KGs. Each Knowledge Gap (KG) describes the reasoning abilities\nneeded to arrive at a resolution. After identifying KGs for each question, we\nexamine the skew in the distribution of questions for each KG. We then\nintroduce a targeted question generation model to reduce this skew, which\nallows us to generate new types of questions for an image. These new questions\ncan be added to existing VQA datasets to increase the diversity of questions\nand reduce the skew.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:27:43 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 21:53:59 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Bajaj", "Goonmeet", ""], ["Bandyopadhyay", "Bortik", ""], ["Schmidt", "Daniel", ""], ["Maneriker", "Pranav", ""], ["Myers", "Christopher", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "2004.03756", "submitter": "Sunpreet Singh Arora", "authors": "Cori Tymoszek, Sunpreet S. Arora, Kim Wagner, and Anil K. Jain", "title": "DashCam Pay: A System for In-vehicle Payments Using Face and Voice", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our ongoing work on developing a system, called DashCam Pay, that\nenables in-vehicle payments in a seamless and secure manner using face and\nvoice biometrics. A plug-and-play device (dashcam) mounted in the vehicle is\nused to capture face images and voice commands of passengers.\nPrivacy-preserving biometric comparison techniques are used to compare the\nbiometric data captured by the dashcam with the biometric data enrolled on the\nusers' mobile devices over a wireless interface (e.g., Bluetooth or Wi-Fi\nDirect) to determine the payer. Once the payer is identified, payment is\nconducted using the enrolled payment credential on the mobile device of the\npayer. We conduct preliminary analysis on data collected using a commercially\navailable dashcam to show the feasibility of building the proposed system. A\nprototype of the proposed system is also developed in Android. DashCam Pay can\nbe integrated as a software solution by dashcam or vehicle manufacturers to\nenable open loop in-vehicle payments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:28:33 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 22:28:41 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Tymoszek", "Cori", ""], ["Arora", "Sunpreet S.", ""], ["Wagner", "Kim", ""], ["Jain", "Anil K.", ""]]}, {"id": "2004.03791", "submitter": "Longguang Wang", "authors": "Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang Yang, Wei An, and\n  Yulan Guo", "title": "Learning A Single Network for Scale-Arbitrary Super-Resolution", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the performance of single image super-resolution (SR) has been\nsignificantly improved with powerful networks. However, these networks are\ndeveloped for image SR with a single specific integer scale (e.g., x2;x3,x4),\nand cannot be used for non-integer and asymmetric SR. In this paper, we propose\nto learn a scale-arbitrary image SR network from scale-specific networks.\nSpecifically, we propose a plug-in module for existing SR networks to perform\nscale-arbitrary SR, which consists of multiple scale-aware feature adaption\nblocks and a scale-aware upsampling layer. Moreover, we introduce a scale-aware\nknowledge transfer paradigm to transfer knowledge from scale-specific networks\nto the scale-arbitrary network. Our plug-in module can be easily adapted to\nexisting networks to achieve scale-arbitrary SR. These networks plugged with\nour module can achieve promising results for non-integer and asymmetric SR\nwhile maintaining state-of-the-art performance for SR with integer scale\nfactors. Besides, the additional computational and memory cost of our module is\nvery small.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:40:15 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 14:01:58 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wang", "Longguang", ""], ["Wang", "Yingqian", ""], ["Lin", "Zaiping", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2004.03796", "submitter": "Naiyan Wang", "authors": "Zhichao Li, Naiyan Wang", "title": "DMLO: Deep Matching LiDAR Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR odometry is a fundamental task for various areas such as robotics,\nautonomous driving. This problem is difficult since it requires the systems to\nbe highly robust running in noisy real-world data. Existing methods are mostly\nlocal iterative methods. Feature-based global registration methods are not\npreferred since extracting accurate matching pairs in the nonuniform and sparse\nLiDAR data remains challenging. In this paper, we present Deep Matching LiDAR\nOdometry (DMLO), a novel learning-based framework which makes the feature\nmatching method applicable to LiDAR odometry task. Unlike many recent\nlearning-based methods, DMLO explicitly enforces geometry constraints in the\nframework. Specifically, DMLO decomposes the 6-DoF pose estimation into two\nparts, a learning-based matching network which provides accurate\ncorrespondences between two scans and rigid transformation estimation with a\nclose-formed solution by Singular Value Decomposition (SVD). Comprehensive\nexperimental results on real-world datasets KITTI and Argoverse demonstrate\nthat our DMLO dramatically outperforms existing learning-based methods and\ncomparable with the state-of-the-art geometry based approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:52:49 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 02:25:38 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Li", "Zhichao", ""], ["Wang", "Naiyan", ""]]}, {"id": "2004.03804", "submitter": "Hanchen Ye", "authors": "Hanchen Ye, Xiaofan Zhang, Zhize Huang, Gengsheng Chen, Deming Chen", "title": "HybridDNN: A Framework for High-Performance Hybrid DNN Accelerator\n  Design and Implementation", "comments": "Published as a conference paper at Design Automation Conference 2020\n  (DAC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To speedup Deep Neural Networks (DNN) accelerator design and enable effective\nimplementation, we propose HybridDNN, a framework for building high-performance\nhybrid DNN accelerators and delivering FPGA-based hardware implementations.\nNovel techniques include a highly flexible and scalable architecture with a\nhybrid Spatial/Winograd convolution (CONV) Processing Engine (PE), a\ncomprehensive design space exploration tool, and a complete design flow to\nfully support accelerator design and implementation. Experimental results show\nthat the accelerators generated by HybridDNN can deliver 3375.7 and 83.3 GOPS\non a high-end FPGA (VU9P) and an embedded FPGA (PYNQ-Z1), respectively, which\nachieve a 1.8x higher performance improvement compared to the state-of-art\naccelerator designs. This demonstrates that HybridDNN is flexible and scalable\nand can target both cloud and embedded hardware platforms with vastly different\nresource constraints.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 04:28:38 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Ye", "Hanchen", ""], ["Zhang", "Xiaofan", ""], ["Huang", "Zhize", ""], ["Chen", "Gengsheng", ""], ["Chen", "Deming", ""]]}, {"id": "2004.03805", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen\n  Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason\n  Saragih, Matthias Nie{\\ss}ner, Rohit Pandey, Sean Fanello, Gordon Wetzstein,\n  Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B\n  Goldman, Michael Zollh\\\"ofer", "title": "State of the Art on Neural Rendering", "comments": "Eurographics 2020 survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient rendering of photo-realistic virtual worlds is a long standing\neffort of computer graphics. Modern graphics techniques have succeeded in\nsynthesizing photo-realistic images from hand-crafted scene representations.\nHowever, the automatic generation of shape, materials, lighting, and other\naspects of scenes remains a challenging problem that, if solved, would make\nphoto-realistic computer graphics more widely accessible. Concurrently,\nprogress in computer vision and machine learning have given rise to a new\napproach to image synthesis and editing, namely deep generative models. Neural\nrendering is a new and rapidly emerging field that combines generative machine\nlearning techniques with physical knowledge from computer graphics, e.g., by\nthe integration of differentiable rendering into network training. With a\nplethora of applications in computer graphics and vision, neural rendering is\npoised to become a new area in the graphics community, yet no survey of this\nemerging field exists. This state-of-the-art report summarizes the recent\ntrends and applications of neural rendering. We focus on approaches that\ncombine classic computer graphics techniques with deep generative models to\nobtain controllable and photo-realistic outputs. Starting with an overview of\nthe underlying computer graphics and machine learning concepts, we discuss\ncritical aspects of neural rendering approaches. This state-of-the-art report\nis focused on the many important use cases for the described algorithms such as\nnovel view synthesis, semantic photo manipulation, facial and body reenactment,\nrelighting, free-viewpoint video, and the creation of photo-realistic avatars\nfor virtual and augmented reality telepresence. Finally, we conclude with a\ndiscussion of the social implications of such technology and investigate open\nresearch problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 04:36:31 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Tewari", "Ayush", ""], ["Fried", "Ohad", ""], ["Thies", "Justus", ""], ["Sitzmann", "Vincent", ""], ["Lombardi", "Stephen", ""], ["Sunkavalli", "Kalyan", ""], ["Martin-Brualla", "Ricardo", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Nie\u00dfner", "Matthias", ""], ["Pandey", "Rohit", ""], ["Fanello", "Sean", ""], ["Wetzstein", "Gordon", ""], ["Zhu", "Jun-Yan", ""], ["Theobalt", "Christian", ""], ["Agrawala", "Maneesh", ""], ["Shechtman", "Eli", ""], ["Goldman", "Dan B", ""], ["Zollh\u00f6fer", "Michael", ""]]}, {"id": "2004.03811", "submitter": "Takayuki Nakatsuka Mr", "authors": "Takayuki Nakatsuka, Kazuyoshi Yoshii, Yuki Koyama, Satoru Fukayama,\n  Masataka Goto, and Shigeo Morishima", "title": "MirrorNet: A Deep Bayesian Approach to Reflective 2D Pose Estimation\n  from Human Images", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a statistical approach to 2D pose estimation from human\nimages. The main problems with the standard supervised approach, which is based\non a deep recognition (image-to-pose) model, are that it often yields\nanatomically implausible poses, and its performance is limited by the amount of\npaired data. To solve these problems, we propose a semi-supervised method that\ncan make effective use of images with and without pose annotations.\nSpecifically, we formulate a hierarchical generative model of poses and images\nby integrating a deep generative model of poses from pose features with that of\nimages from poses and image features. We then introduce a deep recognition\nmodel that infers poses from images. Given images as observed data, these\nmodels can be trained jointly in a hierarchical variational autoencoding\n(image-to-pose-to-feature-to-pose-to-image) manner. The results of experiments\nshow that the proposed reflective architecture makes estimated poses\nanatomically plausible, and the performance of pose estimation improved by\nintegrating the recognition and generative models and also by feeding\nnon-annotated images.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:02:48 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Nakatsuka", "Takayuki", ""], ["Yoshii", "Kazuyoshi", ""], ["Koyama", "Yuki", ""], ["Fukayama", "Satoru", ""], ["Goto", "Masataka", ""], ["Morishima", "Shigeo", ""]]}, {"id": "2004.03815", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xun Wang, Leimin Zhang, Chaoxi Xu, Gang Yang, Xirong Li", "title": "Feature Re-Learning with Data Augmentation for Video Relevance\n  Prediction", "comments": "accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": "10.1109/TKDE.2019.2947442", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the relevance between two given videos with respect to their\nvisual content is a key component for content-based video recommendation and\nretrieval. Thanks to the increasing availability of pre-trained image and video\nconvolutional neural network models, deep visual features are widely used for\nvideo content representation. However, as how two videos are relevant is\ntask-dependent, such off-the-shelf features are not always optimal for all\ntasks. Moreover, due to varied concerns including copyright, privacy and\nsecurity, one might have access to only pre-computed video features rather than\noriginal videos. We propose in this paper feature re-learning for improving\nvideo relevance prediction, with no need of revisiting the original video\ncontent. In particular, re-learning is realized by projecting a given deep\nfeature into a new space by an affine transformation. We optimize the\nre-learning process by a novel negative-enhanced triplet ranking loss. In order\nto generate more training data, we propose a new data augmentation strategy\nwhich works directly on frame-level and video-level features. Extensive\nexperiments in the context of the Hulu Content-based Video Relevance Prediction\nChallenge 2018 justify the effectiveness of the proposed method and its\nstate-of-the-art performance for content-based video relevance prediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:22:41 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Dong", "Jianfeng", ""], ["Wang", "Xun", ""], ["Zhang", "Leimin", ""], ["Xu", "Chaoxi", ""], ["Yang", "Gang", ""], ["Li", "Xirong", ""]]}, {"id": "2004.03828", "submitter": "Yi Wang", "authors": "Yi Wang, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, Jiaya Jia", "title": "Attentive Normalization for Conditional Image Generation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional convolution-based generative adversarial networks synthesize\nimages based on hierarchical local operations, where long-range dependency\nrelation is implicitly modeled with a Markov chain. It is still not sufficient\nfor categories with complicated structures. In this paper, we characterize\nlong-range dependence with attentive normalization (AN), which is an extension\nto traditional instance normalization. Specifically, the input feature map is\nsoftly divided into several regions based on its internal semantic similarity,\nwhich are respectively normalized. It enhances consistency between distant\nregions with semantic correspondence. Compared with self-attention GAN, our\nattentive normalization does not need to measure the correlation of all\nlocations, and thus can be directly applied to large-size feature maps without\nmuch computational burden. Extensive experiments on class-conditional image\ngeneration and semantic inpainting verify the efficacy of our proposed module.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:12:25 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wang", "Yi", ""], ["Chen", "Ying-Cong", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""], ["Jia", "Jiaya", ""]]}, {"id": "2004.03830", "submitter": "Xiao Jiang", "authors": "Xiao Jiang, Gang Li, Yu Liu, Xiao-Ping Zhang, You He", "title": "Change Detection in Heterogeneous Optical and SAR Remote Sensing Images\n  via Deep Homogeneous Feature Fusion", "comments": "15 pages, 14 figures, Accepted by IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing", "journal-ref": null, "doi": "10.1109/JSTARS.2020.2983993", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in heterogeneous remote sensing images is crucial for\ndisaster damage assessment. Recent methods use homogenous transformation, which\ntransforms the heterogeneous optical and SAR remote sensing images into the\nsame feature space, to achieve change detection. Such transformations mainly\noperate on the low-level feature space and may corrupt the semantic content,\ndeteriorating the performance of change detection. To solve this problem, this\npaper presents a new homogeneous transformation model termed deep homogeneous\nfeature fusion (DHFF) based on image style transfer (IST). Unlike the existing\nmethods, the DHFF method segregates the semantic content and the style features\nin the heterogeneous images to perform homogeneous transformation. The\nseparation of the semantic content and the style in homogeneous transformation\nprevents the corruption of image semantic content, especially in the regions of\nchange. In this way, the detection performance is improved with accurate\nhomogeneous transformation. Furthermore, we present a new iterative IST (IIST)\nstrategy, where the cost function in each IST iteration measures and thus\nmaximizes the feature homogeneity in additional new feature subspaces for\nchange detection. After that, change detection is accomplished accurately on\nthe original and the transformed images that are in the same feature space.\nReal remote sensing images acquired by SAR and optical satellites are utilized\nto evaluate the performance of the proposed method. The experiments demonstrate\nthat the proposed DHFF method achieves significant improvement for change\ndetection in heterogeneous optical and SAR remote sensing images, in terms of\nboth accuracy rate and Kappa index.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:27:37 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Jiang", "Xiao", ""], ["Li", "Gang", ""], ["Liu", "Yu", ""], ["Zhang", "Xiao-Ping", ""], ["He", "You", ""]]}, {"id": "2004.03842", "submitter": "Hayoung Kim", "authors": "Hayoung Kim, Dongchan Kim, Gihoon Kim, Jeongmin Cho and Kunsoo Huh", "title": "Multi-Head Attention based Probabilistic Vehicle Trajectory Prediction", "comments": "6 pages, 5 figures, 2020 IEEE Intelligent Vehicles Symposium (IV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents online-capable deep learning model for probabilistic\nvehicle trajectory prediction. We propose a simple encoder-decoder architecture\nbased on multi-head attention. The proposed model generates the distribution of\nthe predicted trajectories for multiple vehicles in parallel. Our approach to\nmodel the interactions can learn to attend to a few influential vehicles in an\nunsupervised manner, which can improve the interpretability of the network. The\nexperiments using naturalistic trajectories at highway show the clear\nimprovement in terms of positional error on both longitudinal and lateral\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 06:58:51 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 07:53:56 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 09:47:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kim", "Hayoung", ""], ["Kim", "Dongchan", ""], ["Kim", "Gihoon", ""], ["Cho", "Jeongmin", ""], ["Huh", "Kunsoo", ""]]}, {"id": "2004.03848", "submitter": "Weiwei Jiang", "authors": "Weiwei Jiang", "title": "MNIST-MIX: A Multi-language Handwritten Digit Recognition Dataset", "comments": "3 pages, 1 figure, 2 tables", "journal-ref": "IOP SciNotes, 2020", "doi": "10.1088/2633-1357/abad0e", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we contribute a multi-language handwritten digit recognition\ndataset named MNIST-MIX, which is the largest dataset of the same type in terms\nof both languages and data samples. With the same data format with MNIST,\nMNIST-MIX can be seamlessly applied in existing studies for handwritten digit\nrecognition. By introducing digits from 10 different languages, MNIST-MIX\nbecomes a more challenging dataset and its imbalanced classification requires a\nbetter design of models. We also present the results of applying a LeNet model\nwhich is pre-trained on MNIST as the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:17:32 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Jiang", "Weiwei", ""]]}, {"id": "2004.03860", "submitter": "Matti Pellikka D.Sc.", "authors": "Matti Pellikka and Valtteri Lahtinen", "title": "A Robust Method for Image Stitching", "comments": null, "journal-ref": "Pattern Analysis and Applications, 2021", "doi": "10.1007/s10044-021-01005-8", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for large-scale image stitching that is robust\nagainst repetitive patterns and featureless regions in the imagery. In such\ncases, state-of-the-art image stitching methods easily produce image alignment\nartifacts, since they may produce false pairwise image registrations that are\nin conflict within the global connectivity graph. Our method augments the\ncurrent methods by collecting all the plausible pairwise image registration\ncandidates, among which globally consistent candidates are chosen. This enables\nthe stitching process to determine the correct pairwise registrations by\nutilizing all the available information from the whole imagery, such as\nunambiguous registrations outside the repeating pattern and featureless\nregions. We formalize the method as a weighted multigraph whose nodes represent\nthe individual image transformations from the composite image, and whose sets\nof multiple edges between two nodes represent all the plausible transformations\nbetween the pixel coordinates of the two images. The edge weights represent the\nplausibility of the transformations. The image transformations and the edge\nweights are solved from a non-linear minimization problem with linear\nconstraints, for which a projection method is used. As an example, we apply the\nmethod in a large-scale scanning application where the transformations are\nprimarily translations with only slight rotation and scaling component. Despite\nthese simplifications, the state-of-the-art methods do not produce adequate\nresults in such applications, since the image overlap is small, which can be\nfeatureless or repetitive, and misalignment artifacts and their concealment are\nunacceptable.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:53:31 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 12:25:49 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 04:52:35 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Pellikka", "Matti", ""], ["Lahtinen", "Valtteri", ""]]}, {"id": "2004.03867", "submitter": "Litu Rout", "authors": "Litu Rout, Indranil Misra, S Manthira Moorthi, Debajyoti Dhar", "title": "S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for\n  Multi-Spectral Band Synthesis", "comments": "Computer Vision and Pattern Recognition (CVPR) Workshop on Large\n  Scale Computer Vision for Remote Sensing Imagery", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersection of adversarial learning and satellite image processing is an\nemerging field in remote sensing. In this study, we intend to address synthesis\nof high resolution multi-spectral satellite imagery using adversarial learning.\nGuided by the discovery of attention mechanism, we regulate the process of band\nsynthesis through spatio-spectral Laplacian attention. Further, we use\nWasserstein GAN with gradient penalty norm to improve training and stability of\nadversarial learning. In this regard, we introduce a new cost function for the\ndiscriminator based on spatial attention and domain adaptation loss. We\ncritically analyze the qualitative and quantitative results compared with\nstate-of-the-art methods using widely adopted evaluation metrics. Our\nexperiments on datasets of three different sensors, namely LISS-3, LISS-4, and\nWorldView-2 show that attention learning performs favorably against\nstate-of-the-art methods. Using the proposed method we provide an additional\ndata product in consistent with existing high resolution bands. Furthermore, we\nsynthesize over 4000 high resolution scenes covering various terrains to\nanalyze scientific fidelity. At the end, we demonstrate plausible large scale\nreal world applications of the synthesized band.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 08:07:00 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Rout", "Litu", ""], ["Misra", "Indranil", ""], ["Moorthi", "S Manthira", ""], ["Dhar", "Debajyoti", ""]]}, {"id": "2004.03879", "submitter": "Litu Rout", "authors": "Litu Rout, Saumyaa Shah, S Manthira Moorthi, Debajyoti Dhar", "title": "Monte-Carlo Siamese Policy on Actor for Satellite Image Super Resolution", "comments": "Computer Vision and Pattern Recognition (CVPR) Workshop on Large\n  Scale Computer Vision for Remote Sensing Imagery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years supervised and adversarial learning have been widely\nadopted in various complex computer vision tasks. It seems natural to wonder\nwhether another branch of artificial intelligence, commonly known as\nReinforcement Learning (RL) can benefit such complex vision tasks. In this\nstudy, we explore the plausible usage of RL in super resolution of remote\nsensing imagery. Guided by recent advances in super resolution, we propose a\ntheoretical framework that leverages the benefits of supervised and\nreinforcement learning. We argue that a straightforward implementation of RL is\nnot adequate to address ill-posed super resolution as the action variables are\nnot fully known. To tackle this issue, we propose to parameterize action\nvariables by matrices, and train our policy network using Monte-Carlo sampling.\nWe study the implications of parametric action space in a model-free\nenvironment from theoretical and empirical perspective. Furthermore, we analyze\nthe quantitative and qualitative results on both remote sensing and non-remote\nsensing datasets. Based on our experiments, we report considerable improvement\nover state-of-the-art methods by encapsulating supervised models in a\nreinforcement learning framework.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 08:39:08 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Rout", "Litu", ""], ["Shah", "Saumyaa", ""], ["Moorthi", "S Manthira", ""], ["Dhar", "Debajyoti", ""]]}, {"id": "2004.03882", "submitter": "Youyi Song", "authors": "Youyi Song, Zhen Yu, Teng Zhou, Jeremy Yuen-Chun Teoh, Baiying Lei,\n  Kup-Sze Choi, Jing Qin", "title": "CNN in CT Image Segmentation: Beyound Loss Function for Expoliting\n  Ground Truth Images", "comments": "4 pages, 3 figures, and having been accepted by ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting more information from ground truth (GT) images now is a new\nresearch direction for further improving CNN's performance in CT image\nsegmentation. Previous methods focus on devising the loss function for\nfulfilling such a purpose. However, it is rather difficult to devise a general\nand optimization-friendly loss function. We here present a novel and practical\nmethod that exploits GT images beyond the loss function. Our insight is that\nfeature maps of two CNNs trained respectively on GT and CT images should be\nsimilar on some metric space, because they both are used to describe the same\nobjects for the same purpose. We hence exploit GT images by enforcing such two\nCNNs' feature maps to be consistent. We assess the proposed method on two data\nsets, and compare its performance to several competitive methods. Extensive\nexperimental results show that the proposed method is effective, outperforming\nall the compared methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 08:44:39 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Song", "Youyi", ""], ["Yu", "Zhen", ""], ["Zhou", "Teng", ""], ["Teoh", "Jeremy Yuen-Chun", ""], ["Lei", "Baiying", ""], ["Choi", "Kup-Sze", ""], ["Qin", "Jing", ""]]}, {"id": "2004.03891", "submitter": "Apratim Bhattacharyya", "authors": "Shweta Mahajan, Apratim Bhattacharyya, Mario Fritz, Bernt Schiele,\n  Stefan Roth", "title": "Normalizing Flows with Multi-Scale Autoregressive Priors", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models are an important class of exact inference models\nthat admit efficient inference and sampling for image synthesis. Owing to the\nefficiency constraints on the design of the flow layers, e.g. split coupling\nflow layers in which approximately half the pixels do not undergo further\ntransformations, they have limited expressiveness for modeling long-range data\ndependencies compared to autoregressive models that rely on conditional\npixel-wise generation. In this work, we improve the representational power of\nflow-based models by introducing channel-wise dependencies in their latent\nspace through multi-scale autoregressive priors (mAR). Our mAR prior for models\nwith split coupling flow layers (mAR-SCF) can better capture dependencies in\ncomplex multimodal data. The resulting model achieves state-of-the-art density\nestimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that\nmAR-SCF allows for improved image generation quality, with gains in FID and\nInception scores compared to state-of-the-art flow-based models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 09:07:11 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Mahajan", "Shweta", ""], ["Bhattacharyya", "Apratim", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""], ["Roth", "Stefan", ""]]}, {"id": "2004.03892", "submitter": "Youyi Song", "authors": "Youyi Song, Lei Zhu, Baiying Lei, Bin Sheng, Qi Dou, Jing Qin, Kup-Sze\n  Choi", "title": "Constrained Multi-shape Evolution for Overlapping Cytoplasm Segmentation", "comments": "12 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting overlapping cytoplasm of cells in cervical smear images is a\nclinically essential task, for quantitatively measuring cell-level features in\norder to diagnose cervical cancer. This task, however, remains rather\nchallenging, mainly due to the deficiency of intensity (or color) information\nin the overlapping region. Although shape prior-based models that compensate\nintensity deficiency by introducing prior shape information (shape priors)\nabout cytoplasm are firmly established, they often yield visually implausible\nresults, mainly because they model shape priors only by limited shape\nhypotheses about cytoplasm, exploit cytoplasm-level shape priors alone, and\nimpose no shape constraint on the resulting shape of the cytoplasm. In this\npaper, we present a novel and effective shape prior-based approach, called\nconstrained multi-shape evolution, that segments all overlapping cytoplasms in\nthe clump simultaneously by jointly evolving each cytoplasm's shape guided by\nthe modeled shape priors. We model local shape priors (cytoplasm--level) by an\ninfinitely large shape hypothesis set which contains all possible shapes of the\ncytoplasm. In the shape evolution, we compensate intensity deficiency for the\nsegmentation by introducing not only the modeled local shape priors but also\nglobal shape priors (clump--level) modeled by considering mutual shape\nconstraints of cytoplasms in the clump. We also constrain the resulting shape\nin each evolution to be in the built shape hypothesis set, for further reducing\nimplausible segmentation results. We evaluated the proposed method in two\ntypical cervical smear datasets, and the extensive experimental results show\nthat the proposed method is effective to segment overlapping cytoplasm,\nconsistently outperforming the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 09:08:07 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 12:00:02 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Song", "Youyi", ""], ["Zhu", "Lei", ""], ["Lei", "Baiying", ""], ["Sheng", "Bin", ""], ["Dou", "Qi", ""], ["Qin", "Jing", ""], ["Choi", "Kup-Sze", ""]]}, {"id": "2004.03898", "submitter": "Michael Gygli", "authors": "Michael Gygli, Jasper Uijlings, Vittorio Ferrari", "title": "Towards Reusable Network Components by Learning Compatible\n  Representations", "comments": "Preprint; To be presented at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to make a first step towards compatible and hence\nreusable network components. Rather than training networks for different tasks\nindependently, we adapt the training process to produce network components that\nare compatible across tasks. In particular, we split a network into two\ncomponents, a features extractor and a target task head, and propose various\napproaches to accomplish compatibility between them. We systematically analyse\nthese approaches on the task of image classification on standard datasets. We\ndemonstrate that we can produce components which are directly compatible\nwithout any fine-tuning or compromising accuracy on the original tasks.\nAfterwards, we demonstrate the use of compatible components on three\napplications: Unsupervised domain adaptation, transferring classifiers across\nfeature extractors with different architectures, and increasing the\ncomputational efficiency of transfer learning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 09:21:37 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 16:40:59 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 13:31:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gygli", "Michael", ""], ["Uijlings", "Jasper", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2004.03915", "submitter": "Ming Liu", "authors": "Ming Liu, Zhilu Zhang, Liya Hou, Wangmeng Zuo, Lei Zhang", "title": "Deep Adaptive Inference Networks for Single Image Super-Resolution", "comments": "Code can be found at https://github.com/csmliu/AdaDSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed tremendous progress in single image\nsuper-resolution (SISR) owing to the deployment of deep convolutional neural\nnetworks (CNNs). For most existing methods, the computational cost of each SISR\nmodel is irrelevant to local image content, hardware platform and application\nscenario. Nonetheless, content and resource adaptive model is more preferred,\nand it is encouraging to apply simpler and efficient networks to the easier\nregions with less details and the scenarios with restricted efficiency\nconstraints. In this paper, we take a step forward to address this issue by\nleveraging the adaptive inference networks for deep SISR (AdaDSR). In\nparticular, our AdaDSR involves an SISR model as backbone and a lightweight\nadapter module which takes image features and resource constraint as input and\npredicts a map of local network depth. Adaptive inference can then be performed\nwith the support of efficient sparse convolution, where only a fraction of the\nlayers in the backbone is performed at a given position according to its\npredicted depth. The network learning can be formulated as the joint\noptimization of reconstruction and network depth losses. In the inference\nstage, the average depth can be flexibly tuned to meet a range of efficiency\nconstraints. Experiments demonstrate the effectiveness and adaptability of our\nAdaDSR in contrast to its counterparts (e.g., EDSR and RCAN).\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 10:08:20 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Liu", "Ming", ""], ["Zhang", "Zhilu", ""], ["Hou", "Liya", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "2004.03939", "submitter": "Jiawen Lyn", "authors": "Jiawen Lyn, Sen Yan", "title": "Image super-resolution reconstruction based on attention mechanism and\n  feature fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at the problems that the convolutional neural networks neglect to\ncapture the inherent attributes of natural images and extract features only in\na single scale in the field of image super-resolution reconstruction, a network\nstructure based on attention mechanism and multi-scale feature fusion is\nproposed. By using the attention mechanism, the network can effectively\nintegrate the non-local information and second-order features of the image, so\nas to improve the feature expression ability of the network. At the same time,\nthe convolution kernel of different scales is used to extract the multi-scale\ninformation of the image, so as to preserve the complete information\ncharacteristics at different scales. Experimental results show that the\nproposed method can achieve better performance over other representative\nsuper-resolution reconstruction algorithms in objective quantitative metrics\nand visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 11:20:10 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Lyn", "Jiawen", ""], ["Yan", "Sen", ""]]}, {"id": "2004.03948", "submitter": "Yang Zhang", "authors": "Yang Zhang, Changhui Hu, Xiaobo Lu", "title": "Improved YOLOv3 Object Classification in Intelligent Transportation\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology of vehicle and driver detection in Intelligent Transportation\nSystem(ITS) is a hot topic in recent years. In particular, the driver detection\nis still a challenging problem which is conductive to supervising traffic order\nand maintaining public safety. In this paper, an algorithm based on YOLOv3 is\nproposed to realize the detection and classification of vehicles, drivers, and\npeople on the highway, so as to achieve the purpose of distinguishing driver\nand passenger and form a one-to-one correspondence between vehicles and\ndrivers. The proposed model and contrast experiment are conducted on our\nself-build traffic driver's face database. The effectiveness of our proposed\nalgorithm is validated by extensive experiments and verified under various\ncomplex highway conditions. Compared with other advanced vehicle and driver\ndetection technologies, the model has a good performance and is robust to road\nblocking, different attitudes, and extreme lighting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 11:45:13 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Zhang", "Yang", ""], ["Hu", "Changhui", ""], ["Lu", "Xiaobo", ""]]}, {"id": "2004.03955", "submitter": "Gang Liu", "authors": "Gang Liu and Jing Wang", "title": "Dendrite Net: A White-Box Module for Classification, Regression, and\n  System Identification", "comments": "Dendrite (DD)---A new \"block\" since decades. Renamed DD for avoiding\n  confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a basic machine learning algorithm, named Dendrite Net or\nDD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's\nmain concept is that the algorithm can recognize this class after learning, if\nthe output's logical expression contains the corresponding class's logical\nrelationship among inputs ($ and \\backslash or \\backslash not $). Experiments\nand results: DD, the first white-box machine learning algorithm, showed\nexcellent system identification performance for the black-box system. Secondly,\nit was verified by nine real-world applications that DD brought better\ngeneralization capability relative to MLP architecture that imitated neurons'\ncell body (Cell body Net) for regression. Thirdly, by MNIST and FASHION-MNIST\ndatasets, it was verified that DD showed higher testing accuracy under greater\ntraining loss than Cell body Net for classification. The number of modules can\neffectively adjust DD's logical expression capacity, which avoids over-fitting\nand makes it easy to get a model with outstanding generalization capability.\nFinally, repeated experiments in $ MATLAB $ and $ PyTorch $ ($ Python $)\ndemonstrated that DD was faster than Cell body Net both in epoch and\nforward-propagation. We highlight DD's white-box attribute, controllable\nprecision for better generalization capability, and lower computational\ncomplexity. Not only can DD be used for generalized engineering, but DD has\nvast development potential as a module for deep learning. DD code is available\nat https://github.com/liugang1234567/Gang-neuron.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:02:16 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 10:22:57 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 18:46:11 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2020 08:09:26 GMT"}, {"version": "v5", "created": "Tue, 27 Oct 2020 07:54:57 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liu", "Gang", ""], ["Wang", "Jing", ""]]}, {"id": "2004.03961", "submitter": "Jianwei Liu", "authors": "Jianwei Liu, Jinsong Han, Feng Lin, Kui Ren", "title": "Adversary Helps: Gradient-based Device-Free Domain-Independent Gesture\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless signal-based gesture recognition has promoted the developments of VR\ngame, smart home, etc. However, traditional approaches suffer from the\ninfluence of the domain gap. Low recognition accuracy occurs when the\nrecognition model is trained in one domain but is used in another domain.\nThough some solutions, such as adversarial learning, transfer learning and\nbody-coordinate velocity profile, have been proposed to achieve cross-domain\nrecognition, these solutions more or less have flaws. In this paper, we define\nthe concept of domain gap and then propose a more promising solution, namely\nDI, to eliminate domain gap and further achieve domain-independent gesture\nrecognition. DI leverages the sign map of the gradient map as the domain gap\neliminator to improve the recognition accuracy. We conduct experiments with ten\ndomains and ten gestures. The experiment results show that DI can achieve the\nrecognition accuracies of 87.13%, 90.12% and 94.45% on KNN, SVM and CNN, which\noutperforms existing solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:20:44 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Liu", "Jianwei", ""], ["Han", "Jinsong", ""], ["Lin", "Feng", ""], ["Ren", "Kui", ""]]}, {"id": "2004.03967", "submitter": "Johanna Wald", "authors": "Johanna Wald, Helisa Dhamo, Nassir Navab, Federico Tombari", "title": "Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions", "comments": "first two authors contributed equally, CVPR 2020, video\n  https://youtu.be/8D3HjYf6cYw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding has been of high interest in computer vision. It\nencompasses not only identifying objects in a scene, but also their\nrelationships within the given context. With this goal, a recent line of works\ntackles 3D semantic segmentation and scene layout prediction. In our work we\nfocus on scene graphs, a data structure that organizes the entities of a scene\nin a graph, where objects are nodes and their relationships modeled as edges.\nWe leverage inference on scene graphs as a way to carry out 3D scene\nunderstanding, mapping objects and their relationships. In particular, we\npropose a learned method that regresses a scene graph from the point cloud of a\nscene. Our novel architecture is based on PointNet and Graph Convolutional\nNetworks (GCN). In addition, we introduce 3DSSG, a semi-automatically generated\ndataset, that contains semantically rich scene graphs of 3D scenes. We show the\napplication of our method in a domain-agnostic retrieval task, where graphs\nserve as an intermediate representation for 3D-3D and 2D-3D matching.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:25:25 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wald", "Johanna", ""], ["Dhamo", "Helisa", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2004.03989", "submitter": "M\\'arton V\\'eges", "authors": "Marton Veges, Andras Lorincz", "title": "Multi-Person Absolute 3D Human Pose Estimation with Weak Depth\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 3D human pose estimation one of the biggest problems is the lack of large,\ndiverse datasets. This is especially true for multi-person 3D pose estimation,\nwhere, to our knowledge, there are only machine generated annotations available\nfor training. To mitigate this issue, we introduce a network that can be\ntrained with additional RGB-D images in a weakly supervised fashion. Due to the\nexistence of cheap sensors, videos with depth maps are widely available, and\nour method can exploit a large, unannotated dataset. Our algorithm is a\nmonocular, multi-person, absolute pose estimator. We evaluate the algorithm on\nseveral benchmarks, showing a consistent improvement in error rates. Also, our\nmodel achieves state-of-the-art results on the MuPoTS-3D dataset by a\nconsiderable margin.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 13:29:22 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Veges", "Marton", ""], ["Lorincz", "Andras", ""]]}, {"id": "2004.04023", "submitter": "Denis Pastory Rubanga Mr.", "authors": "Denis P.Rubanga, Loyani K. Loyani, Mgaya Richard, Sawahiko Shimada", "title": "A Deep Learning Approach for Determining Effects of Tuta Absoluta in\n  Tomato Plants", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early quantification of Tuta absoluta pest's effects in tomato plants is a\nvery important factor in controlling and preventing serious damages of the\npest. The invasion of Tuta absoluta is considered a major threat to tomato\nproduction causing heavy loss ranging from 80 to 100 percent when not properly\nmanaged. Therefore, real-time and early quantification of tomato leaf miner\nTuta absoluta, can play an important role in addressing the issue of pest\nmanagement and enhance farmers' decisions. In this study, we propose a\nConvolutional Neural Network (CNN) approach in determining the effects of Tuta\nabsoluta in tomato plants. Four CNN pre-trained architectures (VGG16, VGG19,\nResNet and Inception-V3) were used in training classifiers on a dataset\ncontaining health and infested tomato leaves collected from real field\nexperiments. Among the pre-trained architectures, experimental results showed\nthat Inception-V3 yielded the best results with an average accuracy of 87.2\npercent in estimating the severity status of Tuta absoluta in tomato plants.\nThe pre-trained models could also easily identify High Tuta severity status\ncompared to other severity status (Low tuta and No tuta)\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:41:38 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Rubanga", "Denis P.", ""], ["Loyani", "Loyani K.", ""], ["Richard", "Mgaya", ""], ["Shimada", "Sawahiko", ""]]}, {"id": "2004.04069", "submitter": "Peter Hancock", "authors": "P. J. B. Hancock, R. S. Somai and V. R. Mileva", "title": "Convolutional neural net face recognition works in non-human-like ways", "comments": "8 pages, 2 figures. Submitted to Royal Society Open Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) give state of the art performance in\nmany pattern recognition problems but can be fooled by carefully crafted\npatterns of noise. We report that CNN face recognition systems also make\nsurprising \"errors\". We tested six commercial face recognition CNNs and found\nthat they outperform typical human participants on standard face matching\ntasks. However, they also declare matches that humans would not, where one\nimage from the pair has been transformed to look a different sex or race. This\nis not due to poor performance; the best CNNs perform almost perfectly on the\nhuman face matching tasks, but also declare the most matches for faces of a\ndifferent apparent race or sex. Although differing on the salience of sex and\nrace, humans and computer systems are not working in completely different ways.\nThey tend to find the same pairs of images difficult, suggesting some agreement\nabout the underlying similarity space.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 15:42:14 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 11:59:06 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Hancock", "P. J. B.", ""], ["Somai", "R. S.", ""], ["Mileva", "V. R.", ""]]}, {"id": "2004.04081", "submitter": "Andrew Hobbs", "authors": "Andrew Hobbs, Stacey Svetlichnaya", "title": "Satellite-based Prediction of Forage Conditions for Livestock in\n  Northern Kenya", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first dataset of satellite images labeled with\nforage quality by on-the-ground experts and provides proof of concept for\napplying computer vision methods to index-based drought insurance. We also\npresent the results of a collaborative benchmark tool used to crowdsource an\naccurate machine learning model on the dataset. Our methods significantly\noutperform the existing technology for an insurance program in Northern Kenya,\nsuggesting that a computer vision-based approach could substantially benefit\npastoralists, whose exposure to droughts is severe and worsening with climate\nchange.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:03:50 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 18:12:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Hobbs", "Andrew", ""], ["Svetlichnaya", "Stacey", ""]]}, {"id": "2004.04090", "submitter": "Jan Quenzel", "authors": "Jan Quenzel, Radu Alexandru Rosu, Thomas L\\\"abe, Cyrill Stachniss, and\n  Sven Behnke", "title": "Beyond Photometric Consistency: Gradient-based Dissimilarity for\n  Improving Visual Odometry and Stereo Matching", "comments": "Accepted for International Conference on Robotic and Automation\n  (ICRA), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation and map building are central ingredients of autonomous robots\nand typically rely on the registration of sensor data. In this paper, we\ninvestigate a new metric for registering images that builds upon on the idea of\nthe photometric error. Our approach combines a gradient orientation-based\nmetric with a magnitude-dependent scaling term. We integrate both into stereo\nestimation as well as visual odometry systems and show clear benefits for\ntypical disparity and direct image registration tasks when using our proposed\nmetric. Our experimental evaluation indicats that our metric leads to more\nrobust and more accurate estimates of the scene depth as well as camera\ntrajectory. Thus, the metric improves camera pose estimation and in turn the\nmapping capabilities of mobile robots. We believe that a series of existing\nvisual odometry and visual SLAM systems can benefit from the findings reported\nin this paper.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:13:25 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Quenzel", "Jan", ""], ["Rosu", "Radu Alexandru", ""], ["L\u00e4be", "Thomas", ""], ["Stachniss", "Cyrill", ""], ["Behnke", "Sven", ""]]}, {"id": "2004.04091", "submitter": "Xun Xu", "authors": "Xun Xu, Gim Hee Lee", "title": "Weakly Supervised Semantic Point Cloud Segmentation:Towards 10X Fewer\n  Labels", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis has received much attention recently; and segmentation\nis one of the most important tasks. The success of existing approaches is\nattributed to deep network design and large amount of labelled training data,\nwhere the latter is assumed to be always available. However, obtaining 3d point\ncloud segmentation labels is often very costly in practice. In this work, we\npropose a weakly supervised point cloud segmentation approach which requires\nonly a tiny fraction of points to be labelled in the training stage. This is\nmade possible by learning gradient approximation and exploitation of additional\nspatial and color smoothness constraints. Experiments are done on three public\ndatasets with different degrees of weak supervision. In particular, our\nproposed method can produce results that are close to and sometimes even better\nthan its fully supervised counterpart with 10$\\times$ fewer labels.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:14:41 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Xu", "Xun", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2004.04093", "submitter": "Manel Mart\\'inez-Ram\\'on", "authors": "Meenu Ajith, Aswathy Rajendra Kurup, and Manel Mart\\'inez-Ram\\'on", "title": "Time accelerated image super-resolution using shallow residual feature\n  representative network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in deep learning indicate significant progress in the\nfield of single image super-resolution. With the advent of these techniques,\nhigh-resolution image with high peak signal to noise ratio (PSNR) and excellent\nperceptual quality can be reconstructed. The major challenges associated with\nexisting deep convolutional neural networks are their computational complexity\nand time; the increasing depth of the networks, often result in high space\ncomplexity. To alleviate these issues, we developed an innovative shallow\nresidual feature representative network (SRFRN) that uses a bicubic\ninterpolated low-resolution image as input and residual representative units\n(RFR) which include serially stacked residual non-linear convolutions.\nFurthermore, the reconstruction of the high-resolution image is done by\ncombining the output of the RFR units and the residual output from the bicubic\ninterpolated LR image. Finally, multiple experiments have been performed on the\nbenchmark datasets and the proposed model illustrates superior performance for\nhigher scales. Besides, this model also exhibits faster execution time compared\nto all the existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:17:42 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Ajith", "Meenu", ""], ["Kurup", "Aswathy Rajendra", ""], ["Mart\u00ednez-Ram\u00f3n", "Manel", ""]]}, {"id": "2004.04122", "submitter": "Nibaran Das", "authors": "Arnab Banerjee, Nibaran Das, Mita Nasipuri", "title": "Skin Diseases Detection using LBP and WLD- An Ensembling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In all developing and developed countries in the world, skin diseases are\nbecoming a very frequent health problem for the humans of all age groups. Skin\nproblems affect mental health, develop addiction to alcohol and drugs and\nsometimes causes social isolation. Considering the importance, we propose an\nautomatic technique to detect three popular skin diseases- Leprosy, Tinea\nversicolor and Vitiligofrom the images of skin lesions. The proposed technique\ninvolves Weber local descriptor and Local binary pattern to represent texture\npattern of the affected skin regions. This ensemble technique achieved 91.38%\naccuracy using multi-level support vector machine classifier, where features\nare extracted from different regions that are based on center of gravity. We\nhave also applied some popular deep learn-ing networks such as MobileNet,\nResNet_152, GoogLeNet,DenseNet_121, and ResNet_101. We get 89% accuracy using\nResNet_101. The ensemble approach clearly outperform all of the used deep\nlearning networks. This imaging tool will be useful for early skin disease\nscreening.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:09:59 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Banerjee", "Arnab", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2004.04136", "submitter": "Michael Laskin", "authors": "Aravind Srinivas, Michael Laskin, Pieter Abbeel", "title": "CURL: Contrastive Unsupervised Representations for Reinforcement\n  Learning", "comments": "First two authors contributed equally, website:\n  https://mishalaskin.github.io/curl code: https://github.com/MishaLaskin/curl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CURL: Contrastive Unsupervised Representations for Reinforcement\nLearning. CURL extracts high-level features from raw pixels using contrastive\nlearning and performs off-policy control on top of the extracted features. CURL\noutperforms prior pixel-based methods, both model-based and model-free, on\ncomplex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and\n1.2x performance gains at the 100K environment and interaction steps benchmarks\nrespectively. On the DeepMind Control Suite, CURL is the first image-based\nalgorithm to nearly match the sample-efficiency of methods that use state-based\nfeatures. Our code is open-sourced and available at\nhttps://github.com/MishaLaskin/curl.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:40:43 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 17:54:47 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 16:37:04 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 15:34:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Srinivas", "Aravind", ""], ["Laskin", "Michael", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2004.04141", "submitter": "Leslie Smith", "authors": "Leslie N. Smith, Adam Conovaloff", "title": "Empirical Perspectives on One-Shot Semi-supervised Learning", "comments": "Short paper with interesting results pointing to further\n  investigation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest obstacles in the adoption of deep neural networks for new\napplications is that training the network typically requires a large number of\nmanually labeled training samples. We empirically investigate the scenario\nwhere one has access to large amounts of unlabeled data but require labeling\nonly a single prototypical sample per class in order to train a deep network\n(i.e., one-shot semi-supervised learning). Specifically, we investigate the\nrecent results reported in FixMatch for one-shot semi-supervised learning to\nunderstand the factors that affect and impede high accuracies and reliability\nfor one-shot semi-supervised learning of Cifar-10. For example, we discover\nthat one barrier to one-shot semi-supervised learning for high-performance\nimage classification is the unevenness of class accuracy during the training.\nThese results point to solutions that might enable more widespread adoption of\none-shot semi-supervised training methods for new applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:51:06 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Smith", "Leslie N.", ""], ["Conovaloff", "Adam", ""]]}, {"id": "2004.04143", "submitter": "Junhwa Hur", "authors": "Junhwa Hur, Stefan Roth", "title": "Self-Supervised Monocular Scene Flow Estimation", "comments": "To appear at CVPR 2020 (Oral); a typo corrected in the reference\n  section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow estimation has been receiving increasing attention for 3D\nenvironment perception. Monocular scene flow estimation -- obtaining 3D\nstructure and 3D motion from two temporally consecutive images -- is a highly\nill-posed problem, and practical solutions are lacking to date. We propose a\nnovel monocular scene flow method that yields competitive accuracy and\nreal-time performance. By taking an inverse problem view, we design a single\nconvolutional neural network (CNN) that successfully estimates depth and 3D\nmotion simultaneously from a classical optical flow cost volume. We adopt\nself-supervised learning with 3D loss functions and occlusion reasoning to\nleverage unlabeled data. We validate our design choices, including the proxy\nloss and augmentation setup. Our model achieves state-of-the-art accuracy among\nunsupervised/self-supervised learning approaches to monocular scene flow, and\nyields competitive results for the optical flow and monocular depth estimation\nsub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields\npromising results in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:55:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 22:17:10 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "2004.04147", "submitter": "Lia Morra", "authors": "Lia Morra, Francesco Manigrasso, Giuseppe Canto, Claudio Gianfrate,\n  Enrico Guarino, Fabrizio Lamberti", "title": "Slicing and dicing soccer: automatic detection of complex events from\n  spatio-temporal data", "comments": "accepted at 17th International Conference on Image Analysis and\n  Recognition ICIAR 2020", "journal-ref": null, "doi": "10.1007/978-3-030-50347-5_11", "report-no": null, "categories": "cs.CV cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of events in sport videos has im-portant applications\nfor data analytics, as well as for broadcasting andmedia companies. This paper\npresents a comprehensive approach for de-tecting a wide range of complex events\nin soccer videos starting frompositional data. The event detector is designed\nas a two-tier system thatdetectsatomicandcomplex events. Atomic events are\ndetected basedon temporal and logical combinations of the detected objects,\ntheir rel-ative distances, as well as spatio-temporal features such as velocity\nandacceleration. Complex events are defined as temporal and logical\ncom-binations of atomic and complex events, and are expressed by meansof a\ndeclarative Interval Temporal Logic (ITL). The effectiveness of theproposed\napproach is demonstrated over 16 different events, includingcomplex situations\nsuch as tackles and filtering passes. By formalizingevents based on principled\nITL, it is possible to easily perform reason-ing tasks, such as understanding\nwhich passes or crosses result in a goalbeing scored. To counterbalance the\nlack of suitable, annotated publicdatasets, we built on an open source soccer\nsimulation engine to re-lease the synthetic SoccER (Soccer Event Recognition)\ndataset, whichincludes complete positional data and annotations for more than\n1.6 mil-lion atomic events and 9,000 complex events. The dataset and code\nareavailable at https://gitlab.com/grains2/slicing-and-dicing-soccer\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 17:57:50 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 07:30:55 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Morra", "Lia", ""], ["Manigrasso", "Francesco", ""], ["Canto", "Giuseppe", ""], ["Gianfrate", "Claudio", ""], ["Guarino", "Enrico", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2004.04177", "submitter": "Alireza Vafaei Sadr", "authors": "Alireza Vafaei Sadr, Farida Farsian", "title": "Inpainting via Generative Adversarial Networks for CMB data analysis", "comments": "19 pages, 21 figures. Prepared for submission to JCAP. All codes will\n  be published after acceptance", "journal-ref": null, "doi": "10.1088/1475-7516/2021/03/012", "report-no": null, "categories": "astro-ph.CO cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new method to inpaint the CMB signal in regions\nmasked out following a point source extraction process. We adopt a modified\nGenerative Adversarial Network (GAN) and compare different combinations of\ninternal (hyper-)parameters and training strategies. We study the performance\nusing a suitable $\\mathcal{C}_r$ variable in order to estimate the performance\nregarding the CMB power spectrum recovery. We consider a test set where one\npoint source is masked out in each sky patch with a 1.83 $\\times$ 1.83 squared\ndegree extension, which, in our gridding, corresponds to 64 $\\times$ 64 pixels.\nThe GAN is optimized for estimating performance on Planck 2018 total intensity\nsimulations. The training makes the GAN effective in reconstructing a masking\ncorresponding to about 1500 pixels with $1\\%$ error down to angular scales\ncorresponding to about 5 arcminutes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 18:00:10 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 07:38:15 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sadr", "Alireza Vafaei", ""], ["Farsian", "Farida", ""]]}, {"id": "2004.04180", "submitter": "Paul Henderson", "authors": "Paul Henderson, Vagia Tsiminaki, Christoph H. Lampert", "title": "Leveraging 2D Data to Learn Textured 3D Mesh Generation", "comments": "CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous methods have been proposed for probabilistic generative modelling of\n3D objects. However, none of these is able to produce textured objects, which\nrenders them of limited use for practical tasks. In this work, we present the\nfirst generative model of textured 3D meshes. Training such a model would\ntraditionally require a large dataset of textured meshes, but unfortunately,\nexisting datasets of meshes lack detailed textures. We instead propose a new\ntraining methodology that allows learning from collections of 2D images without\nany 3D information. To do so, we train our model to explain a distribution of\nimages by modelling each image as a 3D foreground object placed in front of a\n2D background. Thus, it learns to generate meshes that when rendered, produce\nimages similar to those in its training set.\n  A well-known problem when generating meshes with deep networks is the\nemergence of self-intersections, which are problematic for many use-cases. As a\nsecond contribution we therefore introduce a new generation process for 3D\nmeshes that guarantees no self-intersections arise, based on the physical\nintuition that faces should push one another out of the way as they move.\n  We conduct extensive experiments on our approach, reporting quantitative and\nqualitative results on both synthetic data and natural images. These show our\nmethod successfully learns to generate plausible and diverse textured 3D\nsamples for five challenging object classes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 18:00:37 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Henderson", "Paul", ""], ["Tsiminaki", "Vagia", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2004.04192", "submitter": "Elijah Cole", "authors": "Elijah Cole, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean,\n  Christophe Botella, Dan Morris, Nebojsa Jojic, Pierre Bonnet, Alexis Joly", "title": "The GeoLifeCLEF 2020 Dataset", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the geographic distribution of species is a key concern in\nconservation. By pairing species occurrences with environmental features,\nresearchers can model the relationship between an environment and the species\nwhich may be found there. To facilitate research in this area, we present the\nGeoLifeCLEF 2020 dataset, which consists of 1.9 million species observations\npaired with high-resolution remote sensing imagery, land cover data, and\naltitude, in addition to traditional low-resolution climate and soil variables.\nWe also discuss the GeoLifeCLEF 2020 competition, which aims to use this\ndataset to advance the state-of-the-art in location-based species\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 18:30:00 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Cole", "Elijah", ""], ["Deneu", "Benjamin", ""], ["Lorieul", "Titouan", ""], ["Servajean", "Maximilien", ""], ["Botella", "Christophe", ""], ["Morris", "Dan", ""], ["Jojic", "Nebojsa", ""], ["Bonnet", "Pierre", ""], ["Joly", "Alexis", ""]]}, {"id": "2004.04199", "submitter": "Liang Lin", "authors": "Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, and Liang Lin", "title": "Transferable, Controllable, and Inconspicuous Adversarial Attacks on\n  Person Re-identification With Deep Mis-Ranking", "comments": "Accepted in CVPR 2020. To attack ReID, we propose a\n  learning-to-mis-rank formulation to perturb the ranking of the system output", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of DNNs has driven the extensive applications of person\nre-identification (ReID) into a new era. However, whether ReID inherits the\nvulnerability of DNNs remains unexplored. To examine the robustness of ReID\nsystems is rather important because the insecurity of ReID systems may cause\nsevere losses, e.g., the criminals may use the adversarial perturbations to\ncheat the CCTV systems. In this work, we examine the insecurity of current\nbest-performing ReID models by proposing a learning-to-mis-rank formulation to\nperturb the ranking of the system output. As the cross-dataset transferability\nis crucial in the ReID domain, we also perform a back-box attack by developing\na novel multi-stage network architecture that pyramids the features of\ndifferent levels to extract general and transferable features for the\nadversarial perturbations. Our method can control the number of malicious\npixels by using differentiable multi-shot sampling. To guarantee the\ninconspicuousness of the attack, we also propose a new perception loss to\nachieve better visual quality. Extensive experiments on four of the largest\nReID benchmarks (i.e., Market1501 [45], CUHK03 [18], DukeMTMC [33], and MSMT17\n[40]) not only show the effectiveness of our method, but also provides\ndirections of the future improvement in the robustness of ReID systems. For\nexample, the accuracy of one of the best-performing ReID systems drops sharply\nfrom 91.8% to 1.4% after being attacked by our method. Some attack results are\nshown in Fig. 1. The code is available at\nhttps://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 18:48:29 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Wang", "Hongjun", ""], ["Wang", "Guangrun", ""], ["Li", "Ya", ""], ["Zhang", "Dongyu", ""], ["Lin", "Liang", ""]]}, {"id": "2004.04209", "submitter": "Anna Petrovskaia", "authors": "Anna Petrovskaia, Raghavendra B. Jana, Ivan V. Oseledets", "title": "A single image deep learning approach to restoration of corrupted remote\n  sensing products", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing images are used for a variety of analyses, from agricultural\nmonitoring, to disaster relief, to resource planning, among others. The images\ncan be corrupted due to a number of reasons, including instrument errors and\nnatural obstacles such as clouds. We present here a novel approach for\nreconstruction of missing information in such cases using only the corrupted\nimage as the input. The Deep Image Prior methodology eliminates the need for a\npre-trained network or an image database. It is shown that the approach easily\nbeats the performance of traditional single-image methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:11:32 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Petrovskaia", "Anna", ""], ["Jana", "Raghavendra B.", ""], ["Oseledets", "Ivan V.", ""]]}, {"id": "2004.04242", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Rui Wang, Subhransu Maji", "title": "Deep Manifold Prior", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a prior for manifold structured data, such as surfaces of 3D\nshapes, where deep neural networks are adopted to reconstruct a target shape\nusing gradient descent starting from a random initialization. We show that\nsurfaces generated this way are smooth, with limiting behavior characterized by\nGaussian processes, and we mathematically derive such properties for\nfully-connected as well as convolutional networks. We demonstrate our method in\na variety of manifold reconstruction applications, such as point cloud\ndenoising and interpolation, achieving considerably better results against\ncompetitive baselines while requiring no training data. We also show that when\ntraining data is available, our method allows developing alternate\nparametrizations of surfaces under the framework of AtlasNet, leading to a\ncompact network architecture and better reconstruction results on standard\nimage to shape reconstruction benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:47:56 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Gadelha", "Matheus", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "2004.04244", "submitter": "Aishwarya Jadhav", "authors": "Aishwarya Jadhav", "title": "Variable Rate Video Compression using a Hybrid Recurrent Convolutional\n  Learning Framework", "comments": null, "journal-ref": "2020 International Conference on Computer Communication and\n  Informatics (ICCCI)", "doi": "10.1109/ICCCI48352.2020.9104085", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural network-based image compression techniques have been\nable to outperform traditional codecs and have opened the gates for the\ndevelopment of learning-based video codecs. However, to take advantage of the\nhigh temporal correlation in videos, more sophisticated architectures need to\nbe employed. This paper presents PredEncoder, a hybrid video compression\nframework based on the concept of predictive auto-encoding that models the\ntemporal correlations between consecutive video frames using a prediction\nnetwork which is then combined with a progressive encoder network to exploit\nthe spatial redundancies. A variable-rate block encoding scheme has been\nproposed in the paper that leads to remarkably high quality to bit-rate ratios.\nBy joint training and fine-tuning of this hybrid architecture, PredEncoder has\nbeen able to gain significant improvement over the MPEG-4 codec and has\nachieved bit-rate savings over the H.264 codec in the low to medium bit-rate\nrange for HD videos and comparable results over most bit-rates for non-HD\nvideos. This paper serves to demonstrate how neural architectures can be\nleveraged to perform at par with the highly optimized traditional methodologies\nin the video compression domain.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:49:25 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 20:21:24 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jadhav", "Aishwarya", ""]]}, {"id": "2004.04249", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Mohammad Samragh, Tara Javidi, Farinaz Koushanfar", "title": "GeneCAI: Genetic Evolution for Acquiring Compact AI", "comments": null, "journal-ref": null, "doi": "10.1145/3377930.3390226", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving\ntowards more complex architectures to achieve higher inference accuracy. Model\ncompression techniques can be leveraged to efficiently deploy such\ncompute-intensive architectures on resource-limited mobile devices. Such\nmethods comprise various hyper-parameters that require per-layer customization\nto ensure high accuracy. Choosing such hyper-parameters is cumbersome as the\npertinent search space grows exponentially with model layers. This paper\nintroduces GeneCAI, a novel optimization method that automatically learns how\nto tune per-layer compression hyper-parameters. We devise a bijective\ntranslation scheme that encodes compressed DNNs to the genotype space. The\noptimality of each genotype is measured using a multi-objective score based on\naccuracy and number of floating point operations. We develop customized genetic\noperations to iteratively evolve the non-dominated solutions towards the\noptimal Pareto front, thus, capturing the optimal trade-off between model\naccuracy and complexity. GeneCAI optimization method is highly scalable and can\nachieve a near-linear performance boost on distributed multi-GPU platforms. Our\nextensive evaluations demonstrate that GeneCAI outperforms existing rule-based\nand reinforcement learning methods in DNN compression by finding models that\nlie on a better accuracy-complexity Pareto curve.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:56:37 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 04:35:42 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Samragh", "Mohammad", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2004.04278", "submitter": "Daniel Silver Dr.", "authors": "Daniel L. Silver and Jabun Nasa", "title": "Estimating Grape Yield on the Vine from Multiple Images", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A), 4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating grape yield prior to harvest is important to commercial vineyard\nproduction as it informs many vineyard and winery decisions. Currently, the\nprocess of yield estimation is time consuming and varies in its accuracy from\n75-90\\% depending on the experience of the viticulturist. This paper proposes a\nmultiple task learning (MTL) convolutional neural network (CNN) approach that\nuses images captured by inexpensive smart phones secured in a simple tripod\narrangement. The CNN models use MTL transfer from autoencoders to achieve 85\\%\naccuracy from image data captured 6 days prior to harvest.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:58:58 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Silver", "Daniel L.", ""], ["Nasa", "Jabun", ""]]}, {"id": "2004.04306", "submitter": "Colin Cooke", "authors": "Colin L. Cooke, Fanjie Kong, Amey Chaware, Kevin C. Zhou, Kanghyun\n  Kim, Rong Xu, D. Michael Ando, Samuel J. Yang, Pavan Chandra Konda, Roarke\n  Horstmeyer", "title": "Physics-enhanced machine learning for virtual fluorescence microscopy", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method of data-driven microscope design for\nvirtual fluorescence microscopy. Our results show that by including a model of\nillumination within the first layers of a deep convolutional neural network, it\nis possible to learn task-specific LED patterns that substantially improve the\nability to infer fluorescence image information from unstained transmission\nmicroscopy images. We validated our method on two different experimental\nsetups, with different magnifications and different sample types, to show a\nconsistent improvement in performance as compared to conventional illumination\nmethods. Additionally, to understand the importance of learned illumination on\ninference task, we varied the dynamic range of the fluorescent image targets\n(from one to seven bits), and showed that the margin of improvement for learned\npatterns increased with the information content of the target. This work\ndemonstrates the power of programmable optical elements at enabling better\nmachine learning algorithm performance and at providing physical insight into\nnext generation of machine-controlled imaging systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 00:17:00 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 22:19:15 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Cooke", "Colin L.", ""], ["Kong", "Fanjie", ""], ["Chaware", "Amey", ""], ["Zhou", "Kevin C.", ""], ["Kim", "Kanghyun", ""], ["Xu", "Rong", ""], ["Ando", "D. Michael", ""], ["Yang", "Samuel J.", ""], ["Konda", "Pavan Chandra", ""], ["Horstmeyer", "Roarke", ""]]}, {"id": "2004.04312", "submitter": "Andrea Burns", "authors": "Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko, Bryan A.\n  Plummer", "title": "Learning to Scale Multilingual Representations for Vision-Language Tasks", "comments": "ECCV 2020 accepted spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multilingual vision-language models either require a large number of\nadditional parameters for each supported language, or suffer performance\ndegradation as languages are added. In this paper, we propose a Scalable\nMultilingual Aligned Language Representation (SMALR) that supports many\nlanguages with few model parameters without sacrificing downstream task\nperformance. SMALR learns a fixed size language-agnostic representation for\nmost words in a multilingual vocabulary, keeping language-specific features for\njust a few. We use a masked cross-language modeling loss to align features with\ncontext from other languages. Additionally, we propose a cross-lingual\nconsistency module that ensures predictions made for a query and its machine\ntranslation are comparable. The effectiveness of SMALR is demonstrated with ten\ndiverse languages, over twice the number supported in vision-language tasks to\ndate. We evaluate on multilingual image-sentence retrieval and outperform prior\nwork by 3-4% with less than 1/5th the training parameters compared to other\nword embedding methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:03:44 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 19:01:28 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Burns", "Andrea", ""], ["Kim", "Donghyun", ""], ["Wijaya", "Derry", ""], ["Saenko", "Kate", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "2004.04317", "submitter": "Zhe Shen", "authors": "Zhe Shen, Peng Sun, Yubo Lang, Lei Liu, Silong Peng", "title": "Identification of splicing edges in tampered image based on Dichromatic\n  Reflection Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging is a sophisticated process combining a plenty of photovoltaic\nconversions, which lead to some spectral signatures beyond visual perception in\nthe final images. Any manipulation against an original image will destroy these\nsignatures and inevitably leave some traces in the final forgery. Therefore we\npresent a novel optic-physical method to discriminate splicing edges from\nnatural edges in a tampered image. First, we transform the forensic image from\nRGB into color space of S and o1o2. Then on the assumption of Dichromatic\nReflection Model, edges in the image are discovered by composite gradient and\nclassified into different types based on their different photometric\nproperties. Finally, splicing edges are reserved against natural ones by a\nsimple logical algorithm. Experiment results show the efficacy of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:25:28 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Shen", "Zhe", ""], ["Sun", "Peng", ""], ["Lang", "Yubo", ""], ["Liu", "Lei", ""], ["Peng", "Silong", ""]]}, {"id": "2004.04320", "submitter": "Ka-Ho Chow", "authors": "Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei,\n  Yanzhao Wu", "title": "TOG: Targeted Adversarial Objectness Gradient Attacks on Real-time\n  Object Detection Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of real-time huge data capturing has pushed the deep\nlearning and data analytic computing to the edge systems. Real-time object\nrecognition on the edge is one of the representative deep neural network (DNN)\npowered edge systems for real-world mission-critical applications, such as\nautonomous driving and augmented reality. While DNN powered object detection\nedge systems celebrate many life-enriching opportunities, they also open doors\nfor misuse and abuse. This paper presents three Targeted adversarial Objectness\nGradient attacks, coined as TOG, which can cause the state-of-the-art deep\nobject detection networks to suffer from object-vanishing, object-fabrication,\nand object-mislabeling attacks. We also present a universal objectness gradient\nattack to use adversarial transferability for black-box attacks, which is\neffective on any inputs with negligible attack time cost, low human\nperceptibility, and particularly detrimental to object detection edge systems.\nWe report our experimental measurements using two benchmark datasets (PASCAL\nVOC and MS COCO) on two state-of-the-art detection algorithms (YOLO and SSD).\nThe results demonstrate serious adversarial vulnerabilities and the compelling\nneed for developing robust object detection systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:36:23 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Chow", "Ka-Ho", ""], ["Liu", "Ling", ""], ["Gursoy", "Mehmet Emre", ""], ["Truex", "Stacey", ""], ["Wei", "Wenqi", ""], ["Wu", "Yanzhao", ""]]}, {"id": "2004.04322", "submitter": "Juyong Zhang", "authors": "Yuxin Yao, Bailin Deng, Weiwei Xu, Juyong Zhang", "title": "Quasi-Newton Solver for Robust Non-Rigid Registration", "comments": "Accepted to CVPR2020. The source code is available at\n  https://github.com/Juyong/Fast_RNRR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imperfect data (noise, outliers and partial overlap) and high degrees of\nfreedom make non-rigid registration a classical challenging problem in computer\nvision. Existing methods typically adopt the $\\ell_{p}$ type robust estimator\nto regularize the fitting and smoothness, and the proximal operator is used to\nsolve the resulting non-smooth problem. However, the slow convergence of these\nalgorithms limits its wide applications. In this paper, we propose a\nformulation for robust non-rigid registration based on a globally smooth robust\nestimator for data fitting and regularization, which can handle outliers and\npartial overlaps. We apply the majorization-minimization algorithm to the\nproblem, which reduces each iteration to solving a simple least-squares problem\nwith L-BFGS. Extensive experiments demonstrate the effectiveness of our method\nfor non-rigid alignment between two shapes with outliers and partial overlap,\nwith quantitative evaluation showing that it outperforms state-of-the-art\nmethods in terms of registration accuracy and computational speed. The source\ncode is available at https://github.com/Juyong/Fast_RNRR.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:45:05 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Yao", "Yuxin", ""], ["Deng", "Bailin", ""], ["Xu", "Weiwei", ""], ["Zhang", "Juyong", ""]]}, {"id": "2004.04336", "submitter": "Kentaro Wada", "authors": "Kentaro Wada, Edgar Sucar, Stephen James, Daniel Lenton, Andrew J.\n  Davison", "title": "MoreFusion: Multi-object Reasoning for 6D Pose Estimation from\n  Volumetric Fusion", "comments": "10 pages, 10 figures, IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots and other smart devices need efficient object-based scene\nrepresentations from their on-board vision systems to reason about contact,\nphysics and occlusion. Recognized precise object models will play an important\nrole alongside non-parametric reconstructions of unrecognized structures. We\npresent a system which can estimate the accurate poses of multiple known\nobjects in contact and occlusion from real-time, embodied multi-view vision.\nOur approach makes 3D object pose proposals from single RGB-D views,\naccumulates pose estimates and non-parametric occupancy information from\nmultiple views as the camera moves, and performs joint optimization to estimate\nconsistent, non-intersecting poses for multiple objects in contact.\n  We verify the accuracy and robustness of our approach experimentally on 2\nobject datasets: YCB-Video, and our own challenging Cluttered YCB-Video. We\ndemonstrate a real-time robotics application where a robot arm precisely and\norderly disassembles complicated piles of objects, using only on-board RGB-D\nvision.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:29:30 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Wada", "Kentaro", ""], ["Sucar", "Edgar", ""], ["James", "Stephen", ""], ["Lenton", "Daniel", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2004.04340", "submitter": "Hao Sun", "authors": "Hao Sun, Zhiqun Zhao and Zhihai He", "title": "Reciprocal Learning Networks for Human Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that the human trajectory is not only forward predictable, but\nalso backward predictable. Both forward and backward trajectories follow the\nsame social norms and obey the same physical constraints with the only\ndifference in their time directions. Based on this unique property, we develop\na new approach, called reciprocal learning, for human trajectory prediction.\nTwo networks, forward and backward prediction networks, are tightly coupled,\nsatisfying the reciprocal constraint, which allows them to be jointly learned.\nBased on this constraint, we borrow the concept of adversarial attacks of deep\nneural networks, which iteratively modifies the input of the network to match\nthe given or forced network output, and develop a new method for network\nprediction, called reciprocal attack for matched prediction. It further\nimproves the prediction accuracy. Our experimental results on benchmark\ndatasets demonstrate that our new method outperforms the state-of-the-art\nmethods for human trajectory prediction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:50:29 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Sun", "Hao", ""], ["Zhao", "Zhiqun", ""], ["He", "Zhihai", ""]]}, {"id": "2004.04342", "submitter": "Yang Yang", "authors": "Adam Golinski, Reza Pourreza, Yang Yang, Guillaume Sautiere, Taco S\n  Cohen", "title": "Feedback Recurrent Autoencoder for Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep generative modeling have enabled efficient modeling\nof high dimensional data distributions and opened up a new horizon for solving\ndata compression problems. Specifically, autoencoder based learned image or\nvideo compression solutions are emerging as strong competitors to traditional\napproaches. In this work, We propose a new network architecture, based on\ncommon and well studied components, for learned video compression operating in\nlow latency mode. Our method yields state of the art MS-SSIM/rate performance\non the high-resolution UVG dataset, among both learned video compression\napproaches and classical video compression methods (H.265 and H.264) in the\nrate range of interest for streaming applications. Additionally, we provide an\nanalysis of existing approaches through the lens of their underlying\nprobabilistic graphical models. Finally, we point out issues with temporal\nconsistency and color shift observed in empirical evaluation, and suggest\ndirections forward to alleviate those.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:58:07 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Golinski", "Adam", ""], ["Pourreza", "Reza", ""], ["Yang", "Yang", ""], ["Sautiere", "Guillaume", ""], ["Cohen", "Taco S", ""]]}, {"id": "2004.04345", "submitter": "Chaoqiang Zhao", "authors": "Chaoqiang Zhao, Gary G. Yen, Qiyu Sun, Chongzhen Zhang and Yang Tang", "title": "Masked GANs for Unsupervised Depth and Pose Prediction with Scale\n  Consistency", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3044181", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that adversarial learning can be used for\nunsupervised monocular depth and visual odometry (VO) estimation, in which the\nadversarial loss and the geometric image reconstruction loss are utilized as\nthe mainly supervisory signals to train the whole unsupervised framework.\nHowever, the performance of the adversarial framework and image reconstruction\nis usually limited by occlusions and the visual field changes between frames.\nThis paper proposes a masked generative adversarial network (GAN) for\nunsupervised monocular depth and ego-motion estimation.The MaskNet and Boolean\nmask scheme are designed in this framework to eliminate the effects of\nocclusions and impacts of visual field changes on the reconstruction loss and\nadversarial loss, respectively. Furthermore, we also consider the scale\nconsistency of our pose network by utilizing a new scale-consistency loss, and\ntherefore, our pose network is capable of providing the full camera trajectory\nover a long monocular sequence. Extensive experiments on the KITTI dataset show\nthat each component proposed in this paper contributes to the performance, and\nboth our depth and trajectory predictions achieve competitive performance on\nthe KITTI and Make3D datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 03:12:52 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 07:12:01 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 14:05:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhao", "Chaoqiang", ""], ["Yen", "Gary G.", ""], ["Sun", "Qiyu", ""], ["Zhang", "Chongzhen", ""], ["Tang", "Yang", ""]]}, {"id": "2004.04388", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Naveen Venkat, Ambareesh Revanur, Rahul M V, R.\n  Venkatesh Babu", "title": "Towards Inheritable Models for Open-Set Domain Adaptation", "comments": "CVPR 2020 (Oral). Code available at\n  https://github.com/val-iisc/inheritune", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a tremendous progress in Domain Adaptation (DA) for visual\nrecognition tasks. Particularly, open-set DA has gained considerable attention\nwherein the target domain contains additional unseen categories. Existing\nopen-set DA approaches demand access to a labeled source dataset along with\nunlabeled target instances. However, this reliance on co-existing source and\ntarget data is highly impractical in scenarios where data-sharing is restricted\ndue to its proprietary nature or privacy concerns. Addressing this, we\nintroduce a practical DA paradigm where a source-trained model is used to\nfacilitate adaptation in the absence of the source dataset in future. To this\nend, we formalize knowledge inheritability as a novel concept and propose a\nsimple yet effective solution to realize inheritable models suitable for the\nabove practical paradigm. Further, we present an objective way to quantify\ninheritability to enable the selection of the most suitable source model for a\ngiven target domain, even in the absence of the source data. We provide\ntheoretical insights followed by a thorough empirical evaluation demonstrating\nstate-of-the-art open-set domain adaptation performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:16:30 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Venkat", "Naveen", ""], ["Revanur", "Ambareesh", ""], ["M", "Rahul", "V"], ["Babu", "R. Venkatesh", ""]]}, {"id": "2004.04393", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Naveen Venkat, Rahul M V, R. Venkatesh Babu", "title": "Universal Source-Free Domain Adaptation", "comments": "CVPR 2020. Code available at https://github.com/val-iisc/usfda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a strong incentive to develop versatile learning techniques that can\ntransfer the knowledge of class-separability from a labeled source domain to an\nunlabeled target domain in the presence of a domain-shift. Existing domain\nadaptation (DA) approaches are not equipped for practical DA scenarios as a\nresult of their reliance on the knowledge of source-target label-set\nrelationship (e.g. Closed-set, Open-set or Partial DA). Furthermore, almost all\nprior unsupervised DA works require coexistence of source and target samples\neven during deployment, making them unsuitable for real-time adaptation. Devoid\nof such impractical assumptions, we propose a novel two-stage learning process.\n1) In the Procurement stage, we aim to equip the model for future source-free\ndeployment, assuming no prior knowledge of the upcoming category-gap and\ndomain-shift. To achieve this, we enhance the model's ability to reject\nout-of-source distribution samples by leveraging the available source data, in\na novel generative classifier framework. 2) In the Deployment stage, the goal\nis to design a unified adaptation algorithm capable of operating across a wide\nrange of category-gaps, with no access to the previously seen source samples.\nTo this end, in contrast to the usage of complex adversarial training regimes,\nwe define a simple yet effective source-free adaptation objective by utilizing\na novel instance-level weighting mechanism, named as Source Similarity Metric\n(SSM). A thorough evaluation shows the practical usability of the proposed\nlearning framework with superior DA performance even over state-of-the-art\nsource-dependent approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:26:20 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Venkat", "Naveen", ""], ["M", "Rahul", "V"], ["Babu", "R. Venkatesh", ""]]}, {"id": "2004.04394", "submitter": "Kakeru Mitsuno", "authors": "Kakeru Mitsuno, Junichi Miyao and Takio Kurita", "title": "Hierarchical Group Sparse Regularization for Deep Convolutional Neural\n  Networks", "comments": "Accepted to IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a deep neural network (DNN), the number of the parameters is usually huge\nto get high learning performances. For that reason, it costs a lot of memory\nand substantial computational resources, and also causes overfitting. It is\nknown that some parameters are redundant and can be removed from the network\nwithout decreasing performance. Many sparse regularization criteria have been\nproposed to solve this problem. In a convolutional neural network (CNN), group\nsparse regularizations are often used to remove unnecessary subsets of the\nweights, such as filters or channels. When we apply a group sparse\nregularization for the weights connected to a neuron as a group, each\nconvolution filter is not treated as a target group in the regularization. In\nthis paper, we introduce the concept of hierarchical grouping to solve this\nproblem, and we propose several hierarchical group sparse regularization\ncriteria for CNNs. Our proposed the hierarchical group sparse regularization\ncan treat the weight for the input-neuron or the output-neuron as a group and\nconvolutional filter as a group in the same group to prune the unnecessary\nsubsets of weights. As a result, we can prune the weights more adequately\ndepending on the structure of the network and the number of channels keeping\nhigh performance. In the experiment, we investigate the effectiveness of the\nproposed sparse regularizations through intensive comparison experiments on\npublic datasets with several network architectures. Code is available on\nGitHub: \"https://github.com/K-Mitsuno/hierarchical-group-sparse-regularization\"\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:27:06 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Mitsuno", "Kakeru", ""], ["Miyao", "Junichi", ""], ["Kurita", "Takio", ""]]}, {"id": "2004.04396", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee and Junhee Seok", "title": "Score-Guided Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Generative Adversarial Network (GAN) that introduces an\nevaluator module using pre-trained networks. The proposed model, called\nscore-guided GAN (ScoreGAN), is trained with an evaluation metric for GANs,\ni.e., the Inception score, as a rough guide for the training of the generator.\nBy using another pre-trained network instead of the Inception network, ScoreGAN\ncircumvents the overfitting of the Inception network in order that generated\nsamples do not correspond to adversarial examples of the Inception network.\nAlso, to prevent the overfitting, the evaluation metrics are employed only as\nan auxiliary role, while the conventional target of GANs is mainly used.\nEvaluated with the CIFAR-10 dataset, ScoreGAN demonstrated an Inception score\nof 10.36$\\pm$0.15, which corresponds to state-of-the-art performance.\nFurthermore, to generalize the effectiveness of ScoreGAN, the model was further\nevaluated with another dataset, i.e., the CIFAR-100; as a result, ScoreGAN\noutperformed the other existing methods, where the Fr\\'echet Inception Distance\n(FID) was 13.98.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:32:43 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 04:27:26 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Lee", "Minhyeok", ""], ["Seok", "Junhee", ""]]}, {"id": "2004.04398", "submitter": "Da Li", "authors": "Da Li, Timothy Hospedales", "title": "Online Meta-Learning for Multi-Source and Semi-Supervised Domain\n  Adaptation", "comments": "ECCV 2020 CR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) is the topical problem of adapting models from\nlabelled source datasets so that they perform well on target datasets where\nonly unlabelled or partially labelled data is available. Many methods have been\nproposed to address this problem through different ways to minimise the domain\nshift between source and target datasets. In this paper we take an orthogonal\nperspective and propose a framework to further enhance performance by\nmeta-learning the initial conditions of existing DA algorithms. This is\nchallenging compared to the more widely considered setting of few-shot\nmeta-learning, due to the length of the computation graph involved. Therefore\nwe propose an online shortest-path meta-learning framework that is both\ncomputationally tractable and practically effective for improving DA\nperformance. We present variants for both multi-source unsupervised domain\nadaptation (MSDA), and semi-supervised domain adaptation (SSDA). Importantly,\nour approach is agnostic to the base adaptation algorithm, and can be applied\nto improve many techniques. Experimentally, we demonstrate improvements on\nclassic (DANN) and recent (MCD and MME) techniques for MSDA and SSDA, and\nultimately achieve state of the art results on several DA benchmarks including\nthe largest scale DomainNet.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:48:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 12:55:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Da", ""], ["Hospedales", "Timothy", ""]]}, {"id": "2004.04400", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Siddharth Seth, Varun Jampani, Mugalodi Rakesh,\n  R. Venkatesh Babu, Anirban Chakraborty", "title": "Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image\n  Synthesis", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera captured human pose is an outcome of several sources of variation.\nPerformance of supervised 3D pose estimation approaches comes at the cost of\ndispensing with variations, such as shape and appearance, that may be useful\nfor solving other related tasks. As a result, the learned model not only\ninculcates task-bias but also dataset-bias because of its strong reliance on\nthe annotated samples, which also holds true for weakly-supervised models.\nAcknowledging this, we propose a self-supervised learning framework to\ndisentangle such variations from unlabeled video frames. We leverage the prior\nknowledge on human skeleton and poses in the form of a single part-based 2D\npuppet model, human pose articulation constraints, and a set of unpaired 3D\nposes. Our differentiable formalization, bridging the representation gap\nbetween the 3D pose and spatial part maps, not only facilitates discovery of\ninterpretable pose disentanglement but also allows us to operate on videos with\ndiverse camera movements. Qualitative results on unseen in-the-wild datasets\nestablish our superior generalization across multiple tasks beyond the primary\ntasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate\nstate-of-the-art weakly-supervised 3D pose estimation performance on both\nHuman3.6M and MPI-INF-3DHP datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:55:01 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Seth", "Siddharth", ""], ["Jampani", "Varun", ""], ["Rakesh", "Mugalodi", ""], ["Babu", "R. Venkatesh", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2004.04432", "submitter": "Mizuho Nishio", "authors": "Mizuho Nishio, Sho Koyasu, Shunjiro Noguchi, Takao Kiguchi, Kanako\n  Nakatsu, Thai Akasaka, Hiroki Yamada, Kyo Itoh", "title": "Automatic detection of acute ischemic stroke using non-contrast computed\n  tomography and two-stage deep learning model", "comments": null, "journal-ref": "Computer Methods and Programs in Biomedicine 196 (2020) 105711", "doi": "10.1016/j.cmpb.2020.105711", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Purpose: We aimed to develop and evaluate an automatic acute\nischemic stroke-related (AIS) detection system involving a two-stage deep\nlearning model.\n  Methods: We included 238 cases from two different institutions. AIS-related\nfindings were annotated on each of the 238 sets of head CT images by referring\nto head magnetic resonance imaging (MRI) images in which an MRI examination was\nperformed within 24 h following the CT scan. These 238 annotated cases were\ndivided into a training set including 189 cases and test set including 49\ncases. Subsequently, a two-stage deep learning detection model was constructed\nfrom the training set using the You Only Look Once v3 model and Visual Geometry\nGroup 16 classification model. Then, the two-stage model performed the AIS\ndetection process in the test set. To assess the detection model's results, a\nboard-certified radiologist also evaluated the test set head CT images with and\nwithout the aid of the detection model. The sensitivity of AIS detection and\nnumber of false positives were calculated for the evaluation of the test set\ndetection results. The sensitivity of the radiologist with and without the\nsoftware detection results was compared using the McNemar test. A p-value of\nless than 0.05 was considered statistically significant.\n  Results: For the two-stage model and radiologist without and with the use of\nthe software results, the sensitivity was 37.3%, 33.3%, and 41.3%,\nrespectively, and the number of false positives per one case was 1.265, 0.327,\nand 0.388, respectively. On using the two-stage detection model's results, the\nboard-certified radiologist's detection sensitivity significantly improved\n(p-value = 0.0313).\n  Conclusions: Our detection system involving the two-stage deep learning model\nsignificantly improved the radiologist's sensitivity in AIS detection.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:14:24 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:56:01 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nishio", "Mizuho", ""], ["Koyasu", "Sho", ""], ["Noguchi", "Shunjiro", ""], ["Kiguchi", "Takao", ""], ["Nakatsu", "Kanako", ""], ["Akasaka", "Thai", ""], ["Yamada", "Hiroki", ""], ["Itoh", "Kyo", ""]]}, {"id": "2004.04433", "submitter": "Marcel B\\\"uhler", "authors": "Marcel C. B\\\"uhler, Andr\\'es Romero, Radu Timofte", "title": "DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution", "comments": "19 pages. Supplementary material is available on the project page.\n  Accepted for oral presentation at the 15th Asian Conference on Computer\n  Vision (ACCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) is by definition ill-posed. There are infinitely many\nplausible high-resolution variants for a given low-resolution natural image.\nMost of the current literature aims at a single deterministic solution of\neither high reconstruction fidelity or photo-realistic perceptual quality. In\nthis work, we propose an explorative facial super-resolution framework,\nDeepSEE, for Deep disentangled Semantic Explorative Extreme super-resolution.\nTo the best of our knowledge, DeepSEE is the first method to leverage semantic\nmaps for explorative super-resolution. In particular, it provides control of\nthe semantic regions, their disentangled appearance and it allows a broad range\nof image manipulations. We validate DeepSEE on faces, for up to 32x\nmagnification and exploration of the space of super-resolution. Our code and\nmodels are available at: https://mcbuehler.github.io/DeepSEE/\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:14:42 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 12:55:35 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 08:48:07 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["B\u00fchler", "Marcel C.", ""], ["Romero", "Andr\u00e9s", ""], ["Timofte", "Radu", ""]]}, {"id": "2004.04446", "submitter": "Yuqing Wang", "authors": "Yuqing Wang, Zhaoliang Xu, Hao Shen, Baoshan Cheng, Lirong Yang", "title": "CenterMask: single shot instance segmentation with point representation", "comments": "To appear at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a single-shot instance segmentation method, which\nis simple, fast and accurate. There are two main challenges for one-stage\ninstance segmentation: object instances differentiation and pixel-wise feature\nalignment. Accordingly, we decompose the instance segmentation into two\nparallel subtasks: Local Shape prediction that separates instances even in\noverlapping conditions, and Global Saliency generation that segments the whole\nimage in a pixel-to-pixel manner. The outputs of the two branches are assembled\nto form the final instance masks. To realize that, the local shape information\nis adopted from the representation of object center points. Totally trained\nfrom scratch and without any bells and whistles, the proposed CenterMask\nachieves 34.5 mask AP with a speed of 12.3 fps, using a single-model with\nsingle-scale training/testing on the challenging COCO dataset. The accuracy is\nhigher than all other one-stage instance segmentation methods except the 5\ntimes slower TensorMask, which shows the effectiveness of CenterMask. Besides,\nour method can be easily embedded to other one-stage object detectors such as\nFCOS and performs well, showing the generalization of CenterMask.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:35:15 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 05:12:10 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Wang", "Yuqing", ""], ["Xu", "Zhaoliang", ""], ["Shen", "Hao", ""], ["Cheng", "Baoshan", ""], ["Yang", "Lirong", ""]]}, {"id": "2004.04454", "submitter": "Toshinari Morimoto", "authors": "Toshinari Morimoto, Su-Yun Huang", "title": "TensorProjection Layer: A Tensor-Based Dimensionality Reduction Method\n  in CNN", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a dimensionality reduction method applied to\ntensor-structured data as a hidden layer (we call it TensorProjection Layer) in\na convolutional neural network. Our proposed method transforms input tensors\ninto ones with a smaller dimension by projection. The directions of projection\nare viewed as training parameters associated with our proposed layer and\ntrained via a supervised learning criterion such as minimization of the\ncross-entropy loss function. We discuss the gradients of the loss function with\nrespect to the parameters associated with our proposed layer. We also implement\nsimple numerical experiments to evaluate the performance of the\nTensorProjection Layer.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:52:49 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Morimoto", "Toshinari", ""], ["Huang", "Su-Yun", ""]]}, {"id": "2004.04455", "submitter": "Tiancheng Lin", "authors": "Tiancheng Lin, Yuanfan Guo, Canqian Yang, Jiancheng Yang and Yi Xu", "title": "Decoupled Gradient Harmonized Detector for Partial Annotation:\n  Application to Signet Ring Cell Detection", "comments": "accepted to Neurocomputing; 1st runner up of MICCAI DigestPath2019\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of signet ring cell carcinoma dramatically improves the\nsurvival rate of patients. Due to lack of public dataset and expert-level\nannotations, automatic detection on signet ring cell (SRC) has not been\nthoroughly investigated. In MICCAI DigestPath2019 challenge, apart from\nforeground (SRC region)-background (normal tissue area) class imbalance, SRCs\nare partially annotated due to costly medical image annotation, which\nintroduces extra label noise. To address the issues simultaneously, we propose\nDecoupled Gradient Harmonizing Mechanism (DGHM) and embed it into\nclassification loss, denoted as DGHM-C loss. Specifically, besides positive\n(SRCs) and negative (normal tissues) examples, we further decouple noisy\nexamples from clean examples and harmonize the corresponding gradient\ndistributions in classification respectively. Without whistles and bells, we\nachieved the 2nd place in the challenge. Ablation studies and controlled label\nmissing rate experiments demonstrate that DGHM-C loss can bring substantial\nimprovement in partially annotated object detection.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:53:11 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lin", "Tiancheng", ""], ["Guo", "Yuanfan", ""], ["Yang", "Canqian", ""], ["Yang", "Jiancheng", ""], ["Xu", "Yi", ""]]}, {"id": "2004.04462", "submitter": "Alexandre Boulch", "authors": "Alexandre Boulch, Gilles Puy, and Renaud Marlet", "title": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art methods for point cloud processing are based on the\nnotion of point convolution, for which several approaches have been proposed.\nIn this paper, inspired by discrete convolution in image processing, we provide\na formulation to relate and analyze a number of point convolution methods. We\nalso propose our own convolution variant, that separates the estimation of\ngeometry-less kernel weights and their alignment to the spatial support of\nfeatures. Additionally, we define a point sampling strategy for convolution\nthat is both effective and fast. Finally, using our convolution and sampling\nstrategy, we show competitive results on classification and semantic\nsegmentation benchmarks while being time and memory efficient.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 10:12:45 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 14:48:15 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 10:32:36 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Boulch", "Alexandre", ""], ["Puy", "Gilles", ""], ["Marlet", "Renaud", ""]]}, {"id": "2004.04467", "submitter": "Stanislav Pidhorskyi", "authors": "Stanislav Pidhorskyi, Donald Adjeroh, Gianfranco Doretto", "title": "Adversarial Latent Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoder networks are unsupervised approaches aiming at combining\ngenerative and representational properties by learning simultaneously an\nencoder-generator map. Although studied extensively, the issues of whether they\nhave the same generative power of GANs, or learn disentangled representations,\nhave not been fully addressed. We introduce an autoencoder that tackles these\nissues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a\ngeneral architecture that can leverage recent improvements on GAN training\nprocedures. We designed two autoencoders: one based on a MLP encoder, and\nanother based on a StyleGAN generator, which we call StyleALAE. We verify the\ndisentanglement properties of both architectures. We show that StyleALAE can\nnot only generate 1024x1024 face images with comparable quality of StyleGAN,\nbut at the same resolution can also produce face reconstructions and\nmanipulations based on real images. This makes ALAE the first autoencoder able\nto compare with, and go beyond the capabilities of a generator-only type of\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 10:33:44 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Pidhorskyi", "Stanislav", ""], ["Adjeroh", "Donald", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "2004.04485", "submitter": "Edgar Sucar", "authors": "Edgar Sucar, Kentaro Wada, and Andrew Davison", "title": "NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction", "comments": "to be published in 3DV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of scene representation is crucial in both the shape inference\nalgorithms it requires and the smart applications it enables. We present\nefficient and optimisable multi-class learned object descriptors together with\na novel probabilistic and differential rendering engine, for principled full\nobject shape inference from one or more RGB-D images. Our framework allows for\naccurate and robust 3D object reconstruction which enables multiple\napplications including robot grasping and placing, augmented reality, and the\nfirst object-level SLAM system capable of optimising object poses and shapes\njointly with camera trajectory.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:09:56 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 16:41:59 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sucar", "Edgar", ""], ["Wada", "Kentaro", ""], ["Davison", "Andrew", ""]]}, {"id": "2004.04491", "submitter": "Shidong Wang", "authors": "S. Wang, Y. Guan, L. Shao", "title": "Multi-Granularity Canonical Appearance Pooling for Remote Sensing Scene\n  Classification", "comments": "This paper is going to be published by IEEE Transactions on Image\n  Processing", "journal-ref": "IEEE Transactions on Image Processing 29, 5396--5407 (2020)", "doi": "10.1109/TIP.2020.2983560", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising remote sensing scene images remains challenging due to large\nvisual-semantic discrepancies. These mainly arise due to the lack of detailed\nannotations that can be employed to align pixel-level representations with\nhigh-level semantic labels. As the tagging process is labour-intensive and\nsubjective, we hereby propose a novel Multi-Granularity Canonical Appearance\nPooling (MG-CAP) to automatically capture the latent ontological structure of\nremote sensing datasets. We design a granular framework that allows\nprogressively cropping the input image to learn multi-grained features. For\neach specific granularity, we discover the canonical appearance from a set of\npre-defined transformations and learn the corresponding CNN features through a\nmaxout-based Siamese style architecture. Then, we replace the standard CNN\nfeatures with Gaussian covariance matrices and adopt the proper matrix\nnormalisations for improving the discriminative power of features. Besides, we\nprovide a stable solution for training the eigenvalue-decomposition function\n(EIG) in a GPU and demonstrate the corresponding back-propagation using matrix\ncalculus. Extensive experiments have shown that our framework can achieve\npromising results in public remote sensing scene datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 11:24:00 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Wang", "S.", ""], ["Guan", "Y.", ""], ["Shao", "L.", ""]]}, {"id": "2004.04504", "submitter": "Vitor Alexandre Campos Figueiredo V.A.C", "authors": "Vitor Alexandre Campos Figueiredo, Samuel Mafra and Joel Rodrigues", "title": "A Proposed IoT Smart Trap using Computer Vision for Sustainable Pest\n  Control in Coffee Culture", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is emerging as a multi-purpose technology with\nenormous potential for improving the quality of life in several areas. In\nparticular, IoT has been applied in agriculture to make it more sustainable\necologically. For instance, electronic traps have the potential to perform pest\ncontrol without any pesticide. In this paper, a smart trap with IoT\ncapabilities that uses computer vision to identify the insect of interest is\nproposed. The solution includes 1) an embedded system with camera, GPS sensor\nand motor actuators; 2) an IoT middleware as database service provider, and 3)\na Web application to present data by a configurable heat map. The demonstration\nof proposed solution is exposed and the main conclusions are the perception\nabout pest concentration at the plantation and the viability as alternative\npest control over traditional control based on pesticides.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:04:15 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Figueiredo", "Vitor Alexandre Campos", ""], ["Mafra", "Samuel", ""], ["Rodrigues", "Joel", ""]]}, {"id": "2004.04520", "submitter": "Jun Li", "authors": "Jun Li, Hongfu Liu, Zhiqiang Tao, Handong Zhao, and Yun Fu", "title": "Learnable Subspace Clustering", "comments": "IEEE Transactions on Neural Networks and Learning Systems (accepted\n  with minor revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the large-scale subspace clustering (LSSC) problem with\nmillion data points. Many popular subspace clustering methods cannot directly\nhandle the LSSC problem although they have been considered as state-of-the-art\nmethods for small-scale data points. A basic reason is that these methods often\nchoose all data points as a big dictionary to build huge coding models, which\nresults in a high time and space complexity. In this paper, we develop a\nlearnable subspace clustering paradigm to efficiently solve the LSSC problem.\nThe key idea is to learn a parametric function to partition the\nhigh-dimensional subspaces into their underlying low-dimensional subspaces\ninstead of the expensive costs of the classical coding models. Moreover, we\npropose a unified robust predictive coding machine (RPCM) to learn the\nparametric function, which can be solved by an alternating minimization\nalgorithm. In addition, we provide a bounded contraction analysis of the\nparametric function. To the best of our knowledge, this paper is the first work\nto efficiently cluster millions of data points among the subspace clustering\nmethods. Experiments on million-scale datasets verify that our paradigm\noutperforms the related state-of-the-art methods in both efficiency and\neffectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 12:53:28 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Li", "Jun", ""], ["Liu", "Hongfu", ""], ["Tao", "Zhiqiang", ""], ["Zhao", "Handong", ""], ["Fu", "Yun", ""]]}, {"id": "2004.04534", "submitter": "Lin-Zhuo Chen", "authors": "Lin-Zhuo Chen, Zheng Lin, Ziqin Wang, Yong-Liang Yang, and Ming-Ming\n  Cheng", "title": "Spatial Information Guided Convolution for Real-Time RGBD Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3049332", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D spatial information is known to be beneficial to the semantic segmentation\ntask. Most existing methods take 3D spatial data as an additional input,\nleading to a two-stream segmentation network that processes RGB and 3D spatial\ninformation separately. This solution greatly increases the inference time and\nseverely limits its scope for real-time applications. To solve this problem, we\npropose Spatial information guided Convolution (S-Conv), which allows efficient\nRGB feature and 3D spatial information integration. S-Conv is competent to\ninfer the sampling offset of the convolution kernel guided by the 3D spatial\ninformation, helping the convolutional layer adjust the receptive field and\nadapt to geometric transformations. S-Conv also incorporates geometric\ninformation into the feature learning process by generating spatially adaptive\nconvolutional weights. The capability of perceiving geometry is largely\nenhanced without much affecting the amount of parameters and computational\ncost. We further embed S-Conv into a semantic segmentation network, called\nSpatial information Guided convolutional Network (SGNet), resulting in\nreal-time inference and state-of-the-art performance on NYUDv2 and SUNRGBD\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 13:38:05 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 04:24:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Lin-Zhuo", ""], ["Lin", "Zheng", ""], ["Wang", "Ziqin", ""], ["Yang", "Yong-Liang", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2004.04546", "submitter": "Laetitia Teodorescu", "authors": "Laetitia Teodorescu, Katja Hofmann, and Pierre-Yves Oudeyer", "title": "SpatialSim: Recognizing Spatial Configurations of Objects with Graph\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing precise geometrical configurations of groups of objects is a key\ncapability of human spatial cognition, yet little studied in the deep learning\nliterature so far. In particular, a fundamental problem is how a machine can\nlearn and compare classes of geometric spatial configurations that are\ninvariant to the point of view of an external observer. In this paper we make\ntwo key contributions. First, we propose SpatialSim (Spatial Similarity), a\nnovel geometrical reasoning benchmark, and argue that progress on this\nbenchmark would pave the way towards a general solution to address this\nchallenge in the real world. This benchmark is composed of two tasks:\nIdentification and Comparison, each one instantiated in increasing levels of\ndifficulty. Secondly, we study how relational inductive biases exhibited by\nfully-connected message-passing Graph Neural Networks (MPGNNs) are useful to\nsolve those tasks, and show their advantages over less relational baselines\nsuch as Deep Sets and unstructured models such as Multi-Layer Perceptrons.\nFinally, we highlight the current limits of GNNs in these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 14:13:20 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 18:16:31 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Teodorescu", "Laetitia", ""], ["Hofmann", "Katja", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2004.04548", "submitter": "Phong Nguyen-Ha", "authors": "Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Janne Heikkila", "title": "Sequential View Synthesis with Transformer", "comments": "Code is available at: https://github.com/phongnhhn92/TransformerGQN;\n  Supplementary material: https://bit.ly/3kEgnzU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of novel view synthesis by means of neural\nrendering, where we are interested in predicting the novel view at an arbitrary\ncamera pose based on a given set of input images from other viewpoints. Using\nthe known query pose and input poses, we create an ordered set of observations\nthat leads to the target view. Thus, the problem of single novel view synthesis\nis reformulated as a sequential view prediction task. In this paper, the\nproposed Transformer-based Generative Query Network (T-GQN) extends the\nneural-rendering methods by adding two new concepts. First, we use multi-view\nattention learning between context images to obtain multiple implicit scene\nrepresentations. Second, we introduce a sequential rendering decoder to predict\nan image sequence, including the target view, based on the learned\nrepresentations. Finally, we evaluate our model on various challenging datasets\nand demonstrate that our model not only gives consistent predictions but also\ndoesn't require any retraining for finetuning.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 14:15:27 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 08:53:28 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Nguyen-Ha", "Phong", ""], ["Huynh", "Lam", ""], ["Rahtu", "Esa", ""], ["Heikkila", "Janne", ""]]}, {"id": "2004.04572", "submitter": "Tony Tung", "authors": "Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung", "title": "ARCH: Animatable Reconstruction of Clothed Humans", "comments": "10 pages, 10 figures, CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans),\na novel end-to-end framework for accurate reconstruction of animation-ready 3D\nclothed humans from a monocular image. Existing approaches to digitize 3D\nhumans struggle to handle pose variations and recover details. Also, they do\nnot produce models that are animation ready. In contrast, ARCH is a learned\npose-aware model that produces detailed 3D rigged full-body human avatars from\na single unconstrained RGB image. A Semantic Space and a Semantic Deformation\nField are created using a parametric 3D body estimator. They allow the\ntransformation of 2D/3D clothed humans into a canonical space, reducing\nambiguities in geometry caused by pose variations and occlusions in training\ndata. Detailed surface geometry and appearance are learned using an implicit\nfunction representation with spatial local features. Furthermore, we propose\nadditional per-pixel supervision on the 3D reconstruction using opacity-aware\ndifferentiable rendering. Our experiments indicate that ARCH increases the\nfidelity of the reconstructed humans. We obtain more than 50% lower\nreconstruction errors for standard metrics compared to state-of-the-art methods\non public datasets. We also show numerous qualitative examples of animated,\nhigh-quality reconstructed avatars unseen in the literature so far.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:23:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 19:14:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Huang", "Zeng", ""], ["Xu", "Yuanlu", ""], ["Lassner", "Christoph", ""], ["Li", "Hao", ""], ["Tung", "Tony", ""]]}, {"id": "2004.04581", "submitter": "Yude Wang", "authors": "Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen", "title": "Self-supervised Equivariant Attention Mechanism for Weakly Supervised\n  Semantic Segmentation", "comments": "To appear at CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-level weakly supervised semantic segmentation is a challenging problem\nthat has been deeply studied in recent years. Most of advanced solutions\nexploit class activation map (CAM). However, CAMs can hardly serve as the\nobject mask due to the gap between full and weak supervisions. In this paper,\nwe propose a self-supervised equivariant attention mechanism (SEAM) to discover\nadditional supervision and narrow the gap. Our method is based on the\nobservation that equivariance is an implicit constraint in fully supervised\nsemantic segmentation, whose pixel-level labels take the same spatial\ntransformation as the input images during data augmentation. However, this\nconstraint is lost on the CAMs trained by image-level supervision. Therefore,\nwe propose consistency regularization on predicted CAMs from various\ntransformed images to provide self-supervision for network learning. Moreover,\nwe propose a pixel correlation module (PCM), which exploits context appearance\ninformation and refines the prediction of current pixel by its similar\nneighbors, leading to further improvement on CAMs consistency. Extensive\nexperiments on PASCAL VOC 2012 dataset demonstrate our method outperforms\nstate-of-the-art methods using the same level of supervision. The code is\nreleased online.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 14:57:57 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Wang", "Yude", ""], ["Zhang", "Jie", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2004.04582", "submitter": "Md. Rezaul Karim", "authors": "Md. Rezaul Karim, Till D\\\"ohmen, Dietrich Rebholz-Schuhmann, Stefan\n  Decker, Michael Cochez, and Oya Beyan", "title": "DeepCOVIDExplainer: Explainable COVID-19 Diagnosis Based on Chest X-ray\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Amid the coronavirus disease(COVID-19) pandemic, humanity experiences a rapid\nincrease in infection numbers across the world. Challenge hospitals are faced\nwith, in the fight against the virus, is the effective screening of incoming\npatients. One methodology is the assessment of chest radiography(CXR) images,\nwhich usually requires expert radiologist's knowledge. In this paper, we\npropose an explainable deep neural networks(DNN)-based method for automatic\ndetection of COVID-19 symptoms from CXR images, which we call\nDeepCOVIDExplainer. We used 15,959 CXR images of 15,854 patients, covering\nnormal, pneumonia, and COVID-19 cases. CXR images are first comprehensively\npreprocessed, before being augmented and classified with a neural ensemble\nmethod, followed by highlighting class-discriminating regions using\ngradient-guided class activation maps(Grad-CAM++) and layer-wise relevance\npropagation(LRP). Further, we provide human-interpretable explanations of the\npredictions. Evaluation results based on hold-out data show that our approach\ncan identify COVID-19 confidently with a positive predictive value(PPV) of\n91.6%, 92.45%, and 96.12%; precision, recall, and F1 score of 94.6%, 94.3%, and\n94.6%, respectively for normal, pneumonia, and COVID-19 cases, respectively,\nmaking it comparable or improved results over recent approaches. We hope that\nour findings will be a useful contribution to the fight against COVID-19 and,\nin more general, towards an increasing acceptance and adoption of AI-assisted\napplications in the clinical practice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:03:58 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 10:25:47 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 20:31:13 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Karim", "Md. Rezaul", ""], ["D\u00f6hmen", "Till", ""], ["Rebholz-Schuhmann", "Dietrich", ""], ["Decker", "Stefan", ""], ["Cochez", "Michael", ""], ["Beyan", "Oya", ""]]}, {"id": "2004.04617", "submitter": "Lilla Zollei", "authors": "Jieyu Cheng, Adrian V. Dalca, Bruce Fischl, Lilla Zollei (for the\n  Alzheimer's Disease Neuroimaging Initiative)", "title": "Cortical surface registration using unsupervised learning", "comments": "cortical surface registration, deep network, unsupervised learning,\n  registration, deep learning, cortical, spherical, invertible", "journal-ref": "NeuroImage, Volume 221, 1 November 2020, 117161", "doi": "10.1016/j.neuroimage.2020.117161", "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-rigid cortical registration is an important and challenging task due to\nthe geometric complexity of the human cortex and the high degree of\ninter-subject variability. A conventional solution is to use a spherical\nrepresentation of surface properties and perform registration by aligning\ncortical folding patterns in that space. This strategy produces accurate\nspatial alignment but often requires a high computational cost. Recently,\nconvolutional neural networks (CNNs) have demonstrated the potential to\ndramatically speed up volumetric registration. However, due to distortions\nintroduced by projecting a sphere to a 2D plane, a direct application of recent\nlearning-based methods to surfaces yields poor results. In this study, we\npresent SphereMorph, a diffeomorphic registration framework for cortical\nsurfaces using deep networks that addresses these issues. SphereMorph uses a\nUNet-style network associated with a spherical kernel to learn the displacement\nfield and warps the sphere using a modified spatial transformer layer. We\npropose a resampling weight in computing the data fitting loss to account for\ndistortions introduced by polar projection, and demonstrate the performance of\nour proposed method on two tasks, including cortical parcellation and\ngroup-wise functional area alignment. The experiments show that the proposed\nSphereMorph is capable of modeling the geometric registration problem in a CNN\nframework and demonstrate superior registration accuracy and computational\nefficiency. The source code of SphereMorph will be released to the public upon\nacceptance of this manuscript at https://github.com/voxelmorph/spheremorph.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:59:13 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 09:06:55 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cheng", "Jieyu", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Dalca", "Adrian V.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Fischl", "Bruce", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Zollei", "Lilla", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "2004.04627", "submitter": "Xiao Song", "authors": "Xiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang, Jianping Shi", "title": "AdaStereo: A Simple and Efficient Approach for Adaptive Stereo Matching", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, records on stereo matching benchmarks are constantly broken by\nend-to-end disparity networks. However, the domain adaptation ability of these\ndeep models is quite poor. Addressing such problem, we present a novel\ndomain-adaptive pipeline called AdaStereo that aims to align multi-level\nrepresentations for deep stereo matching networks. Compared to previous methods\nfor adaptive stereo matching, our AdaStereo realizes a more standard, complete\nand effective domain adaptation pipeline. Firstly, we propose a non-adversarial\nprogressive color transfer algorithm for input image-level alignment. Secondly,\nwe design an efficient parameter-free cost normalization layer for internal\nfeature-level alignment. Lastly, a highly related auxiliary task,\nself-supervised occlusion-aware reconstruction is presented to narrow down the\ngaps in output space. Our AdaStereo models achieve state-of-the-art\ncross-domain performance on multiple stereo benchmarks, including KITTI,\nMiddlebury, ETH3D, and DrivingStereo, even outperforming disparity networks\nfinetuned with target-domain ground-truths.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:15:13 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 09:20:20 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 03:40:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Song", "Xiao", ""], ["Yang", "Guorun", ""], ["Zhu", "Xinge", ""], ["Zhou", "Hui", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""]]}, {"id": "2004.04630", "submitter": "Michael Strecke", "authors": "Michael Strecke and Joerg Stueckler", "title": "Where Does It End? -- Reasoning About Hidden Surfaces by Object\n  Intersection Constraints", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  2020, Project page: https://cosection.is.tue.mpg.de/, Source code:\n  https://github.com/EmbodiedVision/cosection", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00961", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scene understanding is an essential capability in robotics and VR/AR.\nIn this paper we propose Co-Section, an optimization-based approach to 3D\ndynamic scene reconstruction, which infers hidden shape information from\nintersection constraints. An object-level dynamic SLAM frontend detects,\nsegments, tracks and maps dynamic objects in the scene. Our optimization\nbackend completes the shapes using hull and intersection constraints between\nthe objects. In experiments, we demonstrate our approach on real and synthetic\ndynamic scene datasets. We also assess the shape completion performance of our\nmethod quantitatively. To the best of our knowledge, our approach is the first\nmethod to incorporate such physical plausibility constraints on object\nintersections for shape completion of dynamic objects in an energy minimization\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:18:02 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 17:32:38 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 15:26:40 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Strecke", "Michael", ""], ["Stueckler", "Joerg", ""]]}, {"id": "2004.04634", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Yingxue Pang, Yingce Xia, Zhibo Chen, Jiebo Luo", "title": "TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired\n  Images", "comments": "ECCV 2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised image-to-image translation (UI2I) task deals with learning a\nmapping between two domains without paired images. While existing UI2I methods\nusually require numerous unpaired images from different domains for training,\nthere are many scenarios where training data is quite limited. In this paper,\nwe argue that even if each domain contains a single image, UI2I can still be\nachieved. To this end, we propose TuiGAN, a generative model that is trained on\nonly two unpaired images and amounts to one-shot unsupervised learning. With\nTuiGAN, an image is translated in a coarse-to-fine manner where the generated\nimage is gradually refined from global structures to local details. We conduct\nextensive experiments to verify that our versatile method can outperform strong\nbaselines on a wide variety of UI2I tasks. Moreover, TuiGAN is capable of\nachieving comparable performance with the state-of-the-art UI2I models trained\nwith sufficient data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:23:59 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 02:23:03 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Lin", "Jianxin", ""], ["Pang", "Yingxue", ""], ["Xia", "Yingce", ""], ["Chen", "Zhibo", ""], ["Luo", "Jiebo", ""]]}, {"id": "2004.04641", "submitter": "Alireza Ghaffari", "authors": "Alireza Ghaffari, Yvon Savaria", "title": "CNN2Gate: Toward Designing a General Framework for Implementation of\n  Convolutional Neural Networks on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have a major impact on our society\nbecause of the numerous services they provide. On the other hand, they require\nconsiderable computing power. To satisfy these requirements, it is possible to\nuse graphic processing units (GPUs). However, high power consumption and\nlimited external IOs constrain their usability and suitability in industrial\nand mission-critical scenarios. Recently, the number of researches that utilize\nFPGAs to implement CNNs are increasing rapidly. This is due to the lower power\nconsumption and easy reconfigurability offered by these platforms. Because of\nthe research efforts put into topics such as architecture, synthesis and\noptimization, some new challenges are arising to integrate such hardware\nsolutions to high-level machine learning software libraries. This paper\nintroduces an integrated framework (CNN2Gate) that supports compilation of a\nCNN model for an FPGA target. CNN2Gate exploits the OpenCL synthesis workflow\nfor FPGAs offered by commercial vendors. CNN2Gate is capable of parsing CNN\nmodels from several popular high-level machine learning libraries such as\nKeras, Pytorch, Caffe2 etc. CNN2Gate extracts computation flow of layers, in\naddition to weights and biases and applies a \"given\" fixed-point quantization.\nFurthermore, it writes this information in the proper format for OpenCL\nsynthesis tools that are then used to build and run the project on FPGA.\nCNN2Gate performs design-space exploration using a reinforcement learning agent\nand fits the design on different FPGAs with limited logic resources\nautomatically. This paper reports results of automatic synthesis and\ndesign-space exploration of AlexNet and VGG-16 on various Intel FPGA platforms.\nCNN2Gate achieves a latency of 205 ms for VGG-16 and 18 ms for AlexNet on the\nFPGA.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 01:57:53 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 00:59:40 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Ghaffari", "Alireza", ""], ["Savaria", "Yvon", ""]]}, {"id": "2004.04668", "submitter": "Neerav Karani", "authors": "Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu", "title": "Test-Time Adaptable Neural Networks for Robust Medical Image\n  Segmentation", "comments": "Published in Medical Image Analysis journal:\n  https://doi.org/10.1016/j.media.2020.101907", "journal-ref": "Medical Image Analysis, Volume 68, 2021, 101907, ISSN 1361-8415.\n  http://www.sciencedirect.com/science/article/pii/S1361841520302711", "doi": "10.1016/j.media.2020.101907", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) work very well for supervised learning\nproblems when the training dataset is representative of the variations expected\nto be encountered at test time. In medical image segmentation, this premise is\nviolated when there is a mismatch between training and test images in terms of\ntheir acquisition details, such as the scanner model or the protocol.\nRemarkable performance degradation of CNNs in this scenario is well documented\nin the literature. To address this problem, we design the segmentation CNN as a\nconcatenation of two sub-networks: a relatively shallow image normalization\nCNN, followed by a deep CNN that segments the normalized image. We train both\nthese sub-networks using a training dataset, consisting of annotated images\nfrom a particular scanner and protocol setting. Now, at test time, we adapt the\nimage normalization sub-network for \\emph{each test image}, guided by an\nimplicit prior on the predicted segmentation labels. We employ an independently\ntrained denoising autoencoder (DAE) in order to model such an implicit prior on\nplausible anatomical segmentation labels. We validate the proposed idea on\nmulti-center Magnetic Resonance imaging datasets of three anatomies: brain,\nheart and prostate. The proposed test-time adaptation consistently provides\nperformance improvement, demonstrating the promise and generality of the\napproach. Being agnostic to the architecture of the deep CNN, the second\nsub-network, the proposed design can be utilized with any segmentation network\nto increase robustness to variations in imaging scanners and protocols. Our\ncode is available at:\n\\url{https://github.com/neerakara/test-time-adaptable-neural-networks-for-domain-generalization}.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:57:27 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 11:01:39 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 12:07:31 GMT"}, {"version": "v4", "created": "Sat, 23 Jan 2021 16:14:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Karani", "Neerav", ""], ["Erdil", "Ertunc", ""], ["Chaitanya", "Krishna", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2004.04674", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Milad Sikaroudi, Sobhan Shafiei, H.R. Tizhoosh,\n  Fakhri Karray, Mark Crowley", "title": "Fisher Discriminant Triplet and Contrastive Losses for Training Siamese\n  Networks", "comments": "Accepted (to appear) in International Joint Conference on Neural\n  Networks (IJCNN) 2020, IEEE, in IEEE World Congress on Computational\n  Intelligence (WCCI) 2020", "journal-ref": "International Joint Conference on Neural Networks (IJCNN), IEEE,\n  2020", "doi": "10.1109/IJCNN48605.2020.9206833", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese neural network is a very powerful architecture for both feature\nextraction and metric learning. It usually consists of several networks that\nshare weights. The Siamese concept is topology-agnostic and can use any neural\nnetwork as its backbone. The two most popular loss functions for training these\nnetworks are the triplet and contrastive loss functions. In this paper, we\npropose two novel loss functions, named Fisher Discriminant Triplet (FDT) and\nFisher Discriminant Contrastive (FDC). The former uses anchor-neighbor-distant\ntriplets while the latter utilizes pairs of anchor-neighbor and anchor-distant\nsamples. The FDT and FDC loss functions are designed based on the statistical\nformulation of the Fisher Discriminant Analysis (FDA), which is a linear\nsubspace learning method. Our experiments on the MNIST and two challenging and\npublicly available histopathology datasets show the effectiveness of the\nproposed loss functions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:27:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Sikaroudi", "Milad", ""], ["Shafiei", "Sobhan", ""], ["Tizhoosh", "H. R.", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2004.04690", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Rongmei Lin, Zhen Liu, James M. Rehg, Liam Paull, Li\n  Xiong, Le Song, Adrian Weller", "title": "Orthogonal Over-Parameterized Training", "comments": "CVPR 2021 Oral (43 Pages, Substantial Update from v3, Typos Fixed\n  from v5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inductive bias of a neural network is largely determined by the\narchitecture and the training algorithm. To achieve good generalization, how to\neffectively train a neural network is of great importance. We propose a novel\northogonal over-parameterized training (OPT) framework that can provably\nminimize the hyperspherical energy which characterizes the diversity of neurons\non a hypersphere. By maintaining the minimum hyperspherical energy during\ntraining, OPT can greatly improve the empirical generalization. Specifically,\nOPT fixes the randomly initialized weights of the neurons and learns an\northogonal transformation that applies to these neurons. We consider multiple\nways to learn such an orthogonal transformation, including unrolling\northogonalization algorithms, applying orthogonal parameterization, and\ndesigning orthogonality-preserving gradient descent. For better scalability, we\npropose the stochastic OPT which performs orthogonal transformation\nstochastically for partial dimensions of neurons. Interestingly, OPT reveals\nthat learning a proper coordinate system for neurons is crucial to\ngeneralization. We provide some insights on why OPT yields better\ngeneralization. Extensive experiments validate the superiority of OPT over the\nstandard training.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:16:38 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 20:22:30 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 06:18:55 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 11:31:31 GMT"}, {"version": "v5", "created": "Wed, 31 Mar 2021 01:07:38 GMT"}, {"version": "v6", "created": "Sat, 5 Jun 2021 00:31:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Weiyang", ""], ["Lin", "Rongmei", ""], ["Liu", "Zhen", ""], ["Rehg", "James M.", ""], ["Paull", "Liam", ""], ["Xiong", "Li", ""], ["Song", "Le", ""], ["Weller", "Adrian", ""]]}, {"id": "2004.04692", "submitter": "Yiming Li", "authors": "Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao\n  Xia", "title": "Rethinking the Trigger of Backdoor Attack", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attack intends to inject hidden backdoor into the deep neural\nnetworks (DNNs), such that the prediction of the infected model will be\nmaliciously changed if the hidden backdoor is activated by the attacker-defined\ntrigger, while it performs well on benign samples. Currently, most of existing\nbackdoor attacks adopted the setting of \\emph{static} trigger, $i.e.,$ triggers\nacross the training and testing images follow the same appearance and are\nlocated in the same area. In this paper, we revisit this attack paradigm by\nanalyzing the characteristics of the static trigger. We demonstrate that such\nan attack paradigm is vulnerable when the trigger in testing images is not\nconsistent with the one used for training. We further explore how to utilize\nthis property for backdoor defense, and discuss how to alleviate such\nvulnerability of existing attacks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:19:37 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 10:22:58 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 17:25:49 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Li", "Yiming", ""], ["Zhai", "Tongqing", ""], ["Wu", "Baoyuan", ""], ["Jiang", "Yong", ""], ["Li", "Zhifeng", ""], ["Xia", "Shutao", ""]]}, {"id": "2004.04697", "submitter": "Travis Manderson", "authors": "Travis Manderson, Stefan Wapnick, David Meger, and Gregory Dudek", "title": "Learning to Drive Off Road on Smooth Terrain in Unstructured\n  Environments Using an On-Board Camera and Sparse Aerial Images", "comments": "ICRA 2020. Video and project details can be found at\n  http://www.cim.mcgill.ca/mrl/offroad_driving/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning to drive on smooth terrain while\nsimultaneously avoiding collisions in challenging off-road and unstructured\noutdoor environments using only visual inputs. Our approach applies a hybrid\nmodel-based and model-free reinforcement learning method that is entirely\nself-supervised in labeling terrain roughness and collisions using on-board\nsensors. Notably, we provide both first-person and overhead aerial image inputs\nto our model. We find that the fusion of these complementary inputs improves\nplanning foresight and makes the model robust to visual obstructions. Our\nresults show the ability to generalize to environments with plentiful\nvegetation, various types of rock, and sandy trails. During evaluation, our\npolicy attained 90% smooth terrain traversal and reduced the proportion of\nrough terrain driven over by 6.1 times compared to a model using only\nfirst-person imagery.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:27:09 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Manderson", "Travis", ""], ["Wapnick", "Stefan", ""], ["Meger", "David", ""], ["Dudek", "Gregory", ""]]}, {"id": "2004.04699", "submitter": "Jose M. Alvarez", "authors": "Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan Ivanecky, Hanson\n  Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet, Jose M.\n  Alvarez", "title": "Scalable Active Learning for Object Detection", "comments": "accepted at IEEE-IV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks trained in a fully supervised fashion are the dominant\ntechnology in perception-based autonomous driving systems. While collecting\nlarge amounts of unlabeled data is already a major undertaking, only a subset\nof it can be labeled by humans due to the effort needed for high-quality\nannotation. Therefore, finding the right data to label has become a key\nchallenge. Active learning is a powerful technique to improve data efficiency\nfor supervised learning methods, as it aims at selecting the smallest possible\ntraining set to reach a required performance. We have built a scalable\nproduction system for active learning in the domain of autonomous driving. In\nthis paper, we describe the resulting high-level design, sketch some of the\nchallenges and their solutions, present our current results at scale, and\nbriefly describe the open problems and future directions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:28:56 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Haussmann", "Elmar", ""], ["Fenzi", "Michele", ""], ["Chitta", "Kashyap", ""], ["Ivanecky", "Jan", ""], ["Xu", "Hanson", ""], ["Roy", "Donna", ""], ["Mittel", "Akshita", ""], ["Koumchatzky", "Nicolas", ""], ["Farabet", "Clement", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "2004.04725", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee,\n  Alexander G. Schwing, Jan Kautz", "title": "Instance-aware, Context-focused, and Memory-efficient Weakly Supervised\n  Object Detection", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised learning has emerged as a compelling tool for object\ndetection by reducing the need for strong supervision during training. However,\nmajor challenges remain: (1) differentiation of object instances can be\nambiguous; (2) detectors tend to focus on discriminative parts rather than\nentire objects; (3) without ground truth, object proposals have to be redundant\nfor high recalls, causing significant memory consumption. Addressing these\nchallenges is difficult, as it often requires to eliminate uncertainties and\ntrivial solutions. To target these issues we develop an instance-aware and\ncontext-focused unified framework. It employs an instance-aware self-training\nalgorithm and a learnable Concrete DropBlock while devising a memory-efficient\nsequential batch back-propagation. Our proposed method achieves\nstate-of-the-art results on COCO ($12.1\\% ~AP$, $24.8\\% ~AP_{50}$), VOC 2007\n($54.9\\% ~AP$), and VOC 2012 ($52.1\\% ~AP$), improving baselines by great\nmargins. In addition, the proposed method is the first to benchmark ResNet\nbased models and weakly supervised video object detection. Code, models, and\nmore details will be made available at: https://github.com/NVlabs/wetectron.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:57:09 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 16:26:07 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 07:32:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Yu", "Zhiding", ""], ["Yang", "Xiaodong", ""], ["Liu", "Ming-Yu", ""], ["Lee", "Yong Jae", ""], ["Schwing", "Alexander G.", ""], ["Kautz", "Jan", ""]]}, {"id": "2004.04727", "submitter": "Jia-Bin Huang", "authors": "Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang", "title": "3D Photography using Context-aware Layered Depth Inpainting", "comments": "CVPR 2020. Project page:\n  https://shihmengli.github.io/3D-Photo-Inpainting/ Code:\n  https://github.com/vt-vl-lab/3d-photo-inpainting Demo:\n  https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for converting a single RGB-D input image into a 3D photo\n- a multi-layer representation for novel view synthesis that contains\nhallucinated color and depth structures in regions occluded in the original\nview. We use a Layered Depth Image with explicit pixel connectivity as\nunderlying representation, and present a learning-based inpainting model that\nsynthesizes new local color-and-depth content into the occluded region in a\nspatial context-aware manner. The resulting 3D photos can be efficiently\nrendered with motion parallax using standard graphics engines. We validate the\neffectiveness of our method on a wide range of challenging everyday scenes and\nshow fewer artifacts compared with the state of the arts.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:59:06 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 02:19:03 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 14:21:03 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Shih", "Meng-Li", ""], ["Su", "Shih-Yang", ""], ["Kopf", "Johannes", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2004.04730", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer", "title": "X3D: Expanding Architectures for Efficient Video Recognition", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents X3D, a family of efficient video networks that\nprogressively expand a tiny 2D image classification architecture along multiple\nnetwork axes, in space, time, width and depth. Inspired by feature selection\nmethods in machine learning, a simple stepwise network expansion approach is\nemployed that expands a single axis in each step, such that good accuracy to\ncomplexity trade-off is achieved. To expand X3D to a specific target\ncomplexity, we perform progressive forward expansion followed by backward\ncontraction. X3D achieves state-of-the-art performance while requiring 4.8x and\n5.5x fewer multiply-adds and parameters for similar accuracy as previous work.\nOur most surprising finding is that networks with high spatiotemporal\nresolution can perform well, while being extremely light in terms of network\nwidth and parameters. We report competitive accuracy at unprecedented\nefficiency on video classification and detection benchmarks. Code will be\navailable at: https://github.com/facebookresearch/SlowFast\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:59:47 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Feichtenhofer", "Christoph", ""]]}, {"id": "2004.04736", "submitter": "Rodney LaLonde III", "authors": "Rodney LaLonde, Ziyue Xu, Ismail Irmakci, Sanjay Jain, Ulas Bagci", "title": "Capsules for Biomedical Image Segmentation", "comments": "Extension of the non-archival Capsules of Object Segmentation with\n  experiments on both clinical and pre-clinical pathological lung segmentation\n  from CT scans and muscular and adipose tissue segmentation from MR images.\n  Accepted for publication in Medical Image Analysis. DOI:\n  https://doi.org/10.1016/j.media.2020.101889. arXiv admin note: text overlap\n  with arXiv:1804.04241", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work expands the use of capsule networks to the task of object\nsegmentation for the first time in the literature. This is made possible via\nthe introduction of locally-constrained routing and transformation matrix\nsharing, which reduces the parameter/memory burden and allows for the\nsegmentation of objects at large resolutions. To compensate for the loss of\nglobal information in constraining the routing, we propose the concept of\n\"deconvolutional\" capsules to create a deep encoder-decoder style network,\ncalled SegCaps. We extend the masked reconstruction regularization to the task\nof segmentation and perform thorough ablation experiments on each component of\nour method. The proposed convolutional-deconvolutional capsule network,\nSegCaps, shows state-of-the-art results while using a fraction of the\nparameters of popular segmentation networks. To validate our proposed method,\nwe perform experiments segmenting pathological lungs from clinical and\npre-clinical thoracic computed tomography (CT) scans and segmenting muscle and\nadipose (fat) tissue from magnetic resonance imaging (MRI) scans of human\nsubjects' thighs. Notably, our experiments in lung segmentation represent the\nlargest-scale study in pathological lung segmentation in the literature, where\nwe conduct experiments across five extremely challenging datasets, containing\nboth clinical and pre-clinical subjects, and nearly 2000 computed-tomography\nscans. Our newly developed segmentation platform outperforms other methods\nacross all datasets while utilizing less than 5% of the parameters in the\npopular U-Net for biomedical image segmentation. Further, we demonstrate\ncapsules' ability to generalize to unseen rotations/reflections on natural\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 03:01:31 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 21:53:16 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["LaLonde", "Rodney", ""], ["Xu", "Ziyue", ""], ["Irmakci", "Ismail", ""], ["Jain", "Sanjay", ""], ["Bagci", "Ulas", ""]]}, {"id": "2004.04775", "submitter": "Habiba Saim", "authors": "M. Hammad Masood, Habiba Saim, Murtaza Taj, Mian M. Awais", "title": "Early Disease Diagnosis for Rice Crop", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing techniques provide automatic estimation of crop damage due to\nvarious diseases. However, early detection can prevent or reduce the extend of\ndamage itself. The limited performance of existing techniques in early\ndetection is lack of localized information. We instead propose a dataset with\nannotations for each diseased segment in each image. Unlike existing\napproaches, instead of classifying images into either healthy or diseased, we\npropose to provide localized classification for each segment of an images. Our\nmethod is based on Mask RCNN and provides location as well as extend of\ninfected regions on the plant. Thus the extend of damage on the crop can be\nestimated. Our method has obtained overall 87.6% accuracy on the proposed\ndataset as compared to 58.4% obtained without incorporating localized\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 19:05:43 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Masood", "M. Hammad", ""], ["Saim", "Habiba", ""], ["Taj", "Murtaza", ""], ["Awais", "Mian M.", ""]]}, {"id": "2004.04788", "submitter": "Bekir Z Demiray", "authors": "Bekir Z Demiray, Muhammed Sit, Ibrahim Demir", "title": "D-SRGAN: DEM Super-Resolution with Generative Adversarial Networks", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIDAR (light detection and ranging) is an optical remote-sensing technique\nthat measures the distance between sensor and object, and the reflected energy\nfrom the object. Over the years, LIDAR data has been used as the primary source\nof Digital Elevation Models (DEMs). DEMs have been used in a variety of\napplications like road extraction, hydrological modeling, flood mapping, and\nsurface analysis. A number of studies in flooding suggest the usage of\nhigh-resolution DEMs as inputs in the applications improve the overall\nreliability and accuracy. Despite the importance of high-resolution DEM, many\nareas in the United States and the world do not have access to high-resolution\nDEM due to technological limitations or the cost of the data collection. With\nrecent development in Graphical Processing Units (GPU) and novel algorithms,\ndeep learning techniques have become attractive to researchers for their\nperformance in learning features from high-resolution datasets. Numerous new\nmethods have been proposed such as Generative Adversarial Networks (GANs) to\ncreate intelligent models that correct and augment large-scale datasets. In\nthis paper, a GAN based model is developed and evaluated, inspired by single\nimage super-resolution methods, to increase the spatial resolution of a given\nDEM dataset up to 4 times without additional information related to data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 19:57:49 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 17:42:46 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Demiray", "Bekir Z", ""], ["Sit", "Muhammed", ""], ["Demir", "Ibrahim", ""]]}, {"id": "2004.04795", "submitter": "Sajad Norouzi", "authors": "Sajad Norouzi, David J. Fleet, Mohammad Norouzi", "title": "Exemplar VAE: Linking Generative Models, Nearest Neighbor Retrieval, and\n  Data Augmentation", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Exemplar VAEs, a family of generative models that bridge the gap\nbetween parametric and non-parametric, exemplar based generative models.\nExemplar VAE is a variant of VAE with a non-parametric prior in the latent\nspace based on a Parzen window estimator. To sample from it, one first draws a\nrandom exemplar from a training set, then stochastically transforms that\nexemplar into a latent code and a new observation. We propose retrieval\naugmented training (RAT) as a way to speed up Exemplar VAE training by using\napproximate nearest neighbor search in the latent space to define a lower bound\non log marginal likelihood. To enhance generalization, model parameters are\nlearned using exemplar leave-one-out and subsampling. Experiments demonstrate\nthe effectiveness of Exemplar VAEs on density estimation and representation\nlearning. Importantly, generative data augmentation using Exemplar VAEs on\npermutation invariant MNIST and Fashion MNIST reduces classification error from\n1.17% to 0.69% and from 8.56% to 8.16%.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:21:45 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 21:06:22 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 18:51:11 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Norouzi", "Sajad", ""], ["Fleet", "David J.", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2004.04807", "submitter": "Tolga Birdal", "authors": "Mai Bui and Tolga Birdal and Haowen Deng and Shadi Albarqouni and\n  Leonidas Guibas and Slobodan Ilic and Nassir Navab", "title": "6D Camera Relocalization in Ambiguous Scenes via Continuous Multimodal\n  Inference", "comments": "Accepted for publication at ECCV 2020. Project page under\n  https://multimodal3dvision.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multimodal camera relocalization framework that captures\nambiguities and uncertainties with continuous mixture models defined on the\nmanifold of camera poses. In highly ambiguous environments, which can easily\narise due to symmetries and repetitive structures in the scene, computing one\nplausible solution (what most state-of-the-art methods currently regress) may\nnot be sufficient. Instead we predict multiple camera pose hypotheses as well\nas the respective uncertainty for each prediction. Towards this aim, we use\nBingham distributions, to model the orientation of the camera pose, and a\nmultivariate Gaussian to model the position, with an end-to-end deep neural\nnetwork. By incorporating a Winner-Takes-All training scheme, we finally obtain\na mixture model that is well suited for explaining ambiguities in the scene,\nyet does not suffer from mode collapse, a common problem with mixture density\nnetworks. We introduce a new dataset specifically designed to foster camera\nlocalization research in ambiguous environments and exhaustively evaluate our\nmethod on synthetic as well as real data on both ambiguous scenes and on\nnon-ambiguous benchmark datasets. We plan to release our code and dataset under\n$\\href{https://multimodal3dvision.github.io}{multimodal3dvision.github.io}$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:55:06 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 07:06:27 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bui", "Mai", ""], ["Birdal", "Tolga", ""], ["Deng", "Haowen", ""], ["Albarqouni", "Shadi", ""], ["Guibas", "Leonidas", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "2004.04822", "submitter": "Jiachen Xu", "authors": "Zheng Nie, Jiachen Xu, Shengchang Zhang", "title": "Analysis on DeepLabV3+ Performance for Automatic Steel Defects Detection", "comments": "10 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our works experimented DeepLabV3+ with different backbones on a large volume\nof steel images aiming to automatically detect different types of steel\ndefects. Our methods applied random weighted augmentation to balance different\ndefects types in the training set. And then applied DeeplabV3+ model three\ndifferent backbones, ResNet, DenseNet and EfficientNet, on segmenting defection\nregions on the steel images. Based on experiments, we found that applying\nResNet101 or EfficientNet as backbones could reach the best IoU scores on the\ntest set, which is around 0.57, comparing with 0.325 for using DenseNet. Also,\nDeepLabV3+ model with ResNet101 as backbone has the fewest training time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 21:17:01 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 03:50:16 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Nie", "Zheng", ""], ["Xu", "Jiachen", ""], ["Zhang", "Shengchang", ""]]}, {"id": "2004.04851", "submitter": "Ankan Bansal", "authors": "Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama\n  Chellappa", "title": "Spatial Priming for Detecting Human-Object Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative spatial layout of a human and an object is an important cue for\ndetermining how they interact. However, until now, spatial layout has been used\njust as side-information for detecting human-object interactions (HOIs). In\nthis paper, we present a method for exploiting this spatial layout information\nfor detecting HOIs in images. The proposed method consists of a layout module\nwhich primes a visual module to predict the type of interaction between a human\nand an object. The visual and layout modules share information through lateral\nconnections at several stages. The model uses predictions from the layout\nmodule as a prior to the visual module and the prediction from the visual\nmodule is given as the final output. It also incorporates semantic information\nabout the object using word2vec vectors. The proposed model reaches an mAP of\n24.79% for HICO-Det dataset which is about 2.8% absolute points higher than the\ncurrent state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:20:30 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Bansal", "Ankan", ""], ["Rambhatla", "Sai Saketh", ""], ["Shrivastava", "Abhinav", ""], ["Chellappa", "Rama", ""]]}, {"id": "2004.04866", "submitter": "Mart\\'in Palazzo", "authors": "Martin Palazzo, Patricio Yankilevich, Pierre Beauseroy", "title": "Latent regularization for feature selection using kernel methods in\n  tumor classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transcriptomics of cancer tumors are characterized with tens of thousands\nof gene expression features. Patient prognosis or tumor stage can be assessed\nby machine learning techniques like supervised classification tasks given a\ngene expression profile. Feature selection is a useful approach to select the\nkey genes which helps to classify tumors. In this work we propose a feature\nselection method based on Multiple Kernel Learning that results in a reduced\nsubset of genes and a custom kernel that improves the classification\nperformance when used in support vector classification. During the feature\nselection process this method performs a novel latent regularisation by\nrelaxing the supervised target problem by introducing unsupervised structure\nobtained from the latent space learned by a non linear dimensionality reduction\nmodel. An improvement of the generalization capacity is obtained and assessed\nby the tumor classification performance on new unseen test samples when the\nclassifier is trained with the features selected by the proposed method in\ncomparison with other supervised feature selection approaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 00:46:02 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Palazzo", "Martin", ""], ["Yankilevich", "Patricio", ""], ["Beauseroy", "Pierre", ""]]}, {"id": "2004.04871", "submitter": "Satish Viswanath", "authors": "Amir Reza Sadri, Andrew Janowczyk, Ren Zou, Ruchika Verma, Niha Beig,\n  Jacob Antunes, Anant Madabhushi, Pallavi Tiwari, Satish E. Viswanath", "title": "MRQy: An Open-Source Tool for Quality Control of MR Imaging Data", "comments": "28 pages, 7 figures. Submitted to Medical Physics", "journal-ref": null, "doi": "10.1002/mp.14593", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We sought to develop a quantitative tool to quickly determine relative\ndifferences in MRI volumes both within and between large MR imaging cohorts\n(such as available in The Cancer Imaging Archive (TCIA)), in order to help\ndetermine the generalizability of radiomics and machine learning schemes to\nunseen datasets. The tool is intended to help quantify presence of (a) site- or\nscanner-specific variations in image resolution, field-of-view, or image\ncontrast, or (b) imaging artifacts such as noise, motion, inhomogeneity,\nringing, or aliasing; which can adversely affect relative image quality between\ndata cohorts. We present MRQy, a new open-source quality control tool to (a)\ninterrogate MRI cohorts for site- or equipment-based differences, and (b)\nquantify the impact of MRI artifacts on relative image quality; to help\ndetermine how to correct for these variations prior to model development. MRQy\nextracts a series of quality measures (e.g. noise ratios, variation metrics,\nentropy and energy criteria) and MR image metadata (e.g. voxel resolution,\nimage dimensions) for subsequent interrogation via a specialized HTML5 based\nfront-end designed for real-time filtering and trend visualization. MRQy was\nused to evaluate (a) n=133 brain MRIs from TCIA (7 sites), and (b) n=104 rectal\nMRIs (3 local sites). MRQy measures revealed significant site-specific\nvariations in both cohorts, indicating potential batch effects. Marked\ndifferences in specific MRQy measures were also able to identify outlier MRI\ndatasets that needed to be corrected for common MR imaging artifacts. MRQy is\ndesigned to be a standalone, unsupervised tool that can be efficiently run on a\nstandard desktop computer. It has been made freely accessible at\n\\url{http://github.com/ccipd/MRQy} for wider community use and feedback.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 01:30:51 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 17:42:08 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 14:04:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sadri", "Amir Reza", ""], ["Janowczyk", "Andrew", ""], ["Zou", "Ren", ""], ["Verma", "Ruchika", ""], ["Beig", "Niha", ""], ["Antunes", "Jacob", ""], ["Madabhushi", "Anant", ""], ["Tiwari", "Pallavi", ""], ["Viswanath", "Satish E.", ""]]}, {"id": "2004.04907", "submitter": "M\\'arton Karsai", "authors": "Jacob Levy Abitbol and M\\'arton Karsai", "title": "Socioeconomic correlations of urban patterns inferred from aerial\n  images: interpreting activation maps of Convolutional Neural Networks", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urbanisation is a great challenge for modern societies, promising better\naccess to economic opportunities while widening socioeconomic inequalities.\nAccurately tracking how this process unfolds has been challenging for\ntraditional data collection methods, while remote sensing information offers an\nalternative to gather a more complete view on these societal changes. By\nfeeding a neural network with satellite images one may recover the\nsocioeconomic information associated to that area, however these models lack to\nexplain how visual features contained in a sample, trigger a given prediction.\nHere we close this gap by predicting socioeconomic status across France from\naerial images and interpreting class activation mappings in terms of urban\ntopology. We show that the model disregards the spatial correlations existing\nbetween urban class and socioeconomic status to derive its predictions. These\nresults pave the way to build interpretable models, which may help to better\ntrack and understand urbanisation and its consequences.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 04:57:20 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Abitbol", "Jacob Levy", ""], ["Karsai", "M\u00e1rton", ""]]}, {"id": "2004.04911", "submitter": "Sher Dadakhanov", "authors": "Sher Dadakhanov", "title": "Analyze and Development System with Multiple Biometric Identification", "comments": "Multiple Biometric Identification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cause of a rapid increase in technological development, increasing identity\ntheft, consumer fraud, the threat to personal data is also increasing every\nday. Methods developed earlier to ensure personal the information from the\nthefts was not effective and safe. Biometrics were introduced when it was\nneeded technology for more efficient security of personal information.\nOld-fashioned traditional approaches like Personal identification number( PIN),\npasswords, keys, login ID can be forgotten, stolen or lost. In biometric\nauthentication system, user may not remember any passwords or carry any keys.\nAs people they recognize each other by the physical appearance and behavioral\ncharacteristics that biometric systems use physical characteristics, such as\nfingerprints, facial recognition, voice recognition, in order to distinguish\nbetween the actual user and scammer. In order to increase safety in 2005,\nbiometric identification methods were developed government and business\nsectors, but today it has reached almost all private sectors as Banking,\nFinance, home security and protection, healthcare, business security and\nsecurity etc. Since biometric samples and templates of a biometric system\nhaving one biometric character to detect and the user can be replaced and\nduplicated, the new idea of merging multiple biometric identification\ntechnologies has so-called multimodal biometric recognition systems have been\nintroduced that use two or more biometric data characteristics of the\nindividual that can be identified as a real user or not.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 05:08:54 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Dadakhanov", "Sher", ""]]}, {"id": "2004.04912", "submitter": "Xin Xu", "authors": "Xin Xu, Lei Liu, Weifeng Liu, Meng Wang, Ruimin Hu", "title": "Person Re-Identification via Active Hard Sample Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating a large-scale image dataset is very tedious, yet necessary for\ntraining person re-identification models. To alleviate such a problem, we\npresent an active hard sample mining framework via training an effective re-ID\nmodel with the least labeling efforts. Considering that hard samples can\nprovide informative patterns, we first formulate an uncertainty estimation to\nactively select hard samples to iteratively train a re-ID model from scratch.\nThen, intra-diversity estimation is designed to reduce the redundant hard\nsamples by maximizing their diversity. Moreover, we propose a computer-assisted\nidentity recommendation module embedded in the active hard sample mining\nframework to help human annotators to rapidly and accurately label the selected\nsamples. Extensive experiments were carried out to demonstrate the\neffectiveness of our method on several public datasets. Experimental results\nindicate that our method can reduce 57%, 63%, and 49% annotation efforts on the\nMarket1501, MSMT17, and CUHK03, respectively, while maximizing the performance\nof the re-ID model.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 05:11:11 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 06:49:29 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Xu", "Xin", ""], ["Liu", "Lei", ""], ["Liu", "Weifeng", ""], ["Wang", "Meng", ""], ["Hu", "Ruimin", ""]]}, {"id": "2004.04914", "submitter": "Qiuxia Lin", "authors": "Shuang Li, Chi Harold Liu, Qiuxia Lin, Qi Wen, Limin Su, Gao Huang,\n  Zhengming Ding", "title": "Deep Residual Correction Network for Partial Domain Adaptation", "comments": "Accepted by T-PAMI, 2020", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2964173", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep domain adaptation methods have achieved appealing performance by\nlearning transferable representations from a well-labeled source domain to a\ndifferent but related unlabeled target domain. Most existing works assume\nsource and target data share the identical label space, which is often\ndifficult to be satisfied in many real-world applications. With the emergence\nof big data, there is a more practical scenario called partial domain\nadaptation, where we are always accessible to a more large-scale source domain\nwhile working on a relative small-scale target domain. In this case, the\nconventional domain adaptation assumption should be relaxed, and the target\nlabel space tends to be a subset of the source label space. Intuitively,\nreinforcing the positive effects of the most relevant source subclasses and\nreducing the negative impacts of irrelevant source subclasses are of vital\nimportance to address partial domain adaptation challenge. This paper proposes\nan efficiently-implemented Deep Residual Correction Network (DRCN) by plugging\none residual block into the source network along with the task-specific feature\nlayer, which effectively enhances the adaptation from source to target and\nexplicitly weakens the influence from the irrelevant source classes.\nSpecifically, the plugged residual block, which consists of several\nfully-connected layers, could deepen basic network and boost its feature\nrepresentation capability correspondingly. Moreover, we design a weighted\nclass-wise domain alignment loss to couple two domains by matching the feature\ndistributions of shared classes between source and target. Comprehensive\nexperiments on partial, traditional and fine-grained cross-domain visual\nrecognition demonstrate that DRCN is superior to the competitive deep domain\nadaptation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 06:07:16 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Li", "Shuang", ""], ["Liu", "Chi Harold", ""], ["Lin", "Qiuxia", ""], ["Wen", "Qi", ""], ["Su", "Limin", ""], ["Huang", "Gao", ""], ["Ding", "Zhengming", ""]]}, {"id": "2004.04917", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani and Liwei Wu and Shengli Hu and Joel Tetreault and\n  Alejandro Jaimes", "title": "Multimodal Categorization of Crisis Events in Social Media", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR 2020)", "journal-ref": "Conference on Computer Vision and Pattern Recognition (CVPR 2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent developments in image classification and natural language processing,\ncoupled with the rapid growth in social media usage, have enabled fundamental\nadvances in detecting breaking events around the world in real-time. Emergency\nresponse is one such area that stands to gain from these advances. By\nprocessing billions of texts and images a minute, events can be automatically\ndetected to enable emergency response workers to better assess rapidly evolving\nsituations and deploy resources accordingly. To date, most event detection\ntechniques in this area have focused on image-only or text-only approaches,\nlimiting detection performance and impacting the quality of information\ndelivered to crisis response teams. In this paper, we present a new multimodal\nfusion method that leverages both images and texts as input. In particular, we\nintroduce a cross-attention module that can filter uninformative and misleading\ncomponents from weak modalities on a sample by sample basis. In addition, we\nemploy a multimodal graph-based approach to stochastically transition between\nembeddings of different multimodal pairs during training to better regularize\nthe learning process as well as dealing with limited training data by\nconstructing new matched pairs from different samples. We show that our method\noutperforms the unimodal approaches and strong multimodal baselines by a large\nmargin on three crisis-related tasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 06:31:30 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Wu", "Liwei", ""], ["Hu", "Shengli", ""], ["Tetreault", "Joel", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "2004.04923", "submitter": "Yanchao Yang", "authors": "Yanchao Yang, Dong Lao, Ganesh Sundaramoorthi and Stefano Soatto", "title": "Phase Consistent Ecological Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two criteria to regularize the optimization involved in learning\na classifier in a domain where no annotated data are available, leveraging\nannotated data in a different domain, a problem known as unsupervised domain\nadaptation. We focus on the task of semantic segmentation, where annotated\nsynthetic data are aplenty, but annotating real data is laborious. The first\ncriterion, inspired by visual psychophysics, is that the map between the two\nimage domains be phase-preserving. This restricts the set of possible learned\nmaps, while enabling enough flexibility to transfer semantic information. The\nsecond criterion aims to leverage ecological statistics, or regularities in the\nscene which are manifest in any image of it, regardless of the characteristics\nof the illuminant or the imaging sensor. It is implemented using a deep neural\nnetwork that scores the likelihood of each possible segmentation given a single\nun-annotated image. Incorporating these two priors in a standard domain\nadaptation framework improves performance across the board in the most common\nunsupervised domain adaptation benchmarks for semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 06:58:03 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Yang", "Yanchao", ""], ["Lao", "Dong", ""], ["Sundaramoorthi", "Ganesh", ""], ["Soatto", "Stefano", ""]]}, {"id": "2004.04933", "submitter": "Yukun Huang", "authors": "Yukun Huang, Zheng-Jun Zha, Xueyang Fu, Richang Hong, Liang Li", "title": "Real-world Person Re-Identification via Degradation Invariance Learning", "comments": "To appear in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) in real-world scenarios usually suffers from\nvarious degradation factors, e.g., low-resolution, weak illumination, blurring\nand adverse weather. On the one hand, these degradations lead to severe\ndiscriminative information loss, which significantly obstructs identity\nrepresentation learning; on the other hand, the feature mismatch problem caused\nby low-level visual variations greatly reduces retrieval performance. An\nintuitive solution to this problem is to utilize low-level image restoration\nmethods to improve the image quality. However, existing restoration methods\ncannot directly serve to real-world Re-ID due to various limitations, e.g., the\nrequirements of reference samples, domain gap between synthesis and reality,\nand incompatibility between low-level and high-level methods. In this paper, to\nsolve the above problem, we propose a degradation invariance learning framework\nfor real-world person Re-ID. By introducing a self-supervised disentangled\nrepresentation learning strategy, our method is able to simultaneously extract\nidentity-related robust features and remove real-world degradations without\nextra supervision. We use low-resolution images as the main demonstration, and\nexperiments show that our approach is able to achieve state-of-the-art\nperformance on several Re-ID benchmarks. In addition, our framework can be\neasily extended to other real-world degradation factors, such as weak\nillumination, with only a few modifications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 07:58:50 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Huang", "Yukun", ""], ["Zha", "Zheng-Jun", ""], ["Fu", "Xueyang", ""], ["Hong", "Richang", ""], ["Li", "Liang", ""]]}, {"id": "2004.04940", "submitter": "Yuxin Wang", "authors": "Yuxin Wang, Hongtao Xie, Zhengjun Zha, Mengting Xing, Zilong Fu and\n  Yongdong Zhang", "title": "ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene\n  Text Detection", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection has witnessed rapid development in recent years.\nHowever, there still exists two main challenges: 1) many methods suffer from\nfalse positives in their text representations; 2) the large scale variance of\nscene texts makes it hard for network to learn samples. In this paper, we\npropose the ContourNet, which effectively handles these two problems taking a\nfurther step toward accurate arbitrary-shaped text detection. At first, a\nscale-insensitive Adaptive Region Proposal Network (Adaptive-RPN) is proposed\nto generate text proposals by only focusing on the Intersection over Union\n(IoU) values between predicted and ground-truth bounding boxes. Then a novel\nLocal Orthogonal Texture-aware Module (LOTM) models the local texture\ninformation of proposal features in two orthogonal directions and represents\ntext region with a set of contour points. Considering that the strong\nunidirectional or weakly orthogonal activation is usually caused by the\nmonotonous texture characteristic of false-positive patterns (e.g. streaks.),\nour method effectively suppresses these false positives by only outputting\npredictions with high response value in both orthogonal directions. This gives\nmore accurate description of text regions. Extensive experiments on three\nchallenging datasets (Total-Text, CTW1500 and ICDAR2015) verify that our method\nachieves the state-of-the-art performance. Code is available at\nhttps://github.com/wangyuxin87/ContourNet.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:15:23 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Wang", "Yuxin", ""], ["Xie", "Hongtao", ""], ["Zha", "Zhengjun", ""], ["Xing", "Mengting", ""], ["Fu", "Zilong", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2004.04943", "submitter": "Beichen Zhang", "authors": "Beichen Zhang (1), Liang Li (2), Shijie Yang (1, 2), Shuhui Wang (2),\n  Zheng-Jun Zha (3), Qingming Huang (1, 2, 4) ((1) University of Chinese\n  Academy of Sciences. (2) Key Lab of Intell. Info. Process., Inst. of Comput.\n  Tech., Chinese Academy of Sciences. (3) University of Science and Technology\n  of China. (4) Peng Cheng Laboratory.)", "title": "State-Relabeling Adversarial Active Learning", "comments": "Accepted as Oral at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is to design label-efficient algorithms by sampling the most\nrepresentative samples to be labeled by an oracle. In this paper, we propose a\nstate relabeling adversarial active learning model (SRAAL), that leverages both\nthe annotation and the labeled/unlabeled state information for deriving the\nmost informative unlabeled samples. The SRAAL consists of a representation\ngenerator and a state discriminator. The generator uses the complementary\nannotation information with traditional reconstruction information to generate\nthe unified representation of samples, which embeds the semantic into the whole\ndata representation. Then, we design an online uncertainty indicator in the\ndiscriminator, which endues unlabeled samples with different importance. As a\nresult, we can select the most informative samples based on the discriminator's\npredicted state. We also design an algorithm to initialize the labeled pool,\nwhich makes subsequent sampling more efficient. The experiments conducted on\nvarious datasets show that our model outperforms the previous state-of-art\nactive learning methods and our initially sampling algorithm achieves better\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:23:59 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Beichen", ""], ["Li", "Liang", ""], ["Yang", "Shijie", ""], ["Wang", "Shuhui", ""], ["Zha", "Zheng-Jun", ""], ["Huang", "Qingming", ""]]}, {"id": "2004.04954", "submitter": "Piotr Bojanowski", "authors": "Lina Mezghani, Sainbayar Sukhbaatar, Arthur Szlam, Armand Joulin,\n  Piotr Bojanowski", "title": "Learning to Visually Navigate in Photorealistic Environments Without any\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to navigate in a realistic setting where an agent must rely solely\non visual inputs is a challenging task, in part because the lack of position\ninformation makes it difficult to provide supervision during training. In this\npaper, we introduce a novel approach for learning to navigate from image inputs\nwithout external supervision or reward. Our approach consists of three stages:\nlearning a good representation of first-person views, then learning to explore\nusing memory, and finally learning to navigate by setting its own goals. The\nmodel is trained with intrinsic rewards only so that it can be applied to any\nenvironment with image observations. We show the benefits of our approach by\ntraining an agent to navigate challenging photo-realistic environments from the\nGibson dataset with RGB inputs only.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 08:59:32 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Mezghani", "Lina", ""], ["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Joulin", "Armand", ""], ["Bojanowski", "Piotr", ""]]}, {"id": "2004.04955", "submitter": "Jinlin Liu", "authors": "Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui\n  Zhang, Xian-sheng Hua", "title": "Boosting Semantic Human Matting with Coarse Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic human matting aims to estimate the per-pixel opacity of the\nforeground human regions. It is quite challenging and usually requires user\ninteractive trimaps and plenty of high quality annotated data. Annotating such\nkind of data is labor intensive and requires great skills beyond normal users,\nespecially considering the very detailed hair part of humans. In contrast,\ncoarse annotated human dataset is much easier to acquire and collect from the\npublic dataset. In this paper, we propose to use coarse annotated data coupled\nwith fine annotated data to boost end-to-end semantic human matting without\ntrimaps as extra input. Specifically, we train a mask prediction network to\nestimate the coarse semantic mask using the hybrid data, and then propose a\nquality unification network to unify the quality of the previous coarse mask\noutputs. A matting refinement network takes in the unified mask and the input\nimage to predict the final alpha matte. The collected coarse annotated dataset\nenriches our dataset significantly, allows generating high quality alpha matte\nfor real images. Experimental results show that the proposed method performs\ncomparably against state-of-the-art methods. Moreover, the proposed method can\nbe used for refining coarse annotated public dataset, as well as semantic\nsegmentation methods, which reduces the cost of annotating high quality human\ndata to a great extent.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:11:02 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Liu", "Jinlin", ""], ["Yao", "Yuan", ""], ["Hou", "Wendi", ""], ["Cui", "Miaomiao", ""], ["Xie", "Xuansong", ""], ["Zhang", "Changshui", ""], ["Hua", "Xian-sheng", ""]]}, {"id": "2004.04962", "submitter": "Jiale Li", "authors": "Jiale Li, Shujie Luo, Ziqi Zhu, Hang Dai, Andrey S. Krylov, Yong Ding,\n  and Ling Shao", "title": "3D IoU-Net: IoU Guided 3D Object Detector for Point Clouds", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing point cloud based 3D object detectors focus on the tasks of\nclassification and box regression. However, another bottleneck in this area is\nachieving an accurate detection confidence for the Non-Maximum Suppression\n(NMS) post-processing. In this paper, we add a 3D IoU prediction branch to the\nregular classification and regression branches. The predicted IoU is used as\nthe detection confidence for NMS. In order to obtain a more accurate IoU\nprediction, we propose a 3D IoU-Net with IoU sensitive feature learning and an\nIoU alignment operation. To obtain a perspective-invariant prediction head, we\npropose an Attentive Corner Aggregation (ACA) module by aggregating a local\npoint cloud feature from each perspective of eight corners and adaptively\nweighting the contribution of each perspective with different attentions. We\npropose a Corner Geometry Encoding (CGE) module for geometry information\nembedding. To the best of our knowledge, this is the first time geometric\nembedding information has been introduced in proposal feature learning. These\ntwo feature parts are then adaptively fused by a multi-layer perceptron (MLP)\nnetwork as our IoU sensitive feature. The IoU alignment operation is introduced\nto resolve the mismatching between the bounding box regression head and IoU\nprediction, thereby further enhancing the accuracy of IoU prediction. The\nexperimental results on the KITTI car detection benchmark show that 3D IoU-Net\nwith IoU perception achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:24:29 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Li", "Jiale", ""], ["Luo", "Shujie", ""], ["Zhu", "Ziqi", ""], ["Dai", "Hang", ""], ["Krylov", "Andrey S.", ""], ["Ding", "Yong", ""], ["Shao", "Ling", ""]]}, {"id": "2004.04963", "submitter": "Toru Tamaki", "authors": "Kento Terao, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda, Shun'ichi\n  Satoh", "title": "Rephrasing visual questions by specifying the entropy of the answer\n  distribution", "comments": "10 pages", "journal-ref": null, "doi": "10.1587/transinf.2020EDP7089", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) is a task of answering a visual question that\nis a pair of question and image. Some visual questions are ambiguous and some\nare clear, and it may be appropriate to change the ambiguity of questions from\nsituation to situation. However, this issue has not been addressed by any prior\nwork. We propose a novel task, rephrasing the questions by controlling the\nambiguity of the questions. The ambiguity of a visual question is defined by\nthe use of the entropy of the answer distribution predicted by a VQA model. The\nproposed model rephrases a source question given with an image so that the\nrephrased question has the ambiguity (or entropy) specified by users. We\npropose two learning strategies to train the proposed model with the VQA v2\ndataset, which has no ambiguity information. We demonstrate the advantage of\nour approach that can control the ambiguity of the rephrased questions, and an\ninteresting observation that it is harder to increase than to reduce ambiguity.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:32:37 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Terao", "Kento", ""], ["Tamaki", "Toru", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Satoh", "Shun'ichi", ""]]}, {"id": "2004.04968", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Tenga Wakamiya, Kensho Hara, Yutaka Satoh", "title": "Would Mega-scale Datasets Further Enhance Spatiotemporal 3D CNNs?", "comments": "Codes and pre-trained models are publicly available:\n  https://github.com/kenshohara/3D-ResNets-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we collect and use a video dataset to further improve spatiotemporal\n3D Convolutional Neural Networks (3D CNNs)? In order to positively answer this\nopen question in video recognition, we have conducted an exploration study\nusing a couple of large-scale video datasets and 3D CNNs. In the early era of\ndeep neural networks, 2D CNNs have been better than 3D CNNs in the context of\nvideo recognition. Recent studies revealed that 3D CNNs can outperform 2D CNNs\ntrained on a large-scale video dataset. However, we heavily rely on\narchitecture exploration instead of dataset consideration. Therefore, in the\npresent paper, we conduct exploration study in order to improve spatiotemporal\n3D CNNs as follows: (i) Recently proposed large-scale video datasets help\nimprove spatiotemporal 3D CNNs in terms of video classification accuracy. We\nreveal that a carefully annotated dataset (e.g., Kinetics-700) effectively\npre-trains a video representation for a video classification task. (ii) We\nconfirm the relationships between #category/#instance and video classification\naccuracy. The results show that #category should initially be fixed, and then\n#instance is increased on a video dataset in case of dataset construction.\n(iii) In order to practically extend a video dataset, we simply concatenate\npublicly available datasets, such as Kinetics-700 and Moments in Time (MiT)\ndatasets. Compared with Kinetics-700 pre-training, we further enhance\nspatiotemporal 3D CNNs with the merged dataset, e.g., +0.9, +3.4, and +1.1 on\nUCF-101, HMDB-51, and ActivityNet datasets, respectively, in terms of\nfine-tuning. (iv) In terms of recognition architecture, the Kinetics-700 and\nmerged dataset pre-trained models increase the recognition performance to 200\nlayers with the Residual Network (ResNet), while the Kinetics-400 pre-trained\nmodel cannot successfully optimize the 200-layer architecture.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:44:19 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Wakamiya", "Tenga", ""], ["Hara", "Kensho", ""], ["Satoh", "Yutaka", ""]]}, {"id": "2004.04977", "submitter": "Evangelos Ntavelis", "authors": "Evangelos Ntavelis, Andr\\'es Romero, Iason Kastanis, Luc Van Gool and\n  Radu Timofte", "title": "SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing\n  Objects", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58542-6_24", "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image generation gave rise to powerful tools for semantic\nimage editing. However, existing approaches can either operate on a single\nimage or require an abundance of additional information. They are not capable\nof handling the complete set of editing operations, that is addition,\nmanipulation or removal of semantic concepts. To address these limitations, we\npropose SESAME, a novel generator-discriminator pair for Semantic Editing of\nScenes by Adding, Manipulating or Erasing objects. In our setup, the user\nprovides the semantic labels of the areas to be edited and the generator\nsynthesizes the corresponding pixels. In contrast to previous methods that\nemploy a discriminator that trivially concatenates semantics and image as an\ninput, the SESAME discriminator is composed of two input streams that\nindependently process the image and its semantics, using the latter to\nmanipulate the results of the former. We evaluate our model on a diverse set of\ndatasets and report state-of-the-art performance on two tasks: (a) image\nmanipulation and (b) image generation conditioned on semantic labels.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:19:19 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 14:52:01 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ntavelis", "Evangelos", ""], ["Romero", "Andr\u00e9s", ""], ["Kastanis", "Iason", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2004.04979", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Zheng-Jun Zha, Xierong Zhu, Na Jiang", "title": "Co-Saliency Spatio-Temporal Interaction Network for Person\n  Re-Identification in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims at identifying a certain pedestrian across\nnon-overlapping camera networks. Video-based re-identification approaches have\ngained significant attention recently, expanding image-based approaches by\nlearning features from multiple frames. In this work, we propose a novel\nCo-Saliency Spatio-Temporal Interaction Network (CSTNet) for person\nre-identification in videos. It captures the common salient foreground regions\namong video frames and explores the spatial-temporal long-range context\ninterdependency from such regions, towards learning discriminative pedestrian\nrepresentation. Specifically, multiple co-saliency learning modules within\nCSTNet are designed to utilize the correlated information across video frames\nto extract the salient features from the task-relevant regions and suppress\nbackground interference. Moreover, multiple spatialtemporal interaction modules\nwithin CSTNet are proposed, which exploit the spatial and temporal long-range\ncontext interdependencies on such features and spatial-temporal information\ncorrelation, to enhance feature representation. Extensive experiments on two\nbenchmarks have demonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:23:58 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 10:04:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Liu", "Jiawei", ""], ["Zha", "Zheng-Jun", ""], ["Zhu", "Xierong", ""], ["Jiang", "Na", ""]]}, {"id": "2004.04981", "submitter": "Yizhou Zhou", "authors": "Yizhou Zhou, Xiaoyan Sun, Chong Luo, Zheng-Jun Zha and Wenjun Zeng", "title": "Spatiotemporal Fusion in 3D CNNs: A Probabilistic View", "comments": "To be published in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success in still image recognition, deep neural networks for\nspatiotemporal signal tasks (such as human action recognition in videos) still\nsuffers from low efficacy and inefficiency over the past years. Recently, human\nexperts have put more efforts into analyzing the importance of different\ncomponents in 3D convolutional neural networks (3D CNNs) to design more\npowerful spatiotemporal learning backbones. Among many others, spatiotemporal\nfusion is one of the essentials. It controls how spatial and temporal signals\nare extracted at each layer during inference. Previous attempts usually start\nby ad-hoc designs that empirically combine certain convolutions and then draw\nconclusions based on the performance obtained by training the corresponding\nnetworks. These methods only support network-level analysis on limited number\nof fusion strategies. In this paper, we propose to convert the spatiotemporal\nfusion strategies into a probability space, which allows us to perform\nnetwork-level evaluations of various fusion strategies without having to train\nthem separately. Besides, we can also obtain fine-grained numerical information\nsuch as layer-level preference on spatiotemporal fusion within the probability\nspace. Our approach greatly boosts the efficiency of analyzing spatiotemporal\nfusion. Based on the probability space, we further generate new fusion\nstrategies which achieve the state-of-the-art performance on four well-known\naction recognition datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:40:35 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhou", "Yizhou", ""], ["Sun", "Xiaoyan", ""], ["Luo", "Chong", ""], ["Zha", "Zheng-Jun", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2004.04989", "submitter": "Ionut Cosmin Duta", "authors": "Ionut Cosmin Duta, Li Liu, Fan Zhu, Ling Shao", "title": "Improved Residual Networks for Image and Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) represent a powerful type of convolutional neural\nnetwork (CNN) architecture, widely adopted and used in various tasks. In this\nwork we propose an improved version of ResNets. Our proposed improvements\naddress all three main components of a ResNet: the flow of information through\nthe network layers, the residual building block, and the projection shortcut.\nWe are able to show consistent improvements in accuracy and learning\nconvergence over the baseline. For instance, on ImageNet dataset, using the\nResNet with 50 layers, for top-1 accuracy we can report a 1.19% improvement\nover the baseline in one setting and around 2% boost in another. Importantly,\nthese improvements are obtained without increasing the model complexity. Our\nproposed approach allows us to train extremely deep networks, while the\nbaseline shows severe optimization issues. We report results on three tasks\nover six datasets: image classification (ImageNet, CIFAR-10 and CIFAR-100),\nobject detection (COCO) and video action recognition (Kinetics-400 and\nSomething-Something-v2). In the deep learning era, we establish a new milestone\nfor the depth of a CNN. We successfully train a 404-layer deep CNN on the\nImageNet dataset and a 3002-layer network on CIFAR-10 and CIFAR-100, while the\nbaseline is not able to converge at such extreme depths. Code is available at:\nhttps://github.com/iduta/iresnet\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 11:09:50 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Duta", "Ionut Cosmin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "2004.04993", "submitter": "QuanMeng Ma", "authors": "QuanMeng Ma, Guang Jiang and DianZhi Lai", "title": "Robust Line Segments Matching via Graph Convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line matching plays an essential role in structure from motion (SFM) and\nsimultaneous localization and mapping (SLAM), especially in low-textured and\nrepetitive scenes. In this paper, we present a new method of using a graph\nconvolution network to match line segments in a pair of images, and we design a\ngraph-based strategy of matching line segments with relaxing to an optimal\ntransport problem. In contrast to hand-crafted line matching algorithms, our\napproach learns local line segment descriptor and the matching simultaneously\nthrough end-to-end training. The results show our method outperforms the\nstate-of-the-art techniques, and especially, the recall is improved from 45.28%\nto 70.47% under a similar presicion. The code of our work is available at\nhttps://github.com/mameng1/GraphLineMatching.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 11:33:18 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 04:58:30 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ma", "QuanMeng", ""], ["Jiang", "Guang", ""], ["Lai", "DianZhi", ""]]}, {"id": "2004.05020", "submitter": "Yaran Chen", "authors": "Yaran Chen, Ruiyuan Gao, Fenggang Liu and Dongbin Zhao", "title": "ModuleNet: Knowledge-inherited Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Neural Architecture Search (NAS) can bring improvement to deep\nmodels, they always neglect precious knowledge of existing models.\n  The computation and time costing property in NAS also means that we should\nnot start from scratch to search, but make every attempt to reuse the existing\nknowledge.\n  In this paper, we discuss what kind of knowledge in a model can and should be\nused for new architecture design.\n  Then, we propose a new NAS algorithm, namely ModuleNet, which can fully\ninherit knowledge from existing convolutional neural networks.\n  To make full use of existing models, we decompose existing models into\ndifferent \\textit{module}s which also keep their weights, consisting of a\nknowledge base.\n  Then we sample and search for new architecture according to the knowledge\nbase.\n  Unlike previous search algorithms, and benefiting from inherited knowledge,\nour method is able to directly search for architectures in the macro space by\nNSGA-II algorithm without tuning parameters in these \\textit{module}s.\n  Experiments show that our strategy can efficiently evaluate the performance\nof new architecture even without tuning weights in convolutional layers.\n  With the help of knowledge we inherited, our search results can always\nachieve better performance on various datasets (CIFAR10, CIFAR100) over\noriginal architectures.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:03:52 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 03:39:26 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Chen", "Yaran", ""], ["Gao", "Ruiyuan", ""], ["Liu", "Fenggang", ""], ["Zhao", "Dongbin", ""]]}, {"id": "2004.05021", "submitter": "Dechao Meng", "authors": "Dechao Meng and Liang Li and Xuejing Liu and Yadong Li and Shijie Yang\n  and Zhengjun Zha and Xingyu Gao and Shuhui Wang and Qingming Huang", "title": "Parsing-based View-aware Embedding Network for Vehicle Re-Identification", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-Identification is to find images of the same vehicle from various\nviews in the cross-camera scenario. The main challenges of this task are the\nlarge intra-instance distance caused by different views and the subtle\ninter-instance discrepancy caused by similar vehicles. In this paper, we\npropose a parsing-based view-aware embedding network (PVEN) to achieve the\nview-aware feature alignment and enhancement for vehicle ReID. First, we\nintroduce a parsing network to parse a vehicle into four different views, and\nthen align the features by mask average pooling. Such alignment provides a\nfine-grained representation of the vehicle. Second, in order to enhance the\nview-aware features, we design a common-visible attention to focus on the\ncommon visible views, which not only shortens the distance among\nintra-instances, but also enlarges the discrepancy of inter-instances. The PVEN\nhelps capture the stable discriminative information of vehicle under different\nviews. The experiments conducted on three datasets show that our model\noutperforms state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:06:09 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Meng", "Dechao", ""], ["Li", "Liang", ""], ["Liu", "Xuejing", ""], ["Li", "Yadong", ""], ["Yang", "Shijie", ""], ["Zha", "Zhengjun", ""], ["Gao", "Xingyu", ""], ["Wang", "Shuhui", ""], ["Huang", "Qingming", ""]]}, {"id": "2004.05024", "submitter": "Marvin Lerousseau", "authors": "Marvin Lerousseau, Maria Vakalopoulou, Marion Classe, Julien Adam,\n  Enzo Battistella, Alexandre Carr\\'e, Th\\'eo Estienne, Th\\'eophraste Henry,\n  Eric Deutsch, Nikos Paragios", "title": "Weakly supervised multiple instance learning histopathological tumor\n  segmentation", "comments": "Accepted MICCAI 2020; added code + results url; 10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological image segmentation is a challenging and important topic in\nmedical imaging with tremendous potential impact in clinical practice. State of\nthe art methods rely on hand-crafted annotations which hinder clinical\ntranslation since histology suffers from significant variations between cancer\nphenotypes. In this paper, we propose a weakly supervised framework for whole\nslide imaging segmentation that relies on standard clinical annotations,\navailable in most medical systems. In particular, we exploit a multiple\ninstance learning scheme for training models. The proposed framework has been\nevaluated on multi-locations and multi-centric public data from The Cancer\nGenome Atlas and the PatchCamelyon dataset. Promising results when compared\nwith experts' annotations demonstrate the potentials of the presented approach.\nThe complete framework, including $6481$ generated tumor maps and data\nprocessing, is available at https://github.com/marvinler/tcga_segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 13:12:47 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 15:53:24 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 16:47:54 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 12:26:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lerousseau", "Marvin", ""], ["Vakalopoulou", "Maria", ""], ["Classe", "Marion", ""], ["Adam", "Julien", ""], ["Battistella", "Enzo", ""], ["Carr\u00e9", "Alexandre", ""], ["Estienne", "Th\u00e9o", ""], ["Henry", "Th\u00e9ophraste", ""], ["Deutsch", "Eric", ""], ["Paragios", "Nikos", ""]]}, {"id": "2004.05048", "submitter": "James Murphy", "authors": "Shukun Zhang and James M. Murphy", "title": "Hyperspectral Image Clustering with Spatially-Regularized Ultrametrics", "comments": "5 pages, 2 columns, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for the unsupervised clustering of hyperspectral images\nbased on spatially regularized spectral clustering with ultrametric path\ndistances. The proposed method efficiently combines data density and geometry\nto distinguish between material classes in the data, without the need for\ntraining labels. The proposed method is efficient, with quasilinear scaling in\nthe number of data points, and enjoys robust theoretical performance\nguarantees. Extensive experiments on synthetic and real HSI data demonstrate\nits strong performance compared to benchmark and state-of-the-art methods. In\nparticular, the proposed method achieves not only excellent labeling accuracy,\nbut also efficiently estimates the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 14:27:41 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Shukun", ""], ["Murphy", "James M.", ""]]}, {"id": "2004.05054", "submitter": "Evgeny Izutov", "authors": "Evgeny Izutov", "title": "ASL Recognition with Metric-Learning based Lightweight Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades the set of human tasks that are solved by machines was\nextended dramatically. From simple image classification problems researchers\nnow move towards solving more sophisticated and vital problems, like,\nautonomous driving and language translation. The case of language translation\nincludes a challenging area of sign language translation that incorporates both\nimage and language processing. We make a step in that direction by proposing a\nlightweight network for ASL gesture recognition with a performance sufficient\nfor practical applications. The proposed solution demonstrates impressive\nrobustness on MS-ASL dataset and in live mode for continuous sign gesture\nrecognition scenario. Additionally, we describe how to combine action\nrecognition model training with metric-learning to train the network on the\ndatabase of limited size. The training code is available as part of Intel\nOpenVINO Training Extensions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 14:41:30 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Izutov", "Evgeny", ""]]}, {"id": "2004.05077", "submitter": "Agostinho J\\'unior", "authors": "Janderson Ferreira (1), Agostinho A. F. J\\'unior (1), Yves M. Galv\\~ao\n  (1), Bruno J. T. Fernandes (1) and Pablo Barros (1 and 2) ((1) Universidade\n  de Pernambuco - Escola Polit\\'ecnica de Pernambuco, (2) Cognitive\n  Architecture for Collaborative Technologies Unit - Istituto Italiano di\n  Tecnologia)", "title": "CNN Encoder to Reduce the Dimensionality of Data Image for Motion\n  Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications need path planning algorithms to solve tasks in\ndifferent areas, such as social applications, autonomous cars, and tracking\nactivities. And most importantly motion planning. Although the use of path\nplanning is sufficient in most motion planning scenarios, they represent\npotential bottlenecks in large environments with dynamic changes. To tackle\nthis problem, the number of possible routes could be reduced to make it easier\nfor path planning algorithms to find the shortest path with less efforts. An\ntraditional algorithm for path planning is the A*, it uses an heuristic to work\nfaster than other solutions. In this work, we propose a CNN encoder capable of\neliminating useless routes for motion planning problems, then we combine the\nproposed neural network output with A*. To measure the efficiency of our\nsolution, we propose a database with different scenarios of motion planning\nproblems. The evaluated metric is the number of the iterations to find the\nshortest path. The A* was compared with the CNN Encoder (proposal) with A*. In\nall evaluated scenarios, our solution reduced the number of iterations by more\nthan 60\\%.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:44:52 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Ferreira", "Janderson", "", "1 and 2"], ["J\u00fanior", "Agostinho A. F.", "", "1 and 2"], ["Galv\u00e3o", "Yves M.", "", "1 and 2"], ["Fernandes", "Bruno J. T.", "", "1 and 2"], ["Barros", "Pablo", "", "1 and 2"]]}, {"id": "2004.05085", "submitter": "Thanh-Dat Truong", "authors": "Thanh-Dat Truong, Chi Nhan Duong, Kha Gia Quach, Dung Nguyen, Ngan Le,\n  Khoa Luu, and Tien D. Bui", "title": "Beyond Disentangled Representations: An Attentive Angular Distillation\n  Approach to Large-scale Lightweight Age-Invariant Face Recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.10620", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representations have been commonly adopted to Age-invariant Face\nRecognition (AiFR) tasks. However, these methods have reached some limitations\nwith (1) the requirement of large-scale face recognition (FR) training data\nwith age labels, which is limited in practice; (2) heavy deep network\narchitecture for high performance; and (3) their evaluations are usually taken\nplace on age-related face databases while neglecting the standard large-scale\nFR databases to guarantee its robustness. This work presents a novel Attentive\nAngular Distillation (AAD) approach to Large-scale Lightweight AiFR that\novercomes these limitations. Given two high-performance heavy networks as\nteachers with different specialized knowledge, AAD introduces a learning\nparadigm to efficiently distill the age-invariant attentive and angular\nknowledge from those teachers to a lightweight student network making it more\npowerful with higher FR accuracy and robust against age factor. Consequently,\nAAD approach is able to take the advantages of both FR datasets with and\nwithout age labels to train an AiFR model. Far apart from prior distillation\nmethods mainly focusing on accuracy and compression ratios in closed-set\nproblems, our AAD aims to solve the open-set problem, i.e. large-scale face\nrecognition. Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and\nMegaFace-FGNet with one million distractors have demonstrated the efficiency of\nthe proposed approach. This work also presents a new longitudinal face aging\n(LogiFace) database for further studies in age-related facial problems in\nfuture.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 01:28:47 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Truong", "Thanh-Dat", ""], ["Duong", "Chi Nhan", ""], ["Quach", "Kha Gia", ""], ["Nguyen", "Dung", ""], ["Le", "Ngan", ""], ["Luu", "Khoa", ""], ["Bui", "Tien D.", ""]]}, {"id": "2004.05100", "submitter": "Rohit Jena", "authors": "Rohit Jena, Shirsendu Sukanta Halder, Katia Sycara", "title": "MA 3 : Model Agnostic Adversarial Augmentation for Few Shot learning", "comments": "Accepted at CVPR Workshop on Visual Learning with Limited Labels 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent developments in vision-related problems using deep neural\nnetworks, there still remains a wide scope in the improvement of generalizing\nthese models to unseen examples. In this paper, we explore the domain of\nfew-shot learning with a novel augmentation technique. In contrast to other\ngenerative augmentation techniques, where the distribution over input images\nare learnt, we propose to learn the probability distribution over the image\ntransformation parameters which are easier and quicker to learn. Our technique\nis fully differentiable which enables its extension to versatile data-sets and\nbase models. We evaluate our proposed method on multiple base-networks and 2\ndata-sets to establish the robustness and efficiency of this method. We obtain\nan improvement of nearly 4% by adding our augmentation module without making\nany change in network architectures. We also make the code readily available\nfor usage by the community.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:35:49 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Jena", "Rohit", ""], ["Halder", "Shirsendu Sukanta", ""], ["Sycara", "Katia", ""]]}, {"id": "2004.05111", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B. D.\n  Sorensen", "title": "Deep transfer learning for improving single-EEG arousal detection", "comments": "Accepted for presentation at EMBC2020", "journal-ref": null, "doi": "10.1109/EMBC44109.2020.9176723", "report-no": null, "categories": "cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets in sleep science present challenges for machine learning algorithms\ndue to differences in recording setups across clinics. We investigate two deep\ntransfer learning strategies for overcoming the channel mismatch problem for\ncases where two datasets do not contain exactly the same setup leading to\ndegraded performance in single-EEG models. Specifically, we train a baseline\nmodel on multivariate polysomnography data and subsequently replace the first\ntwo layers to prepare the architecture for single-channel\nelectroencephalography data. Using a fine-tuning strategy, our model yields\nsimilar performance to the baseline model (F1=0.682 and F1=0.694,\nrespectively), and was significantly better than a comparable single-channel\nmodel. Our results are promising for researchers working with small databases\nwho wish to use deep learning models pre-trained on larger databases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:51:06 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:18:28 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "2004.05151", "submitter": "Xiao Liang", "authors": "Seyed Omid Sajedi, Xiao Liang", "title": "Model Uncertainty Quantification for Reliable Deep Vision Structural\n  Health Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision leveraging deep learning has achieved significant success in\nthe last decade. Despite the promising performance of the existing deep models\nin the recent literature, the extent of models' reliability remains unknown.\nStructural health monitoring (SHM) is a crucial task for the safety and\nsustainability of structures, and thus prediction mistakes can have fatal\noutcomes. This paper proposes Bayesian inference for deep vision SHM models\nwhere uncertainty can be quantified using the Monte Carlo dropout sampling.\nThree independent case studies for cracks, local damage identification, and\nbridge component detection are investigated using Bayesian inference. Aside\nfrom better prediction results, mean class softmax variance and entropy, the\ntwo uncertainty metrics, are shown to have good correlations with\nmisclassifications. While the uncertainty metrics can be used to trigger human\nintervention and potentially improve prediction results, interpretation of\nuncertainty masks can be challenging. Therefore, surrogate models are\nintroduced to take the uncertainty as input such that the performance can be\nfurther boosted. The proposed methodology in this paper can be applied to\nfuture deep vision SHM frameworks to incorporate model uncertainty in the\ninspection processes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:54:10 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Sajedi", "Seyed Omid", ""], ["Liang", "Xiao", ""]]}, {"id": "2004.05155", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta,\n  Ruslan Salakhutdinov", "title": "Learning to Explore using Active Neural SLAM", "comments": "Published in ICLR-2020. See the project webpage at\n  https://devendrachaplot.github.io/projects/Neural-SLAM for supplementary\n  videos. The code is available at\n  https://github.com/devendrachaplot/Neural-SLAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a modular and hierarchical approach to learn policies for\nexploring 3D environments, called `Active Neural SLAM'. Our approach leverages\nthe strengths of both classical and learning-based methods, by using analytical\npath planners with learned SLAM module, and global and local policies. The use\nof learning provides flexibility with respect to input modalities (in the SLAM\nmodule), leverages structural regularities of the world (in global policies),\nand provides robustness to errors in state estimation (in local policies). Such\nuse of learning within each module retains its benefits, while at the same\ntime, hierarchical decomposition and modular training allow us to sidestep the\nhigh sample complexities associated with training end-to-end policies. Our\nexperiments in visually and physically realistic simulated 3D environments\ndemonstrate the effectiveness of our approach over past learning and\ngeometry-based approaches. The proposed model can also be easily transferred to\nthe PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal\nNavigation Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:57:29 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Gandhi", "Dhiraj", ""], ["Gupta", "Saurabh", ""], ["Gupta", "Abhinav", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2004.05199", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, Daniel Cremers", "title": "Hamiltonian Dynamics for Real-World Shape Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical problem of 3D shape interpolation and propose a\nnovel, physically plausible approach based on Hamiltonian dynamics. While most\nprior work focuses on synthetic input shapes, our formulation is designed to be\napplicable to real-world scans with imperfect input correspondences and various\ntypes of noise. To that end, we use recent progress on dynamic thin shell\nsimulation and divergence-free shape deformation and combine them to address\nthe inverse problem of finding a plausible intermediate sequence for two input\nshapes. In comparison to prior work that mainly focuses on small distortion of\nconsecutive frames, we explicitly model volume preservation and momentum\nconservation, as well as an anisotropic local distortion model. We argue that,\nin order to get a robust interpolation for imperfect inputs, we need to model\nthe input noise explicitly which results in an alignment based formulation.\nFinally, we show a qualitative and quantitative improvement over prior work on\na broad range of synthetic and scanned data. Besides being more robust to noisy\ninputs, our method yields exactly volume preserving intermediate shapes, avoids\nself-intersections and is scalable to high resolution scans.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 18:38:52 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Eisenberger", "Marvin", ""], ["Cremers", "Daniel", ""]]}, {"id": "2004.05214", "submitter": "Sergiu Oprea", "authors": "Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John\n  Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez and\n  Antonis Argyros", "title": "A Review on Deep Learning Techniques for Video Prediction", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3045007", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict, anticipate and reason about future outcomes is a key\ncomponent of intelligent decision-making systems. In light of the success of\ndeep learning in computer vision, deep-learning-based video prediction emerged\nas a promising research direction. Defined as a self-supervised learning task,\nvideo prediction represents a suitable framework for representation learning,\nas it demonstrated potential capabilities for extracting meaningful\nrepresentations of the underlying patterns in natural videos. Motivated by the\nincreasing interest in this task, we provide a review on the deep learning\nmethods for prediction in video sequences. We firstly define the video\nprediction fundamentals, as well as mandatory background concepts and the most\nused datasets. Next, we carefully analyze existing video prediction models\norganized according to a proposed taxonomy, highlighting their contributions\nand their significance in the field. The summary of the datasets and methods is\naccompanied with experimental results that facilitate the assessment of the\nstate of the art on a quantitative basis. The paper is summarized by drawing\nsome general conclusions, identifying open research challenges and by pointing\nout future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 19:58:44 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 00:24:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Oprea", "Sergiu", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Garcia-Garcia", "Alberto", ""], ["Castro-Vargas", "John Alejandro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""], ["Argyros", "Antonis", ""]]}, {"id": "2004.05224", "submitter": "Yaodong Cui", "authors": "Yaodong Cui, Ren Chen, Wenbo Chu, Long Chen, Daxin Tian, Ying Li,\n  Dongpu Cao", "title": "Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A\n  Review", "comments": null, "journal-ref": "IEEE Transactions on Intelligent Transportation Systems.(2021)", "doi": "10.1109/TITS.2020.3023541", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles were experiencing rapid development in the past few\nyears. However, achieving full autonomy is not a trivial task, due to the\nnature of the complex and dynamic driving environment. Therefore, autonomous\nvehicles are equipped with a suite of different sensors to ensure robust,\naccurate environmental perception. In particular, the camera-LiDAR fusion is\nbecoming an emerging research theme. However, so far there has been no critical\nreview that focuses on deep-learning-based camera-LiDAR fusion methods. To\nbridge this gap and motivate future research, this paper devotes to review\nrecent deep-learning-based data fusion approaches that leverage both image and\npoint cloud. This review gives a brief overview of deep learning on image and\npoint cloud data processing. Followed by in-depth reviews of camera-LiDAR\nfusion methods in depth completion, object detection, semantic segmentation,\ntracking and online cross-sensor calibration, which are organized based on\ntheir respective fusion levels. Furthermore, we compare these methods on\npublicly available datasets. Finally, we identified gaps and over-looked\nchallenges between current academic researches and real-world applications.\nBased on these observations, we provide our insights and point out promising\nresearch directions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 20:43:14 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 14:12:13 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cui", "Yaodong", ""], ["Chen", "Ren", ""], ["Chu", "Wenbo", ""], ["Chen", "Long", ""], ["Tian", "Daxin", ""], ["Li", "Ying", ""], ["Cao", "Dongpu", ""]]}, {"id": "2004.05232", "submitter": "Mohamed Chaabane", "authors": "Mohamed Chaabane, Lionel Gueguen, Ameni Trabelsi, Ross Beveridge and\n  Stephen O'Hara", "title": "End-to-end Learning Improves Static Object Geo-localization in Monocular\n  Video", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately estimating the position of static objects, such as traffic lights,\nfrom the moving camera of a self-driving car is a challenging problem. In this\nwork, we present a system that improves the localization of static objects by\njointly-optimizing the components of the system via learning. Our system is\ncomprised of networks that perform: 1) 5DoF object pose estimation from a\nsingle image, 2) association of objects between pairs of frames, and 3)\nmulti-object tracking to produce the final geo-localization of the static\nobjects within the scene. We evaluate our approach using a publicly-available\ndata set, focusing on traffic lights due to data availability. For each\ncomponent, we compare against contemporary alternatives and show\nsignificantly-improved performance. We also show that the end-to-end system\nperformance is further improved via joint-training of the constituent models.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 21:10:34 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 21:00:00 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 18:44:28 GMT"}, {"version": "v4", "created": "Sun, 3 Jan 2021 17:36:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chaabane", "Mohamed", ""], ["Gueguen", "Lionel", ""], ["Trabelsi", "Ameni", ""], ["Beveridge", "Ross", ""], ["O'Hara", "Stephen", ""]]}, {"id": "2004.05233", "submitter": "Gang Yao", "authors": "Gang Yao, Ryan Saltus, Ashwin Dani", "title": "Shape Estimation for Elongated Deformable Object using B-spline Chained\n  Multiple Random Matrices Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a B-spline chained multiple random matrices representation is\nproposed to model geometric characteristics of an elongated deformable object.\nThe hyper degrees of freedom structure of the elongated deformable object make\nits shape estimation challenging. Based on the likelihood function of the\nproposed model, an expectation-maximization (EM) method is derived to estimate\nthe shape of the elongated deformable object. A split and merge method based on\nthe Euclidean minimum spanning tree (EMST) is proposed to provide\ninitialization for the EM algorithm. The proposed algorithm is evaluated for\nthe shape estimation of the elongated deformable objects in scenarios, such as\nthe static rope with various configurations (including configurations with\nintersection), the continuous manipulation of a rope and a plastic tube, and\nthe assembly of two plastic tubes. The execution time is computed and the\naccuracy of the shape estimation results is evaluated based on the comparisons\nbetween the estimated width values and its ground-truth, and the intersection\nover union (IoU) metric.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 21:15:54 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Yao", "Gang", ""], ["Saltus", "Ryan", ""], ["Dani", "Ashwin", ""]]}, {"id": "2004.05234", "submitter": "Sam Nguyen", "authors": "Sam Nguyen, Brenda Ng, Alan D. Kaplan and Priyadip Ray", "title": "Attend and Decode: 4D fMRI Task State Decoding Using Attention Models", "comments": null, "journal-ref": "Proceedings of the Machine Learning for Health NeurIPS Workshop,\n  PMLR 136:267-279, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) is a neuroimaging modality that\ncaptures the blood oxygen level in a subject's brain while the subject either\nrests or performs a variety of functional tasks under different conditions.\nGiven fMRI data, the problem of inferring the task, known as task state\ndecoding, is challenging due to the high dimensionality (hundreds of million\nsampling points per datum) and complex spatio-temporal blood flow patterns\ninherent in the data. In this work, we propose to tackle the fMRI task state\ndecoding problem by casting it as a 4D spatio-temporal classification problem.\nWe present a novel architecture called Brain Attend and Decode (BAnD), that\nuses residual convolutional neural networks for spatial feature extraction and\nself-attention mechanisms for temporal modeling. We achieve significant\nperformance gain compared to previous works on a 7-task benchmark from the\nlarge-scale Human Connectome Project-Young Adult (HCP-YA) dataset. We also\ninvestigate the transferability of BAnD's extracted features on unseen HCP\ntasks, either by freezing the spatial feature extraction layers and retraining\nthe temporal model, or finetuning the entire model. The pre-trained features\nfrom BAnD are useful on similar tasks while finetuning them yields competitive\nresults on unseen tasks/conditions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 21:29:34 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 06:35:49 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Nguyen", "Sam", ""], ["Ng", "Brenda", ""], ["Kaplan", "Alan D.", ""], ["Ray", "Priyadip", ""]]}, {"id": "2004.05247", "submitter": "Floris Van Breugel", "authors": "Bryson Lingenfelter, Arunava Nag, and Floris van Breugel", "title": "FLIVVER: Fly Lobula Inspired Visual Velocity Estimation & Ranging", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mechanism by which a tiny insect or insect-sized robot could estimate its\nabsolute velocity and distance to nearby objects remains unknown. However, this\nability is critical for behaviors that require estimating wind direction during\nflight, such as odor-plume tracking. Neuroscience and behavior studies with\ninsects have shown that they rely on the perception of image motion, or optic\nflow, to estimate relative motion, equivalent to a ratio of their velocity and\ndistance to objects in the world. The key open challenge is therefore to\ndecouple these two states from a single measurement of their ratio. Although\nmodern SLAM (Simultaneous Localization and Mapping) methods provide a solution\nto this problem for robotic systems, these methods typically rely on\ncomputations that insects likely cannot perform, such as simultaneously\ntracking multiple individual visual features, remembering a 3D map of the\nworld, and solving nonlinear optimization problems using iterative algorithms.\nHere we present a novel algorithm, FLIVVER, which combines the geometry of\ndynamic forward motion with inspiration from insect visual processing to\n\\textit{directly} estimate absolute ground velocity from a combination of optic\nflow and acceleration information. Our algorithm provides a clear hypothesis\nfor how insects might estimate absolute velocity, and also provides a\ntheoretical framework for designing fast analog circuitry for efficient state\nestimation, which could be applied to insect-sized robots.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 22:35:13 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Lingenfelter", "Bryson", ""], ["Nag", "Arunava", ""], ["van Breugel", "Floris", ""]]}, {"id": "2004.05261", "submitter": "Quoc-Huy Tran", "authors": "Sanjay Haresh, Sateesh Kumar, M. Zeeshan Zia, Quoc-Huy Tran", "title": "Towards Anomaly Detection in Dashcam Videos", "comments": "To appear at IV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inexpensive sensing and computation, as well as insurance innovations, have\nmade smart dashboard cameras ubiquitous. Increasingly, simple model-driven\ncomputer vision algorithms focused on lane departures or safe following\ndistances are finding their way into these devices. Unfortunately, the\nlong-tailed distribution of road hazards means that these hand-crafted\npipelines are inadequate for driver safety systems. We propose to apply\ndata-driven anomaly detection ideas from deep learning to dashcam videos, which\nhold the promise of bridging this gap. Unfortunately, there exists almost no\nliterature applying anomaly understanding to moving cameras, and\ncorrespondingly there is also a lack of relevant datasets. To counter this\nissue, we present a large and diverse dataset of truck dashcam videos, namely\nRetroTrucks, that includes normal and anomalous driving scenes. We apply: (i)\none-class classification loss and (ii) reconstruction-based loss, for anomaly\ndetection on RetroTrucks as well as on existing static-camera datasets. We\nintroduce formulations for modeling object interactions in this context as\npriors. Our experiments indicate that our dataset is indeed more challenging\nthan standard anomaly detection datasets, and previous anomaly detection\nmethods do not perform well here out-of-the-box. In addition, we share insights\ninto the behavior of these two important families of anomaly detection\napproaches on dashcam data.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 00:10:40 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 02:04:09 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Haresh", "Sanjay", ""], ["Kumar", "Sateesh", ""], ["Zia", "M. Zeeshan", ""], ["Tran", "Quoc-Huy", ""]]}, {"id": "2004.05275", "submitter": "Yeji Shen", "authors": "Yeji Shen, C.-C. Jay Kuo", "title": "Multi-View Matching (MVM): Facilitating Multi-Person 3D Pose Estimation\n  Learning with Action-Frozen People Video", "comments": "16 pages, 6 figures, submitted JVCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle the challeging problem of multi-person 3D pose estimation from a\nsingle image, we propose a multi-view matching (MVM) method in this work. The\nMVM method generates reliable 3D human poses from a large-scale video dataset,\ncalled the Mannequin dataset, that contains action-frozen people immitating\nmannequins. With a large amount of in-the-wild video data labeled by 3D\nsupervisions automatically generated by MVM, we are able to train a neural\nnetwork that takes a single image as the input for multi-person 3D pose\nestimation. The core technology of MVM lies in effective alignment of 2D poses\nobtained from multiple views of a static scene that has a strong geometric\nconstraint. Our objective is to maximize mutual consistency of 2D poses\nestimated in multiple frames, where geometric constraints as well as appearance\nsimilarities are taken into account simultaneously. To demonstrate the\neffectiveness of 3D supervisions provided by the MVM method, we conduct\nexperiments on the 3DPW and the MSCOCO datasets and show that our proposed\nsolution offers the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 01:09:50 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Shen", "Yeji", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2004.05284", "submitter": "Kunyuan Du", "authors": "Kunyuan Du, Ya Zhang, Haibing Guan", "title": "From Quantized DNNs to Quantizable DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Quantizable DNNs, a special type of DNNs that can\nflexibly quantize its bit-width (denoted as `bit modes' thereafter) during\nexecution without further re-training. To simultaneously optimize for all bit\nmodes, a combinational loss of all bit modes is proposed, which enforces\nconsistent predictions ranging from low-bit mode to 32-bit mode. This\nConsistency-based Loss may also be viewed as certain form of regularization\nduring training. Because outputs of matrix multiplication in different bit\nmodes have different distributions, we introduce Bit-Specific Batch\nNormalization so as to reduce conflicts among different bit modes. Experiments\non CIFAR100 and ImageNet have shown that compared to quantized DNNs,\nQuantizable DNNs not only have much better flexibility, but also achieve even\nhigher classification accuracy. Ablation studies further verify that the\nregularization through the consistency-based loss indeed improves the model's\ngeneralization performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 02:25:46 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Du", "Kunyuan", ""], ["Zhang", "Ya", ""], ["Guan", "Haibing", ""]]}, {"id": "2004.05298", "submitter": "An Xu", "authors": "An Xu, Zhouyuan Huo, Heng Huang", "title": "Exploit Where Optimizer Explores via Residuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to train the neural networks faster, many efforts have been devoted\nto exploring a better solution trajectory, but few have been put into\nexploiting the existing solution trajectory. To exploit the trajectory of\n(momentum) stochastic gradient descent (SGD(m)) method, we propose a novel\nmethod named SGD(m) with residuals (RSGD(m)), which leads to a performance\nboost of both the convergence and generalization. Our new method can also be\napplied to other optimizers such as ASGD and Adam. We provide theoretical\nanalysis to show that RSGD achieves a smaller growth rate of the generalization\nerror and the same (but empirically better) convergence rate compared with SGD.\nExtensive deep learning experiments on image classification, language modeling\nand graph convolutional neural networks show that the proposed algorithm is\nfaster than SGD(m)/Adam at the initial training stage, and similar to or better\nthan SGD(m) at the end of training with better generalization error.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 03:50:59 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 12:13:54 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Xu", "An", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "2004.05303", "submitter": "Ziwei Liao", "authors": "Ziwei Liao, Wei Wang, Xianyu Qi, Xiaoyu Zhang, Lin Xue, Jianzhen Jiao\n  and Ran Wei", "title": "Object-oriented SLAM using Quadrics and Symmetry Properties for Indoor\n  Environments", "comments": "Submission to IROS 2020. Video: https://youtu.be/u9zRBp4TPIs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at the application environment of indoor mobile robots, this paper\nproposes a sparse object-level SLAM algorithm based on an RGB-D camera. A\nquadric representation is used as a landmark to compactly model objects,\nincluding their position, orientation, and occupied space. The state-of-art\nquadric-based SLAM algorithm faces the observability problem caused by the\nlimited perspective under the plane trajectory of the mobile robot. To solve\nthe problem, the proposed algorithm fuses both object detection and point cloud\ndata to estimate the quadric parameters. It finishes the quadric initialization\nbased on a single frame of RGB-D data, which significantly reduces the\nrequirements for perspective changes. As objects are often observed locally,\nthe proposed algorithm uses the symmetrical properties of indoor artificial\nobjects to estimate the occluded parts to obtain more accurate quadric\nparameters. Experiments have shown that compared with the state-of-art\nalgorithm, especially on the forward trajectory of mobile robots, the proposed\nalgorithm significantly improves the accuracy and convergence speed of quadric\nreconstruction. Finally, we made available an opensource implementation to\nreplicate the experiments.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 04:15:25 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liao", "Ziwei", ""], ["Wang", "Wei", ""], ["Qi", "Xianyu", ""], ["Zhang", "Xiaoyu", ""], ["Xue", "Lin", ""], ["Jiao", "Jianzhen", ""], ["Wei", "Ran", ""]]}, {"id": "2004.05304", "submitter": "Yuenan Hou", "authors": "Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, Chen Change Loy", "title": "Inter-Region Affinity Distillation for Road Marking Segmentation", "comments": "10 pages, 10 figures; This paper is accepted by CVPR 2020; Our code\n  is available at https://github.com/cardwing/Codes-for-IntRA-KD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distilling knowledge from a large deep teacher\nnetwork to a much smaller student network for the task of road marking\nsegmentation. In this work, we explore a novel knowledge distillation (KD)\napproach that can transfer 'knowledge' on scene structure more effectively from\na teacher to a student model. Our method is known as Inter-Region Affinity KD\n(IntRA-KD). It decomposes a given road scene image into different regions and\nrepresents each region as a node in a graph. An inter-region affinity graph is\nthen formed by establishing pairwise relationships between nodes based on their\nsimilarity in feature distribution. To learn structural knowledge from the\nteacher network, the student is required to match the graph generated by the\nteacher. The proposed method shows promising results on three large-scale road\nmarking segmentation benchmarks, i.e., ApolloScape, CULane and LLAMAS, by\ntaking various lightweight models as students and ResNet-101 as the teacher.\nIntRA-KD consistently brings higher performance gains on all lightweight\nmodels, compared to previous distillation methods. Our code is available at\nhttps://github.com/cardwing/Codes-for-IntRA-KD.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 04:26:37 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Hou", "Yuenan", ""], ["Ma", "Zheng", ""], ["Liu", "Chunxiao", ""], ["Hui", "Tak-Wai", ""], ["Loy", "Chen Change", ""]]}, {"id": "2004.05310", "submitter": "Langechuan Liu", "authors": "Xu Dong, Pengluo Wang, Pengyue Zhang, Langechuan Liu", "title": "Probabilistic Oriented Object Detection in Automotive Radar", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous radar has been an integral part of advanced driver assistance\nsystems due to its robustness to adverse weather and various lighting\nconditions. Conventional automotive radars use digital signal processing (DSP)\nalgorithms to process raw data into sparse radar pins that do not provide\ninformation regarding the size and orientation of the objects. In this paper,\nwe propose a deep-learning based algorithm for radar object detection. The\nalgorithm takes in radar data in its raw tensor representation and places\nprobabilistic oriented bounding boxes around the detected objects in\nbird's-eye-view space. We created a new multimodal dataset with 102544 frames\nof raw radar and synchronized LiDAR data. To reduce human annotation effort we\ndeveloped a scalable pipeline to automatically annotate ground truth using\nLiDAR as reference. Based on this dataset we developed a vehicle detection\npipeline using raw radar data as the only input. Our best performing radar\ndetection model achieves 77.28\\% AP under oriented IoU of 0.3. To the best of\nour knowledge, this is the first attempt to investigate object detection with\nraw radar data for conventional corner automotive radars.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 05:29:32 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 03:49:39 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Dong", "Xu", ""], ["Wang", "Pengluo", ""], ["Zhang", "Pengyue", ""], ["Liu", "Langechuan", ""]]}, {"id": "2004.05319", "submitter": "Balamurali Murugesan", "authors": "Balamurali Murugesan, Sricharan Vijayarangan, Kaushik Sarveswaran,\n  Keerthi Ram and Mohanasankar Sivaprakasam", "title": "KD-MRI: A knowledge distillation framework for image reconstruction and\n  image restoration in MRI workflow", "comments": "Accepted in MIDL 2020. Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning networks are being developed in every stage of the MRI workflow\nand have provided state-of-the-art results. However, this has come at the cost\nof increased computation requirement and storage. Hence, replacing the networks\nwith compact models at various stages in the MRI workflow can significantly\nreduce the required storage space and provide considerable speedup. In computer\nvision, knowledge distillation is a commonly used method for model compression.\nIn our work, we propose a knowledge distillation (KD) framework for the image\nto image problems in the MRI workflow in order to develop compact,\nlow-parameter models without a significant drop in performance. We propose a\ncombination of the attention-based feature distillation method and imitation\nloss and demonstrate its effectiveness on the popular MRI reconstruction\narchitecture, DC-CNN. We conduct extensive experiments using Cardiac, Brain,\nand Knee MRI datasets for 4x, 5x and 8x accelerations. We observed that the\nstudent network trained with the assistance of the teacher using our proposed\nKD framework provided significant improvement over the student network trained\nwithout assistance across all the datasets and acceleration factors.\nSpecifically, for the Knee dataset, the student network achieves $65\\%$\nparameter reduction, 2x faster CPU running time, and 1.5x faster GPU running\ntime compared to the teacher. Furthermore, we compare our attention-based\nfeature distillation method with other feature distillation methods. We also\nconduct an ablative study to understand the significance of attention-based\ndistillation and imitation loss. We also extend our KD framework for MRI\nsuper-resolution and show encouraging results.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 06:21:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Murugesan", "Balamurali", ""], ["Vijayarangan", "Sricharan", ""], ["Sarveswaran", "Kaushik", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "2004.05324", "submitter": "Ankita Pasad", "authors": "Ankita Pasad, Ariel Gordon, Tsung-Yi Lin, Anelia Angelova", "title": "Improving Semantic Segmentation through Spatio-Temporal Consistency\n  Learned from Videos", "comments": "Learning from Unlabeled Videos, CVPR Workshop, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage unsupervised learning of depth, egomotion, and camera intrinsics\nto improve the performance of single-image semantic segmentation, by enforcing\n3D-geometric and temporal consistency of segmentation masks across video\nframes. The predicted depth, egomotion, and camera intrinsics are used to\nprovide an additional supervision signal to the segmentation model,\nsignificantly enhancing its quality, or, alternatively, reducing the number of\nlabels the segmentation model needs. Our experiments were performed on the\nScanNet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 07:09:29 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 23:55:36 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Pasad", "Ankita", ""], ["Gordon", "Ariel", ""], ["Lin", "Tsung-Yi", ""], ["Angelova", "Anelia", ""]]}, {"id": "2004.05343", "submitter": "Kuldeep Purohit", "authors": "Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan", "title": "Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion\n  Deblurring", "comments": "Accepted at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of motion deblurring of dynamic scenes.\nAlthough end-to-end fully convolutional designs have recently advanced the\nstate-of-the-art in non-uniform motion deblurring, their performance-complexity\ntrade-off is still sub-optimal. Existing approaches achieve a large receptive\nfield by increasing the number of generic convolution layers and kernel-size,\nbut this comes at the expense of of the increase in model size and inference\nspeed. In this work, we propose an efficient pixel adaptive and feature\nattentive design for handling large blur variations across different spatial\nlocations and process each test image adaptively. We also propose an effective\ncontent-aware global-local filtering module that significantly improves\nperformance by considering not only global dependencies but also by dynamically\nexploiting neighbouring pixel information. We use a patch-hierarchical\nattentive architecture composed of the above module that implicitly discovers\nthe spatial variations in the blur present in the input image and in turn,\nperforms local and global modulation of intermediate features. Extensive\nqualitative and quantitative comparisons with prior art on deblurring\nbenchmarks demonstrate that our design offers significant improvements over the\nstate-of-the-art in accuracy as well as speed.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 09:24:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Suin", "Maitreya", ""], ["Purohit", "Kuldeep", ""], ["Rajagopalan", "A. N.", ""]]}, {"id": "2004.05352", "submitter": "Hyunjae Kim", "authors": "Hyunjae Kim, Yookyung Koh, Jinheon Baek, Jaewoo Kang", "title": "Exploring The Spatial Reasoning Ability of Neural Models in Human IQ\n  Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural models have performed impressively well on various tasks such\nas image recognition and question answering, their reasoning ability has been\nmeasured in only few studies. In this work, we focus on spatial reasoning and\nexplore the spatial understanding of neural models. First, we describe the\nfollowing two spatial reasoning IQ tests: rotation and shape composition. Using\nwell-defined rules, we constructed datasets that consist of various complexity\nlevels. We designed a variety of experiments in terms of generalization, and\nevaluated six different baseline models on the newly generated datasets. We\nprovide an analysis of the results and factors that affect the generalization\nabilities of models. Also, we analyze how neural models solve spatial reasoning\ntests with visual aids. Our findings would provide valuable insights into\nunderstanding a machine and the difference between a machine and human.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 09:41:46 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Kim", "Hyunjae", ""], ["Koh", "Yookyung", ""], ["Baek", "Jinheon", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2004.05381", "submitter": "Sebastian Feld", "authors": "Sebastian Feld (1), Andreas Sedlmeier (1), Markus Friedrich (1), Jan\n  Franz (1), Lenz Belzner (2) ((1) Mobile and Distributed Systems Group LMU\n  Munich, (2) MaibornWolff Munich)", "title": "Bayesian Surprise in Indoor Environments", "comments": "10 pages, 16 figures", "journal-ref": "Proceedings of the 27th ACM SIGSPATIAL International Conference on\n  Advances in Geographic Information Systems (SIGSPATIAL '19), 2019, p. 129-138", "doi": "10.1145/3347146.3359358", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method to identify unexpected structures in 2D\nfloor plans using the concept of Bayesian Surprise. Taking into account that a\nperson's expectation is an important aspect of the perception of space, we\nexploit the theory of Bayesian Surprise to robustly model expectation and thus\nsurprise in the context of building structures. We use Isovist Analysis, which\nis a popular space syntax technique, to turn qualitative object attributes into\nquantitative environmental information. Since isovists are location-specific\npatterns of visibility, a sequence of isovists describes the spatial perception\nduring a movement along multiple points in space. We then use Bayesian Surprise\nin a feature space consisting of these isovist readings. To demonstrate the\nsuitability of our approach, we take \"snapshots\" of an agent's local\nenvironment to provide a short list of images that characterize a traversed\ntrajectory through a 2D indoor environment. Those fingerprints represent\nsurprising regions of a tour, characterize the traversed map and enable indoor\nLBS to focus more on important regions. Given this idea, we propose to use\n\"surprise\" as a new dimension of context in indoor location-based services\n(LBS). Agents of LBS, such as mobile robots or non-player characters in\ncomputer games, may use the context surprise to focus more on important regions\nof a map for a better use or understanding of the floor plan.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:09:51 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Feld", "Sebastian", ""], ["Sedlmeier", "Andreas", ""], ["Friedrich", "Markus", ""], ["Franz", "Jan", ""], ["Belzner", "Lenz", ""]]}, {"id": "2004.05383", "submitter": "Sebastian Feld", "authors": "Sebastian Feld (1), Steffen Illium (1), Andreas Sedlmeier (1), Lenz\n  Belzner (2) ((1) Mobile and Distributed Systems Group LMU Munich, (2)\n  MaibornWolff Munich)", "title": "Trajectory annotation using sequences of spatial perception", "comments": "10 pages, 17 figures", "journal-ref": "Proceedings of the 26th ACM SIGSPATIAL International Conference on\n  Advances in Geographic Information Systems (SIGSPATIAL '18), 2018, p. 329-338", "doi": "10.1145/3274895.3274968", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the near future, more and more machines will perform tasks in the vicinity\nof human spaces or support them directly in their spatially bound activities.\nIn order to simplify the verbal communication and the interaction between\nrobotic units and/or humans, reliable and robust systems w.r.t. noise and\nprocessing results are needed. This work builds a foundation to address this\ntask. By using a continuous representation of spatial perception in interiors\nlearned from trajectory data, our approach clusters movement in dependency to\nits spatial context. We propose an unsupervised learning approach based on a\nneural autoencoding that learns semantically meaningful continuous encodings of\nspatio-temporal trajectory data. This learned encoding can be used to form\nprototypical representations. We present promising results that clear the path\nfor future applications.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:22:27 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Feld", "Sebastian", ""], ["Illium", "Steffen", ""], ["Sedlmeier", "Andreas", ""], ["Belzner", "Lenz", ""]]}, {"id": "2004.05405", "submitter": "Enzo Tartaglione", "authors": "Enzo Tartaglione, Carlo Alberto Barbano, Claudio Berzovini, Marco\n  Calandri and Marco Grangetto", "title": "Unveiling COVID-19 from Chest X-ray with deep learning: a hurdles race\n  with small data", "comments": null, "journal-ref": "Int. J. Environ. Res. Public Health 2020, 17(18), 6933", "doi": "10.3390/ijerph17186933", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility to use widespread and simple chest X-ray (CXR) imaging for\nearly screening of COVID-19 patients is attracting much interest from both the\nclinical and the AI community. In this study we provide insights and also raise\nwarnings on what is reasonable to expect by applying deep-learning to COVID\nclassification of CXR images. We provide a methodological guide and critical\nreading of an extensive set of statistical results that can be obtained using\ncurrently available datasets. In particular, we take the challenge posed by\ncurrent small size COVID data and show how significant can be the bias\nintroduced by transfer-learning using larger public non-COVID CXR datasets. We\nalso contribute by providing results on a medium size COVID CXR dataset, just\ncollected by one of the major emergency hospitals in Northern Italy during the\npeak of the COVID pandemic. These novel data allow us to contribute to validate\nthe generalization capacity of preliminary results circulating in the\nscientific community. Our conclusions shed some light into the possibility to\neffectively discriminate COVID using CXR.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 13:58:17 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tartaglione", "Enzo", ""], ["Barbano", "Carlo Alberto", ""], ["Berzovini", "Claudio", ""], ["Calandri", "Marco", ""], ["Grangetto", "Marco", ""]]}, {"id": "2004.05422", "submitter": "Arash Ashtari", "authors": "Arash Ashtari", "title": "The Role of Stem Noise in Visual Perception and Image Quality\n  Measurement", "comments": "16 pages,19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers reference free quality assessment of distorted and noisy\nimages. Specifically, it considers the first and second order statistics of\nstem noise that can be evaluated given any image. In the research field of\nImage quality Assessment (IQA), the stem noise is defined as the input of an\nAuto Regressive (AR) process, from which a low-energy and de-correlated version\nof the image can be recovered. To estimate the AR model parameters and\nassociated stem noise energy, the Yule-walker equations are used such that the\naccompanying Auto Correlation Function (ACF) coefficients can be treated as\nmodel parameters for image reconstruction. To characterize systematic signal\ndependent and signal independent distortions, the mean and variance of stem\nnoise can be evaluated over the image. Crucially, this paper shows that these\nstatistics have a predictive validity in relation to human ratings of image\nquality. Furthermore, under certain kinds of image distortion, stem noise\nstatistics show very significant correlations with established measures of\nimage quality.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 15:10:21 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ashtari", "Arash", ""]]}, {"id": "2004.05430", "submitter": "Sen Lin", "authors": "Sen Lin, Kaichen Chi", "title": "Underwater Image Enhancement Based on Structure-Texture Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at the problems of color distortion, blur and excessive noise of\nunderwater image, an underwater image enhancement algorithm based on\nstructure-texture reconstruction is proposed. Firstly, the color equalization\nof the degraded image is realized by the automatic color enhancement algorithm;\nSecondly, the relative total variation is introduced to decompose the image\ninto the structure layer and texture layer; Then, the best background light\npoint is selected based on brightness, gradient discrimination, and hue\njudgment, the transmittance of the backscatter component is obtained by the red\ndark channel prior, which is substituted into the imaging model to remove the\nfogging phenomenon in the structure layer. Enhancement of effective details in\nthe texture layer by multi scale detail enhancement algorithm and binary mask;\nFinally, the structure layer and texture layer are reconstructed to get the\nfinal image. The experimental results show that the algorithm can effectively\nbalance the hue, saturation, and clarity of underwater image, and has good\nperformance in different underwater environments.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 15:52:07 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Lin", "Sen", ""], ["Chi", "Kaichen", ""]]}, {"id": "2004.05436", "submitter": "Muhammad Ilyas Dr.", "authors": "Muhammad Ilyas, Hina Rehman and Amine Nait-ali", "title": "Detection of Covid-19 From Chest X-ray Images Using Artificial\n  Intelligence: An Early Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2019, the entire world is facing a situation of health emergency due to a\nnewly emerged coronavirus (COVID-19). Almost 196 countries are affected by\ncovid-19, while USA, Italy, China, Spain, Iran, and France have the maximum\nactive cases of COVID-19. The issues, medical and healthcare departments are\nfacing in delay of detecting the COVID-19. Several artificial intelligence\nbased system are designed for the automatic detection of COVID-19 using chest\nx-rays. In this article we will discuss the different approaches used for the\ndetection of COVID-19 and the challenges we are facing. It is mandatory to\ndevelop an automatic detection system to prevent the transfer of the virus\nthrough contact. Several deep learning architecture are deployed for the\ndetection of COVID-19 such as ResNet, Inception, Googlenet etc. All these\napproaches are detecting the subjects suffering with pneumonia while its hard\nto decide whether the pneumonia is caused by COVID-19 or due to any other\nbacterial or fungal attack.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 16:15:53 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ilyas", "Muhammad", ""], ["Rehman", "Hina", ""], ["Nait-ali", "Amine", ""]]}, {"id": "2004.05471", "submitter": "Burak Uzkent", "authors": "Han Lin Aung, Burak Uzkent, Marshall Burke, David Lobell, Stefano\n  Ermon", "title": "Farmland Parcel Delineation Using Spatio-temporal Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Farm parcel delineation provides cadastral data that is important in\ndeveloping and managing climate change policies. Specifically, farm parcel\ndelineation informs applications in downstream governmental policies of land\nallocation, irrigation, fertilization, green-house gases (GHG's), etc. This\ndata can also be useful for the agricultural insurance sector for assessing\ncompensations following damages associated with extreme weather events - a\ngrowing trend related to climate change. Using satellite imaging can be a\nscalable and cost effective manner to perform the task of farm parcel\ndelineation to collect this valuable data. In this paper, we break down this\ntask using satellite imaging into two approaches: 1) Segmentation of parcel\nboundaries, and 2) Segmentation of parcel areas. We implemented variations of\nUNets, one of which takes into account temporal information, which achieved the\nbest results on our dataset on farmland parcels in France in 2017.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 19:49:09 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 16:34:41 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Aung", "Han Lin", ""], ["Uzkent", "Burak", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "2004.05495", "submitter": "Yanchao Yang", "authors": "Yanchao Yang, Yutong Chen and Stefano Soatto", "title": "Learning to Manipulate Individual Objects in an Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to train a generative model with latent factors that are\n(approximately) independent and localized. This means that perturbing the\nlatent variables affects only local regions of the synthesized image,\ncorresponding to objects. Unlike other unsupervised generative models, ours\nenables object-centric manipulation, without requiring object-level\nannotations, or any form of annotation for that matter. The key to our method\nis the combination of spatial disentanglement, enforced by a Contextual\nInformation Separation loss, and perceptual cycle-consistency, enforced by a\nloss that penalizes changes in the image partition in response to perturbations\nof the latent factors. We test our method's ability to allow independent\ncontrol of spatial and semantic factors of variability on existing datasets and\nalso introduce two new ones that highlight the limitations of current methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 21:50:20 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Yang", "Yanchao", ""], ["Chen", "Yutong", ""], ["Soatto", "Stefano", ""]]}, {"id": "2004.05498", "submitter": "Yanchao Yang", "authors": "Yanchao Yang and Stefano Soatto", "title": "FDA: Fourier Domain Adaptation for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple method for unsupervised domain adaptation, whereby the\ndiscrepancy between the source and target distributions is reduced by swapping\nthe low-frequency spectrum of one with the other. We illustrate the method in\nsemantic segmentation, where densely annotated images are aplenty in one domain\n(synthetic data), but difficult to obtain in another (real images). Current\nstate-of-the-art methods are complex, some requiring adversarial optimization\nto render the backbone of a neural network invariant to the discrete domain\nselection variable. Our method does not require any training to perform the\ndomain alignment, just a simple Fourier Transform and its inverse. Despite its\nsimplicity, it achieves state-of-the-art performance in the current benchmarks,\nwhen integrated into a relatively standard semantic segmentation model. Our\nresults indicate that even simple procedures can discount nuisance variability\nin the data that more sophisticated methods struggle to learn away.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 22:20:48 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Yang", "Yanchao", ""], ["Soatto", "Stefano", ""]]}, {"id": "2004.05507", "submitter": "Ameni Trabelsi", "authors": "Ameni Trabelsi, Mohamed Chaabane, Nathaniel Blanchard and Ross\n  Beveridge", "title": "A Pose Proposal and Refinement Network for Better Object Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel, end-to-end 6D object pose estimation\nmethod that operates on RGB inputs. Our approach is composed of 2 main\ncomponents: the first component classifies the objects in the input image and\nproposes an initial 6D pose estimate through a multi-task, CNN-based\nencoder/multi-decoder module. The second component, a refinement module,\nincludes a renderer and a multi-attentional pose refinement network, which\niteratively refines the estimated poses by utilizing both appearance features\nand flow vectors. Our refiner takes advantage of the hybrid representation of\nthe initial pose estimates to predict the relative errors with respect to the\ntarget poses. It is further augmented by a spatial multi-attention block that\nemphasizes objects' discriminative feature parts. Experiments on three\nbenchmarks for 6D pose estimation show that our proposed pipeline outperforms\nstate-of-the-art RGB-based methods with competitive runtime performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 23:13:54 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 15:41:11 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Trabelsi", "Ameni", ""], ["Chaabane", "Mohamed", ""], ["Blanchard", "Nathaniel", ""], ["Beveridge", "Ross", ""]]}, {"id": "2004.05508", "submitter": "Hancheng Zhu", "authors": "Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi", "title": "MetaIQA: Deep Meta-learning for No-Reference Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, increasing interest has been drawn in exploiting deep convolutional\nneural networks (DCNNs) for no-reference image quality assessment (NR-IQA).\nDespite of the notable success achieved, there is a broad consensus that\ntraining DCNNs heavily relies on massive annotated data. Unfortunately, IQA is\na typical small sample problem. Therefore, most of the existing DCNN-based IQA\nmetrics operate based on pre-trained networks. However, these pre-trained\nnetworks are not designed for IQA task, leading to generalization problem when\nevaluating different types of distortions. With this motivation, this paper\npresents a no-reference IQA metric based on deep meta-learning. The underlying\nidea is to learn the meta-knowledge shared by human when evaluating the quality\nof images with various distortions, which can then be adapted to unknown\ndistortions easily. Specifically, we first collect a number of NR-IQA tasks for\ndifferent distortions. Then meta-learning is adopted to learn the prior\nknowledge shared by diversified distortions. Finally, the quality prior model\nis fine-tuned on a target NR-IQA task for quickly obtaining the quality model.\nExtensive experiments demonstrate that the proposed metric outperforms the\nstate-of-the-arts by a large margin. Furthermore, the meta-model learned from\nsynthetic distortions can also be easily generalized to authentic distortions,\nwhich is highly desired in real-world applications of IQA metrics.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 23:36:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhu", "Hancheng", ""], ["Li", "Leida", ""], ["Wu", "Jinjian", ""], ["Dong", "Weisheng", ""], ["Shi", "Guangming", ""]]}, {"id": "2004.05511", "submitter": "Dung Tran Hoang", "authors": "Hoang-Dung Tran, Stanley Bak, Weiming Xiang and Taylor T.Johnson", "title": "Verification of Deep Convolutional Neural Networks Using ImageStars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have redefined the state-of-the-art in\nmany real-world applications, such as facial recognition, image classification,\nhuman pose estimation, and semantic segmentation. Despite their success, CNNs\nare vulnerable to adversarial attacks, where slight changes to their inputs may\nlead to sharp changes in their output in even well-trained networks. Set-based\nanalysis methods can detect or prove the absence of bounded adversarial\nattacks, which can then be used to evaluate the effectiveness of neural network\ntraining methodology. Unfortunately, existing verification approaches have\nlimited scalability in terms of the size of networks that can be analyzed.\n  In this paper, we describe a set-based framework that successfully deals with\nreal-world CNNs, such as VGG16 and VGG19, that have high accuracy on ImageNet.\nOur approach is based on a new set representation called the ImageStar, which\nenables efficient exact and over-approximative analysis of CNNs. ImageStars\nperform efficient set-based analysis by combining operations on concrete images\nwith linear programming (LP). Our approach is implemented in a tool called NNV,\nand can verify the robustness of VGG networks with respect to a small set of\ninput states, derived from adversarial attacks, such as the DeepFool attack.\nThe experimental results show that our approach is less conservative and faster\nthan existing zonotope methods, such as those used in DeepZ, and the polytope\nmethod used in DeepPoly.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 00:37:21 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 20:02:06 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Tran", "Hoang-Dung", ""], ["Bak", "Stanley", ""], ["Xiang", "Weiming", ""], ["Johnson", "Taylor T.", ""]]}, {"id": "2004.05520", "submitter": "Chen Chen", "authors": "Changlin Li and Taojiannan Yang and Sijie Zhu and Chen Chen and\n  Shanyue Guan", "title": "Density Map Guided Object Detection in Aerial Images", "comments": "CVPR 2020 EarthVision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in high-resolution aerial images is a challenging task\nbecause of 1) the large variation in object size, and 2) non-uniform\ndistribution of objects. A common solution is to divide the large aerial image\ninto small (uniform) crops and then apply object detection on each small crop.\nIn this paper, we investigate the image cropping strategy to address these\nchallenges. Specifically, we propose a Density-Map guided object detection\nNetwork (DMNet), which is inspired from the observation that the object density\nmap of an image presents how objects distribute in terms of the pixel intensity\nof the map. As pixel intensity varies, it is able to tell whether a region has\nobjects or not, which in turn provides guidance for cropping images\nstatistically. DMNet has three key components: a density map generation module,\nan image cropping module and an object detector. DMNet generates a density map\nand learns scale information based on density intensities to form cropping\nregions. Extensive experiments show that DMNet achieves state-of-the-art\nperformance on two popular aerial image datasets, i.e. VisionDrone and UAVDT.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 01:32:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Li", "Changlin", ""], ["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Guan", "Shanyue", ""]]}, {"id": "2004.05523", "submitter": "Yi Ding", "authors": "Yi Ding, Guozheng Wu, Dajiang Chen, Ning Zhang, Linpeng Gong,\n  Mingsheng Cao, Zhiguang Qin", "title": "DeepEDN: A Deep Learning-based Image Encryption and Decryption Network\n  for Internet of Medical Things", "comments": null, "journal-ref": "IEEE Internet of Things Journal,2020", "doi": "10.1109/JIOT.2020.3012452", "report-no": null, "categories": "cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Medical Things (IoMT) can connect many medical imaging equipments\nto the medical information network to facilitate the process of diagnosing and\ntreating for doctors. As medical image contains sensitive information, it is of\nimportance yet very challenging to safeguard the privacy or security of the\npatient. In this work, a deep learning based encryption and decryption network\n(DeepEDN) is proposed to fulfill the process of encrypting and decrypting the\nmedical image. Specifically, in DeepEDN, the Cycle-Generative Adversarial\nNetwork (Cycle-GAN) is employed as the main learning network to transfer the\nmedical image from its original domain into the target domain. Target domain is\nregarded as a \"Hidden Factors\" to guide the learning model for realizing the\nencryption. The encrypted image is restored to the original (plaintext) image\nthrough a reconstruction network to achieve an image decryption. In order to\nfacilitate the data mining directly from the privacy-protected environment, a\nregion of interest(ROI)-mining-network is proposed to extract the interested\nobject from the encrypted image. The proposed DeepEDN is evaluated on the chest\nX-ray dataset. Extensive experimental results and security analysis show that\nthe proposed method can achieve a high level of security with a good\nperformance in efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 01:42:47 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 14:19:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ding", "Yi", ""], ["Wu", "Guozheng", ""], ["Chen", "Dajiang", ""], ["Zhang", "Ning", ""], ["Gong", "Linpeng", ""], ["Cao", "Mingsheng", ""], ["Qin", "Zhiguang", ""]]}, {"id": "2004.05525", "submitter": "Ethan Weber", "authors": "Ethan Weber, Hassan Kan\\'e", "title": "Building Disaster Damage Assessment in Satellite Imagery with\n  Multi-Temporal Fusion", "comments": "Accepted for presentation at the ICLR 2020 AI For Earth Sciences\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic change detection and disaster damage assessment are currently\nprocedures requiring a huge amount of labor and manual work by satellite\nimagery analysts. In the occurrences of natural disasters, timely change\ndetection can save lives. In this work, we report findings on problem framing,\ndata processing and training procedures which are specifically helpful for the\ntask of building damage assessment using the newly released xBD dataset. Our\ninsights lead to substantial improvement over the xBD baseline models, and we\nscore among top results on the xView2 challenge leaderboard. We release our\ncode used for the competition.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 02:06:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Weber", "Ethan", ""], ["Kan\u00e9", "Hassan", ""]]}, {"id": "2004.05529", "submitter": "Fangzhou Mu", "authors": "Fangzhou Mu, Yingyu Liang, Yin Li", "title": "Gradients as Features for Deep Representation Learning", "comments": "ICLR 2020 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of deep representation learning--the\nefficient adaption of a pre-trained deep network to different tasks.\nSpecifically, we propose to explore gradient-based features. These features are\ngradients of the model parameters with respect to a task-specific loss given an\ninput sample. Our key innovation is the design of a linear model that\nincorporates both gradient and activation of the pre-trained network. We show\nthat our model provides a local linear approximation to an underlying deep\nmodel, and discuss important theoretical insights. Moreover, we present an\nefficient algorithm for the training and inference of our model without\ncomputing the actual gradient. Our method is evaluated across a number of\nrepresentation-learning tasks on several datasets and using different network\narchitectures. Strong results are obtained in all settings, and are\nwell-aligned with our theoretical insights.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 02:57:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Mu", "Fangzhou", ""], ["Liang", "Yingyu", ""], ["Li", "Yin", ""]]}, {"id": "2004.05531", "submitter": "Tianyun Zhang", "authors": "Tianyun Zhang, Xiaolong Ma, Zheng Zhan, Shanglin Zhou, Minghai Qin,\n  Fei Sun, Yen-Kuang Chen, Caiwen Ding, Makan Fardad and Yanzhi Wang", "title": "A Unified DNN Weight Compression Framework Using Reweighted Optimization\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the large model size and intensive computation requirement of deep\nneural networks (DNNs), weight pruning techniques have been proposed and\ngenerally fall into two categories, i.e., static regularization-based pruning\nand dynamic regularization-based pruning. However, the former method currently\nsuffers either complex workloads or accuracy degradation, while the latter one\ntakes a long time to tune the parameters to achieve the desired pruning rate\nwithout accuracy loss. In this paper, we propose a unified DNN weight pruning\nframework with dynamically updated regularization terms bounded by the\ndesignated constraint, which can generate both non-structured sparsity and\ndifferent kinds of structured sparsity. We also extend our method to an\nintegrated framework for the combination of different DNN compression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 02:59:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Tianyun", ""], ["Ma", "Xiaolong", ""], ["Zhan", "Zheng", ""], ["Zhou", "Shanglin", ""], ["Qin", "Minghai", ""], ["Sun", "Fei", ""], ["Chen", "Yen-Kuang", ""], ["Ding", "Caiwen", ""], ["Fardad", "Makan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2004.05534", "submitter": "Weibo Huang", "authors": "Weibo Huang, Hong Liu, Weiwei Wan", "title": "Online Initialization and Extrinsic Spatial-Temporal Calibration for\n  Monocular Visual-Inertial Odometry", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an online initialization method for bootstrapping the\noptimization-based monocular visual-inertial odometry (VIO). The method can\nonline calibrate the relative transformation (spatial) and time offsets\n(temporal) among camera and IMU, as well as estimate the initial values of\nmetric scale, velocity, gravity, gyroscope bias, and accelerometer bias during\nthe initialization stage. To compensate for the impact of time offset, our\nmethod includes two short-term motion interpolation algorithms for the camera\nand IMU pose estimation. Besides, it includes a three-step process to\nincrementally estimate the parameters from coarse to fine. First, the extrinsic\nrotation, gyroscope bias, and time offset are estimated by minimizing the\nrotation difference between the camera and IMU. Second, the metric scale,\ngravity, and extrinsic translation are approximately estimated by using the\ncompensated camera poses and ignoring the accelerometer bias. Third, these\nvalues are refined by taking into account the accelerometer bias and the\ngravitational magnitude. For further optimizing the system states, a nonlinear\noptimization algorithm, which considers the time offset, is introduced for\nglobal and local optimization. Experimental results on public datasets show\nthat the initial values and the extrinsic parameters, as well as the sensor\nposes, can be accurately estimated by the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 03:13:08 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Huang", "Weibo", ""], ["Liu", "Hong", ""], ["Wan", "Weiwei", ""]]}, {"id": "2004.05538", "submitter": "Kai Zhu", "authors": "Kai Zhu, Wei Zhai, Zheng-Jun Zha, Yang Cao", "title": "Self-Supervised Tuning for Few-Shot Segmentation", "comments": "Accepted to IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation aims at assigning a category label to each image pixel\nwith few annotated samples. It is a challenging task since the dense prediction\ncan only be achieved under the guidance of latent features defined by sparse\nannotations. Existing meta-learning method tends to fail in generating\ncategory-specifically discriminative descriptor when the visual features\nextracted from support images are marginalized in embedding space. To address\nthis issue, this paper presents an adaptive tuning framework, in which the\ndistribution of latent features across different episodes is dynamically\nadjusted based on a self-segmentation scheme, augmenting category-specific\ndescriptors for label prediction. Specifically, a novel self-supervised\ninner-loop is firstly devised as the base learner to extract the underlying\nsemantic features from the support image. Then, gradient maps are calculated by\nback-propagating self-supervised loss through the obtained features, and\nleveraged as guidance for augmenting the corresponding elements in embedding\nspace. Finally, with the ability to continuously learn from different episodes,\nan optimization-based meta-learner is adopted as outer loop of our proposed\nframework to gradually refine the segmentation results. Extensive experiments\non benchmark PASCAL-$5^{i}$ and COCO-$20^{i}$ datasets demonstrate the\nsuperiority of our proposed method over state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 03:53:53 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 02:52:44 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhu", "Kai", ""], ["Zhai", "Wei", ""], ["Zha", "Zheng-Jun", ""], ["Cao", "Yang", ""]]}, {"id": "2004.05543", "submitter": "Minyoung Chung", "authors": "Minyoung Chung, Jusang Lee, Sanguk Park, Minkyung Lee, Chae Eun Lee,\n  Jeongjin Lee, Yeong-Gil Shin", "title": "Individual Tooth Detection and Identification from Dental Panoramic\n  X-Ray Images via Point-wise Localization and Distance Regularization", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.artmed.2020.101996", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dental panoramic X-ray imaging is a popular diagnostic method owing to its\nvery small dose of radiation. For an automated computer-aided diagnosis system\nin dental clinics, automatic detection and identification of individual teeth\nfrom panoramic X-ray images are critical prerequisites. In this study, we\npropose a point-wise tooth localization neural network by introducing a spatial\ndistance regularization loss. The proposed network initially performs center\npoint regression for all the anatomical teeth (i.e., 32 points), which\nautomatically identifies each tooth. A novel distance regularization penalty is\nemployed on the 32 points by considering $L_2$ regularization loss of Laplacian\non spatial distances. Subsequently, teeth boxes are individually localized\nusing a cascaded neural network on a patch basis. A multitask offset training\nis employed on the final output to improve the localization accuracy. Our\nmethod successfully localizes not only the existing teeth but also missing\nteeth; consequently, highly accurate detection and identification are achieved.\nThe experimental results demonstrate that the proposed algorithm outperforms\nstate-of-the-art approaches by increasing the average precision of teeth\ndetection by 15.71% compared to the best performing method. The accuracy of\nidentification achieved a precision of 0.997 and recall value of 0.972.\nMoreover, the proposed network does not require any additional identification\nalgorithm owing to the preceding regression of the fixed 32 points regardless\nof the existence of the teeth.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 04:14:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chung", "Minyoung", ""], ["Lee", "Jusang", ""], ["Park", "Sanguk", ""], ["Lee", "Minkyung", ""], ["Lee", "Chae Eun", ""], ["Lee", "Jeongjin", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "2004.05551", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Linchao Zhu, Zhiming Luo, Shaozi Li, Yi Yang, Nicu Sebe", "title": "OpenMix: Reviving Known Knowledge for Discovering Novel Visual\n  Categories in An Open World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of discovering new classes in unlabeled\nvisual data given labeled data from disjoint classes. Existing methods\ntypically first pre-train a model with labeled data, and then identify new\nclasses in unlabeled data via unsupervised clustering. However, the labeled\ndata that provide essential knowledge are often underexplored in the second\nstep. The challenge is that the labeled and unlabeled examples are from\nnon-overlapping classes, which makes it difficult to build the learning\nrelationship between them. In this work, we introduce OpenMix to mix the\nunlabeled examples from an open set and the labeled examples from known\nclasses, where their non-overlapping labels and pseudo-labels are\nsimultaneously mixed into a joint label distribution. OpenMix dynamically\ncompounds examples in two ways. First, we produce mixed training images by\nincorporating labeled examples with unlabeled examples. With the benefits of\nunique prior knowledge in novel class discovery, the generated pseudo-labels\nwill be more credible than the original unlabeled predictions. As a result,\nOpenMix helps to prevent the model from overfitting on unlabeled samples that\nmay be assigned with wrong pseudo-labels. Second, the first way encourages the\nunlabeled examples with high class-probabilities to have considerable accuracy.\nWe introduce these examples as reliable anchors and further integrate them with\nunlabeled samples. This enables us to generate more combinations in unlabeled\nexamples and exploit finer object relations among the new classes. Experiments\non three classification datasets demonstrate the effectiveness of the proposed\nOpenMix, which is superior to state-of-the-art methods in novel class\ndiscovery.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 05:52:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhong", "Zhun", ""], ["Zhu", "Linchao", ""], ["Luo", "Zhiming", ""], ["Li", "Shaozi", ""], ["Yang", "Yi", ""], ["Sebe", "Nicu", ""]]}, {"id": "2004.05554", "submitter": "Shaohua Li", "authors": "Shaohua Li, Xiuchao Sui, Jie Fu, Yong Liu, Rick Siow Mong Goh", "title": "Feature Lenses: Plug-and-play Neural Modules for\n  Transformation-Invariant Visual Representations", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are known to be brittle under various\nimage transformations, including rotations, scalings, and changes of lighting\nconditions. We observe that the features of a transformed image are drastically\ndifferent from the ones of the original image. To make CNNs more invariant to\ntransformations, we propose \"Feature Lenses\", a set of ad-hoc modules that can\nbe easily plugged into a trained model (referred to as the \"host model\"). Each\nindividual lens reconstructs the original features given the features of a\ntransformed image under a particular transformation. These lenses jointly\ncounteract feature distortions caused by various transformations, thus making\nthe host model more robust without retraining. By only updating lenses, the\nhost model is freed from iterative updating when facing new transformations\nabsent in the training data; as feature semantics are preserved, downstream\napplications, such as classifiers and detectors, automatically gain robustness\nwithout retraining. Lenses are trained in a self-supervised fashion with no\nannotations, by minimizing a novel \"Top-K Activation Contrast Loss\" between\nlens-transformed features and original features. Evaluated on ImageNet,\nMNIST-rot, and CIFAR-10, Feature Lenses show clear advantages over baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 06:36:15 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Li", "Shaohua", ""], ["Sui", "Xiuchao", ""], ["Fu", "Jie", ""], ["Liu", "Yong", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2004.05560", "submitter": "Feng Xue", "authors": "Feng Xue, Guirong Zhuo, Ziyuan Huang, Wufei Fu, Zhuoyue Wu, Marcelo H.\n  Ang Jr", "title": "Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation\n  for Autonomous Driving Applications", "comments": "8 pages, 10 figures, accepted by 2020 IEEE/RJS International\n  Conference on Intelligent Robots and Systems(IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, self-supervised methods for monocular depth estimation has\nrapidly become an significant branch of depth estimation task, especially for\nautonomous driving applications. Despite the high overall precision achieved,\ncurrent methods still suffer from a) imprecise object-level depth inference and\nb) uncertain scale factor. The former problem would cause texture copy or\nprovide inaccurate object boundary, and the latter would require current\nmethods to have an additional sensor like LiDAR to provide depth ground-truth\nor stereo camera as additional training inputs, which makes them difficult to\nimplement. In this work, we propose to address these two problems together by\nintroducing DNet. Our contributions are twofold: a) a novel dense connected\nprediction (DCP) layer is proposed to provide better object-level depth\nestimation and b) specifically for autonomous driving scenarios, dense\ngeometrical constrains (DGC) is introduced so that precise scale factor can be\nrecovered without additional cost for autonomous vehicles. Extensive\nexperiments have been conducted and, both DCP layer and DGC module are proved\nto be effectively solving the aforementioned problems respectively. Thanks to\nDCP layer, object boundary can now be better distinguished in the depth map and\nthe depth is more continues on object level. It is also demonstrated that the\nperformance of using DGC to perform scale recovery is comparable to that using\nground-truth information, when the camera height is given and the ground point\ntakes up more than 1.03\\% of the pixels. Code is available at\nhttps://github.com/TJ-IPLab/DNet.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 07:57:03 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 10:48:55 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Xue", "Feng", ""], ["Zhuo", "Guirong", ""], ["Huang", "Ziyuan", ""], ["Fu", "Wufei", ""], ["Wu", "Zhuoyue", ""], ["Ang", "Marcelo H.", "Jr"]]}, {"id": "2004.05565", "submitter": "Alvin Wan", "authors": "Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian,\n  Saining Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, Peter Vajda, Joseph E.\n  Gonzalez", "title": "FBNetV2: Differentiable Neural Architecture Search for Spatial and\n  Channel Dimensions", "comments": "8 pages, 10 figures, accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Neural Architecture Search (DNAS) has demonstrated great\nsuccess in designing state-of-the-art, efficient neural networks. However,\nDARTS-based DNAS's search space is small when compared to other search\nmethods', since all candidate network layers must be explicitly instantiated in\nmemory. To address this bottleneck, we propose a memory and computationally\nefficient DNAS variant: DMaskingNAS. This algorithm expands the search space by\nup to $10^{14}\\times$ over conventional DNAS, supporting searches over spatial\nand channel dimensions that are otherwise prohibitively expensive: input\nresolution and number of filters. We propose a masking mechanism for feature\nmap reuse, so that memory and computational costs stay nearly constant as the\nsearch space expands. Furthermore, we employ effective shape propagation to\nmaximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield\nstate-of-the-art performance when compared with all previous architectures.\nWith up to 421$\\times$ less search cost, DMaskingNAS finds models with 0.9%\nhigher accuracy, 15% fewer FLOPs than MobileNetV3-Small; and with similar\naccuracy but 20% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2\noutperforms MobileNetV3 by 2.6% in accuracy, with equivalent model size.\nFBNetV2 models are open-sourced at\nhttps://github.com/facebookresearch/mobile-vision.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 08:52:15 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Wan", "Alvin", ""], ["Dai", "Xiaoliang", ""], ["Zhang", "Peizhao", ""], ["He", "Zijian", ""], ["Tian", "Yuandong", ""], ["Xie", "Saining", ""], ["Wu", "Bichen", ""], ["Yu", "Matthew", ""], ["Xu", "Tao", ""], ["Chen", "Kan", ""], ["Vajda", "Peter", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2004.05571", "submitter": "Pan Zhang", "authors": "Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, Fang Wen", "title": "Cross-domain Correspondence Learning for Exemplar-based Image\n  Translation", "comments": "Accepted as a CVPR 2020 oral paper", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for exemplar-based image translation, which\nsynthesizes a photo-realistic image from the input in a distinct domain (e.g.,\nsemantic segmentation mask, or edge map, or pose keypoints), given an exemplar\nimage. The output has the style (e.g., color, texture) in consistency with the\nsemantically corresponding objects in the exemplar. We propose to jointly learn\nthe crossdomain correspondence and the image translation, where both tasks\nfacilitate each other and thus can be learned with weak supervision. The images\nfrom distinct domains are first aligned to an intermediate domain where dense\ncorrespondence is established. Then, the network synthesizes images based on\nthe appearance of semantically corresponding patches in the exemplar. We\ndemonstrate the effectiveness of our approach in several image translation\ntasks. Our method is superior to state-of-the-art methods in terms of image\nquality significantly, with the image style faithful to the exemplar with\nsemantic consistency. Moreover, we show the utility of our method for several\napplications\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:10:57 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Pan", ""], ["Zhang", "Bo", ""], ["Chen", "Dong", ""], ["Yuan", "Lu", ""], ["Wen", "Fang", ""]]}, {"id": "2004.05573", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Weiying Wang, Ludan Ruan, Linli Yao, Qin Jin", "title": "YouMakeup VQA Challenge: Towards Fine-grained Action Understanding in\n  Domain-Specific Videos", "comments": "CVPR LVVU Workshop 2020 YouMakeup VQA Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the YouMakeup VQA Challenge 2020 is to provide a common benchmark\nfor fine-grained action understanding in domain-specific videos e.g. makeup\ninstructional videos. We propose two novel question-answering tasks to evaluate\nmodels' fine-grained action understanding abilities. The first task is\n\\textbf{Facial Image Ordering}, which aims to understand visual effects of\ndifferent actions expressed in natural language to the facial object. The\nsecond task is \\textbf{Step Ordering}, which aims to measure cross-modal\nsemantic alignments between untrimmed videos and multi-sentence texts. In this\npaper, we present the challenge guidelines, the dataset used, and performances\nof baseline models on the two proposed tasks. The baseline codes and models are\nreleased at \\url{https://github.com/AIM3-RUC/YouMakeup_Baseline}.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:25:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chen", "Shizhe", ""], ["Wang", "Weiying", ""], ["Ruan", "Ludan", ""], ["Yao", "Linli", ""], ["Jin", "Qin", ""]]}, {"id": "2004.05575", "submitter": "Koteswar Rao Jerripothula", "authors": "Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, Junsong Yuan", "title": "Image Co-skeletonization via Co-segmentation", "comments": "13 pages, 12 figures, Submitted to IEEE Transactions on Image\n  Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the joint processing of images have certainly shown its\nadvantages over individual processing. Different from the existing works geared\ntowards co-segmentation or co-localization, in this paper, we explore a new\njoint processing topic: image co-skeletonization, which is defined as joint\nskeleton extraction of objects in an image collection. Object skeletonization\nin a single natural image is a challenging problem because there is hardly any\nprior knowledge about the object. Therefore, we resort to the idea of object\nco-skeletonization, hoping that the commonness prior that exists across the\nimages may help, just as it does for other joint processing problems such as\nco-segmentation. We observe that the skeleton can provide good scribbles for\nsegmentation, and skeletonization, in turn, needs good segmentation. Therefore,\nwe propose a coupled framework for co-skeletonization and co-segmentation tasks\nso that they are well informed by each other, and benefit each other\nsynergistically. Since it is a new problem, we also construct a benchmark\ndataset by annotating nearly 1.8k images spread across 38 categories. Extensive\nexperiments demonstrate that the proposed method achieves promising results in\nall the three possible scenarios of joint-processing: weakly-supervised,\nsupervised, and unsupervised.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:35:54 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jerripothula", "Koteswar Rao", ""], ["Cai", "Jianfei", ""], ["Lu", "Jiangbo", ""], ["Yuan", "Junsong", ""]]}, {"id": "2004.05578", "submitter": "Oliver Werner", "authors": "Oliver Werner, Kimberlin M.H. van Wijnen, Wiro J. Niessen, Marius de\n  Groot, Meike W. Vernooij, Florian Dubost, Marleen de Bruijne", "title": "When Weak Becomes Strong: Robust Quantification of White Matter\n  Hyperintensities in Brain MRI scans", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To measure the volume of specific image structures, a typical approach is to\nfirst segment those structures using a neural network trained on voxel-wise\n(strong) labels and subsequently compute the volume from the segmentation. A\nmore straightforward approach would be to predict the volume directly using a\nneural network based regression approach, trained on image-level (weak) labels\nindicating volume.\n  In this article, we compared networks optimized with weak and strong labels,\nand study their ability to generalize to other datasets. We experimented with\nwhite matter hyperintensity (WMH) volume prediction in brain MRI scans. Neural\nnetworks were trained on a large local dataset and their performance was\nevaluated on four independent public datasets. We showed that networks\noptimized using only weak labels reflecting WMH volume generalized better for\nWMH volume prediction than networks optimized with voxel-wise segmentations of\nWMH. The attention maps of networks trained with weak labels did not seem to\ndelineate WMHs, but highlighted instead areas with smooth contours around or\nnear WMHs. By correcting for possible confounders we showed that networks\ntrained on weak labels may have learnt other meaningful features that are more\nsuited to generalization to unseen data. Our results suggest that for imaging\nbiomarkers that can be derived from segmentations, training networks to predict\nthe biomarker directly may provide more robust results than solving an\nintermediate segmentation step.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 10:14:14 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Werner", "Oliver", ""], ["van Wijnen", "Kimberlin M. H.", ""], ["Niessen", "Wiro J.", ""], ["de Groot", "Marius", ""], ["Vernooij", "Meike W.", ""], ["Dubost", "Florian", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2004.05582", "submitter": "Timo Milbich", "authors": "Timo Milbich and Karsten Roth and Biagio Brattoli and Bj\\\"orn Ommer", "title": "Sharing Matters for Generalization in Deep Metric Learning", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the similarity between images constitutes the foundation for\nnumerous vision tasks. The common paradigm is discriminative metric learning,\nwhich seeks an embedding that separates different training classes. However,\nthe main challenge is to learn a metric that not only generalizes from training\nto novel, but related, test samples. It should also transfer to different\nobject classes. So what complementary information is missed by the\ndiscriminative paradigm? Besides finding characteristics that separate between\nclasses, we also need them to likely occur in novel categories, which is\nindicated if they are shared across training classes. This work investigates\nhow to learn such characteristics without the need for extra annotations or\ntraining data. By formulating our approach as a novel triplet sampling\nstrategy, it can be easily applied on top of recent ranking loss frameworks.\nExperiments show that, independent of the underlying network architecture and\nthe specific ranking loss, our approach significantly improves performance in\ndeep metric learning, leading to new the state-of-the-art results on various\nstandard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 10:21:15 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 14:17:01 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Milbich", "Timo", ""], ["Roth", "Karsten", ""], ["Brattoli", "Biagio", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2004.05595", "submitter": "Toru Tamaki", "authors": "Kento Terao, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda, Shun'ichi\n  Satoh", "title": "Which visual questions are difficult to answer? Analysis with Entropy of\n  Answer Distributions", "comments": "accepted by IEEE access available at\n  https://doi.org/10.1109/ACCESS.2020.3022063 as \"An Entropy Clustering\n  Approach for Assessing Visual Question Difficulty\"", "journal-ref": "IEEE Access, Vol.8, pp. 180633-180645, Sep 2020", "doi": "10.1109/ACCESS.2020.3022063", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach to identify the difficulty of visual questions\nfor Visual Question Answering (VQA) without direct supervision or annotations\nto the difficulty. Prior works have considered the diversity of ground-truth\nanswers of human annotators. In contrast, we analyze the difficulty of visual\nquestions based on the behavior of multiple different VQA models. We propose to\ncluster the entropy values of the predicted answer distributions obtained by\nthree different models: a baseline method that takes as input images and\nquestions, and two variants that take as input images only and questions only.\nWe use a simple k-means to cluster the visual questions of the VQA v2\nvalidation set. Then we use state-of-the-art methods to determine the accuracy\nand the entropy of the answer distributions for each cluster. A benefit of the\nproposed method is that no annotation of the difficulty is required, because\nthe accuracy of each cluster reflects the difficulty of visual questions that\nbelong to it. Our approach can identify clusters of difficult visual questions\nthat are not answered correctly by state-of-the-art methods. Detailed analysis\non the VQA v2 dataset reveals that 1) all methods show poor performances on the\nmost difficult cluster (about 10% accuracy), 2) as the cluster difficulty\nincreases, the answers predicted by the different methods begin to differ, and\n3) the values of cluster entropy are highly correlated with the cluster\naccuracy. We show that our approach has the advantage of being able to assess\nthe difficulty of visual questions without ground-truth (i.e. the test set of\nVQA v2) by assigning them to one of the clusters. We expect that this can\nstimulate the development of novel directions of research and new algorithms.\nClustering results are available online at https://github.com/tttamaki/vqd .\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 12:06:03 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 22:40:58 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Terao", "Kento", ""], ["Tamaki", "Toru", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Satoh", "Shun'ichi", ""]]}, {"id": "2004.05640", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Haoran Deng, Xiaoyang Huang, Bingbing Ni, Yi Xu", "title": "Relational Learning between Multiple Pulmonary Nodules via Deep Set\n  Attention Transformers", "comments": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI\n  2020)", "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098722", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis and treatment of multiple pulmonary nodules are clinically\nimportant but challenging. Prior studies on nodule characterization use\nsolitary-nodule approaches on multiple nodular patients, which ignores the\nrelations between nodules. In this study, we propose a multiple instance\nlearning (MIL) approach and empirically prove the benefit to learn the\nrelations between multiple nodules. By treating the multiple nodules from a\nsame patient as a whole, critical relational information between\nsolitary-nodule voxels is extracted. To our knowledge, it is the first study to\nlearn the relations between multiple pulmonary nodules. Inspired by recent\nadvances in natural language processing (NLP) domain, we introduce a\nself-attention transformer equipped with 3D CNN, named {NoduleSAT}, to replace\ntypical pooling-based aggregation in multiple instance learning. Extensive\nexperiments on lung nodule false positive reduction on LUNA16 database, and\nmalignancy classification on LIDC-IDRI database, validate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 16:05:08 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Yang", "Jiancheng", ""], ["Deng", "Haoran", ""], ["Huang", "Xiaoyang", ""], ["Ni", "Bingbing", ""], ["Xu", "Yi", ""]]}, {"id": "2004.05645", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen, Lina Yao, Yu Zhang", "title": "Residual Attention U-Net for Automated Multi-Class Segmentation of\n  COVID-19 Chest CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel coronavirus disease 2019 (COVID-19) has been spreading rapidly\naround the world and caused significant impact on the public health and\neconomy. However, there is still lack of studies on effectively quantifying the\nlung infection caused by COVID-19. As a basic but challenging task of the\ndiagnostic framework, segmentation plays a crucial role in accurate\nquantification of COVID-19 infection measured by computed tomography (CT)\nimages. To this end, we proposed a novel deep learning algorithm for automated\nsegmentation of multiple COVID-19 infection regions. Specifically, we use the\nAggregated Residual Transformations to learn a robust and expressive feature\nrepresentation and apply the soft attention mechanism to improve the capability\nof the model to distinguish a variety of symptoms of the COVID-19. With a\npublic CT image dataset, we validate the efficacy of the proposed algorithm in\ncomparison with other competing methods. Experimental results demonstrate the\noutstanding performance of our algorithm for automated segmentation of COVID-19\nChest CT images. Our study provides a promising deep leaning-based segmentation\ntool to lay a foundation to quantitative diagnosis of COVID-19 lung infection\nin CT images.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 16:24:59 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chen", "Xiaocong", ""], ["Yao", "Lina", ""], ["Zhang", "Yu", ""]]}, {"id": "2004.05679", "submitter": "Qian Xie", "authors": "Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, Jun\n  Wang", "title": "MLCVNet: Multi-Level Context VoteNet for 3D Object Detection", "comments": "To be presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the 3D object detection task by capturing\nmulti-level contextual information with the self-attention mechanism and\nmulti-scale feature fusion. Most existing 3D object detection methods recognize\nobjects individually, without giving any consideration on contextual\ninformation between these objects. Comparatively, we propose Multi-Level\nContext VoteNet (MLCVNet) to recognize 3D objects correlatively, building on\nthe state-of-the-art VoteNet. We introduce three context modules into the\nvoting and classifying stages of VoteNet to encode contextual information at\ndifferent levels. Specifically, a Patch-to-Patch Context (PPC) module is\nemployed to capture contextual information between the point patches, before\nvoting for their corresponding object centroid points. Subsequently, an\nObject-to-Object Context (OOC) module is incorporated before the proposal and\nclassification stage, to capture the contextual information between object\ncandidates. Finally, a Global Scene Context (GSC) module is designed to learn\nthe global scene context. We demonstrate these by capturing contextual\ninformation at patch, object and scene levels. Our method is an effective way\nto promote detection accuracy, achieving new state-of-the-art detection\nperformance on challenging 3D object detection datasets, i.e., SUN RGBD and\nScanNet. We also release our code at https://github.com/NUAAXQ/MLCVNet.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 19:10:24 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Xie", "Qian", ""], ["Lai", "Yu-Kun", ""], ["Wu", "Jing", ""], ["Wang", "Zhoutao", ""], ["Zhang", "Yiming", ""], ["Xu", "Kai", ""], ["Wang", "Jun", ""]]}, {"id": "2004.05682", "submitter": "Chenglin Yang", "authors": "Chenglin Yang, Adam Kortylewski, Cihang Xie, Yinzhi Cao, and Alan\n  Yuille", "title": "PatchAttack: A Black-box Texture-based Attack with Reinforcement\n  Learning", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-based attacks introduce a perceptible but localized change to the input\nthat induces misclassification. A limitation of current patch-based black-box\nattacks is that they perform poorly for targeted attacks, and even for the less\nchallenging non-targeted scenarios, they require a large number of queries. Our\nproposed PatchAttack is query efficient and can break models for both targeted\nand non-targeted attacks. PatchAttack induces misclassifications by\nsuperimposing small textured patches on the input image. We parametrize the\nappearance of these patches by a dictionary of class-specific textures. This\ntexture dictionary is learned by clustering Gram matrices of feature\nactivations from a VGG backbone. PatchAttack optimizes the position and texture\nparameters of each patch using reinforcement learning. Our experiments show\nthat PatchAttack achieves > 99% success rate on ImageNet for a wide range of\narchitectures, while only manipulating 3% of the image for non-targeted attacks\nand 10% on average for targeted attacks. Furthermore, we show that PatchAttack\ncircumvents state-of-the-art adversarial defense methods successfully.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 19:31:09 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 22:36:25 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yang", "Chenglin", ""], ["Kortylewski", "Adam", ""], ["Xie", "Cihang", ""], ["Cao", "Yinzhi", ""], ["Yuille", "Alan", ""]]}, {"id": "2004.05685", "submitter": "Mertcan Cokbas", "authors": "Mertcan Cokbas, Prakash Ishwar, Janusz Konrad", "title": "Low-Resolution Overhead Thermal Tripwire for Occupancy Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart buildings use occupancy sensing for various tasks ranging from\nenergy-efficient HVAC and lighting to space-utilization analysis and emergency\nresponse. We propose a people counting system which uses a low-resolution\nthermal sensor. Unlike previous people-counting systems based on thermal\nsensors, we use an overhead tripwire configuration at entryways to detect and\ntrack transient entries or exits. We develop two distinct people counting\nalgorithms for this configuration. To evaluate our algorithms, we have\ncollected and labeled a low-resolution thermal video dataset using the proposed\nsystem. The dataset, the first of its kind, is public and available for\ndownload. We also propose new evaluation metrics that are more suitable for\nsystems that are subject to drift and jitter.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 19:48:19 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 19:18:19 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 04:23:14 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 16:30:08 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Cokbas", "Mertcan", ""], ["Ishwar", "Prakash", ""], ["Konrad", "Janusz", ""]]}, {"id": "2004.05698", "submitter": "Sharmin Pathan", "authors": "Sharmin Pathan, Anant Tripathi", "title": "Y-net: Biomedical Image Segmentation and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep clustering architecture alongside image segmentation for\nmedical image analysis. The main idea is based on unsupervised learning to\ncluster images on severity of the disease in the subject's sample, and this\nimage is then segmented to highlight and outline regions of interest. We start\nwith training an autoencoder on the images for segmentation. The encoder part\nfrom the autoencoder branches out to a clustering node and segmentation node.\nDeep clustering using Kmeans clustering is performed at the clustering branch\nand a lightweight model is used for segmentation. Each of the branches use\nextracted features from the autoencoder. We demonstrate our results on ISIC\n2018 Skin Lesion Analysis Towards Melanoma Detection and Cityscapes datasets\nfor segmentation and clustering. The proposed architecture beats UNet and\nDeepLab results on the two datasets, and has less than half the number of\nparameters. We use the deep clustering branch for clustering images into four\nclusters. Our approach can be applied to work with high complexity datasets of\nmedical imaging for analyzing survival prediction for severe diseases or\ncustomizing treatment based on how far the disease has propagated. Clustering\npatients can help understand how binning should be done on real valued features\nto reduce feature sparsity and improve accuracy on classification tasks. The\nproposed architecture can provide an early diagnosis and reduce human\nintervention on labeling as it can become quite costly as the datasets grow\nlarger. The main idea is to propose a one shot approach to segmentation with\ndeep clustering.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 21:08:31 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 02:08:16 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Pathan", "Sharmin", ""], ["Tripathi", "Anant", ""]]}, {"id": "2004.05704", "submitter": "Robik Shrestha", "authors": "Robik Shrestha, Kushal Kafle, Christopher Kanan", "title": "A negative case analysis of visual grounding methods for VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 21:45:23 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 17:38:04 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Shrestha", "Robik", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "2004.05713", "submitter": "Juan Castorena", "authors": "Juan Castorena, Manish Bhattarai, Diane Oyen", "title": "Learning Spatial Relationships between Samples of Patent Image Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary image based classification and retrieval of documents of an\nintellectual nature is a very challenging problem. Variations in the binary\nimage generation mechanisms which are subject to the document artisan designer\nincluding drawing style, view-point, inclusion of multiple image components are\nplausible causes for increasing the complexity of the problem. In this work, we\npropose a method suitable to binary images which bridges some of the successes\nof deep learning (DL) to alleviate the problems introduced by the\naforementioned variations. The method consists on extracting the shape of\ninterest from the binary image and applying a non-Euclidean geometric\nneural-net architecture to learn the local and global spatial relationships of\nthe shape. Empirical results show that our method is in some sense invariant to\nthe image generation mechanism variations and achieves results outperforming\nexisting methods in a patent image dataset benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:05:19 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 20:07:38 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 14:26:30 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Castorena", "Juan", ""], ["Bhattarai", "Manish", ""], ["Oyen", "Diane", ""]]}, {"id": "2004.05717", "submitter": "Eduardo Jos\\'e Da Silva Luz", "authors": "Eduardo Luz, Pedro Lopes Silva, Rodrigo Silva, Ludmila Silva, Gladston\n  Moreira and David Menotti", "title": "Towards an Effective and Efficient Deep Learning Model for COVID-19\n  Patterns Detection in X-ray Images", "comments": "This is a preprint of an article published in Research on Biomedical\n  Engineering. The final authenticated version is available online at\n  https://doi.org/10.1007/s42600-021-00151-6", "journal-ref": null, "doi": "10.1007/s42600-021-00151-6", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confronting the pandemic of COVID-19, is nowadays one of the most prominent\nchallenges of the human species. A key factor in slowing down the virus\npropagation is the rapid diagnosis and isolation of infected patients. The\nstandard method for COVID-19 identification, the Reverse transcription\npolymerase chain reaction method, is time-consuming and in short supply due to\nthe pandemic. Thus, researchers have been looking for alternative screening\nmethods and deep learning applied to chest X-rays of patients has been showing\npromising results. Despite their success, the computational cost of these\nmethods remains high, which imposes difficulties to their accessibility and\navailability. Thus, the main goal of this work is to propose an accurate yet\nefficient method in terms of memory and processing time for the problem of\nCOVID-19 screening in chest X-rays. Methods: To achieve the defined objective\nwe exploit and extend the EfficientNet family of deep artificial neural\nnetworks which are known for their high accuracy and low footprints in other\napplications. We also exploit the underlying taxonomy of the problem with a\nhierarchical classifier. A dataset of 13,569 X-ray images divided into healthy,\nnon-COVID-19 pneumonia, and COVID-19 patients is used to train the proposed\napproaches and other 5 competing architectures. Finally, 231 images of the\nthree classes were used to assess the quality of the methods. Results: The\nresults show that the proposed approach was able to produce a high-quality\nmodel, with an overall accuracy of 93.9%, COVID-19, sensitivity of 96.8% and\npositive prediction of 100%, while having from 5 to 30 times fewer parameters\nthan other than the other tested architectures. Larger and more heterogeneous\ndatabases are still needed for validation before claiming that deep learning\ncan assist physicians in the task of detecting COVID-19 in X-ray images.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:26:56 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 03:32:48 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 19:03:46 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 15:43:10 GMT"}, {"version": "v5", "created": "Sat, 24 Apr 2021 12:36:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Luz", "Eduardo", ""], ["Silva", "Pedro Lopes", ""], ["Silva", "Rodrigo", ""], ["Silva", "Ludmila", ""], ["Moreira", "Gladston", ""], ["Menotti", "David", ""]]}, {"id": "2004.05718", "submitter": "Gabriele Corso", "authors": "Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\\`o, Petar\n  Veli\\v{c}kovi\\'c", "title": "Principal Neighbourhood Aggregation for Graph Nets", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been shown to be effective models for\ndifferent predictive tasks on graph-structured data. Recent work on their\nexpressive power has focused on isomorphism tasks and countable feature spaces.\nWe extend this theoretical framework to include continuous features - which\noccur regularly in real-world input domains and within the hidden layers of\nGNNs - and we demonstrate the requirement for multiple aggregation functions in\nthis context. Accordingly, we propose Principal Neighbourhood Aggregation\n(PNA), a novel architecture combining multiple aggregators with degree-scalers\n(which generalize the sum aggregator). Finally, we compare the capacity of\ndifferent models to capture and exploit the graph structure via a novel\nbenchmark containing multiple tasks taken from classical graph theory,\nalongside existing benchmarks from real-world domains, all of which demonstrate\nthe strength of our model. With this work, we hope to steer some of the GNN\nresearch towards new aggregation methods which we believe are essential in the\nsearch for powerful and robust models.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:30:00 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 16:33:10 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 15:40:07 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 16:36:18 GMT"}, {"version": "v5", "created": "Thu, 31 Dec 2020 08:23:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Corso", "Gabriele", ""], ["Cavalleri", "Luca", ""], ["Beaini", "Dominique", ""], ["Li\u00f2", "Pietro", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2004.05731", "submitter": "Lavanya Umapathy", "authors": "Lavanya Umapathy (1 and 2), Mahesh Bharath Keerthivasan (1 and 2),\n  Jean-Phillipe Galons (2), Wyatt Unger (2), Diego Martin (2), Maria I Altbach\n  (2) and Ali Bilgin (1 and 2 and 3) ((1) Department of Electrical and Computer\n  Engineering, University of Arizona, Tucson, Arizona, (2) Department of\n  Medical Imaging, University of Arizona, Tucson, Arizona, (3) Department of\n  Biomedical Engineering, University of Arizona, Tucson, Arizona)", "title": "A Comparison of Deep Learning Convolution Neural Networks for Liver\n  Segmentation in Radial Turbo Spin Echo Images", "comments": "3 pages, 4 figures, 1 table. Published in Proceedings of\n  International Society for Magnetic Resonance in Medicine 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion-robust 2D Radial Turbo Spin Echo (RADTSE) pulse sequence can provide a\nhigh-resolution composite image, T2-weighted images at multiple echo times\n(TEs), and a quantitative T2 map, all from a single k-space acquisition. In\nthis work, we use a deep-learning convolutional neural network (CNN) for the\nsegmentation of liver in abdominal RADTSE images. A modified UNET architecture\nwith generalized dice loss objective function was implemented. Three 2D CNNs\nwere trained, one for each image type obtained from the RADTSE sequence. On\nevaluating the performance of the CNNs on the validation set, we found that\nCNNs trained on TE images or the T2 maps had higher average dice scores than\nthe composite images. This, in turn, implies that the information regarding T2\nvariation in tissues aids in improving the segmentation performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 00:19:02 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Umapathy", "Lavanya", "", "1 and 2"], ["Keerthivasan", "Mahesh Bharath", "", "1 and 2"], ["Galons", "Jean-Phillipe", "", "1 and 2 and 3"], ["Unger", "Wyatt", "", "1 and 2 and 3"], ["Martin", "Diego", "", "1 and 2 and 3"], ["Altbach", "Maria I", "", "1 and 2 and 3"], ["Bilgin", "Ali", "", "1 and 2 and 3"]]}, {"id": "2004.05745", "submitter": "Hongruixuan Chen", "authors": "Hongruixuan Chen and Chen Wu and Bo Du and Liangepei Zhang", "title": "Deep Siamese Domain Adaptation Convolutional Neural Network for\n  Cross-domain Change Detection in Multispectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has achieved promising performance in the change\ndetection task. However, the deep models are task-specific and data set bias\noften exists, thus it is difficult to transfer a network trained on one\nmulti-temporal data set (source domain) to another multi-temporal data set with\nvery limited (even no) labeled data (target domain). In this paper, we propose\na novel deep siamese domain adaptation convolutional neural network (DSDANet)\narchitecture for cross-domain change detection. In DSDANet, a siamese\nconvolutional neural network first extracts spatial-spectral features from\nmulti-temporal images. Then, through multiple kernel maximum mean discrepancy\n(MK-MMD), the learned feature representation is embedded into a reproducing\nkernel Hilbert space (RKHS), in which the distribution of two domains can be\nexplicitly matched. By optimizing the network parameters and kernel\ncoefficients with the source labeled data and target unlabeled data, the\nDSDANet can learn transferrable feature representation that can bridge the\ndiscrepancy between two domains. To the best of our knowledge, it is the first\ntime that such a domain adaptation-based deep network is proposed for change\ndetection. The theoretical analysis and experimental results demonstrate the\neffectiveness and potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 02:15:04 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chen", "Hongruixuan", ""], ["Wu", "Chen", ""], ["Du", "Bo", ""], ["Zhang", "Liangepei", ""]]}, {"id": "2004.05746", "submitter": "Mohammad Farhadi Bajestani", "authors": "Mohammad Farhadi Bajestani, Mehdi Ghasemi, Sarma Vrudhula and Yezhou\n  Yang", "title": "Enabling Incremental Knowledge Transfer for Object Detection at the Edge", "comments": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshop (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection using deep neural networks (DNNs) involves a huge amount of\ncomputation which impedes its implementation on resource/energy-limited\nuser-end devices. The reason for the success of DNNs is due to having knowledge\nover all different domains of observed environments. However, we need a limited\nknowledge of the observed environment at inference time which can be learned\nusing a shallow neural network (SHNN). In this paper, a system-level design is\nproposed to improve the energy consumption of object detection on the user-end\ndevice. An SHNN is deployed on the user-end device to detect objects in the\nobserving environment. Also, a knowledge transfer mechanism is implemented to\nupdate the SHNN model using the DNN knowledge when there is a change in the\nobject domain. DNN knowledge can be obtained from a powerful edge device\nconnected to the user-end device through LAN or Wi-Fi. Experiments demonstrate\nthat the energy consumption of the user-end device and the inference time can\nbe improved by 78% and 71% compared with running the deep model on the user-end\ndevice.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 02:19:18 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 13:31:22 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bajestani", "Mohammad Farhadi", ""], ["Ghasemi", "Mehdi", ""], ["Vrudhula", "Sarma", ""], ["Yang", "Yezhou", ""]]}, {"id": "2004.05749", "submitter": "Longlong Jing", "authors": "Longlong Jing, Yucheng Chen, Ling Zhang, Mingyi He, Yingli Tian", "title": "Self-supervised Feature Learning by Cross-modality and Cross-view\n  Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of supervised learning requires large-scale ground truth labels\nwhich are very expensive, time-consuming, or may need special skills to\nannotate. To address this issue, many self- or un-supervised methods are\ndeveloped. Unlike most existing self-supervised methods to learn only 2D image\nfeatures or only 3D point cloud features, this paper presents a novel and\neffective self-supervised learning approach to jointly learn both 2D image\nfeatures and 3D point cloud features by exploiting cross-modality and\ncross-view correspondences without using any human annotated labels.\nSpecifically, 2D image features of rendered images from different views are\nextracted by a 2D convolutional neural network, and 3D point cloud features are\nextracted by a graph convolution neural network. Two types of features are fed\ninto a two-layer fully connected neural network to estimate the cross-modality\ncorrespondence. The three networks are jointly trained (i.e. cross-modality) by\nverifying whether two sampled data of different modalities belong to the same\nobject, meanwhile, the 2D convolutional neural network is additionally\noptimized through minimizing intra-object distance while maximizing\ninter-object distance of rendered images in different views (i.e. cross-view).\nThe effectiveness of the learned 2D and 3D features is evaluated by\ntransferring them on five different tasks including multi-view 2D shape\nrecognition, 3D shape recognition, multi-view 2D shape retrieval, 3D shape\nretrieval, and 3D part-segmentation. Extensive evaluations on all the five\ndifferent tasks across different datasets demonstrate strong generalization and\neffectiveness of the learned 2D and 3D features by the proposed self-supervised\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 02:57:25 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jing", "Longlong", ""], ["Chen", "Yucheng", ""], ["Zhang", "Ling", ""], ["He", "Mingyi", ""], ["Tian", "Yingli", ""]]}, {"id": "2004.05758", "submitter": "Jong Chul Ye", "authors": "Yujin Oh, Sangjoon Park, Jong Chul Ye", "title": "Deep Learning COVID-19 Features on CXR using Limited Training Data Sets", "comments": "Accepted for IEEE Trans. on Medical Imaging Special Issue on\n  Imaging-based Diagnosis of COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the global pandemic of COVID-19, the use of artificial intelligence to\nanalyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is\nbecoming important. Unfortunately, due to the emergent nature of the COVID-19\npandemic, a systematic collection of the CXR data set for deep neural network\ntraining is difficult. To address this problem, here we propose a patch-based\nconvolutional neural network approach with a relatively small number of\ntrainable parameters for COVID-19 diagnosis. The proposed method is inspired by\nour statistical analysis of the potential imaging biomarkers of the CXR\nradiographs. Experimental results show that our method achieves\nstate-of-the-art performance and provides clinically interpretable saliency\nmaps, which are useful for COVID-19 diagnosis and patient triage.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 03:44:42 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 16:07:25 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Oh", "Yujin", ""], ["Park", "Sangjoon", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2004.05763", "submitter": "Jing Zhang", "authors": "Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat\n  Saleh, Tong Zhang, Nick Barnes", "title": "UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional\n  Variational Autoencoders", "comments": "Accepted by IEEE CVPR 2020 (ORAL). Code:\n  https://github.com/JingZhang617/UCNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first framework (UCNet) to employ uncertainty\nfor RGB-D saliency detection by learning from the data labeling process.\nExisting RGB-D saliency detection methods treat the saliency detection task as\na point estimation problem, and produce a single saliency map following a\ndeterministic learning pipeline. Inspired by the saliency data labeling\nprocess, we propose probabilistic RGB-D saliency detection network via\nconditional variational autoencoders to model human annotation uncertainty and\ngenerate multiple saliency maps for each input image by sampling in the latent\nspace. With the proposed saliency consensus process, we are able to generate an\naccurate saliency map based on these multiple predictions. Quantitative and\nqualitative evaluations on six challenging benchmark datasets against 18\ncompeting algorithms demonstrate the effectiveness of our approach in learning\nthe distribution of saliency maps, leading to a new state-of-the-art in RGB-D\nsaliency detection.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 04:12:59 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Jing", ""], ["Fan", "Deng-Ping", ""], ["Dai", "Yuchao", ""], ["Anwar", "Saeed", ""], ["Saleh", "Fatemeh Sadat", ""], ["Zhang", "Tong", ""], ["Barnes", "Nick", ""]]}, {"id": "2004.05790", "submitter": "Yaoyao Zhong", "authors": "Yaoyao Zhong and Weihong Deng", "title": "Towards Transferable Adversarial Attack against Deep Face Recognition", "comments": "This article has been accepted by IEEE Transactions on Information\n  Forensics and Security. TALFW database is available at\n  http://www.whdeng.cn/TALFW/index.html", "journal-ref": null, "doi": "10.1109/TIFS.2020.3036801", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has achieved great success in the last five years due to the\ndevelopment of deep learning methods. However, deep convolutional neural\nnetworks (DCNNs) have been found to be vulnerable to adversarial examples. In\nparticular, the existence of transferable adversarial examples can severely\nhinder the robustness of DCNNs since this type of attacks can be applied in a\nfully black-box manner without queries on the target system. In this work, we\nfirst investigate the characteristics of transferable adversarial attacks in\nface recognition by showing the superiority of feature-level methods over\nlabel-level methods. Then, to further improve transferability of feature-level\nadversarial examples, we propose DFANet, a dropout-based method used in\nconvolutional layers, which can increase the diversity of surrogate models and\nobtain ensemble-like effects. Extensive experiments on state-of-the-art face\nmodels with various training databases, loss functions and network\narchitectures show that the proposed method can significantly enhance the\ntransferability of existing attack methods. Finally, by applying DFANet to the\nLFW database, we generate a new set of adversarial face pairs that can\nsuccessfully attack four commercial APIs without any queries. This TALFW\ndatabase is available to facilitate research on the robustness and defense of\ndeep face recognition.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 06:44:33 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 14:07:04 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhong", "Yaoyao", ""], ["Deng", "Weihong", ""]]}, {"id": "2004.05794", "submitter": "Zhe Jiang", "authors": "Zhe Jiang, Yu Zhang, Dongqing Zou, Jimmy Ren, Jiancheng Lv, Yebin Liu", "title": "Learning Event-Based Motion Deblurring", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering sharp video sequence from a motion-blurred image is highly\nill-posed due to the significant loss of motion information in the blurring\nprocess. For event-based cameras, however, fast motion can be captured as\nevents at high time rate, raising new opportunities to exploring effective\nsolutions. In this paper, we start from a sequential formulation of event-based\nmotion deblurring, then show how its optimization can be unfolded with a novel\nend-to-end deep architecture. The proposed architecture is a convolutional\nrecurrent neural network that integrates visual and temporal knowledge of both\nglobal and local scales in principled manner. To further improve the\nreconstruction, we propose a differentiable directional event filtering module\nto effectively extract rich boundary prior from the stream of events. We\nconduct extensive experiments on the synthetic GoPro dataset and a large newly\nintroduced dataset captured by a DAVIS240C camera. The proposed approach\nachieves state-of-the-art reconstruction quality, and generalizes better to\nhandling real-world motion blur.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:01:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jiang", "Zhe", ""], ["Zhang", "Yu", ""], ["Zou", "Dongqing", ""], ["Ren", "Jimmy", ""], ["Lv", "Jiancheng", ""], ["Liu", "Yebin", ""]]}, {"id": "2004.05795", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai and Nuno Vasconcelos", "title": "Rethinking Differentiable Search for Mixed-Precision Neural Networks", "comments": "accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision networks, with weights and activations quantized to low\nbit-width, are widely used to accelerate inference on edge devices. However,\ncurrent solutions are uniform, using identical bit-width for all filters. This\nfails to account for the different sensitivities of different filters and is\nsuboptimal. Mixed-precision networks address this problem, by tuning the\nbit-width to individual filter requirements. In this work, the problem of\noptimal mixed-precision network search (MPS) is considered. To circumvent its\ndifficulties of discrete search space and combinatorial optimization, a new\ndifferentiable search architecture is proposed, with several novel\ncontributions to advance the efficiency by leveraging the unique properties of\nthe MPS problem. The resulting Efficient differentiable MIxed-Precision network\nSearch (EdMIPS) method is effective at finding the optimal bit allocation for\nmultiple popular networks, and can search a large model, e.g. Inception-V3,\ndirectly on ImageNet without proxy task in a reasonable amount of time. The\nlearned mixed-precision networks significantly outperform their uniform\ncounterparts.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:02:23 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Cai", "Zhaowei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2004.05804", "submitter": "Weihong Quan", "authors": "Haoran Li, Weihong Quan, Meijun Yan, Jin zhang, Xiaoli Gong and Jin\n  Zhou", "title": "Multi-modal Datasets for Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowdays, most datasets used to train and evaluate super-resolution models are\nsingle-modal simulation datasets. However, due to the variety of image\ndegradation types in the real world, models trained on single-modal simulation\ndatasets do not always have good robustness and generalization ability in\ndifferent degradation scenarios. Previous work tended to focus only on\ntrue-color images. In contrast, we first proposed real-world black-and-white\nold photo datasets for super-resolution (OID-RW), which is constructed using\ntwo methods of manually filling pixels and shooting with different cameras. The\ndataset contains 82 groups of images, including 22 groups of character type and\n60 groups of landscape and architecture. At the same time, we also propose a\nmulti-modal degradation dataset (MDD400) to solve the super-resolution\nreconstruction in real-life image degradation scenarios. We managed to simulate\nthe process of generating degraded images by the following four methods:\ninterpolation algorithm, CNN network, GAN network and capturing videos with\ndifferent bit rates. Our experiments demonstrate that not only the models\ntrained on our dataset have better generalization capability and robustness,\nbut also the trained images can maintain better edge contours and texture\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:39:52 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Li", "Haoran", ""], ["Quan", "Weihong", ""], ["Yan", "Meijun", ""], ["zhang", "Jin", ""], ["Gong", "Xiaoli", ""], ["Zhou", "Jin", ""]]}, {"id": "2004.05805", "submitter": "Tiexin Qin", "authors": "Tiexin Qin and Wenbin Li and Yinghuan Shi and Yang Gao", "title": "Diversity Helps: Unsupervised Few-shot Learning via Distribution\n  Shift-based Data Augmentation", "comments": "10 pages, 8 figures. Code: https://github.com/WonderSeven/ULDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to learn a new concept when only a few training\nexamples are available, which has been extensively explored in recent years.\nHowever, most of the current works heavily rely on a large-scale labeled\nauxiliary set to train their models in an episodic-training paradigm. Such a\nkind of supervised setting basically limits the widespread use of few-shot\nlearning algorithms. Instead, in this paper, we develop a novel framework\ncalled Unsupervised Few-shot Learning via Distribution Shift-based Data\nAugmentation (ULDA), which pays attention to the distribution diversity inside\neach constructed pretext few-shot task when using data augmentation.\nImportantly, we highlight the value and importance of the distribution\ndiversity in the augmentation-based pretext few-shot tasks, which can\neffectively alleviate the overfitting problem and make the few-shot model learn\nmore robust feature representations. In ULDA, we systemically investigate the\neffects of different augmentation techniques and propose to strengthen the\ndistribution diversity (or difference) between the query set and support set in\neach few-shot task, by augmenting these two sets diversely (i.e., distribution\nshifting). In this way, even incorporated with simple augmentation techniques\n(e.g., random crop, color jittering, or rotation), our ULDA can produce a\nsignificant improvement. In the experiments, few-shot models learned by ULDA\ncan achieve superior generalization performance and obtain state-of-the-art\nresults in a variety of established few-shot learning tasks on Omniglot and\nminiImageNet. The source code is available in\nhttps://github.com/WonderSeven/ULDA.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 07:41:56 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 06:00:36 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Qin", "Tiexin", ""], ["Li", "Wenbin", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2004.05815", "submitter": "Zhaoqi Su", "authors": "Zhaoqi Su and Weilin Wan and Tao Yu and Lingjie Liu and Lu Fang and\n  Wenping Wang and Yebin Liu", "title": "MulayCap: Multi-layer Human Performance Capture Using A Monocular Video\n  Camera", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3027763", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MulayCap, a novel human performance capture method using a\nmonocular video camera without the need for pre-scanning. The method uses\n\"multi-layer\" representations for geometry reconstruction and texture\nrendering, respectively. For geometry reconstruction, we decompose the clothed\nhuman into multiple geometry layers, namely a body mesh layer and a garment\npiece layer. The key technique behind is a Garment-from-Video (GfV) method for\noptimizing the garment shape and reconstructing the dynamic cloth to fit the\ninput video sequence, based on a cloth simulation model which is effectively\nsolved with gradient descent. For texture rendering, we decompose each input\nimage frame into a shading layer and an albedo layer, and propose a method for\nfusing a fixed albedo map and solving for detailed garment geometry using the\nshading layer. Compared with existing single view human performance capture\nsystems, our \"multi-layer\" approach bypasses the tedious and time consuming\nscanning step for obtaining a human specific mesh template. Experimental\nresults demonstrate that MulayCap produces realistic rendering of dynamically\nchanging details that has not been achieved in any previous monocular video\ncamera systems. Benefiting from its fully semantic modeling, MulayCap can be\napplied to various important editing applications, such as cloth editing,\nre-targeting, relighting, and AR applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 08:13:37 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 10:49:35 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 08:00:34 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Su", "Zhaoqi", ""], ["Wan", "Weilin", ""], ["Yu", "Tao", ""], ["Liu", "Lingjie", ""], ["Fang", "Lu", ""], ["Wang", "Wenping", ""], ["Liu", "Yebin", ""]]}, {"id": "2004.05821", "submitter": "Robert McCraith", "authors": "Robert McCraith, Lukas Neumann, Andrew Zisserman, Andrea Vedaldi", "title": "Monocular Depth Estimation with Self-supervised Instance Adaptation", "comments": "IROS submission, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-supervised learning havedemonstrated that it is\npossible to learn accurate monoculardepth reconstruction from raw video data,\nwithout using any 3Dground truth for supervision. However, in robotics\napplications,multiple views of a scene may or may not be available, depend-ing\non the actions of the robot, switching between monocularand multi-view\nreconstruction. To address this mixed setting,we proposed a new approach that\nextends any off-the-shelfself-supervised monocular depth reconstruction system\nto usemore than one image at test time. Our method builds on astandard prior\nlearned to perform monocular reconstruction,but uses self-supervision at test\ntime to further improve thereconstruction accuracy when multiple images are\navailable.When used to update the correct components of the model, thisapproach\nis highly-effective. On the standard KITTI bench-mark, our self-supervised\nmethod consistently outperformsall the previous methods with an average 25%\nreduction inabsolute error for the three common setups (monocular, stereoand\nmonocular+stereo), and comes very close in accuracy whencompared to the\nfully-supervised state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 08:32:03 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["McCraith", "Robert", ""], ["Neumann", "Lukas", ""], ["Zisserman", "Andrew", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2004.05830", "submitter": "Hyeong-Seok Choi", "authors": "Hyeong-Seok Choi, Changdae Park, Kyogu Lee", "title": "From Inference to Generation: End-to-end Fully Self-supervised\n  Generation of Human Face from Speech", "comments": "18 pages, 12 figures, Published as a conference paper at\n  International Conference on Learning Representations (ICLR) 2020.\n  (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work seeks the possibility of generating the human face from voice\nsolely based on the audio-visual data without any human-labeled annotations. To\nthis end, we propose a multi-modal learning framework that links the inference\nstage and generation stage. First, the inference networks are trained to match\nthe speaker identity between the two different modalities. Then the trained\ninference networks cooperate with the generation network by giving conditional\ninformation about the voice. The proposed method exploits the recent\ndevelopment of GANs techniques and generates the human face directly from the\nspeech waveform making our system fully end-to-end. We analyze the extent to\nwhich the network can naturally disentangle two latent factors that contribute\nto the generation of a face image - one that comes directly from a speech\nsignal and the other that is not related to it - and explore whether the\nnetwork can learn to generate natural human face image distribution by modeling\nthese factors. Experimental results show that the proposed network can not only\nmatch the relationship between the human face and speech, but can also generate\nthe high-quality human face sample conditioned on its speech. Finally, the\ncorrelation between the generated face and the corresponding speech is\nquantitatively measured to analyze the relationship between the two modalities.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 09:01:49 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Choi", "Hyeong-Seok", ""], ["Park", "Changdae", ""], ["Lee", "Kyogu", ""]]}, {"id": "2004.05834", "submitter": "Yabo Xiao", "authors": "Yabo Xiao, Dongdong Yu, Xiaojuan Wang, Tianqi Lv, Yiqi Fan, Lingrui Wu", "title": "SPCNet:Spatial Preserve and Content-aware Network for Human Pose\n  Estimation", "comments": "8 pages,6 figures, accepted for presentation at the 24th European\n  Conference on Artificial Intelligence (ECAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is a fundamental yet challenging task in computer\nvision. Although deep learning techniques have made great progress in this\narea, difficult scenarios (e.g., invisible keypoints, occlusions, complex\nmulti-person scenarios, and abnormal poses) are still not well-handled. To\nalleviate these issues, we propose a novel Spatial Preserve and Content-aware\nNetwork(SPCNet), which includes two effective modules: Dilated Hourglass\nModule(DHM) and Selective Information Module(SIM). By using the Dilated\nHourglass Module, we can preserve the spatial resolution along with large\nreceptive field. Similar to Hourglass Network, we stack the DHMs to get the\nmulti-stage and multi-scale information. Then, a Selective Information Module\nis designed to select relatively important features from different levels under\na sufficient consideration of spatial content-aware mechanism and thus\nconsiderably improves the performance. Extensive experiments on MPII, LSP and\nFLIC human pose estimation benchmarks demonstrate the effectiveness of our\nnetwork. In particular, we exceed previous methods and achieve the\nstate-of-the-art performance on three aforementioned benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 09:14:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Xiao", "Yabo", ""], ["Yu", "Dongdong", ""], ["Wang", "Xiaojuan", ""], ["Lv", "Tianqi", ""], ["Fan", "Yiqi", ""], ["Wu", "Lingrui", ""]]}, {"id": "2004.05846", "submitter": "Isht Dwivedi", "authors": "Isht Dwivedi, Srikanth Malla, Behzad Dariush, Chiho Choi", "title": "SSP: Single Shot Future Trajectory Prediction", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a robust solution to future trajectory forecast, which can be\npractically applicable to autonomous agents in highly crowded environments. For\nthis, three aspects are particularly addressed in this paper. First, we use\ncomposite fields to predict future locations of all road agents in a\nsingle-shot, which results in a constant time complexity, regardless of the\nnumber of agents in the scene. Second, interactions between agents are modeled\nas a non-local response, enabling spatial relationships between different\nlocations to be captured temporally as well (i.e., in spatio-temporal\ninteractions). Third, the semantic context of the scene are modeled and take\ninto account the environmental constraints that potentially influence the\nfuture motion. To this end, we validate the robustness of the proposed approach\nusing the ETH, UCY, and SDD datasets and highlight its practical functionality\ncompared to the current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 09:56:38 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 01:37:26 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Dwivedi", "Isht", ""], ["Malla", "Srikanth", ""], ["Dariush", "Behzad", ""], ["Choi", "Chiho", ""]]}, {"id": "2004.05859", "submitter": "Hung-Yu Tseng", "authors": "Hung-Yu Tseng, Yi-Wen Chen, Yi-Hsuan Tsai, Sifei Liu, Yen-Yu Lin,\n  Ming-Hsuan Yang", "title": "Regularizing Meta-Learning via Gradient Dropout", "comments": "Code: https://github.com/hytseng0509/DropGrad", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing attention on learning-to-learn new tasks using only a few\nexamples, meta-learning has been widely used in numerous problems such as\nfew-shot classification, reinforcement learning, and domain generalization.\nHowever, meta-learning models are prone to overfitting when there are no\nsufficient training tasks for the meta-learners to generalize. Although\nexisting approaches such as Dropout are widely used to address the overfitting\nproblem, these methods are typically designed for regularizing models of a\nsingle task in supervised training. In this paper, we introduce a simple yet\neffective method to alleviate the risk of overfitting for gradient-based\nmeta-learning. Specifically, during the gradient-based adaptation stage, we\nrandomly drop the gradient in the inner-loop optimization of each parameter in\ndeep neural networks, such that the augmented gradients improve generalization\nto new tasks. We present a general form of the proposed gradient dropout\nregularization and show that this term can be sampled from either the Bernoulli\nor Gaussian distribution. To validate the proposed method, we conduct extensive\nexperiments and analysis on numerous computer vision tasks, demonstrating that\nthe gradient dropout regularization mitigates the overfitting problem and\nimproves the performance upon various gradient-based meta-learning frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 10:47:02 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Tseng", "Hung-Yu", ""], ["Chen", "Yi-Wen", ""], ["Tsai", "Yi-Hsuan", ""], ["Liu", "Sifei", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2004.05873", "submitter": "Yiming Xu", "authors": "Yiming Xu, Akil Narayan, Hoang Tran and Clayton G. Webster", "title": "Analysis of The Ratio of $\\ell_1$ and $\\ell_2$ Norms in Compressed\n  Sensing", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first propose a novel criterion that guarantees that an $s$-sparse signal\nis the local minimizer of the $\\ell_1/\\ell_2$ objective; our criterion is\ninterpretable and useful in practice. We also give the first uniform recovery\ncondition using a geometric characterization of the null space of the\nmeasurement matrix, and show that this condition is easily satisfied for a\nclass of random matrices. We also present analysis on the robustness of the\nprocedure when noise pollutes data. Numerical experiments are provided that\ncompare $\\ell_1/\\ell_2$ with some other popular non-convex methods in\ncompressed sensing. Finally, we propose a novel initialization approach to\naccelerate the numerical optimization procedure. We call this initialization\napproach \\emph{support selection}, and we demonstrate that it empirically\nimproves the performance of existing $\\ell_1/\\ell_2$ algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 11:35:41 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 00:09:48 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Xu", "Yiming", ""], ["Narayan", "Akil", ""], ["Tran", "Hoang", ""], ["Webster", "Clayton G.", ""]]}, {"id": "2004.05884", "submitter": "Dongxian Wu", "authors": "Dongxian Wu, Shu-tao Xia, Yisen Wang", "title": "Adversarial Weight Perturbation Helps Robust Generalization", "comments": "To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study on improving the robustness of deep neural networks against\nadversarial examples grows rapidly in recent years. Among them, adversarial\ntraining is the most promising one, which flattens the input loss landscape\n(loss change with respect to input) via training on adversarially perturbed\nexamples. However, how the widely used weight loss landscape (loss change with\nrespect to weight) performs in adversarial training is rarely explored. In this\npaper, we investigate the weight loss landscape from a new perspective, and\nidentify a clear correlation between the flatness of weight loss landscape and\nrobust generalization gap. Several well-recognized adversarial training\nimprovements, such as early stopping, designing new objective functions, or\nleveraging unlabeled data, all implicitly flatten the weight loss landscape.\nBased on these observations, we propose a simple yet effective Adversarial\nWeight Perturbation (AWP) to explicitly regularize the flatness of weight loss\nlandscape, forming a double-perturbation mechanism in the adversarial training\nframework that adversarially perturbs both inputs and weights. Extensive\nexperiments demonstrate that AWP indeed brings flatter weight loss landscape\nand can be easily incorporated into various existing adversarial training\nmethods to further boost their adversarial robustness.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 12:05:01 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:46:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wu", "Dongxian", ""], ["Xia", "Shu-tao", ""], ["Wang", "Yisen", ""]]}, {"id": "2004.05908", "submitter": "Tianyang Shi", "authors": "Xinhui Song and Tianyang Shi and Tianjia Shao and Yi Yuan and Zunlei\n  Feng and Changjie Fan", "title": "Unsupervised Facial Action Unit Intensity Estimation via Differentiable\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic intensity estimation of facial action units (AUs) from a single\nimage plays a vital role in facial analysis systems. One big challenge for\ndata-driven AU intensity estimation is the lack of sufficient AU label data.\nDue to the fact that AU annotation requires strong domain expertise, it is\nexpensive to construct an extensive database to learn deep models. The limited\nnumber of labeled AUs as well as identity differences and pose variations\nfurther increases the estimation difficulties. Considering all these\ndifficulties, we propose an unsupervised framework GE-Net for facial AU\nintensity estimation from a single image, without requiring any annotated AU\ndata. Our framework performs differentiable optimization, which iteratively\nupdates the facial parameters (i.e., head pose, AU parameters and identity\nparameters) to match the input image. GE-Net consists of two modules: a\ngenerator and a feature extractor. The generator learns to \"render\" a face\nimage from a set of facial parameters in a differentiable way, and the feature\nextractor extracts deep features for measuring the similarity of the rendered\nimage and input real image. After the two modules are trained and fixed, the\nframework searches optimal facial parameters by minimizing the differences of\nthe extracted features between the rendered image and the input image.\nExperimental results demonstrate that our method can achieve state-of-the-art\nresults compared with existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 12:56:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Song", "Xinhui", ""], ["Shi", "Tianyang", ""], ["Shao", "Tianjia", ""], ["Yuan", "Yi", ""], ["Feng", "Zunlei", ""], ["Fan", "Changjie", ""]]}, {"id": "2004.05909", "submitter": "Tao Zhang", "authors": "Tao Zhang, Wei Li", "title": "k-decay: A New Method For Learning Rate Schedule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that optimizing the learning rate (LR) schedule can be\na very accurate and efficient way to train the deep neural networks. In this\npaper, we propose the k-decay method, in which the rate of change (ROC) of the\nLR is changed by its k-th order derivative, to obtain the new LR schedule. In\nthe new LR schedule, a new hyper-parameter $k$ controls the change degree of\nLR, whereas the original method of $k$ at 1. By repeatedly using the k-decay\nmethod, one can identify the best LR schedule. We evaluate the k-decay method\non CIFAR And ImageNet datasets with different neural networks (ResNet, Wide\nResNet, and DenseNet). Our experiments show that the k-decay method can achieve\nimprovements over the state-of-the-art results on most of them. The accuracy\nimproved by 1.08% on the CIFAR-10 dataset, and by 2.07% on the CIFAR-100\ndataset. On the ImageNet, accuracy improved by 1.25%. Our method is not only\nefficient but also easy to use.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 12:58:45 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 06:47:54 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 13:03:36 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 10:17:13 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Tao", ""], ["Li", "Wei", ""]]}, {"id": "2004.05937", "submitter": "Lin Wang", "authors": "Lin Wang and Kuk-Jin Yoon", "title": "Knowledge Distillation and Student-Teacher Learning for Visual\n  Intelligence: A Review and New Outlooks", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence(TPAMI),2021. Some references are updated in this version", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3055564", "report-no": "https://ieeexplore.ieee.org/document/9340578", "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural models in recent years have been successful in almost every\nfield, including extremely complex problem statements. However, these models\nare huge in size, with millions (and even billions) of parameters, thus\ndemanding more heavy computation power and failing to be deployed on edge\ndevices. Besides, the performance boost is highly dependent on redundant\nlabeled data. To achieve faster speeds and to handle the problems caused by the\nlack of data, knowledge distillation (KD) has been proposed to transfer\ninformation learned from one model to another. KD is often characterized by the\nso-called `Student-Teacher' (S-T) learning framework and has been broadly\napplied in model compression and knowledge transfer. This paper is about KD and\nS-T learning, which are being actively studied in recent years. First, we aim\nto provide explanations of what KD is and how/why it works. Then, we provide a\ncomprehensive survey on the recent progress of KD methods together with S-T\nframeworks typically for vision tasks. In general, we consider some fundamental\nquestions that have been driving this research area and thoroughly generalize\nthe research progress and technical details. Additionally, we systematically\nanalyze the research status of KD in vision applications. Finally, we discuss\nthe potentials and open challenges of existing methods and prospect the future\ndirections of KD and S-T learning.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 13:45:38 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 06:53:08 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 01:27:02 GMT"}, {"version": "v4", "created": "Sun, 11 Oct 2020 13:33:33 GMT"}, {"version": "v5", "created": "Wed, 6 Jan 2021 08:16:08 GMT"}, {"version": "v6", "created": "Mon, 25 Jan 2021 12:37:46 GMT"}, {"version": "v7", "created": "Thu, 17 Jun 2021 07:17:50 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wang", "Lin", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "2004.05966", "submitter": "Sibo Zhang", "authors": "Sibo Zhang, Yuexin Ma, Ruigang Yang", "title": "CVPR 2019 WAD Challenge on Trajectory Prediction and 3D Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the CVPR 2019 challenge on Autonomous Driving. Baidu's\nRobotics and Autonomous Driving Lab (RAL) providing 150 minutes labeled\nTrajectory and 3D Perception dataset including about 80k lidar point cloud and\n1000km trajectories for urban traffic. The challenge has two tasks in (1)\nTrajectory Prediction and (2) 3D Lidar Object Detection. There are more than\n200 teams submitted results on Leaderboard and more than 1000 participants\nattended the workshop.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 06:36:33 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 22:24:03 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhang", "Sibo", ""], ["Ma", "Yuexin", ""], ["Yang", "Ruigang", ""]]}, {"id": "2004.05972", "submitter": "Zhe Cui", "authors": "Zhe Cui, Jianjiang Feng, Jie Zhou", "title": "Dense Registration and Mosaicking of Fingerprints by Training an\n  End-to-End Network", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2020.3017926", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense registration of fingerprints is a challenging task due to elastic skin\ndistortion, low image quality, and self-similarity of ridge pattern. To\novercome the limitation of handcraft features, we propose to train an\nend-to-end network to directly output pixel-wise displacement field between two\nfingerprints. The proposed network includes a siamese network for feature\nembedding, and a following encoder-decoder network for regressing displacement\nfield. By applying displacement fields reliably estimated by tracing high\nquality fingerprint videos to challenging fingerprints, we synthesize a large\nnumber of training fingerprint pairs with ground truth displacement fields. In\naddition, based on the proposed registration algorithm, we propose a\nfingerprint mosaicking method based on optimal seam selection. Registration and\nmatching experiments on FVC2004 databases, Tsinghua Distorted Fingerprint (TDF)\ndatabase, and NIST SD27 latent fingerprint database show that our registration\nmethod outperforms previous dense registration methods in accuracy and\nefficiency. Mosaicking experiment on FVC2004 DB1 demonstrates that the proposed\nalgorithm produced higher quality fingerprints than other algorithms which also\nvalidates the performance of our registration algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:47:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cui", "Zhe", ""], ["Feng", "Jianjiang", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.05973", "submitter": "Shreya Ghosh", "authors": "Shreya Ghosh, Abhinav Dhall, Garima Sharma, Sarthak Gupta, Nicu Sebe", "title": "Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver\n  Gaze Zone Estimation Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelling of human behavior analysis data is a complex and time consuming\ntask. In this paper, a fully automatic technique for labelling an image based\ngaze behavior dataset for driver gaze zone estimation is proposed. Domain\nknowledge can be added to the data recording paradigm and later labels can be\ngenerated in an automatic manner using speech to text conversion. In order to\nremove the noise in STT due to different ethnicity, the speech frequency and\nenergy are analysed. The resultant Driver Gaze in the Wild DGW dataset contains\n586 recordings, captured during different times of the day including evening.\nThe large scale dataset contains 338 subjects with an age range of 18-63 years.\nAs the data is recorded in different lighting conditions, an illumination\nrobust layer is proposed in the Convolutional Neural Network (CNN). The\nextensive experiments show the variance in the database resembling real-world\nconditions and the effectiveness of the proposed CNN pipeline. The proposed\nnetwork is also fine-tuned for the eye gaze prediction task, which shows the\ndiscriminativeness of the representation learnt by our network on the proposed\nDGW dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:47:34 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:53:22 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 17:14:40 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ghosh", "Shreya", ""], ["Dhall", "Abhinav", ""], ["Sharma", "Garima", ""], ["Gupta", "Sarthak", ""], ["Sebe", "Nicu", ""]]}, {"id": "2004.05993", "submitter": "Michael Jones", "authors": "Bharathkumar Ramachandra, Michael J. Jones, Ranga Raju Vatsavai", "title": "A Survey of Single-Scene Video Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey article summarizes research trends on the topic of anomaly\ndetection in video feeds of a single scene. We discuss the various problem\nformulations, publicly available datasets and evaluation criteria. We\ncategorize and situate past research into an intuitive taxonomy and provide a\ncomprehensive comparison of the accuracy of many algorithms on standard test\nsets. Finally, we also provide best practices and suggest some possible\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:09:50 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 18:09:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ramachandra", "Bharathkumar", ""], ["Jones", "Michael J.", ""], ["Vatsavai", "Ranga Raju", ""]]}, {"id": "2004.06002", "submitter": "Hongkai Zhang", "authors": "Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, Xilin Chen", "title": "Dynamic R-CNN: Towards High Quality Object Detection via Dynamic\n  Training", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although two-stage object detectors have continuously advanced the\nstate-of-the-art performance in recent years, the training process itself is\nfar from crystal. In this work, we first point out the inconsistency problem\nbetween the fixed network settings and the dynamic training procedure, which\ngreatly affects the performance. For example, the fixed label assignment\nstrategy and regression loss function cannot fit the distribution change of\nproposals and thus are harmful to training high quality detectors.\nConsequently, we propose Dynamic R-CNN to adjust the label assignment criteria\n(IoU threshold) and the shape of regression loss function (parameters of\nSmoothL1 Loss) automatically based on the statistics of proposals during\ntraining. This dynamic design makes better use of the training samples and\npushes the detector to fit more high quality samples. Specifically, our method\nimproves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP$_{90}$ on the MS\nCOCO dataset with no extra overhead. Codes and models are available at\nhttps://github.com/hkzhang95/DynamicRCNN.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:20:25 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 07:28:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhang", "Hongkai", ""], ["Chang", "Hong", ""], ["Ma", "Bingpeng", ""], ["Wang", "Naiyan", ""], ["Chen", "Xilin", ""]]}, {"id": "2004.06014", "submitter": "Yedid Hoshen", "authors": "Yael Vinker and Nir Zabari and Yedid Hoshen", "title": "Training End-to-end Single Image Generators without GANs", "comments": "Project page: http://www.vision.huji.ac.il/augurone", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AugurOne, a novel approach for training single image generative\nmodels. Our approach trains an upscaling neural network using non-affine\naugmentations of the (single) input image, particularly including non-rigid\nthin plate spline image warps. The extensive augmentations significantly\nincrease the in-sample distribution for the upsampling network enabling the\nupscaling of highly variable inputs. A compact latent space is jointly learned\nallowing for controlled image synthesis. Differently from Single Image GAN, our\napproach does not require GAN training and takes place in an end-to-end fashion\nallowing fast and stable training. We experimentally evaluate our method and\nshow that it obtains compelling novel animations of single-image, as well as,\nstate-of-the-art performance on conditional generation tasks e.g.\npaint-to-image and edges-to-image.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:58:03 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Vinker", "Yael", ""], ["Zabari", "Nir", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2004.06030", "submitter": "Yilun Du", "authors": "Yilun Du, Shuang Li, Igor Mordatch", "title": "Compositional Visual Generation and Inference with Energy Based Models", "comments": "NeurIPS 2020 Spotlight; Website at\n  https://energy-based-model.github.io/compositional-generation-inference/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital aspect of human intelligence is the ability to compose increasingly\ncomplex concepts out of simpler ideas, enabling both rapid learning and\nadaptation of knowledge. In this paper we show that energy-based models can\nexhibit this ability by directly combining probability distributions. Samples\nfrom the combined distribution correspond to compositions of concepts. For\nexample, given a distribution for smiling faces, and another for male faces, we\ncan combine them to generate smiling male faces. This allows us to generate\nnatural images that simultaneously satisfy conjunctions, disjunctions, and\nnegations of concepts. We evaluate compositional generation abilities of our\nmodel on the CelebA dataset of natural faces and synthetic 3D scene images. We\nalso demonstrate other unique advantages of our model, such as the ability to\ncontinually learn and incorporate new concepts, or infer compositions of\nconcept properties underlying an image.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:01:40 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 22:50:40 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 09:26:00 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Du", "Yilun", ""], ["Li", "Shuang", ""], ["Mordatch", "Igor", ""]]}, {"id": "2004.06042", "submitter": "Yawei Luo", "authors": "Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang", "title": "Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim at the problem named One-Shot Unsupervised Domain Adaptation. Unlike\ntraditional Unsupervised Domain Adaptation, it assumes that only one unlabeled\ntarget sample can be available when learning to adapt. This setting is\nrealistic but more challenging, in which conventional adaptation approaches are\nprone to failure due to the scarce of unlabeled target data. To this end, we\npropose a novel Adversarial Style Mining approach, which combines the style\ntransfer module and task-specific module into an adversarial manner.\nSpecifically, the style transfer module iteratively searches for harder\nstylized images around the one-shot target sample according to the current\nlearning state, leading the task model to explore the potential styles that are\ndifficult to solve in the almost unseen target domain, thus boosting the\nadaptation performance in a data-scarce scenario. The adversarial learning\nframework makes the style transfer module and task-specific module benefit each\nother during the competition. Extensive experiments on both cross-domain\nclassification and segmentation benchmarks verify that ASM achieves\nstate-of-the-art adaptation performance under the challenging one-shot setting.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:18:46 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Luo", "Yawei", ""], ["Liu", "Ping", ""], ["Guan", "Tao", ""], ["Yu", "Junqing", ""], ["Yang", "Yi", ""]]}, {"id": "2004.06130", "submitter": "Ariel Ephrat", "authors": "Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T.\n  Freeman, Michael Rubinstein, Michal Irani and Tali Dekel", "title": "SpeedNet: Learning the Speediness in Videos", "comments": "Accepted to CVPR 2020 (oral). Project webpage:\n  http://speednet-cvpr20.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to automatically predict the \"speediness\" of moving objects in\nvideos---whether they move faster, at, or slower than their \"natural\" speed.\nThe core component in our approach is SpeedNet---a novel deep network trained\nto detect if a video is playing at normal rate, or if it is sped up. SpeedNet\nis trained on a large corpus of natural videos in a self-supervised manner,\nwithout requiring any manual annotations. We show how this single, binary\nclassification network can be used to detect arbitrary rates of speediness of\nobjects. We demonstrate prediction results by SpeedNet on a wide range of\nvideos containing complex natural motions, and examine the visual cues it\nutilizes for making those predictions. Importantly, we show that through\npredicting the speed of videos, the model learns a powerful and meaningful\nspace-time representation that goes beyond simple motion cues. We demonstrate\nhow those learned features can boost the performance of self-supervised action\nrecognition, and can be used for video retrieval. Furthermore, we also apply\nSpeedNet for generating time-varying, adaptive video speedups, which can allow\nviewers to watch videos faster, but with less of the jittery, unnatural motions\ntypical to videos that are sped up uniformly.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:00:27 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 14:33:04 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Benaim", "Sagie", ""], ["Ephrat", "Ariel", ""], ["Lang", "Oran", ""], ["Mosseri", "Inbar", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""], ["Irani", "Michal", ""], ["Dekel", "Tali", ""]]}, {"id": "2004.06154", "submitter": "Enkhtogtokh Togootogtokh", "authors": "Enkhtogtokh Togootogtokh, Christian Micheloni, Gian Luca Foresti, Niki\n  Martinel", "title": "An Efficient UAV-based Artificial Intelligence Framework for Real-Time\n  Visual Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Unmanned Aerial Vehicles equipped with state of the art artificial\nintelligence (AI) technologies are opening to a wide plethora of novel and\ninteresting applications. While this field received a strong impact from the\nrecent AI breakthroughs, most of the provided solutions either entirely rely on\ncommercial software or provide a weak integration interface which denies the\ndevelopment of additional techniques. This leads us to propose a novel and\nefficient framework for the UAV-AI joint technology. Intelligent UAV systems\nencounter complex challenges to be tackled without human control. One of these\ncomplex challenges is to be able to carry out computer vision tasks in\nreal-time use cases. In this paper we focus on this challenge and introduce a\nmulti-layer AI (MLAI) framework to allow easy integration of ad-hoc\nvisual-based AI applications. To show its features and its advantages, we\nimplemented and evaluated different modern visual-based deep learning models\nfor object detection, target tracking and target handover.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 18:53:12 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Togootogtokh", "Enkhtogtokh", ""], ["Micheloni", "Christian", ""], ["Foresti", "Gian Luca", ""], ["Martinel", "Niki", ""]]}, {"id": "2004.06163", "submitter": "Wei Zhou", "authors": "Wei Zhou, Qiuping Jiang, Yuwang Wang, Zhibo Chen, Weiping Li", "title": "Blind Quality Assessment for Image Superresolution Using Deep Two-Stream\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous image superresolution (SR) algorithms have been proposed for\nreconstructing high-resolution (HR) images from input images with lower spatial\nresolutions. However, effectively evaluating the perceptual quality of SR\nimages remains a challenging research problem. In this paper, we propose a\nno-reference/blind deep neural network-based SR image quality assessor\n(DeepSRQ). To learn more discriminative feature representations of various\ndistorted SR images, the proposed DeepSRQ is a two-stream convolutional network\nincluding two subcomponents for distorted structure and texture SR images.\nDifferent from traditional image distortions, the artifacts of SR images cause\nboth image structure and texture quality degradation. Therefore, we choose the\ntwo-stream scheme that captures different properties of SR inputs instead of\ndirectly learning features from one image stream. Considering the human visual\nsystem (HVS) characteristics, the structure stream focuses on extracting\nfeatures in structural degradations, while the texture stream focuses on the\nchange in textural distributions. In addition, to augment the training data and\nensure the category balance, we propose a stride-based adaptive cropping\napproach for further improvement. Experimental results on three publicly\navailable SR image quality databases demonstrate the effectiveness and\ngeneralization ability of our proposed DeepSRQ method compared with\nstate-of-the-art image quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:14:28 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zhou", "Wei", ""], ["Jiang", "Qiuping", ""], ["Wang", "Yuwang", ""], ["Chen", "Zhibo", ""], ["Li", "Weiping", ""]]}, {"id": "2004.06165", "submitter": "Xiujun Li", "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei\n  Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "comments": "ECCV 2020, Code and pre-trained models are released:\n  https://github.com/microsoft/Oscar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-training methods of learning cross-modal representations on\nimage-text pairs are becoming popular for vision-language tasks. While existing\nmethods simply concatenate image region features and text features as input to\nthe model to be pre-trained and use self-attention to learn image-text semantic\nalignments in a brute force manner, in this paper, we propose a new learning\nmethod Oscar (Object-Semantics Aligned Pre-training), which uses object tags\ndetected in images as anchor points to significantly ease the learning of\nalignments. Our method is motivated by the observation that the salient objects\nin an image can be accurately detected, and are often mentioned in the paired\ntext. We pre-train an Oscar model on the public corpus of 6.5 million\ntext-image pairs, and fine-tune it on downstream tasks, creating new\nstate-of-the-arts on six well-established vision-language understanding and\ngeneration tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:18:10 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 03:29:46 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 04:57:31 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 01:18:25 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 00:46:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Xiujun", ""], ["Yin", "Xi", ""], ["Li", "Chunyuan", ""], ["Zhang", "Pengchuan", ""], ["Hu", "Xiaowei", ""], ["Zhang", "Lei", ""], ["Wang", "Lijuan", ""], ["Hu", "Houdong", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.06172", "submitter": "Kanav Vats", "authors": "Kanav Vats, Mehrnaz Fani, Pascale Walters, David A. Clausi, John Zelek", "title": "Event detection in coarsely annotated sports videos via parallel multi\n  receptive field 1D convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In problems such as sports video analytics, it is difficult to obtain\naccurate frame level annotations and exact event duration because of the\nlengthy videos and sheer volume of video data. This issue is even more\npronounced in fast-paced sports such as ice hockey. Obtaining annotations on a\ncoarse scale can be much more practical and time efficient. We propose the task\nof event detection in coarsely annotated videos. We introduce a multi-tower\ntemporal convolutional network architecture for the proposed task. The network,\nwith the help of multiple receptive fields, processes information at various\ntemporal scales to account for the uncertainty with regard to the exact event\nlocation and duration. We demonstrate the effectiveness of the multi-receptive\nfield architecture through appropriate ablation studies. The method is\nevaluated on two tasks - event detection in coarsely annotated hockey videos in\nthe NHL dataset and event spotting in soccer on the SoccerNet dataset. The two\ndatasets lack frame-level annotations and have very distinct event frequencies.\nExperimental results demonstrate the effectiveness of the network by obtaining\na 55% average F1 score on the NHL dataset and by achieving competitive\nperformance compared to the state of the art on the SoccerNet dataset. We\nbelieve our approach will help develop more practical pipelines for event\ndetection in sports video.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:51:25 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Vats", "Kanav", ""], ["Fani", "Mehrnaz", ""], ["Walters", "Pascale", ""], ["Clausi", "David A.", ""], ["Zelek", "John", ""]]}, {"id": "2004.06180", "submitter": "Neha Bhargava", "authors": "Neha Bhargava and Fabio Cuzzolin", "title": "Challenges and Opportunities for Computer Vision in Real-life Soccer\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore some of the applications of computer vision to\nsports analytics. Sport analytics deals with understanding and discovering\npatterns from a corpus of sports data. Analysing such data provides important\nperformance metrics for the players, for instance in soccer matches, that could\nbe useful for estimating their fitness and strengths. Team level statistics can\nalso be estimated from such analysis. This paper mainly focuses on some the\nchallenges and opportunities presented by sport video analysis in computer\nvision. Specifically, we use our multi-camera setup as a framework to discuss\nsome of the real-life challenges for machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:06:23 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Bhargava", "Neha", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2004.06193", "submitter": "Rajat Koner", "authors": "Rajat Koner, Suprosanna Shit and Volker Tresp", "title": "Relation Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extraction of a scene graph with objects as nodes and mutual\nrelationships as edges is the basis for a deep understanding of image content.\nDespite recent advances, such as message passing and joint classification, the\ndetection of visual relationships remains a challenging task due to sub-optimal\nexploration of the mutual interaction among the visual objects. In this work,\nwe propose a novel transformer formulation for scene graph generation and\nrelation prediction. We leverage the encoder-decoder architecture of the\ntransformer for rich feature embedding of nodes and edges. Specifically, we\nmodel the node-to-node interaction with the self-attention of the transformer\nencoder and the edge-to-node interaction with the cross-attention of the\ntransformer decoder. Further, we introduce a novel positional embedding\nsuitable to handle edges in the decoder. Finally, our relation prediction\nmodule classifies the directed relation from the learned node and edge\nembedding. We name this architecture as Relation Transformer Network (RTN). On\nthe Visual Genome and GQA dataset, we have achieved an overall mean of 4.85%\nand 3.1% point improvement in comparison with state-of-the-art methods. Our\nexperiments show that Relation Transformer can efficiently model context across\nvarious datasets with small, medium, and large-scale relation classification.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 20:47:01 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 21:10:56 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Koner", "Rajat", ""], ["Shit", "Suprosanna", ""], ["Tresp", "Volker", ""]]}, {"id": "2004.06209", "submitter": "Youssouf Chherawala", "authors": "Youssouf Chherawala, Hans J. G. A. Dolfing, Ryan S. Dixon, and Jerome\n  R. Bellegarda", "title": "Embedded Large-Scale Handwritten Chinese Character Recognition", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053084", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As handwriting input becomes more prevalent, the large symbol inventory\nrequired to support Chinese handwriting recognition poses unique challenges.\nThis paper describes how the Apple deep learning recognition system can\naccurately handle up to 30,000 Chinese characters while running in real-time\nacross a range of mobile devices. To achieve acceptable accuracy, we paid\nparticular attention to data collection conditions, representativeness of\nwriting styles, and training regimen. We found that, with proper care, even\nlarger inventories are within reach. Our experiments show that accuracy only\ndegrades slowly as the inventory increases, as long as we use training data of\nsufficient quality and in sufficient quantity.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 21:21:34 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chherawala", "Youssouf", ""], ["Dolfing", "Hans J. G. A.", ""], ["Dixon", "Ryan S.", ""], ["Bellegarda", "Jerome R.", ""]]}, {"id": "2004.06229", "submitter": "Shanglin Yang", "authors": "Shizhu Liu, Shanglin Yang, and Hui Zhou", "title": "Imitation Learning for Fashion Style Based on Hierarchical Multimodal\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is a complex social phenomenon. People follow fashion styles from\ndemonstrations by experts or fashion icons. However, for machine agent,\nlearning to imitate fashion experts from demonstrations can be challenging,\nespecially for complex styles in environments with high-dimensional, multimodal\nobservations. Most existing research regarding fashion outfit composition\nutilizes supervised learning methods to mimic the behaviors of style icons.\nThese methods suffer from distribution shift: because the agent greedily\nimitates some given outfit demonstrations, it can drift away from one style to\nanother styles given subtle differences. In this work, we propose an\nadversarial inverse reinforcement learning formulation to recover reward\nfunctions based on hierarchical multimodal representation (HM-AIRL) during the\nimitation process. The hierarchical joint representation can more\ncomprehensively model the expert composited outfit demonstrations to recover\nthe reward function. We demonstrate that the proposed HM-AIRL model is able to\nrecover reward functions that are robust to changes in multimodal observations,\nenabling us to learn policies under significant variation between different\nstyles.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 23:02:25 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Liu", "Shizhu", ""], ["Yang", "Shanglin", ""], ["Zhou", "Hui", ""]]}, {"id": "2004.06239", "submitter": "Chunyu Wang", "authors": "Hanyue Tu, Chunyu Wang, Wenjun Zeng", "title": "VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to estimate 3D poses of multiple people from multiple\ncamera views. In contrast to the previous efforts which require to establish\ncross-view correspondence based on noisy and incomplete 2D pose estimations, we\npresent an end-to-end solution which directly operates in the $3$D space,\ntherefore avoids making incorrect decisions in the 2D space. To achieve this\ngoal, the features in all camera views are warped and aggregated in a common 3D\nspace, and fed into Cuboid Proposal Network (CPN) to coarsely localize all\npeople. Then we propose Pose Regression Network (PRN) to estimate a detailed 3D\npose for each proposal. The approach is robust to occlusion which occurs\nfrequently in practice. Without bells and whistles, it outperforms the\nstate-of-the-arts on the public datasets. Code will be released at\nhttps://github.com/microsoft/multiperson-pose-estimation-pytorch.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 23:50:01 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 23:58:03 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 07:40:19 GMT"}, {"version": "v4", "created": "Mon, 24 Aug 2020 11:01:32 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Tu", "Hanyue", ""], ["Wang", "Chunyu", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2004.06244", "submitter": "Azar Barkousaraie", "authors": "Azar Sadeghnejad-Barkousaraie, Gyanendra Bohara, Steve Jiang, Dan\n  Nguyen", "title": "A reinforcement learning application of guided Monte Carlo Tree Search\n  algorithm for beam orientation selection in radiation therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the large combinatorial problem, current beam orientation optimization\nalgorithms for radiotherapy, such as column generation (CG), are typically\nheuristic or greedy in nature, leading to suboptimal solutions. We propose a\nreinforcement learning strategy using Monte Carlo Tree Search capable of\nfinding a superior beam orientation set and in less time than CG.We utilized a\nreinforcement learning structure involving a supervised learning network to\nguide Monte Carlo tree search (GTS) to explore the decision space of beam\norientation selection problem. We have previously trained a deep neural network\n(DNN) that takes in the patient anatomy, organ weights, and current beams, and\nthen approximates beam fitness values, indicating the next best beam to add.\nThis DNN is used to probabilistically guide the traversal of the branches of\nthe Monte Carlo decision tree to add a new beam to the plan. To test the\nfeasibility of the algorithm, we solved for 5-beam plans, using 13 test\nprostate cancer patients, different from the 57 training and validation\npatients originally trained the DNN. To show the strength of GTS to other\nsearch methods, performances of three other search methods including a guided\nsearch, uniform tree search and random search algorithms are also provided. On\naverage GTS outperforms all other methods, it find a solution better than CG in\n237 seconds on average, compared to CG which takes 360 seconds, and outperforms\nall other methods in finding a solution with lower objective function value in\nless than 1000 seconds. Using our guided tree search (GTS) method we were able\nto maintain a similar planning target volume (PTV) coverage within 1% error,\nand reduce the organ at risk (OAR) mean dose for body, rectum, left and right\nfemoral heads, but a slight increase of 1% in bladder mean dose.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 00:28:15 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Sadeghnejad-Barkousaraie", "Azar", ""], ["Bohara", "Gyanendra", ""], ["Jiang", "Steve", ""], ["Nguyen", "Dan", ""]]}, {"id": "2004.06267", "submitter": "Mertalp Ocal", "authors": "Mertalp Ocal, Armin Mustafa", "title": "RealMonoDepth: Self-Supervised Monocular Depth Estimation for General\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalised self-supervised learning approach for monocular\nestimation of the real depth across scenes with diverse depth ranges from\n1--100s of meters. Existing supervised methods for monocular depth estimation\nrequire accurate depth measurements for training. This limitation has led to\nthe introduction of self-supervised methods that are trained on stereo image\npairs with a fixed camera baseline to estimate disparity which is transformed\nto depth given known calibration. Self-supervised approaches have demonstrated\nimpressive results but do not generalise to scenes with different depth ranges\nor camera baselines. In this paper, we introduce RealMonoDepth a\nself-supervised monocular depth estimation approach which learns to estimate\nthe real scene depth for a diverse range of indoor and outdoor scenes. A novel\nloss function with respect to the true scene depth based on relative depth\nscaling and warping is proposed. This allows self-supervised training of a\nsingle network with multiple data sets for scenes with diverse depth ranges\nfrom both stereo pair and in the wild moving camera data sets. A comprehensive\nperformance evaluation across five benchmark data sets demonstrates that\nRealMonoDepth provides a single trained network which generalises depth\nestimation across indoor and outdoor scenes, consistently outperforming\nprevious self-supervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 02:03:10 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ocal", "Mertalp", ""], ["Mustafa", "Armin", ""]]}, {"id": "2004.06271", "submitter": "Pirazh Khorramshahi", "authors": "Pirazh Khorramshahi, Neehar Peri, Jun-cheng Chen, Rama Chellappa", "title": "The Devil is in the Details: Self-Supervised Attention for Vehicle\n  Re-Identification", "comments": "This work has been accepted European Conference on Computer Vision\n  (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the research community has approached the problem of vehicle\nre-identification (re-id) with attention-based models, specifically focusing on\nregions of a vehicle containing discriminative information. These re-id methods\nrely on expensive key-point labels, part annotations, and additional attributes\nincluding vehicle make, model, and color. Given the large number of vehicle\nre-id datasets with various levels of annotations, strongly-supervised methods\nare unable to scale across different domains. In this paper, we present\nSelf-supervised Attention for Vehicle Re-identification (SAVER), a novel\napproach to effectively learn vehicle-specific discriminative features. Through\nextensive experimentation, we show that SAVER improves upon the\nstate-of-the-art on challenging VeRi, VehicleID, Vehicle-1M and VERI-Wild\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 02:24:47 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 16:34:43 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 06:08:17 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Khorramshahi", "Pirazh", ""], ["Peri", "Neehar", ""], ["Chen", "Jun-cheng", ""], ["Chellappa", "Rama", ""]]}, {"id": "2004.06272", "submitter": "Yangxin Wu", "authors": "Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan\n  Liang, Liang Lin", "title": "Bidirectional Graph Reasoning Network for Panoptic Segmentation", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches on panoptic segmentation resort to a single end-to-end\nnetwork to combine the tasks of instance segmentation and semantic\nsegmentation. However, prior models only unified the two related tasks at the\narchitectural level via a multi-branch scheme or revealed the underlying\ncorrelation between them by unidirectional feature fusion, which disregards the\nexplicit semantic and co-occurrence relations among objects and background.\nInspired by the fact that context information is critical to recognize and\nlocalize the objects, and inclusive object details are significant to parse the\nbackground scene, we thus investigate on explicitly modeling the correlations\nbetween object and background to achieve a holistic understanding of an image\nin the panoptic segmentation task. We introduce a Bidirectional Graph Reasoning\nNetwork (BGRNet), which incorporates graph structure into the conventional\npanoptic segmentation network to mine the intra-modular and intermodular\nrelations within and between foreground things and background stuff classes. In\nparticular, BGRNet first constructs image-specific graphs in both instance and\nsemantic segmentation branches that enable flexible reasoning at the proposal\nlevel and class level, respectively. To establish the correlations between\nseparate branches and fully leverage the complementary relations between things\nand stuff, we propose a Bidirectional Graph Connection Module to diffuse\ninformation across branches in a learnable fashion. Experimental results\ndemonstrate the superiority of our BGRNet that achieves the new\nstate-of-the-art performance on challenging COCO and ADE20K panoptic\nsegmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 02:32:10 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Wu", "Yangxin", ""], ["Zhang", "Gengwei", ""], ["Gao", "Yiming", ""], ["Deng", "Xiajun", ""], ["Gong", "Ke", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""]]}, {"id": "2004.06297", "submitter": "Thao Do", "authors": "Thao Do, Yalew Tolcha, Tae Joon Jun, Daeyoung Kim", "title": "Smart Inference for Multidigit Convolutional Neural Network based\n  Barcode Decoding", "comments": "Published on 2020 (25th) International Conference on Pattern\n  Recognition (ICPR)", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9412707", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Barcodes are ubiquitous and have been used in most of critical daily\nactivities for decades. However, most of traditional decoders require\nwell-founded barcode under a relatively standard condition. While wilder\nconditioned barcodes such as underexposed, occluded, blurry, wrinkled and\nrotated are commonly captured in reality, those traditional decoders show\nweakness of recognizing. Several works attempted to solve those challenging\nbarcodes, but many limitations still exist. This work aims to solve the\ndecoding problem using deep convolutional neural network with the possibility\nof running on portable devices. Firstly, we proposed a special modification of\ninference based on the feature of having checksum and test-time augmentation,\nnamed as Smart Inference (SI) in prediction phase of a trained model. SI\nconsiderably boosts accuracy and reduces the false prediction for trained\nmodels. Secondly, we have created a large practical evaluation dataset of real\ncaptured 1D barcode under various challenging conditions to test our methods\nvigorously, which is publicly available for other researchers. The experiments'\nresults demonstrated the SI effectiveness with the highest accuracy of 95.85%\nwhich outperformed many existing decoders on the evaluation set. Finally, we\nsuccessfully minimized the best model by knowledge distillation to a shallow\nmodel which is shown to have high accuracy (90.85%) with good inference speed\nof 34.2 ms per image on a real edge device.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 04:30:34 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 07:13:08 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 08:42:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Do", "Thao", ""], ["Tolcha", "Yalew", ""], ["Jun", "Tae Joon", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2004.06302", "submitter": "Eugene Belilovsky", "authors": "Mateusz Michalkiewicz, Sarah Parisot, Stavros Tsogkas, Mahsa\n  Baktashmotlagh, Anders Eriksson, Eugene Belilovsky", "title": "Few-Shot Single-View 3-D Object Reconstruction with Compositional Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive performance of deep convolutional neural networks in\nsingle-view 3D reconstruction suggests that these models perform non-trivial\nreasoning about the 3D structure of the output space. However, recent work has\nchallenged this belief, showing that complex encoder-decoder architectures\nperform similarly to nearest-neighbor baselines or simple linear decoder models\nthat exploit large amounts of per category data in standard benchmarks. On the\nother hand settings where 3D shape must be inferred for new categories with few\nexamples are more natural and require models that generalize about shapes. In\nthis work we demonstrate experimentally that naive baselines do not apply when\nthe goal is to learn to reconstruct novel objects using very few examples, and\nthat in a \\emph{few-shot} learning setting, the network must learn concepts\nthat can be applied to new categories, avoiding rote memorization. To address\ndeficiencies in existing approaches to this problem, we propose three\napproaches that efficiently integrate a class prior into a 3D reconstruction\nmodel, allowing to account for intra-class variability and imposing an implicit\ncompositional structure that the model should learn. Experiments on the popular\nShapeNet database demonstrate that our method significantly outperform existing\nbaselines on this task in the few-shot setting.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 04:53:34 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 01:21:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Michalkiewicz", "Mateusz", ""], ["Parisot", "Sarah", ""], ["Tsogkas", "Stavros", ""], ["Baktashmotlagh", "Mahsa", ""], ["Eriksson", "Anders", ""], ["Belilovsky", "Eugene", ""]]}, {"id": "2004.06305", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Tao Ruan, Yunchao Wei, Yi Yang, Tao Mei", "title": "VehicleNet: Learning Robust Visual Representation for Vehicle\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental challenge of vehicle re-identification (re-id) is to learn\nrobust and discriminative visual representation, given the significant\nintra-class vehicle variations across different camera views. As the existing\nvehicle datasets are limited in terms of training images and viewpoints, we\npropose to build a unique large-scale vehicle dataset (called VehicleNet) by\nharnessing four public vehicle datasets, and design a simple yet effective\ntwo-stage progressive approach to learning more robust visual representation\nfrom VehicleNet. The first stage of our approach is to learn the generic\nrepresentation for all domains (i.e., source vehicle datasets) by training with\nthe conventional classification loss. This stage relaxes the full alignment\nbetween the training and testing domains, as it is agnostic to the target\nvehicle domain. The second stage is to fine-tune the trained model purely based\non the target vehicle set, by minimizing the distribution discrepancy between\nour VehicleNet and any target domain. We discuss our proposed multi-source\ndataset VehicleNet and evaluate the effectiveness of the two-stage progressive\nrepresentation learning through extensive experiments. We achieve the\nstate-of-art accuracy of 86.07% mAP on the private test set of AICity\nChallenge, and competitive results on two other public vehicle re-id datasets,\ni.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the\nlearned robust representations can pave the way for vehicle re-id in the\nreal-world environments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 05:06:38 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zheng", "Zhedong", ""], ["Ruan", "Tao", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""], ["Mei", "Tao", ""]]}, {"id": "2004.06320", "submitter": "Mentar Mahmudi", "authors": "Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh\n  Durgesh, Andrew S. Chung, Lorenz Hauswald, Viet Hoang Pham, Maximilian\n  M\\\"uhlegg, Sebastian Dorn, Tiffany Fernandez, Martin J\\\"anicke, Sudesh\n  Mirashi, Chiragkumar Savani, Martin Sturm, Oleksandr Vorobiov, Martin Oelker,\n  Sebastian Garreis, Peter Schuberth", "title": "A2D2: Audi Autonomous Driving Dataset", "comments": "https://www.a2d2.audi/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research in machine learning, mobile robotics, and autonomous driving is\naccelerated by the availability of high quality annotated data. To this end, we\nrelease the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of\nsimultaneously recorded images and 3D point clouds, together with 3D bounding\nboxes, semantic segmentation, instance segmentation, and data extracted from\nthe automotive bus. Our sensor suite consists of six cameras and five LiDAR\nunits, providing full 360 degree coverage. The recorded data is time\nsynchronized and mutually registered. Annotations are for non-sequential\nframes: 41,277 frames with semantic segmentation image and point cloud labels,\nof which 12,497 frames also have 3D bounding box annotations for objects within\nthe field of view of the front camera. In addition, we provide 392,556\nsequential frames of unannotated sensor data for recordings in three cities in\nthe south of Germany. These sequences contain several loops. Faces and vehicle\nnumber plates are blurred due to GDPR legislation and to preserve anonymity.\nA2D2 is made available under the CC BY-ND 4.0 license, permitting commercial\nuse subject to the terms of the license. Data and further information are\navailable at http://www.a2d2.audi.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 06:45:07 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Geyer", "Jakob", ""], ["Kassahun", "Yohannes", ""], ["Mahmudi", "Mentar", ""], ["Ricou", "Xavier", ""], ["Durgesh", "Rupesh", ""], ["Chung", "Andrew S.", ""], ["Hauswald", "Lorenz", ""], ["Pham", "Viet Hoang", ""], ["M\u00fchlegg", "Maximilian", ""], ["Dorn", "Sebastian", ""], ["Fernandez", "Tiffany", ""], ["J\u00e4nicke", "Martin", ""], ["Mirashi", "Sudesh", ""], ["Savani", "Chiragkumar", ""], ["Sturm", "Martin", ""], ["Vorobiov", "Oleksandr", ""], ["Oelker", "Martin", ""], ["Garreis", "Sebastian", ""], ["Schuberth", "Peter", ""]]}, {"id": "2004.06333", "submitter": "Pinhao Song", "authors": "Hong Liu, Pinhao Song, Runwei Ding", "title": "WQT and DG-YOLO: towards domain generalization in underwater object\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A General Underwater Object Detector (GUOD) should perform well on most of\nunderwater circumstances. However, with limited underwater dataset,\nconventional object detection methods suffer from domain shift severely. This\npaper aims to build a GUOD with small underwater dataset with limited types of\nwater quality. First, we propose a data augmentation method Water Quality\nTransfer (WQT) to increase domain diversity of the original small dataset.\nSecond, for mining the semantic information from data generated by WQT, DG-YOLO\nis proposed, which consists of three parts: YOLOv3, DIM and IRM penalty.\nFinally, experiments on original and synthetic URPC2019 dataset prove that\nWQT+DG-YOLO achieves promising performance of domain generalization in\nunderwater object detection.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:36:15 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Liu", "Hong", ""], ["Song", "Pinhao", ""], ["Ding", "Runwei", ""]]}, {"id": "2004.06334", "submitter": "Saket Chaturvedi", "authors": "Saket S. Chaturvedi, Kajol Gupta, Vaishali Ninawe, Prakash S. Prasad", "title": "Automated Diabetic Retinopathy Grading using Deep Convolutional Neural\n  Network", "comments": "\\c{opyright} 20xx IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy is a global health problem, influences 100 million\nindividuals worldwide, and in the next few decades, these incidences are\nexpected to reach epidemic proportions. Diabetic Retinopathy is a subtle eye\ndisease that can cause sudden, irreversible vision loss. The early-stage\nDiabetic Retinopathy diagnosis can be challenging for human experts,\nconsidering the visual complexity of fundus photography retinal images.\nHowever, Early Stage detection of Diabetic Retinopathy can significantly alter\nthe severe vision loss problem. The competence of computer-aided detection\nsystems to accurately detect the Diabetic Retinopathy had popularized them\namong researchers. In this study, we have utilized a pre-trained DenseNet121\nnetwork with several modifications and trained on APTOS 2019 dataset. The\nproposed method outperformed other state-of-the-art networks in early-stage\ndetection and achieved 96.51% accuracy in severity grading of Diabetic\nRetinopathy for multi-label classification and achieved 94.44% accuracy for\nsingle-class classification method. Moreover, the precision, recall, f1-score,\nand quadratic weighted kappa for our network was reported as 86%, 87%, 86%, and\n91.96%, respectively. Our proposed architecture is simultaneously very simple,\naccurate, and efficient concerning computational time and space.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:37:21 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Chaturvedi", "Saket S.", ""], ["Gupta", "Kajol", ""], ["Ninawe", "Vaishali", ""], ["Prasad", "Prakash S.", ""]]}, {"id": "2004.06341", "submitter": "Kensuke Nakamura", "authors": "Kensuke Nakamura, Stefano Soatto, Byung-Woo Hong", "title": "Stochastic batch size for adaptive regularization in deep network\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a first-order stochastic optimization algorithm incorporating\nadaptive regularization applicable to machine learning problems in deep\nlearning framework. The adaptive regularization is imposed by stochastic\nprocess in determining batch size for each model parameter at each optimization\niteration. The stochastic batch size is determined by the update probability of\neach parameter following a distribution of gradient norms in consideration of\ntheir local and global properties in the neural network architecture where the\nrange of gradient norms may vary within and across layers. We empirically\ndemonstrate the effectiveness of our algorithm using an image classification\ntask based on conventional network models applied to commonly used benchmark\ndatasets. The quantitative evaluation indicates that our algorithm outperforms\nthe state-of-the-art optimization algorithms in generalization while providing\nless sensitivity to the selection of batch size which often plays a critical\nrole in optimization, thus achieving more robustness to the selection of\nregularity.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:54:53 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Nakamura", "Kensuke", ""], ["Soatto", "Stefano", ""], ["Hong", "Byung-Woo", ""]]}, {"id": "2004.06366", "submitter": "Trung Quang Tran", "authors": "Trung Q. Tran, Giang V. Nguyen, Daeyoung Kim", "title": "Simple Multi-Resolution Representation Learning for Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation - the process of recognizing human keypoints in a given\nimage - is one of the most important tasks in computer vision and has a wide\nrange of applications including movement diagnostics, surveillance, or\nself-driving vehicle. The accuracy of human keypoint prediction is increasingly\nimproved thanks to the burgeoning development of deep learning. Most existing\nmethods solved human pose estimation by generating heatmaps in which the ith\nheatmap indicates the location confidence of the ith keypoint. In this paper,\nwe introduce novel network structures referred to as multi-resolution\nrepresentation learning for human keypoint prediction. At different resolutions\nin the learning process, our networks branch off and use extra layers to learn\nheatmap generation. We firstly consider the architectures for generating the\nmulti-resolution heatmaps after obtaining the lowest-resolution feature maps.\nOur second approach allows learning during the process of feature extraction in\nwhich the heatmaps are generated at each resolution of the feature extractor.\nThe first and second approaches are referred to as multi-resolution heatmap\nlearning and multi-resolution feature map learning respectively. Our\narchitectures are simple yet effective, achieving good performance. We\nconducted experiments on two common benchmarks for human pose estimation:\nMSCOCO and MPII dataset. The code is made publicly available at\nhttps://github.com/tqtrunghnvn/SimMRPose.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:03:16 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 06:01:11 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Tran", "Trung Q.", ""], ["Nguyen", "Giang V.", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2004.06370", "submitter": "Stefan Haller", "authors": "Stefan Haller, Paul Swoboda, Bogdan Savchynskyy", "title": "Exact MAP-Inference by Confining Combinatorial Search with LP Relaxation", "comments": "32nd AAAI Conference on Artificial Intelligence, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the MAP-inference problem for graphical models, which is a valued\nconstraint satisfaction problem defined on real numbers with a natural\nsummation operation. We propose a family of relaxations (different from the\nfamous Sherali-Adams hierarchy), which naturally define lower bounds for its\noptimum. This family always contains a tight relaxation and we give an\nalgorithm able to find it and therefore, solve the initial non-relaxed NP-hard\nproblem.\n  The relaxations we consider decompose the original problem into two\nnon-overlapping parts: an easy LP-tight part and a difficult one. For the\nlatter part a combinatorial solver must be used. As we show in our experiments,\nin a number of applications the second, difficult part constitutes only a small\nfraction of the whole problem. This property allows to significantly reduce the\ncomputational time of the combinatorial solver and therefore solve problems\nwhich were out of reach before.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:10:47 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Haller", "Stefan", ""], ["Swoboda", "Paul", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "2004.06375", "submitter": "Stefan Haller", "authors": "Stefan Haller, Mangal Prakash, Lisa Hutschenreiter, Tobias Pietzsch,\n  Carsten Rother, Florian Jug, Paul Swoboda, Bogdan Savchynskyy", "title": "A Primal-Dual Solver for Large-Scale Tracking-by-Assignment", "comments": "23rd International Conference on Artificial Intelligence and\n  Statistics (AISTATS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast approximate solver for the combinatorial problem known as\ntracking-by-assignment, which we apply to cell tracking. The latter plays a key\nrole in discovery in many life sciences, especially in cell and developmental\nbiology. So far, in the most general setting this problem was addressed by\noff-the-shelf solvers like Gurobi, whose run time and memory requirements\nrapidly grow with the size of the input. In contrast, for our method this\ngrowth is nearly linear.\n  Our contribution consists of a new (1) decomposable compact representation of\nthe problem; (2) dual block-coordinate ascent method for optimizing the\ndecomposition-based dual; and (3) primal heuristics that reconstructs a\nfeasible integer solution based on the dual information. Compared to solving\nthe problem with Gurobi, we observe an up to~60~times speed-up, while reducing\nthe memory footprint significantly. We demonstrate the efficacy of our method\non real-world tracking problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:26:18 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Haller", "Stefan", ""], ["Prakash", "Mangal", ""], ["Hutschenreiter", "Lisa", ""], ["Pietzsch", "Tobias", ""], ["Rother", "Carsten", ""], ["Jug", "Florian", ""], ["Swoboda", "Paul", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "2004.06376", "submitter": "Michael Firman", "authors": "Jamie Watson, Michael Firman, Aron Monszpart, Gabriel J. Brostow", "title": "Footprints and Free Space from a Single Color Image", "comments": "Accepted to CVPR 2020 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:29:17 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Watson", "Jamie", ""], ["Firman", "Michael", ""], ["Monszpart", "Aron", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "2004.06382", "submitter": "Wei Wang", "authors": "Wei Wang, Shaodi You, Sezer Karaoglu, Theo Gevers", "title": "Kinship Identification through Joint Learning Using Kinship Verification\n  Ensembles", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship verification is a well-explored task: identifying whether or not two\npersons are kin. In contrast, kinship identification has been largely ignored\nso far. Kinship identification aims to further identify the particular type of\nkinship. An extension to kinship verification run short to properly obtain\nidentification, because existing verification networks are individually trained\non specific kinships and do not consider the context between different kinship\ntypes. Also, existing kinship verification datasets have biased\npositive-negative distributions which are different than real-world\ndistributions. To this end, we propose a novel kinship identification approach\nbased on joint training of kinship verification ensembles and classification\nmodules. We propose to rebalance the training dataset to become more realistic.\nLarge scale experiments demonstrate the appealing performance on kinship\nidentification. The experiments further show significant performance\nimprovement of kinship verification when trained on the same dataset with more\nrealistic distributions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:34:25 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 09:32:35 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 11:28:17 GMT"}, {"version": "v4", "created": "Mon, 24 Aug 2020 06:35:42 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wang", "Wei", ""], ["You", "Shaodi", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "2004.06402", "submitter": "Onur Tasar", "authors": "Onur Tasar, Yuliya Tarabalka, Alain Giros, Pierre Alliez, S\\'ebastien\n  Clerc", "title": "StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of\n  Very High Resolution Satellite Images by Data Standardization", "comments": "Accepted at CVPR EarthVision Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation for semantic segmentation has recently been actively\nstudied to increase the generalization capabilities of deep learning models.\nThe vast majority of the domain adaptation methods tackle single-source case,\nwhere the model trained on a single source domain is adapted to a target\ndomain. However, these methods have limited practical real world applications,\nsince usually one has multiple source domains with different data\ndistributions. In this work, we deal with the multi-source domain adaptation\nproblem. Our method, namely StandardGAN, standardizes each source and target\ndomains so that all the data have similar data distributions. We then use the\nstandardized source domains to train a classifier and segment the standardized\ntarget domain. We conduct extensive experiments on two remote sensing data\nsets, in which the first one consists of multiple cities from a single country,\nand the other one contains multiple cities from different countries. Our\nexperimental results show that the standardized data generated by StandardGAN\nallow the classifiers to generate significantly better segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 10:16:50 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Tasar", "Onur", ""], ["Tarabalka", "Yuliya", ""], ["Giros", "Alain", ""], ["Alliez", "Pierre", ""], ["Clerc", "S\u00e9bastien", ""]]}, {"id": "2004.06409", "submitter": "Majed El Helou", "authors": "Majed El Helou and Ruofan Zhou and Frank Schmutz and Fabrice Guibert\n  and Sabine S\\\"usstrunk", "title": "Divergence-Based Adaptive Extreme Video Completion", "comments": null, "journal-ref": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2020)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme image or video completion, where, for instance, we only retain 1% of\npixels in random locations, allows for very cheap sampling in terms of the\nrequired pre-processing. The consequence is, however, a reconstruction that is\nchallenging for humans and inpainting algorithms alike. We propose an extension\nof a state-of-the-art extreme image completion algorithm to extreme video\ncompletion. We analyze a color-motion estimation approach based on color\nKL-divergence that is suitable for extremely sparse scenarios. Our algorithm\nleverages the estimate to adapt between its spatial and temporal filtering when\nreconstructing the sparse randomly-sampled video. We validate our results on 50\npublicly-available videos using reconstruction PSNR and mean opinion scores.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 10:41:07 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Helou", "Majed El", ""], ["Zhou", "Ruofan", ""], ["Schmutz", "Frank", ""], ["Guibert", "Fabrice", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2004.06468", "submitter": "Gu Wang", "authors": "Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab,\n  Federico Tombari", "title": "Self6D: Self-Supervised Monocular 6D Object Pose Estimation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D object pose estimation is a fundamental problem in computer vision.\nConvolutional Neural Networks (CNNs) have recently proven to be capable of\npredicting reliable 6D pose estimates even from monocular images. Nonetheless,\nCNNs are identified as being extremely data-driven, and acquiring adequate\nannotations is oftentimes very time-consuming and labor intensive. To overcome\nthis shortcoming, we propose the idea of monocular 6D pose estimation by means\nof self-supervised learning, removing the need for real annotations. After\ntraining our proposed network fully supervised with synthetic RGB data, we\nleverage recent advances in neural rendering to further self-supervise the\nmodel on unannotated real RGB-D data, seeking for a visually and geometrically\noptimal alignment. Extensive evaluations demonstrate that our proposed\nself-supervision is able to significantly enhance the model's original\nperformance, outperforming all other methods relying on synthetic data or\nemploying elaborate techniques from the domain adaptation realm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:16:36 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 10:33:40 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 23:47:56 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Gu", ""], ["Manhardt", "Fabian", ""], ["Shao", "Jianzhun", ""], ["Ji", "Xiangyang", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2004.06500", "submitter": "Jacob Shermeyer", "authors": "Jacob Shermeyer, Daniel Hogan, Jason Brown, Adam Van Etten, Nicholas\n  Weir, Fabio Pacifici, Ronny Haensch, Alexei Bastidas, Scott Soenen, Todd\n  Bacastow, Ryan Lewis", "title": "SpaceNet 6: Multi-Sensor All Weather Mapping Dataset", "comments": "To appear in CVPR EarthVision Proceedings, 10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Within the remote sensing domain, a diverse set of acquisition modalities\nexist, each with their own unique strengths and weaknesses. Yet, most of the\ncurrent literature and open datasets only deal with electro-optical (optical)\ndata for different detection and segmentation tasks at high spatial\nresolutions. optical data is often the preferred choice for geospatial\napplications, but requires clear skies and little cloud cover to work well.\nConversely, Synthetic Aperture Radar (SAR) sensors have the unique capability\nto penetrate clouds and collect during all weather, day and night conditions.\nConsequently, SAR data are particularly valuable in the quest to aid disaster\nresponse, when weather and cloud cover can obstruct traditional optical\nsensors. Despite all of these advantages, there is little open data available\nto researchers to explore the effectiveness of SAR for such applications,\nparticularly at very-high spatial resolutions, i.e. <1m Ground Sample Distance\n(GSD).\n  To address this problem, we present an open Multi-Sensor All Weather Mapping\n(MSAW) dataset and challenge, which features two collection modalities (both\nSAR and optical). The dataset and challenge focus on mapping and building\nfootprint extraction using a combination of these data sources. MSAW covers 120\nkm^2 over multiple overlapping collects and is annotated with over 48,000\nunique building footprints labels, enabling the creation and evaluation of\nmapping algorithms for multi-modal data. We present a baseline and benchmark\nfor building footprint extraction with SAR data and find that state-of-the-art\nsegmentation models pre-trained on optical data, and then trained on SAR (F1\nscore of 0.21) outperform those trained on SAR data alone (F1 score of 0.135).\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:43:11 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Shermeyer", "Jacob", ""], ["Hogan", "Daniel", ""], ["Brown", "Jason", ""], ["Van Etten", "Adam", ""], ["Weir", "Nicholas", ""], ["Pacifici", "Fabio", ""], ["Haensch", "Ronny", ""], ["Bastidas", "Alexei", ""], ["Soenen", "Scott", ""], ["Bacastow", "Todd", ""], ["Lewis", "Ryan", ""]]}, {"id": "2004.06502", "submitter": "Kangning Liu", "authors": "Kangning Liu, Shuhang Gu, Andres Romero, Radu Timofte", "title": "Unsupervised Multimodal Video-to-Video Translation via Self-Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing unsupervised video-to-video translation methods fail to produce\ntranslated videos which are frame-wise realistic, semantic information\npreserving and video-level consistent. In this work, we propose UVIT, a novel\nunsupervised video-to-video translation model. Our model decomposes the style\nand the content, uses the specialized encoder-decoder structure and propagates\nthe inter-frame information through bidirectional recurrent neural network\n(RNN) units. The style-content decomposition mechanism enables us to achieve\nstyle consistent video translation results as well as provides us with a good\ninterface for modality flexible translation. In addition, by changing the input\nframes and style codes incorporated in our translation, we propose a video\ninterpolation loss, which captures temporal information within the sequence to\ntrain our building blocks in a self-supervised manner. Our model can produce\nphoto-realistic, spatio-temporal consistent translated videos in a multimodal\nway. Subjective and objective experimental results validate the superiority of\nour model over existing methods. More details can be found on our project\nwebsite: https://uvit.netlify.com\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 13:44:30 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Liu", "Kangning", ""], ["Gu", "Shuhang", ""], ["Romero", "Andres", ""], ["Timofte", "Radu", ""]]}, {"id": "2004.06517", "submitter": "Adalberto Claudio Quiros", "authors": "Adalberto Claudio Quiros, Roderick Murray-Smith, and Ke Yuan", "title": "Learning a low dimensional manifold of real cancer tissue with\n  PathologyGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of deep learning in digital pathology shows promise on improving\ndisease diagnosis and understanding. We present a deep generative model that\nlearns to simulate high-fidelity cancer tissue images while mapping the real\nimages onto an interpretable low dimensional latent space. The key to the model\nis an encoder trained by a previously developed generative adversarial network,\nPathologyGAN. We study the latent space using 249K images from two breast\ncancer cohorts. We find that the latent space encodes morphological\ncharacteristics of tissues (e.g. patterns of cancer, lymphocytes, and stromal\ncells). In addition, the latent space reveals distinctly enriched clusters of\ntissue architectures in the high-risk patient group.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:18:00 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Quiros", "Adalberto Claudio", ""], ["Murray-Smith", "Roderick", ""], ["Yuan", "Ke", ""]]}, {"id": "2004.06524", "submitter": "Viktoriia Sharmanska", "authors": "Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, Novi\n  Quadrianto", "title": "Contrastive Examples for Addressing the Tyranny of the Majority", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision algorithms, e.g. for face recognition, favour groups of\nindividuals that are better represented in the training data. This happens\nbecause of the generalization that classifiers have to make. It is simpler to\nfit the majority groups as this fit is more important to overall error. We\npropose to create a balanced training dataset, consisting of the original\ndataset plus new data points in which the group memberships are intervened,\nminorities become majorities and vice versa. We show that current generative\nadversarial networks are a powerful tool for learning these data points, called\ncontrastive examples. We experiment with the equalized odds bias measure on\ntabular data as well as image data (CelebA and Diversity in Faces datasets).\nContrastive examples allow us to expose correlations between group membership\nand other seemingly neutral features. Whenever a causal graph is available, we\ncan put those contrastive examples in the perspective of counterfactuals.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:06:44 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Sharmanska", "Viktoriia", ""], ["Hendricks", "Lisa Anne", ""], ["Darrell", "Trevor", ""], ["Quadrianto", "Novi", ""]]}, {"id": "2004.06550", "submitter": "Radu P Horaud", "authors": "Mostafa Sadeghi, Sylvain Guy, Adrien Raison, Xavier Alameda-Pineda and\n  Radu Horaud", "title": "Unsupervised Performance Analysis of 3D Face Alignment", "comments": "The paper requires major changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of analyzing the performance of 3D face alignment\n(3DFA) algorithms. Traditionally, performance analysis relies on carefully\nannotated datasets. Here, these annotations correspond to the 3D coordinates of\na set of pre-defined facial landmarks. However, this annotation process, be it\nmanual or automatic, is rarely error-free, which strongly biases the analysis.\nIn contrast, we propose a fully unsupervised methodology based on robust\nstatistics and a parametric confidence test. We revisit the problem of robust\nestimation of the rigid transformation between two point sets and we describe\ntwo algorithms, one based on a mixture between a Gaussian and a uniform\ndistribution, and another one based on the generalized Student's\nt-distribution. We show that these methods are robust to up to 50% outliers,\nwhich makes them suitable for mapping a face, from an unknown pose to a frontal\npose, in the presence of facial expressions and occlusions. Using these methods\nin conjunction with large datasets of face images, we build a statistical\nfrontal facial model and an associated parametric confidence metric, eventually\nused for performance analysis. We empirically show that the proposed pipeline\nis neither method-biased nor data-biased, and that it can be used to assess\nboth the performance of 3DFA algorithms and the accuracy of annotations of face\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:33:57 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 13:01:59 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 10:16:15 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 08:37:44 GMT"}, {"version": "v5", "created": "Tue, 25 May 2021 13:23:36 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sadeghi", "Mostafa", ""], ["Guy", "Sylvain", ""], ["Raison", "Adrien", ""], ["Alameda-Pineda", "Xavier", ""], ["Horaud", "Radu", ""]]}, {"id": "2004.06558", "submitter": "Arnaud Dapogny", "authors": "Arnaud Dapogny, K\\'evin Bailly and Matthieu Cord", "title": "Deep Entwined Learning Head Pose and Face Alignment Inside an\n  Attentional Cascade with Doubly-Conditional fusion", "comments": "Accepted for publication as an oral session @IEEE FG2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head pose estimation and face alignment constitute a backbone preprocessing\nfor many applications relying on face analysis. While both are closely related\ntasks, they are generally addressed separately, e.g. by deducing the head pose\nfrom the landmark locations. In this paper, we propose to entwine face\nalignment and head pose tasks inside an attentional cascade. This cascade uses\na geometry transfer network for integrating heterogeneous annotations to\nenhance landmark localization accuracy. Furthermore, we propose a\ndoubly-conditional fusion scheme to select relevant feature maps, and regions\nthereof, based on a current head pose and landmark localization estimate. We\nempirically show the benefit of entwining head pose and landmark localization\nobjectives inside our architecture, and that the proposed AC-DC model enhances\nthe state-of-the-art accuracy on multiple databases for both face alignment and\nhead pose estimation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 14:42:35 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Dapogny", "Arnaud", ""], ["Bailly", "K\u00e9vin", ""], ["Cord", "Matthieu", ""]]}, {"id": "2004.06567", "submitter": "Dominik Fay", "authors": "Dominik Fay, Jens Sj\\\"olund and Tobias J. Oechtering", "title": "Decentralized Differentially Private Segmentation with PATE", "comments": "Under review for MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to preserving privacy in medical machine learning, two\nimportant considerations are (1) keeping data local to the institution and (2)\navoiding inference of sensitive information from the trained model. These are\noften addressed using federated learning and differential privacy,\nrespectively. However, the commonly used Federated Averaging algorithm requires\na high degree of synchronization between participating institutions. For this\nreason, we turn our attention to Private Aggregation of Teacher Ensembles\n(PATE), where all local models can be trained independently without\ninter-institutional communication. The purpose of this paper is thus to explore\nhow PATE -- originally designed for classification -- can best be adapted for\nsemantic segmentation. To this end, we build low-dimensional representations of\nsegmentation masks which the student can obtain through low-sensitivity queries\nto the private aggregator. On the Brain Tumor Segmentation (BraTS 2019)\ndataset, an Autoencoder-based PATE variant achieves a higher Dice coefficient\nfor the same privacy guarantee than prior work based on noisy Federated\nAveraging.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 00:05:48 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Fay", "Dominik", ""], ["Sj\u00f6lund", "Jens", ""], ["Oechtering", "Tobias J.", ""]]}, {"id": "2004.06568", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Rita SahaRay, Sayan Chakrabarty, Sayan Bhadra", "title": "Robust Generalised Quadratic Discriminant Analysis", "comments": "Pre-print. Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic discriminant analysis (QDA) is a widely used statistical tool to\nclassify observations from different multivariate Normal populations. The\ngeneralized quadratic discriminant analysis (GQDA) classification\nrule/classifier, which generalizes the QDA and the minimum Mahalanobis distance\n(MMD) classifiers to discriminate between populations with underlying\nelliptically symmetric distributions competes quite favorably with the QDA\nclassifier when it is optimal and performs much better when QDA fails under\nnon-Normal underlying distributions, e.g. Cauchy distribution. However, the\nclassification rule in GQDA is based on the sample mean vector and the sample\ndispersion matrix of a training sample, which are extremely non-robust under\ndata contamination. In real world, since it is quite common to face data highly\nvulnerable to outliers, the lack of robustness of the classical estimators of\nthe mean vector and the dispersion matrix reduces the efficiency of the GQDA\nclassifier significantly, increasing the misclassification errors. The present\npaper investigates the performance of the GQDA classifier when the classical\nestimators of the mean vector and the dispersion matrix used therein are\nreplaced by various robust counterparts. Applications to various real data sets\nas well as simulation studies reveal far better performance of the proposed\nrobust versions of the GQDA classifier. A Comparative study has been made to\nadvocate the appropriate choice of the robust estimators to be used in a\nspecific situation of the degree of contamination of the data sets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 18:21:06 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ghosh", "Abhik", ""], ["SahaRay", "Rita", ""], ["Chakrabarty", "Sayan", ""], ["Bhadra", "Sayan", ""]]}, {"id": "2004.06569", "submitter": "Davood Karimi", "authors": "Davood Karimi, Ali Gholipour", "title": "Improving Calibration and Out-of-Distribution Detection in Medical Image\n  Segmentation with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have shown to be powerful medical image\nsegmentation models. In this study, we address some of the main unresolved\nissues regarding these models. Specifically, training of these models on small\nmedical image datasets is still challenging, with many studies promoting\ntechniques such as transfer learning. Moreover, these models are infamous for\nproducing over-confident predictions and for failing silently when presented\nwith out-of-distribution (OOD) data at test time. In this paper, we advocate\nfor multi-task learning, i.e., training a single model on several different\ndatasets, spanning several different organs of interest and different imaging\nmodalities. We show that not only a single CNN learns to automatically\nrecognize the context and accurately segment the organ of interest in each\ncontext, but also that such a joint model often has more accurate and\nbetter-calibrated predictions than dedicated models trained separately on each\ndataset. Our experiments show that multi-task learning can outperform transfer\nlearning in medical image segmentation tasks. For detecting OOD data, we\npropose a method based on spectral analysis of CNN feature maps. We show that\ndifferent datasets, representing different imaging modalities and/or different\norgans of interest, have distinct spectral signatures, which can be used to\nidentify whether or not a test image is similar to the images used to train a\nmodel. We show that this approach is far more accurate than OOD detection based\non prediction uncertainty. The methods proposed in this paper contribute\nsignificantly to improving the accuracy and reliability of CNN-based medical\nimage segmentation models.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:42:51 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 13:52:33 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Karimi", "Davood", ""], ["Gholipour", "Ali", ""]]}, {"id": "2004.06578", "submitter": "Muhammad E. H. Chowdhury", "authors": "Tawsifur Rahman, Muhammad E. H. Chowdhury, Amith Khandakar, Khandaker\n  R. Islam, Khandaker F. Islam, Zaid B. Mahbub, Muhammad A. Kadir, Saad Kashem", "title": "Transfer Learning with Deep Convolutional Neural Network (CNN) for\n  Pneumonia Detection using Chest X-ray", "comments": "13 Figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2003.13145", "journal-ref": "Appl. Sci. 2020, 10(9), 3233", "doi": "10.3390/app10093233", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumonia is a life-threatening disease, which occurs in the lungs caused by\neither bacterial or viral infection. It can be life-endangering if not acted\nupon in the right time and thus an early diagnosis of pneumonia is vital. The\naim of this paper is to automatically detect bacterial and viral pneumonia\nusing digital x-ray images. It provides a detailed report on advances made in\nmaking accurate detection of pneumonia and then presents the methodology\nadopted by the authors. Four different pre-trained deep Convolutional Neural\nNetwork (CNN)- AlexNet, ResNet18, DenseNet201, and SqueezeNet were used for\ntransfer learning. 5247 Bacterial, viral and normal chest x-rays images\nunderwent preprocessing techniques and the modified images were trained for the\ntransfer learning based classification task. In this work, the authors have\nreported three schemes of classifications: normal vs pneumonia, bacterial vs\nviral pneumonia and normal, bacterial and viral pneumonia. The classification\naccuracy of normal and pneumonia images, bacterial and viral pneumonia images,\nand normal, bacterial and viral pneumonia were 98%, 95%, and 93.3%\nrespectively. This is the highest accuracy in any scheme than the accuracies\nreported in the literature. Therefore, the proposed study can be useful in\nfaster-diagnosing pneumonia by the radiologist and can help in the fast airport\nscreening of pneumonia patients.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:03:48 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Rahman", "Tawsifur", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Islam", "Khandaker R.", ""], ["Islam", "Khandaker F.", ""], ["Mahbub", "Zaid B.", ""], ["Kadir", "Muhammad A.", ""], ["Kashem", "Saad", ""]]}, {"id": "2004.06587", "submitter": "Andre Kelm", "authors": "Andr\\'e Peter Kelm and Udo Z\\\"olzer", "title": "Walk the Lines: Object Contour Tracing CNN for Contour Completion of\n  Ships", "comments": "Submission to the ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new contour tracing algorithm to enhance the results of the\nlatest object contour detectors. The goal is to achieve a perfectly closed, 1\npixel wide and detailed object contour, since this type of contour could be\nanalyzed using methods such as Fourier descriptors. Convolutional Neural\nNetworks (CNNs) are rarely used for contour tracing. However, we find CNNs are\ntailor-made for this task and that's why we present the Walk the Lines (WtL)\nalgorithm, a standard regression CNN trained to follow object contours. To make\nthe first step, we train the CNN only on ship contours, but the principle is\nalso applicable to other objects. Input data are the image and the associated\nobject contour prediction of the recently published RefineContourNet. The WtL\ngets a center pixel, which defines an input section and an angle for rotating\nthis section. Ideally, the center pixel moves on the contour, while the angle\ndescribes upcoming directional contour changes. The WtL predicts its steps\npixelwise in a selfrouting way. To obtain a complete object contour the WtL\nruns in parallel at different image locations and the traces of its individual\npaths are summed. In contrast to the comparable Non-Maximum Suppression method,\nour approach produces connected contours with finer details. Finally, the\nobject contour is binarized under the condition of being closed. In case all\nprocedures work as desired, excellent ship segmentations with high IoUs are\nproduced, showing details such as antennas and ship superstructures that are\neasily omitted by other segmentation methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:19:04 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Kelm", "Andr\u00e9 Peter", ""], ["Z\u00f6lzer", "Udo", ""]]}, {"id": "2004.06592", "submitter": "Aythami Morales", "authors": "Ignacio Serna, Alejandro Pe\\~na, Aythami Morales, and Julian Fierrez", "title": "InsideBias: Measuring Bias in Deep Networks and Application to Face\n  Gender Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the biases in learning processes based on deep neural\nnetwork architectures. We analyze how bias affects deep learning processes\nthrough a toy example using the MNIST database and a case study in gender\ndetection from face images. We employ two gender detection models based on\npopular deep neural networks. We present a comprehensive analysis of bias\neffects when using an unbalanced training dataset on the features learned by\nthe models. We show how bias impacts in the activations of gender detection\nmodels based on face images. We finally propose InsideBias, a novel method to\ndetect biased models. InsideBias is based on how the models represent the\ninformation instead of how they perform, which is the normal practice in other\nexisting methods for bias detection. Our strategy with InsideBias allows to\ndetect biased models with very few samples (only 15 images in our case study).\nOur experiments include 72K face images from 24K identities and 3 ethnic\ngroups.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:20:50 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 16:01:44 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 10:24:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Serna", "Ignacio", ""], ["Pe\u00f1a", "Alejandro", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""]]}, {"id": "2004.06638", "submitter": "Nanxuan Zhao", "authors": "Nanxuan Zhao, Zhirong Wu, Rynson W.H. Lau, Stephen Lin", "title": "Distilling Localization for Self-Supervised Representation Learning", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in contrastive learning has revolutionized unsupervised\nrepresentation learning. Concretely, multiple views (augmentations) from the\nsame image are encouraged to map to the similar embeddings, while views from\ndifferent images are pulled apart. In this paper, through visualizing and\ndiagnosing classification errors, we observe that current contrastive models\nare ineffective at localizing the foreground object, limiting their ability to\nextract discriminative high-level features. This is due to the fact that view\ngeneration process considers pixels in an image uniformly. To address this\nproblem, we propose a data-driven approach for learning invariance to\nbackgrounds. It first estimates foreground saliency in images and then creates\naugmentations by copy-and-pasting the foreground onto a variety of backgrounds.\nThe learning still follows the instance discrimination pretext task, so that\nthe representation is trained to disregard background content and focus on the\nforeground. We study a variety of saliency estimation methods, and find that\nmost methods lead to improvements for contrastive learning. With this approach\n(DiLo), significant performance is achieved for self-supervised learning on\nImageNet classification, and also for object detection on PASCAL VOC and\nMSCOCO.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:29:42 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 15:45:14 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhao", "Nanxuan", ""], ["Wu", "Zhirong", ""], ["Lau", "Rynson W. H.", ""], ["Lin", "Stephen", ""]]}, {"id": "2004.06643", "submitter": "Hanxiang Hao", "authors": "Hanxiang Hao, Sriram Baireddy, Emily R. Bartusiak, Latisha Konz, Kevin\n  LaTourette, Michael Gribbons, Moses Chan, Mary L. Comer, Edward J. Delp", "title": "An Attention-Based System for Damage Assessment Using Satellite Imagery", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When disaster strikes, accurate situational information and a fast, effective\nresponse are critical to save lives. Widely available, high resolution\nsatellite images enable emergency responders to estimate locations, causes, and\nseverity of damage. Quickly and accurately analyzing the extensive amount of\nsatellite imagery available, though, requires an automatic approach. In this\npaper, we present Siam-U-Net-Attn model - a multi-class deep learning model\nwith an attention mechanism - to assess damage levels of buildings given a pair\nof satellite images depicting a scene before and after a disaster. We evaluate\nthe proposed method on xView2, a large-scale building damage assessment\ndataset, and demonstrate that the proposed approach achieves accurate damage\nscale classification and building segmentation results simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:37:55 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Hao", "Hanxiang", ""], ["Baireddy", "Sriram", ""], ["Bartusiak", "Emily R.", ""], ["Konz", "Latisha", ""], ["LaTourette", "Kevin", ""], ["Gribbons", "Michael", ""], ["Chan", "Moses", ""], ["Comer", "Mary L.", ""], ["Delp", "Edward J.", ""]]}, {"id": "2004.06657", "submitter": "Ioanna Ntinou", "authors": "Ioanna Ntinou and Enrique Sanchez and Adrian Bulat and Michel Valstar\n  and Georgios Tzimiropoulos", "title": "A Transfer Learning approach to Heatmap Regression for Action Unit\n  intensity estimation", "comments": "Submitted for review to IEEE Trans. on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action Units (AUs) are geometrically-based atomic facial muscle movements\nknown to produce appearance changes at specific facial locations. Motivated by\nthis observation we propose a novel AU modelling problem that consists of\njointly estimating their localisation and intensity. To this end, we propose a\nsimple yet efficient approach based on Heatmap Regression that merges both\nproblems into a single task. A Heatmap models whether an AU occurs or not at a\ngiven spatial location. To accommodate the joint modelling of AUs intensity, we\npropose variable size heatmaps, with their amplitude and size varying according\nto the labelled intensity. Using Heatmap Regression, we can inherit from the\nprogress recently witnessed in facial landmark localisation. Building upon the\nsimilarities between both problems, we devise a transfer learning approach\nwhere we exploit the knowledge of a network trained on large-scale facial\nlandmark datasets. In particular, we explore different alternatives for\ntransfer learning through a) fine-tuning, b) adaptation layers, c) attention\nmaps, and d) reparametrisation. Our approach effectively inherits the rich\nfacial features produced by a strong face alignment network, with minimal extra\ncomputational cost. We empirically validate that our system sets a new\nstate-of-the-art on three popular datasets, namely BP4D, DISFA, and FERA2017.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:51:13 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ntinou", "Ioanna", ""], ["Sanchez", "Enrique", ""], ["Bulat", "Adrian", ""], ["Valstar", "Michel", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2004.06673", "submitter": "Tongxue Zhou", "authors": "Tongxue Zhou, St\\'ephane Canu, Su Ruan", "title": "An automatic COVID-19 CT segmentation network using spatial and channel\n  attention mechanism", "comments": "14 pages, 6 figures", "journal-ref": "International journal of imaging systems and technology, 2020", "doi": "10.1002/ima.22527", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease (COVID-19) pandemic has led to a devastating effect\non the global public health. Computed Tomography (CT) is an effective tool in\nthe screening of COVID-19. It is of great importance to rapidly and accurately\nsegment COVID-19 from CT to help diagnostic and patient monitoring. In this\npaper, we propose a U-Net based segmentation network using attention mechanism.\nAs not all the features extracted from the encoders are useful for\nsegmentation, we propose to incorporate an attention mechanism including a\nspatial and a channel attention, to a U-Net architecture to re-weight the\nfeature representation spatially and channel-wise to capture rich contextual\nrelationships for better feature representation. In addition, the focal tversky\nloss is introduced to deal with small lesion segmentation. The experiment\nresults, evaluated on a COVID-19 CT segmentation dataset where 473 CT slices\nare available, demonstrate the proposed method can achieve an accurate and\nrapid segmentation on COVID-19 segmentation. The method takes only 0.29 second\nto segment a single CT slice. The obtained Dice Score, Sensitivity and\nSpecificity are 83.1%, 86.7% and 99.3%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:21:11 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 16:31:04 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 09:21:19 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 13:16:08 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zhou", "Tongxue", ""], ["Canu", "St\u00e9phane", ""], ["Ruan", "Su", ""]]}, {"id": "2004.06675", "submitter": "Firoj Alam", "authors": "Muhammad Imran, Firoj Alam, Umair Qazi, Steve Peterson and Ferda Ofli", "title": "Rapid Damage Assessment Using Social Media Images by Combining Human and\n  Machine Intelligence", "comments": "Accepted at ISCRAM 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid damage assessment is one of the core tasks that response organizations\nperform at the onset of a disaster to understand the scale of damage to\ninfrastructures such as roads, bridges, and buildings. This work analyzes the\nusefulness of social media imagery content to perform rapid damage assessment\nduring a real-world disaster. An automatic image processing system, which was\nactivated in collaboration with a volunteer response organization, processed\n~280K images to understand the extent of damage caused by the disaster. The\nsystem achieved an accuracy of 76% computed based on the feedback received from\nthe domain experts who analyzed ~29K system-processed images during the\ndisaster. An extensive error analysis reveals several insights and challenges\nfaced by the system, which are vital for the research community to advance this\nline of research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:26:36 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Imran", "Muhammad", ""], ["Alam", "Firoj", ""], ["Qazi", "Umair", ""], ["Peterson", "Steve", ""], ["Ofli", "Ferda", ""]]}, {"id": "2004.06688", "submitter": "Anuroop Sriram", "authors": "Anuroop Sriram, Jure Zbontar, Tullie Murrell, Aaron Defazio, C.\n  Lawrence Zitnick, Nafissa Yakubova, Florian Knoll, and Patricia Johnson", "title": "End-to-End Variational Networks for Accelerated MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The slow acquisition speed of magnetic resonance imaging (MRI) has led to the\ndevelopment of two complementary methods: acquiring multiple views of the\nanatomy simultaneously (parallel imaging) and acquiring fewer samples than\nnecessary for traditional signal processing methods (compressed sensing). While\nthe combination of these methods has the potential to allow much faster scan\ntimes, reconstruction from such undersampled multi-coil data has remained an\nopen problem. In this paper, we present a new approach to this problem that\nextends previously proposed variational methods by learning fully end-to-end.\nOur method obtains new state-of-the-art results on the fastMRI dataset for both\nbrain and knee MRIs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:42:13 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 04:02:57 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Sriram", "Anuroop", ""], ["Zbontar", "Jure", ""], ["Murrell", "Tullie", ""], ["Defazio", "Aaron", ""], ["Zitnick", "C. Lawrence", ""], ["Yakubova", "Nafissa", ""], ["Knoll", "Florian", ""], ["Johnson", "Patricia", ""]]}, {"id": "2004.06689", "submitter": "Guang Yang A", "authors": "Shaoping Hu, Yuan Gao, Zhangming Niu, Yinghui Jiang, Lao Li, Xianglu\n  Xiao, Minhao Wang, Evandro Fei Fang, Wade Menpes-Smith, Jun Xia, Hui Ye and\n  Guang Yang", "title": "Weakly Supervised Deep Learning for COVID-19 Infection Detection and\n  Classification from CT Images", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An outbreak of a novel coronavirus disease (i.e., COVID-19) has been recorded\nin Wuhan, China since late December 2019, which subsequently became pandemic\naround the world. Although COVID-19 is an acutely treated disease, it can also\nbe fatal with a risk of fatality of 4.03% in China and the highest of 13.04% in\nAlgeria and 12.67% Italy (as of 8th April 2020). The onset of serious illness\nmay result in death as a consequence of substantial alveolar damage and\nprogressive respiratory failure. Although laboratory testing, e.g., using\nreverse transcription polymerase chain reaction (RT-PCR), is the golden\nstandard for clinical diagnosis, the tests may produce false negatives.\nMoreover, under the pandemic situation, shortage of RT-PCR testing resources\nmay also delay the following clinical decision and treatment. Under such\ncircumstances, chest CT imaging has become a valuable tool for both diagnosis\nand prognosis of COVID-19 patients. In this study, we propose a weakly\nsupervised deep learning strategy for detecting and classifying COVID-19\ninfection from CT images. The proposed method can minimise the requirements of\nmanual labelling of CT images but still be able to obtain accurate infection\ndetection and distinguish COVID-19 from non-COVID-19 cases. Based on the\npromising results obtained qualitatively and quantitatively, we can envisage a\nwide deployment of our developed technique in large-scale clinical studies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:45:03 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Hu", "Shaoping", ""], ["Gao", "Yuan", ""], ["Niu", "Zhangming", ""], ["Jiang", "Yinghui", ""], ["Li", "Lao", ""], ["Xiao", "Xianglu", ""], ["Wang", "Minhao", ""], ["Fang", "Evandro Fei", ""], ["Menpes-Smith", "Wade", ""], ["Xia", "Jun", ""], ["Ye", "Hui", ""], ["Yang", "Guang", ""]]}, {"id": "2004.06698", "submitter": "Gi-Cheon Kang", "authors": "Gi-Cheon Kang, Junseok Park, Hwaran Lee, Byoung-Tak Zhang, Jin-Hwa Kim", "title": "DialGraph: Sparse Graph Learning Networks for Visual Dialog", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog is a task of answering a sequence of questions grounded in an\nimage utilizing a dialog history. Previous studies have implicitly explored the\nproblem of reasoning semantic structures among the history using softmax\nattention. However, we argue that the softmax attention yields dense structures\nthat could distract to answer the questions requiring partial or even no\ncontextual information. In this paper, we formulate the visual dialog tasks as\ngraph structure learning tasks. To tackle the problem, we propose Sparse Graph\nLearning Networks (SGLNs) consisting of a multimodal node embedding module and\na sparse graph learning module. The proposed model explicitly learn sparse\ndialog structures by incorporating binary and score edges, leveraging a new\nstructural loss function. Then, it finally outputs the answer, updating each\nnode via a message passing framework. As a result, the proposed model\noutperforms the state-of-the-art approaches on the VisDial v1.0 dataset, only\nusing 10.95% of the dialog history, as well as improves interpretability\ncompared to baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:52:41 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Kang", "Gi-Cheon", ""], ["Park", "Junseok", ""], ["Lee", "Hwaran", ""], ["Zhang", "Byoung-Tak", ""], ["Kim", "Jin-Hwa", ""]]}, {"id": "2004.06704", "submitter": "Yue Zhao", "authors": "Dian Shao, Yue Zhao, Bo Dai and Dahua Lin", "title": "FineGym: A Hierarchical Video Dataset for Fine-grained Action\n  Understanding", "comments": "CVPR 2020 Oral (3 strong accepts); Project page:\n  https://sdolivia.github.io/FineGym/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On public benchmarks, current action recognition techniques have achieved\ngreat success. However, when used in real-world applications, e.g. sport\nanalysis, which requires the capability of parsing an activity into phases and\ndifferentiating between subtly different actions, their performances remain far\nfrom being satisfactory. To take action recognition to a new level, we develop\nFineGym, a new dataset built on top of gymnastic videos. Compared to existing\naction recognition datasets, FineGym is distinguished in richness, quality, and\ndiversity. In particular, it provides temporal annotations at both action and\nsub-action levels with a three-level semantic hierarchy. For example, a\n\"balance beam\" event will be annotated as a sequence of elementary sub-actions\nderived from five sets: \"leap-jump-hop\", \"beam-turns\", \"flight-salto\",\n\"flight-handspring\", and \"dismount\", where the sub-action in each set will be\nfurther annotated with finely defined class labels. This new level of\ngranularity presents significant challenges for action recognition, e.g. how to\nparse the temporal structures from a coherent action, and how to distinguish\nbetween subtly different action classes. We systematically investigate\nrepresentative methods on this dataset and obtain a number of interesting\nfindings. We hope this dataset could advance research towards action\nunderstanding.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:55:21 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Shao", "Dian", ""], ["Zhao", "Yue", ""], ["Dai", "Bo", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.06711", "submitter": "Weilin Huang", "authors": "Yuechen Yu, Yilei Xiong, Weilin Huang, Matthew R. Scott", "title": "Deformable Siamese Attention Networks for Visual Object Tracking", "comments": "CVPR 2020, with code available at:\n  https://github.com/msight-tech/research-siamattn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese-based trackers have achieved excellent performance on visual object\ntracking. However, the target template is not updated online, and the features\nof the target template and search image are computed independently in a Siamese\narchitecture. In this paper, we propose Deformable Siamese Attention Networks,\nreferred to as SiamAttn, by introducing a new Siamese attention mechanism that\ncomputes deformable self-attention and cross-attention. The self attention\nlearns strong context information via spatial attention, and selectively\nemphasizes interdependent channel-wise features with channel attention. The\ncross-attention is capable of aggregating rich contextual inter-dependencies\nbetween the target template and the search image, providing an implicit manner\nto adaptively update the target template. In addition, we design a region\nrefinement module that computes depth-wise cross correlations between the\nattentional features for more accurate tracking. We conduct experiments on six\nbenchmarks, where our method achieves new state of-the-art results,\noutperforming the strong baseline, SiamRPN++ [24], by 0.464->0.537 and\n0.415->0.470 EAO on VOT 2016 and 2018. Our code is available at:\nhttps://github.com/msight-tech/research-siamattn.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:59:08 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 04:07:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yu", "Yuechen", ""], ["Xiong", "Yilei", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "2004.06718", "submitter": "Zhang Qian", "authors": "Zhang Qian, Wang Bo, Wen Wei, Li Hai, Liu Jun Hui", "title": "Line Art Correlation Matching Feature Transfer Network for Automatic\n  Animation Colorization", "comments": "8pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic animation line art colorization is a challenging computer vision\nproblem, since the information of the line art is highly sparse and abstracted\nand there exists a strict requirement for the color and style consistency\nbetween frames. Recently, a lot of Generative Adversarial Network (GAN) based\nimage-to-image translation methods for single line art colorization have\nemerged. They can generate perceptually appealing results conditioned on line\nart images. However, these methods can not be adopted for the purpose of\nanimation colorization because there is a lack of consideration of the\nin-between frame consistency. Existing methods simply input the previous\ncolored frame as a reference to color the next line art, which will mislead the\ncolorization due to the spatial misalignment of the previous colored frame and\nthe next line art especially at positions where apparent changes happen. To\naddress these challenges, we design a kind of correlation matching feature\ntransfer model (called CMFT) to align the colored reference feature in a\nlearnable way and integrate the model into an U-Net based generator in a\ncoarse-to-fine manner. This enables the generator to transfer the layer-wise\nsynchronized features from the deep semantic code to the content progressively.\nExtension evaluation shows that CMFT model can effectively improve the\nin-between consistency and the quality of colored frames especially when the\nmotion is intense and diverse.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 06:50:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 13:21:23 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 09:03:16 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Qian", "Zhang", ""], ["Bo", "Wang", ""], ["Wei", "Wen", ""], ["Hai", "Li", ""], ["Hui", "Liu Jun", ""]]}, {"id": "2004.06780", "submitter": "Naoufel Werghi Dr.", "authors": "Taimur Hassan, Samet Akcay, Mohammed Bennamoun, Salman Khan, Naoufel\n  Werghi", "title": "Cascaded Structure Tensor Framework for Robust Identification of Heavily\n  Occluded Baggage Items from X-ray Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, baggage scanning has globally become one of the\nprime aviation security concerns. Manual screening of the baggage items is\ntedious, error-prone, and compromise privacy. Hence, many researchers have\ndeveloped X-ray imagery-based autonomous systems to address these shortcomings.\nThis paper presents a cascaded structure tensor framework that can\nautomatically extract and recognize suspicious items in heavily occluded and\ncluttered baggage. The proposed framework is unique, as it intelligently\nextracts each object by iteratively picking contour-based transitional\ninformation from different orientations and uses only a single feed-forward\nconvolutional neural network for the recognition. The proposed framework has\nbeen rigorously evaluated using a total of 1,067,381 X-ray scans from publicly\navailable GDXray and SIXray datasets where it outperformed the state-of-the-art\nsolutions by achieving the mean average precision score of 0.9343 on GDXray and\n0.9595 on SIXray for recognizing the highly cluttered and overlapping\nsuspicious items. Furthermore, the proposed framework computationally achieves\n4.76\\% superior run-time performance as compared to the existing solutions\nbased on publicly available object detectors\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 20:00:55 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Hassan", "Taimur", ""], ["Akcay", "Samet", ""], ["Bennamoun", "Mohammed", ""], ["Khan", "Salman", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2004.06799", "submitter": "Roozbeh Mottaghi", "authors": "Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric\n  Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt,\n  Matthew Wallingford, Luca Weihs, Mark Yatskar, Ali Farhadi", "title": "RoboTHOR: An Open Simulation-to-Real Embodied AI Platform", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition ecosystems (e.g. ImageNet, Pascal, COCO) have undeniably\nplayed a prevailing role in the evolution of modern computer vision. We argue\nthat interactive and embodied visual AI has reached a stage of development\nsimilar to visual recognition prior to the advent of these ecosystems.\nRecently, various synthetic environments have been introduced to facilitate\nresearch in embodied AI. Notwithstanding this progress, the crucial question of\nhow well models trained in simulation generalize to reality has remained\nlargely unanswered. The creation of a comparable ecosystem for\nsimulation-to-real embodied AI presents many challenges: (1) the inherently\ninteractive nature of the problem, (2) the need for tight alignments between\nreal and simulated worlds, (3) the difficulty of replicating physical\nconditions for repeatable experiments, (4) and the associated cost. In this\npaper, we introduce RoboTHOR to democratize research in interactive and\nembodied visual AI. RoboTHOR offers a framework of simulated environments\npaired with physical counterparts to systematically explore and overcome the\nchallenges of simulation-to-real transfer, and a platform where researchers\nacross the globe can remotely test their embodied models in the physical world.\nAs a first benchmark, our experiments show there exists a significant gap\nbetween the performance of models trained in simulation when they are tested in\nboth simulations and their carefully constructed physical analogs. We hope that\nRoboTHOR will spur the next stage of evolution in embodied computer vision.\nRoboTHOR can be accessed at the following link:\nhttps://ai2thor.allenai.org/robothor\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 20:52:49 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Deitke", "Matt", ""], ["Han", "Winson", ""], ["Herrasti", "Alvaro", ""], ["Kembhavi", "Aniruddha", ""], ["Kolve", "Eric", ""], ["Mottaghi", "Roozbeh", ""], ["Salvador", "Jordi", ""], ["Schwenk", "Dustin", ""], ["VanderBilt", "Eli", ""], ["Wallingford", "Matthew", ""], ["Weihs", "Luca", ""], ["Yatskar", "Mark", ""], ["Farhadi", "Ali", ""]]}, {"id": "2004.06816", "submitter": "Hoel Kervadec", "authors": "Hoel Kervadec, Jose Dolz, Shanshan Wang, Eric Granger, Ismail Ben Ayed", "title": "Bounding boxes for weakly supervised segmentation: Global constraints\n  get close to full supervision", "comments": "Full paper, accepted for presentation at MIDL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel weakly supervised learning segmentation based on several\nglobal constraints derived from box annotations. Particularly, we leverage a\nclassical tightness prior to a deep learning setting via imposing a set of\nconstraints on the network outputs. Such a powerful topological prior prevents\nsolutions from excessive shrinking by enforcing any horizontal or vertical line\nwithin the bounding box to contain, at least, one pixel of the foreground\nregion. Furthermore, we integrate our deep tightness prior with a global\nbackground emptiness constraint, guiding training with information outside the\nbounding box. We demonstrate experimentally that such a global constraint is\nmuch more powerful than standard cross-entropy for the background class. Our\noptimization problem is challenging as it takes the form of a large set of\ninequality constraints on the outputs of deep networks. We solve it with\nsequence of unconstrained losses based on a recent powerful extension of the\nlog-barrier method, which is well-known in the context of interior-point\nmethods. This accommodates standard stochastic gradient descent (SGD) for\ntraining deep networks, while avoiding computationally expensive and unstable\nLagrangian dual steps and projections. Extensive experiments over two different\npublic data sets and applications (prostate and brain lesions) demonstrate that\nthe synergy between our global tightness and emptiness priors yield very\ncompetitive performances, approaching full supervision and outperforming\nsignificantly DeepCut. Furthermore, our approach removes the need for\ncomputationally expensive proposal generation. Our code is shared anonymously.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 22:11:22 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Kervadec", "Hoel", ""], ["Dolz", "Jose", ""], ["Wang", "Shanshan", ""], ["Granger", "Eric", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2004.06824", "submitter": "Md Hasib Zunair", "authors": "Hasib Zunair and A. Ben Hamza", "title": "Melanoma Detection using Adversarial Training and Deep Transfer Learning", "comments": "Published in the Journal of Physics in Medicine and Biology (PMB),\n  April 2020. Codes at https://github.com/hasibzunair/adversarial-lesions", "journal-ref": null, "doi": "10.1088/1361-6560/ab86d3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion datasets consist predominantly of normal samples with only a\nsmall percentage of abnormal ones, giving rise to the class imbalance problem.\nAlso, skin lesion images are largely similar in overall appearance owing to the\nlow inter-class variability. In this paper, we propose a two-stage framework\nfor automatic classification of skin lesion images using adversarial training\nand transfer learning toward melanoma detection. In the first stage, we\nleverage the inter-class variation of the data distribution for the task of\nconditional image synthesis by learning the inter-class mapping and\nsynthesizing under-represented class samples from the over-represented ones\nusing unpaired image-to-image translation. In the second stage, we train a deep\nconvolutional neural network for skin lesion classification using the original\ntraining set combined with the newly synthesized under-represented class\nsamples. The training of this classifier is carried out by minimizing the focal\nloss function, which assists the model in learning from hard examples, while\ndown-weighting the easy ones. Experiments conducted on a dermatology image\nbenchmark demonstrate the superiority of our proposed approach over several\nstandard baseline methods, achieving significant performance improvements.\nInterestingly, we show through feature visualization and analysis that our\nmethod leads to context based lesion assessment that can reach an expert\ndermatologist level.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 22:46:20 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 16:46:09 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zunair", "Hasib", ""], ["Hamza", "A. Ben", ""]]}, {"id": "2004.06848", "submitter": "Kyle Olszewski", "authors": "Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria, Zhili Chen,\n  Weikai Chen, Hao Li", "title": "Intuitive, Interactive Beard and Hair Synthesis with Generative Models", "comments": "To be presented in the 2020 Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020, Oral Presentation). Supplementary video can be seen\n  at: https://www.youtube.com/watch?v=v4qOtBATrvM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive approach to synthesizing realistic variations in\nfacial hair in images, ranging from subtle edits to existing hair to the\naddition of complex and challenging hair in images of clean-shaven subjects. To\ncircumvent the tedious and computationally expensive tasks of modeling,\nrendering and compositing the 3D geometry of the target hairstyle using the\ntraditional graphics pipeline, we employ a neural network pipeline that\nsynthesizes realistic and detailed images of facial hair directly in the target\nimage in under one second. The synthesis is controlled by simple and sparse\nguide strokes from the user defining the general structural and color\nproperties of the target hairstyle. We qualitatively and quantitatively\nevaluate our chosen method compared to several alternative approaches. We show\ncompelling interactive editing results with a prototype user interface that\nallows novice users to progressively refine the generated image to match their\ndesired hairstyle, and demonstrate that our approach also allows for flexible\nand high-fidelity scalp hair synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 01:20:10 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Olszewski", "Kyle", ""], ["Ceylan", "Duygu", ""], ["Xing", "Jun", ""], ["Echevarria", "Jose", ""], ["Chen", "Zhili", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "2004.06853", "submitter": "Saeed Anwar", "authors": "Mehrdad Shoeiby, Mohammad Ali Armin, Sadegh Aliakbarian, Saeed Anwar,\n  Lars Petersson", "title": "Mosaic Super-resolution via Sequential Feature Pyramid Networks", "comments": "Accepted by IEEE CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in the design of multi-spectral cameras have led to great interests\nin a wide range of applications, from astronomy to autonomous driving. However,\nsuch cameras inherently suffer from a trade-off between the spatial and\nspectral resolution. In this paper, we propose to address this limitation by\nintroducing a novel method to carry out super-resolution on raw mosaic images,\nmulti-spectral or RGB Bayer, captured by modern real-time single-shot mosaic\nsensors. To this end, we design a deep super-resolution architecture that\nbenefits from a sequential feature pyramid along the depth of the network.\nThis, in fact, is achieved by utilizing a convolutional LSTM (ConvLSTM) to\nlearn the inter-dependencies between features at different receptive fields.\nAdditionally, by investigating the effect of different attention mechanisms in\nour framework, we show that a ConvLSTM inspired module is able to provide\nsuperior attention in our context. Our extensive experiments and analyses\nevidence that our approach yields significant super-resolution quality,\noutperforming current state-of-the-art mosaic super-resolution methods on both\nBayer and multi-spectral images. Additionally, to the best of our knowledge,\nour method is the first specialized method to super-resolve mosaic images,\nwhether it be multi-spectral or Bayer.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 01:46:24 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Shoeiby", "Mehrdad", ""], ["Armin", "Mohammad Ali", ""], ["Aliakbarian", "Sadegh", ""], ["Anwar", "Saeed", ""], ["Petersson", "Lars", ""]]}, {"id": "2004.06860", "submitter": "Manoranjan Paul PhD", "authors": "Cameron White, Manoranjan Paul, and Subrata Chakraborty", "title": "A Practical Blockchain Framework using Image Hashing for Image\n  Authentication", "comments": "This is un-published paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is a relatively new technology that can be seen as a decentralised\ndatabase. Blockchain systems heavily rely on cryptographic hash functions to\nstore their data, which makes it difficult to tamper with any data stored in\nthe system. A topic that was researched along with blockchain is image\nauthentication. Image authentication focuses on investigating and maintaining\nthe integrity of images. As a blockchain system can be useful for maintaining\ndata integrity, image authentication has the potential to be enhanced by\nblockchain. There are many techniques that can be used to authenticate images;\nthe technique investigated by this work is image hashing. Image hashing is a\ntechnique used to calculate how similar two different images are. This is done\nby converting the images into hashes and then comparing them using a distance\nformula. To investigate the topic, an experiment involving a simulated\nblockchain was created. The blockchain acted as a database for images. This\nblockchain was made up of devices which contained their own unique image\nhashing algorithms. The blockchain was tested by creating modified copies of\nthe images contained in the database, and then submitting them to the\nblockchain to see if it will return the original image. Through this experiment\nit was discovered that it is plausible to create an image authentication system\nusing blockchain and image hashing. However, the design proposed by this work\nrequires refinement, as it appears to struggle in some situations. This work\nshows that blockchain can be a suitable approach for authenticating images,\nparticularly via image hashing. Other observations include that using multiple\nimage hash algorithms at the same time can increase performance in some cases,\nas well as that each type of test done to the blockchain has its own unique\npattern to its data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 02:57:32 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["White", "Cameron", ""], ["Paul", "Manoranjan", ""], ["Chakraborty", "Subrata", ""]]}, {"id": "2004.06882", "submitter": "Sujit Gujar Dr", "authors": "Manisha Padala, Debojit Das, and Sujit Gujar", "title": "Effect of Input Noise Dimension in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are by far the most successful\ngenerative models. Learning the transformation which maps a low dimensional\ninput noise to the data distribution forms the foundation for GANs. Although\nthey have been applied in various domains, they are prone to certain challenges\nlike mode collapse and unstable training. To overcome the challenges,\nresearchers have proposed novel loss functions, architectures, and optimization\nmethods. In our work here, unlike the previous approaches, we focus on the\ninput noise and its role in the generation.\n  We aim to quantitatively and qualitatively study the effect of the dimension\nof the input noise on the performance of GANs. For quantitative measures,\ntypically \\emph{Fr\\'{e}chet Inception Distance (FID)} and \\emph{Inception Score\n(IS)} are used as performance measure on image data-sets. We compare the FID\nand IS values for DCGAN and WGAN-GP. We use three different image data-sets --\neach consisting of different levels of complexity. Through our experiments, we\nshow that the right dimension of input noise for optimal results depends on the\ndata-set and architecture used. We also observe that the state of the art\nperformance measures does not provide enough useful insights. Hence we conclude\nthat we need further theoretical analysis for understanding the relationship\nbetween the low dimensional distribution and the generated images. We also\nrequire better performance measures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 04:56:52 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Padala", "Manisha", ""], ["Das", "Debojit", ""], ["Gujar", "Sujit", ""]]}, {"id": "2004.06887", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Kenneth M.C.\n  Cheung, Michael To, Zhen Qian, Demetri Terzopoulos", "title": "Analysis of Scoliosis From Spinal X-Ray Images", "comments": "6 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoliosis is a congenital disease in which the spine is deformed from its\nnormal shape. Measurement of scoliosis requires labeling and identification of\nvertebrae in the spine. Spine radiographs are the most cost-effective and\naccessible modality for imaging the spine. Reliable and accurate vertebrae\nsegmentation in spine radiographs is crucial in image-guided spinal assessment,\ndisease diagnosis, and treatment planning. Conventional assessments rely on\ntedious and time-consuming manual measurement, which is subject to\ninter-observer variability. A fully automatic method that can accurately\nidentify and segment the associated vertebrae is unavailable in the literature.\nLeveraging a carefully-adjusted U-Net model with progressive side outputs, we\npropose an end-to-end segmentation model that provides a fully automatic and\nreliable segmentation of the vertebrae associated with scoliosis measurement.\nOur experimental results from a set of anterior-posterior spine X-Ray images\nindicate that our model, which achieves an average Dice score of 0.993,\npromises to be an effective tool in the identification and labeling of spinal\nvertebrae, eventually helping doctors in the reliable estimation of scoliosis.\nMoreover, estimation of Cobb angles from the segmented vertebrae further\ndemonstrates the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 05:36:28 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Huang", "Chao", ""], ["Tang", "Hui", ""], ["Fan", "Wei", ""], ["Cheung", "Kenneth M. C.", ""], ["To", "Michael", ""], ["Qian", "Zhen", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2004.06904", "submitter": "Xin Ning", "authors": "Xin Ning, Shaohui Xu, Xiaoli Dong, Weijun Li, Fangzhe Nan and Yuanzhou\n  Yao", "title": "Continuous learning of face attribute synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generative adversarial network (GAN) exhibits great superiority in the\nface attribute synthesis task. However, existing methods have very limited\neffects on the expansion of new attributes. To overcome the limitations of a\nsingle network in new attribute synthesis, a continuous learning method for\nface attribute synthesis is proposed in this work. First, the feature vector of\nthe input image is extracted and attribute direction regression is performed in\nthe feature space to obtain the axes of different attributes. The feature\nvector is then linearly guided along the axis so that images with target\nattributes can be synthesized by the decoder. Finally, to make the network\ncapable of continuous learning, the orthogonal direction modification module is\nused to extend the newly-added attributes. Experimental results show that the\nproposed method can endow a single network with the ability to learn attributes\ncontinuously, and, as compared to those produced by the current\nstate-of-the-art methods, the synthetic attributes have higher accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 06:44:13 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ning", "Xin", ""], ["Xu", "Shaohui", ""], ["Dong", "Xiaoli", ""], ["Li", "Weijun", ""], ["Nan", "Fangzhe", ""], ["Yao", "Yuanzhou", ""]]}, {"id": "2004.06912", "submitter": "Zheng Jiang", "authors": "Zheng Jiang, Menghan Hu, Lei Fan, Yaling Pan, Wei Tang, Guangtao Zhai,\n  Yong Lu", "title": "Combining Visible Light and Infrared Imaging for Efficient Detection of\n  Respiratory Infections such as COVID-19 on Portable Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) has become a serious global epidemic in\nthe past few months and caused huge loss to human society worldwide. For such a\nlarge-scale epidemic, early detection and isolation of potential virus carriers\nis essential to curb the spread of the epidemic. Recent studies have shown that\none important feature of COVID-19 is the abnormal respiratory status caused by\nviral infections. During the epidemic, many people tend to wear masks to reduce\nthe risk of getting sick. Therefore, in this paper, we propose a portable\nnon-contact method to screen the health condition of people wearing masks\nthrough analysis of the respiratory characteristics. The device mainly consists\nof a FLIR one thermal camera and an Android phone. This may help identify those\npotential patients of COVID-19 under practical scenarios such as pre-inspection\nin schools and hospitals. In this work, we perform the health screening through\nthe combination of the RGB and thermal videos obtained from the dual-mode\ncamera and deep learning architecture.We first accomplish a respiratory data\ncapture technique for people wearing masks by using face recognition. Then, a\nbidirectional GRU neural network with attention mechanism is applied to the\nrespiratory data to obtain the health screening result. The results of\nvalidation experiments show that our model can identify the health status on\nrespiratory with the accuracy of 83.7\\% on the real-world dataset. The abnormal\nrespiratory data and part of normal respiratory data are collected from Ruijin\nHospital Affiliated to The Shanghai Jiao Tong University Medical School. Other\nnormal respiratory data are obtained from healthy people around our\nresearchers. This work demonstrates that the proposed portable and intelligent\nhealth screening device can be used as a pre-scan method for respiratory\ninfections, which may help fight the current COVID-19 epidemic.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 07:22:02 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Jiang", "Zheng", ""], ["Hu", "Menghan", ""], ["Fan", "Lei", ""], ["Pan", "Yaling", ""], ["Tang", "Wei", ""], ["Zhai", "Guangtao", ""], ["Lu", "Yong", ""]]}, {"id": "2004.06918", "submitter": "Megane Millan", "authors": "M\\'egane Millan and Catherine Achard", "title": "Explaining Regression Based Neural Network Model", "comments": "7 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Several methods have been proposed to explain Deep Neural Network (DNN).\nHowever, to our knowledge, only classification networks have been studied to\ntry to determine which input dimensions motivated the decision. Furthermore, as\nthere is no ground truth to this problem, results are only assessed\nqualitatively in regards to what would be meaningful for a human. In this work,\nwe design an experimental settings where the ground truth can been established:\nwe generate ideal signals and disrupted signals with errors and learn a neural\nnetwork that determines the quality of the signals. This quality is simply a\nscore based on the distance between the disrupted signals and the corresponding\nideal signal. We then try to find out how the network estimated this score and\nhope to find the time-step and dimensions of the signal where errors are\npresent. This experimental setting enables us to compare several methods for\nnetwork explanation and to propose a new method, named AGRA for Accurate\nGradient, based on several trainings that decrease the noise present in most\nstate-of-the-art results. Comparative results show that the proposed method\noutperforms state-of-the-art methods for locating time-steps where errors occur\nin the signal.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 07:38:40 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Millan", "M\u00e9gane", ""], ["Achard", "Catherine", ""]]}, {"id": "2004.06930", "submitter": "Uma K", "authors": "D.Sabari Nathan, K.Uma, D Synthiya Vinothini, B. Sathya Bama, S. M. Md\n  Mansoor Roomi", "title": "Light Weight Residual Dense Attention Net for Spectral Reconstruction\n  from RGB Images", "comments": "6pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Hyperspectral Imaging is the acquisition of spectral and spatial information\nof a particular scene. Capturing such information from a specialized\nhyperspectral camera remains costly. Reconstructing such information from the\nRGB image achieves a better solution in both classification and object\nrecognition tasks. This work proposes a novel light weight network with very\nless number of parameters about 233,059 parameters based on Residual dense\nmodel with attention mechanism to obtain this solution. This network uses\nCoordination Convolutional Block to get the spatial information. The weights\nfrom this block are shared by two independent feature extraction mechanisms,\none by dense feature extraction and the other by the multiscale hierarchical\nfeature extraction. Finally, the features from both the feature extraction\nmechanisms are globally fused to produce the 31 spectral bands. The network is\ntrained with NTIRE 2020 challenge dataset and thus achieved 0.0457 MRAE metric\nvalue with less computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 07:58:15 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 03:04:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nathan", "D. Sabari", ""], ["Uma", "K.", ""], ["Vinothini", "D Synthiya", ""], ["Bama", "B. Sathya", ""], ["Roomi", "S. M. Md Mansoor", ""]]}, {"id": "2004.06957", "submitter": "Val\\'ery Dewil", "authors": "Val\\'ery Dewil, J\\'er\\'emy Anger, Axel Davy, Thibaud Ehret, Pablo\n  Arias, Gabriele Facciolo", "title": "Self-Supervised training for blind multi-frame video denoising", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised approach for training multi-frame video\ndenoising networks. These networks predict frame t from a window of frames\naround t. Our self-supervised approach benefits from the video temporal\nconsistency by penalizing a loss between the predicted frame t and a\nneighboring target frame, which are aligned using an optical flow. We use the\nproposed strategy for online internal learning, where a pre-trained network is\nfine-tuned to denoise a new unknown noise type from a single video. After a few\nframes, the proposed fine-tuning reaches and sometimes surpasses the\nperformance of a state-of-the-art network trained with supervision. In\naddition, for a wide range of noise types, it can be applied blindly without\nknowing the noise distribution. We demonstrate this by showing results on blind\ndenoising of different synthetic and realistic noises.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:08:09 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:22:46 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 13:28:10 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 17:18:55 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Dewil", "Val\u00e9ry", ""], ["Anger", "J\u00e9r\u00e9my", ""], ["Davy", "Axel", ""], ["Ehret", "Thibaud", ""], ["Arias", "Pablo", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "2004.06965", "submitter": "Yu-Syuan Xu", "authors": "Yu-Syuan Xu, Shou-Yao Roy Tseng, Yu Tseng, Hsien-Kai Kuo, Yi-Min Tsai", "title": "Unified Dynamic Convolutional Network for Super-Resolution with\n  Variational Degradations", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have achieved remarkable results on\nSingle Image Super-Resolution (SISR). Despite considering only a single\ndegradation, recent studies also include multiple degrading effects to better\nreflect real-world cases. However, most of the works assume a fixed combination\nof degrading effects, or even train an individual network for different\ncombinations. Instead, a more practical approach is to train a single network\nfor wide-ranging and variational degradations. To fulfill this requirement,\nthis paper proposes a unified network to accommodate the variations from\ninter-image (cross-image variations) and intra-image (spatial variations).\nDifferent from the existing works, we incorporate dynamic convolution which is\na far more flexible alternative to handle different variations. In SISR with\nnon-blind setting, our Unified Dynamic Convolutional Network for Variational\nDegradations (UDVD) is evaluated on both synthetic and real images with an\nextensive set of variations. The qualitative results demonstrate the\neffectiveness of UDVD over various existing works. Extensive experiments show\nthat our UDVD achieves favorable or comparable performance on both synthetic\nand real images.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:21:01 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Xu", "Yu-Syuan", ""], ["Tseng", "Shou-Yao Roy", ""], ["Tseng", "Yu", ""], ["Kuo", "Hsien-Kai", ""], ["Tsai", "Yi-Min", ""]]}, {"id": "2004.06967", "submitter": "Matteo Luperto", "authors": "Matteo Luperto, Luca Fochetta, Francesco Amigoni", "title": "Exploration of Indoor Environments Predicting the Layout of Partially\n  Observed Rooms", "comments": null, "journal-ref": null, "doi": "10.5555/3461017.3461113", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exploration tasks in which an autonomous mobile robot\nincrementally builds maps of initially unknown indoor environments. In such\ntasks, the robot makes a sequence of decisions on where to move next that,\nusually, are based on knowledge about the observed parts of the environment. In\nthis paper, we present an approach that exploits a prediction of the geometric\nstructure of the unknown parts of an environment to improve exploration\nperformance. In particular, we leverage an existing method that reconstructs\nthe layout of an environment starting from a partial grid map and that predicts\nthe shape of partially observed rooms on the basis of geometric features\nrepresenting the regularities of the indoor environment. Then, we originally\nemploy the predicted layout to estimate the amount of new area the robot would\nobserve from candidate locations in order to inform the selection of the next\nbest location and to early stop the exploration when no further relevant area\nis expected to be discovered. Experimental activities show that our approach is\nable to effectively predict the layout of partially observed rooms and to use\nsuch knowledge to speed up the exploration.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:28:40 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Luperto", "Matteo", ""], ["Fochetta", "Luca", ""], ["Amigoni", "Francesco", ""]]}, {"id": "2004.06970", "submitter": "Mohit Lamba", "authors": "Mansha Lamba, Raunak Goswami, Mr. Vinay, Mohit Lamba", "title": "Roommate Compatibility Detection Through Machine Learning Techniques", "comments": "Detected severe inconsitencies in this pre-eliminary work and\n  requires a thorough re-examination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to develop an artificially intelligent system which aims at\nchecking the compatibility between the roommates of same or different sex\nsharing a common area of residence. There are a few key factors determining\none's compatibility with the other person. Interpersonal behaviour ,\nsituational awareness, communication skills. Here we are trying to build a\nsystem that evaluates user on these key factors not via pen paper test but\nthrough a highly engaging set of questions and answers. Hence using these\nscores as an input to our machine learning algorithm which is based on previous\ntrends to come up with percentage probability of user being compatible with\nanother user. With the growing population there is always a challenge for\norganisation and educational institutions to make the students and their\nemployees more and more productive and in such cases a person's social\nenvironment comes into play. A person may be a genius but as long as he is not\nable to work well with his peers there will always be a chance of more\nproductive performance. It is a well-established fact that human are and have\nalways been a social animal and this has helped in creating communities of\nlike-minded people. Many times, even when there are a large no of people\nemployed to do a particular task the result may not be as expected as people\nmay not compatible in working with one another. This at the end creates\nperformance gaps, hinders organisation success and in many cases loss of\nprecious resources. Our intent is not to remove the non-compatible people from\nthe picture but to find out the perfect compatible match for the person\nelsewhere that will not only save the resources will also enable effective use\nof resources. Through the use of various machine learning classification\ntechniques, we intent to do this.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:34:55 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 13:27:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lamba", "Mansha", ""], ["Goswami", "Raunak", ""], ["Vinay", "Mr.", ""], ["Lamba", "Mohit", ""]]}, {"id": "2004.06971", "submitter": "Guillaume Vaudaux-Ruth", "authors": "Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard (ISIR,\n  PIROS, SU)", "title": "ActionSpotter: Deep Reinforcement Learning Framework for Temporal Action\n  Spotting in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarizing video content is an important task in many applications. This\ntask can be defined as the computation of the ordered list of actions present\nin a video. Such a list could be extracted using action detection algorithms.\nHowever, it is not necessary to determine the temporal boundaries of actions to\nknow their existence. Moreover, localizing precise boundaries usually requires\ndense video analysis to be effective. In this work, we propose to directly\ncompute this ordered list by sparsely browsing the video and selecting one\nframe per action instance, task known as action spotting in literature. To do\nthis, we propose ActionSpotter, a spotting algorithm that takes advantage of\nDeep Reinforcement Learning to efficiently spot actions while adapting its\nvideo browsing speed, without additional supervision. Experiments performed on\ndatasets THUMOS14 and ActivityNet show that our framework outperforms state of\nthe art detection methods. In particular, the spotting mean Average Precision\non THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of\nvideo.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:36:37 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 16:43:56 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Vaudaux-Ruth", "Guillaume", "", "ISIR,\n  PIROS, SU"], ["Chan-Hon-Tong", "Adrien", "", "ISIR,\n  PIROS, SU"], ["Achard", "Catherine", "", "ISIR,\n  PIROS, SU"]]}, {"id": "2004.07003", "submitter": "Akash Palrecha Mr.", "authors": "Atmadeep Banerjee, Akash Palrecha", "title": "MXR-U-Nets for Real Time Hyperspectral Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, CNNs have made significant contributions to applications in\nimage generation, super-resolution and style transfer. In this paper, we build\nupon the work of Howard and Gugger, He et al. and Misra, D. and propose a CNN\narchitecture that accurately reconstructs hyperspectral images from their RGB\ncounterparts. We also propose a much shallower version of our best model with a\n10% relative memory footprint and 3x faster inference, thus enabling real-time\nvideo applications while still experiencing only about a 0.5% decrease in\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:06:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Banerjee", "Atmadeep", ""], ["Palrecha", "Akash", ""]]}, {"id": "2004.07007", "submitter": "Max Schwarz", "authors": "Umashankar Deekshith, Nishit Gajjar, Max Schwarz, Sven Behnke", "title": "Visual Descriptor Learning from Monocular Video", "comments": "International Conference on Computer Vision Theory and Applications\n  (VISAPP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence estimation is one of the most widely researched and yet only\npartially solved area of computer vision with many applications in tracking,\nmapping, recognition of objects and environment. In this paper, we propose a\nnovel way to estimate dense correspondence on an RGB image where visual\ndescriptors are learned from video examples by training a fully convolutional\nnetwork. Most deep learning methods solve this by training the network with a\nlarge set of expensive labeled data or perform labeling through strong 3D\ngenerative models using RGB-D videos. Our method learns from RGB videos using\ncontrastive loss, where relative labeling is estimated from optical flow. We\ndemonstrate the functionality in a quantitative analysis on rendered videos,\nwhere ground truth information is available. Not only does the method perform\nwell on test data with the same background, it also generalizes to situations\nwith a new background. The descriptors learned are unique and the\nrepresentations determined by the network are global. We further show the\napplicability of the method to real-world videos.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:19:57 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Deekshith", "Umashankar", ""], ["Gajjar", "Nishit", ""], ["Schwarz", "Max", ""], ["Behnke", "Sven", ""]]}, {"id": "2004.07011", "submitter": "Luigi Tommaso Luppino", "authors": "Luigi T.Luppino, Mads A. Hansen, Michael Kampffmeyer, Filippo M.\n  Bianchi, Gabriele Moser, Robert Jenssen, Stian N. Anfinsen", "title": "Code-Aligned Autoencoders for Unsupervised Change Detection in\n  Multimodal Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation with convolutional autoencoders has recently been used as\nan approach to multimodal change detection in bitemporal satellite images. A\nmain challenge is the alignment of the code spaces by reducing the contribution\nof change pixels to the learning of the translation function. Many existing\napproaches train the networks by exploiting supervised information of the\nchange areas, which, however, is not always available. We propose to extract\nrelational pixel information captured by domain-specific affinity matrices at\nthe input and use this to enforce alignment of the code spaces and reduce the\nimpact of change pixels on the learning objective. A change prior is derived in\nan unsupervised fashion from pixel pair affinities that are comparable across\ndomains. To achieve code space alignment we enforce that pixel with similar\naffinity relations in the input domains should be correlated also in code\nspace. We demonstrate the utility of this procedure in combination with cycle\nconsistency. The proposed approach are compared with state-of-the-art deep\nlearning algorithms. Experiments conducted on four real datasets show the\neffectiveness of our methodology.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:24:51 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Luppino", "Luigi T.", ""], ["Hansen", "Mads A.", ""], ["Kampffmeyer", "Michael", ""], ["Bianchi", "Filippo M.", ""], ["Moser", "Gabriele", ""], ["Jenssen", "Robert", ""], ["Anfinsen", "Stian N.", ""]]}, {"id": "2004.07018", "submitter": "Clint Sebastian", "authors": "Clint Sebastian, Raffaele Imbriaco, Egor Bondarev, Peter H.N. de With", "title": "Contextual Pyramid Attention Network for Building Segmentation in Aerial\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building extraction from aerial images has several applications in problems\nsuch as urban planning, change detection, and disaster management. With the\nincreasing availability of data, Convolutional Neural Networks (CNNs) for\nsemantic segmentation of remote sensing imagery has improved significantly in\nrecent years. However, convolutions operate in local neighborhoods and fail to\ncapture non-local features that are essential in semantic understanding of\naerial images. In this work, we propose to improve building segmentation of\ndifferent sizes by capturing long-range dependencies using contextual pyramid\nattention (CPA). The pathways process the input at multiple scales efficiently\nand combine them in a weighted manner, similar to an ensemble model. The\nproposed method obtains state-of-the-art performance on the Inria Aerial Image\nLabelling Dataset with minimal computation costs. Our method improves 1.8\npoints over current state-of-the-art methods and 12.6 points higher than\nexisting baselines on the Intersection over Union (IoU) metric without any\npost-processing. Code and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:36:26 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Sebastian", "Clint", ""], ["Imbriaco", "Raffaele", ""], ["Bondarev", "Egor", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2004.07035", "submitter": "Alistair Young", "authors": "Edward Ferdian, Avan Suinesiaputra, David Dubowitz, Debbie Zhao, Alan\n  Wang, Brett Cowan, Alistair Young", "title": "4DFlowNet: Super-Resolution 4D Flow MRI using Deep Learning and\n  Computational Fluid Dynamics", "comments": "accepted to Frontiers in Cardiovascular Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  4D-flow magnetic resonance imaging (MRI) is an emerging imaging technique\nwhere spatiotemporal 3D blood velocity can be captured with full volumetric\ncoverage in a single non-invasive examination. This enables qualitative and\nquantitative analysis of hemodynamic flow parameters of the heart and great\nvessels. An increase in the image resolution would provide more accuracy and\nallow better assessment of the blood flow, especially for patients with\nabnormal flows. However, this must be balanced with increasing imaging time.\nThe recent success of deep learning in generating super resolution images shows\npromise for implementation in medical images. We utilized computational fluid\ndynamics simulations to generate fluid flow simulations and represent them as\nsynthetic 4D flow MRI data. We built our training dataset to mimic actual 4D\nflow MRI data with its corresponding noise distribution. Our novel 4DFlowNet\nnetwork was trained on this synthetic 4D flow data and was capable in producing\nnoise-free super resolution 4D flow phase images with upsample factor of 2. We\nalso tested the 4DFlowNet in actual 4D flow MR images of a phantom and normal\nvolunteer data, and demonstrated comparable results with the actual flow rate\nmeasurements giving an absolute relative error of 0.6 to 5.8% and 1.1 to 3.8%\nin the phantom data and normal volunteer data, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:16:52 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ferdian", "Edward", ""], ["Suinesiaputra", "Avan", ""], ["Dubowitz", "David", ""], ["Zhao", "Debbie", ""], ["Wang", "Alan", ""], ["Cowan", "Brett", ""], ["Young", "Alistair", ""]]}, {"id": "2004.07041", "submitter": "David Tellez", "authors": "David Tellez, Diederik Hoppener, Cornelis Verhoef, Dirk Grunhagen,\n  Pieter Nierop, Michal Drozdzal, Jeroen van der Laak, Francesco Ciompi", "title": "Extending Unsupervised Neural Image Compression With Supervised\n  Multitask Learning", "comments": "Medical Imaging with Deep Learning 2020 (MIDL20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of training convolutional neural networks on\ngigapixel histopathology images to predict image-level targets. For this\npurpose, we extend Neural Image Compression (NIC), an image compression\nframework that reduces the dimensionality of these images using an encoder\nnetwork trained unsupervisedly. We propose to train this encoder using\nsupervised multitask learning (MTL) instead. We applied the proposed MTL NIC to\ntwo histopathology datasets and three tasks. First, we obtained\nstate-of-the-art results in the Tumor Proliferation Assessment Challenge of\n2016 (TUPAC16). Second, we successfully classified histopathological growth\npatterns in images with colorectal liver metastasis (CLM). Third, we predicted\npatient risk of death by learning directly from overall survival in the same\nCLM data. Our experimental results suggest that the representations learned by\nthe MTL objective are: (1) highly specific, due to the supervised training\nsignal, and (2) transferable, since the same features perform well across\ndifferent tasks. Additionally, we trained multiple encoders with different\ntraining objectives, e.g. unsupervised and variants of MTL, and observed a\npositive correlation between the number of tasks in MTL and the system\nperformance on the TUPAC16 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:20:22 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Tellez", "David", ""], ["Hoppener", "Diederik", ""], ["Verhoef", "Cornelis", ""], ["Grunhagen", "Dirk", ""], ["Nierop", "Pieter", ""], ["Drozdzal", "Michal", ""], ["van der Laak", "Jeroen", ""], ["Ciompi", "Francesco", ""]]}, {"id": "2004.07054", "submitter": "Yu-Huan Wu", "authors": "Yu-Huan Wu, Shang-Hua Gao, Jie Mei, Jun Xu, Deng-Ping Fan, Rong-Guo\n  Zhang, Ming-Ming Cheng", "title": "JCS: An Explainable COVID-19 Diagnosis System by Joint Classification\n  and Segmentation", "comments": "To appear in IEEE Transactions on Image Processing. Dataset and code\n  are available at https://github.com/yuhuan-wu/JCS", "journal-ref": null, "doi": "10.1109/TIP.2021.3058783", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic\ndisease in over 200 countries, influencing billions of humans. To control the\ninfection, identifying and separating the infected people is the most crucial\nstep. The main diagnostic tool is the Reverse Transcription Polymerase Chain\nReaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high\nenough to effectively prevent the pandemic. The chest CT scan test provides a\nvaluable complementary tool to the RT-PCR test, and it can identify the\npatients in the early-stage with high sensitivity. However, the chest CT scan\ntest is usually time-consuming, requiring about 21.5 minutes per case. This\npaper develops a novel Joint Classification and Segmentation (JCS) system to\nperform real-time and explainable COVID-19 chest CT diagnosis. To train our JCS\nsystem, we construct a large scale COVID-19 Classification and Segmentation\n(COVID-CS) dataset, with 144,167 chest CT images of 400 COVID-19 patients and\n350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with\nfine-grained pixel-level labels of opacifications, which are increased\nattenuation of the lung parenchyma. We also have annotated lesion counts,\nopacification areas, and locations and thus benefit various diagnosis aspects.\nExtensive experiments demonstrate that the proposed JCS diagnosis system is\nvery efficient for COVID-19 classification and segmentation. It obtains an\naverage sensitivity of 95.0% and a specificity of 93.0% on the classification\ntest set, and 78.5% Dice score on the segmentation test set of our COVID-CS\ndataset. The COVID-CS dataset and code are available at\nhttps://github.com/yuhuan-wu/JCS.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:30:40 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:06:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wu", "Yu-Huan", ""], ["Gao", "Shang-Hua", ""], ["Mei", "Jie", ""], ["Xu", "Jun", ""], ["Fan", "Deng-Ping", ""], ["Zhang", "Rong-Guo", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2004.07064", "submitter": "Alistair Young", "authors": "Edward Ferdian, Avan Suinesiaputra, Kenneth Fung, Nay Aung, Elena\n  Lukaschuk, Ahmet Barutcu, Edd Maclean, Jose Paiva, Stefan K. Piechnik, Stefan\n  Neubauer, Steffen E Petersen, and Alistair A. Young", "title": "Fully Automated Myocardial Strain Estimation from CMR Tagged Images\n  using a Deep Learning Framework in the UK Biobank", "comments": "accepted in Radiology Cardiothoracic Imaging", "journal-ref": "Radiology: Cardiothoracic Imaging 2020; 2(1):e190032", "doi": "10.1148/ryct.2020190032", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: To demonstrate the feasibility and performance of a fully automated\ndeep learning framework to estimate myocardial strain from short-axis cardiac\nmagnetic resonance tagged images. Methods and Materials: In this retrospective\ncross-sectional study, 4508 cases from the UK Biobank were split randomly into\n3244 training and 812 validation cases, and 452 test cases. Ground truth\nmyocardial landmarks were defined and tracked by manual initialization and\ncorrection of deformable image registration using previously validated software\nwith five readers. The fully automatic framework consisted of 1) a\nconvolutional neural network (CNN) for localization, and 2) a combination of a\nrecurrent neural network (RNN) and a CNN to detect and track the myocardial\nlandmarks through the image sequence for each slice. Radial and circumferential\nstrain were then calculated from the motion of the landmarks and averaged on a\nslice basis. Results: Within the test set, myocardial end-systolic\ncircumferential Green strain errors were -0.001 +/- 0.025, -0.001 +/- 0.021,\nand 0.004 +/- 0.035 in basal, mid, and apical slices respectively (mean +/-\nstd. dev. of differences between predicted and manual strain). The framework\nreproduced significant reductions in circumferential strain in diabetics,\nhypertensives, and participants with previous heart attack. Typical processing\ntime was ~260 frames (~13 slices) per second on an NVIDIA Tesla K40 with 12GB\nRAM, compared with 6-8 minutes per slice for the manual analysis. Conclusions:\nThe fully automated RNNCNN framework for analysis of myocardial strain enabled\nunbiased strain evaluation in a high-throughput workflow, with similar ability\nto distinguish impairment due to diabetes, hypertension, and previous heart\nattack.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:49:15 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ferdian", "Edward", ""], ["Suinesiaputra", "Avan", ""], ["Fung", "Kenneth", ""], ["Aung", "Nay", ""], ["Lukaschuk", "Elena", ""], ["Barutcu", "Ahmet", ""], ["Maclean", "Edd", ""], ["Paiva", "Jose", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Petersen", "Steffen E", ""], ["Young", "Alistair A.", ""]]}, {"id": "2004.07071", "submitter": "Ruchi Chauhan", "authors": "Alakh Desai, Ruchi Chauhan, Jayanthi Sivaswamy", "title": "Image Segmentation Using Hybrid Representations", "comments": "4 pages, 6 figures, to be published in ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores a hybrid approach to segmentation as an alternative to a\npurely data-driven approach. We introduce an end-to-end U-Net based network\ncalled DU-Net, which uses additional frequency preserving features, namely the\nScattering Coefficients (SC), for medical image segmentation. SC are\ntranslation invariant and Lipschitz continuous to deformations which help\nDU-Net outperform other conventional CNN counterparts on four datasets and two\nsegmentation tasks: Optic Disc and Optic Cup in color fundus images and fetal\nHead in ultrasound images. The proposed method shows remarkable improvement\nover the basic U-Net with performance competitive to state-of-the-art methods.\nThe results indicate that it is possible to use a lighter network trained with\nfewer images (without any augmentation) to attain good segmentation results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:07:35 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Desai", "Alakh", ""], ["Chauhan", "Ruchi", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "2004.07088", "submitter": "Giulio Lovisotto", "authors": "Giulio Lovisotto, Henry Turner, Simon Eberz and Ivan Martinovic", "title": "Seeing Red: PPG Biometrics Using Smartphone Cameras", "comments": "8 pages, 15th IEEE Computer Society Workshop on Biometrics 2020", "journal-ref": null, "doi": "10.1109/CVPRW50498.2020.00417", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a system that enables photoplethysmogram\n(PPG)-based authentication by using a smartphone camera. PPG signals are\nobtained by recording a video from the camera as users are resting their finger\non top of the camera lens. The signals can be extracted based on subtle changes\nin the video that are due to changes in the light reflection properties of the\nskin as the blood flows through the finger. We collect a dataset of PPG\nmeasurements from a set of 15 users over the course of 6-11 sessions per user\nusing an iPhone X for the measurements. We design an authentication pipeline\nthat leverages the uniqueness of each individual's cardiovascular system,\nidentifying a set of distinctive features from each heartbeat. We conduct a set\nof experiments to evaluate the recognition performance of the PPG biometric\ntrait, including cross-session scenarios which have been disregarded in\nprevious work. We found that when aggregating sufficient samples for the\ndecision we achieve an EER as low as 8%, but that the performance greatly\ndecreases in the cross-session scenario, with an average EER of 20%.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:50:36 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Lovisotto", "Giulio", ""], ["Turner", "Henry", ""], ["Eberz", "Simon", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2004.07098", "submitter": "Edouard Yvinec", "authors": "Edouard Yvinec, Arnaud Dapogny, and K\\'evin Bailly", "title": "DeeSCo: Deep heterogeneous ensemble with Stochastic Combinatory loss for\n  gaze estimation", "comments": "7 pages, 6 figures, FG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From medical research to gaming applications, gaze estimation is becoming a\nvaluable tool. While there exists a number of hardware-based solutions, recent\ndeep learning-based approaches, coupled with the availability of large-scale\ndatabases, have allowed to provide a precise gaze estimate using only consumer\nsensors. However, there remains a number of questions, regarding the problem\nformulation, architectural choices and learning paradigms for designing gaze\nestimation systems in order to bridge the gap between geometry-based systems\ninvolving specific hardware and approaches using consumer sensors only. In this\npaper, we introduce a deep, end-to-end trainable ensemble of heatmap-based weak\npredictors for 2D/3D gaze estimation. We show that, through heterogeneous\narchitectural design of these weak predictors, we can improve the decorrelation\nbetween the latter predictors to design more robust deep ensemble models.\nFurthermore, we propose a stochastic combinatory loss that consists in randomly\nsampling combinations of weak predictors at train time. This allows to train\nbetter individual weak predictors, with lower correlation between them. This,\nin turns, allows to significantly enhance the performance of the deep ensemble.\nWe show that our Deep heterogeneous ensemble with Stochastic Combinatory loss\n(DeeSCo) outperforms state-of-the-art approaches for 2D/3D gaze estimation on\nmultiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 14:06:31 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Yvinec", "Edouard", ""], ["Dapogny", "Arnaud", ""], ["Bailly", "K\u00e9vin", ""]]}, {"id": "2004.07109", "submitter": "Yutao Cui", "authors": "Yutao Cui, Cheng Jiang, Limin Wang, Gangshan Wu", "title": "Fully Convolutional Online Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has turned out to be effective for improving tracking\nperformance. However, it could be simply applied for classification branch, but\nstill remains challenging for adapting to regression branch due to the complex\ndesign. To tackle this issue, we present the first fully convolutional online\ntracking framework (FCOT), with a focus on enabling online learning for both\nclassification and regression branches. Our key contribution is to introduce an\nonline regression model generator (RMG) based on the carefully designed\nanchor-free box regression branch, which enables our FCOT to be more effective\nin handling target deformation during tracking procedure. In addition, to deal\nwith the confusion of similar objects, we devise a simple yet effective\nmulti-scale classification branch to improve both accuracy and robustness of\nFCOT. Due to its simplicity in design, our FCOT could be trained and deployed\nin a fully convolutional manner with a running speed of 45FPS. The proposed\nFCOT sets a new state-of-the-art results on six benchmarks including VOT2018,\nLaSOT, TrackingNet, GOT-10k, UAV123, and NFS. Particularly, among real-time\ntrackers, our FCOT achieves EAO of 0.456 on VOT2018, NP of 0.678 on LaSOT, NP\nof 0.828 on TrackingNet, and AO of 0.640 on GOT-10k. The code and models will\nbe made available at https://github.com/MCG-NJU/FCOT.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 14:21:57 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 16:30:13 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 04:06:32 GMT"}, {"version": "v4", "created": "Fri, 12 Mar 2021 06:47:21 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Cui", "Yutao", ""], ["Jiang", "Cheng", ""], ["Wang", "Limin", ""], ["Wu", "Gangshan", ""]]}, {"id": "2004.07124", "submitter": "Linhao Li", "authors": "Linhao Li, Zhiqiang Zhou, Bo Wang, Lingjuan Miao and Hua Zong", "title": "A Novel CNN-based Method for Accurate Ship Detection in HR Optical\n  Remote Sensing Images via Rotated Bounding Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, reliable and accurate ship detection in optical remote sensing\nimages is still challenging. Even the state-of-the-art convolutional neural\nnetwork (CNN) based methods cannot obtain very satisfactory results. To more\naccurately locate the ships in diverse orientations, some recent methods\nconduct the detection via the rotated bounding box. However, it further\nincreases the difficulty of detection, because an additional variable of ship\norientation must be accurately predicted in the algorithm. In this paper, a\nnovel CNN-based ship detection method is proposed, by overcoming some common\ndeficiencies of current CNN-based methods in ship detection. Specifically, to\ngenerate rotated region proposals, current methods have to predefine\nmulti-oriented anchors, and predict all unknown variables together in one\nregression process, limiting the quality of overall prediction. By contrast, we\nare able to predict the orientation and other variables independently, and yet\nmore effectively, with a novel dual-branch regression network, based on the\nobservation that the ship targets are nearly rotation-invariant in remote\nsensing images. Next, a shape-adaptive pooling method is proposed, to overcome\nthe limitation of typical regular ROI-pooling in extracting the features of the\nships with various aspect ratios. Furthermore, we propose to incorporate\nmultilevel features via the spatially-variant adaptive pooling. This novel\napproach, called multilevel adaptive pooling, leads to a compact feature\nrepresentation more qualified for the simultaneous ship classification and\nlocalization. Finally, detailed ablation study performed on the proposed\napproaches is provided, along with some useful insights. Experimental results\ndemonstrate the great superiority of the proposed method in ship detection.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 14:48:46 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 03:22:37 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Li", "Linhao", ""], ["Zhou", "Zhiqiang", ""], ["Wang", "Bo", ""], ["Miao", "Lingjuan", ""], ["Zong", "Hua", ""]]}, {"id": "2004.07160", "submitter": "Cong Wang", "authors": "Cong Wang, Witold Pedrycz, ZhiWu Li, MengChu Zhou", "title": "Residual-driven Fuzzy C-Means Clustering for Image Segmentation", "comments": "14 pages, 13 figures, 6 tables", "journal-ref": "IEEE/CAA Journal of Automatica Sinica, 2020", "doi": "10.1109/JAS.2020.1003420", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its inferior characteristics, an observed (noisy) image's direct use\ngives rise to poor segmentation results. Intuitively, using its noise-free\nimage can favorably impact image segmentation. Hence, the accurate estimation\nof the residual between observed and noise-free images is an important task. To\ndo so, we elaborate on residual-driven Fuzzy C-Means (FCM) for image\nsegmentation, which is the first approach that realizes accurate residual\nestimation and leads noise-free image to participate in clustering. We propose\na residual-driven FCM framework by integrating into FCM a residual-related\nfidelity term derived from the distribution of different types of noise. Built\non this framework, we present a weighted $\\ell_{2}$-norm fidelity term by\nweighting mixed noise distribution, thus resulting in a universal\nresidual-driven FCM algorithm in presence of mixed or unknown noise. Besides,\nwith the constraint of spatial information, the residual estimation becomes\nmore reliable than that only considering an observed image itself. Supporting\nexperiments on synthetic, medical, and real-world images are conducted. The\nresults demonstrate the superior effectiveness and efficiency of the proposed\nalgorithm over existing FCM-related algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:46:09 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 14:15:04 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wang", "Cong", ""], ["Pedrycz", "Witold", ""], ["Li", "ZhiWu", ""], ["Zhou", "MengChu", ""]]}, {"id": "2004.07165", "submitter": "Enrique Sanchez", "authors": "Enrique Sanchez, Michel Valstar", "title": "A recurrent cycle consistency loss for progressive face-to-face\n  synthesis", "comments": "Accepted to FG 2020 (Oral). arXiv admin note: substantial text\n  overlap with arXiv:1811.03492", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a major flaw of the cycle consistency loss when used to\npreserve the input appearance in the face-to-face synthesis domain. In\nparticular, we show that the images generated by a network trained using this\nloss conceal a noise that hinders their use for further tasks. To overcome this\nlimitation, we propose a ''recurrent cycle consistency loss\" which for\ndifferent sequences of target attributes minimises the distance between the\noutput images, independent of any intermediate step. We empirically validate\nnot only that our loss enables the re-use of generated images, but that it also\nimproves their quality. In addition, we propose the very first network that\ncovers the task of unconstrained landmark-guided face-to-face synthesis.\nContrary to previous works, our proposed approach enables the transfer of a\nparticular set of input features to a large span of poses and expressions,\nwhereby the target landmarks become the ground-truth points. We then evaluate\nthe consistency of our proposed approach to synthesise faces at the target\nlandmarks. To the best of our knowledge, we are the first to propose a loss to\novercome the limitation of the cycle consistency loss, and the first to propose\nan ''in-the-wild'' landmark guided synthesis approach. Code and models for this\npaper can be found in https://github.com/ESanchezLozano/GANnotation\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:53:41 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Sanchez", "Enrique", ""], ["Valstar", "Michel", ""]]}, {"id": "2004.07173", "submitter": "Aythami Morales", "authors": "Alejandro Pe\\~na, Ignacio Serna, Aythami Morales, and Julian Fierrez", "title": "Bias in Multimodal AI: Testbed for Fair Automatic Recruitment", "comments": null, "journal-ref": "IEEE CVPR Workshop on Fair, Data Efficient and Trusted Computer\n  Vision, Washington, Seattle, USA, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of decision-making algorithms in society is rapidly increasing\nnowadays, while concerns about their transparency and the possibility of these\nalgorithms becoming new sources of discrimination are arising. In fact, many\nrelevant automated systems have been shown to make decisions based on sensitive\ninformation or discriminate certain social groups (e.g. certain biometric\nsystems for person recognition). With the aim of studying how current\nmultimodal algorithms based on heterogeneous sources of information are\naffected by sensitive elements and inner biases in the data, we propose a\nfictitious automated recruitment testbed: FairCVtest. We train automatic\nrecruitment algorithms using a set of multimodal synthetic profiles consciously\nscored with gender and racial biases. FairCVtest shows the capacity of the\nArtificial Intelligence (AI) behind such recruitment tool to extract sensitive\ninformation from unstructured data, and exploit it in combination to data\nbiases in undesirable (unfair) ways. Finally, we present a list of recent works\ndeveloping techniques capable of removing sensitive information from the\ndecision-making process of deep learning architectures. We have used one of\nthese algorithms (SensitiveNets) to experiment discrimination-aware learning\nfor the elimination of sensitive information in our multimodal AI framework.\nOur methodology and results show how to generate fairer AI-based tools in\ngeneral, and in particular fairer automated recruitment systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:58:05 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Pe\u00f1a", "Alejandro", ""], ["Serna", "Ignacio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""]]}, {"id": "2004.07193", "submitter": "Zhirong Wu", "authors": "Yizhuo Zhang, Zhirong Wu, Houwen Peng, and Stephen Lin", "title": "A Transductive Approach for Video Object Segmentation", "comments": "To Appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised video object segmentation aims to separate a target object\nfrom a video sequence, given the mask in the first frame. Most of current\nprevailing methods utilize information from additional modules trained in other\ndomains like optical flow and instance segmentation, and as a result they do\nnot compete with other methods on common ground. To address this issue, we\npropose a simple yet strong transductive method, in which additional modules,\ndatasets, and dedicated architectural designs are not needed. Our method takes\na label propagation approach where pixel labels are passed forward based on\nfeature similarity in an embedding space. Different from other propagation\nmethods, ours diffuses temporal information in a holistic manner which take\naccounts of long-term object appearance. In addition, our method requires few\nadditional computational overhead, and runs at a fast $\\sim$37 fps speed. Our\nsingle model with a vanilla ResNet50 backbone achieves an overall score of 72.3\non the DAVIS 2017 validation set and 63.1 on the test set. This simple yet high\nperforming and efficient method can serve as a solid baseline that facilitates\nfuture research. Code and models are available at\n\\url{https://github.com/microsoft/transductive-vos.pytorch}.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:39:36 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 16:15:04 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhang", "Yizhuo", ""], ["Wu", "Zhirong", ""], ["Peng", "Houwen", ""], ["Lin", "Stephen", ""]]}, {"id": "2004.07200", "submitter": "Jingkang Wang", "authors": "Tianshi Cao, Jingkang Wang, Yining Zhang, Sivabalan Manivasagam", "title": "BabyAI++: Towards Grounded-Language Learning beyond Memorization", "comments": "Accepted to the ICLR 2020 workshop: Beyond tabula rasa in RL\n  (BeTR-RL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite success in many real-world tasks (e.g., robotics), reinforcement\nlearning (RL) agents still learn from tabula rasa when facing new and dynamic\nscenarios. By contrast, humans can offload this burden through textual\ndescriptions. Although recent works have shown the benefits of instructive\ntexts in goal-conditioned RL, few have studied whether descriptive texts help\nagents to generalize across dynamic environments. To promote research in this\ndirection, we introduce a new platform, BabyAI++, to generate various dynamic\nenvironments along with corresponding descriptive texts. Moreover, we benchmark\nseveral baselines inherited from the instruction following setting and develop\na novel approach towards visually-grounded language learning on our platform.\nExtensive experiments show strong evidence that using descriptive texts\nimproves the generalization of RL agents across environments with varied\ndynamics.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:58:19 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Cao", "Tianshi", ""], ["Wang", "Jingkang", ""], ["Zhang", "Yining", ""], ["Manivasagam", "Sivabalan", ""]]}, {"id": "2004.07209", "submitter": "Adri\\`a Arbu\\'es-Sang\\\"uesa", "authors": "Adri\\`a Arbu\\'es-Sang\\\"uesa, Adri\\'an Mart\\'in, Javier Fern\\'andez,\n  Coloma Ballester, Gloria Haro", "title": "Using Player's Body-Orientation to Model Pass Feasibility in Soccer", "comments": "Accepted at the Computer Vision in Sports Workshop at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a monocular video of a soccer match, this paper presents a\ncomputational model to estimate the most feasible pass at any given time. The\nmethod leverages offensive player's orientation (plus their location) and\nopponents' spatial configuration to compute the feasibility of pass events\nwithin players of the same team. Orientation data is gathered from body pose\nestimations that are properly projected onto the 2D game field; moreover, a\ngeometrical solution is provided, through the definition of a feasibility\nmeasure, to determine which players are better oriented towards each other.\nOnce analyzed more than 6000 pass events, results show that, by including\norientation as a feasibility measure, a robust computational model can be\nbuilt, reaching more than 0.7 Top-3 accuracy. Finally, the combination of the\norientation feasibility measure with the recently introduced Expected\nPossession Value metric is studied; promising results are obtained, thus\nshowing that existing models can be refined by using orientation as a key\nfeature. These models could help both coaches and analysts to have a better\nunderstanding of the game and to improve the players' decision-making process.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:09:51 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Arbu\u00e9s-Sang\u00fcesa", "Adri\u00e0", ""], ["Mart\u00edn", "Adri\u00e1n", ""], ["Fern\u00e1ndez", "Javier", ""], ["Ballester", "Coloma", ""], ["Haro", "Gloria", ""]]}, {"id": "2004.07210", "submitter": "Abbas Cheddad", "authors": "Abbas Cheddad", "title": "On Box-Cox Transformation for Image Normality and Pattern Classification", "comments": "The paper has 4 Tables and 6 Figures", "journal-ref": "IEEE Access, vol. 8, pp. 154975-154983, 2020", "doi": "10.1109/ACCESS.2020.3018874", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unique member of the power transformation family is known as the Box-Cox\ntransformation. The latter can be seen as a mathematical operation that leads\nto finding the optimum lambda ({\\lambda}) value that maximizes the\nlog-likelihood function to transform a data to a normal distribution and to\nreduce heteroscedasticity. In data analytics, a normality assumption underlies\na variety of statistical test models. This technique, however, is best known in\nstatistical analysis to handle one-dimensional data. Herein, this paper\nrevolves around the utility of such a tool as a pre-processing step to\ntransform two-dimensional data, namely, digital images and to study its effect.\nMoreover, to reduce time complexity, it suffices to estimate the parameter\nlambda in real-time for large two-dimensional matrices by merely considering\ntheir probability density function as a statistical inference of the underlying\ndata distribution. We compare the effect of this light-weight Box-Cox\ntransformation with well-established state-of-the-art low light image\nenhancement techniques. We also demonstrate the effectiveness of our approach\nthrough several test-bed data sets for generic improvement of visual appearance\nof images and for ameliorating the performance of a colour pattern\nclassification algorithm as an example application. Results with and without\nthe proposed approach, are compared using the AlexNet (transfer deep learning)\npretrained model. To the best of our knowledge, this is the first time that the\nBox-Cox transformation is extended to digital images by exploiting histogram\ntransformation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:10:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 20:00:21 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 13:13:17 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Cheddad", "Abbas", ""]]}, {"id": "2004.07268", "submitter": "Luisa Polania", "authors": "Luisa F. Polania, Mauricio Flores, Yiran Li, and Matthew Nokleby", "title": "Learning Furniture Compatibility with Graph Neural Networks", "comments": "Accepted for publication at CVPR Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graph neural network (GNN) approach to the problem of predicting\nthe stylistic compatibility of a set of furniture items from images. While most\nexisting results are based on siamese networks which evaluate pairwise\ncompatibility between items, the proposed GNN architecture exploits relational\ninformation among groups of items. We present two GNN models, both of which\ncomprise a deep CNN that extracts a feature representation for each image, a\ngated recurrent unit (GRU) network that models interactions between the\nfurniture items in a set, and an aggregation function that calculates the\ncompatibility score. In the first model, a generalized contrastive loss\nfunction that promotes the generation of clustered embeddings for items\nbelonging to the same furniture set is introduced. Also, in the first model,\nthe edge function between nodes in the GRU and the aggregation function are\nfixed in order to limit model complexity and allow training on smaller\ndatasets; in the second model, the edge function and aggregation function are\nlearned directly from the data. We demonstrate state-of-the art accuracy for\ncompatibility prediction and \"fill in the blank\" tasks on the Bonn and\nSingapore furniture datasets. We further introduce a new dataset, called the\nTarget Furniture Collections dataset, which contains over 6000 furniture items\nthat have been hand-curated by stylists to make up 1632 compatible sets. We\nalso demonstrate superior prediction accuracy on this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:04:06 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Polania", "Luisa F.", ""], ["Flores", "Mauricio", ""], ["Li", "Yiran", ""], ["Nokleby", "Matthew", ""]]}, {"id": "2004.07301", "submitter": "Andrey Guzhov", "authors": "Andrey Guzhov, Federico Raue, J\\\"orn Hees and Andreas Dengel", "title": "ESResNet: Environmental Sound Classification Based on Visual Domain\n  Models", "comments": "8 pages, 4 figures; submitted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental Sound Classification (ESC) is an active research area in the\naudio domain and has seen a lot of progress in the past years. However, many of\nthe existing approaches achieve high accuracy by relying on domain-specific\nfeatures and architectures, making it harder to benefit from advances in other\nfields (e.g., the image domain). Additionally, some of the past successes have\nbeen attributed to a discrepancy of how results are evaluated (i.e., on\nunofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall\nprogression of the field.\n  The contribution of this paper is twofold. First, we present a model that is\ninherently compatible with mono and stereo sound inputs. Our model is based on\nsimple log-power Short-Time Fourier Transform (STFT) spectrograms and combines\nthem with several well-known approaches from the image domain (i.e., ResNet,\nSiamese-like networks and attention). We investigate the influence of\ncross-domain pre-training, architectural changes, and evaluate our model on\nstandard datasets. We find that our model out-performs all previously known\napproaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10),\n91.5 % (ESC-50) and 84.2 % / 85.4 % (US8K mono / stereo).\n  Second, we provide a comprehensive overview of the actual state of the field,\nby differentiating several previously reported results on the US8K dataset\nbetween official or unofficial splits. For better reproducibility, our code\n(including any re-implementations) is made available.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 19:07:55 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Guzhov", "Andrey", ""], ["Raue", "Federico", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "2004.07312", "submitter": "Rohit Gupta", "authors": "Rohit Gupta and Mubarak Shah", "title": "RescueNet: Joint Building Segmentation and Damage Assessment from\n  Satellite Imagery", "comments": "7 pages, 3 figures, submitted to ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and fine-grained information about the extent of damage to buildings\nis essential for directing Humanitarian Aid and Disaster Response (HADR)\noperations in the immediate aftermath of any natural calamity. In recent years,\nsatellite and UAV (drone) imagery has been used for this purpose, sometimes\naided by computer vision algorithms. Existing Computer Vision approaches for\nbuilding damage assessment typically rely on a two stage approach, consisting\nof building detection using an object detection model, followed by damage\nassessment through classification of the detected building tiles. These\nmulti-stage methods are not end-to-end trainable, and suffer from poor overall\nresults. We propose RescueNet, a unified model that can simultaneously segment\nbuildings and assess the damage levels to individual buildings and can be\ntrained end-toend. In order to to model the composite nature of this problem,\nwe propose a novel localization aware loss function, which consists of a Binary\nCross Entropy loss for building segmentation, and a foreground only selective\nCategorical Cross-Entropy loss for damage classification, and show significant\nimprovement over the widely used Cross-Entropy loss. RescueNet is tested on the\nlarge scale and diverse xBD dataset and achieves significantly better building\nsegmentation and damage classification performance than previous methods and\nachieves generalization across varied geographical regions and disaster types.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 19:52:09 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Gupta", "Rohit", ""], ["Shah", "Mubarak", ""]]}, {"id": "2004.07317", "submitter": "Manuel Burghardt", "authors": "Bernhard Liebl and Manuel Burghardt", "title": "An Evaluation of DNN Architectures for Page Segmentation of Historical\n  Newspapers", "comments": "Evaluation of deep neural networks for the segmentation of pages of\n  historical newspapers; 21 pages total (incl. references and appendix), 7\n  figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important and particularly challenging step in the optical character\nrecognition (OCR) of historical documents with complex layouts, such as\nnewspapers, is the separation of text from non-text content (e.g. page borders\nor illustrations). This step is commonly referred to as page segmentation.\nWhile various rule-based algorithms have been proposed, the applicability of\nDeep Neural Networks (DNNs) for this task recently has gained a lot of\nattention. In this paper, we perform a systematic evaluation of 11 different\npublished DNN backbone architectures and 9 different tiling and scaling\nconfigurations for separating text, tables or table column lines. We also show\nthe influence of the number of labels and the number of training pages on the\nsegmentation quality, which we measure using the Matthews Correlation\nCoefficient. Our results show that (depending on the task) Inception-ResNet-v2\nand EfficientNet backbones work best, vertical tiling is generally preferable\nto other tiling approaches, and training data that comprises 30 to 40 pages\nwill be sufficient most of the time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 20:05:54 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Liebl", "Bernhard", ""], ["Burghardt", "Manuel", ""]]}, {"id": "2004.07339", "submitter": "Mohamed Elmahdy", "authors": "Nicola Pezzotti, Sahar Yousefi, Mohamed S. Elmahdy, Jeroen van Gemert,\n  Christophe Sch\\\"ulke, Mariya Doneva, Tim Nielsen, Sergey Kastryulin,\n  Boudewijn P.F. Lelieveldt, Matthias J.P. van Osch, Elwin de Weerdt, Marius\n  Staring", "title": "An Adaptive Intelligence Algorithm for Undersampled Knee MRI\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive intelligence aims at empowering machine learning techniques with the\nadditional use of domain knowledge. In this work, we present the application of\nadaptive intelligence to accelerate MR acquisition. Starting from undersampled\nk-space data, an iterative learning-based reconstruction scheme inspired by\ncompressed sensing theory is used to reconstruct the images. We adopt deep\nneural networks to refine and correct prior reconstruction assumptions given\nthe training data. The network was trained and tested on a knee MRI dataset\nfrom the 2019 fastMRI challenge organized by Facebook AI Research and NYU\nLangone Health. All submissions to the challenge were initially ranked based on\nsimilarity with a known groundtruth, after which the top 4 submissions were\nevaluated radiologically. Our method was evaluated by the fastMRI organizers on\nan independent challenge dataset. It ranked #1, shared #1, and #3 on\nrespectively the 8x accelerated multi-coil, the 4x multi-coil, and the 4x\nsingle-coil track. This demonstrates the superior performance and wide\napplicability of the method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 20:59:56 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 15:19:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Pezzotti", "Nicola", ""], ["Yousefi", "Sahar", ""], ["Elmahdy", "Mohamed S.", ""], ["van Gemert", "Jeroen", ""], ["Sch\u00fclke", "Christophe", ""], ["Doneva", "Mariya", ""], ["Nielsen", "Tim", ""], ["Kastryulin", "Sergey", ""], ["Lelieveldt", "Boudewijn P. F.", ""], ["van Osch", "Matthias J. P.", ""], ["de Weerdt", "Elwin", ""], ["Staring", "Marius", ""]]}, {"id": "2004.07386", "submitter": "Rajkumar Muthusamy Dr.", "authors": "Rajkumar Muthusamy, Xiaoqian Huang, Yahya Zweiri, Lakmal Seneviratne\n  and Dongming Gan", "title": "Neuromorphic Event-Based Slip Detection and suppression in Robotic\n  Grasping and Manipulation", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slip detection is essential for robots to make robust grasping and fine\nmanipulation. In this paper, a novel dynamic vision-based finger system for\nslip detection and suppression is proposed. We also present a baseline and\nfeature based approach to detect object slips under illumination and vibration\nuncertainty. A threshold method is devised to autonomously sample noise in\nreal-time to improve slip detection. Moreover, a fuzzy based suppression\nstrategy using incipient slip feedback is proposed for regulating the grip\nforce. A comprehensive experimental study of our proposed approaches under\nuncertainty and system for high-performance precision manipulation are\npresented. We also propose a slip metric to evaluate such performance\nquantitatively. Results indicate that the system can effectively detect\nincipient slip events at a sampling rate of 2kHz ($\\Delta t = 500\\mu s$) and\nsuppress them before a gross slip occurs. The event-based approach holds\npromises to high precision manipulation task requirement in industrial\nmanufacturing and household services.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 23:12:30 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Muthusamy", "Rajkumar", ""], ["Huang", "Xiaoqian", ""], ["Zweiri", "Yahya", ""], ["Seneviratne", "Lakmal", ""], ["Gan", "Dongming", ""]]}, {"id": "2004.07392", "submitter": "Tatiana Tommasi", "authors": "Antonio Alliegro, Davide Boscaini, Tatiana Tommasi", "title": "Joint Supervised and Self-Supervised Learning for 3D Real-World\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud processing and 3D shape understanding are very challenging tasks\nfor which deep learning techniques have demonstrated great potentials. Still\nfurther progresses are essential to allow artificial intelligent agents to\ninteract with the real world, where the amount of annotated data may be limited\nand integrating new sources of knowledge becomes crucial to support autonomous\nlearning. Here we consider several possible scenarios involving synthetic and\nreal-world point clouds where supervised learning fails due to data scarcity\nand large domain gaps. We propose to enrich standard feature representations by\nleveraging self-supervision through a multi-task model that can solve a 3D\npuzzle while learning the main task of shape classification or part\nsegmentation. An extensive analysis investigating few-shot, transfer learning\nand cross-domain settings shows the effectiveness of our approach with\nstate-of-the-art results for 3D shape classification and part segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 23:34:03 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Alliegro", "Antonio", ""], ["Boscaini", "Davide", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2004.07394", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Rodrigo Borba Pinheiro, Yannick Berthoumieu", "title": "Generalized Shortest Path-based Superpixels for Accurate Segmentation of\n  Spherical Images", "comments": null, "journal-ref": "International Conference on Pattern Recognition (ICPR 2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing superpixel methods are designed to segment standard planar\nimages as pre-processing for computer vision pipelines. Nevertheless, the\nincreasing number of applications based on wide angle capture devices, mainly\ngenerating 360{\\deg} spherical images, have enforced the need for dedicated\nsuperpixel approaches. In this paper, we introduce a new superpixel method for\nspherical images called SphSPS (for Spherical Shortest Path-based Superpixels).\nOur approach respects the spherical geometry and generalizes the notion of\nshortest path between a pixel and a superpixel center on the 3D spherical\nacquisition space. We show that the feature information on such path can be\nefficiently integrated into our clustering framework and jointly improves the\nrespect of object contours and the shape regularity. To relevantly evaluate\nthis last aspect in the spherical space, we also generalize a planar global\nregularity metric. Finally, the proposed SphSPS method obtains significantly\nbetter performance than both planar and recent spherical superpixel approaches\non the reference 360{\\deg} spherical panorama segmentation dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 23:41:32 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 12:26:35 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 13:00:43 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Pinheiro", "Rodrigo Borba", ""], ["Berthoumieu", "Yannick", ""]]}, {"id": "2004.07398", "submitter": "Rajkumar Muthusamy DSc (Tech)", "authors": "Rajkumar Muthusamy, Abdulla Ayyad, Mohamad Halwani, Yahya Zweiri,\n  Dongming Gan and Lakmal Seneviratne", "title": "Neuromorphic Eye-in-Hand Visual Servoing", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic vision plays a major role in factory automation to service robot\napplications. However, the traditional use of frame-based camera sets a\nlimitation on continuous visual feedback due to their low sampling rate and\nredundant data in real-time image processing, especially in the case of\nhigh-speed tasks. Event cameras give human-like vision capabilities such as\nobserving the dynamic changes asynchronously at a high temporal resolution\n($1\\mu s$) with low latency and wide dynamic range.\n  In this paper, we present a visual servoing method using an event camera and\na switching control strategy to explore, reach and grasp to achieve a\nmanipulation task. We devise three surface layers of active events to directly\nprocess stream of events from relative motion. A purely event based approach is\nadopted to extract corner features, localize them robustly using heat maps and\ngenerate virtual features for tracking and alignment. Based on the visual\nfeedback, the motion of the robot is controlled to make the temporal upcoming\nevent features converge to the desired event in spatio-temporal space. The\ncontroller switches its strategy based on the sequence of operation to\nestablish a stable grasp. The event based visual servoing (EVBS) method is\nvalidated experimentally using a commercial robot manipulator in an eye-in-hand\nconfiguration. Experiments prove the effectiveness of the EBVS method to track\nand grasp objects of different shapes without the need for re-tuning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 23:57:54 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Muthusamy", "Rajkumar", ""], ["Ayyad", "Abdulla", ""], ["Halwani", "Mohamad", ""], ["Zweiri", "Yahya", ""], ["Gan", "Dongming", ""], ["Seneviratne", "Lakmal", ""]]}, {"id": "2004.07399", "submitter": "Shivam Kalra", "authors": "Mohammed Adnan, Shivam Kalra, Hamid R. Tizhoosh", "title": "Representation Learning of Histopathology Images using Graph Neural\n  Networks", "comments": "Published in CVMI at CVPR Workshops, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning for Whole Slide Images (WSIs) is pivotal in\ndeveloping image-based systems to achieve higher precision in diagnostic\npathology. We propose a two-stage framework for WSI representation learning. We\nsample relevant patches using a color-based method and use graph neural\nnetworks to learn relations among sampled patches to aggregate the image\ninformation into a single vector representation. We introduce attention via\ngraph pooling to automatically infer patches with higher relevance. We\ndemonstrate the performance of our approach for discriminating two sub-types of\nlung cancers, Lung Adenocarcinoma (LUAD) & Lung Squamous Cell Carcinoma (LUSC).\nWe collected 1,026 lung cancer WSIs with the 40$\\times$ magnification from The\nCancer Genome Atlas (TCGA) dataset, the largest public repository of\nhistopathology images and achieved state-of-the-art accuracy of 88.8% and AUC\nof 0.89 on lung cancer sub-type classification by extracting features from a\npre-trained DenseNet\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 00:09:20 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 16:39:26 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Adnan", "Mohammed", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "Hamid R.", ""]]}, {"id": "2004.07407", "submitter": "Aryan Mobiny", "authors": "Aryan Mobiny, Pietro Antonio Cicalese, Samira Zare, Pengyu Yuan,\n  Mohammadsajad Abavisani, Carol C. Wu, Jitesh Ahuja, Patricia M. de Groot,\n  Hien Van Nguyen", "title": "Radiologist-Level COVID-19 Detection Using CT Scans with Detail-Oriented\n  Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiographic images offer an alternative method for the rapid screening and\nmonitoring of Coronavirus Disease 2019 (COVID-19) patients. This approach is\nlimited by the shortage of radiology experts who can provide a timely\ninterpretation of these images. Motivated by this challenge, our paper proposes\na novel learning architecture, called Detail-Oriented Capsule Networks\n(DECAPS), for the automatic diagnosis of COVID-19 from Computed Tomography (CT)\nscans. Our network combines the strength of Capsule Networks with several\narchitecture improvements meant to boost classification accuracies. First,\nDECAPS uses an Inverted Dynamic Routing mechanism which increases model\nstability by preventing the passage of information from non-descriptive\nregions. Second, DECAPS employs a Peekaboo training procedure which uses a\ntwo-stage patch crop and drop strategy to encourage the network to generate\nactivation maps for every target concept. The network then uses the activation\nmaps to focus on regions of interest and combines both coarse and fine-grained\nrepresentations of the data. Finally, we use a data augmentation method based\non conditional generative adversarial networks to deal with the issue of data\nscarcity. Our model achieves 84.3% precision, 91.5% recall, and 96.1% area\nunder the ROC curve, significantly outperforming state-of-the-art methods. We\ncompare the performance of the DECAPS model with three experienced,\nwell-trained thoracic radiologists and show that the architecture significantly\noutperforms them. While further studies on larger datasets are required to\nconfirm this finding, our results imply that architectures like DECAPS can be\nused to assist radiologists in the CT scan mediated diagnosis of COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 00:48:32 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Mobiny", "Aryan", ""], ["Cicalese", "Pietro Antonio", ""], ["Zare", "Samira", ""], ["Yuan", "Pengyu", ""], ["Abavisani", "Mohammadsajad", ""], ["Wu", "Carol C.", ""], ["Ahuja", "Jitesh", ""], ["de Groot", "Patricia M.", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "2004.07414", "submitter": "Jungtaek Kim", "authors": "Jungtaek Kim, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, Jaesik Park", "title": "Combinatorial 3D Shape Generation via Sequential Assembly", "comments": "14 pages, 20 figures, 1 table, presented at NeurIPS 2020 Workshop on\n  Machine Learning for Engineering Modeling, Simulation, and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential assembly with geometric primitives has drawn attention in robotics\nand 3D vision since it yields a practical blueprint to construct a target\nshape. However, due to its combinatorial property, a greedy method falls short\nof generating a sequence of volumetric primitives. To alleviate this\nconsequence induced by a huge number of feasible combinations, we propose a\ncombinatorial 3D shape generation framework. The proposed framework reflects an\nimportant aspect of human generation processes in real life -- we often create\na 3D shape by sequentially assembling unit primitives with geometric\nconstraints. To find the desired combination regarding combination evaluations,\nwe adopt Bayesian optimization, which is able to exploit and explore\nefficiently the feasible regions constrained by the current primitive\nplacements. An evaluation function conveys global structure guidance for an\nassembly process and stability in terms of gravity and external forces\nsimultaneously. Experimental results demonstrate that our method successfully\ngenerates combinatorial 3D shapes and simulates more realistic generation\nprocesses. We also introduce a new dataset for combinatorial 3D shape\ngeneration. All the codes are available at\n\\url{https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation}.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 01:23:14 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 03:51:49 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kim", "Jungtaek", ""], ["Chung", "Hyunsoo", ""], ["Lee", "Jinhwi", ""], ["Cho", "Minsu", ""], ["Park", "Jaesik", ""]]}, {"id": "2004.07438", "submitter": "Maur\\'icio Pamplona Segundo", "authors": "Rodrigo Minetto, Mauricio Pamplona Segundo, Gilbert Rotich, Sudeep\n  Sarkar", "title": "Measuring Human and Economic Activity from Satellite Imagery to Support\n  City-Scale Decision-Making during COVID-19 Pandemic", "comments": "13 pages, 10 figures, 2 tables", "journal-ref": null, "doi": "10.1109/TBDATA.2020.3032839", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 outbreak forced governments worldwide to impose lockdowns and\nquarantines to prevent virus transmission. As a consequence, there are\ndisruptions in human and economic activities all over the globe. The recovery\nprocess is also expected to be rough. Economic activities impact social\nbehaviors, which leave signatures in satellite images that can be automatically\ndetected and classified. Satellite imagery can support the decision-making of\nanalysts and policymakers by providing a different kind of visibility into the\nunfolding economic changes. In this work, we use a deep learning approach that\ncombines strategic location sampling and an ensemble of lightweight\nconvolutional neural networks (CNNs) to recognize specific elements in\nsatellite images that could be used to compute economic indicators based on it,\nautomatically. This CNN ensemble framework ranked third place in the US\nDepartment of Defense xView challenge, the most advanced benchmark for object\ndetection in satellite images. We show the potential of our framework for\ntemporal analysis using the US IARPA Function Map of the World (fMoW) dataset.\nWe also show results on real examples of different sites before and after the\nCOVID-19 outbreak to illustrate different measurable indicators. Our code and\nannotated high-resolution aerial scenes before and after the outbreak are\navailable on GitHub (https://github.com/maups/covid19-satellite-analysis).\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:47:11 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 05:23:35 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 00:17:00 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 14:45:11 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Minetto", "Rodrigo", ""], ["Segundo", "Mauricio Pamplona", ""], ["Rotich", "Gilbert", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "2004.07443", "submitter": "Weiyi Xie", "authors": "Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Bram van Ginneken", "title": "Relational Modeling for Robust and Efficient Pulmonary Lobe Segmentation\n  in CT Scans", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2020.2995108", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary lobe segmentation in computed tomography scans is essential for\nregional assessment of pulmonary diseases. Recent works based on convolution\nneural networks have achieved good performance for this task. However, they are\nstill limited in capturing structured relationships due to the nature of\nconvolution. The shape of the pulmonary lobes affect each other and their\nborders relate to the appearance of other structures, such as vessels, airways,\nand the pleural wall. We argue that such structural relationships play a\ncritical role in the accurate delineation of pulmonary lobes when the lungs are\naffected by diseases such as COVID-19 or COPD.\n  In this paper, we propose a relational approach (RTSU-Net) that leverages\nstructured relationships by introducing a novel non-local neural network\nmodule. The proposed module learns both visual and geometric relationships\namong all convolution features to produce self-attention weights.\n  With a limited amount of training data available from COVID-19 subjects, we\ninitially train and validate RTSU-Net on a cohort of 5000 subjects from the\nCOPDGene study (4000 for training and 1000 for evaluation). Using models\npre-trained on COPDGene, we apply transfer learning to retrain and evaluate\nRTSU-Net on 470 COVID-19 suspects (370 for retraining and 100 for evaluation).\nExperimental results show that RTSU-Net outperforms three baselines and\nperforms robustly on cases with severe lung infection due to COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:54:12 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 09:51:40 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 14:57:52 GMT"}, {"version": "v4", "created": "Tue, 12 May 2020 16:20:51 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Xie", "Weiyi", ""], ["Jacobs", "Colin", ""], ["Charbonnier", "Jean-Paul", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2004.07456", "submitter": "Jianfeng Li", "authors": "Gang Peng, Yuezhi Zheng, Jianfeng Li, Jin Yang, Zhonghua Deng", "title": "Single upper limb pose estimation method based on improved stacked\n  hourglass network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, most high-accuracy single-person pose estimation methods have\nhigh computational complexity and insufficient real-time performance due to the\ncomplex structure of the network model. However, a single-person pose\nestimation method with high real-time performance also needs to improve its\naccuracy due to the simple structure of the network model. It is currently\ndifficult to achieve both high accuracy and real-time performance in\nsingle-person pose estimation. For use in human-machine cooperative operations,\nthis paper proposes a single-person upper limb pose estimation method based on\nan end-to-end approach for accurate and real-time limb pose estimation. Using\nthe stacked hourglass network model, a single-person upper limb skeleton key\npoint detection model was designed.Deconvolution was employed to replace the\nup-sampling operation of the hourglass module in the original model, solving\nthe problem of rough feature maps. Integral regression was used to calculate\nthe position coordinates of key points of the skeleton, reducing quantization\nerrors and calculations. Experiments showed that the developed single-person\nupper limb skeleton key point detection model achieves high accuracy and that\nthe pose estimation method based on the end-to-end approach provides high\naccuracy and real-time performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 04:48:40 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Peng", "Gang", ""], ["Zheng", "Yuezhi", ""], ["Li", "Jianfeng", ""], ["Yang", "Jin", ""], ["Deng", "Zhonghua", ""]]}, {"id": "2004.07464", "submitter": "Wenwen Yu", "authors": "Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, Rong Xiao", "title": "PICK: Processing Key Information Extraction from Documents using\n  Improved Graph Learning-Convolutional Networks", "comments": "Accepted by ICPR2020. Code at\n  https://github.com/wenwenyu/PICK-pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision with state-of-the-art deep learning models has achieved huge\nsuccess in the field of Optical Character Recognition (OCR) including text\ndetection and recognition tasks recently. However, Key Information Extraction\n(KIE) from documents as the downstream task of OCR, having a large number of\nuse scenarios in real-world, remains a challenge because documents not only\nhave textual features extracting from OCR systems but also have semantic visual\nfeatures that are not fully exploited and play a critical role in KIE. Too\nlittle work has been devoted to efficiently make full use of both textual and\nvisual features of the documents. In this paper, we introduce PICK, a framework\nthat is effective and robust in handling complex documents layout for KIE by\ncombining graph learning with graph convolution operation, yielding a richer\nsemantic representation containing the textual and visual features and global\nlayout without ambiguity. Extensive experiments on real-world datasets have\nbeen conducted to show that our method outperforms baselines methods by\nsignificant margins. Our code is available at\nhttps://github.com/wenwenyu/PICK-pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 05:20:16 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 16:26:07 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 08:13:53 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yu", "Wenwen", ""], ["Lu", "Ning", ""], ["Qi", "Xianbiao", ""], ["Gong", "Ping", ""], ["Xiao", "Rong", ""]]}, {"id": "2004.07472", "submitter": "Yanru Huang", "authors": "Yanru Huang, Feiyu Zhu, Zheni Zeng, Xi Qiu, Yuan Shen, Jianan Wu", "title": "SQE: a Self Quality Evaluation Metric for Parameters Optimization in\n  Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel self quality evaluation metric SQE for parameters\noptimization in the challenging yet critical multi-object tracking task.\nCurrent evaluation metrics all require annotated ground truth, thus will fail\nin the test environment and realistic circumstances prohibiting further\noptimization after training. By contrast, our metric reflects the internal\ncharacteristics of trajectory hypotheses and measures tracking performance\nwithout ground truth. We demonstrate that trajectories with different qualities\nexhibit different single or multiple peaks over feature distance distribution,\ninspiring us to design a simple yet effective method to assess the quality of\ntrajectories using a two-class Gaussian mixture model. Experiments mainly on\nMOT16 Challenge data sets verify the effectiveness of our method in both\ncorrelating with existing metrics and enabling parameters self-optimization to\nachieve better performance. We believe that our conclusions and method are\ninspiring for future multi-object tracking in practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:07:29 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Huang", "Yanru", ""], ["Zhu", "Feiyu", ""], ["Zeng", "Zheni", ""], ["Qiu", "Xi", ""], ["Shen", "Yuan", ""], ["Wu", "Jianan", ""]]}, {"id": "2004.07480", "submitter": "Yuxiang Sun", "authors": "Tianyu Liu, Qinghai Liao, Lu Gan, Fulong Ma, Jie Cheng, Xupeng Xie,\n  Zhe Wang, Yingbing Chen, Yilong Zhu, Shuyang Zhang, Zhengyong Chen, Yang Liu,\n  Meng Xie, Yang Yu, Zitong Guo, Guang Li, Peidong Yuan, Dong Han, Yuying Chen,\n  Haoyang Ye, Jianhao Jiao, Peng Yun, Zhenhua Xu, Hengli Wang, Huaiyang Huang,\n  Sukai Wang, Peide Cai, Yuxiang Sun, Yandong Liu, Lujia Wang, Ming Liu", "title": "The Role of the Hercules Autonomous Vehicle During the COVID-19\n  Pandemic: An Autonomous Logistic Vehicle for Contactless Goods Transportation", "comments": null, "journal-ref": "IEEE Robotics and Automation Magazine, 2021", "doi": "10.1109/MRA.2020.3045040", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since early 2020, the coronavirus disease 2019 (COVID-19) has spread rapidly\nacross the world. As at the date of writing this article, the disease has been\nglobally reported in 223 countries and regions, infected over 108 million\npeople and caused over 2.4 million deaths (https://covid19.who.int/, accessed\non Feb. 17, 2021). Avoiding person-to-person transmission is an effective\napproach to control and prevent the pandemic. However, many daily activities,\nsuch as transporting goods in our daily life, inevitably involve\nperson-to-person contact. Using an autonomous logistic vehicle to achieve\ncontact-less goods transportation could alleviate this issue. For example, it\ncan reduce the risk of virus transmission between the driver and customers.\nMoreover, many countries have imposed tough lockdown measures to reduce the\nvirus transmission (e.g., retail, catering) during the pandemic, which causes\ninconveniences for human daily life. Autonomous vehicle can deliver the goods\nbought by humans, so that humans can get the goods without going out. These\ndemands motivate us to develop an autonomous vehicle, named as Hercules, for\ncontact-less goods transportation during the COVID-19 pandemic. The vehicle is\nevaluated through real-world delivering tasks under various traffic conditions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:37:06 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 04:43:52 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Liu", "Tianyu", ""], ["Liao", "Qinghai", ""], ["Gan", "Lu", ""], ["Ma", "Fulong", ""], ["Cheng", "Jie", ""], ["Xie", "Xupeng", ""], ["Wang", "Zhe", ""], ["Chen", "Yingbing", ""], ["Zhu", "Yilong", ""], ["Zhang", "Shuyang", ""], ["Chen", "Zhengyong", ""], ["Liu", "Yang", ""], ["Xie", "Meng", ""], ["Yu", "Yang", ""], ["Guo", "Zitong", ""], ["Li", "Guang", ""], ["Yuan", "Peidong", ""], ["Han", "Dong", ""], ["Chen", "Yuying", ""], ["Ye", "Haoyang", ""], ["Jiao", "Jianhao", ""], ["Yun", "Peng", ""], ["Xu", "Zhenhua", ""], ["Wang", "Hengli", ""], ["Huang", "Huaiyang", ""], ["Wang", "Sukai", ""], ["Cai", "Peide", ""], ["Sun", "Yuxiang", ""], ["Liu", "Yandong", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "2004.07482", "submitter": "Fatemeh Sadat Saleh", "authors": "Fatemeh Saleh, Sadegh Aliakbarian, Mathieu Salzmann, Stephen Gould", "title": "ArTIST: Autoregressive Trajectory Inpainting and Scoring for Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core components in online multiple object tracking (MOT)\nframeworks is associating new detections with existing tracklets, typically\ndone via a scoring function. Despite the great advances in MOT, designing a\nreliable scoring function remains a challenge. In this paper, we introduce a\nprobabilistic autoregressive generative model to score tracklet proposals by\ndirectly measuring the likelihood that a tracklet represents natural motion.\nOne key property of our model is its ability to generate multiple likely\nfutures of a tracklet given partial observations. This allows us to not only\nscore tracklets but also effectively maintain existing tracklets when the\ndetector fails to detect some objects even for a long time, e.g., due to\nocclusion, by sampling trajectories so as to inpaint the gaps caused by\nmisdetection. Our experiments demonstrate the effectiveness of our approach to\nscoring and inpainting tracklets on several MOT benchmark datasets. We\nadditionally show the generality of our generative model by using it to produce\nfuture representations in the challenging task of human motion prediction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:43:11 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Saleh", "Fatemeh", ""], ["Aliakbarian", "Sadegh", ""], ["Salzmann", "Mathieu", ""], ["Gould", "Stephen", ""]]}, {"id": "2004.07485", "submitter": "Jiajun Tang", "authors": "Jiajun Tang, Jin Xia, Xinzhi Mu, Bo Pang, Cewu Lu", "title": "Asynchronous Interaction Aggregation for Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding interaction is an essential part of video action detection. We\npropose the Asynchronous Interaction Aggregation network (AIA) that leverages\ndifferent interactions to boost action detection. There are two key designs in\nit: one is the Interaction Aggregation structure (IA) adopting a uniform\nparadigm to model and integrate multiple types of interaction; the other is the\nAsynchronous Memory Update algorithm (AMU) that enables us to achieve better\nperformance by modeling very long-term interaction dynamically without huge\ncomputation cost. We provide empirical evidence to show that our network can\ngain notable accuracy from the integrative interactions and is easy to train\nend-to-end. Our method reports the new state-of-the-art performance on AVA\ndataset, with 3.7 mAP gain (12.6% relative improvement) on validation split\ncomparing to our strong baseline. The results on dataset UCF101-24 and\nEPIC-Kitchens further illustrate the effectiveness of our approach. Source code\nwill be made public at: https://github.com/MVIG-SJTU/AlphAction .\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:03:20 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Tang", "Jiajun", ""], ["Xia", "Jin", ""], ["Mu", "Xinzhi", ""], ["Pang", "Bo", ""], ["Lu", "Cewu", ""]]}, {"id": "2004.07489", "submitter": "Liping Zhang", "authors": "Liping Zhang, Weijun Li, Xin Ning", "title": "A Local Descriptor with Physiological Characteristic for Finger Vein\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local feature descriptors exhibit great superiority in finger vein\nrecognition due to their stability and robustness against local changes in\nimages. However, most of these are methods use general-purpose descriptors that\ndo not consider finger vein-specific features. In this work, we propose a\nfinger vein-specific local feature descriptors based physiological\ncharacteristic of finger vein patterns, i.e., histogram of oriented\nphysiological Gabor responses (HOPGR), for finger vein recognition. First,\nprior of directional characteristic of finger vein patterns is obtained in an\nunsupervised manner. Then the physiological Gabor filter banks are set up based\non the prior information to extract the physiological responses and\norientation. Finally, to make feature has robustness against local changes in\nimages, histogram is generated as output by dividing the image into\nnon-overlapping cells and overlapping blocks. Extensive experimental results on\nseveral databases clearly demonstrate that the proposed method outperforms most\ncurrent state-of-the-art finger vein recognition methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:22:28 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhang", "Liping", ""], ["Li", "Weijun", ""], ["Ning", "Xin", ""]]}, {"id": "2004.07507", "submitter": "Janghyeon Lee", "authors": "Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, Junmo Kim", "title": "Continual Learning with Extended Kronecker-factored Approximate\n  Curvature", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quadratic penalty method for continual learning of neural\nnetworks that contain batch normalization (BN) layers. The Hessian of a loss\nfunction represents the curvature of the quadratic penalty function, and a\nKronecker-factored approximate curvature (K-FAC) is used widely to practically\ncompute the Hessian of a neural network. However, the approximation is not\nvalid if there is dependence between examples, typically caused by BN layers in\ndeep network architectures. We extend the K-FAC method so that the\ninter-example relations are taken into account and the Hessian of deep neural\nnetworks can be properly approximated under practical assumptions. We also\npropose a method of weight merging and reparameterization to properly handle\nstatistical parameters of BN, which plays a critical role for continual\nlearning with BN, and a method that selects hyperparameters without source task\ndata. Our method shows better performance than baselines in the permuted MNIST\ntask with BN layers and in sequential learning from the ImageNet classification\ntask to fine-grained classification tasks with ResNet-50, without any explicit\nor implicit use of source task data for hyperparameter selection.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:58:47 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Lee", "Janghyeon", ""], ["Hong", "Hyeong Gwon", ""], ["Joo", "Donggyu", ""], ["Kim", "Junmo", ""]]}, {"id": "2004.07511", "submitter": "Tom Vermeire", "authors": "Tom Vermeire, David Martens", "title": "Explainable Image Classification with Evidence Counterfactual", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of state-of-the-art modeling techniques for image\nclassification impedes the ability to explain model predictions in an\ninterpretable way. Existing explanation methods generally create importance\nrankings in terms of pixels or pixel groups. However, the resulting\nexplanations lack an optimal size, do not consider feature dependence and are\nonly related to one class. Counterfactual explanation methods are considered\npromising to explain complex model decisions, since they are associated with a\nhigh degree of human interpretability. In this paper, SEDC is introduced as a\nmodel-agnostic instance-level explanation method for image classification to\nobtain visual counterfactual explanations. For a given image, SEDC searches a\nsmall set of segments that, in case of removal, alters the classification. As\nimage classification tasks are typically multiclass problems, SEDC-T is\nproposed as an alternative method that allows specifying a target\ncounterfactual class. We compare SEDC(-T) with popular feature importance\nmethods such as LRP, LIME and SHAP, and we describe how the mentioned\nimportance ranking issues are addressed. Moreover, concrete examples and\nexperiments illustrate the potential of our approach (1) to obtain trust and\ninsight, and (2) to obtain input for model improvement by explaining\nmisclassifications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 08:02:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Vermeire", "Tom", ""], ["Martens", "David", ""]]}, {"id": "2004.07514", "submitter": "Jonghwan Mun", "authors": "Jonghwan Mun, Minsu Cho, Bohyung Han", "title": "Local-Global Video-Text Interactions for Temporal Grounding", "comments": "CVPR 2020; code available in\n  https://github.com/JonghwanMun/LGI4temporalgrounding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of text-to-video temporal grounding, which\naims to identify the time interval in a video semantically relevant to a text\nquery. We tackle this problem using a novel regression-based model that learns\nto extract a collection of mid-level features for semantic phrases in a text\nquery, which corresponds to important semantic entities described in the query\n(e.g., actors, objects, and actions), and reflect bi-modal interactions between\nthe linguistic features of the query and the visual features of the video in\nmultiple levels. The proposed method effectively predicts the target time\ninterval by exploiting contextual information from local to global during\nbi-modal interactions. Through in-depth ablation studies, we find out that\nincorporating both local and global context in video and text interactions is\ncrucial to the accurate grounding. Our experiment shows that the proposed\nmethod outperforms the state of the arts on Charades-STA and ActivityNet\nCaptions datasets by large margins, 7.44\\% and 4.61\\% points at Recall@tIoU=0.5\nmetric, respectively. Code is available in\nhttps://github.com/JonghwanMun/LGI4temporalgrounding.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 08:10:41 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Mun", "Jonghwan", ""], ["Cho", "Minsu", ""], ["Han", "Bohyung", ""]]}, {"id": "2004.07532", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Sergio Romero-Tapiador, Julian Fierrez and Ruben\n  Vera-Rodriguez", "title": "DeepFakes Evolution: Analysis of Facial Regions and Fake Detection\n  Performance", "comments": null, "journal-ref": "Proc. International Conference on Pattern Recognition Workshops\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media forensics has attracted a lot of attention in the last years in part\ndue to the increasing concerns around DeepFakes. Since the initial DeepFake\ndatabases from the 1st generation such as UADFV and FaceForensics++ up to the\nlatest databases of the 2nd generation such as Celeb-DF and DFDC, many visual\nimprovements have been carried out, making fake videos almost indistinguishable\nto the human eye. This study provides an exhaustive analysis of both 1st and\n2nd DeepFake generations in terms of facial regions and fake detection\nperformance. Two different methods are considered in our experimental\nframework: i) the traditional one followed in the literature and based on\nselecting the entire face as input to the fake detection system, and ii) a\nnovel approach based on the selection of specific facial regions as input to\nthe fake detection system.\n  Among all the findings resulting from our experiments, we highlight the poor\nfake detection results achieved even by the strongest state-of-the-art fake\ndetectors in the latest DeepFake databases of the 2nd generation, with Equal\nError Rate results ranging from 15% to 30%. These results remark the necessity\nof further research to develop more sophisticated fake detectors.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 08:49:32 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:24:22 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tolosana", "Ruben", ""], ["Romero-Tapiador", "Sergio", ""], ["Fierrez", "Julian", ""], ["Vera-Rodriguez", "Ruben", ""]]}, {"id": "2004.07538", "submitter": "Mingjie Sun", "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim, Bingfeng Zhang, Yao Zhao", "title": "Fast Template Matching and Update for Video Object Tracking and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the main task we aim to tackle is the multi-instance\nsemi-supervised video object segmentation across a sequence of frames where\nonly the first-frame box-level ground-truth is provided. Detection-based\nalgorithms are widely adopted to handle this task, and the challenges lie in\nthe selection of the matching method to predict the result as well as to decide\nwhether to update the target template using the newly predicted result. The\nexisting methods, however, make these selections in a rough and inflexible way,\ncompromising their performance. To overcome this limitation, we propose a novel\napproach which utilizes reinforcement learning to make these two decisions at\nthe same time. Specifically, the reinforcement learning agent learns to decide\nwhether to update the target template according to the quality of the predicted\nresult. The choice of the matching method will be determined at the same time,\nbased on the action history of the reinforcement learning agent. Experiments\nshow that our method is almost 10 times faster than the previous\nstate-of-the-art method with even higher accuracy (region similarity of 69.1%\non DAVIS 2017 dataset).\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 08:58:45 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Sun", "Mingjie", ""], ["Xiao", "Jimin", ""], ["Lim", "Eng Gee", ""], ["Zhang", "Bingfeng", ""], ["Zhao", "Yao", ""]]}, {"id": "2004.07544", "submitter": "Adrien Deli\\`ege Mr", "authors": "Anthony Cioppa, Adrien Deli\\`ege, Noor Ul Huda, Rikke Gade, Marc Van\n  Droogenbroeck, Thomas B. Moeslund", "title": "Multimodal and multiview distillation for real-time player detection on\n  a football field", "comments": "Accepted for the CVSports workshop of CVPR 2020 ; 8 pages +\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the occupancy of public sports facilities is essential to assess\ntheir use and to motivate their construction in new places. In the case of a\nfootball field, the area to cover is large, thus several regular cameras should\nbe used, which makes the setup expensive and complex. As an alternative, we\ndeveloped a system that detects players from a unique cheap and wide-angle\nfisheye camera assisted by a single narrow-angle thermal camera. In this work,\nwe train a network in a knowledge distillation approach in which the student\nand the teacher have different modalities and a different view of the same\nscene. In particular, we design a custom data augmentation combined with a\nmotion detection algorithm to handle the training in the region of the fisheye\ncamera not covered by the thermal one. We show that our solution is effective\nin detecting players on the whole field filmed by the fisheye camera. We\nevaluate it quantitatively and qualitatively in the case of an online\ndistillation, where the student detects players in real time while being\ncontinuously adapted to the latest video conditions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:16:20 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Cioppa", "Anthony", ""], ["Deli\u00e8ge", "Adrien", ""], ["Huda", "Noor Ul", ""], ["Gade", "Rikke", ""], ["Van Droogenbroeck", "Marc", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2004.07568", "submitter": "Fa-Ting Hong", "authors": "Fa-Ting Hong, Wei-Hong Li, Wei-Shi Zheng", "title": "Learning to Detect Important People in Unlabelled Images for\n  Semi-supervised Important People Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important people detection is to automatically detect the individuals who\nplay the most important roles in a social event image, which requires the\ndesigned model to understand a high-level pattern. However, existing methods\nrely heavily on supervised learning using large quantities of annotated image\nsamples, which are more costly to collect for important people detection than\nfor individual entity recognition (eg, object recognition). To overcome this\nproblem, we propose learning important people detection on partially annotated\nimages. Our approach iteratively learns to assign pseudo-labels to individuals\nin un-annotated images and learns to update the important people detection\nmodel based on data with both labels and pseudo-labels. To alleviate the\npseudo-labelling imbalance problem, we introduce a ranking strategy for\npseudo-label estimation, and also introduce two weighting strategies: one for\nweighting the confidence that individuals are important people to strengthen\nthe learning on important people and the other for neglecting noisy unlabelled\nimages (ie, images without any important people). We have collected two\nlarge-scale datasets for evaluation. The extensive experimental results clearly\nconfirm the efficacy of our method attained by leveraging unlabelled images for\nimproving the performance of important people detection.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:09:37 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hong", "Fa-Ting", ""], ["Li", "Wei-Hong", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2004.07570", "submitter": "Ildoo Kim", "authors": "Ildoo Kim, Woonhyuk Baek, Sungwoong Kim", "title": "Spatially Attentive Output Layer for Image Classification", "comments": "First two authors contributed equally. Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most convolutional neural networks (CNNs) for image classification use a\nglobal average pooling (GAP) followed by a fully-connected (FC) layer for\noutput logits. However, this spatial aggregation procedure inherently restricts\nthe utilization of location-specific information at the output layer, although\nthis spatial information can be beneficial for classification. In this paper,\nwe propose a novel spatial output layer on top of the existing convolutional\nfeature maps to explicitly exploit the location-specific output information. In\nspecific, given the spatial feature maps, we replace the previous GAP-FC layer\nwith a spatially attentive output layer (SAOL) by employing a attention mask on\nspatial logits. The proposed location-specific attention selectively aggregates\nspatial logits within a target region, which leads to not only the performance\nimprovement but also spatially interpretable outputs. Moreover, the proposed\nSAOL also permits to fully exploit location-specific self-supervision as well\nas self-distillation to enhance the generalization ability during training. The\nproposed SAOL with self-supervision and self-distillation can be easily plugged\ninto existing CNNs. Experimental results on various classification tasks with\nrepresentative architectures show consistent performance improvements by SAOL\nat almost the same computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:11:38 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Kim", "Ildoo", ""], ["Baek", "Woonhyuk", ""], ["Kim", "Sungwoong", ""]]}, {"id": "2004.07624", "submitter": "Shumao Pang", "authors": "Yujia Zhou, Shumao Pang, Jun Cheng, Yuhang Sun, Yi Wu, Lei Zhao, Yaqin\n  Liu, Zhentai Lu, Wei Yang, and Qianjin Feng", "title": "Unsupervised Deformable Medical Image Registration via Pyramidal\n  Residual Deformation Fields Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation field estimation is an important and challenging issue in many\nmedical image registration applications. In recent years, deep learning\ntechnique has become a promising approach for simplifying registration\nproblems, and has been gradually applied to medical image registration.\nHowever, most existing deep learning registrations do not consider the problem\nthat when the receptive field cannot cover the corresponding features in the\nmoving image and the fixed image, it cannot output accurate displacement\nvalues. In fact, due to the limitation of the receptive field, the 3 x 3 kernel\nhas difficulty in covering the corresponding features at high/original\nresolution. Multi-resolution and multi-convolution techniques can improve but\nfail to avoid this problem. In this study, we constructed pyramidal feature\nsets on moving and fixed images and used the warped moving and fixed features\nto estimate their \"residual\" deformation field at each scale, called the\nPyramidal Residual Deformation Field Estimation module (PRDFE-Module). The\n\"total\" deformation field at each scale was computed by upsampling and weighted\nsumming all the \"residual\" deformation fields at all its previous scales, which\ncan effectively and accurately transfer the deformation fields from low\nresolution to high resolution and is used for warping the moving features at\neach scale. Simulation and real brain data results show that our method\nimproves the accuracy of the registration and the rationality of the\ndeformation field.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 12:24:27 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhou", "Yujia", ""], ["Pang", "Shumao", ""], ["Cheng", "Jun", ""], ["Sun", "Yuhang", ""], ["Wu", "Yi", ""], ["Zhao", "Lei", ""], ["Liu", "Yaqin", ""], ["Lu", "Zhentai", ""], ["Yang", "Wei", ""], ["Feng", "Qianjin", ""]]}, {"id": "2004.07629", "submitter": "Nergis Tomen", "authors": "Ioannis Lelekas, Nergis Tomen, Silvia L. Pintea and Jan C. van Gemert", "title": "Top-Down Networks: A coarse-to-fine reimagination of CNNs", "comments": "CVPR Workshop Deep Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological vision adopts a coarse-to-fine information processing pathway,\nfrom initial visual detection and binding of salient features of a visual\nscene, to the enhanced and preferential processing given relevant stimuli. On\nthe contrary, CNNs employ a fine-to-coarse processing, moving from local,\nedge-detecting filters to more global ones extracting abstract representations\nof the input. In this paper we reverse the feature extraction part of standard\nbottom-up architectures and turn them upside-down: We propose top-down\nnetworks. Our proposed coarse-to-fine pathway, by blurring higher frequency\ninformation and restoring it only at later stages, offers a line of defence\nagainst adversarial attacks that introduce high frequency noise. Moreover,\nsince we increase image resolution with depth, the high resolution of the\nfeature map in the final convolutional layer contributes to the explainability\nof the network's decision making process. This favors object-driven decisions\nover context driven ones, and thus provides better localized class activation\nmaps. This paper offers empirical evidence for the applicability of the\ntop-down resolution processing to various existing architectures on multiple\nvisual tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 12:29:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Lelekas", "Ioannis", ""], ["Tomen", "Nergis", ""], ["Pintea", "Silvia L.", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2004.07639", "submitter": "Fabian Timm", "authors": "Thomas Michalke, Di Feng, Claudius Gl\\\"aser, and Fabian Timm", "title": "Where can I drive? A System Approach: Deep Ego Corridor Estimation for\n  Robust Automated Driving", "comments": "8 pages, preprint, accepted at IEEE ITSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection is an essential part of the perception sub-architecture of any\nautomated driving (AD) or advanced driver assistance system (ADAS). When\nfocusing on low-cost, large scale products for automated driving, model-driven\napproaches for the detection of lane markings have proven good performance.\nMore recently, data-driven approaches have been proposed that target the\ndrivable area / freespace mainly in inner-city applications. Focus of these\napproaches is less on lane-based driving due to the fact that the lane concept\ndoes not fully apply in unstructured, residential inner-city environments.\nSo-far the concept of drivable area is seldom used for highway and inter-urban\napplications due to the specific requirements of these scenarios that require\nclear lane associations of all traffic participants. We believe that\nlane-based, mapless driving in inter-urban and highway scenarios is still not\nfully handled with sufficient robustness and availability. Especially for\nchallenging weather situations such as heavy rain, fog, low-standing sun,\ndarkness or reflections in puddles, the mapless detection of lane markings\ndecreases significantly or completely fails. We see potential in applying\nspecifically designed data-driven freespace approaches in more lane-based\ndriving applications for highways and inter-urban use. Therefore, we propose to\nclassify specifically a drivable corridor of the ego lane on pixel level with a\ndeep learning approach. Our approach is kept computationally efficient with\nonly 0.66 million parameters allowing its application in large scale products.\nThus, we were able to easily integrate into an online AD system of a test\nvehicle. We demonstrate the performance of our approach under challenging\nconditions qualitatively and quantitatively in comparison to a state-of-the-art\nmodel-driven approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:04:18 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 11:59:56 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Michalke", "Thomas", ""], ["Feng", "Di", ""], ["Gl\u00e4ser", "Claudius", ""], ["Timm", "Fabian", ""]]}, {"id": "2004.07657", "submitter": "Muhammad Zaigham Zaheer", "authors": "Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, Seung-Ik Lee", "title": "Old is Gold: Redefining the Adversarially Learned One-Class Classifier\n  Training Paradigm", "comments": "Accepted at the Conference on Computer Vision and Pattern Recognition\n  CVPR 2020.\n  http://openaccess.thecvf.com/content_CVPR_2020/html/Zaheer_Old_Is_Gold_Redefining_the_Adversarially_Learned_One-Class_Classifier_Training_CVPR_2020_paper.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular method for anomaly detection is to use the generator of an\nadversarial network to formulate anomaly scores over reconstruction loss of\ninput. Due to the rare occurrence of anomalies, optimizing such networks can be\na cumbersome task. Another possible approach is to use both generator and\ndiscriminator for anomaly detection. However, attributed to the involvement of\nadversarial training, this model is often unstable in a way that the\nperformance fluctuates drastically with each training step. In this study, we\npropose a framework that effectively generates stable results across a wide\nrange of training steps and allows us to use both the generator and the\ndiscriminator of an adversarial model for efficient and robust anomaly\ndetection. Our approach transforms the fundamental role of a discriminator from\nidentifying real and fake data to distinguishing between good and bad quality\nreconstructions. To this end, we prepare training examples for the good quality\nreconstruction by employing the current generator, whereas poor quality\nexamples are obtained by utilizing an old state of the same generator. This\nway, the discriminator learns to detect subtle distortions that often appear in\nreconstructions of the anomaly inputs. Extensive experiments performed on\nCaltech-256 and MNIST image datasets for novelty detection show superior\nresults. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our\nmodel achieves a frame-level AUC of 98.1%, surpassing recent state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:48:58 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 02:29:19 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 02:24:09 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 08:06:34 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zaheer", "Muhammad Zaigham", ""], ["Lee", "Jin-ha", ""], ["Astrid", "Marcella", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "2004.07676", "submitter": "Nicol\\`o Bonettini", "authors": "Nicol\\`o Bonettini, Edoardo Daniele Cannas, Sara Mandelli, Luca Bondi,\n  Paolo Bestagini, Stefano Tubaro", "title": "Video Face Manipulation Detection Through Ensemble of CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, several techniques for facial manipulation in videos\nhave been successfully developed and made available to the masses (i.e.,\nFaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in\nvideo sequences with incredibly realistic results and a very little effort.\nDespite the usefulness of these tools in many fields, if used maliciously, they\ncan have a significantly bad impact on society (e.g., fake news spreading,\ncyber bullying through fake revenge porn). The ability of objectively detecting\nwhether a face has been manipulated in a video sequence is then a task of\nutmost importance. In this paper, we tackle the problem of face manipulation\ndetection in video sequences targeting modern facial manipulation techniques.\nIn particular, we study the ensembling of different trained Convolutional\nNeural Network (CNN) models. In the proposed solution, different models are\nobtained starting from a base network (i.e., EfficientNetB4) making use of two\ndifferent concepts: (i) attention layers; (ii) siamese training. We show that\ncombining these networks leads to promising face manipulation detection results\non two publicly available datasets with more than 119000 videos.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:19:40 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bonettini", "Nicol\u00f2", ""], ["Cannas", "Edoardo Daniele", ""], ["Mandelli", "Sara", ""], ["Bondi", "Luca", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2004.07682", "submitter": "Nicol\\`o Bonettini", "authors": "Nicol\\`o Bonettini, Paolo Bestagini, Simone Milani, Stefano Tubaro", "title": "On the use of Benford's law to detect GAN-generated images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Generative Adversarial Network (GAN) architectures has given\nanyone the ability of generating incredibly realistic synthetic imagery. The\nmalicious diffusion of GAN-generated images may lead to serious social and\npolitical consequences (e.g., fake news spreading, opinion formation, etc.). It\nis therefore important to regulate the widespread distribution of synthetic\nimagery by developing solutions able to detect them. In this paper, we study\nthe possibility of using Benford's law to discriminate GAN-generated images\nfrom natural photographs. Benford's law describes the distribution of the most\nsignificant digit for quantized Discrete Cosine Transform (DCT) coefficients.\nExtending and generalizing this property, we show that it is possible to\nextract a compact feature vector from an image. This feature vector can be fed\nto an extremely simple classifier for GAN-generated image detection purpose.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:42:14 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bonettini", "Nicol\u00f2", ""], ["Bestagini", "Paolo", ""], ["Milani", "Simone", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2004.07684", "submitter": "Zhen Mingmin", "authors": "Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang\n  Shang, Tian Fang, Quan Long", "title": "Joint Semantic Segmentation and Boundary Detection using Iterative\n  Pyramid Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a joint multi-task learning framework for semantic\nsegmentation and boundary detection. The critical component in the framework is\nthe iterative pyramid context module (PCM), which couples two tasks and stores\nthe shared latent semantics to interact between the two tasks. For semantic\nboundary detection, we propose the novel spatial gradient fusion to suppress\nnonsemantic edges. As semantic boundary detection is the dual task of semantic\nsegmentation, we introduce a loss function with boundary consistency constraint\nto improve the boundary pixel accuracy for semantic segmentation. Our extensive\nexperiments demonstrate superior performance over state-of-the-art works, not\nonly in semantic segmentation but also in semantic boundary detection. In\nparticular, a mean IoU score of 81:8% on Cityscapes test set is achieved\nwithout using coarse data or any external data for semantic segmentation. For\nsemantic boundary detection, we improve over previous state-of-the-art works by\n9.9% in terms of AP and 6:8% in terms of MF(ODS).\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:46:58 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhen", "Mingmin", ""], ["Wang", "Jinglu", ""], ["Zhou", "Lei", ""], ["Li", "Shiwei", ""], ["Shen", "Tianwei", ""], ["Shang", "Jiaxiang", ""], ["Fang", "Tian", ""], ["Long", "Quan", ""]]}, {"id": "2004.07691", "submitter": "Florin Condrea", "authors": "Florin Condrea, Victor-Andrei Ivan, Marius Leordeanu", "title": "In Search of Life: Learning from Synthetic Data to Detect Vital Signs in\n  Videos", "comments": "Computer Vision and Pattern Recognition (CVPR) Workshop on Computer\n  Vision for Physiological Measurement (CVPM) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting vital signs in videos, such as the estimation of\nheart and respiration rates, is a challenging research problem in computer\nvision with important applications in the medical field. One of the key\ndifficulties in tackling this task is the lack of sufficient supervised\ntraining data, which severely limits the use of powerful deep neural networks.\nIn this paper we address this limitation through a novel deep learning\napproach, in which a recurrent deep neural network is trained to detect vital\nsigns in the infrared thermal domain from purely synthetic data. What is most\nsurprising is that our novel method for synthetic training data generation is\ngeneral, relatively simple and uses almost no prior medical domain knowledge.\nMoreover, our system, which is trained in a purely automatic manner and needs\nno human annotation, also learns to predict the respiration or heart intensity\nsignal for each moment in time and to detect the region of interest that is\nmost relevant for the given task, e.g. the nose area in the case of\nrespiration. We test the effectiveness of our proposed system on the recent\nLCAS dataset and obtain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:02:46 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 18:18:39 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Condrea", "Florin", ""], ["Ivan", "Victor-Andrei", ""], ["Leordeanu", "Marius", ""]]}, {"id": "2004.07703", "submitter": "Fei Pan", "authors": "Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, In So Kweon", "title": "Unsupervised Intra-domain Adaptation for Semantic Segmentation through\n  Self-Supervision", "comments": "Accepted to CVPR 2020 as an Oral Presentation. Code is available at\n  https://github.com/feipan664/IntraDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network-based approaches have achieved remarkable\nprogress in semantic segmentation. However, these approaches heavily rely on\nannotated data which are labor intensive. To cope with this limitation,\nautomatically annotated data generated from graphic engines are used to train\nsegmentation models. However, the models trained from synthetic data are\ndifficult to transfer to real images. To tackle this issue, previous works have\nconsidered directly adapting models from the source data to the unlabeled\ntarget data (to reduce the inter-domain gap). Nonetheless, these techniques do\nnot consider the large distribution gap among the target data itself\n(intra-domain gap). In this work, we propose a two-step self-supervised domain\nadaptation approach to minimize the inter-domain and intra-domain gap together.\nFirst, we conduct the inter-domain adaptation of the model; from this\nadaptation, we separate the target domain into an easy and hard split using an\nentropy-based ranking function. Finally, to decrease the intra-domain gap, we\npropose to employ a self-supervised adaptation technique from the easy to the\nhard split. Experimental results on numerous benchmark datasets highlight the\neffectiveness of our method against existing state-of-the-art approaches. The\nsource code is available at https://github.com/feipan664/IntraDA.git.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:24:11 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 12:25:50 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 08:03:50 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 11:29:25 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Pan", "Fei", ""], ["Shin", "Inkyu", ""], ["Rameau", "Francois", ""], ["Lee", "Seokju", ""], ["Kweon", "In So", ""]]}, {"id": "2004.07711", "submitter": "Lamberto Ballan", "authors": "Guglielmo Camporese, Pasquale Coscia, Antonino Furnari, Giovanni Maria\n  Farinella, Lamberto Ballan", "title": "Knowledge Distillation for Action Anticipation via Label Smoothing", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human capability to anticipate near future from visual observations and\nnon-verbal cues is essential for developing intelligent systems that need to\ninteract with people. Several research areas, such as human-robot interaction\n(HRI), assisted living or autonomous driving need to foresee future events to\navoid crashes or help people. Egocentric scenarios are classic examples where\naction anticipation is applied due to their numerous applications. Such\nchallenging task demands to capture and model domain's hidden structure to\nreduce prediction uncertainty. Since multiple actions may equally occur in the\nfuture, we treat action anticipation as a multi-label problem with missing\nlabels extending the concept of label smoothing. This idea resembles the\nknowledge distillation process since useful information is injected into the\nmodel during training. We implement a multi-modal framework based on long\nshort-term memory (LSTM) networks to summarize past observations and make\npredictions at different time steps. We perform extensive experiments on\nEPIC-Kitchens and EGTEA Gaze+ datasets including more than 2500 and 100 action\nclasses, respectively. The experiments show that label smoothing systematically\nimproves performance of state-of-the-art models for action anticipation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:38:53 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 13:28:40 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Camporese", "Guglielmo", ""], ["Coscia", "Pasquale", ""], ["Furnari", "Antonino", ""], ["Farinella", "Giovanni Maria", ""], ["Ballan", "Lamberto", ""]]}, {"id": "2004.07728", "submitter": "Keyan Ding", "authors": "Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli", "title": "Image Quality Assessment: Unifying Structure and Texture Similarity", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2020.3045810", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective measures of image quality generally operate by comparing pixels of\na \"degraded\" image to those of the original. Relative to human observers, these\nmeasures are overly sensitive to resampling of texture regions (e.g., replacing\none patch of grass with another). Here, we develop the first full-reference\nimage quality model with explicit tolerance to texture resampling. Using a\nconvolutional neural network, we construct an injective and differentiable\nfunction that transforms images to multi-scale overcomplete representations. We\ndemonstrate empirically that the spatial averages of the feature maps in this\nrepresentation capture texture appearance, in that they provide a set of\nsufficient statistical constraints to synthesize a wide variety of texture\npatterns. We then describe an image quality method that combines correlations\nof these spatial averages (\"texture similarity\") with correlations of the\nfeature maps (\"structure similarity\"). The parameters of the proposed measure\nare jointly optimized to match human ratings of image quality, while minimizing\nthe reported distances between subimages cropped from the same texture images.\nExperiments show that the optimized method explains human perceptual scores,\nboth on conventional image quality databases, as well as on texture databases.\nThe measure also offers competitive performance on related tasks such as\ntexture classification and retrieval. Finally, we show that our method is\nrelatively insensitive to geometric transformations (e.g., translation and\ndilation), without use of any specialized training or data augmentation. Code\nis available at https://github.com/dingkeyan93/DISTS.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:11:46 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 05:17:04 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 12:56:44 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ding", "Keyan", ""], ["Ma", "Kede", ""], ["Wang", "Shiqi", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "2004.07745", "submitter": "Mathieu Labussi\\`ere", "authors": "Mathieu Labussi\\`ere, C\\'eline Teuli\\`ere, Fr\\'ed\\'eric Bernardin,\n  Omar Ait-Aider", "title": "Blur Aware Calibration of Multi-Focus Plenoptic Camera", "comments": "2020 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel calibration algorithm for Multi-Focus Plenoptic\nCameras (MFPCs) using raw images only. The design of such cameras is usually\ncomplex and relies on precise placement of optic elements. Several calibration\nprocedures have been proposed to retrieve the camera parameters but relying on\nsimplified models, reconstructed images to extract features, or multiple\ncalibrations when several types of micro-lens are used. Considering blur\ninformation, we propose a new Blur Aware Plenoptic (BAP) feature. It is first\nexploited in a pre-calibration step that retrieves initial camera parameters,\nand secondly to express a new cost function for our single optimization\nprocess. The effectiveness of our calibration method is validated by\nquantitative and qualitative experiments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:29:34 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Labussi\u00e8re", "Mathieu", ""], ["Teuli\u00e8re", "C\u00e9line", ""], ["Bernardin", "Fr\u00e9d\u00e9ric", ""], ["Ait-Aider", "Omar", ""]]}, {"id": "2004.07769", "submitter": "Pei Wang", "authors": "Pei Wang, Nuno Vasconcelos", "title": "SCOUT: Self-aware Discriminant Counterfactual Explanations", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counterfactual visual explanations is considered. A new family\nof discriminant explanations is introduced. These produce heatmaps that\nattribute high scores to image regions informative of a classifier prediction\nbut not of a counter class. They connect attributive explanations, which are\nbased on a single heat map, to counterfactual explanations, which account for\nboth predicted class and counter class. The latter are shown to be computable\nby combination of two discriminant explanations, with reversed class pairs. It\nis argued that self-awareness, namely the ability to produce classification\nconfidence scores, is important for the computation of discriminant\nexplanations, which seek to identify regions where it is easy to discriminate\nbetween prediction and counter class. This suggests the computation of\ndiscriminant explanations by the combination of three attribution maps. The\nresulting counterfactual explanations are optimization free and thus much\nfaster than previous methods. To address the difficulty of their evaluation, a\nproxy task and set of quantitative metrics are also proposed. Experiments under\nthis protocol show that the proposed counterfactual explanations outperform the\nstate of the art while achieving much higher speeds, for popular networks. In a\nhuman-learning machine teaching experiment, they are also shown to improve mean\nstudent accuracy from chance level to 95\\%.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:05:49 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Wang", "Pei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2004.07777", "submitter": "Bhanuka Mahanama", "authors": "Bhanuka Mahanama, Yasith Jayawardana and Sampath Jayarathna", "title": "Gaze-Net: Appearance-Based Gaze Estimation using Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on appearance based gaze estimation indicate the ability of\nNeural Networks to decode gaze information from facial images encompassing pose\ninformation. In this paper, we propose Gaze-Net: A capsule network capable of\ndecoding, representing, and estimating gaze information from ocular region\nimages. We evaluate our proposed system using two publicly available datasets,\nMPIIGaze (200,000+ images in the wild) and Columbia Gaze (5000+ images of users\nwith 21 gaze directions observed at 5 camera angles/positions). Our model\nachieves a Mean Absolute Error (MAE) of 2.84$^\\circ$ for Combined angle error\nestimate within dataset for MPI-IGaze dataset. Further, model achieves a MAE of\n10.04$^\\circ$ for across dataset gaze estimation error for Columbia gaze\ndataset. Through transfer learning, the error is reduced to 5.9$^\\circ$. The\nresults show this approach is promising with implications towards using\ncommodity webcams to develop low-cost multi-user gaze tracking systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:12:06 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Mahanama", "Bhanuka", ""], ["Jayawardana", "Yasith", ""], ["Jayarathna", "Sampath", ""]]}, {"id": "2004.07780", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, J\\\"orn-Henrik Jacobsen, Claudio Michaelis, Richard\n  Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann", "title": "Shortcut Learning in Deep Neural Networks", "comments": "perspective article published at Nature Machine Intelligence\n  (https://doi.org/10.1038/s42256-020-00257-z)", "journal-ref": null, "doi": "10.1038/s42256-020-00257-z", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has triggered the current rise of artificial intelligence and\nis the workhorse of today's machine intelligence. Numerous success stories have\nrapidly spread all over science, industry and society, but its limitations have\nonly recently come into focus. In this perspective we seek to distil how many\nof deep learning's problem can be seen as different symptoms of the same\nunderlying problem: shortcut learning. Shortcuts are decision rules that\nperform well on standard benchmarks but fail to transfer to more challenging\ntesting conditions, such as real-world scenarios. Related issues are known in\nComparative Psychology, Education and Linguistics, suggesting that shortcut\nlearning may be a common characteristic of learning systems, biological and\nartificial alike. Based on these observations, we develop a set of\nrecommendations for model interpretation and benchmarking, highlighting recent\nadvances in machine learning to improve robustness and transferability from the\nlab to real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:18:49 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 08:03:44 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 09:10:46 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 13:53:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Geirhos", "Robert", ""], ["Jacobsen", "J\u00f6rn-Henrik", ""], ["Michaelis", "Claudio", ""], ["Zemel", "Richard", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "2004.07786", "submitter": "Bing Shuai", "authors": "Bing Shuai, Andrew G. Berneshawi, Davide Modolo, Joseph Tighe", "title": "Multi-Object Tracking with Siamese Track-RCNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking systems often consist of a combination of a detector, a\nshort term linker, a re-identification feature extractor and a solver that\ntakes the output from these separate components and makes a final prediction.\nDifferently, this work aims to unify all these in a single tracking system.\nTowards this, we propose Siamese Track-RCNN, a two stage detect-and-track\nframework which consists of three functional branches: (1) the detection branch\nlocalizes object instances; (2) the Siamese-based track branch estimates the\nobject motion and (3) the object re-identification branch re-activates the\npreviously terminated tracks when they re-emerge. We test our tracking system\non two popular datasets of the MOTChallenge. Siamese Track-RCNN achieves\nsignificantly higher results than the state-of-the-art, while also being much\nmore efficient, thanks to its unified design.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:28:52 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Shuai", "Bing", ""], ["Berneshawi", "Andrew G.", ""], ["Modolo", "Davide", ""], ["Tighe", "Joseph", ""]]}, {"id": "2004.07788", "submitter": "Sinead Kearney", "authors": "Sinead Kearney, Wenbin Li, Martin Parsons, Kwang In Kim, Darren Cosker", "title": "RGBD-Dog: Predicting Canine Pose from RGBD Sensors", "comments": "18 pages, 16 figures, to be published in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic extraction of animal \\reb{3D} pose from images without markers\nis of interest in a range of scientific fields. Most work to date predicts\nanimal pose from RGB images, based on 2D labelling of joint positions. However,\ndue to the difficult nature of obtaining training data, no ground truth dataset\nof 3D animal motion is available to quantitatively evaluate these approaches.\nIn addition, a lack of 3D animal pose data also makes it difficult to train 3D\npose-prediction methods in a similar manner to the popular field of body-pose\nprediction. In our work, we focus on the problem of 3D canine pose estimation\nfrom RGBD images, recording a diverse range of dog breeds with several\nMicrosoft Kinect v2s, simultaneously obtaining the 3D ground truth skeleton via\na motion capture system. We generate a dataset of synthetic RGBD images from\nthis data. A stacked hourglass network is trained to predict 3D joint\nlocations, which is then constrained using prior models of shape and pose. We\nevaluate our model on both synthetic and real RGBD images and compare our\nresults to previously published work fitting canine models to images. Finally,\ndespite our training set consisting only of dog data, visual inspection implies\nthat our network can produce good predictions for images of other quadrupeds --\ne.g. horses or cats -- when their pose is similar to that contained in our\ntraining set.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:34:45 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Kearney", "Sinead", ""], ["Li", "Wenbin", ""], ["Parsons", "Martin", ""], ["Kim", "Kwang In", ""], ["Cosker", "Darren", ""]]}, {"id": "2004.07802", "submitter": "Mikhail Khodak", "authors": "Liam Li, Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar", "title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "comments": "ICLR 2021 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art methods for neural architecture search (NAS) exploit\ngradient-based optimization by relaxing the problem into continuous\noptimization over architectures and shared-weights, a noisy process that\nremains poorly understood. We argue for the study of single-level empirical\nrisk minimization to understand NAS with weight-sharing, reducing the design of\nNAS methods to devising optimizers and regularizers that can quickly obtain\nhigh-quality solutions to this problem. Invoking the theory of mirror descent,\nwe present a geometry-aware framework that exploits the underlying structure of\nthis optimization to return sparse architectural parameters, leading to simple\nyet novel algorithms that enjoy fast convergence guarantees and achieve\nstate-of-the-art accuracy on the latest NAS benchmarks in computer vision.\nNotably, we exceed the best published results for both CIFAR and ImageNet on\nboth the DARTS search space and NAS-Bench201; on the latter we achieve\nnear-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory\nand experiments demonstrate a principled way to co-design optimizers and\ncontinuous relaxations of discrete NAS search spaces.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:46:39 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 16:03:42 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 22:20:48 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 14:44:17 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 17:47:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Li", "Liam", ""], ["Khodak", "Mikhail", ""], ["Balcan", "Maria-Florina", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2004.07879", "submitter": "Ashok Goel", "authors": "Snejana Sheghava and Ashok Goel", "title": "Symmetry as an Organizing Principle for Geometric Intelligence", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration of geometrical patterns stimulates imagination and encourages\nabstract reasoning which is a distinctive feature of human intelligence. In\ncognitive science, Gestalt principles such as symmetry have often explained\nsignificant aspects of human perception. We present a computational technique\nfor building artificial intelligence (AI) agents that use symmetry as the\norganizing principle for addressing Dehaene's test of geometric intelligence\n\\cite{dehaene2006core}. The performance of our model is on par with extant AI\nmodels of problem solving on the Dehaene's test and seems correlated with some\nelements of human behavior on the same test.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:58:15 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Sheghava", "Snejana", ""], ["Goel", "Ashok", ""]]}, {"id": "2004.07882", "submitter": "Zongwei Zhou", "authors": "Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B. Gotway, Jianming\n  Liang", "title": "Models Genesis", "comments": "Journal version of arXiv:1908.06912, accepted by Medical Image\n  Analysis", "journal-ref": null, "doi": "10.1016/j.media.2020.101840", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from natural images to medical images has been established\nas one of the most practical paradigms in deep learning for medical image\nanalysis. To fit this paradigm, however, 3D imaging tasks in the most prominent\nimaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D,\nlosing rich 3D anatomical information, thereby inevitably compromising its\nperformance. To overcome this limitation, we have built a set of models, called\nGeneric Autodidactic Models, nicknamed Models Genesis, because they are created\nex nihilo (with no manual labeling), self-taught (learnt by self-supervision),\nand generic (served as source models for generating application-specific target\nmodels). Our extensive experiments demonstrate that our Models Genesis\nsignificantly outperform learning from scratch and existing pre-trained 3D\nmodels in all five target 3D applications covering both segmentation and\nclassification. More importantly, learning a model from scratch simply in 3D\nmay not necessarily yield performance better than transfer learning from\nImageNet in 2D, but our Models Genesis consistently top any 2D/2.5D approaches\nincluding fine-tuning the models pre-trained from ImageNet as well as\nfine-tuning the 2D versions of our Models Genesis, confirming the importance of\n3D anatomical information and significance of Models Genesis for 3D medical\nimaging. This performance is attributed to our unified self-supervised learning\nframework, built on a simple yet powerful observation: the sophisticated and\nrecurrent anatomy in medical images can serve as strong yet free supervision\nsignals for deep models to learn common anatomical representation automatically\nvia self-supervision. As open science, all codes and pre-trained Models Genesis\nare available at https://github.com/MrGiovanni/ModelsGenesis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 20:37:33 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 05:34:40 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 06:29:32 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 19:58:08 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zhou", "Zongwei", ""], ["Sodha", "Vatsal", ""], ["Pang", "Jiaxuan", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "2004.07903", "submitter": "Jeremy Tan", "authors": "Jeremy Tan and Bernhard Kainz", "title": "Divergent Search for Few-Shot Image Classification", "comments": "Submitted to GECCO2020 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is unlabelled and the target task is not known a priori, divergent\nsearch offers a strategy for learning a wide range of skills. Having such a\nrepertoire allows a system to adapt to new, unforeseen tasks. Unlabelled image\ndata is plentiful, but it is not always known which features will be required\nfor downstream tasks. We propose a method for divergent search in the few-shot\nimage classification setting and evaluate with Omniglot and Mini-ImageNet. This\nhigh-dimensional behavior space includes all possible ways of partitioning the\ndata. To manage divergent search in this space, we rely on a meta-learning\nframework to integrate useful features from diverse tasks into a single model.\nThe final layer of this model is used as an index into the `archive' of all\npast behaviors. We search for regions in the behavior space that the current\narchive cannot reach. As expected, divergent search is outperformed by models\nwith a strong bias toward the evaluation tasks. But it is able to match and\nsometimes exceed the performance of models that have a weak bias toward the\ntarget task or none at all. This demonstrates that divergent search is a viable\napproach, even in high-dimensional behavior spaces.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:47:50 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tan", "Jeremy", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2004.07923", "submitter": "Ramin Jafari", "authors": "R. Jafari, P. Spincemaille, J. Zhang, T. D. Nguyen, M. R. Prince, X.\n  Luo, J. Cho, D. Margolis, Y. Wang", "title": "Deep Neural Network (DNN) for Water/Fat Separation: Supervised Training,\n  Unsupervised Training, and No Training", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To use a deep neural network (DNN) for solving the optimization\nproblem of water/fat separation and to compare supervised and unsupervised\ntraining.\n  Methods: The current T2*-IDEAL algorithm for solving fat/water separation is\ndependent on initialization. Recently, deep neural networks (DNN) have been\nproposed to solve fat/water separation without the need for suitable\ninitialization. However, this approach requires supervised training of DNN\n(STD) using the reference fat/water separation images. Here we propose two\nnovel DNN water/fat separation methods 1) unsupervised training of DNN (UTD)\nusing the physical forward problem as the cost function during training, and 2)\nno-training of DNN (NTD) using physical cost and backpropagation to directly\nreconstruct a single dataset. The STD, UTD and NTD methods were compared with\nthe reference T2*-IDEAL.\n  Results: All DNN methods generated consistent water/fat separation results\nthat agreed well with T2*-IDEAL under proper initialization.\n  Conclusion: The water/fat separation problem can be solved using unsupervised\ndeep neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:24:29 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Jafari", "R.", ""], ["Spincemaille", "P.", ""], ["Zhang", "J.", ""], ["Nguyen", "T. D.", ""], ["Prince", "M. R.", ""], ["Luo", "X.", ""], ["Cho", "J.", ""], ["Margolis", "D.", ""], ["Wang", "Y.", ""]]}, {"id": "2004.07931", "submitter": "Zheng Dang", "authors": "Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua and Mathieu\n  Salzmann", "title": "Eigendecomposition-Free Training of Deep Networks for Linear\n  Least-Square Problems", "comments": "16 pages, Accepted by TPAMI. arXiv admin note: substantial text\n  overlap with arXiv:1803.08071", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical Computer Vision problems, such as essential matrix computation\nand pose estimation from 3D to 2D correspondences, can be tackled by solving a\nlinear least-square problem, which can be done by finding the eigenvector\ncorresponding to the smallest, or zero, eigenvalue of a matrix representing a\nlinear system. Incorporating this in deep learning frameworks would allow us to\nexplicitly encode known notions of geometry, instead of having the network\nimplicitly learn them from data. However, performing eigendecomposition within\na network requires the ability to differentiate this operation. While\ntheoretically doable, this introduces numerical instability in the optimization\nprocess in practice. In this paper, we introduce an eigendecomposition-free\napproach to training a deep network whose loss depends on the eigenvector\ncorresponding to a zero eigenvalue of a matrix predicted by the network. We\ndemonstrate that our approach is much more robust than explicit differentiation\nof the eigendecomposition using two general tasks, outlier rejection and\ndenoising, with several practical examples including wide-baseline stereo, the\nperspective-n-point problem, and ellipse fitting. Empirically, our method has\nbetter convergence properties and yields state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 04:29:34 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Dang", "Zheng", ""], ["Yi", "Kwang Moo", ""], ["Hu", "Yinlin", ""], ["Wang", "Fei", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2004.07936", "submitter": "Weijian Li", "authors": "Weijian Li, Haofu Liao, Shun Miao, Le Lu, and Jiebo Luo", "title": "Unsupervised Learning of Landmarks based on Inter-Intra Subject\n  Consistencies", "comments": "Accepted to ICPR-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised learning approach to image landmark discovery\nby incorporating the inter-subject landmark consistencies on facial images.\nThis is achieved via an inter-subject mapping module that transforms original\nsubject landmarks based on an auxiliary subject-related structure. To recover\nfrom the transformed images back to the original subject, the landmark detector\nis forced to learn spatial locations that contain the consistent semantic\nmeanings both for the paired intra-subject images and between the paired\ninter-subject images. Our proposed method is extensively evaluated on two\npublic facial image datasets (MAFL, AFLW) with various settings. Experimental\nresults indicate that our method can extract the consistent landmarks for both\ndatasets and achieve better performances compared to the previous\nstate-of-the-art methods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:38:16 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 23:04:42 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Weijian", ""], ["Liao", "Haofu", ""], ["Miao", "Shun", ""], ["Lu", "Le", ""], ["Luo", "Jiebo", ""]]}, {"id": "2004.07941", "submitter": "Keval Doshi", "authors": "Keval Doshi, Yasin Yilmaz", "title": "Continual Learning for Anomaly Detection in Surveillance Videos", "comments": "accepted to CVPR 2020: Workshop on Continual Learning in Computer\n  Vision. arXiv admin note: text overlap with arXiv:2004.02072", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in surveillance videos has been recently gaining attention.\nA challenging aspect of high-dimensional applications such as video\nsurveillance is continual learning. While current state-of-the-art deep\nlearning approaches perform well on existing public datasets, they fail to work\nin a continual learning framework due to computational and storage issues.\nFurthermore, online decision making is an important but mostly neglected factor\nin this domain. Motivated by these research gaps, we propose an online anomaly\ndetection method for surveillance videos using transfer learning and continual\nlearning, which in turn significantly reduces the training complexity and\nprovides a mechanism for continually learning from recent data without\nsuffering from catastrophic forgetting. Our proposed algorithm leverages the\nfeature extraction power of neural network-based models for transfer learning,\nand the continual learning capability of statistical detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:41:20 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2004.07944", "submitter": "Panagiotis Meletis", "authors": "Panagiotis Meletis, Xiaoxiao Wen, Chenyang Lu, Daan de Geus, Gijs\n  Dubbelman", "title": "Cityscapes-Panoptic-Parts and PASCAL-Panoptic-Parts datasets for Scene\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this technical report, we present two novel datasets for image scene\nunderstanding. Both datasets have annotations compatible with panoptic\nsegmentation and additionally they have part-level labels for selected semantic\nclasses. This report describes the format of the two datasets, the annotation\nprotocols, the merging strategies, and presents the datasets statistics. The\ndatasets labels together with code for processing and visualization will be\npublished at https://github.com/tue-mps/panoptic_parts.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:42:51 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Meletis", "Panagiotis", ""], ["Wen", "Xiaoxiao", ""], ["Lu", "Chenyang", ""], ["de Geus", "Daan", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "2004.07945", "submitter": "Mahdi Rad", "authors": "Mahdi Rad, Peter M. Roth, Vincent Lepetit", "title": "ALCN: Adaptive Local Contrast Normalization", "comments": "This version corresponds to the pre-print of the paper accepted for\n  Computer Vision and Image Understanding (CVIU). arXiv admin note: substantial\n  text overlap with arXiv:1708.09633", "journal-ref": null, "doi": "10.1016/j.cviu.2020.102947", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make Robotics and Augmented Reality applications robust to illumination\nchanges, the current trend is to train a Deep Network with training images\ncaptured under many different lighting conditions. Unfortunately, creating such\na training set is a very unwieldy and complex task. We therefore propose a\nnovel illumination normalization method that can easily be used for different\nproblems with challenging illumination conditions. Our preliminary experiments\nshow that among current normalization methods, the Difference-of Gaussians\nmethod remains a very good baseline, and we introduce a novel illumination\nnormalization model that generalizes it. Our key insight is then that the\nnormalization parameters should depend on the input image, and we aim to train\na Convolutional Neural Network to predict these parameters from the input\nimage. This, however, cannot be done in a supervised manner, as the optimal\nparameters are not known a priori. We thus designed a method to train this\nnetwork jointly with another network that aims to recognize objects under\ndifferent illuminations: The latter network performs well when the former\nnetwork predicts good values for the normalization parameters. We show that our\nmethod significantly outperforms standard normalization methods and would also\nbe appear to be universal since it does not have to be re-trained for each new\napplication. Our method improves the robustness to light changes of\nstate-of-the-art 3D object detection and face recognition methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 13:40:03 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Rad", "Mahdi", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2004.07950", "submitter": "Alexander Pashevich", "authors": "Alexander Pashevich, Igor Kalevatykh, Ivan Laptev, Cordelia Schmid", "title": "Learning visual policies for building 3D shape categories", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation and assembly tasks require non-trivial planning of actions\ndepending on the environment and the final goal. Previous work in this domain\noften assembles particular instances of objects from known sets of primitives.\nIn contrast, we aim to handle varying sets of primitives and to construct\ndifferent objects of a shape category. Given a single object instance of a\ncategory, e.g. an arch, and a binary shape classifier, we learn a visual policy\nto assemble other instances of the same category. In particular, we propose a\ndisassembly procedure and learn a state policy that discovers new object\ninstances and their assembly plans in state space. We then render simulated\nstates in the observation space and learn a heatmap representation to predict\nalternative actions from a given input image. To validate our approach, we\nfirst demonstrate its efficiency for building object categories in state space.\nWe then show the success of our visual policies for building arches from\ndifferent primitives. Moreover, we demonstrate (i) the reactive ability of our\nmethod to re-assemble objects using additional primitives and (ii) the robust\nperformance of our policy for unseen primitives resembling building blocks used\nduring training. Our visual assembly policies are trained with no real images\nand reach up to 95% success rate when evaluated on a real robot.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:29:10 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 22:24:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Pashevich", "Alexander", ""], ["Kalevatykh", "Igor", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2004.07955", "submitter": "Jiawang Bai", "authors": "Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao\n  Xia, En-hui Yang", "title": "Targeted Attack for Deep Hashing based Retrieval", "comments": "Accepted by ECCV 2020 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep hashing based retrieval method is widely adopted in large-scale\nimage and video retrieval. However, there is little investigation on its\nsecurity. In this paper, we propose a novel method, dubbed deep hashing\ntargeted attack (DHTA), to study the targeted attack on such retrieval.\nSpecifically, we first formulate the targeted attack as a point-to-set\noptimization, which minimizes the average distance between the hash code of an\nadversarial example and those of a set of objects with the target label. Then\nwe design a novel component-voting scheme to obtain an anchor code as the\nrepresentative of the set of hash codes of objects with the target label, whose\noptimality guarantee is also theoretically derived. To balance the performance\nand perceptibility, we propose to minimize the Hamming distance between the\nhash code of the adversarial example and the anchor code under the\n$\\ell^\\infty$ restriction on the perturbation. Extensive experiments verify\nthat DHTA is effective in attacking both deep hashing based image retrieval and\nvideo retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 08:36:58 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 01:25:12 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 08:24:04 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Bai", "Jiawang", ""], ["Chen", "Bin", ""], ["Li", "Yiming", ""], ["Wu", "Dongxian", ""], ["Guo", "Weiwei", ""], ["Xia", "Shu-tao", ""], ["Yang", "En-hui", ""]]}, {"id": "2004.07965", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu, Puneet Sharma, Ashish Sharma, Imon Banerjee,\n  Hari Trivedi, Saptarshi Purkayastha, Priyanshu Sinha, Alexandre\n  Cadrin-Chenevert, Nabile Safdar, Judy Wawira Gichoya", "title": "A DICOM Framework for Machine Learning Pipelines against Real-Time\n  Radiology Images", "comments": "Preprint", "journal-ref": "Journal of Digital Imaging (JDI), 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Executing machine learning (ML) pipelines in real-time on radiology images is\nhard due to the limited computing resources in clinical environments and the\nlack of efficient data transfer capabilities to run them on research clusters.\nWe propose Niffler, an integrated framework that enables the execution of ML\npipelines at research clusters by efficiently querying and retrieving radiology\nimages from the Picture Archiving and Communication Systems (PACS) of the\nhospitals. Niffler uses the Digital Imaging and Communications in Medicine\n(DICOM) protocol to fetch and store imaging data and provides metadata\nextraction capabilities and Application programming interfaces (APIs) to apply\nfilters on the images. Niffler further enables the sharing of the outcomes from\nthe ML pipelines in a de-identified manner. Niffler has been running stable for\nmore than 19 months and has supported several research projects at the\ndepartment. In this paper, we present its architecture and three of its use\ncases: an inferior vena cava (IVC) filter detection from the images in\nreal-time, identification of scanner utilization, and scanner clock\ncalibration. Evaluations on the Niffler prototype highlight its feasibility and\nefficiency in facilitating the ML pipelines on the images and metadata in\nreal-time and retrospectively.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 21:06:49 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 11:59:04 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 03:16:23 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 04:55:24 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Sharma", "Puneet", ""], ["Sharma", "Ashish", ""], ["Banerjee", "Imon", ""], ["Trivedi", "Hari", ""], ["Purkayastha", "Saptarshi", ""], ["Sinha", "Priyanshu", ""], ["Cadrin-Chenevert", "Alexandre", ""], ["Safdar", "Nabile", ""], ["Gichoya", "Judy Wawira", ""]]}, {"id": "2004.07967", "submitter": "Tomo Miyazaki", "authors": "Huy Manh Nguyen, Tomo Miyazaki, Yoshihiro Sugaya, Shinichiro Omachi", "title": "Multiple Visual-Semantic Embedding for Video Retrieval from Query\n  Sentence", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual-semantic embedding aims to learn a joint embedding space where related\nvideo and sentence instances are located close to each other. Most existing\nmethods put instances in a single embedding space. However, they struggle to\nembed instances due to the difficulty of matching visual dynamics in videos to\ntextual features in sentences. A single space is not enough to accommodate\nvarious videos and sentences. In this paper, we propose a novel framework that\nmaps instances into multiple individual embedding spaces so that we can capture\nmultiple relationships between instances, leading to compelling video\nretrieval. We propose to produce a final similarity between instances by fusing\nsimilarities measured in each embedding space using a weighted sum strategy. We\ndetermine the weights according to a sentence. Therefore, we can flexibly\nemphasize an embedding space. We conducted sentence-to-video retrieval\nexperiments on a benchmark dataset. The proposed method achieved superior\nperformance, and the results are competitive to state-of-the-art methods. These\nexperimental results demonstrated the effectiveness of the proposed multiple\nembedding approach compared to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 21:12:32 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Nguyen", "Huy Manh", ""], ["Miyazaki", "Tomo", ""], ["Sugaya", "Yoshihiro", ""], ["Omachi", "Shinichiro", ""]]}, {"id": "2004.07995", "submitter": "Ruizhe Li", "authors": "Ruizhe Li, Dorothee Auer, Christian Wagner, Xin Chen", "title": "A generic ensemble based deep convolutional neural network for\n  semi-supervised medical image segmentation", "comments": "Accepted for publication at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2020", "journal-ref": "2020 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based image segmentation has achieved the state-of-the-art\nperformance in many medical applications such as lesion quantification, organ\ndetection, etc. However, most of the methods rely on supervised learning, which\nrequire a large set of high-quality labeled data. Data annotation is generally\nan extremely time-consuming process. To address this problem, we propose a\ngeneric semi-supervised learning framework for image segmentation based on a\ndeep convolutional neural network (DCNN). An encoder-decoder based DCNN is\ninitially trained using a few annotated training samples. This initially\ntrained model is then copied into sub-models and improved iteratively using\nrandom subsets of unlabeled data with pseudo labels generated from models\ntrained in the previous iteration. The number of sub-models is gradually\ndecreased to one in the final iteration. We evaluate the proposed method on a\npublic grand-challenge dataset for skin lesion segmentation. Our method is able\nto significantly improve beyond fully supervised model learning by\nincorporating unlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 23:41:50 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Li", "Ruizhe", ""], ["Auer", "Dorothee", ""], ["Wagner", "Christian", ""], ["Chen", "Xin", ""]]}, {"id": "2004.07999", "submitter": "Angelina Wang", "authors": "Angelina Wang and Alexander Liu and Ryan Zhang and Anat Kleiman and\n  Leslie Kim and Dora Zhao and Iroha Shirai and Arvind Narayanan and Olga\n  Russakovsky", "title": "REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets", "comments": "Extended version of ECCV 2020 Spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models are known to perpetuate and even amplify the biases\npresent in the data. However, these data biases frequently do not become\napparent until after the models are deployed. Our work tackles this issue and\nenables the preemptive analysis of large-scale datasets. REVISE (REvealing\nVIsual biaSEs) is a tool that assists in the investigation of a visual dataset,\nsurfacing potential biases along three dimensions: (1) object-based, (2)\nperson-based, and (3) geography-based. Object-based biases relate to the size,\ncontext, or diversity of the depicted objects. Person-based metrics focus on\nanalyzing the portrayal of people within the dataset. Geography-based analyses\nconsider the representation of different geographic locations. These three\ndimensions are deeply intertwined in how they interact to bias a dataset, and\nREVISE sheds light on this; the responsibility then lies with the user to\nconsider the cultural and historical context, and to determine which of the\nrevealed biases may be problematic. The tool further assists the user by\nsuggesting actionable steps that may be taken to mitigate the revealed biases.\nOverall, the key aim of our work is to tackle the machine learning bias problem\nearly in the pipeline. REVISE is available at\nhttps://github.com/princetonvisualai/revise-tool\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 23:54:37 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 23:46:59 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 15:54:37 GMT"}, {"version": "v4", "created": "Fri, 23 Jul 2021 18:41:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Angelina", ""], ["Liu", "Alexander", ""], ["Zhang", "Ryan", ""], ["Kleiman", "Anat", ""], ["Kim", "Leslie", ""], ["Zhao", "Dora", ""], ["Shirai", "Iroha", ""], ["Narayanan", "Arvind", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2004.08008", "submitter": "Alexander Wong", "authors": "Linda Wang, Mahmoud Famouri, and Alexander Wong", "title": "DepthNet Nano: A Highly Compact Self-Normalizing Neural Network for\n  Monocular Depth Estimation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is an active area of research in the field of computer\nvision, and has garnered significant interest due to its rising demand in a\nlarge number of applications ranging from robotics and unmanned aerial vehicles\nto autonomous vehicles. A particularly challenging problem in this area is\nmonocular depth estimation, where the goal is to infer depth from a single\nimage. An effective strategy that has shown considerable promise in recent\nyears for tackling this problem is the utilization of deep convolutional neural\nnetworks. Despite these successes, the memory and computational requirements of\nsuch networks have made widespread deployment in embedded scenarios very\nchallenging. In this study, we introduce DepthNet Nano, a highly compact self\nnormalizing network for monocular depth estimation designed using a human\nmachine collaborative design strategy, where principled network design\nprototyping based on encoder-decoder design principles are coupled with\nmachine-driven design exploration. The result is a compact deep neural network\nwith highly customized macroarchitecture and microarchitecture designs, as well\nas self-normalizing characteristics, that are highly tailored for the task of\nembedded depth estimation. The proposed DepthNet Nano possesses a highly\nefficient network architecture (e.g., 24X smaller and 42X fewer MAC operations\nthan Alhashim et al. on KITTI), while still achieving comparable performance\nwith state-of-the-art networks on the NYU-Depth V2 and KITTI datasets.\nFurthermore, experiments on inference speed and energy efficiency on a Jetson\nAGX Xavier embedded module further illustrate the efficacy of DepthNet Nano at\ndifferent resolutions and power budgets (e.g., ~14 FPS and >0.46\nimages/sec/watt at 384 X 1280 at a 30W power budget on KITTI).\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 00:41:35 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Wang", "Linda", ""], ["Famouri", "Mahmoud", ""], ["Wong", "Alexander", ""]]}, {"id": "2004.08028", "submitter": "Haidong Zhu", "authors": "Chuanzi He, Haidong Zhu, Jiyang Gao, Kan Chen, Ram Nevatia", "title": "CPARR: Category-based Proposal Analysis for Referring Relationships", "comments": "CVPR 2020 Workshop on Multimodal Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of referring relationships is to localize subject and object\nentities in an image satisfying a relationship query, which is given in the\nform of \\texttt{<subject, predicate, object>}. This requires simultaneous\nlocalization of the subject and object entities in a specified relationship. We\nintroduce a simple yet effective proposal-based method for referring\nrelationships. Different from the existing methods such as SSAS, our method can\ngenerate a high-resolution result while reducing its complexity and ambiguity.\nOur method is composed of two modules: a category-based proposal generation\nmodule to select the proposals related to the entities and a predicate analysis\nmodule to score the compatibility of pairs of selected proposals. We show\nstate-of-the-art performance on the referring relationship task on two public\ndatasets: Visual Relationship Detection and Visual Genome.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:54:01 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["He", "Chuanzi", ""], ["Zhu", "Haidong", ""], ["Gao", "Jiyang", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "2004.08029", "submitter": "Jaybie de Guzman", "authors": "Jaybie A. de Guzman, Kanchana Thilakarathna, Aruna Seneviratne", "title": "Conservative Plane Releasing for Spatial Privacy Protection in Mixed\n  Reality", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) or mixed reality (MR) platforms require spatial\nunderstanding to detect objects or surfaces, often including their structural\n(i.e. spatial geometry) and photometric (e.g. color, and texture) attributes,\nto allow applications to place virtual or synthetic objects seemingly\n\"anchored\" on to real world objects; in some cases, even allowing interactions\nbetween the physical and virtual objects. These functionalities require AR/MR\nplatforms to capture the 3D spatial information with high resolution and\nfrequency; however, these pose unprecedented risks to user privacy. Aside from\nobjects being detected, spatial information also reveals the location of the\nuser with high specificity, e.g. in which part of the house the user is. In\nthis work, we propose to leverage spatial generalizations coupled with\nconservative releasing to provide spatial privacy while maintaining data\nutility. We designed an adversary that builds up on existing place and shape\nrecognition methods over 3D data as attackers to which the proposed spatial\nprivacy approach can be evaluated against. Then, we simulate user movement\nwithin spaces which reveals more of their space as they move around utilizing\n3D point clouds collected from Microsoft HoloLens. Results show that revealing\nno more than 11 generalized planes--accumulated from successively revealed\nspaces with large enough radius, i.e. $r\\leq1.0m$--can make an adversary fail\nin identifying the spatial location of the user for at least half of the time.\nFurthermore, if the accumulated spaces are of smaller radius, i.e. each\nsuccessively revealed space is $r\\leq 0.5m$, we can release up to 29\ngeneralized planes while enjoying both better data utility and privacy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:57:58 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["de Guzman", "Jaybie A.", ""], ["Thilakarathna", "Kanchana", ""], ["Seneviratne", "Aruna", ""]]}, {"id": "2004.08030", "submitter": "Predrag Lazic", "authors": "Predrag Lazic", "title": "Smartphone camera based pointer", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large screen displays are omnipresent today as a part of infrastructure for\npresentations and entertainment. Also powerful smartphones with integrated\ncamera(s) are ubiquitous. However, there are not many ways in which smartphones\nand screens can interact besides casting the video from a smartphone. In this\npaper, we present a novel idea that turns a smartphone into a direct virtual\npointer on the screen using the phone's camera. The idea and its implementation\nare simple, robust, efficient and fun to use. Besides the mathematical concepts\nof the idea we accompany the paper with a small javascript project\n(www.mobiletvgames.com) which demonstrates the possibility of the new\ninteraction technique presented as a massive multiplayer game in the HTML5\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:59:23 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lazic", "Predrag", ""]]}, {"id": "2004.08051", "submitter": "Keuntaek Lee", "authors": "Keuntaek Lee, Bogdan Vlahov, Jason Gibson, James M. Rehg, Evangelos A.\n  Theodorou", "title": "Approximate Inverse Reinforcement Learning from Vision-based Imitation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a method for obtaining an implicit objective\nfunction for vision-based navigation. The proposed methodology relies on\nImitation Learning, Model Predictive Control (MPC), and an interpretation\ntechnique used in Deep Neural Networks. We use Imitation Learning as a means to\ndo Inverse Reinforcement Learning in order to create an approximate cost\nfunction generator for a visual navigation challenge. The resulting cost\nfunction, the costmap, is used in conjunction with MPC for real-time control\nand outperforms other state-of-the-art costmap generators in novel\nenvironments. The proposed process allows for simple training and robustness to\nout-of-sample data. We apply our method to the task of vision-based autonomous\ndriving in multiple real and simulated environments and show its\ngeneralizability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:36:50 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 03:37:41 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 19:52:37 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lee", "Keuntaek", ""], ["Vlahov", "Bogdan", ""], ["Gibson", "Jason", ""], ["Rehg", "James M.", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "2004.08052", "submitter": "Mohammad Rahimzadeh", "authors": "Mohammad Rahimzadeh, Abolfazl Attar", "title": "A modified deep convolutional neural network for detecting COVID-19 and\n  pneumonia from chest X-ray images based on the concatenation of Xception and\n  ResNet50V2", "comments": "This is a preprint of an article published in Informatics in Medicine\n  Unlocked journal. The final authenticated version is available online at\n  https://doi.org/10.1016/j.imu.2020.100360. The Code is available at\n  https://github.com/mr7495/covid19", "journal-ref": null, "doi": "10.1016/j.imu.2020.100360", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we have trained several deep convolutional networks with\nintroduced training techniques for classifying X-ray images into three classes:\nnormal, pneumonia, and COVID-19, based on two open-source datasets. Our data\ncontains 180 X-ray images that belong to persons infected with COVID-19, and we\nattempted to apply methods to achieve the best possible results. In this\nresearch, we introduce some training techniques that help the network learn\nbetter when we have an unbalanced dataset (fewer cases of COVID-19 along with\nmore cases from other classes). We also propose a neural network that is a\nconcatenation of the Xception and ResNet50V2 networks. This network achieved\nthe best accuracy by utilizing multiple features extracted by two robust\nnetworks. For evaluating our network, we have tested it on 11302 images to\nreport the actual accuracy achievable in real circumstances. The average\naccuracy of the proposed network for detecting COVID-19 cases is 99.50%, and\nthe overall average accuracy for all classes is 91.4%.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:38:39 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 01:14:11 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Rahimzadeh", "Mohammad", ""], ["Attar", "Abolfazl", ""]]}, {"id": "2004.08055", "submitter": "Zhiyuan Liang", "authors": "Tao Li, Zhiyuan Liang, Sanyuan Zhao, Jiahao Gong, Jianbing Shen", "title": "Self-Learning with Rectification Strategy for Human Parsing", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we solve the sample shortage problem in the human parsing\ntask. We begin with the self-learning strategy, which generates pseudo-labels\nfor unlabeled data to retrain the model. However, directly using noisy\npseudo-labels will cause error amplification and accumulation. Considering the\ntopology structure of human body, we propose a trainable graph reasoning method\nthat establishes internal structural connections between graph nodes to correct\ntwo typical errors in the pseudo-labels, i.e., the global structural error and\nthe local consistency error. For the global error, we first transform\ncategory-wise features into a high-level graph model with coarse-grained\nstructural information, and then decouple the high-level graph to reconstruct\nthe category features. The reconstructed features have a stronger ability to\nrepresent the topology structure of the human body. Enlarging the receptive\nfield of features can effectively reducing the local error. We first project\nfeature pixels into a local graph model to capture pixel-wise relations in a\nhierarchical graph manner, then reverse the relation information back to the\npixels. With the global structural and local consistency modules, these errors\nare rectified and confident pseudo-labels are generated for retraining.\nExtensive experiments on the LIP and the ATR datasets demonstrate the\neffectiveness of our global and local rectification modules. Our method\noutperforms other state-of-the-art methods in supervised human parsing tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 03:51:30 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Li", "Tao", ""], ["Liang", "Zhiyuan", ""], ["Zhao", "Sanyuan", ""], ["Gong", "Jiahao", ""], ["Shen", "Jianbing", ""]]}, {"id": "2004.08058", "submitter": "Yuexiang Li", "authors": "Jiawei Chen, Yuexiang Li, Kai Ma, Yefeng Zheng", "title": "Generative Adversarial Networks for Video-to-Video Domain Adaptation", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endoscopic videos from multicentres often have different imaging conditions,\ne.g., color and illumination, which make the models trained on one domain\nusually fail to generalize well to another. Domain adaptation is one of the\npotential solutions to address the problem. However, few of existing works\nfocused on the translation of video-based data. In this work, we propose a\nnovel generative adversarial network (GAN), namely VideoGAN, to transfer the\nvideo-based data across different domains. As the frames of a video may have\nsimilar content and imaging conditions, the proposed VideoGAN has an X-shape\ngenerator to preserve the intra-video consistency during translation.\nFurthermore, a loss function, namely color histogram loss, is proposed to tune\nthe color distribution of each translated frame. Two colonoscopic datasets from\ndifferent centres, i.e., CVC-Clinic and ETIS-Larib, are adopted to evaluate the\nperformance of domain adaptation of our VideoGAN. Experimental results\ndemonstrate that the adapted colonoscopic video generated by our VideoGAN can\nsignificantly boost the segmentation accuracy, i.e., an improvement of 5%, of\ncolorectal polyps on multicentre datasets. As our VideoGAN is a general network\narchitecture, we also evaluate its performance with the CamVid driving video\ndataset on the cloudy-to-sunny translation task. Comprehensive experiments show\nthat the domain gap could be substantially narrowed down by our VideoGAN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 04:16:37 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Chen", "Jiawei", ""], ["Li", "Yuexiang", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2004.08066", "submitter": "Toshihisa Tanaka", "authors": "Yuki Hagiwara and Toshihisa Tanaka", "title": "YuruGAN: Yuru-Chara Mascot Generator Using Generative Adversarial\n  Networks With Clustering Small Dataset", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A yuru-chara is a mascot character created by local governments and companies\nfor publicizing information on areas and products. Because it takes various\ncosts to create a yuruchara, the utilization of machine learning techniques\nsuch as generative adversarial networks (GANs) can be expected. In recent\nyears, it has been reported that the use of class conditions in a dataset for\nGANs training stabilizes learning and improves the quality of the generated\nimages. However, it is difficult to apply class conditional GANs when the\namount of original data is small and when a clear class is not given, such as a\nyuruchara image. In this paper, we propose a class conditional GAN based on\nclustering and data augmentation. Specifically, first, we performed clustering\nbased on K-means++ on the yuru-chara image dataset and converted it into a\nclass conditional dataset. Next, data augmentation was performed on the class\nconditional dataset so that the amount of data was increased five times. In\naddition, we built a model that incorporates ResBlock and self-attention into a\nnetwork based on class conditional GAN and trained the class conditional\nyuru-chara dataset. As a result of evaluating the generated images, the effect\non the generated images by the difference of the clustering method was\nconfirmed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:18:49 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hagiwara", "Yuki", ""], ["Tanaka", "Toshihisa", ""]]}, {"id": "2004.08067", "submitter": "Jaeyeon Jang", "authors": "Jaeyeon Jang and Chang Ouk Kim", "title": "One-vs-Rest Network-based Deep Probability Model for Open Set\n  Recognition", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unknown examples that are unseen during training often appear in real-world\ncomputer vision tasks, and an intelligent self-learning system should be able\nto differentiate between known and unknown examples. Open set recognition,\nwhich addresses this problem, has been studied for approximately a decade.\nHowever, conventional open set recognition methods based on deep neural\nnetworks (DNNs) lack a foundation for post recognition score analysis. In this\npaper, we propose a DNN structure in which multiple one-vs-rest sigmoid\nnetworks follow a convolutional neural network feature extractor. A one-vs-rest\nnetwork, which is composed of rectified linear unit activation functions for\nthe hidden layers and a single sigmoid target class output node, can maximize\nthe ability to learn information from nonmatch examples. Furthermore, the\nnetwork yields a sophisticated nonlinear features-to-output mapping that is\nexplainable in the feature space. By introducing extreme value theory-based\ncalibration techniques, the nonlinear and explainable mapping provides a\nwell-grounded class membership probability models. Our experiments show that\none-vs-rest networks can provide more informative hidden representations for\nunknown examples than the commonly used SoftMax layer. In addition, the\nproposed probability model outperformed the state-of-the art methods in open\nset classification scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:24:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 06:37:47 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Jang", "Jaeyeon", ""], ["Kim", "Chang Ouk", ""]]}, {"id": "2004.08070", "submitter": "Alasdair Tran", "authors": "Alasdair Tran, Alexander Mathews, Lexing Xie", "title": "Transform and Tell: Entity-Aware News Image Captioning", "comments": "Published in CVPR 2020. Code is available at\n  https://github.com/alasdairtran/transform-and-tell and demo is available at\n  https://transform-and-tell.ml", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 13035-13045", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end model which generates captions for images embedded\nin news articles. News images present two key challenges: they rely on\nreal-world knowledge, especially about named entities; and they typically have\nlinguistically rich captions that include uncommon words. We address the first\nchallenge by associating words in the caption with faces and objects in the\nimage, via a multi-modal, multi-head attention mechanism. We tackle the second\nchallenge with a state-of-the-art transformer language model that uses\nbyte-pair-encoding to generate captions as a sequence of word parts. On the\nGoodNews dataset, our model outperforms the previous state of the art by a\nfactor of four in CIDEr score (13 to 54). This performance gain comes from a\nunique combination of language models, word representation, image embeddings,\nface embeddings, object embeddings, and improvements in neural network design.\nWe also introduce the NYTimes800k dataset which is 70% larger than GoodNews,\nhas higher article quality, and includes the locations of images within\narticles as an additional contextual cue.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:44:37 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 01:21:14 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tran", "Alasdair", ""], ["Mathews", "Alexander", ""], ["Xie", "Lexing", ""]]}, {"id": "2004.08074", "submitter": "Abe Motoshi", "authors": "Motoshi Abe, Junichi Miyao, Takio Kurita", "title": "Adaptive Neuron-wise Discriminant Criterion and Adaptive Center Loss at\n  Hidden Layer for Deep Convolutional Neural Network", "comments": "Accepted to IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep convolutional neural network (CNN) has been widely used in image\nclassification and gives better classification accuracy than the other\ntechniques. The softmax cross-entropy loss function is often used for\nclassification tasks. There are some works to introduce the additional terms in\nthe objective function for training to make the features of the output layer\nmore discriminative. The neuron-wise discriminant criterion makes the input\nfeature of each neuron in the output layer discriminative by introducing the\ndiscriminant criterion to each of the features. Similarly, the center loss was\nintroduced to the features before the softmax activation function for face\nrecognition to make the deep features discriminative. The ReLU function is\noften used for the network as an active function in the hidden layers of the\nCNN. However, it is observed that the deep features trained by using the ReLU\nfunction are not discriminative enough and show elongated shapes. In this\npaper, we propose to use the neuron-wise discriminant criterion at the output\nlayer and the center-loss at the hidden layer. Also, we introduce the online\ncomputation of the means of each class with the exponential forgetting. We\nnamed them adaptive neuron-wise discriminant criterion and adaptive center\nloss, respectively. The effectiveness of the integration of the adaptive\nneuron-wise discriminant criterion and the adaptive center loss is shown by the\nexperiments with MNSIT, FashionMNIST, CIFAR10, CIFAR100, and STL10. Source code\nis at https://github.com/i13abe/Adaptive-discriminant-and-center\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:52:24 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Abe", "Motoshi", ""], ["Miyao", "Junichi", ""], ["Kurita", "Takio", ""]]}, {"id": "2004.08079", "submitter": "Ebin Zacharias", "authors": "Ebin Zacharias, Martin Teuchler and B\\'en\\'edicte Bernier", "title": "Image Processing Based Scene-Text Detection and Recognition with\n  Tesseract", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Recognition is one of the challenging tasks of computer vision with\nconsiderable practical interest. Optical character recognition (OCR) enables\ndifferent applications for automation. This project focuses on word detection\nand recognition in natural images. In comparison to reading text in scanned\ndocuments, the targeted problem is significantly more challenging. The use case\nin focus facilitates the possibility to detect the text area in natural scenes\nwith greater accuracy because of the availability of images under constraints.\nThis is achieved using a camera mounted on a truck capturing likewise images\nround-the-clock. The detected text area is then recognized using Tesseract OCR\nengine. Even though it benefits low computational power requirements, the model\nis limited to only specific use cases. This paper discusses a critical false\npositive case scenario occurred while testing and elaborates the strategy used\nto alleviate the problem. The project achieved a correct character recognition\nrate of more than 80\\%. This paper outlines the stages of development, the\nmajor challenges and some of the interesting findings of the project.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 06:58:35 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Zacharias", "Ebin", ""], ["Teuchler", "Martin", ""], ["Bernier", "B\u00e9n\u00e9dicte", ""]]}, {"id": "2004.08083", "submitter": "Arkabandhu Chowdhury", "authors": "Arkabandhu Chowdhury, Dipak Chaudhari, Swarat Chaudhuri, Chris\n  Jermaine", "title": "Meta-Meta Classification for One-Shot Learning", "comments": "10 pages without references, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach, called meta-meta classification, to learning in\nsmall-data settings. In this approach, one uses a large set of learning\nproblems to design an ensemble of learners, where each learner has high bias\nand low variance and is skilled at solving a specific type of learning problem.\nThe meta-meta classifier learns how to examine a given learning problem and\ncombine the various learners to solve the problem. The meta-meta learning\napproach is especially suited to solving few-shot learning tasks, as it is\neasier to learn to classify a new learning problem with little data than it is\nto apply a learning algorithm to a small data set. We evaluate the approach on\na one-shot, one-class-versus-all classification task and show that it is able\nto outperform traditional meta-learning as well as ensembling approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 07:05:03 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 16:46:20 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 02:28:08 GMT"}, {"version": "v4", "created": "Sun, 14 Jun 2020 01:02:11 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chowdhury", "Arkabandhu", ""], ["Chaudhari", "Dipak", ""], ["Chaudhuri", "Swarat", ""], ["Jermaine", "Chris", ""]]}, {"id": "2004.08096", "submitter": "Yanghua Jin", "authors": "Naofumi Akimoto, Huachun Zhu, Yanghua Jin, Yoshimitsu Aoki", "title": "Fast Soft Color Segmentation", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of soft color segmentation, defined as decomposing a\ngiven image into several RGBA layers, each containing only homogeneous color\nregions. The resulting layers from decomposition pave the way for applications\nthat benefit from layer-based editing, such as recoloring and compositing of\nimages and videos. The current state-of-the-art approach for this problem is\nhindered by slow processing time due to its iterative nature, and consequently\ndoes not scale to certain real-world scenarios. To address this issue, we\npropose a neural network based method for this task that decomposes a given\nimage into multiple layers in a single forward pass. Furthermore, our method\nseparately decomposes the color layers and the alpha channel layers. By\nleveraging a novel training objective, our method achieves proper assignment of\ncolors amongst layers. As a consequence, our method achieve promising quality\nwithout existing issue of inference speed for iterative approaches. Our\nthorough experimental analysis shows that our method produces qualitative and\nquantitative results comparable to previous methods while achieving a 300,000x\nspeed improvement. Finally, we utilize our proposed method on several\napplications, and demonstrate its speed advantage, especially in video editing.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 07:43:33 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Akimoto", "Naofumi", ""], ["Zhu", "Huachun", ""], ["Jin", "Yanghua", ""], ["Aoki", "Yoshimitsu", ""]]}, {"id": "2004.08107", "submitter": "Ruxin Wang", "authors": "Ruxin Wang, Shuyuan Chen, Chaojie Ji, Ye Li", "title": "Cascaded Context Enhancement Network for Automatic Skin Lesion\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion segmentation is an important step for automatic melanoma\ndiagnosis. Due to the non-negligible diversity of lesions from different\npatients, extracting powerful context for fine-grained semantic segmentation is\nstill challenging today. Although the deep convolutional neural network (CNNs)\nhave made significant improvements on skin lesion segmentation, they often fail\nto reserve the spatial details and long-range dependencies context due to\nconsecutive convolution striding and pooling operations inside CNNs. In this\npaper, we formulate a cascaded context enhancement neural network for automatic\nskin lesion segmentation. A new cascaded context aggregation (CCA) module with\na gate-based information integration approach is proposed to sequentially and\nselectively aggregate original image and multi-level features from the encoder\nsub-network. The generated context is further utilized to guide discriminative\nfeatures extraction by the designed context-guided local affinity (CGL) module.\nFurthermore, an auxiliary loss is added to the CCA module for refining the\nprediction. In our work, we evaluate our approach on four public skin\ndermoscopy image datasets. The proposed method achieves the Jaccard Index (JA)\nof 87.1%, 80.3%, 83.4%, and 86.6% on ISIC-2016, ISIC-2017, ISIC-2018, and PH2\ndatasets, which are higher than other state-of-the-art models respectively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:25:17 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 04:07:30 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 08:01:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Ruxin", ""], ["Chen", "Shuyuan", ""], ["Ji", "Chaojie", ""], ["Li", "Ye", ""]]}, {"id": "2004.08112", "submitter": "Li Rui", "authors": "Rui Li and Chenxi Duan", "title": "LiteDenseNet: A Lightweight Network for Hyperspectral Image\n  Classification", "comments": "The random split among training, test, and validation is not\n  acceptable in cube-based methods, which may lead to test data leakage", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral Image (HSI) classification based on deep learning has been an\nattractive area in recent years. However, as a kind of data-driven algorithm,\ndeep learning method usually requires numerous computational resources and\nhigh-quality labelled dataset, while the cost of high-performance computing and\ndata annotation is expensive. In this paper, to reduce dependence on massive\ncalculation and labelled samples, we propose a lightweight network architecture\n(LiteDenseNet) based on DenseNet for Hyperspectral Image Classification.\nInspired by GoogLeNet and PeleeNet, we design a 3D two-way dense layer to\ncapture the local and global features of the input. As convolution is a\ncomputationally intensive operation, we introduce group convolution to decrease\ncalculation cost and parameter size further. Thus, the number of parameters and\nthe consumptions of calculation are observably less than contrapositive deep\nlearning methods, which means LiteDenseNet owns simpler architecture and higher\nefficiency. A series of quantitative experiences on 6 widely used hyperspectral\ndatasets show that the proposed LiteDenseNet obtains the state-of-the-art\nperformance, even though when the absence of labelled samples is severe.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:38:52 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:15:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Li", "Rui", ""], ["Duan", "Chenxi", ""]]}, {"id": "2004.08116", "submitter": "Motoshi Abe", "authors": "Hideki Oki, Motoshi Abe, Junichi Miyao, Takio Kurita", "title": "Triplet Loss for Knowledge Distillation", "comments": "Accepted to IJCNN 2020, Source code is at\n  https://github.com/i13abe/Triplet-Loss-for-Knowledge-Distillation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has spread rapidly, and deeper, larger models\nhave been proposed. However, the calculation cost becomes enormous as the size\nof the models becomes larger. Various techniques for compressing the size of\nthe models have been proposed to improve performance while reducing\ncomputational costs. One of the methods to compress the size of the models is\nknowledge distillation (KD). Knowledge distillation is a technique for\ntransferring knowledge of deep or ensemble models with many parameters (teacher\nmodel) to smaller shallow models (student model). Since the purpose of\nknowledge distillation is to increase the similarity between the teacher model\nand the student model, we propose to introduce the concept of metric learning\ninto knowledge distillation to make the student model closer to the teacher\nmodel using pairs or triplets of the training samples. In metric learning, the\nresearchers are developing the methods to build a model that can increase the\nsimilarity of outputs for similar samples. Metric learning aims at reducing the\ndistance between similar and increasing the distance between dissimilar. The\nfunctionality of the metric learning to reduce the differences between similar\noutputs can be used for the knowledge distillation to reduce the differences\nbetween the outputs of the teacher model and the student model. Since the\noutputs of the teacher model for different objects are usually different, the\nstudent model needs to distinguish them. We think that metric learning can\nclarify the difference between the different outputs, and the performance of\nthe student model could be improved. We have performed experiments to compare\nthe proposed method with state-of-the-art knowledge distillation methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:48:29 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Oki", "Hideki", ""], ["Abe", "Motoshi", ""], ["Miyao", "Junichi", ""], ["Kurita", "Takio", ""]]}, {"id": "2004.08118", "submitter": "Ebin Zacharias", "authors": "Ebin Zacharias, Didier Stricker, Martin Teuchler and Kripasindhu\n  Sarkar", "title": "Object Detection and Recognition of Swap-Bodies using Camera mounted on\n  a Vehicle", "comments": "13 pages 9 figures 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and identification is a challenging area of computer vision\nand a fundamental requirement for autonomous cars. This project aims to jointly\nperform object detection of a swap-body and to find the type of swap-body by\nreading an ILU code using an efficient optical character recognition (OCR)\nmethod. Recent research activities have drastically improved deep learning\ntechniques which proves to enhance the field of computer vision. Collecting\nenough images for training the model is a critical step towards achieving good\nresults. The data for training were collected from different locations with\nmaximum possible variations and the details are explained. In addition, data\naugmentation methods applied for training has proved to be effective in\nimproving the performance of the trained model. Training the model achieved\ngood results and the test results are also provided. The final model was tested\nwith images and videos. Finally, this paper also draws attention to some of the\nmajor challenges faced during various stages of the project and the possible\nsolutions applied.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:49:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Zacharias", "Ebin", ""], ["Stricker", "Didier", ""], ["Teuchler", "Martin", ""], ["Sarkar", "Kripasindhu", ""]]}, {"id": "2004.08122", "submitter": "Mohamed Elmahdy", "authors": "Laurens Beljaards, Mohamed S. Elmahdy, Fons Verbeek, Marius Staring", "title": "A Cross-Stitch Architecture for Joint Registration and Segmentation in\n  Adaptive Radiotherapy", "comments": "Accepted to MIDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, joint registration and segmentation has been formulated in a deep\nlearning setting, by the definition of joint loss functions. In this work, we\ninvestigate joining these tasks at the architectural level. We propose a\nregistration network that integrates segmentation propagation between images,\nand a segmentation network to predict the segmentation directly. These networks\nare connected into a single joint architecture via so-called cross-stitch\nunits, allowing information to be exchanged between the tasks in a learnable\nmanner. The proposed method is evaluated in the context of adaptive\nimage-guided radiotherapy, using daily prostate CT imaging. Two datasets from\ndifferent institutes and manufacturers were involved in the study. The first\ndataset was used for training (12 patients) and validation (6 patients), while\nthe second dataset was used as an independent test set (14 patients). In terms\nof mean surface distance, our approach achieved $1.06 \\pm 0.3$ mm, $0.91 \\pm\n0.4$ mm, $1.27 \\pm 0.4$ mm, and $1.76 \\pm 0.8$ mm on the validation set and\n$1.82 \\pm 2.4$ mm, $2.45 \\pm 2.4$ mm, $2.45 \\pm 5.0$ mm, and $2.57 \\pm 2.3$ mm\non the test set for the prostate, bladder, seminal vesicles, and rectum,\nrespectively. The proposed multi-task network outperformed single-task\nnetworks, as well as a network only joined through the loss function, thus\ndemonstrating the capability to leverage the individual strengths of the\nsegmentation and registration tasks. The obtained performance as well as the\ninference speed make this a promising candidate for daily re-contouring in\nadaptive radiotherapy, potentially reducing treatment-related side effects and\nimproving quality-of-life after treatment.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:55:23 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Beljaards", "Laurens", ""], ["Elmahdy", "Mohamed S.", ""], ["Verbeek", "Fons", ""], ["Staring", "Marius", ""]]}, {"id": "2004.08141", "submitter": "Shuvozit Ghose", "authors": "Shuvozit Ghose, Pinaki Nath Chowdhury, Partha Pratim Roy, Umapada Pal", "title": "Modeling Extent-of-Texture Information for Ground Terrain Recognition", "comments": "Accepted in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground Terrain Recognition is a difficult task as the context information\nvaries significantly over the regions of a ground terrain image. In this paper,\nwe propose a novel approach towards ground-terrain recognition via modeling the\nExtent-of-Texture information to establish a balance between the order-less\ntexture component and ordered-spatial information locally. At first, the\nproposed method uses a CNN backbone feature extractor network to capture\nmeaningful information of a ground terrain image, and model the extent of\ntexture and shape information locally. Then, the order-less texture information\nand ordered shape information are encoded in a patch-wise manner, which is\nutilized by intra-domain message passing module to make every patch aware of\neach other for rich feature learning. Next, the Extent-of-Texture (EoT) Guided\nInter-domain Message Passing module combines the extent of texture and shape\ninformation with the encoded texture and shape information in a patch-wise\nfashion for sharing knowledge to balance out the order-less texture information\nwith ordered shape information. Further, Bilinear model generates a pairwise\ncorrelation between the order-less texture information and ordered shape\ninformation. Finally, the ground-terrain image classification is performed by a\nfully connected layer. The experimental results indicate superior performance\nof the proposed model over existing state-of-the-art techniques on publicly\navailable datasets like DTD, MINC and GTOS-mobile.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 09:39:35 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 10:03:12 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ghose", "Shuvozit", ""], ["Chowdhury", "Pinaki Nath", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "2004.08154", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li,\n  Cewu Lu", "title": "Detailed 2D-3D Joint Representation for Human-Object Interaction", "comments": "Accepted to CVPR 2020, supplementary materials included, code\n  available:https://github.com/DirtyHarryLYL/DJ-RN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection lies at the core of action\nunderstanding. Besides 2D information such as human/object appearance and\nlocations, 3D pose is also usually utilized in HOI learning since its\nview-independence. However, rough 3D body joints just carry sparse body\ninformation and are not sufficient to understand complex interactions. Thus, we\nneed detailed 3D body shape to go further. Meanwhile, the interacted object in\n3D is also not fully studied in HOI learning. In light of these, we propose a\ndetailed 2D-3D joint representation learning method. First, we utilize the\nsingle-view human body capture method to obtain detailed 3D body, face and hand\nshapes. Next, we estimate the 3D object location and size with reference to the\n2D human-object spatial configuration and object category priors. Finally, a\njoint learning framework and cross-modal consistency tasks are proposed to\nlearn the joint HOI representation. To better evaluate the 2D ambiguity\nprocessing capacity of models, we propose a new benchmark named Ambiguous-HOI\nconsisting of hard ambiguous images. Extensive experiments in large-scale HOI\nbenchmark and Ambiguous-HOI show impressive effectiveness of our method. Code\nand data are available at https://github.com/DirtyHarryLYL/DJ-RN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 10:22:12 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 04:51:52 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Li", "Yong-Lu", ""], ["Liu", "Xinpeng", ""], ["Lu", "Han", ""], ["Wang", "Shiyi", ""], ["Liu", "Junqi", ""], ["Li", "Jiefeng", ""], ["Lu", "Cewu", ""]]}, {"id": "2004.08189", "submitter": "Juana Valeria Hurtado", "authors": "Juana Valeria Hurtado, Rohit Mohan, Wolfram Burgard, Abhinav Valada", "title": "MOPT: Multi-Object Panoptic Tracking", "comments": "Code & models are available at\n  http://rl.uni-freiburg.de/research/panoptictracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive understanding of dynamic scenes is a critical prerequisite for\nintelligent robots to autonomously operate in their environment. Research in\nthis domain, which encompasses diverse perception problems, has primarily been\nfocused on addressing specific tasks individually rather than modeling the\nability to understand dynamic scenes holistically. In this paper, we introduce\na novel perception task denoted as multi-object panoptic tracking (MOPT), which\nunifies the conventionally disjoint tasks of semantic segmentation, instance\nsegmentation, and multi-object tracking. MOPT allows for exploiting pixel-level\nsemantic information of 'thing' and 'stuff' classes, temporal coherence, and\npixel-level associations over time, for the mutual benefit of each of the\nindividual sub-problems. To facilitate quantitative evaluations of MOPT in a\nunified manner, we propose the soft panoptic tracking quality (sPTQ) metric. As\na first step towards addressing this task, we propose the novel\nPanopticTrackNet architecture that builds upon the state-of-the-art top-down\npanoptic segmentation network EfficientPS by adding a new tracking head to\nsimultaneously learn all sub-tasks in an end-to-end manner. Additionally, we\npresent several strong baselines that combine predictions from state-of-the-art\npanoptic segmentation and multi-object tracking models for comparison. We\npresent extensive quantitative and qualitative evaluations of both vision-based\nand LiDAR-based MOPT that demonstrate encouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:45:28 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 14:57:01 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hurtado", "Juana Valeria", ""], ["Mohan", "Rohit", ""], ["Burgard", "Wolfram", ""], ["Valada", "Abhinav", ""]]}, {"id": "2004.08190", "submitter": "Weijian Li", "authors": "Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo,\n  Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, and Shun Miao", "title": "Structured Landmark Detection via Topology-Adapting Deep Graph Learning", "comments": "Accepted to ECCV-20. Camera-ready with supplementary material", "journal-ref": null, "doi": "10.1007/978-3-030-58545-7_16", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image landmark detection aims to automatically identify the locations of\npredefined fiducial points. Despite recent success in this field,\nhigher-ordered structural modeling to capture implicit or explicit\nrelationships among anatomical landmarks has not been adequately exploited. In\nthis work, we present a new topology-adapting deep graph learning approach for\naccurate anatomical facial and medical (e.g., hand, pelvis) landmark detection.\nThe proposed method constructs graph signals leveraging both local image\nfeatures and global shape features. The adaptive graph topology naturally\nexplores and lands on task-specific structures which are learned end-to-end\nwith two Graph Convolutional Networks (GCNs). Extensive experiments are\nconducted on three public facial image datasets (WFLW, 300W, and COFW-68) as\nwell as three real-world X-ray medical datasets (Cephalometric (public), Hand\nand Pelvis). Quantitative results comparing with the previous state-of-the-art\napproaches across all studied datasets indicating the superior performance in\nboth robustness and accuracy. Qualitative visualizations of the learned graph\ntopologies demonstrate a physically plausible connectivity laying behind the\nlandmarks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:55:03 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 18:16:18 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 19:32:01 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 12:36:05 GMT"}, {"version": "v5", "created": "Thu, 16 Jul 2020 15:29:51 GMT"}, {"version": "v6", "created": "Thu, 23 Jul 2020 17:00:51 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Weijian", ""], ["Lu", "Yuhang", ""], ["Zheng", "Kang", ""], ["Liao", "Haofu", ""], ["Lin", "Chihung", ""], ["Luo", "Jiebo", ""], ["Cheng", "Chi-Tung", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Kuo", "Chang-Fu", ""], ["Miao", "Shun", ""]]}, {"id": "2004.08195", "submitter": "Pablo Barros", "authors": "Pablo Barros, Nikhil Churamani, Alessandra Sciutti", "title": "The FaceChannel: A Light-weight Deep Neural Network for Facial\n  Expression Recognition", "comments": "Accepted at the Workshop on Affect Recognition in-the-wild:\n  Uni/Multi-Modal Analysis & VA-AU-Expression Challenges, FG2020", "journal-ref": null, "doi": "10.1109/FG47880.2020.00070", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current state-of-the-art models for automatic FER are based on very deep\nneural networks that are difficult to train. This makes it challenging to adapt\nthese models to changing conditions, a requirement from FER models given the\nsubjective nature of affect perception and understanding. In this paper, we\naddress this problem by formalizing the FaceChannel, a light-weight neural\nnetwork that has much fewer parameters than common deep neural networks. We\nperform a series of experiments on different benchmark datasets to demonstrate\nhow the FaceChannel achieves a comparable, if not better, performance, as\ncompared to the current state-of-the-art in FER.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 12:03:14 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Barros", "Pablo", ""], ["Churamani", "Nikhil", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "2004.08206", "submitter": "Friedrich Kruber", "authors": "Friedrich Kruber, Eduardo S\\'anchez Morales, Samarjit Chakraborty,\n  Michael Botsch", "title": "Vehicle Position Estimation with Aerial Imagery from Unmanned Aerial\n  Vehicles", "comments": "Copyright 20xx IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2020 IEEE Intelligent Vehicles Symposium (IV)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of real-world data is a key element for novel developments\nin the fields of automotive and traffic research. Aerial imagery has the major\nadvantage of recording multiple objects simultaneously and overcomes\nlimitations such as occlusions. However, there are only few data sets\navailable. This work describes a process to estimate a precise vehicle position\nfrom aerial imagery. A robust object detection is crucial for reliable results,\nhence the state-of-the-art deep neural network Mask-RCNN is applied for that\npurpose. Two training data sets are employed: The first one is optimized for\ndetecting the test vehicle, while the second one consists of randomly selected\nimages recorded on public roads. To reduce errors, several aspects are\naccounted for, such as the drone movement and the perspective projection from a\nphotograph. The estimated position is comapared with a reference system\ninstalled in the test vehicle. It is shown, that a mean accuracy of 20 cm can\nbe achieved with flight altitudes up to 100 m, Full-HD resolution and a\nframe-by-frame detection. A reliable position estimation is the basis for\nfurther data processing, such as obtaining additional vehicle state variables.\nThe source code, training weights, labeled data and example videos are made\npublicly available. This supports researchers to create new traffic data sets\nwith specific local conditions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 12:29:40 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 11:42:30 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Kruber", "Friedrich", ""], ["Morales", "Eduardo S\u00e1nchez", ""], ["Chakraborty", "Samarjit", ""], ["Botsch", "Michael", ""]]}, {"id": "2004.08222", "submitter": "Jianbo Liu", "authors": "Jianbo Liu, Junjun He, Jimmy S. Ren, Yu Qiao, Hongsheng Li", "title": "Learning to Predict Context-adaptive Convolution for Semantic\n  Segmentation", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-range contextual information is essential for achieving high-performance\nsemantic segmentation. Previous feature re-weighting methods demonstrate that\nusing global context for re-weighting feature channels can effectively improve\nthe accuracy of semantic segmentation. However, the globally-sharing feature\nre-weighting vector might not be optimal for regions of different classes in\nthe input image. In this paper, we propose a Context-adaptive Convolution\nNetwork (CaC-Net) to predict a spatially-varying feature weighting vector for\neach spatial location of the semantic feature maps. In CaC-Net, a set of\ncontext-adaptive convolution kernels are predicted from the global contextual\ninformation in a parameter-efficient manner. When used for convolution with the\nsemantic feature maps, the predicted convolutional kernels can generate the\nspatially-varying feature weighting factors capturing both global and local\ncontextual information. Comprehensive experimental results show that our\nCaC-Net achieves superior segmentation performance on three public datasets,\nPASCAL Context, PASCAL VOC 2012 and ADE20K.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 13:09:17 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 03:49:13 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Liu", "Jianbo", ""], ["He", "Junjun", ""], ["Ren", "Jimmy S.", ""], ["Qiao", "Yu", ""], ["Li", "Hongsheng", ""]]}, {"id": "2004.08227", "submitter": "Siddharth Tourani", "authors": "Siddharth Tourani, Alexander Shekhovtsov, Carsten Rother, Bogdan\n  Savchynskyy", "title": "MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical\n  Models", "comments": "Accepted in ECCV-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense, discrete Graphical Models with pairwise potentials are a powerful\nclass of models which are employed in state-of-the-art computer vision and\nbio-imaging applications. This work introduces a new MAP-solver, based on the\npopular Dual Block-Coordinate Ascent principle. Surprisingly, by making a small\nchange to the low-performing solver, the Max Product Linear Programming (MPLP)\nalgorithm, we derive the new solver MPLP++ that significantly outperforms all\nexisting solvers by a large margin, including the state-of-the-art solver\nTree-Reweighted Sequential (TRWS) message-passing algorithm. Additionally, our\nsolver is highly parallel, in contrast to TRWS, which gives a further boost in\nperformance with the proposed GPU and multi-thread CPU implementations. We\nverify the superiority of our algorithm on dense problems from publicly\navailable benchmarks, as well, as a new benchmark for 6D Object Pose\nestimation. We also provide an ablation study with respect to graph density.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:20:53 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tourani", "Siddharth", ""], ["Shekhovtsov", "Alexander", ""], ["Rother", "Carsten", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "2004.08246", "submitter": "Domenico Gatti", "authors": "Hassan Abdallah, Asiri Liyanaarachchi, Maranda Saigh, Samantha\n  Silvers, Suzan Arslanturk, Douglas J. Taatjes, Lars Larsson, Bhanu P. Jena,\n  Domenico L. Gatti", "title": "Res-CR-Net, a residual network with a novel architecture optimized for\n  the semantic segmentation of microscopy images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) have been widely used to carry out segmentation\ntasks in both electron and light microscopy. Most DNNs developed for this\npurpose are based on some variation of the encoder-decoder type U-Net\narchitecture, in combination with residual blocks to increase ease of training\nand resilience to gradient degradation. Here we introduce Res-CR-Net, a type of\nDNN that features residual blocks with either a bundle of separable atrous\nconvolutions with different dilation rates or a convolutional LSTM. The number\nof filters used in each residual block and the number of blocks are the only\nhyperparameters that need to be modified in order to optimize the network\ntraining for a variety of different microscopy images.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 21:21:01 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Abdallah", "Hassan", ""], ["Liyanaarachchi", "Asiri", ""], ["Saigh", "Maranda", ""], ["Silvers", "Samantha", ""], ["Arslanturk", "Suzan", ""], ["Taatjes", "Douglas J.", ""], ["Larsson", "Lars", ""], ["Jena", "Bhanu P.", ""], ["Gatti", "Domenico L.", ""]]}, {"id": "2004.08270", "submitter": "Matteo Bustreo", "authors": "Avik Hati, Matteo Bustreo, Diego Sona, Vittorio Murino, Alessio Del\n  Bue", "title": "Weakly Supervised Geodesic Segmentation of Egyptian Mummy CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the task of automatically analyzing 3D volumetric\nscans obtained from computed tomography (CT) devices. In particular, we address\na particular task for which data is very limited: the segmentation of ancient\nEgyptian mummies CT scans. We aim at digitally unwrapping the mummy and\nidentify different segments such as body, bandages and jewelry. The problem is\ncomplex because of the lack of annotated data for the different semantic\nregions to segment, thus discouraging the use of strongly supervised\napproaches. We, therefore, propose a weakly supervised and efficient\ninteractive segmentation method to solve this challenging problem. After\nsegmenting the wrapped mummy from its exterior region using histogram analysis\nand template matching, we first design a voxel distance measure to find an\napproximate solution for the body and bandage segments. Here, we use geodesic\ndistances since voxel features as well as spatial relationship among voxels is\nincorporated in this measure. Next, we refine the solution using a GrabCut\nbased segmentation together with a tracking method on the slices of the scan\nthat assigns labels to different regions in the volume, using limited\nsupervision in the form of scribbles drawn by the user. The efficiency of the\nproposed method is demonstrated using visualizations and validated through\nquantitative measures and qualitative unwrapping of the mummy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:35:00 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hati", "Avik", ""], ["Bustreo", "Matteo", ""], ["Sona", "Diego", ""], ["Murino", "Vittorio", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2004.08298", "submitter": "Antonio Tavera", "authors": "Emanuele Alberti, Antonio Tavera, Carlo Masone, Barbara Caputo", "title": "IDDA: a large-scale multi-domain dataset for autonomous driving", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is key in autonomous driving. Using deep visual\nlearning architectures is not trivial in this context, because of the\nchallenges in creating suitable large scale annotated datasets. This issue has\nbeen traditionally circumvented through the use of synthetic datasets, that\nhave become a popular resource in this field. They have been released with the\nneed to develop semantic segmentation algorithms able to close the visual\ndomain shift between the training and test data. Although exacerbated by the\nuse of artificial data, the problem is extremely relevant in this field even\nwhen training on real data. Indeed, weather conditions, viewpoint changes and\nvariations in the city appearances can vary considerably from car to car, and\neven at test time for a single, specific vehicle. How to deal with domain\nadaptation in semantic segmentation, and how to leverage effectively several\ndifferent data distributions (source domains) are important research questions\nin this field. To support work in this direction, this paper contributes a new\nlarge scale, synthetic dataset for semantic segmentation with more than 100\ndifferent source visual domains. The dataset has been created to explicitly\naddress the challenges of domain shift between training and test data in\nvarious weather and view point conditions, in seven different city types.\nExtensive benchmark experiments assess the dataset, showcasing open challenges\nfor the current state of the art. The dataset will be available at:\nhttps://idda-dataset.github.io/home/ .\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 15:22:38 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Alberti", "Emanuele", ""], ["Tavera", "Antonio", ""], ["Masone", "Carlo", ""], ["Caputo", "Barbara", ""]]}, {"id": "2004.08340", "submitter": "Vahid Moosavi", "authors": "Zifeng Guo, Joao P. Leitao, Nuno E. Simoes, and Vahid Moosavi", "title": "Data-driven Flood Emulation: Speeding up Urban Flood Predictions by Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational complexity has been the bottleneck of applying physically-based\nsimulations on large urban areas with high spatial resolution for efficient and\nsystematic flooding analyses and risk assessments. To address this issue of\nlong computational time, this paper proposes that the prediction of maximum\nwater depth rasters can be considered as an image-to-image translation problem\nwhere the results are generated from input elevation rasters using the\ninformation learned from data rather than by conducting simulations, which can\nsignificantly accelerate the prediction process. The proposed approach was\nimplemented by a deep convolutional neural network trained on flood simulation\ndata of 18 designed hyetographs on three selected catchments. Multiple tests\nwith both designed and real rainfall events were performed and the results show\nthat the flood predictions by neural network uses only 0.5 % of time comparing\nwith physically-based approaches, with promising accuracy and ability of\ngeneralizations. The proposed neural network can also potentially be applied to\ndifferent but relevant problems including flood predictions for urban layout\nplanning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:44:46 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 10:19:29 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Guo", "Zifeng", ""], ["Leitao", "Joao P.", ""], ["Simoes", "Nuno E.", ""], ["Moosavi", "Vahid", ""]]}, {"id": "2004.08345", "submitter": "Sergio Vitale", "authors": "Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio", "title": "Complexity Analysis of an Edge Preserving CNN SAR Despeckling Algorithm", "comments": "Accpeted to International Geoscience and Remote Sensing Symposium,\n  IGARSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAR images are affected by multiplicative noise that impairs their\ninterpretations. In the last decades several methods for SAR denoising have\nbeen proposed and in the last years great attention has moved towards deep\nlearning based solutions. Based on our last proposed convolutional neural\nnetwork for SAR despeckling, here we exploit the effect of the complexity of\nthe network. More precisely, once a dataset has been fixed, we carry out an\nanalysis of the network performance with respect to the number of layers and\nnumbers of features the network is composed of. Evaluation on simulated and\nreal data are carried out. The results show that deeper networks better\ngeneralize on both simulated and real images.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:02:01 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 17:58:48 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Vitale", "Sergio", ""], ["Ferraioli", "Giampaolo", ""], ["Pascazio", "Vito", ""]]}, {"id": "2004.08346", "submitter": "Theodore Tsesmelis", "authors": "Theodore Tsesmelis, Irtiza Hasan, Marco Cristani, Alessio Del Bue and\n  Fabio Galasso", "title": "An integrated light management system with real-time light measurement\n  and human perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illumination is important for well-being, productivity and safety across\nseveral environments, including offices, retail shops and industrial\nwarehouses. Current techniques for setting up lighting require extensive and\nexpert support and need to be repeated if the scene changes. Here we propose\nthe first fully-automated light management system (LMS) which measures lighting\nin real-time, leveraging an RGBD sensor and a radiosity-based light propagation\nmodel. Thanks to the integration of light distribution and perception curves\ninto the radiosity, we outperform a commercial software (Relux) on a newly\nintroduced dataset. Furthermore, our proposed LMS is the first to estimate both\nthe presence and the attention of the people in the environment, as well as\ntheir light perception. Our new LMS adapts therefore lighting to the scene and\nhuman activity and it is capable of saving up to 66%, as we experimentally\nquantify,without compromising the lighting quality.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:03:19 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tsesmelis", "Theodore", ""], ["Hasan", "Irtiza", ""], ["Cristani", "Marco", ""], ["Del Bue", "Alessio", ""], ["Galasso", "Fabio", ""]]}, {"id": "2004.08352", "submitter": "Vineet Mehta", "authors": "Vineet Mehta, Abhinav Dhall, Sujata Pal, Shehroz S. Khan", "title": "Motion and Region Aware Adversarial Learning for Fall Detection with\n  Thermal Imaging", "comments": "8 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic fall detection is a vital technology for ensuring the health and\nsafety of people. Home-based camera systems for fall detection often put\npeople's privacy at risk. Thermal cameras can partially or fully obfuscate\nfacial features, thus preserving the privacy of a person. Another challenge is\nthe less occurrence of falls in comparison to the normal activities of daily\nliving. As fall occurs rarely, it is non-trivial to learn algorithms due to\nclass imbalance. To handle these problems, we formulate fall detection as an\nanomaly detection within an adversarial framework using thermal imaging. We\npresent a novel adversarial network that comprises of two-channel 3D\nconvolutional autoencoders which reconstructs the thermal data and the optical\nflow input sequences respectively. We introduce a technique to track the region\nof interest, a region-based difference constraint, and a joint discriminator to\ncompute the reconstruction error. A larger reconstruction error indicates the\noccurrence of a fall. The experiments on a publicly available thermal fall\ndataset show the superior results obtained compared to the standard baseline.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:17:29 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 22:06:49 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mehta", "Vineet", ""], ["Dhall", "Abhinav", ""], ["Pal", "Sujata", ""], ["Khan", "Shehroz S.", ""]]}, {"id": "2004.08368", "submitter": "Cole Smith", "authors": "Cole Smith, Eric Lin, Dennis Shasha", "title": "Robotic Room Traversal using Optical Range Finding", "comments": "Technical Report TR2018-991", "journal-ref": null, "doi": null, "report-no": "TR2018-991", "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the goal of visiting every part of a room that is not blocked by\nobstacles. Doing so efficiently requires both sensors and planning. Our\nfindings suggest a method of inexpensive optical range finding for robotic room\ntraversal. Our room traversal algorithm relies upon the approximate distance\nfrom the robot to the nearest obstacle in 360 degrees. We then choose the path\nwith the furthest approximate distance. Since millimeter-precision is not\nrequired for our problem, we have opted to develop our own laser range finding\nsolution, in lieu of using more common, but also expensive solutions like light\ndetection and ranging (LIDAR). Rather, our solution uses a laser that casts a\nvisible dot on the target and a common camera (an iPhone, for example). Based\nupon where in the camera frame the laser dot is detected, we may calculate an\nangle between our target and the laser aperture. Using this angle and the known\ndistance between the camera eye and the laser aperture, we may solve all sides\nof a trigonometric model which provides the distance between the robot and the\ntarget.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:48:40 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Smith", "Cole", ""], ["Lin", "Eric", ""], ["Shasha", "Dennis", ""]]}, {"id": "2004.08379", "submitter": "Sivaramakrishnan Rajaraman", "authors": "Sivaramakrishnan Rajaraman, Jen Siegelman, Philip O. Alderson, Lucas\n  S. Folio, Les R. Folio and Sameer K. Antani", "title": "Iteratively Pruned Deep Learning Ensembles for COVID-19 Detection in\n  Chest X-rays", "comments": "11 pages, 8 figures, IEEE Access journal published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate use of iteratively pruned deep learning model ensembles for\ndetecting pulmonary manifestation of COVID-19 with chest X-rays. This disease\nis caused by the novel Severe Acute Respiratory Syndrome Coronavirus 2\n(SARS-CoV-2) virus, also known as the novel Coronavirus (2019-nCoV). A custom\nconvolutional neural network and a selection of ImageNet pretrained models are\ntrained and evaluated at patient-level on publicly available CXR collections to\nlearn modality-specific feature representations. The learned knowledge is\ntransferred and fine-tuned to improve performance and generalization in the\nrelated task of classifying CXRs as normal, showing bacterial pneumonia, or\nCOVID-19-viral abnormalities. The best performing models are iteratively pruned\nto reduce complexity and improve memory efficiency. The predictions of the\nbest-performing pruned models are combined through different ensemble\nstrategies to improve classification performance. Empirical evaluations\ndemonstrate that the weighted average of the best-performing pruned models\nsignificantly improves performance resulting in an accuracy of 99.01% and area\nunder the curve of 0.9972 in detecting COVID-19 findings on CXRs. The combined\nuse of modality-specific knowledge transfer, iterative model pruning, and\nensemble learning resulted in improved predictions. We expect that this model\ncan be quickly adopted for COVID-19 screening using chest radiographs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 00:09:29 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 15:18:01 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 15:05:31 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rajaraman", "Sivaramakrishnan", ""], ["Siegelman", "Jen", ""], ["Alderson", "Philip O.", ""], ["Folio", "Lucas S.", ""], ["Folio", "Les R.", ""], ["Antani", "Sameer K.", ""]]}, {"id": "2004.08385", "submitter": "Noa Garcia", "authors": "Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima", "title": "Knowledge-Based Visual Question Answering in Videos", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.10706", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel video understanding task by fusing knowledge-based and\nvideo question answering. First, we introduce KnowIT VQA, a video dataset with\n24,282 human-generated question-answer pairs about a popular sitcom. The\ndataset combines visual, textual and temporal coherence reasoning together with\nknowledge-based questions, which need of the experience obtained from the\nviewing of the series to be answered. Second, we propose a video understanding\nmodel by combining the visual and textual video content with specific knowledge\nabout the show. Our main findings are: (i) the incorporation of knowledge\nproduces outstanding improvements for VQA in video, and (ii) the performance on\nKnowIT VQA still lags well behind human accuracy, indicating its usefulness for\nstudying current video modelling limitations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 02:06:26 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Garcia", "Noa", ""], ["Otani", "Mayu", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""]]}, {"id": "2004.08388", "submitter": "Zitong Yu", "authors": "Zitong Yu, Yunxiao Qin, Xiaobai Li, Zezheng Wang, Chenxu Zhao, Zhen\n  Lei, Guoying Zhao", "title": "Multi-Modal Face Anti-Spoofing Based on Central Difference Networks", "comments": "1st place in \"Track Multi-Modal\" of ChaLearn Face Anti-spoofing\n  Attack Detection Challenge@CVPR2020; Accepted by CVPR2020 Media Forensics\n  Workshop. arXiv admin note: text overlap with arXiv:2003.04092", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) plays a vital role in securing face recognition\nsystems from presentation attacks. Existing multi-modal FAS methods rely on\nstacked vanilla convolutions, which is weak in describing detailed intrinsic\ninformation from modalities and easily being ineffective when the domain shifts\n(e.g., cross attack and cross ethnicity). In this paper, we extend the central\ndifference convolutional networks (CDCN) \\cite{yu2020searching} to a\nmulti-modal version, intending to capture intrinsic spoofing patterns among\nthree modalities (RGB, depth and infrared). Meanwhile, we also give an\nelaborate study about single-modal based CDCN. Our approach won the first place\nin \"Track Multi-Modal\" as well as the second place in \"Track Single-Modal\n(RGB)\" of ChaLearn Face Anti-spoofing Attack Detection Challenge@CVPR2020\n\\cite{liu2020cross}. Our final submission obtains 1.02$\\pm$0.59\\% and\n4.84$\\pm$1.79\\% ACER in \"Track Multi-Modal\" and \"Track Single-Modal (RGB)\",\nrespectively. The codes are available at{https://github.com/ZitongYu/CDCN}.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 11:42:23 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yu", "Zitong", ""], ["Qin", "Yunxiao", ""], ["Li", "Xiaobai", ""], ["Wang", "Zezheng", ""], ["Zhao", "Chenxu", ""], ["Lei", "Zhen", ""], ["Zhao", "Guoying", ""]]}, {"id": "2004.08423", "submitter": "Xin Chen", "authors": "Xin Chen, Lingxi Xie, Jun Wu, Longhui Wei, Yuhui Xu and Qi Tian", "title": "Fitting the Search Space of Weight-sharing NAS with Graph Convolutional\n  Networks", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search has attracted wide attentions in both academia and\nindustry. To accelerate it, researchers proposed weight-sharing methods which\nfirst train a super-network to reuse computation among different operators,\nfrom which exponentially many sub-networks can be sampled and efficiently\nevaluated. These methods enjoy great advantages in terms of computational\ncosts, but the sampled sub-networks are not guaranteed to be estimated\nprecisely unless an individual training process is taken. This paper owes such\ninaccuracy to the inevitable mismatch between assembled network layers, so that\nthere is a random error term added to each estimation. We alleviate this issue\nby training a graph convolutional network to fit the performance of sampled\nsub-networks so that the impact of random errors becomes minimal. With this\nstrategy, we achieve a higher rank correlation coefficient in the selected set\nof candidates, which consequently leads to better performance of the final\narchitecture. In addition, our approach also enjoys the flexibility of being\nused under different hardware constraints, since the graph convolutional\nnetwork has provided an efficient lookup table of the performance of\narchitectures in the entire search space.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:12:39 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 09:47:03 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Xin", ""], ["Xie", "Lingxi", ""], ["Wu", "Jun", ""], ["Wei", "Longhui", ""], ["Xu", "Yuhui", ""], ["Tian", "Qi", ""]]}, {"id": "2004.08426", "submitter": "Dazhou Guo", "authors": "Dazhou Guo, Dakai Jin, Zhuotun Zhu, Tsung-Ying Ho, Adam P. Harrison,\n  Chun-Hung Chao, Jing Xiao, Alan Yuille, Chien-Yu Lin, Le Lu", "title": "Organ at Risk Segmentation for Head and Neck Cancer using Stratified\n  Learning and Neural Architecture Search", "comments": null, "journal-ref": "IEEE CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OAR segmentation is a critical step in radiotherapy of head and neck (H&N)\ncancer, where inconsistencies across radiation oncologists and prohibitive\nlabor costs motivate automated approaches. However, leading methods using\nstandard fully convolutional network workflows that are challenged when the\nnumber of OARs becomes large, e.g. > 40. For such scenarios, insights can be\ngained from the stratification approaches seen in manual clinical OAR\ndelineation. This is the goal of our work, where we introduce stratified organ\nat risk segmentation (SOARS), an approach that stratifies OARs into anchor,\nmid-level, and small & hard (S&H) categories. SOARS stratifies across two\ndimensions. The first dimension is that distinct processing pipelines are used\nfor each OAR category. In particular, inspired by clinical practices, anchor\nOARs are used to guide the mid-level and S&H categories. The second dimension\nis that distinct network architectures are used to manage the significant\ncontrast, size, and anatomy variations between different OARs. We use\ndifferentiable neural architecture search (NAS), allowing the network to choose\namong 2D, 3D or Pseudo-3D convolutions. Extensive 4-fold cross-validation on\n142 H&N cancer patients with 42 manually labeled OARs, the most comprehensive\nOAR dataset to date, demonstrates that both pipeline- and NAS-stratification\nsignificantly improves quantitative performance over the state-of-the-art (from\n69.52% to 73.68% in absolute Dice scores). Thus, SOARS provides a powerful and\nprincipled means to manage the highly complex segmentation space of OARs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 19:15:48 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Guo", "Dazhou", ""], ["Jin", "Dakai", ""], ["Zhu", "Zhuotun", ""], ["Ho", "Tsung-Ying", ""], ["Harrison", "Adam P.", ""], ["Chao", "Chun-Hung", ""], ["Xiao", "Jing", ""], ["Yuille", "Alan", ""], ["Lin", "Chien-Yu", ""], ["Lu", "Le", ""]]}, {"id": "2004.08443", "submitter": "Andras Rozsa", "authors": "Andras Rozsa, Zheng Zhong, Terrance E. Boult", "title": "Adversarial Attack on Deep Learning-Based Splice Localization", "comments": "This is a pre-print of the original paper accepted at the CVPR\n  Workshop on Media Forensics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regarding image forensics, researchers have proposed various approaches to\ndetect and/or localize manipulations, such as splices. Recent best performing\nimage-forensics algorithms greatly benefit from the application of deep\nlearning, but such tools can be vulnerable to adversarial attacks. Due to the\nfact that most of the proposed adversarial example generation techniques can be\nused only on end-to-end classifiers, the adversarial robustness of\nimage-forensics methods that utilize deep learning only for feature extraction\nhas not been studied yet. Using a novel algorithm capable of directly adjusting\nthe underlying representations of patches we demonstrate on three non\nend-to-end deep learning-based splice localization tools that hiding\nmanipulations of images is feasible via adversarial attacks. While the tested\nimage-forensics methods, EXIF-SC, SpliceRadar, and Noiseprint, rely on feature\nextractors that were trained on different surrogate tasks, we find that the\nformed adversarial perturbations can be transferable among them regarding the\ndeterioration of their localization performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 20:31:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rozsa", "Andras", ""], ["Zhong", "Zheng", ""], ["Boult", "Terrance E.", ""]]}, {"id": "2004.08495", "submitter": "Behzad Hasani", "authors": "Behzad Hasani, Pooran Singh Negi, Mohammad H. Mahoor", "title": "BReG-NeXt: Facial Affect Computing Using Adaptive Residual Networks With\n  Bounded Gradient", "comments": "To appear in IEEE Transactions on Affective Computing journal", "journal-ref": "2020 IEEE Transactions on Affective Computing", "doi": "10.1109/TAFFC.2020.2986440", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces BReG-NeXt, a residual-based network architecture using\na function wtih bounded derivative instead of a simple shortcut path (a.k.a.\nidentity mapping) in the residual units for automatic recognition of facial\nexpressions based on the categorical and dimensional models of affect. Compared\nto ResNet, our proposed adaptive complex mapping results in a shallower network\nwith less numbers of training parameters and floating point operations per\nsecond (FLOPs). Adding trainable parameters to the bypass function further\nimproves fitting and training the network and hence recognizing subtle facial\nexpressions such as contempt with a higher accuracy. We conducted comprehensive\nexperiments on the categorical and dimensional models of affect on the\nchallenging in-the-wild databases of AffectNet, FER2013, and Affect-in-Wild.\nOur experimental results show that our adaptive complex mapping approach\noutperforms the original ResNet consisting of a simple identity mapping as well\nas other state-of-the-art methods for Facial Expression Recognition (FER).\nVarious metrics are reported in both affect models to provide a comprehensive\nevaluation of our method. In the categorical model, BReG-NeXt-50 with only 3.1M\ntraining parameters and 15 MFLOPs, achieves 68.50% and 71.53% accuracy on\nAffectNet and FER2013 databases, respectively. In the dimensional model,\nBReG-NeXt achieves 0.2577 and 0.2882 RMSE value on AffectNet and Affect-in-Wild\ndatabases, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 00:26:36 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Hasani", "Behzad", ""], ["Negi", "Pooran Singh", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "2004.08501", "submitter": "Peri Akiva", "authors": "Peri Akiva, Kristin Dana, Peter Oudemans, Michael Mars", "title": "Finding Berries: Segmentation and Counting of Cranberries using Point\n  Supervision and Shape Priors", "comments": "to be published in proceeding of CVPR 2020 in the Agriculture Vision\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision agriculture has become a key factor for increasing crop yields by\nproviding essential information to decision makers. In this work, we present a\ndeep learning method for simultaneous segmentation and counting of cranberries\nto aid in yield estimation and sun exposure predictions. Notably, supervision\nis done using low cost center point annotations. The approach, named Triple-S\nNetwork, incorporates a three-part loss with shape priors to promote better\nfitting to objects of known shape typical in agricultural scenes. Our results\nimprove overall segmentation performance by more than 6.74% and counting\nresults by 22.91% when compared to state-of-the-art. To train and evaluate the\nnetwork, we have collected the CRanberry Aerial Imagery Dataset (CRAID), the\nlargest dataset of aerial drone imagery from cranberry fields. This dataset\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 01:08:57 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 16:22:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Akiva", "Peri", ""], ["Dana", "Kristin", ""], ["Oudemans", "Peter", ""], ["Mars", "Michael", ""]]}, {"id": "2004.08513", "submitter": "Hamid Reza Vaezi Joze", "authors": "Hamid Reza Vaezi Joze, Ilya Zharkov, Karlton Powell, Carl Ringler,\n  Luming Liang, Andy Roulston, Moshe Lutz, Vivek Pradeep", "title": "ImagePairs: Realistic Super Resolution Dataset via Beam Splitter Camera\n  Rig", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  Workshop (CVPRW), 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super Resolution is the problem of recovering a high-resolution image from a\nsingle or multiple low-resolution images of the same scene. It is an ill-posed\nproblem since high frequency visual details of the scene are completely lost in\nlow-resolution images. To overcome this, many machine learning approaches have\nbeen proposed aiming at training a model to recover the lost details in the new\nscenes. Such approaches include the recent successful effort in utilizing deep\nlearning techniques to solve super resolution problem. As proven, data itself\nplays a significant role in the machine learning process especially deep\nlearning approaches which are data hungry. Therefore, to solve the problem, the\nprocess of gathering data and its formation could be equally as vital as the\nmachine learning technique used. Herein, we are proposing a new data\nacquisition technique for gathering real image data set which could be used as\nan input for super resolution, noise cancellation and quality enhancement\ntechniques. We use a beam-splitter to capture the same scene by a low\nresolution camera and a high resolution camera. Since we also release the raw\nimages, this large-scale dataset could be used for other tasks such as ISP\ngeneration. Unlike current small-scale dataset used for these tasks, our\nproposed dataset includes 11,421 pairs of low-resolution high-resolution images\nof diverse scenes. To our knowledge this is the most complete dataset for super\nresolution, ISP and image quality enhancement. The benchmarking result shows\nhow the new dataset can be successfully used to significantly improve the\nquality of real-world image super resolution.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 03:06:41 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Joze", "Hamid Reza Vaezi", ""], ["Zharkov", "Ilya", ""], ["Powell", "Karlton", ""], ["Ringler", "Carl", ""], ["Liang", "Luming", ""], ["Roulston", "Andy", ""], ["Lutz", "Moshe", ""], ["Pradeep", "Vivek", ""]]}, {"id": "2004.08514", "submitter": "Zhengyang Feng", "authors": "Zhengyang Feng, Qianyu Zhou, Qiqi Gu, Xin Tan, Guangliang Cheng,\n  Xuequan Lu, Jianping Shi, Lizhuang Ma", "title": "DMT: Dynamic Mutual Training for Semi-Supervised Learning", "comments": "Reformatted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent semi-supervised learning methods use pseudo supervision as core idea,\nespecially self-training methods that generate pseudo labels. However, pseudo\nlabels are unreliable. Self-training methods usually rely on single model\nprediction confidence to filter low-confidence pseudo labels, thus remaining\nhigh-confidence errors and wasting many low-confidence correct labels. In this\npaper, we point out it is difficult for a model to counter its own errors.\nInstead, leveraging inter-model disagreement between different models is a key\nto locate pseudo label errors. With this new viewpoint, we propose mutual\ntraining between two different models by a dynamically re-weighted loss\nfunction, called Dynamic Mutual Training (DMT). We quantify inter-model\ndisagreement by comparing predictions from two different models to dynamically\nre-weight loss in training, where a larger disagreement indicates a possible\nerror and corresponds to a lower loss value. Extensive experiments show that\nDMT achieves state-of-the-art performance in both image classification and\nsemantic segmentation. Our codes are released at\nhttps://github.com/voldemortX/DST-CBC .\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 03:12:55 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 03:23:27 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 02:04:24 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Feng", "Zhengyang", ""], ["Zhou", "Qianyu", ""], ["Gu", "Qiqi", ""], ["Tan", "Xin", ""], ["Cheng", "Guangliang", ""], ["Lu", "Xuequan", ""], ["Shi", "Jianping", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2004.08515", "submitter": "Keren Fu", "authors": "Keren Fu and Deng-Ping Fan and Ge-Peng Ji and Qijun Zhao", "title": "JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for\n  RGB-D Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel joint learning and densely-cooperative fusion\n(JL-DCF) architecture for RGB-D salient object detection. Existing models\nusually treat RGB and depth as independent information and design separate\nnetworks for feature extraction from each. Such schemes can easily be\nconstrained by a limited amount of training data or over-reliance on an\nelaborately-designed training process. In contrast, our JL-DCF learns from both\nRGB and depth inputs through a Siamese network. To this end, we propose two\neffective components: joint learning (JL), and densely-cooperative fusion\n(DCF). The JL module provides robust saliency feature learning, while the\nlatter is introduced for complementary feature discovery. Comprehensive\nexperiments on four popular metrics show that the designed framework yields a\nrobust RGB-D saliency detector with good generalization. As a result, JL-DCF\nsignificantly advances the top-1 D3Net model by an average of ~1.9% (S-measure)\nacross six challenging datasets, showing that the proposed framework offers a\npotential solution for real-world applications and could provide more insight\ninto the cross-modality complementarity task. The code will be available at\nhttps://github.com/kerenfu/JLDCF/.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 03:22:40 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Fu", "Keren", ""], ["Fan", "Deng-Ping", ""], ["Ji", "Ge-Peng", ""], ["Zhao", "Qijun", ""]]}, {"id": "2004.08522", "submitter": "Thanh Huy Nguyen", "authors": "Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes,\n  Jean-Marc Le Caillec", "title": "Super-Resolution-based Snake Model -- An Unsupervised Method for\n  Large-Scale Building Extraction using Airborne LiDAR Data and Optical Image", "comments": "30 pages, 15 figures. Submitted to the MDPI Remote Sensing", "journal-ref": "Remote Sensing 2020, 12(11), 1702", "doi": "10.3390/rs12111702", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of buildings in urban and residential scenes has become\na subject of growing interest in the domain of photogrammetry and remote\nsensing, particularly since mid-1990s. Active contour model, colloquially known\nas snake model, has been studied to extract buildings from aerial and satellite\nimagery. However, this task is still very challenging due to the complexity of\nbuilding size, shape, and its surrounding environment. This complexity leads to\na major obstacle for carrying out a reliable large-scale building extraction,\nsince the involved prior information and assumptions on building such as shape,\nsize, and color cannot be generalized over large areas. This paper presents an\nefficient snake model to overcome such challenge, called Super-Resolution-based\nSnake Model (SRSM). The SRSM operates on high-resolution LiDAR-based elevation\nimages -- called z-images -- generated by a super-resolution process applied to\nLiDAR data. The involved balloon force model is also improved to shrink or\ninflate adaptively, instead of inflating the snake continuously. This method is\napplicable for a large scale such as city scale and even larger, while having a\nhigh level of automation and not requiring any prior knowledge nor training\ndata from the urban scenes (hence unsupervised). It achieves high overall\naccuracy when tested on various datasets. For instance, the proposed SRSM\nyields an average area-based Quality of 86.57% and object-based Quality of\n81.60% on the ISPRS Vaihingen benchmark datasets. Compared to other methods\nusing this benchmark dataset, this level of accuracy is highly desirable even\nfor a supervised method. Similarly desirable outcomes are obtained when\ncarrying out the proposed SRSM on the whole City of Quebec (total area of 656\nkm2), yielding an area-based Quality of 62.37% and an object-based Quality of\n63.21%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:00:07 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Nguyen", "Thanh Huy", ""], ["Daniel", "Sylvie", ""], ["Gueriot", "Didier", ""], ["Sintes", "Christophe", ""], ["Caillec", "Jean-Marc Le", ""]]}, {"id": "2004.08526", "submitter": "Seiichi Uchida", "authors": "Masaya Ikoma, Brian Kenji Iwana, Seiichi Uchida", "title": "Effect of Text Color on Word Embeddings", "comments": "to appear at the 14th International Workshop on Document Analysis\n  Systems (DAS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural scenes and documents, we can find the correlation between a text\nand its color. For instance, the word, \"hot\", is often printed in red, while\n\"cold\" is often in blue. This correlation can be thought of as a feature that\nrepresents the semantic difference between the words. Based on this\nobservation, we propose the idea of using text color for word embeddings. While\ntext-only word embeddings (e.g. word2vec) have been extremely successful, they\noften represent antonyms as similar since they are often interchangeable in\nsentences. In this paper, we try two tasks to verify the usefulness of text\ncolor in understanding the meanings of words, especially in identifying\nsynonyms and antonyms. First, we quantify the color distribution of words from\nthe book cover images and analyze the correlation between the color and meaning\nof the word. Second, we try to retrain word embeddings with the color\ndistribution of words as a constraint. By observing the changes in the word\nembeddings of synonyms and antonyms before and after re-training, we aim to\nunderstand the kind of words that have positive or negative effects in their\nword embeddings when incorporating text color information.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 05:14:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ikoma", "Masaya", ""], ["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2004.08541", "submitter": "Mohamed Gani Parisa Beham", "authors": "D.Sabari Nathan and M.Parisa Beham and S. M. Md Mansoor Roomi", "title": "Moire Image Restoration using Multi Level Hyper Vision Net", "comments": "8 pages , 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A moire pattern in the images is resulting from high frequency patterns\ncaptured by the image sensor (colour filter array) that appear after\ndemosaicing. These Moire patterns would appear in natural images of scenes with\nhigh frequency content. The Moire pattern can also vary intensely due to a\nminimal change in the camera direction/positioning. Thus the Moire pattern\ndepreciates the quality of photographs. An important issue in demoireing\npattern is that the Moireing patterns have dynamic structure with varying\ncolors and forms. These challenges makes the demoireing more difficult than\nmany other image restoration tasks. Inspired by these challenges in demoireing,\na multilevel hyper vision net is proposed to remove the Moire pattern to\nimprove the quality of the images. As a key aspect, in this network we involved\nresidual channel attention block that can be used to extract and adaptively\nfuse hierarchical features from all the layers efficiently. The proposed\nalgorithms has been tested with the NTIRE 2020 challenge dataset and thus\nachieved 36.85 and 0.98 Peak to Signal Noise Ratio (PSNR) and Structural\nSimilarity (SSIM) Index respectively.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 06:54:54 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Nathan", "D. Sabari", ""], ["Beham", "M. Parisa", ""], ["Roomi", "S. M. Md Mansoor", ""]]}, {"id": "2004.08546", "submitter": "Chaoyang He", "authors": "Chaoyang He, Murali Annavaram, Salman Avestimehr", "title": "Towards Non-I.I.D. and Invisible Data with FedNAS: Federated Deep\n  Learning via Neural Architecture Search", "comments": "accepted to CVPR 2020 workshop on neural architecture search and\n  beyond for representation learning. Code is released at https://fedml.ai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has been proved to be an effective learning framework\nwhen data cannot be centralized due to privacy, communication costs, and\nregulatory restrictions. When training deep learning models under an FL\nsetting, people employ the predefined model architecture discovered in the\ncentralized environment. However, this predefined architecture may not be the\noptimal choice because it may not fit data with non-identical and independent\ndistribution (non-IID). Thus, we advocate automating federated learning\n(AutoFL) to improve model accuracy and reduce the manual design effort. We\nspecifically study AutoFL via Neural Architecture Search (NAS), which can\nautomate the design process. We propose a Federated NAS (FedNAS) algorithm to\nhelp scattered workers collaboratively searching for a better architecture with\nhigher accuracy. We also build a system based on FedNAS. Our experiments on\nnon-IID dataset show that the architecture searched by FedNAS can outperform\nthe manually predefined architecture.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:04:44 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 23:59:20 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 18:47:25 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 02:18:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["He", "Chaoyang", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2004.08547", "submitter": "Narayana Reddy A", "authors": "Narayana Reddy A, Ranjita Das", "title": "Color Image Segmentation using Adaptive Particle Swarm Optimization and\n  Fuzzy C-means", "comments": "4 pages, 2 figures, Included in conference proceedings of\n  \"International conference in Recent Trends on Electronics & Computer Science\n  (ICRTECS-2019)\" organised by National Institute of Technology, Silchar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation partitions an image into different regions containing pixels\nwith similar attributes. A standard non-contextual variant of Fuzzy C-means\nclustering algorithm (FCM), considering its simplicity is generally used in\nimage segmentation. Using FCM has its disadvantages like it is dependent on the\ninitial guess of the number of clusters and highly sensitive to noise.\nSatisfactory visual segments cannot be obtained using FCM. Particle Swarm\nOptimization (PSO) belongs to the class of evolutionary algorithms and has good\nconvergence speed and fewer parameters compared to Genetic Algorithms (GAs). An\noptimized version of PSO can be combined with FCM to act as a proper\ninitializer for the algorithm thereby reducing its sensitivity to initial\nguess. A hybrid PSO algorithm named Adaptive Particle Swarm Optimization (APSO)\nwhich improves in the calculation of various hyper parameters like inertia\nweight, learning factors over standard PSO, using insights from swarm\nbehaviour, leading to improvement in cluster quality can be used. This paper\npresents a new image segmentation algorithm called Adaptive Particle Swarm\nOptimization and Fuzzy C-means Clustering Algorithm (APSOF), which is based on\nAdaptive Particle Swarm Optimization (APSO) and Fuzzy C-means clustering.\nExperimental results show that APSOF algorithm has edge over FCM in correctly\nidentifying the optimum cluster centers, there by leading to accurate\nclassification of the image pixels. Hence, APSOF algorithm has superior\nperformance in comparison with classic Particle Swarm Optimization (PSO) and\nFuzzy C-means clustering algorithm (FCM) for image segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:11:33 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["A", "Narayana Reddy", ""], ["Das", "Ranjita", ""]]}, {"id": "2004.08552", "submitter": "Xulei Yang", "authors": "Gabriel Tjio, Xulei Yang, Jia Mei Hong, Sum Thai Wong, Vanessa Ding,\n  Andre Choo and Yi Su", "title": "Accurate Tumor Tissue Region Detection with Accelerated Deep\n  Convolutional Neural Networks", "comments": "9 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual annotation of pathology slides for cancer diagnosis is laborious and\nrepetitive. Therefore, much effort has been devoted to develop computer vision\nsolutions. Our approach, (FLASH), is based on a Deep Convolutional Neural\nNetwork (DCNN) architecture. It reduces computational costs and is faster than\ntypical deep learning approaches by two orders of magnitude, making high\nthroughput processing a possibility. In computer vision approaches using deep\nlearning methods, the input image is subdivided into patches which are\nseparately passed through the neural network. Features extracted from these\npatches are used by the classifier to annotate the corresponding region. Our\napproach aggregates all the extracted features into a single matrix before\npassing them to the classifier. Previously, the features are extracted from\noverlapping patches. Aggregating the features eliminates the need for\nprocessing overlapping patches, which reduces the computations required. DCCN\nand FLASH demonstrate high sensitivity (~ 0.96), good precision (~0.78) and\nhigh F1 scores (~0.84). The average time taken to process each sample for FLASH\nand DCNN is 96.6 seconds and 9489.20 seconds, respectively. Our approach was\napproximately 100 times faster than the original DCNN approach while\nsimultaneously preserving high accuracy and precision.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:24:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Tjio", "Gabriel", ""], ["Yang", "Xulei", ""], ["Hong", "Jia Mei", ""], ["Wong", "Sum Thai", ""], ["Ding", "Vanessa", ""], ["Choo", "Andre", ""], ["Su", "Yi", ""]]}, {"id": "2004.08554", "submitter": "Ruoteng Li", "authors": "Ruoteng Li, Xiaoyi Zhang, Shaodi You and Yu Li", "title": "Learning to Dehaze from Realistic Scene with A Fast Physics-based\n  Dehazing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dehazing is a popular computer vision topic for long. A real-time dehazing\nmethod with reliable performance is highly desired for many applications such\nas autonomous driving. While recent learning-based methods require datasets\ncontaining pairs of hazy images and clean ground truth references, it is\ngenerally impossible to capture accurate ground truth in real scenes. Many\nexisting works compromise this difficulty to generate hazy images by rendering\nthe haze from depth on common RGBD datasets using the haze imaging model.\nHowever, there is still a gap between the synthetic datasets and real hazy\nimages as large datasets with high-quality depth are mostly indoor and depth\nmaps for outdoor are imprecise. In this paper, we complement the existing\ndatasets with a new, large, and diverse dehazing dataset containing real\noutdoor scenes from High-Definition (HD) 3D movies. We select a large number of\nhigh-quality frames of real outdoor scenes and render haze on them using depth\nfrom stereo. Our dataset is more realistic than existing ones and we\ndemonstrate that using this dataset greatly improves the dehazing performance\non real scenes. In addition to the dataset, we also propose a light and\nreliable dehazing network inspired by the physics model. Our approach\noutperforms other methods by a large margin and becomes the new\nstate-of-the-art method. Moreover, the light-weight design of the network\nenables our method to run at a real-time speed, which is much faster than other\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:25:25 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 03:58:11 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Li", "Ruoteng", ""], ["Zhang", "Xiaoyi", ""], ["You", "Shaodi", ""], ["Li", "Yu", ""]]}, {"id": "2004.08566", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Fabio Tosi, Konstantinos Batsos, Philippos Mordohai,\n  Stefano Mattoccia", "title": "On the Synergies between Machine Learning and Binocular Stereo for Depth\n  Estimation from Images: a Survey", "comments": "Accepted to TPAMI. Paper version of our CVPR 2019 tutorial:\n  \"Learning-based depth estimation from stereo and monocular images: successes,\n  limitations and future challenges\"\n  (https://sites.google.com/view/cvpr-2019-depth-from-image/home)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is one of the longest-standing problems in computer vision\nwith close to 40 years of studies and research. Throughout the years the\nparadigm has shifted from local, pixel-level decision to various forms of\ndiscrete and continuous optimization to data-driven, learning-based methods.\nRecently, the rise of machine learning and the rapid proliferation of deep\nlearning enhanced stereo matching with new exciting trends and applications\nunthinkable until a few years ago. Interestingly, the relationship between\nthese two worlds is two-way. While machine, and especially deep, learning\nadvanced the state-of-the-art in stereo matching, stereo itself enabled new\nground-breaking methodologies such as self-supervised monocular depth\nestimation based on deep networks. In this paper, we review recent research in\nthe field of learning-based depth estimation from single and binocular images\nhighlighting the synergies, the successes achieved so far and the open\nchallenges the community is going to face in the immediate future.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 09:14:08 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 14:00:23 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Poggi", "Matteo", ""], ["Tosi", "Fabio", ""], ["Batsos", "Konstantinos", ""], ["Mordohai", "Philippos", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2004.08572", "submitter": "Viraj Kulkarni", "authors": "Sudeep Kondal, Viraj Kulkarni, Ashrika Gaikwad, Amit Kharat, Aniruddha\n  Pant", "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale\n  from Radiographs Using Convolutional Neural Networks", "comments": "5 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The severity of knee osteoarthritis is graded using the 5-point\nKellgren-Lawrence (KL) scale where healthy knees are assigned grade 0, and the\nsubsequent grades 1-4 represent increasing severity of the affliction. Although\nseveral methods have been proposed in recent years to develop models that can\nautomatically predict the KL grade from a given radiograph, most models have\nbeen developed and evaluated on datasets not sourced from India. These models\nfail to perform well on the radiographs of Indian patients. In this paper, we\npropose a novel method using convolutional neural networks to automatically\ngrade knee radiographs on the KL scale. Our method works in two connected\nstages: in the first stage, an object detection model segments individual knees\nfrom the rest of the image; in the second stage, a regression model\nautomatically grades each knee separately on the KL scale. We train our model\nusing the publicly available Osteoarthritis Initiative (OAI) dataset and\ndemonstrate that fine-tuning the model before evaluating it on a dataset from a\nprivate hospital significantly improves the mean absolute error from 1.09 (95%\nCI: 1.03-1.15) to 0.28 (95% CI: 0.25-0.32). Additionally, we compare\nclassification and regression models built for the same task and demonstrate\nthat regression outperforms classification.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 09:46:55 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Kondal", "Sudeep", ""], ["Kulkarni", "Viraj", ""], ["Gaikwad", "Ashrika", ""], ["Kharat", "Amit", ""], ["Pant", "Aniruddha", ""]]}, {"id": "2004.08582", "submitter": "Haoran Li", "authors": "Haoran Li, Yaran Chen, Qichao Zhang and Dongbin Zhao", "title": "BiFNet: Bidirectional Fusion Network for Road Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sensor fusion-based road segmentation plays an important role in the\nintelligent driving system since it provides a drivable area. The existing\nmainstream fusion method is mainly to feature fusion in the image space domain\nwhich causes the perspective compression of the road and damages the\nperformance of the distant road. Considering the bird's eye views(BEV) of the\nLiDAR remains the space structure in horizontal plane, this paper proposes a\nbidirectional fusion network(BiFNet) to fuse the image and BEV of the point\ncloud. The network consists of two modules: 1) Dense space transformation\nmodule, which solves the mutual conversion between camera image space and BEV\nspace. 2) Context-based feature fusion module, which fuses the different\nsensors information based on the scenes from corresponding features.This method\nhas achieved competitive results on KITTI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 10:24:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Haoran", ""], ["Chen", "Yaran", ""], ["Zhang", "Qichao", ""], ["Zhao", "Dongbin", ""]]}, {"id": "2004.08595", "submitter": "Jiang-Jiang Liu", "authors": "Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng", "title": "Dynamic Feature Integration for Simultaneous Detection of Salient\n  Object, Edge and Skeleton", "comments": null, "journal-ref": "IEEE TIP 2020", "doi": "10.1109/TIP.2020.3017352", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we solve three low-level pixel-wise vision problems, including\nsalient object segmentation, edge detection, and skeleton extraction, within a\nunified framework. We first show some similarities shared by these tasks and\nthen demonstrate how they can be leveraged for developing a unified framework\nthat can be trained end-to-end. In particular, we introduce a selective\nintegration module that allows each task to dynamically choose features at\ndifferent levels from the shared backbone based on its own characteristics.\nFurthermore, we design a task-adaptive attention module, aiming at\nintelligently allocating information for different tasks according to the image\ncontent priors. To evaluate the performance of our proposed network on these\ntasks, we conduct exhaustive experiments on multiple representative datasets.\nWe will show that though these tasks are naturally quite different, our network\ncan work well on all of them and even perform better than current\nsingle-purpose state-of-the-art methods. In addition, we also conduct adequate\nablation analyses that provide a full understanding of the design principles of\nthe proposed framework. To facilitate future research, source code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 11:10:11 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Liu", "Jiang-Jiang", ""], ["Hou", "Qibin", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2004.08596", "submitter": "Haifeng Li", "authors": "Li Chen, Zewei Xu, Yongjian Fu, Haozhe Huang, Shaowen Wang, Haifeng Li", "title": "DAPnet: A double self-attention convolutional network for segmentation\n  of point clouds", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR point cloud has a complex structure and the 3D semantic labeling of it\nis a challenging task. Existing methods adopt data transformations without\nfully exploring contextual features, which are less efficient and accurate\nproblem. In this study, we propose a double self-attention convolutional\nnetwork, called DAPnet, by combining geometric and contextual features to\ngenerate better segmentation results. The double self-attention module\nincluding point attention module and group attention module originates from the\nself-attention mechanism to extract contextual features of terrestrial objects\nwith various shapes and scales. The contextual features extracted by these\nmodules represent the long-range dependencies between the data and are\nbeneficial to reducing the scale diversity of point cloud objects. The point\nattention module selectively enhances the features by modeling the\ninterdependencies of neighboring points. Meanwhile, the group attention module\nis used to emphasizes interdependent groups of points. We evaluate our method\nbased on the ISPRS 3D Semantic Labeling Contest dataset and find that our model\noutperforms the benchmark by 85.2% with an overall accuracy of 90.7%. The\nimprovements over powerline and car are 7.5% and 13%. By conducting ablation\ncomparison, we find that the point attention module is more effective for the\noverall improvement of the model than the group attention module, and the\nincorporation of the double self-attention module has an average of 7%\nimprovement on the pre-class accuracy of the classes. Moreover, the adoption of\nthe double self-attention module consumes a similar training time as the one\nwithout the attention module for model convergence. The experimental result\nshows the effectiveness and efficiency of the DAPnet for the segmentation of\nLiDAR point clouds. The source codes are available at\nhttps://github.com/RayleighChen/point-attention.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 11:14:28 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chen", "Li", ""], ["Xu", "Zewei", ""], ["Fu", "Yongjian", ""], ["Huang", "Haozhe", ""], ["Wang", "Shaowen", ""], ["Li", "Haifeng", ""]]}, {"id": "2004.08606", "submitter": "Alina Belko Vadimovna", "authors": "Alina Belko, Konstantin Dobratulin and Andrey Kuznetsov", "title": "Feathers dataset for Fine-Grained Visual Categorization", "comments": "6 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a novel dataset FeatherV1, containing 28,272 images of\nfeathers categorized by 595 bird species. It was created to perform taxonomic\nidentification of bird species by a single feather, which can be applied in\namateur and professional ornithology. FeatherV1 is the first publicly available\nbird's plumage dataset for machine learning, and it can raise interest for a\nnew task in fine-grained visual recognition domain. The latest version of the\ndataset can be downloaded at\nhttps://github.com/feathers-dataset/feathersv1-dataset. We also present\nfeathers classification task results. We selected several deep learning\narchitectures (DenseNet based) for categorical crossentropy values comparison\non the provided dataset.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 12:40:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Belko", "Alina", ""], ["Dobratulin", "Konstantin", ""], ["Kuznetsov", "Andrey", ""]]}, {"id": "2004.08609", "submitter": "Yang Xi", "authors": "Hui Li, Xi Yang, ZhenMing Li, TianLun Zhang", "title": "Underwater image enhancement with Image Colorfulness Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the absorption and scattering effects of the water, underwater images\ntend to suffer from many severe problems, such as low contrast, grayed out\ncolors and blurring content. To improve the visual quality of underwater\nimages, we proposed a novel enhancement model, which is a trainable end-to-end\nneural model. Two parts constitute the overall model. The first one is a\nnon-parameter layer for the preliminary color correction, then the second part\nis consisted of parametric layers for a self-adaptive refinement, namely the\nchannel-wise linear shift. For better details, contrast and colorfulness, this\nenhancement network is jointly optimized by the pixel-level and\ncharacteristiclevel training criteria. Through extensive experiments on natural\nunderwater scenes, we show that the proposed method can get high quality\nenhancement results.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 12:44:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Hui", ""], ["Yang", "Xi", ""], ["Li", "ZhenMing", ""], ["Zhang", "TianLun", ""]]}, {"id": "2004.08614", "submitter": "Tejas Gokhale", "authors": "Kuldeep Kulkarni, Tejas Gokhale, Rajhans Singh, Pavan Turaga, Aswin\n  Sankaranarayanan", "title": "Halluci-Net: Scene Completion by Exploiting Object Co-occurrence\n  Relationships", "comments": "Accepted to AI for Content Creation Workshop @CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been substantial progress in image synthesis from\nsemantic labelmaps. However, methods used for this task assume the availability\nof complete and unambiguous labelmaps, with instance boundaries of objects, and\nclass labels for each pixel. This reliance on heavily annotated inputs\nrestricts the application of image synthesis techniques to real-world\napplications, especially under uncertainty due to weather, occlusion, or noise.\nOn the other hand, algorithms that can synthesize images from sparse labelmaps\nor sketches are highly desirable as tools that can guide content creators and\nartists to quickly generate scenes by simply specifying locations of a few\nobjects. In this paper, we address the problem of complex scene completion from\nsparse labelmaps. Under this setting, very few details about the scene (30\\% of\nobject instances) are available as input for image synthesis. We propose a\ntwo-stage deep network based method, called `Halluci-Net', that learns\nco-occurence relationships between objects in scenes, and then exploits these\nrelationships to produce a dense and complete labelmap. The generated dense\nlabelmap can then be used as input by state-of-the-art image synthesis\ntechniques like pix2pixHD to obtain the final image. The proposed method is\nevaluated on the Cityscapes dataset and it outperforms two baselines methods on\nperformance metrics like Fr\\'echet Inception Distance (FID), semantic\nsegmentation accuracy, and similarity in object co-occurrences. We also show\nqualitative results on a subset of ADE20K dataset that contains bedroom images.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 13:12:59 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:04:53 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kulkarni", "Kuldeep", ""], ["Gokhale", "Tejas", ""], ["Singh", "Rajhans", ""], ["Turaga", "Pavan", ""], ["Sankaranarayanan", "Aswin", ""]]}, {"id": "2004.08628", "submitter": "B.S. Vivek", "authors": "Vivek B.S. and R. Venkatesh Babu", "title": "Single-step Adversarial training with Dropout Scheduling", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have shown impressive performance across a spectrum of\ncomputer vision applications including medical diagnosis and autonomous\ndriving. One of the major concerns that these models face is their\nsusceptibility to adversarial attacks. Realizing the importance of this issue,\nmore researchers are working towards developing robust models that are less\naffected by adversarial attacks. Adversarial training method shows promising\nresults in this direction. In adversarial training regime, models are trained\nwith mini-batches augmented with adversarial samples. Fast and simple methods\n(e.g., single-step gradient ascent) are used for generating adversarial\nsamples, in order to reduce computational complexity. It is shown that models\ntrained using single-step adversarial training method (adversarial samples are\ngenerated using non-iterative method) are pseudo robust. Further, this pseudo\nrobustness of models is attributed to the gradient masking effect. However,\nexisting works fail to explain when and why gradient masking effect occurs\nduring single-step adversarial training. In this work, (i) we show that models\ntrained using single-step adversarial training method learn to prevent the\ngeneration of single-step adversaries, and this is due to over-fitting of the\nmodel during the initial stages of training, and (ii) to mitigate this effect,\nwe propose a single-step adversarial training method with dropout scheduling.\nUnlike models trained using existing single-step adversarial training methods,\nmodels trained using the proposed single-step adversarial training method are\nrobust against both single-step and multi-step adversarial attacks, and the\nperformance is on par with models trained using computationally expensive\nmulti-step adversarial training methods, in white-box and black-box settings.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 14:14:00 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["S.", "Vivek B.", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2004.08638", "submitter": "Hafez Farazi", "authors": "Hafez Farazi and Sven Behnke", "title": "Motion Segmentation using Frequency Domain Transformer Networks", "comments": "28th European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Bruges, Belgium, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised prediction is a powerful mechanism to learn representations\nthat capture the underlying structure of the data. Despite recent progress, the\nself-supervised video prediction task is still challenging. One of the critical\nfactors that make the task hard is motion segmentation, which is segmenting\nindividual objects and the background and estimating their motion separately.\nIn video prediction, the shape, appearance, and transformation of each object\nshould be understood only by predicting the next frame in pixel space. To\naddress this task, we propose a novel end-to-end learnable architecture that\npredicts the next frame by modeling foreground and background separately while\nsimultaneously estimating and predicting the foreground motion using Frequency\nDomain Transformer Networks. Experimental evaluations show that this yields\ninterpretable representations and that our approach can outperform some widely\nused video prediction methods like Video Ladder Network and Predictive Gated\nPyramids on synthetic data.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 15:05:11 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "2004.08644", "submitter": "Spyridon Thermos", "authors": "Spyridon Thermos, Petros Daras, Gerasimos Potamianos", "title": "A Deep Learning Approach to Object Affordance Segmentation", "comments": "5 pages, 4 figures, ICASSP 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9054167", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to understand and infer object functionalities is an important step\ntowards robust visual intelligence. Significant research efforts have recently\nfocused on segmenting the object parts that enable specific types of\nhuman-object interaction, the so-called \"object affordances\". However, most\nworks treat it as a static semantic segmentation problem, focusing solely on\nobject appearance and relying on strong supervision and object detection. In\nthis paper, we propose a novel approach that exploits the spatio-temporal\nnature of human-object interaction for affordance segmentation. In particular,\nwe design an autoencoder that is trained using ground-truth labels of only the\nlast frame of the sequence, and is able to infer pixel-wise affordance labels\nin both videos and static images. Our model surpasses the need for object\nlabels and bounding boxes by using a soft-attention mechanism that enables the\nimplicit localization of the interaction hotspot. For evaluation purposes, we\nintroduce the SOR3D-AFF corpus, which consists of human-object interaction\nsequences and supports 9 types of affordances in terms of pixel-wise\nannotation, covering typical manipulations of tool-like objects. We show that\nour model achieves competitive results compared to strongly supervised methods\non SOR3D-AFF, while being able to predict affordances for similar unseen\nobjects in two affordance image-only datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 15:34:41 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Thermos", "Spyridon", ""], ["Daras", "Petros", ""], ["Potamianos", "Gerasimos", ""]]}, {"id": "2004.08656", "submitter": "Renshuai Tao", "authors": "Yanlu Wei, Renshuai Tao, Zhangjie Wu, Yuqing Ma, Libo Zhang, Xianglong\n  Liu", "title": "Occluded Prohibited Items Detection: an X-ray Security Inspection\n  Benchmark and De-occlusion Attention Module", "comments": "9 pages, 7 figures, accepted by ACM Multimedia 2020, data and code\n  are available at https://github.com/OPIXray-author/OPIXray", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security inspection often deals with a piece of baggage or suitcase where\nobjects are heavily overlapped with each other, resulting in an unsatisfactory\nperformance for prohibited items detection in X-ray images. In the literature,\nthere have been rare studies and datasets touching this important topic. In\nthis work, we contribute the first high-quality object detection dataset for\nsecurity inspection, named Occluded Prohibited Items X-ray (OPIXray) image\nbenchmark. OPIXray focused on the widely-occurred prohibited item \"cutter\",\nannotated manually by professional inspectors from the international airport.\nThe test set is further divided into three occlusion levels to better\nunderstand the performance of detectors. Furthermore, to deal with the\nocclusion in X-ray images detection, we propose the De-occlusion Attention\nModule (DOAM), a plug-and-play module that can be easily inserted into and thus\npromote most popular detectors. Despite the heavy occlusion in X-ray imaging,\nshape appearance of objects can be preserved well, and meanwhile different\nmaterials visually appear with different colors and textures. Motivated by\nthese observations, our DOAM simultaneously leverages the different appearance\ninformation of the prohibited item to generate the attention map, which helps\nrefine feature maps for the general detectors. We comprehensively evaluate our\nmodule on the OPIXray dataset, and demonstrate that our module can consistently\nimprove the performance of the state-of-the-art detection methods such as SSD,\nFCOS, etc, and significantly outperforms several widely-used attention\nmechanisms. In particular, the advantages of DOAM are more significant in the\nscenarios with higher levels of occlusion, which demonstrates its potential\napplication in real-world inspections. The OPIXray benchmark and our model are\nreleased at https://github.com/OPIXray-author/OPIXray.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 16:10:55 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 14:30:46 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 06:50:50 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 13:41:24 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wei", "Yanlu", ""], ["Tao", "Renshuai", ""], ["Wu", "Zhangjie", ""], ["Ma", "Yuqing", ""], ["Zhang", "Libo", ""], ["Liu", "Xianglong", ""]]}, {"id": "2004.08665", "submitter": "Clint Sebastian", "authors": "Clint Sebastian, Raffaele Imbriaco, Egor Bondarev, Peter H.N. de With", "title": "Dual Embedding Expansion for Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification plays a crucial role in the management of\ntransportation infrastructure and traffic flow. However, this is a challenging\ntask due to the large view-point variations in appearance, environmental and\ninstance-related factors. Modern systems deploy CNNs to produce unique\nrepresentations from the images of each vehicle instance. Most work focuses on\nleveraging new losses and network architectures to improve the descriptiveness\nof these representations. In contrast, our work concentrates on re-ranking and\nembedding expansion techniques. We propose an efficient approach for combining\nthe outputs of multiple models at various scales while exploiting tracklet and\nneighbor information, called dual embedding expansion (DEx). Additionally, a\ncomparative study of several common image retrieval techniques is presented in\nthe context of vehicle re-ID. Our system yields competitive performance in the\n2020 NVIDIA AI City Challenge with promising results. We demonstrate that DEx\nwhen combined with other re-ranking techniques, can produce an even larger gain\nwithout any additional attribute labels or manual supervision.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 17:14:07 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sebastian", "Clint", ""], ["Imbriaco", "Raffaele", ""], ["Bondarev", "Egor", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2004.08686", "submitter": "Zejiang Shen", "authors": "Zejiang Shen, Kaixuan Zhang, Melissa Dell", "title": "A Large Dataset of Historical Japanese Documents with Complex Layouts", "comments": "8 pages, 8 figures, accepted at CVPR2020 Workshop on Text and\n  Documents in the Deep Learning Era", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based approaches for automatic document layout analysis and\ncontent extraction have the potential to unlock rich information trapped in\nhistorical documents on a large scale. One major hurdle is the lack of large\ndatasets for training robust models. In particular, little training data exist\nfor Asian languages. To this end, we present HJDataset, a Large Dataset of\nHistorical Japanese Documents with Complex Layouts. It contains over 250,000\nlayout element annotations of seven types. In addition to bounding boxes and\nmasks of the content regions, it also includes the hierarchical structures and\nreading orders for layout elements. The dataset is constructed using a\ncombination of human and machine efforts. A semi-rule based method is developed\nto extract the layout elements, and the results are checked by human\ninspectors. The resulting large-scale dataset is used to provide baseline\nperformance analyses for text region detection using state-of-the-art deep\nlearning models. And we demonstrate the usefulness of the dataset on real-world\ndocument digitization tasks. The dataset is available at\nhttps://dell-research-harvard.github.io/HJDataset/.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 18:38:25 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Shen", "Zejiang", ""], ["Zhang", "Kaixuan", ""], ["Dell", "Melissa", ""]]}, {"id": "2004.08690", "submitter": "Hamed Sadeghi", "authors": "Hamed Sadeghi, Shahram Shirani, David W. Capson", "title": "A fast semi-automatic method for classification and counting the number\n  and types of blood cells in an image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel and fast semi-automatic method for segmentation, locating and\ncounting blood cells in an image is proposed. In this method, thresholding is\nused to separate the nucleus from the other parts. We also use Hough transform\nfor circles to locate the center of white cells. Locating and counting of red\ncells is performed using template matching. We make use of finding local\nmaxima, labeling and mean value computation in order to shrink the areas\nobtained after applying Hough transform or template matching, to a single pixel\nas representative of location of each region. The proposed method is very fast\nand computes the number and location of white cells accurately. It is also\ncapable of locating and counting the red cells with a small error.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 19:23:56 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sadeghi", "Hamed", ""], ["Shirani", "Shahram", ""], ["Capson", "David W.", ""]]}, {"id": "2004.08692", "submitter": "Emre Aksan", "authors": "Emre Aksan, Peng Cao, Manuel Kaufmann, Otmar Hilliges", "title": "A Spatio-temporal Transformer for 3D Human Motion Prediction", "comments": "New baselines in the main result table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel Transformer-based architecture for the task\nof generative modelling of 3D human motion. Previous works commonly rely on\nRNN-based models considering shorter forecast horizons reaching a stationary\nand often implausible state quickly. Instead, our focus lies on the generation\nof plausible future developments over longer time horizons. To mitigate the\nissue of convergence to a static pose, we propose a novel architecture that\nleverages the recently proposed self-attention concept. The task of 3D motion\nprediction is inherently spatio-temporal and thus the proposed model learns\nhigh dimensional embeddings for skeletal joints followed by a decoupled\ntemporal and spatial self-attention mechanism. This allows the model to access\npast information directly and to capture spatio-temporal dependencies\nexplicitly. We show empirically that this reduces error accumulation over time\nand allows for the generation of perceptually plausible motion sequences over\nlong time horizons up to 20 seconds as well as accurate short-term predictions.\nAccompanying video available at https://youtu.be/yF0cdt2yCNE.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 19:49:28 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 16:42:55 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Aksan", "Emre", ""], ["Cao", "Peng", ""], ["Kaufmann", "Manuel", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2004.08708", "submitter": "Shakti Kumar", "authors": "Jerrod Parker, Shakti Kumar, Joe Roussy", "title": "Adaptive Attention Span in Computer Vision", "comments": "6 pages with 1 Algorithm, 4 figures, 1 Table and 1 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in Transformers for language modeling have opened new\nareas of research in computer vision. Results from late 2019 showed vast\nperformance increases in both object detection and recognition when\nconvolutions are replaced by local self-attention kernels. Models using local\nself-attention kernels were also shown to have less parameters and FLOPS\ncompared to equivalent architectures that only use convolutions. In this work\nwe propose a novel method for learning the local self-attention kernel size. We\nthen compare its performance to fixed-size local attention and convolution\nkernels. The code for all our experiments and models is available at\nhttps://github.com/JoeRoussy/adaptive-attention-in-cv\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 21:32:47 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Parker", "Jerrod", ""], ["Kumar", "Shakti", ""], ["Roussy", "Joe", ""]]}, {"id": "2004.08740", "submitter": "Yong Wang", "authors": "Yong Wang, Qi Liu, Hongyu Zu, Xiao Liu, Ruichao Xie, Feng Wang", "title": "An end-to-end CNN framework for polarimetric vision tasks based on\n  polarization-parameter-constructing network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-wise operations between polarimetric images are important for\nprocessing polarization information. For the lack of such operations, the\npolarization information cannot be fully utilized in convolutional neural\nnetwork(CNN). In this paper, a novel end-to-end CNN framework for polarization\nvision tasks is proposed, which enables the networks to take full advantage of\npolarimetric images. The framework consists of two sub-networks: a\npolarization-parameter-constructing network (PPCN) and a task network. PPCN\nimplements pixel-wise operations between images in the CNN form with 1x1\nconvolution kernels. It takes raw polarimetric images as input, and outputs\npolarization-parametric images to task network so as to complete a vison task.\nBy training together, the PPCN can learn to provide the most suitable\npolarization-parametric images for the task network and the dataset. Taking\nfaster R-CNN as task network, the experimental results show that compared with\nexisting methods, the proposed framework achieves much higher\nmean-average-precision (mAP) in object detection task\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 01:33:10 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wang", "Yong", ""], ["Liu", "Qi", ""], ["Zu", "Hongyu", ""], ["Liu", "Xiao", ""], ["Xie", "Ruichao", ""], ["Wang", "Feng", ""]]}, {"id": "2004.08744", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Vedanuj Goswami, Devi Parikh", "title": "Are we pretraining it right? Digging deeper into visio-linguistic\n  pretraining", "comments": "23 pages, 6 figures. First two authors contributed equally. More info\n  at https://github.com/facebookresearch/pythia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous recent works have proposed pretraining generic visio-linguistic\nrepresentations and then finetuning them for downstream vision and language\ntasks. While architecture and objective function design choices have received\nattention, the choice of pretraining datasets has received little attention. In\nthis work, we question some of the default choices made in literature. For\ninstance, we systematically study how varying similarity between the\npretraining dataset domain (textual and visual) and the downstream domain\naffects performance. Surprisingly, we show that automatically generated data in\na domain closer to the downstream task (e.g., VQA v2) is a better choice for\npretraining than \"natural\" data but of a slightly different domain (e.g.,\nConceptual Captions). On the other hand, some seemingly reasonable choices of\npretraining datasets were found to be entirely ineffective for some downstream\ntasks. This suggests that despite the numerous recent efforts, vision &\nlanguage pretraining does not quite work \"out of the box\" yet. Overall, as a\nby-product of our study, we find that simple design choices in pretraining can\nhelp us achieve close to state-of-art results on downstream tasks without any\narchitectural changes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 01:55:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Singh", "Amanpreet", ""], ["Goswami", "Vedanuj", ""], ["Parikh", "Devi", ""]]}, {"id": "2004.08745", "submitter": "Jonah Philion", "authors": "Jonah Philion, Amlan Kar, Sanja Fidler", "title": "Learning to Evaluate Perception Models Using Planner-Centric Metrics", "comments": "CVPR 2020 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variants of accuracy and precision are the gold-standard by which the\ncomputer vision community measures progress of perception algorithms. One\nreason for the ubiquity of these metrics is that they are largely\ntask-agnostic; we in general seek to detect zero false negatives or positives.\nThe downside of these metrics is that, at worst, they penalize all incorrect\ndetections equally without conditioning on the task or scene, and at best,\nheuristics need to be chosen to ensure that different mistakes count\ndifferently. In this paper, we propose a principled metric for 3D object\ndetection specifically for the task of self-driving. The core idea behind our\nmetric is to isolate the task of object detection and measure the impact the\nproduced detections would induce on the downstream task of driving. Without\nhand-designing it to, we find that our metric penalizes many of the mistakes\nthat other metrics penalize by design. In addition, our metric downweighs\ndetections based on additional factors such as distance from a detection to the\nego car and the speed of the detection in intuitive ways that other detection\nmetrics do not. For human evaluation, we generate scenes in which standard\nmetrics and our metric disagree and find that humans side with our metric 79%\nof the time. Our project page including an evaluation server can be found at\nhttps://nv-tlabs.github.io/detection-relevance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 02:14:00 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Philion", "Jonah", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2004.08747", "submitter": "Xiaozhen Xie", "authors": "Haijin Zeng, Xiaozhen Xie, Jifeng Ning", "title": "Tensor completion using enhanced multiple modes low-rank prior and total\n  variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel model to recover a low-rank tensor by\nsimultaneously performing double nuclear norm regularized low-rank matrix\nfactorizations to the all-mode matricizations of the underlying tensor. An\nblock successive upper-bound minimization algorithm is applied to solve the\nmodel. Subsequence convergence of our algorithm can be established, and our\nalgorithm converges to the coordinate-wise minimizers in some mild conditions.\nSeveral experiments on three types of public data sets show that our algorithm\ncan recover a variety of low-rank tensors from significantly fewer samples than\nthe other testing tensor completion methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 02:23:06 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 12:35:18 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 00:57:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Zeng", "Haijin", ""], ["Xie", "Xiaozhen", ""], ["Ning", "Jifeng", ""]]}, {"id": "2004.08761", "submitter": "Hao Li", "authors": "Hao Li, Aozhou Wu, Wen Fang, Qingqing Zhang, Mingqing Liu, Qingwen\n  Liu, Wei Chen", "title": "Lightweight Mask R-CNN for Long-Range Wireless Power Transfer Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resonant Beam Charging (RBC) is a wireless charging technology which supports\nmulti-watt power transfer over meter-level distance. The features of safety,\nmobility and simultaneous charging capability enable RBC to charge multiple\nmobile devices safely at the same time. To detect the devices that need to be\ncharged, a Mask R-CNN based dection model is proposed in previous work.\nHowever, considering the constraints of the RBC system, it's not easy to apply\nMask R-CNN in lightweight hardware-embedded devices because of its heavy model\nand huge computation. Thus, we propose a machine learning detection approach\nwhich provides a lighter and faster model based on traditional Mask R-CNN. The\nproposed approach makes the object detection much easier to be transplanted on\nmobile devices and reduce the burden of hardware computation. By adjusting the\nstructure of the backbone and the head part of Mask R-CNN, we reduce the\naverage detection time from $1.02\\mbox{s}$ per image to $0.6132\\mbox{s}$, and\nreduce the model size from $245\\mbox{MB}$ to $47.1\\mbox{MB}$. The improved\nmodel is much more suitable for the application in the RBC system.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 03:50:15 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Hao", ""], ["Wu", "Aozhou", ""], ["Fang", "Wen", ""], ["Zhang", "Qingqing", ""], ["Liu", "Mingqing", ""], ["Liu", "Qingwen", ""], ["Chen", "Wei", ""]]}, {"id": "2004.08769", "submitter": "Subhankar Roy", "authors": "Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe, Elisa\n  Ricci", "title": "TriGAN: Image-to-Image Translation for Multi-Source Domain Adaptation", "comments": null, "journal-ref": "Machine Vision and Applications 2021", "doi": "10.1007/s00138-020-01164-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most domain adaptation methods consider the problem of transferring knowledge\nto the target domain from a single source dataset. However, in practical\napplications, we typically have access to multiple sources. In this paper we\npropose the first approach for Multi-Source Domain Adaptation (MSDA) based on\nGenerative Adversarial Networks. Our method is inspired by the observation that\nthe appearance of a given image depends on three factors: the domain, the style\n(characterized in terms of low-level features variations) and the content. For\nthis reason we propose to project the image features onto a space where only\nthe dependence from the content is kept, and then re-project this invariant\nrepresentation onto the pixel space using the target domain and style. In this\nway, new labeled images can be generated which are used to train a final target\nclassifier. We test our approach using common MSDA benchmarks, showing that it\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 05:07:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Roy", "Subhankar", ""], ["Siarohin", "Aliaksandr", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "2004.08787", "submitter": "Yunpeng Zhai", "authors": "Yunpeng Zhai (1), Shijian Lu (2), Qixiang Ye (3,5), Xuebo Shan (1),\n  Jie Chen (1,5), Rongrong Ji (4,5) and Yonghong Tian (1,5) ((1) Peking\n  University, (2) Nanyang Technological University, (3) University of Chinese\n  Academy of Sciences, (4) Xiamen University, (5) Peng Cheng Laboratory)", "title": "AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive\n  Person Re-identification", "comments": "Accepted by CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive person re-identification (re-ID) is a challenging task,\nespecially when person identities in target domains are unknown. Existing\nmethods attempt to address this challenge by transferring image styles or\naligning feature distributions across domains, whereas the rich unlabeled\nsamples in target domains are not sufficiently exploited. This paper presents a\nnovel augmented discriminative clustering (AD-Cluster) technique that estimates\nand augments person clusters in target domains and enforces the discrimination\nability of re-ID models with the augmented clusters. AD-Cluster is trained by\niterative density-based clustering, adaptive sample augmentation, and\ndiscriminative feature learning. It learns an image generator and a feature\nencoder which aim to maximize the intra-cluster diversity in the sample space\nand minimize the intra-cluster distance in the feature space in an adversarial\nmin-max manner. Finally, AD-Cluster increases the diversity of sample clusters\nand improves the discrimination capability of re-ID models greatly. Extensive\nexperiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms\nthe state-of-the-art with large margins.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 07:34:37 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 09:50:43 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zhai", "Yunpeng", ""], ["Lu", "Shijian", ""], ["Ye", "Qixiang", ""], ["Shan", "Xuebo", ""], ["Chen", "Jie", ""], ["Ji", "Rongrong", ""], ["Tian", "Yonghong", ""]]}, {"id": "2004.08790", "submitter": "Huimin Huang", "authors": "Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang,\n  Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, Jian Wu", "title": "UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a growing interest has been seen in deep learning-based semantic\nsegmentation. UNet, which is one of deep learning networks with an\nencoder-decoder architecture, is widely used in medical image segmentation.\nCombining multi-scale features is one of important factors for accurate\nsegmentation. UNet++ was developed as a modified Unet by designing an\narchitecture with nested and dense skip connections. However, it does not\nexplore sufficient information from full scales and there is still a large room\nfor improvement. In this paper, we propose a novel UNet 3+, which takes\nadvantage of full-scale skip connections and deep supervisions. The full-scale\nskip connections incorporate low-level details with high-level semantics from\nfeature maps in different scales; while the deep supervision learns\nhierarchical representations from the full-scale aggregated feature maps. The\nproposed method is especially benefiting for organs that appear at varying\nscales. In addition to accuracy improvements, the proposed UNet 3+ can reduce\nthe network parameters to improve the computation efficiency. We further\npropose a hybrid loss function and devise a classification-guided module to\nenhance the organ boundary and reduce the over-segmentation in a non-organ\nimage, yielding more accurate segmentation results. The effectiveness of the\nproposed method is demonstrated on two datasets. The code is available at:\ngithub.com/ZJUGiveLab/UNet-Version\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 08:05:59 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Huang", "Huimin", ""], ["Lin", "Lanfen", ""], ["Tong", "Ruofeng", ""], ["Hu", "Hongjie", ""], ["Zhang", "Qiaowei", ""], ["Iwamoto", "Yutaro", ""], ["Han", "Xianhua", ""], ["Chen", "Yen-Wei", ""], ["Wu", "Jian", ""]]}, {"id": "2004.08796", "submitter": "Zhiyu Zhu", "authors": "Zhiyu Zhu, Zhen-Peng Bian, Junhui Hou, Yi Wang, Lap-Pui Chau", "title": "When Residual Learning Meets Dense Aggregation: Rethinking the\n  Aggregation of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various architectures (such as GoogLeNets, ResNets, and DenseNets) have been\nproposed. However, the existing networks usually suffer from either redundancy\nof convolutional layers or insufficient utilization of parameters. To handle\nthese challenging issues, we propose Micro-Dense Nets, a novel architecture\nwith global residual learning and local micro-dense aggregations. Specifically,\nresidual learning aims to efficiently retrieve features from different\nconvolutional blocks, while the micro-dense aggregation is able to enhance each\nblock and avoid redundancy of convolutional layers by lessening residual\naggregations. Moreover, the proposed micro-dense architecture has two\ncharacteristics: pyramidal multi-level feature learning which can widen the\ndeeper layer in a block progressively, and dimension cardinality adaptive\nconvolution which can balance each layer using linearly increasing dimension\ncardinality. The experimental results over three datasets (i.e., CIFAR-10,\nCIFAR-100, and ImageNet-1K) demonstrate that the proposed Micro-Dense Net with\nonly 4M parameters can achieve higher classification accuracy than\nstate-of-the-art networks, while being 12.1$\\times$ smaller depends on the\nnumber of parameters. In addition, our micro-dense block can be integrated with\nneural architecture search based models to boost their performance, validating\nthe advantage of our architecture. We believe our design and findings will be\nbeneficial to the DNN community.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 08:34:52 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 05:21:07 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Zhu", "Zhiyu", ""], ["Bian", "Zhen-Peng", ""], ["Hou", "Junhui", ""], ["Wang", "Yi", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2004.08814", "submitter": "Sibei Yang", "authors": "Sibei Yang, Guanbin Li, Yizhou Yu", "title": "Graph-Structured Referring Expression Reasoning in The Wild", "comments": "CVPR 2020 Accepted Oral Paper. Data and code are available at\n  https://github.com/sibeiyang/sgmn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding referring expressions aims to locate in an image an object referred\nto by a natural language expression. The linguistic structure of a referring\nexpression provides a layout of reasoning over the visual contents, and it is\noften crucial to align and jointly understand the image and the referring\nexpression. In this paper, we propose a scene graph guided modular network\n(SGMN), which performs reasoning over a semantic graph and a scene graph with\nneural modules under the guidance of the linguistic structure of the\nexpression. In particular, we model the image as a structured semantic graph,\nand parse the expression into a language scene graph. The language scene graph\nnot only decodes the linguistic structure of the expression, but also has a\nconsistent representation with the image semantic graph. In addition to\nexploring structured solutions to grounding referring expressions, we also\npropose Ref-Reasoning, a large-scale real-world dataset for structured\nreferring expression reasoning. We automatically generate referring expressions\nover the scene graphs of images using diverse expression templates and\nfunctional programs. This dataset is equipped with real-world visual contents\nas well as semantically rich expressions with different reasoning layouts.\nExperimental results show that our SGMN not only significantly outperforms\nexisting state-of-the-art algorithms on the new Ref-Reasoning dataset, but also\nsurpasses state-of-the-art structured methods on commonly used benchmark\ndatasets. It can also provide interpretable visual evidences of reasoning. Data\nand code are available at https://github.com/sibeiyang/sgmn\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 11:00:30 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yang", "Sibei", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "2004.08878", "submitter": "Qianyu Zhou", "authors": "Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Guangliang Cheng, Xuequan Lu,\n  Jianping Shi, Lizhuang Ma", "title": "Uncertainty-Aware Consistency Regularization for Cross-Domain Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to adapt existing models of the\nsource domain to a new target domain with only unlabeled data. Many\nadversarial-based UDA methods involve high-instability training and have to\ncarefully tune the optimization procedure. Some non-adversarial UDA methods\nemploy a consistency regularization on the target predictions of a student\nmodel and a teacher model under different perturbations, where the teacher\nshares the same architecture with the student and is updated by the exponential\nmoving average of the student. However, these methods suffer from noticeable\nnegative transfer resulting from either the error-prone discriminator network\nor the unreasonable teacher model. In this paper, we propose an\nuncertainty-aware consistency regularization method for cross-domain semantic\nsegmentation. By exploiting the latent uncertainty information of the target\nsamples, more meaningful and reliable knowledge from the teacher model can be\ntransferred to the student model. In addition, we further reveal the reason why\nthe current consistency regularization is often unstable in minimizing the\ndistribution discrepancy. We also show that our method can effectively ease\nthis issue by mining the most reliable and meaningful samples with a dynamic\nweighting scheme of consistency loss. Experiments demonstrate that the proposed\nmethod outperforms the state-of-the-art methods on two domain adaptation\nbenchmarks, $i.e.,$ GTAV $\\rightarrow $ Cityscapes and SYNTHIA $\\rightarrow $\nCityscapes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 15:30:26 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:25:09 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 11:34:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhou", "Qianyu", ""], ["Feng", "Zhengyang", ""], ["Gu", "Qiqi", ""], ["Cheng", "Guangliang", ""], ["Lu", "Xuequan", ""], ["Shi", "Jianping", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2004.08886", "submitter": "Yue Shi", "authors": "Yue Shi, Liangxiu Han, Wenjiang Huang, Sheng Chang, Yingying Dong,\n  Darren Dancey, Lianghao Han", "title": "A Biologically Interpretable Two-stage Deep Neural Network (BIT-DNN) For\n  Vegetation Recognition From Hyperspectral Imagery", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TGRS.2021.3058782", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral-spatial based deep learning models have recently proven to be\neffective in hyperspectral image (HSI) classification for various earth\nmonitoring applications such as land cover classification and agricultural\nmonitoring. However, due to the nature of \"black-box\" model representation, how\nto explain and interpret the learning process and the model decision,\nespecially for vegetation classification, remains an open challenge. This study\nproposes a novel interpretable deep learning model -- a biologically\ninterpretable two-stage deep neural network (BIT-DNN), by incorporating the\nprior-knowledge (i.e. biophysical and biochemical attributes and their\nhierarchical structures of target entities) based spectral-spatial feature\ntransformation into the proposed framework, capable of achieving both high\naccuracy and interpretability on HSI based classification tasks. The proposed\nmodel introduces a two-stage feature learning process: in the first stage, an\nenhanced interpretable feature block extracts the low-level spectral features\nassociated with the biophysical and biochemical attributes of target entities;\nand in the second stage, an interpretable capsule block extracts and\nencapsulates the high-level joint spectral-spatial features representing the\nhierarchical structure of biophysical and biochemical attributes of these\ntarget entities, which provides the model an improved performance on\nclassification and intrinsic interpretability with reduced computational\ncomplexity. We have tested and evaluated the model using four real HSI datasets\nfor four separate tasks (i.e. plant species classification, land cover\nclassification, urban scene recognition, and crop disease recognition tasks).\nThe proposed model has been compared with five state-of-the-art deep learning\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 15:58:19 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 14:26:14 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Shi", "Yue", ""], ["Han", "Liangxiu", ""], ["Huang", "Wenjiang", ""], ["Chang", "Sheng", ""], ["Dong", "Yingying", ""], ["Dancey", "Darren", ""], ["Han", "Lianghao", ""]]}, {"id": "2004.08911", "submitter": "Daniele Meli", "authors": "Michele Ginesi and Daniele Meli and Andrea Roberti and Nicola\n  Sansonetto and Paolo Fiorini", "title": "Autonomous task planning and situation awareness in robotic surgery", "comments": "Submitted to IROS 2020 conference", "journal-ref": "2020 IEEE International Conference on Intelligent Robots and\n  Systems", "doi": "10.1109/IROS45743.2020.9341382", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of robots in minimally invasive surgery has improved the quality of\nstandard surgical procedures. So far, only the automation of simple surgical\nactions has been investigated by researchers, while the execution of structured\ntasks requiring reasoning on the environment and the choice among multiple\nactions is still managed by human surgeons. In this paper, we propose a\nframework to implement surgical task automation. The framework consists of a\ntask-level reasoning module based on answer set programming, a low-level motion\nplanning module based on dynamic movement primitives, and a situation awareness\nmodule. The logic-based reasoning module generates explainable plans and is\nable to recover from failure conditions, which are identified and explained by\nthe situation awareness module interfacing to a human supervisor, for enhanced\nsafety. Dynamic Movement Primitives allow to replicate the dexterity of\nsurgeons and to adapt to obstacles and changes in the environment. The\nframework is validated on different versions of the standard surgical training\npeg-and-ring task.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 17:08:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ginesi", "Michele", ""], ["Meli", "Daniele", ""], ["Roberti", "Andrea", ""], ["Sansonetto", "Nicola", ""], ["Fiorini", "Paolo", ""]]}, {"id": "2004.08915", "submitter": "Ling Lo", "authors": "Ling Lo, Hong-Xia Xie, Hong-Han Shuai, Wen-Huang Cheng", "title": "MER-GCN: Micro Expression Recognition Based on Relation Modeling with\n  Graph Convolutional Network", "comments": "Accepted by IEEE MIPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-Expression (ME) is the spontaneous, involuntary movement of a face that\ncan reveal the true feeling. Recently, increasing researches have paid\nattention to this field combing deep learning techniques. Action units (AUs)\nare the fundamental actions reflecting the facial muscle movements and AU\ndetection has been adopted by many researches to classify facial expressions.\nHowever, the time-consuming annotation process makes it difficult to correlate\nthe combinations of AUs to specific emotion classes. Inspired by the nodes\nrelationship building Graph Convolutional Networks (GCN), we propose an\nend-to-end AU-oriented graph classification network, namely MER-GCN, which uses\n3D ConvNets to extract AU features and applies GCN layers to discover the\ndependency laying between AU nodes for ME categorization. To our best\nknowledge, this work is the first end-to-end architecture for Micro-Expression\nRecognition (MER) using AUs based GCN. The experimental results show that our\napproach outperforms CNN-based MER networks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 17:25:30 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Lo", "Ling", ""], ["Xie", "Hong-Xia", ""], ["Shuai", "Hong-Han", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2004.08933", "submitter": "Jakub Maksymilian Fober", "authors": "Jakub Maksymilian Fober", "title": "Rectification with Visual Sphere perspective: an algebraic alternative\n  for P4P pose estimation", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented algorithm solves P4P problem for tangent pair of coplanar parallel\nlines viewed in perspective with an algebraic equation. Introduction of Visual\nSphere Perspective model extends this algorithm to exotic non-linear\nprojections, where view angle can span to 180{\\deg} and beyond; a hard-limit\nfor rectilinear perspective, common in planar homography and POSIt algorithms.\nThis solution performs full 3D reconstruction of a visible rectangle, including\npose estimation, camera orientation and position, without loop iterations. Full\npose estimation requires some camera-lens information like focal length (for\nrectilinear projection) or a perspective map. For a generic 2D,\nperspective-correct rectification, camera lens information is not required for\nthis method. This paper also presents visual sphere based, iteration-free\nestimation method for camera's focal length.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 18:39:08 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 00:04:24 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 01:01:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Fober", "Jakub Maksymilian", ""]]}, {"id": "2004.08945", "submitter": "Samet Akcay", "authors": "Seyma Yucer, Samet Ak\\c{c}ay, Noura Al-Moubayed, Toby P. Breckon", "title": "Exploring Racial Bias within Face Recognition via per-subject\n  Adversarially-Enabled Data Augmentation", "comments": "CVPR 2020 - Fair, Data Efficient and Trusted Computer Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst face recognition applications are becoming increasingly prevalent\nwithin our daily lives, leading approaches in the field still suffer from\nperformance bias to the detriment of some racial profiles within society. In\nthis study, we propose a novel adversarial derived data augmentation\nmethodology that aims to enable dataset balance at a per-subject level via the\nuse of image-to-image transformation for the transfer of sensitive racial\ncharacteristic facial features. Our aim is to automatically construct a\nsynthesised dataset by transforming facial images across varying racial\ndomains, while still preserving identity-related features, such that racially\ndependant features subsequently become irrelevant within the determination of\nsubject identity. We construct our experiments on three significant face\nrecognition variants: Softmax, CosFace and ArcFace loss over a common\nconvolutional neural network backbone. In a side-by-side comparison, we show\nthe positive impact our proposed technique can have on the recognition\nperformance for (racial) minority groups within an originally imbalanced\ntraining dataset by reducing the pre-race variance in performance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 19:46:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yucer", "Seyma", ""], ["Ak\u00e7ay", "Samet", ""], ["Al-Moubayed", "Noura", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2004.08947", "submitter": "Sebasti\\'an Salazar-Colores", "authors": "Sebasti\\'an Salazar-Colores, Hugo Alberto-Moreno, C\\'esar Javier\n  Ortiz-Echeverri, Gerardo Flores", "title": "Desmoking laparoscopy surgery images using an image-to-image translation\n  guided by an embedded dark channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In laparoscopic surgery, the visibility in the image can be severely degraded\nby the smoke caused by the $CO_2$ injection, and dissection tools, thus\nreducing the visibility of organs and tissues. This lack of visibility\nincreases the surgery time and even the probability of mistakes conducted by\nthe surgeon, then producing negative consequences on the patient's health. In\nthis paper, a novel computational approach to remove the smoke effects is\nintroduced. The proposed method is based on an image-to-image conditional\ngenerative adversarial network in which a dark channel is used as an embedded\nguide mask. Obtained experimental results are evaluated and compared\nquantitatively with other desmoking and dehazing state-of-art methods using the\nmetrics of the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity\n(SSIM) index. Based on these metrics, it is found that the proposed method has\nimproved performance compared to the state-of-the-art. Moreover, the processing\ntime required by our method is 92 frames per second, and thus, it can be\napplied in a real-time medical system trough an embedded device.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 19:51:24 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Salazar-Colores", "Sebasti\u00e1n", ""], ["Alberto-Moreno", "Hugo", ""], ["Ortiz-Echeverri", "C\u00e9sar Javier", ""], ["Flores", "Gerardo", ""]]}, {"id": "2004.08955", "submitter": "Hang Zhang", "authors": "Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi\n  Zhang, Yue Sun, Tong He, Jonas Mueller, R. Manmatha, Mu Li, Alexander Smola", "title": "ResNeSt: Split-Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that featuremap attention and multi-path representation are\nimportant for visual recognition. In this paper, we present a modularized\narchitecture, which applies the channel-wise attention on different network\nbranches to leverage their success in capturing cross-feature interactions and\nlearning diverse representations. Our design results in a simple and unified\ncomputation block, which can be parameterized using only a few variables. Our\nmodel, named ResNeSt, outperforms EfficientNet in accuracy and latency\ntrade-off on image classification. In addition, ResNeSt has achieved superior\ntransfer learning results on several public benchmarks serving as the backbone,\nand has been adopted by the winning entries of COCO-LVIS challenge. The source\ncode for complete system and pretrained models are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:40:31 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 05:05:15 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Hang", ""], ["Wu", "Chongruo", ""], ["Zhang", "Zhongyue", ""], ["Zhu", "Yi", ""], ["Lin", "Haibin", ""], ["Zhang", "Zhi", ""], ["Sun", "Yue", ""], ["He", "Tong", ""], ["Mueller", "Jonas", ""], ["Manmatha", "R.", ""], ["Li", "Mu", ""], ["Smola", "Alexander", ""]]}, {"id": "2004.08960", "submitter": "Prajval Koul", "authors": "Prajval Koul", "title": "Spectral GUI for Automated Tissue and Lesion Segmentation of T1 Weighted\n  Breast MR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Spectral GUI, a multiplatform breast MR image analysis tool\ndesigned to facilitate the segmentation of fibro glandular tissues and lesions\nin T1 weighted breast MR images via a graphical user interface (GUI). Spectral\nGUIR uses spectrum loft method [1] for breast MR image segmentation. Not only\nis it interactive, but robust and expeditious at the same time. Being devoid of\nany machine learning algorithm, it shows exceptionally high execution speed\nwith minimal overheads. The accuracy of the results has been simultaneously\nmeasured using performance metrics and expert entailment. The validity and\napplicability of the tool are discussed in the paper along with a crisp\ncontrast with traditional machine learning principles, establishing the\nunequivocal foundation of it as a competent tool in the field of image\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:52:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Koul", "Prajval", ""]]}, {"id": "2004.08965", "submitter": "Shengchang Zhang", "authors": "Shengchang Zhang, Jie Xiang, Weijian Han", "title": "Machine Learning based Pallets Detection and Tracking in AGVs", "comments": "6 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of automated guided vehicles (AGVs) has played a pivotal role in\nmanufacturing and distribution operations, providing reliable and efficient\nproduct handling. In this project, we constructed a deep learning-based pallets\ndetection and tracking architecture for pallets detection and position\ntracking. By using data preprocessing and augmentation techniques and\nexperiment with hyperparameter tuning, we achieved the result with 25%\nreduction of error rate, 28.5% reduction of false negative rate, and 20%\nreduction of training time.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 21:17:13 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Shengchang", ""], ["Xiang", "Jie", ""], ["Han", "Weijian", ""]]}, {"id": "2004.08977", "submitter": "Shengchang Zhang", "authors": "Shengchang Zhang, Ahmed EI Koubia, Khaled Abdul Karim Mohammed", "title": "Traffic Lane Detection using FCN", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic lane detection is a crucial technology that enables self-driving\ncars to properly position themselves in a multi-lane urban driving\nenvironments. However, detecting diverse road markings in various weather\nconditions is a challenging task for conventional image processing or computer\nvision techniques. In recent years, the application of Deep Learning and Neural\nNetworks in this area has proven to be very effective. In this project, we\ndesigned an Encoder- Decoder, Fully Convolutional Network for lane detection.\nThis model was applied to a real-world large scale dataset and achieved a level\nof accuracy that outperformed our baseline model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 22:25:12 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Shengchang", ""], ["Koubia", "Ahmed EI", ""], ["Mohammed", "Khaled Abdul Karim", ""]]}, {"id": "2004.09033", "submitter": "Dongliang Chang", "authors": "Xiaoxu Li, Dongliang Chang, Zhanyu Ma, Zheng-Hua Tan, Jing-Hao Xue,\n  Jie Cao, Jingyi Yu, and Jun Guo", "title": "OSLNet: Deep Small-Sample Classification with an Orthogonal Softmax\n  Layer", "comments": "TIP 2020. Code available at https://github.com/dongliangchang/OSLNet", "journal-ref": null, "doi": "10.1109/TIP.2020.2990277", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network of multiple nonlinear layers forms a large function\nspace, which can easily lead to overfitting when it encounters small-sample\ndata. To mitigate overfitting in small-sample classification, learning more\ndiscriminative features from small-sample data is becoming a new trend. To this\nend, this paper aims to find a subspace of neural networks that can facilitate\na large decision margin. Specifically, we propose the Orthogonal Softmax Layer\n(OSL), which makes the weight vectors in the classification layer remain\northogonal during both the training and test processes. The Rademacher\ncomplexity of a network using the OSL is only $\\frac{1}{K}$, where $K$ is the\nnumber of classes, of that of a network using the fully connected\nclassification layer, leading to a tighter generalization error bound.\nExperimental results demonstrate that the proposed OSL has better performance\nthan the methods used for comparison on four small-sample benchmark datasets,\nas well as its applicability to large-sample datasets. Codes are available at:\nhttps://github.com/dongliangchang/OSLNet.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 02:41:01 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Xiaoxu", ""], ["Chang", "Dongliang", ""], ["Ma", "Zhanyu", ""], ["Tan", "Zheng-Hua", ""], ["Xue", "Jing-Hao", ""], ["Cao", "Jie", ""], ["Yu", "Jingyi", ""], ["Guo", "Jun", ""]]}, {"id": "2004.09034", "submitter": "Damien Teney", "authors": "Damien Teney, Ehsan Abbasnedjad, Anton van den Hengel", "title": "Learning What Makes a Difference from Counterfactual Examples and\n  Gradient Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary challenges limiting the applicability of deep learning is\nits susceptibility to learning spurious correlations rather than the underlying\nmechanisms of the task of interest. The resulting failure to generalise cannot\nbe addressed by simply using more data from the same distribution. We propose\nan auxiliary training objective that improves the generalization capabilities\nof neural networks by leveraging an overlooked supervisory signal found in\nexisting datasets. We use pairs of minimally-different examples with different\nlabels, a.k.a counterfactual or contrasting examples, which provide a signal\nindicative of the underlying causal structure of the task. We show that such\npairs can be identified in a number of existing datasets in computer vision\n(visual question answering, multi-label image classification) and natural\nlanguage processing (sentiment analysis, natural language inference). The new\ntraining objective orients the gradient of a model's decision function with\npairs of counterfactual examples. Models trained with this technique\ndemonstrate improved performance on out-of-distribution test sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 02:47:49 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Teney", "Damien", ""], ["Abbasnedjad", "Ehsan", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2004.09039", "submitter": "Michael Danielczuk", "authors": "Michael Danielczuk, Anelia Angelova, Vincent Vanhoucke, Ken Goldberg", "title": "X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of\n  Learned Occupancy Distributions", "comments": "IROS 2020. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications in e-commerce, warehouses, healthcare, and home service,\nrobots are often required to search through heaps of objects to grasp a\nspecific target object. For mechanical search, we introduce X-Ray, an algorithm\nbased on learned occupancy distributions. We train a neural network using a\nsynthetic dataset of RGBD heap images labeled for a set of standard bounding\nbox targets with varying aspect ratios. X-Ray minimizes support of the learned\ndistribution as part of a mechanical search policy in both simulated and real\nenvironments. We benchmark these policies against two baseline policies on\n1,000 heaps of 15 objects in simulation where the target object is partially or\nfully occluded. Results suggest that X-Ray is significantly more efficient, as\nit succeeds in extracting the target object 82% of the time, 15% more often\nthan the best-performing baseline. Experiments on an ABB YuMi robot with 20\nheaps of 25 household objects suggest that the learned policy transfers easily\nto a physical system, where it outperforms baseline policies by 15% in success\nrate with 17% fewer actions. Datasets, videos, and experiments are available at\nhttps://sites.google.com/berkeley.edu/x-ray.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 03:25:10 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 19:44:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Danielczuk", "Michael", ""], ["Angelova", "Anelia", ""], ["Vanhoucke", "Vincent", ""], ["Goldberg", "Ken", ""]]}, {"id": "2004.09044", "submitter": "Chi Zhang", "authors": "Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin\n  Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum,\n  Song-Chun Zhu", "title": "Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike\n  Common Sense", "comments": "For high quality figures, please refer to\n  http://wellyzhang.github.io/attach/dark.pdf", "journal-ref": "Engineering, Feb, 2020", "doi": "10.1016/j.eng.2020.01.011", "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep learning is essentially based on a \"big data for\nsmall tasks\" paradigm, under which massive amounts of data are used to train a\nclassifier for a single narrow task. In this paper, we call for a shift that\nflips this paradigm upside down. Specifically, we propose a \"small data for big\ntasks\" paradigm, wherein a single artificial intelligence (AI) system is\nchallenged to develop \"common sense\", enabling it to solve a wide range of\ntasks with little training data. We illustrate the potential power of this new\nparadigm by reviewing models of common sense that synthesize recent\nbreakthroughs in both machine and human vision. We identify functionality,\nphysics, intent, causality, and utility (FPICU) as the five core domains of\ncognitive AI with humanlike common sense. When taken as a unified concept,\nFPICU is concerned with the questions of \"why\" and \"how\", beyond the dominant\n\"what\" and \"where\" framework for understanding vision. They are invisible in\nterms of pixels but nevertheless drive the creation, maintenance, and\ndevelopment of visual scenes. We therefore coin them the \"dark matter\" of\nvision. Just as our universe cannot be understood by merely studying observable\nmatter, we argue that vision cannot be understood without studying FPICU. We\ndemonstrate the power of this perspective to develop cognitive AI systems with\nhumanlike common sense by showing how to observe and apply FPICU with little\ntraining data to solve a wide range of challenging tasks, including tool use,\nplanning, utility inference, and social learning. In summary, we argue that the\nnext generation of AI must embrace \"dark\" humanlike common sense for solving\nnovel tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:07:28 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhu", "Yixin", ""], ["Gao", "Tao", ""], ["Fan", "Lifeng", ""], ["Huang", "Siyuan", ""], ["Edmonds", "Mark", ""], ["Liu", "Hangxin", ""], ["Gao", "Feng", ""], ["Zhang", "Chi", ""], ["Qi", "Siyuan", ""], ["Wu", "Ying Nian", ""], ["Tenenbaum", "Joshua B.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2004.09048", "submitter": "Oladapo Afolabi", "authors": "Oladapo Afolabi, Allen Y. Yang, S. Shankar Sastry", "title": "Extending DeepSDF for automatic 3D shape retrieval and similarity\n  transform estimation", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer graphics and computer vision have found\nsuccessful application of deep neural network models for 3D shapes based on\nsigned distance functions (SDFs) that are useful for shape representation,\nretrieval, and completion. However, this approach has been limited by the need\nto have query shapes in the same canonical scale and pose as those observed\nduring training, restricting its effectiveness on real world scenes. We present\na formulation to overcome this issue by jointly estimating shape and similarity\ntransform parameters. We conduct experiments to demonstrate the effectiveness\nof this formulation on synthetic and real datasets and report favorable\ncomparisons to the state of the art. Finally, we also emphasize the viability\nof this approach as a form of 3D model compression.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:28:45 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 19:01:39 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 05:01:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Afolabi", "Oladapo", ""], ["Yang", "Allen Y.", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "2004.09056", "submitter": "Masahiro Oda Dr.", "authors": "Masahiro Oda, Holger R. Roth, Takayuki Kitasaka, Kazuhiro Furukawa,\n  Ryoji Miyahara, Yoshiki Hirooka, Nassir Navab, Kensaku Mori", "title": "Colonoscope tracking method based on shape estimation network", "comments": "Accepted paper as an oral presentation at SPIE Medical Imaging 2019,\n  San Diego, CA, USA", "journal-ref": "Proceedings of SPIE Medical Imaging 2019: Image-Guided Procedures,\n  Robotic Interventions, and Modeling, Vol.10951, 109510Q", "doi": "10.1117/12.2512729", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a colonoscope tracking method utilizing a colon shape\nestimation method. CT colonography is used as a less-invasive colon diagnosis\nmethod. If colonic polyps or early-stage cancers are found, they are removed in\na colonoscopic examination. In the colonoscopic examination, understanding\nwhere the colonoscope running in the colon is difficult. A colonoscope\nnavigation system is necessary to reduce overlooking of polyps. We propose a\ncolonoscope tracking method for navigation systems. Previous colonoscope\ntracking methods caused large tracking errors because they do not consider\ndeformations of the colon during colonoscope insertions. We utilize the shape\nestimation network (SEN), which estimates deformed colon shape during\ncolonoscope insertions. The SEN is a neural network containing long short-term\nmemory (LSTM) layer. To perform colon shape estimation suitable to the real\nclinical situation, we trained the SEN using data obtained during colonoscope\noperations of physicians. The proposed tracking method performs mapping of the\ncolonoscope tip position to a position in the colon using estimation results of\nthe SEN. We evaluated the proposed method in a phantom study. We confirmed that\ntracking errors of the proposed method was enough small to perform navigation\nin the ascending, transverse, and descending colons.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:10:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Oda", "Masahiro", ""], ["Roth", "Holger R.", ""], ["Kitasaka", "Takayuki", ""], ["Furukawa", "Kazuhiro", ""], ["Miyahara", "Ryoji", ""], ["Hirooka", "Yoshiki", ""], ["Navab", "Nassir", ""], ["Mori", "Kensaku", ""]]}, {"id": "2004.09057", "submitter": "Congcong Wen", "authors": "Congcong Wen, Xiang Li, Xiaojing Yao, Ling Peng, Tianhe Chi", "title": "Airborne LiDAR Point Cloud Classification with Graph Attention\n  Convolution Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airborne light detection and ranging (LiDAR) plays an increasingly\nsignificant role in urban planning, topographic mapping, environmental\nmonitoring, power line detection and other fields thanks to its capability to\nquickly acquire large-scale and high-precision ground information. To achieve\npoint cloud classification, previous studies proposed point cloud deep learning\nmodels that can directly process raw point clouds based on PointNet-like\narchitectures. And some recent works proposed graph convolution neural network\nbased on the inherent topology of point clouds. However, the above point cloud\ndeep learning models only pay attention to exploring local geometric\nstructures, yet ignore global contextual relationships among all points. In\nthis paper, we present a graph attention convolution neural network (GACNN)\nthat can be directly applied to the classification of unstructured 3D point\nclouds obtained by airborne LiDAR. Specifically, we first introduce a graph\nattention convolution module that incorporates global contextual information\nand local structural features. Based on the proposed graph attention\nconvolution module, we further design an end-to-end encoder-decoder network,\nnamed GACNN, to capture multiscale features of the point clouds and therefore\nenable more accurate airborne point cloud classification. Experiments on the\nISPRS 3D labeling dataset show that the proposed model achieves a new\nstate-of-the-art performance in terms of average F1 score (71.5\\%) and a\nsatisfying overall accuracy (83.2\\%). Additionally, experiments further\nconducted on the 2019 Data Fusion Contest Dataset by comparing with other\nprevalent point cloud deep learning models demonstrate the favorable\ngeneralization capability of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:12:31 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wen", "Congcong", ""], ["Li", "Xiang", ""], ["Yao", "Xiaojing", ""], ["Peng", "Ling", ""], ["Chi", "Tianhe", ""]]}, {"id": "2004.09061", "submitter": "Yang You", "authors": "Yang You, Chengkun Li, Yujing Lou, Zhoujun Cheng, Lizhuang Ma, Cewu\n  Lu, Weiming Wang", "title": "Semantic Correspondence via 2D-3D-2D Cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual semantic correspondence is an important topic in computer vision and\ncould help machine understand objects in our daily life. However, most previous\nmethods directly train on correspondences in 2D images, which is end-to-end but\nloses plenty of information in 3D spaces. In this paper, we propose a new\nmethod on predicting semantic correspondences by leveraging it to 3D domain and\nthen project corresponding 3D models back to 2D domain, with their semantic\nlabels. Our method leverages the advantages in 3D vision and can explicitly\nreason about objects self-occlusion and visibility. We show that our method\ngives comparative and even superior results on standard semantic benchmarks. We\nalso conduct thorough and detailed experiments to analyze our network\ncomponents. The code and experiments are publicly available at\nhttps://github.com/qq456cvb/SemanticTransfer.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 05:27:45 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:20:57 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["You", "Yang", ""], ["Li", "Chengkun", ""], ["Lou", "Yujing", ""], ["Cheng", "Zhoujun", ""], ["Ma", "Lizhuang", ""], ["Lu", "Cewu", ""], ["Wang", "Weiming", ""]]}, {"id": "2004.09073", "submitter": "Ranjan Maitra", "authors": "Geoffrey Z. Thompson and Ranjan Maitra", "title": "CatSIM: A Categorical Image Similarity Metric", "comments": "17 pages, 16 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CatSIM, a new similarity metric for binary and multinary two-\nand three-dimensional images and volumes. CatSIM uses a structural similarity\nimage quality paradigm and is robust to small perturbations in location so that\nstructures in similar, but not entirely overlapping, regions of two images are\nrated higher than using simple matching. The metric can also compare arbitrary\nregions inside images. CatSIM is evaluated on artificial data sets, image\nquality assessment surveys and two imaging applications\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 06:03:58 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Thompson", "Geoffrey Z.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2004.09089", "submitter": "Sheng-Yeh Chen", "authors": "Sheng-Yeh Chen and Yung-Yu Chuang", "title": "Deep Exposure Fusion with Deghosting via Homography Estimation and\n  Attention Learning", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cameras have limited dynamic ranges and often produce images with\nsaturated or dark regions using a single exposure. Although the problem could\nbe addressed by taking multiple images with different exposures, exposure\nfusion methods need to deal with ghosting artifacts and detail loss caused by\ncamera motion or moving objects. This paper proposes a deep network for\nexposure fusion. For reducing the potential ghosting problem, our network only\ntakes two images, an underexposed image and an overexposed one. Our network\nintegrates together homography estimation for compensating camera motion,\nattention mechanism for correcting remaining misalignment and moving pixels,\nand adversarial learning for alleviating other remaining artifacts. Experiments\non real-world photos taken using handheld mobile phones show that the proposed\nmethod can generate high-quality images with faithful detail and vivid color\nrendition in both dark and bright areas.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 07:00:14 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Sheng-Yeh", ""], ["Chuang", "Yung-Yu", ""]]}, {"id": "2004.09141", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Johnny Lee, Szymon\n  Rusinkiewicz, Thomas Funkhouser", "title": "Spatial Action Maps for Mobile Manipulation", "comments": "To appear at Robotics: Science and Systems (RSS), 2020. Project page:\n  https://spatial-action-maps.cs.princeton.edu", "journal-ref": null, "doi": "10.15607/RSS.2020.XVI.035", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical end-to-end formulations for learning robotic navigation involve\npredicting a small set of steering command actions (e.g., step forward, turn\nleft, turn right, etc.) from images of the current state (e.g., a bird's-eye\nview of a SLAM reconstruction). Instead, we show that it can be advantageous to\nlearn with dense action representations defined in the same domain as the\nstate. In this work, we present \"spatial action maps,\" in which the set of\npossible actions is represented by a pixel map (aligned with the input image of\nthe current state), where each pixel represents a local navigational endpoint\nat the corresponding scene location. Using ConvNets to infer spatial action\nmaps from state images, action predictions are thereby spatially anchored on\nlocal visual features in the scene, enabling significantly faster learning of\ncomplex behaviors for mobile manipulation tasks with reinforcement learning. In\nour experiments, we task a robot with pushing objects to a goal location, and\nfind that policies learned with spatial action maps achieve much better\nperformance than traditional alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:06:10 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 10:56:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wu", "Jimmy", ""], ["Sun", "Xingyuan", ""], ["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Lee", "Johnny", ""], ["Rusinkiewicz", "Szymon", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2004.09144", "submitter": "Nicola Messina", "authors": "Nicola Messina, Fabrizio Falchi, Andrea Esuli, Giuseppe Amato", "title": "Transformer Reasoning Network for Image-Text Matching and Retrieval", "comments": "Presented at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching is an interesting and fascinating task in modern AI\nresearch. Despite the evolution of deep-learning-based image and text\nprocessing systems, multi-modal matching remains a challenging problem. In this\nwork, we consider the problem of accurate image-text matching for the task of\nmulti-modal large-scale information retrieval. State-of-the-art results in\nimage-text matching are achieved by inter-playing image and text features from\nthe two different processing pipelines, usually using mutual attention\nmechanisms. However, this invalidates any chance to extract separate visual and\ntextual features needed for later indexing steps in large-scale retrieval\nsystems. In this regard, we introduce the Transformer Encoder Reasoning Network\n(TERN), an architecture built upon one of the modern relationship-aware\nself-attentive architectures, the Transformer Encoder (TE). This architecture\nis able to separately reason on the two different modalities and to enforce a\nfinal common abstract concept space by sharing the weights of the deeper\ntransformer layers. Thanks to this design, the implemented network is able to\nproduce compact and very rich visual and textual features available for the\nsuccessive indexing step. Experiments are conducted on the MS-COCO dataset, and\nwe evaluate the results using a discounted cumulative gain metric with\nrelevance computed exploiting caption similarities, in order to assess possibly\nnon-exact but relevant search results. We demonstrate that on this metric we\nare able to achieve state-of-the-art results in the image retrieval task. Our\ncode is freely available at https://github.com/mesnico/TERN.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:09:01 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 15:31:55 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 21:19:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Messina", "Nicola", ""], ["Falchi", "Fabrizio", ""], ["Esuli", "Andrea", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2004.09147", "submitter": "Yi Li", "authors": "Yi Li, Huaibo Huang, Junchi Yu, Ran He, Tieniu Tan", "title": "Cosmetic-Aware Makeup Cleanser", "comments": "Accepted by BTAS 2019 (the 10th IEEE International Conference on\n  Biometrics: Theory, Applications and Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification aims at determining whether a pair of face images belongs\nto the same identity. Recent studies have revealed the negative impact of\nfacial makeup on the verification performance. With the rapid development of\ndeep generative models, this paper proposes a semanticaware makeup cleanser\n(SAMC) to remove facial makeup under different poses and expressions and\nachieve verification via generation. The intuition lies in the fact that makeup\nis a combined effect of multiple cosmetics and tailored treatments should be\nimposed on different cosmetic regions. To this end, we present both\nunsupervised and supervised semantic-aware learning strategies in SAMC. At\nimage level, an unsupervised attention module is jointly learned with the\ngenerator to locate cosmetic regions and estimate the degree. At feature level,\nwe resort to the effort of face parsing merely in training phase and design a\nlocalized texture loss to serve complements and pursue superior synthetic\nquality. The experimental results on four makeuprelated datasets verify that\nSAMC not only produces appealing de-makeup outputs at a resolution of 256*256,\nbut also facilitates makeup-invariant face verification through image\ngeneration.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:18:23 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Yi", ""], ["Huang", "Huaibo", ""], ["Yu", "Junchi", ""], ["He", "Ran", ""], ["Tan", "Tieniu", ""]]}, {"id": "2004.09164", "submitter": "Xiangyu Zhu", "authors": "Xiangyu Zhu, Zhenbo Luo, Pei Fu, Xiang Ji", "title": "VOC-ReID: Vehicle Re-identification based on Vehicle-Orientation-Camera", "comments": "AICity2020 Challenge, CVPR 2020 workshop, code avaible at github(link\n  in abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification is a challenging task due to high intra-class\nvariances and small inter-class variances. In this work, we focus on the\nfailure cases caused by similar background and shape. They pose serve bias on\nsimilarity, making it easier to neglect fine-grained information. To reduce the\nbias, we propose an approach named VOC-ReID, taking the triplet\nvehicle-orientation-camera as a whole and reforming background/shape similarity\nas camera/orientation re-identification. At first, we train models for vehicle,\norientation and camera re-identification respectively. Then we use orientation\nand camera similarity as penalty to get final similarity. Besides, we propose a\nhigh performance baseline boosted by bag of tricks and weakly supervised data\naugmentation. Our algorithm achieves the second place in vehicle\nre-identification at the NVIDIA AI City Challenge 2020.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:44:07 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 07:59:43 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Zhu", "Xiangyu", ""], ["Luo", "Zhenbo", ""], ["Fu", "Pei", ""], ["Ji", "Xiang", ""]]}, {"id": "2004.09166", "submitter": "Matthias Rath", "authors": "Matthias Rath and Alexandru Paul Condurache", "title": "Invariant Integration in Deep Convolutional Feature Space", "comments": "Accepted at ESANN 2020 (European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we show how to incorporate prior knowledge to a deep\nneural network architecture in a principled manner. We enforce feature space\ninvariances using a novel layer based on invariant integration. This allows us\nto construct a complete feature space invariant to finite transformation\ngroups.\n  We apply our proposed layer to explicitly insert invariance properties for\nvision-related classification tasks, demonstrate our approach for the case of\nrotation invariance and report state-of-the-art performance on the\nRotated-MNIST dataset. Our method is especially beneficial when training with\nlimited data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:45:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rath", "Matthias", ""], ["Condurache", "Alexandru Paul", ""]]}, {"id": "2004.09169", "submitter": "Andrei-Timotei Ardelean", "authors": "Andrei-Timotei Ardelean, Lucian Mircea Sasu", "title": "Pose Manipulation with Identity Preservation", "comments": "9 pages, journal article", "journal-ref": "International Journal of Computers Communications & Control, Vol\n  15, Nr 2, 3862, 2020", "doi": "10.15837/ijccc.2020.2.3862", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new model which generates images in novel poses e.g.\nby altering face expression and orientation, from just a few instances of a\nhuman subject. Unlike previous approaches which require large datasets of a\nspecific person for training, our approach may start from a scarce set of\nimages, even from a single image. To this end, we introduce Character Adaptive\nIdentity Normalization GAN (CainGAN) which uses spatial characteristic features\nextracted by an embedder and combined across source images. The identity\ninformation is propagated throughout the network by applying conditional\nnormalization. After extensive adversarial training, CainGAN receives figures\nof faces from a certain individual and produces new ones while preserving the\nperson's identity. Experimental results show that the quality of generated\nimages scales with the size of the input set used during inference.\nFurthermore, quantitative measurements indicate that CainGAN performs better\ncompared to other methods when training data is limited.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:51:31 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ardelean", "Andrei-Timotei", ""], ["Sasu", "Lucian Mircea", ""]]}, {"id": "2004.09179", "submitter": "Julia Lust", "authors": "Julia Lust and Alexandru Paul Condurache", "title": "GraN: An Efficient Gradient-Norm Based Detector for Adversarial and\n  Misclassified Examples", "comments": "Accepted at ESANN 2020 (European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial examples and other\ndata perturbations. Especially in safety critical applications of DNNs, it is\ntherefore crucial to detect misclassified samples. The current state-of-the-art\ndetection methods require either significantly more runtime or more parameters\nthan the original network itself. This paper therefore proposes GraN, a time-\nand parameter-efficient method that is easily adaptable to any DNN.\n  GraN is based on the layer-wise norm of the DNN's gradient regarding the loss\nof the current input-output combination, which can be computed via\nbackpropagation. GraN achieves state-of-the-art performance on numerous problem\nset-ups.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:09:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Lust", "Julia", ""], ["Condurache", "Alexandru Paul", ""]]}, {"id": "2004.09190", "submitter": "Juyong Zhang", "authors": "Hongrui Cai, Yudong Guo, Zhuang Peng, Juyong Zhang", "title": "Landmark Detection and 3D Face Reconstruction for Caricature using a\n  Nonlinear Parametric Model", "comments": "The code is available at https://github.com/Juyong/CaricatureFace", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an artistic abstraction of the human face by distorting or\nexaggerating certain facial features, while still retains a likeness with the\ngiven face. Due to the large diversity of geometric and texture variations,\nautomatic landmark detection and 3D face reconstruction for caricature is a\nchallenging problem and has rarely been studied before. In this paper, we\npropose the first automatic method for this task by a novel 3D approach. To\nthis end, we first build a dataset with various styles of 2D caricatures and\ntheir corresponding 3D shapes, and then build a parametric model on vertex\nbased deformation space for 3D caricature face. Based on the constructed\ndataset and the nonlinear parametric model, we propose a neural network based\nmethod to regress the 3D face shape and orientation from the input 2D\ncaricature image. Ablation studies and comparison with state-of-the-art methods\ndemonstrate the effectiveness of our algorithm design. Extensive experimental\nresults demonstrate that our method works well for various caricatures. Our\nconstructed dataset, source code and trained model are available at\nhttps://github.com/Juyong/CaricatureFace.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:34:52 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 12:52:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cai", "Hongrui", ""], ["Guo", "Yudong", ""], ["Peng", "Zhuang", ""], ["Zhang", "Juyong", ""]]}, {"id": "2004.09197", "submitter": "Chengzhou Tang", "authors": "Chengzhou Tang, Lu Yuan and Ping Tan", "title": "LSM: Learning Subspace Minimization for Low-level Vision", "comments": "To be presented at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the energy minimization problem in low-level vision tasks from a\nnovel perspective. We replace the heuristic regularization term with a\nlearnable subspace constraint, and preserve the data term to exploit domain\nknowledge derived from the first principle of a task. This learning subspace\nminimization (LSM) framework unifies the network structures and the parameters\nfor many low-level vision tasks, which allows us to train a single network for\nmultiple tasks simultaneously with completely shared parameters, and even\ngeneralizes the trained network to an unseen task as long as its data term can\nbe formulated. We demonstrate our LSM framework on four low-level tasks\nincluding interactive image segmentation, video segmentation, stereo matching,\nand optical flow, and validate the network on various datasets. The experiments\nshow that the proposed LSM generates state-of-the-art results with smaller\nmodel size, faster training convergence, and real-time inference.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:49:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Tang", "Chengzhou", ""], ["Yuan", "Lu", ""], ["Tan", "Ping", ""]]}, {"id": "2004.09199", "submitter": "Xialei Liu", "authors": "Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bogdan Raducanu,\n  Andrew D. Bagdanov, Shangling Jui, Joost van de Weijer", "title": "Generative Feature Replay For Class-Incremental Learning", "comments": "Accepted at CVPR2020: Workshop on Continual Learning in Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans are capable of learning new tasks without forgetting previous ones,\nwhile neural networks fail due to catastrophic forgetting between new and\npreviously-learned tasks. We consider a class-incremental setting which means\nthat the task-ID is unknown at inference time. The imbalance between old and\nnew classes typically results in a bias of the network towards the newest ones.\nThis imbalance problem can either be addressed by storing exemplars from\nprevious tasks, or by using image replay methods. However, the latter can only\nbe applied to toy datasets since image generation for complex datasets is a\nhard problem.\n  We propose a solution to the imbalance problem based on generative feature\nreplay which does not require any exemplars. To do this, we split the network\ninto two parts: a feature extractor and a classifier. To prevent forgetting, we\ncombine generative feature replay in the classifier with feature distillation\nin the feature extractor. Through feature generation, our method reduces the\ncomplexity of generative replay and prevents the imbalance problem. Our\napproach is computationally efficient and scalable to large datasets.\nExperiments confirm that our approach achieves state-of-the-art results on\nCIFAR-100 and ImageNet, while requiring only a fraction of the storage needed\nfor exemplar-based continual learning. Code available at\n\\url{https://github.com/xialeiliu/GFR-IL}.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:58:20 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Liu", "Xialei", ""], ["Wu", "Chenshen", ""], ["Menta", "Mikel", ""], ["Herranz", "Luis", ""], ["Raducanu", "Bogdan", ""], ["Bagdanov", "Andrew D.", ""], ["Jui", "Shangling", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2004.09215", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Qi She, Tejo Chalasani, Aljosa Smolic", "title": "CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture\n  Recognition", "comments": "CVPR 2020 Workshop at Continual Learning (CLVISION)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric gestures are the most natural form of communication for humans to\ninteract with wearable devices such as VR/AR helmets and glasses. A major issue\nin such scenarios for real-world applications is that may easily become\nnecessary to add new gestures to the system e.g., a proper VR system should\nallow users to customize gestures incrementally. Traditional deep learning\nmethods require storing all previous class samples in the system and training\nthe model again from scratch by incorporating previous samples and new samples,\nwhich costs humongous memory and significantly increases computation over time.\nIn this work, we demonstrate a lifelong 3D convolutional framework --\nc(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal\ninformation in videos and enables lifelong learning for egocentric gesture\nvideo recognition by learning the feature representation of an exemplar set\nselected from previous class samples. Importantly, we propose a two-stream\nCatNet, which deploys RGB and depth modalities to train two separate networks.\nWe evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and\nshow that CatNets can learn many classes incrementally over a long period of\ntime. Results also demonstrate that the two-stream architecture achieves the\nbest performance on both joint training and class incremental training compared\nto 3 other one-stream architectures. The codes and pre-trained models used in\nthis work are provided at https://github.com/villawang/CatNet.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:36:02 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wang", "Zhengwei", ""], ["She", "Qi", ""], ["Chalasani", "Tejo", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2004.09216", "submitter": "Nils Gessert", "authors": "Nils Gessert and Marcel Bengs and Julia Kr\\\"uger and Roland Opfer and\n  Ann-Christin Ostwaldt and Praveena Manogaran and Sven Schippling and\n  Alexander Schlaefer", "title": "4D Deep Learning for Multiple Sclerosis Lesion Activity Segmentation", "comments": "Accepted at MIDL 2020", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/238UzYB1d9", "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sclerosis lesion activity segmentation is the task of detecting new\nand enlarging lesions that appeared between a baseline and a follow-up brain\nMRI scan. While deep learning methods for single-scan lesion segmentation are\ncommon, deep learning approaches for lesion activity have only been proposed\nrecently. Here, a two-path architecture processes two 3D MRI volumes from two\ntime points. In this work, we investigate whether extending this problem to\nfull 4D deep learning using a history of MRI volumes and thus an extended\nbaseline can improve performance. For this purpose, we design a recurrent\nmulti-encoder-decoder architecture for processing 4D data. We find that adding\nmore temporal information is beneficial and our proposed architecture\noutperforms previous approaches with a lesion-wise true positive rate of 0.84\nat a lesion-wise false positive rate of 0.19.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:41:01 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 19:28:54 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Gessert", "Nils", ""], ["Bengs", "Marcel", ""], ["Kr\u00fcger", "Julia", ""], ["Opfer", "Roland", ""], ["Ostwaldt", "Ann-Christin", ""], ["Manogaran", "Praveena", ""], ["Schippling", "Sven", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2004.09226", "submitter": "Francesco Cricri", "authors": "Nannan Zou, Honglei Zhang, Francesco Cricri, Hamed R. Tavakoli, Jani\n  Lainema, Emre Aksu, Miska Hannuksela, Esa Rahtu", "title": "End-to-End Learning for Video Frame Compression with Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core components of conventional (i.e., non-learned) video codecs\nconsists of predicting a frame from a previously-decoded frame, by leveraging\ntemporal correlations. In this paper, we propose an end-to-end learned system\nfor compressing video frames. Instead of relying on pixel-space motion (as with\noptical flow), our system learns deep embeddings of frames and encodes their\ndifference in latent space. At decoder-side, an attention mechanism is designed\nto attend to the latent space of frames to decide how different parts of the\nprevious and current frame are combined to form the final predicted current\nframe. Spatially-varying channel allocation is achieved by using importance\nmasks acting on the feature-channels. The model is trained to reduce the\nbitrate by minimizing a loss on importance maps and a loss on the probability\noutput by a context model for arithmetic coding. In our experiments, we show\nthat the proposed system achieves high compression rates and high objective\nvisual quality as measured by MS-SSIM and PSNR. Furthermore, we provide\nablation studies where we highlight the contribution of different components.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 12:11:08 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zou", "Nannan", ""], ["Zhang", "Honglei", ""], ["Cricri", "Francesco", ""], ["Tavakoli", "Hamed R.", ""], ["Lainema", "Jani", ""], ["Aksu", "Emre", ""], ["Hannuksela", "Miska", ""], ["Rahtu", "Esa", ""]]}, {"id": "2004.09228", "submitter": "Dongkai Wang", "authors": "Dongkai Wang, Shiliang Zhang", "title": "Unsupervised Person Re-identification via Multi-label Classification", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of unsupervised person re-identification (ReID) lies in\nlearning discriminative features without true labels. This paper formulates\nunsupervised person ReID as a multi-label classification task to progressively\nseek true labels. Our method starts by assigning each person image with a\nsingle-class label, then evolves to multi-label classification by leveraging\nthe updated ReID model for label prediction. The label prediction comprises\nsimilarity computation and cycle consistency to ensure the quality of predicted\nlabels. To boost the ReID model training efficiency in multi-label\nclassification, we further propose the memory-based multi-label classification\nloss (MMCL). MMCL works with memory-based non-parametric classifier and\nintegrates multi-label classification and single-label classification in a\nunified framework. Our label prediction and MMCL work iteratively and\nsubstantially boost the ReID performance. Experiments on several large-scale\nperson ReID datasets demonstrate the superiority of our method in unsupervised\nperson ReID. Our method also allows to use labeled person images in other\ndomains. Under this transfer learning setting, our method also achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 12:13:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wang", "Dongkai", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2004.09251", "submitter": "Luca Ciampi", "authors": "Luca Ciampi and Carlos Santiago and Joao Paulo Costeira and Claudio\n  Gennaro and Giuseppe Amato", "title": "Unsupervised Vehicle Counting via Multiple Camera Domain Adaptation", "comments": "1st International Workshop on New Foundations for Human-Centered AI\n  (NeHuAI) at ECAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring vehicle flows in cities is crucial to improve the urban\nenvironment and quality of life of citizens. Images are the best sensing\nmodality to perceive and assess the flow of vehicles in large areas. Current\ntechnologies for vehicle counting in images hinge on large quantities of\nannotated data, preventing their scalability to city-scale as new cameras are\nadded to the system. This is a recurrent problem when dealing with physical\nsystems and a key research area in Machine Learning and AI. We propose and\ndiscuss a new methodology to design image-based vehicle density estimators with\nfew labeled data via multiple camera domain adaptations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:00:46 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 17:34:22 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ciampi", "Luca", ""], ["Santiago", "Carlos", ""], ["Costeira", "Joao Paulo", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2004.09272", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti, Viveka Kulharia, Puneet K. Dokania, N. Siddharth,\n  Philip H.S. Torr", "title": "A Revised Generative Evaluation of Visual Dialogue", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating Visual Dialogue, the task of answering a sequence of questions\nrelating to a visual input, remains an open research challenge. The current\nevaluation scheme of the VisDial dataset computes the ranks of ground-truth\nanswers in predefined candidate sets, which Massiceti et al. (2018) show can be\nsusceptible to the exploitation of dataset biases. This scheme also does little\nto account for the different ways of expressing the same answer--an aspect of\nlanguage that has been well studied in NLP. We propose a revised evaluation\nscheme for the VisDial dataset leveraging metrics from the NLP literature to\nmeasure consensus between answers generated by the model and a set of relevant\nanswers. We construct these relevant answer sets using a simple and effective\nsemi-supervised method based on correlation, which allows us to automatically\nextend and scale sparse relevance annotations from humans to the entire\ndataset. We release these sets and code for the revised evaluation scheme as\nDenseVisDial, and intend them to be an improvement to the dataset in the face\nof its existing constraints and design choices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:26:45 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 08:48:25 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Massiceti", "Daniela", ""], ["Kulharia", "Viveka", ""], ["Dokania", "Puneet K.", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2004.09305", "submitter": "Peiliang Li", "authors": "Peiliang Li, Jieqi Shi, Shaojie Shen", "title": "Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking", "comments": "cvpr2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directly learning multiple 3D objects motion from sequential images is\ndifficult, while the geometric bundle adjustment lacks the ability to localize\nthe invisible object centroid. To benefit from both the powerful object\nunderstanding skill from deep neural network meanwhile tackle precise geometry\nmodeling for consistent trajectory estimation, we propose a joint\nspatial-temporal optimization-based stereo 3D object tracking method. From the\nnetwork, we detect corresponding 2D bounding boxes on adjacent images and\nregress an initial 3D bounding box. Dense object cues (local depth and local\ncoordinates) that associating to the object centroid are then predicted using a\nregion-based network. Considering both the instant localization accuracy and\nmotion consistency, our optimization models the relations between the object\ncentroid and observed cues into a joint spatial-temporal error function. All\nhistoric cues will be summarized to contribute to the current estimation by a\nper-frame marginalization strategy without repeated computation. Quantitative\nevaluation on the KITTI tracking dataset shows our approach outperforms\nprevious image-based 3D tracking methods by significant margins. We also report\nextensive results on multiple categories and larger datasets (KITTI raw and\nArgoverse Tracking) for future benchmarking.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:59:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Peiliang", ""], ["Shi", "Jieqi", ""], ["Shen", "Shaojie", ""]]}, {"id": "2004.09309", "submitter": "Gil Shomron", "authors": "Gil Shomron, Uri Weiser", "title": "Non-Blocking Simultaneous Multithreading: Embracing the Resiliency of\n  Deep Neural Networks", "comments": "MICRO-53", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known for their inability to utilize\nunderlying hardware resources due to hardware susceptibility to sparse\nactivations and weights. Even in finer granularities, many of the non-zero\nvalues hold a portion of zero-valued bits that may cause inefficiencies when\nexecuted on hardware. Inspired by conventional CPU simultaneous multithreading\n(SMT) that increases computer resource utilization by sharing them across\nseveral threads, we propose non-blocking SMT (NB-SMT) designated for DNN\naccelerators. Like conventional SMT, NB-SMT shares hardware resources among\nseveral execution flows. Yet, unlike SMT, NB-SMT is non-blocking, as it handles\nstructural hazards by exploiting the algorithmic resiliency of DNNs. Instead of\nopportunistically dispatching instructions while they wait in a reservation\nstation for available hardware, NB-SMT temporarily reduces the computation\nprecision to accommodate all threads at once, enabling a non-blocking\noperation. We demonstrate NB-SMT applicability using SySMT, an NB-SMT-enabled\noutput-stationary systolic array (OS-SA). Compared with a conventional OS-SA, a\n2-threaded SySMT consumes 1.4x the area and delivers 2x speedup with 33% energy\nsavings and less than 1% accuracy degradation of state-of-the-art CNNs with\nImageNet. A 4-threaded SySMT consumes 2.5x the area and delivers, for example,\n3.4x speedup and 39% energy savings with 1% accuracy degradation of 40%-pruned\nResNet-18.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 09:29:56 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 20:54:37 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Shomron", "Gil", ""], ["Weiser", "Uri", ""]]}, {"id": "2004.09317", "submitter": "Federico Paredes-Vall\\'es", "authors": "D. B. de Jong, F. Paredes-Vall\\'es, G. C. H. E. de Croon", "title": "How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired\n  Study", "comments": "16 pages, 15 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3083538", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end trained convolutional neural networks have led to a breakthrough\nin optical flow estimation. The most recent advances focus on improving the\noptical flow estimation by improving the architecture and setting a new\nbenchmark on the publicly available MPI-Sintel dataset. Instead, in this\narticle, we investigate how deep neural networks estimate optical flow. A\nbetter understanding of how these networks function is important for (i)\nassessing their generalization capabilities to unseen inputs, and (ii)\nsuggesting changes to improve their performance. For our investigation, we\nfocus on FlowNetS, as it is the prototype of an encoder-decoder neural network\nfor optical flow estimation. Furthermore, we use a filter identification method\nthat has played a major role in uncovering the motion filters present in animal\nbrains in neuropsychological research. The method shows that the filters in the\ndeepest layer of FlowNetS are sensitive to a variety of motion patterns. Not\nonly do we find translation filters, as demonstrated in animal brains, but\nthanks to the easier measurements in artificial neural networks, we even unveil\ndilation, rotation, and occlusion filters. Furthermore, we find similarities in\nthe refinement part of the network and the perceptual filling-in process which\noccurs in the mammal primary visual cortex.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:08:28 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:16:45 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["de Jong", "D. B.", ""], ["Paredes-Vall\u00e9s", "F.", ""], ["de Croon", "G. C. H. E.", ""]]}, {"id": "2004.09320", "submitter": "Max Ehrlich", "authors": "Max Ehrlich, Larry Davis, Ser-Nam Lim, Abhinav Shrivastava", "title": "Quantization Guided JPEG Artifact Correction", "comments": "Published in the proceedings of ECCV 2020, please see our released\n  code and models at https://gitlab.com/Queuecumber/quantization-guided-ac", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The JPEG image compression algorithm is the most popular method of image\ncompression because of its ability for large compression ratios. However, to\nachieve such high compression, information is lost. For aggressive quantization\nsettings, this leads to a noticeable reduction in image quality. Artifact\ncorrection has been studied in the context of deep neural networks for some\ntime, but the current state-of-the-art methods require a different model to be\ntrained for each quality setting, greatly limiting their practical application.\nWe solve this problem by creating a novel architecture which is parameterized\nby the JPEG files quantization matrix. This allows our single model to achieve\nstate-of-the-art performance over models trained for specific quality settings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 00:10:08 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 14:28:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ehrlich", "Max", ""], ["Davis", "Larry", ""], ["Lim", "Ser-Nam", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2004.09321", "submitter": "Marta Bianca Maria Ranzini Ms", "authors": "Marta B.M. Ranzini, Irme Groothuis, Kerstin Kl\\\"aser, M. Jorge\n  Cardoso, Johann Henckel, S\\'ebastien Ourselin, Alister Hart, Marc Modat", "title": "Combining multimodal information for Metal Artefact Reduction: An\n  unsupervised deep learning framework", "comments": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal artefact reduction (MAR) techniques aim at removing metal-induced noise\nfrom clinical images. In Computed Tomography (CT), supervised deep learning\napproaches have been shown effective but limited in generalisability, as they\nmostly rely on synthetic data. In Magnetic Resonance Imaging (MRI) instead, no\nmethod has yet been introduced to correct the susceptibility artefact, still\npresent even in MAR-specific acquisitions. In this work, we hypothesise that a\nmultimodal approach to MAR would improve both CT and MRI. Given their different\nartefact appearance, their complementary information can compensate for the\ncorrupted signal in either modality. We thus propose an unsupervised deep\nlearning method for multimodal MAR. We introduce the use of Locally Normalised\nCross Correlation as a loss term to encourage the fusion of multimodal\ninformation. Experiments show that our approach favours a smoother correction\nin the CT, while promoting signal recovery in the MRI.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:12:00 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ranzini", "Marta B. M.", ""], ["Groothuis", "Irme", ""], ["Kl\u00e4ser", "Kerstin", ""], ["Cardoso", "M. Jorge", ""], ["Henckel", "Johann", ""], ["Ourselin", "S\u00e9bastien", ""], ["Hart", "Alister", ""], ["Modat", "Marc", ""]]}, {"id": "2004.09329", "submitter": "Yingji Zhong", "authors": "Yingji Zhong, Xiaoyu Wang, Shiliang Zhang", "title": "Robust Partial Matching for Person Search in the Wild", "comments": "9 pages, 7 figures, accepted to CVPR 2020. The dataset will be\n  released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various factors like occlusions, backgrounds, etc., would lead to misaligned\ndetected bounding boxes , e.g., ones covering only portions of human body. This\nissue is common but overlooked by previous person search works. To alleviate\nthis issue, this paper proposes an Align-to-Part Network (APNet) for person\ndetection and re-Identification (reID). APNet refines detected bounding boxes\nto cover the estimated holistic body regions, from which discriminative part\nfeatures can be extracted and aligned. Aligned part features naturally\nformulate reID as a partial feature matching procedure, where valid part\nfeatures are selected for similarity computation, while part features on\noccluded or noisy regions are discarded. This design enhances the robustness of\nperson search to real-world challenges with marginal computation overhead. This\npaper also contributes a Large-Scale dataset for Person Search in the wild\n(LSPS), which is by far the largest and the most challenging dataset for person\nsearch. Experiments show that APNet brings considerable performance improvement\non LSPS. Meanwhile, it achieves competitive performance on existing person\nsearch benchmarks like CUHK-SYSU and PRW.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:21:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhong", "Yingji", ""], ["Wang", "Xiaoyu", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2004.09363", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Rahele Kafieh, Milan Sonka, Shakib Yazdani, Ghazaleh\n  Jamalipour Soufi", "title": "Deep-COVID: Predicting COVID-19 From Chest X-Ray Images Using Deep\n  Transfer Learning", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic is causing a major outbreak in more than 150 countries\naround the world, having a severe impact on the health and life of many people\nglobally. One of the crucial step in fighting COVID-19 is the ability to detect\nthe infected patients early enough, and put them under special care. Detecting\nthis disease from radiography and radiology images is perhaps one of the\nfastest ways to diagnose the patients. Some of the early studies showed\nspecific abnormalities in the chest radiograms of patients infected with\nCOVID-19. Inspired by earlier works, we study the application of deep learning\nmodels to detect COVID-19 patients from their chest radiography images. We\nfirst prepare a dataset of 5,000 Chest X-rays from the publicly available\ndatasets. Images exhibiting COVID-19 disease presence were identified by\nboard-certified radiologist. Transfer learning on a subset of 2,000 radiograms\nwas used to train four popular convolutional neural networks, including\nResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease\nin the analyzed chest X-ray images. We evaluated these models on the remaining\n3,000 images, and most of these networks achieved a sensitivity rate of 98%\n($\\pm$ 3%), while having a specificity rate of around 90%. Besides sensitivity\nand specificity rates, we also present the receiver operating characteristic\n(ROC) curve, precision-recall curve, average prediction, and confusion matrix\nof each model. We also used a technique to generate heatmaps of lung regions\npotentially infected by COVID-19 and show that the generated heatmaps contain\nmost of the infected areas annotated by our board certified radiologist. While\nthe achieved performance is very encouraging, further analysis is required on a\nlarger set of COVID-19 images, to have a more reliable estimation of accuracy\nrates. The dataset, model implementations (in PyTorch), and evaluations, are\nall made publicly available for research community at\nhttps://github.com/shervinmin/DeepCovid.git\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:09:14 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 05:42:39 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 14:10:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Minaee", "Shervin", ""], ["Kafieh", "Rahele", ""], ["Sonka", "Milan", ""], ["Yazdani", "Shakib", ""], ["Soufi", "Ghazaleh Jamalipour", ""]]}, {"id": "2004.09374", "submitter": "Maya Aghaei", "authors": "Maya Aghaei, Matteo Bustreo, Pietro Morerio, Nicolo Carissimi, Alessio\n  Del Bue, Vittorio Murino", "title": "Complex-Object Visual Inspection via Multiple Lighting Configurations", "comments": "8 pages, 7 figures, submitted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of an automatic visual inspection system is usually performed in\ntwo stages. While the first stage consists in selecting the most suitable\nhardware setup for highlighting most effectively the defects on the surface to\nbe inspected, the second stage concerns the development of algorithmic\nsolutions to exploit the potentials offered by the collected data.\n  In this paper, first, we present a novel illumination setup embedding four\nillumination configurations to resemble diffused, dark-field, and front\nlighting techniques. Second, we analyze the contributions brought by deploying\nthe proposed setup in training phase only - mimicking the scenario in which an\nalready developed visual inspection system cannot be modified on the customer\nsite - and in evaluation phase. Along with an exhaustive set of experiments, in\nthis paper, we demonstrate the suitability of the proposed setup for effective\nillumination of complex-objects, defined as manufactured items with variable\nsurface characteristics that cannot be determined a priori. Moreover, we\ndiscuss the importance of multiple light configurations availability during\ntraining and their natural boosting effect which, without the need to modify\nthe system design in evaluation phase, lead to improvements in the overall\nsystem performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:25:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Aghaei", "Maya", ""], ["Bustreo", "Matteo", ""], ["Morerio", "Pietro", ""], ["Carissimi", "Nicolo", ""], ["Del Bue", "Alessio", ""], ["Murino", "Vittorio", ""]]}, {"id": "2004.09403", "submitter": "Wanqi Yang", "authors": "Wanqi Yang, Tong Ling, Chengmei Yang, Lei Wang, Yinghuan Shi, Luping\n  Zhou, Ming Yang", "title": "Class Distribution Alignment for Adversarial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing unsupervised domain adaptation methods mainly focused on\naligning the marginal distributions of samples between the source and target\ndomains. This setting does not sufficiently consider the class distribution\ninformation between the two domains, which could adversely affect the reduction\nof domain gap. To address this issue, we propose a novel approach called\nConditional ADversarial Image Translation (CADIT) to explicitly align the class\ndistributions given samples between the two domains. It integrates a\ndiscriminative structure-preserving loss and a joint adversarial generation\nloss. The former effectively prevents undesired label-flipping during the whole\nprocess of image translation, while the latter maintains the joint distribution\nalignment of images and labels. Furthermore, our approach enforces the\nclassification consistence of target domain images before and after adaptation\nto aid the classifier training in both domains. Extensive experiments were\nconducted on multiple benchmark datasets including Digits, Faces, Scenes and\nOffice31, showing that our approach achieved superior classification in the\ntarget domain when compared to the state-of-the-art methods. Also, both\nqualitative and quantitative results well supported our motivation that\naligning the class distributions can indeed improve domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:58:11 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yang", "Wanqi", ""], ["Ling", "Tong", ""], ["Yang", "Chengmei", ""], ["Wang", "Lei", ""], ["Shi", "Yinghuan", ""], ["Zhou", "Luping", ""], ["Yang", "Ming", ""]]}, {"id": "2004.09406", "submitter": "Christina Funke", "authors": "Christina M. Funke, Judy Borowski, Karolina Stosio, Wieland Brendel,\n  Thomas S. A. Wallis, Matthias Bethge", "title": "Five Points to Check when Comparing Visual Perception in Humans and\n  Machines", "comments": "V3: minor changes like in published JOV version\n  (https://doi.org/10.1167/jov.21.3.16) V2: New title; added general section\n  (checklist); manuscript restructured such that each case study is one\n  chapter; adversarial examples in first study replaced by different analysis", "journal-ref": "Journal of Vision 21, no. 3 (2021): 16-16", "doi": "10.1167/jov.21.3.16", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of machines to human-level performance in complex recognition\ntasks, a growing amount of work is directed towards comparing information\nprocessing in humans and machines. These studies are an exciting chance to\nlearn about one system by studying the other. Here, we propose ideas on how to\ndesign, conduct and interpret experiments such that they adequately support the\ninvestigation of mechanisms when comparing human and machine perception. We\ndemonstrate and apply these ideas through three case studies. The first case\nstudy shows how human bias can affect how we interpret results, and that\nseveral analytic tools can help to overcome this human reference point. In the\nsecond case study, we highlight the difference between necessary and sufficient\nmechanisms in visual reasoning tasks. Thereby, we show that contrary to\nprevious suggestions, feedback mechanisms might not be necessary for the tasks\nin question. The third case study highlights the importance of aligning\nexperimental conditions. We find that a previously-observed difference in\nobject recognition does not hold when adapting the experiment to make\nconditions more equitable between humans and machines. In presenting a\nchecklist for comparative studies of visual reasoning in humans and machines,\nwe hope to highlight how to overcome potential pitfalls in design or inference.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:05:36 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 08:37:22 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 16:03:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Funke", "Christina M.", ""], ["Borowski", "Judy", ""], ["Stosio", "Karolina", ""], ["Brendel", "Wieland", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""]]}, {"id": "2004.09411", "submitter": "Chaoyi Zhang", "authors": "Chaoyi Zhang, Yang Song, Lina Yao, Weidong Cai", "title": "Shape-Oriented Convolution Neural Network for Point Cloud Analysis", "comments": "8 pages, 6 figures, AAAI2020", "journal-ref": "https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhangC.4075.pdf", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is a principal data structure adopted for 3D geometric\ninformation encoding. Unlike other conventional visual data, such as images and\nvideos, these irregular points describe the complex shape features of 3D\nobjects, which makes shape feature learning an essential component of point\ncloud analysis. To this end, a shape-oriented message passing scheme dubbed\nShapeConv is proposed to focus on the representation learning of the underlying\nshape formed by each local neighboring point. Despite this intra-shape\nrelationship learning, ShapeConv is also designed to incorporate the contextual\neffects from the inter-shape relationship through capturing the long-ranged\ndependencies between local underlying shapes. This shape-oriented operator is\nstacked into our hierarchical learning architecture, namely Shape-Oriented\nConvolutional Neural Network (SOCNN), developed for point cloud analysis.\nExtensive experiments have been performed to evaluate its significance in the\ntasks of point cloud classification and part segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:11:51 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Chaoyi", ""], ["Song", "Yang", ""], ["Yao", "Lina", ""], ["Cai", "Weidong", ""]]}, {"id": "2004.09412", "submitter": "Ji Gan", "authors": "Ji Gan, Weiqiang Wang, Ke Lu", "title": "Characters as Graphs: Recognizing Online Handwritten Chinese Characters\n  via Spatial Graph Convolutional Network", "comments": "8 pages, 4 figures. A full version of this paper has been submitted\n  to an international journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese is one of the most widely used languages in the world, yet online\nhandwritten Chinese character recognition (OLHCCR) remains challenging. To\nrecognize Chinese characters, one popular choice is to adopt the 2D\nconvolutional neural network (2D-CNN) on the extracted feature images, and\nanother one is to employ the recurrent neural network (RNN) or 1D-CNN on the\ntime-series features. Instead of viewing characters as either static images or\ntemporal trajectories, here we propose to represent characters as geometric\ngraphs, retaining both spatial structures and temporal orders. Accordingly, we\npropose a novel spatial graph convolution network (SGCN) to effectively\nclassify those character graphs for the first time. Specifically, our SGCN\nincorporates the local neighbourhood information via spatial graph convolutions\nand further learns the global shape properties with a hierarchical residual\nstructure. Experiments on IAHCC-UCAS2016, ICDAR-2013, and UNIPEN datasets\ndemonstrate that the SGCN can achieve comparable recognition performance with\nthe state-of-the-art methods for character recognition.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:12:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Gan", "Ji", ""], ["Wang", "Weiqiang", ""], ["Lu", "Ke", ""]]}, {"id": "2004.09420", "submitter": "Anwaar Ulhaq Dr", "authors": "Anwaar Ulhaq, Asim Khan, Douglas Gomes, Manoranjan Paul", "title": "Computer Vision For COVID-19 Control: A Survey", "comments": "24 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has triggered an urgent need to contribute to the fight\nagainst an immense threat to the human population. Computer Vision, as a\nsubfield of Artificial Intelligence, has enjoyed recent success in solving\nvarious complex problems in health care and has the potential to contribute to\nthe fight of controlling COVID-19. In response to this call, computer vision\nresearchers are putting their knowledge base at work to devise effective ways\nto counter COVID-19 challenge and serve the global community. New contributions\nare being shared with every passing day. It motivated us to review the recent\nwork, collect information about available research resources and an indication\nof future research directions. We want to make it available to computer vision\nresearchers to save precious time. This survey paper is intended to provide a\npreliminary review of the available literature on the computer vision efforts\nagainst COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 05:43:52 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 06:01:17 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Ulhaq", "Anwaar", ""], ["Khan", "Asim", ""], ["Gomes", "Douglas", ""], ["Paul", "Manoranjan", ""]]}, {"id": "2004.09430", "submitter": "Dmitriy Goncharov", "authors": "Dmitriy Goncharov and Rostislav Starikov", "title": "Improving correlation method with convolutional neural networks", "comments": "8 pages, 3 figures, 2 tables, 1 formula", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a convolutional neural network for the classification of\ncorrelation responses obtained by correlation filters. The proposed approach\ncan improve the accuracy of classification, as well as achieve invariance to\nthe image classes and parameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:36:01 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Goncharov", "Dmitriy", ""], ["Starikov", "Rostislav", ""]]}, {"id": "2004.09443", "submitter": "Ning Zhang", "authors": "Ning Zhang, Susan Francis, Rayaz Malik, Xin Chen", "title": "A Spatially Constrained Deep Convolutional Neural Network for Nerve\n  Fiber Segmentation in Corneal Confocal Microscopic Images using Inaccurate\n  Annotations", "comments": "4 pages, accepted for publication at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2020", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI\n  2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is one of the most important tasks in medical\nimage analysis. Most state-of-the-art deep learning methods require a large\nnumber of accurately annotated examples for model training. However, accurate\nannotation is difficult to obtain especially in medical applications. In this\npaper, we propose a spatially constrained deep convolutional neural network\n(DCNN) to achieve smooth and robust image segmentation using inaccurately\nannotated labels for training. In our proposed method, image segmentation is\nformulated as a graph optimization problem that is solved by a DCNN model\nlearning process. The cost function to be optimized consists of a unary term\nthat is calculated by cross entropy measurement and a pairwise term that is\nbased on enforcing a local label consistency. The proposed method has been\nevaluated based on corneal confocal microscopic (CCM) images for nerve fiber\nsegmentation, where accurate annotations are extremely difficult to be\nobtained. Based on both the quantitative result of a synthetic dataset and\nqualitative assessment of a real dataset, the proposed method has achieved\nsuperior performance in producing high quality segmentation results even with\ninaccurate labels for training.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:56:13 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Ning", ""], ["Francis", "Susan", ""], ["Malik", "Rayaz", ""], ["Chen", "Xin", ""]]}, {"id": "2004.09476", "submitter": "Chuang Gan", "authors": "Chuang Gan, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio\n  Torralba", "title": "Music Gesture for Visual Sound Separation", "comments": "CVPR 2020. Project page: http://music-gesture.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches have achieved impressive performance on\nvisual sound separation tasks. However, these approaches are mostly built on\nappearance and optical flow like motion feature representations, which exhibit\nlimited abilities to find the correlations between audio signals and visual\npoints, especially when separating multiple instruments of the same types, such\nas multiple violins in a scene. To address this, we propose \"Music Gesture,\" a\nkeypoint-based structured representation to explicitly model the body and\nfinger movements of musicians when they perform music. We first adopt a\ncontext-aware graph network to integrate visual semantic context with body\ndynamics, and then apply an audio-visual fusion model to associate body\nmovements with the corresponding audio signals. Experimental results on three\nmusic performance datasets show: 1) strong improvements upon benchmark metrics\nfor hetero-musical separation tasks (i.e. different instruments); 2) new\nability for effective homo-musical separation for piano, flute, and trumpet\nduets, which to our best knowledge has never been achieved with alternative\nmethods. Project page: http://music-gesture.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:53:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Gan", "Chuang", ""], ["Huang", "Deng", ""], ["Zhao", "Hang", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""]]}, {"id": "2004.09484", "submitter": "Dongdong Chen", "authors": "Ziyu Wan and Bo Zhang and Dongdong Chen and Pan Zhang and Dong Chen\n  and Jing Liao and Fang Wen", "title": "Bringing Old Photos Back to Life", "comments": "CVPR 2020 Oral, project website: http://raywzy.com/Old_Photo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to restore old photos that suffer from severe degradation through\na deep learning approach. Unlike conventional restoration tasks that can be\nsolved through supervised learning, the degradation in real photos is complex\nand the domain gap between synthetic images and real old photos makes the\nnetwork fail to generalize. Therefore, we propose a novel triplet domain\ntranslation network by leveraging real photos along with massive synthetic\nimage pairs. Specifically, we train two variational autoencoders (VAEs) to\nrespectively transform old photos and clean photos into two latent spaces. And\nthe translation between these two latent spaces is learned with synthetic\npaired data. This translation generalizes well to real photos because the\ndomain gap is closed in the compact latent space. Besides, to address multiple\ndegradations mixed in one old photo, we design a global branch with a partial\nnonlocal block targeting to the structured defects, such as scratches and dust\nspots, and a local branch targeting to the unstructured defects, such as noises\nand blurriness. Two branches are fused in the latent space, leading to improved\ncapability to restore old photos from multiple defects. The proposed method\noutperforms state-of-the-art methods in terms of visual quality for old photos\nrestoration.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:59:23 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wan", "Ziyu", ""], ["Zhang", "Bo", ""], ["Chen", "Dongdong", ""], ["Zhang", "Pan", ""], ["Chen", "Dong", ""], ["Liao", "Jing", ""], ["Wen", "Fang", ""]]}, {"id": "2004.09502", "submitter": "Xing Di", "authors": "Xing Di, Benjamin S. Riggan, Shuowen Hu, Nathaniel J. Short, Vishal M.\n  Patel", "title": "Multi-Scale Thermal to Visible Face Verification via Attribute Guided\n  Synthesis", "comments": "accepted by IEEE Transactions on Biometrics, Behavior, and Identity\n  Science (T-BIOM). arXiv admin note: substantial text overlap with\n  arXiv:1901.00889", "journal-ref": null, "doi": "10.1109/TBIOM.2021.3060641", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal-to-visible face verification is a challenging problem due to the\nlarge domain discrepancy between the modalities. Existing approaches either\nattempt to synthesize visible faces from thermal faces or learn\ndomain-invariant robust features from these modalities for cross-modal\nmatching. In this paper, we use attributes extracted from visible images to\nsynthesize attribute-preserved visible images from thermal imagery for\ncross-modal matching. A pre-trained attribute predictor network is used to\nextract the attributes from the visible image. Then, a novel multi-scale\ngenerator is proposed to synthesize the visible image from the thermal image\nguided by the extracted attributes. Finally, a pre-trained VGG-Face network is\nleveraged to extract features from the synthesized image and the input visible\nimage for verification. Extensive experiments evaluated on three datasets (ARL\nFace Database, Visible and Thermal Paired Face Database, and Tufts Face\nDatabase) demonstrate that the proposed method achieves state-of-the-art\nperformance. In particular, it achieves around 2.41\\%, 2.85\\% and 1.77\\%\nimprovements in Equal Error Rate (EER) over the state-of-the-art methods on the\nARL Face Database, Visible and Thermal Paired Face Database, and Tufts Face\nDatabase, respectively. An extended dataset (ARL Face Dataset volume III)\nconsisting of polarimetric thermal faces of 121 subjects is also introduced in\nthis paper. Furthermore, an ablation study is conducted to demonstrate the\neffectiveness of different modules in the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 01:45:05 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 01:50:08 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Di", "Xing", ""], ["Riggan", "Benjamin S.", ""], ["Hu", "Shuowen", ""], ["Short", "Nathaniel J.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2004.09508", "submitter": "Reza Pourreza", "authors": "Vijay Veerabadran, Reza Pourreza, Amirhossein Habibian, Taco Cohen", "title": "Adversarial Distortion for Learned Video Compression", "comments": "CVPR Workshops, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel adversarial lossy video compression model.\nAt extremely low bit-rates, standard video coding schemes suffer from\nunpleasant reconstruction artifacts such as blocking, ringing etc. Existing\nlearned neural approaches to video compression have achieved reasonable success\non reducing the bit-rate for efficient transmission and reduce the impact of\nartifacts to an extent. However, they still tend to produce blurred results\nunder extreme compression. In this paper, we present a deep adversarial learned\nvideo compression model that minimizes an auxiliary adversarial distortion\nobjective. We find this adversarial objective to correlate better with human\nperceptual quality judgement relative to traditional quality metrics such as\nMS-SSIM and PSNR. Our experiments using a state-of-the-art learned video\ncompression system demonstrate a reduction of perceptual artifacts and\nreconstruction of detail lost especially under extremely high compression.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:06:31 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 01:57:34 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 18:42:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Veerabadran", "Vijay", ""], ["Pourreza", "Reza", ""], ["Habibian", "Amirhossein", ""], ["Cohen", "Taco", ""]]}, {"id": "2004.09548", "submitter": "Haofei Xu", "authors": "Haofei Xu, Juyong Zhang", "title": "AANet: Adaptive Aggregation Network for Efficient Stereo Matching", "comments": "CVPR 2020. The improved version AANet+ is also included. Code:\n  https://github.com/haofeixu/aanet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable progress made by learning based stereo matching\nalgorithms, one key challenge remains unsolved. Current state-of-the-art stereo\nmodels are mostly based on costly 3D convolutions, the cubic computational\ncomplexity and high memory consumption make it quite expensive to deploy in\nreal-world applications. In this paper, we aim at completely replacing the\ncommonly used 3D convolutions to achieve fast inference speed while maintaining\ncomparable accuracy. To this end, we first propose a sparse points based\nintra-scale cost aggregation method to alleviate the well-known edge-fattening\nissue at disparity discontinuities. Further, we approximate traditional\ncross-scale cost aggregation algorithm with neural network layers to handle\nlarge textureless regions. Both modules are simple, lightweight, and\ncomplementary, leading to an effective and efficient architecture for cost\naggregation. With these two modules, we can not only significantly speed up\nexisting top-performing models (e.g., $41\\times$ than GC-Net, $4\\times$ than\nPSMNet and $38\\times$ than GA-Net), but also improve the performance of fast\nstereo models (e.g., StereoNet). We also achieve competitive results on Scene\nFlow and KITTI datasets while running at 62ms, demonstrating the versatility\nand high efficiency of the proposed method. Our full framework is available at\nhttps://github.com/haofeixu/aanet .\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 18:07:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Xu", "Haofei", ""], ["Zhang", "Juyong", ""]]}, {"id": "2004.09573", "submitter": "Mirco Fuchs", "authors": "Marie-Sophie von Braun and Patrick Frenzel and Christian K\\\"ading and\n  Mirco Fuchs", "title": "Utilizing Mask R-CNN for Waterline Detection in Canoe Sprint Video\n  Analysis", "comments": "(Accepted / In press) 2020 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshop (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining a waterline in images recorded in canoe sprint training is an\nimportant component for the kinematic parameter analysis to assess an athlete's\nperformance. Here, we propose an approach for the automated waterline\ndetection. First, we utilized a pre-trained Mask R-CNN by means of transfer\nlearning for canoe segmentation. Second, we developed a multi-stage approach to\nestimate a waterline from the outline of the segments. It consists of two\nlinear regression stages and the systematic selection of canoe parts. We then\nintroduced a parameterization of the waterline as a basis for further\nevaluations. Next, we conducted a study among several experts to estimate the\nground truth waterlines. This not only included an average waterline drawn from\nthe individual experts annotations but, more importantly, a measure for the\nuncertainty between individual results. Finally, we assessed our method with\nrespect to the question whether the predicted waterlines are in accordance with\nthe experts annotations. Our method demonstrated a high performance and\nprovides opportunities for new applications in the field of automated video\nanalysis in canoe sprint.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:00:45 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["von Braun", "Marie-Sophie", ""], ["Frenzel", "Patrick", ""], ["K\u00e4ding", "Christian", ""], ["Fuchs", "Mirco", ""]]}, {"id": "2004.09576", "submitter": "Yash Bhalgat", "authors": "Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, Nojun Kwak", "title": "LSQ+: Improving low-bit quantization through learnable offsets and\n  better initialization", "comments": "Camera-ready for Joint Workshop on Efficient Deep Learning in\n  Computer Vision, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike ReLU, newer activation functions (like Swish, H-swish, Mish) that are\nfrequently employed in popular efficient architectures can also result in\nnegative activation values, with skewed positive and negative ranges. Typical\nlearnable quantization schemes [PACT, LSQ] assume unsigned quantization for\nactivations and quantize all negative activations to zero which leads to\nsignificant loss in performance. Naively using signed quantization to\naccommodate these negative values requires an extra sign bit which is expensive\nfor low-bit (2-, 3-, 4-bit) quantization. To solve this problem, we propose\nLSQ+, a natural extension of LSQ, wherein we introduce a general asymmetric\nquantization scheme with trainable scale and offset parameters that can learn\nto accommodate the negative activations. Gradient-based learnable quantization\nschemes also commonly suffer from high instability or variance in the final\ntraining performance, hence requiring a great deal of hyper-parameter tuning to\nreach a satisfactory performance. LSQ+ alleviates this problem by using an\nMSE-based initialization scheme for the quantization parameters. We show that\nthis initialization leads to significantly lower variance in final performance\nacross multiple training runs. Overall, LSQ+ shows state-of-the-art results for\nEfficientNet and MixNet and also significantly outperforms LSQ for low-bit\nquantization of neural nets with Swish activations (e.g.: 1.8% gain with W4A4\nquantization and upto 5.6% gain with W2A2 quantization of EfficientNet-B0 on\nImageNet dataset). To the best of our knowledge, ours is the first work to\nquantize such architectures to extremely low bit-widths.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:04:51 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bhalgat", "Yash", ""], ["Lee", "Jinwon", ""], ["Nagel", "Markus", ""], ["Blankevoort", "Tijmen", ""], ["Kwak", "Nojun", ""]]}, {"id": "2004.09610", "submitter": "Valery Vishnevskiy", "authors": "Valery Vishnevskiy, Jonas Walheim, Sebastian Kozerke", "title": "Deep variational network for rapid 4D flow MRI reconstruction", "comments": "15 pages, 6 figures", "journal-ref": "Nature Machine Intelligence 2, 228-235 (2020)", "doi": "10.1038/s42256-020-0165-6", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase-contrast magnetic resonance imaging (MRI) provides time-resolved\nquantification of blood flow dynamics that can aid clinical diagnosis. Long in\nvivo scan times due to repeated three-dimensional (3D) volume sampling over\ncardiac phases and breathing cycles necessitate accelerated imaging techniques\nthat leverage data correlations. Standard compressed sensing reconstruction\nmethods require tuning of hyperparameters and are computationally expensive,\nwhich diminishes the potential reduction of examination times. We propose an\nefficient model-based deep neural reconstruction network and evaluate its\nperformance on clinical aortic flow data. The network is shown to reconstruct\nundersampled 4D flow MRI data in under a minute on standard consumer hardware.\nRemarkably, the relatively low amounts of tunable parameters allowed the\nnetwork to be trained on images from 11 reference scans while generalizing well\nto retrospective and prospective undersampled data for various acceleration\nfactors and anatomies.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:17:49 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Vishnevskiy", "Valery", ""], ["Walheim", "Jonas", ""], ["Kozerke", "Sebastian", ""]]}, {"id": "2004.09629", "submitter": "Tzofi Klinghoffer", "authors": "Tzofi Klinghoffer, Peter Morales, Young-Gyun Park, Nicholas Evans,\n  Kwanghun Chung, and Laura J. Brattain", "title": "Self-Supervised Feature Extraction for 3D Axon Segmentation", "comments": "Accepted to CVPR Computer Vision for Microscopy Image Analysis\n  Workshop 2020. 7 pages. 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing learning-based methods to automatically trace axons in 3D brain\nimagery often rely on manually annotated segmentation labels. Labeling is a\nlabor-intensive process and is not scalable to whole-brain analysis, which is\nneeded for improved understanding of brain function. We propose a\nself-supervised auxiliary task that utilizes the tube-like structure of axons\nto build a feature extractor from unlabeled data. The proposed auxiliary task\nconstrains a 3D convolutional neural network (CNN) to predict the order of\npermuted slices in an input 3D volume. By solving this task, the 3D CNN is able\nto learn features without ground-truth labels that are useful for downstream\nsegmentation with the 3D U-Net model. To the best of our knowledge, our model\nis the first to perform automated segmentation of axons imaged at subcellular\nresolution with the SHIELD technique. We demonstrate improved segmentation\nperformance over the 3D U-Net model on both the SHIELD PVGPe dataset and the\nBigNeuron Project, single neuron Janelia dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:46:04 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Klinghoffer", "Tzofi", ""], ["Morales", "Peter", ""], ["Park", "Young-Gyun", ""], ["Evans", "Nicholas", ""], ["Chung", "Kwanghun", ""], ["Brattain", "Laura J.", ""]]}, {"id": "2004.09632", "submitter": "Anil Sharma", "authors": "Anil Sharma, Saket Anand, and Sanjit K. Kaul", "title": "Intelligent Querying for Target Tracking in Camera Networks using Deep\n  Q-Learning with n-Step Bootstrapping", "comments": "Camera Selections for Target Tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Surveillance camera networks are a useful infrastructure for various visual\nanalytics applications, where high-level inferences and predictions could be\nmade based on target tracking across the network. Most multi-camera tracking\nworks focus on target re-identification and trajectory association problems to\ntrack the target. However, since camera networks can generate enormous amount\nof video data, inefficient schemes for making re-identification or trajectory\nassociation queries can incur prohibitively large computational requirements.\nIn this paper, we address the problem of intelligent scheduling of\nre-identification queries in a multi-camera tracking setting. To this end, we\nformulate the target tracking problem in a camera network as an MDP and learn a\nreinforcement learning based policy that selects a camera for making a\nre-identification query. The proposed approach to camera selection does not\nassume the knowledge of the camera network topology but the resulting policy\nimplicitly learns it. We have also shown that such a policy can be learnt\ndirectly from data. Using the NLPR MCT and the Duke MTMC multi-camera\nmulti-target tracking benchmarks, we empirically show that the proposed\napproach substantially reduces the number of frames queried.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:49:52 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sharma", "Anil", ""], ["Anand", "Saket", ""], ["Kaul", "Sanjit K.", ""]]}, {"id": "2004.09665", "submitter": "Zexi Chen", "authors": "Zexi Chen, Benjamin Dutton, Bharathkumar Ramachandra, Tianfu Wu, Ranga\n  Raju Vatsavai", "title": "Local Clustering with Mean Teacher for Semi-supervised Learning", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mean Teacher (MT) model of Tarvainen and Valpola has shown favorable\nperformance on several semi-supervised benchmark datasets. MT maintains a\nteacher model's weights as the exponential moving average of a student model's\nweights and minimizes the divergence between their probability predictions\nunder diverse perturbations of the inputs. However, MT is known to suffer from\nconfirmation bias, that is, reinforcing incorrect teacher model predictions. In\nthis work, we propose a simple yet effective method called Local Clustering\n(LC) to mitigate the effect of confirmation bias. In MT, each data point is\nconsidered independent of other points during training; however, data points\nare likely to be close to each other in feature space if they share similar\nfeatures. Motivated by this, we cluster data points locally by minimizing the\npairwise distance between neighboring data points in feature space. Combined\nwith a standard classification cross-entropy objective on labeled data points,\nthe misclassified unlabeled data points are pulled towards high-density regions\nof their correct class with the help of their neighbors, thus improving model\nperformance. We demonstrate on semi-supervised benchmark datasets SVHN and\nCIFAR-10 that adding our LC loss to MT yields significant improvements compared\nto MT and performance comparable to the state of the art in semi-supervised\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 22:31:31 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 00:47:01 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Chen", "Zexi", ""], ["Dutton", "Benjamin", ""], ["Ramachandra", "Bharathkumar", ""], ["Wu", "Tianfu", ""], ["Vatsavai", "Ranga Raju", ""]]}, {"id": "2004.09666", "submitter": "Faisal Mahmood", "authors": "Ming Y. Lu, Drew F. K. Williamson, Tiffany Y. Chen, Richard J. Chen,\n  Matteo Barbieri and Faisal Mahmood", "title": "Data Efficient and Weakly Supervised Computational Pathology on Whole\n  Slide Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly emerging field of computational pathology has the potential to\nenable objective diagnosis, therapeutic response prediction and identification\nof new morphological features of clinical relevance. However, deep\nlearning-based computational pathology approaches either require manual\nannotation of gigapixel whole slide images (WSIs) in fully-supervised settings\nor thousands of WSIs with slide-level labels in a weakly-supervised setting.\nMoreover, whole slide level computational pathology methods also suffer from\ndomain adaptation and interpretability issues. These challenges have prevented\nthe broad adaptation of computational pathology for clinical and research\npurposes. Here we present CLAM - Clustering-constrained attention multiple\ninstance learning, an easy-to-use, high-throughput, and interpretable WSI-level\nprocessing and learning method that only requires slide-level labels while\nbeing data efficient, adaptable and capable of handling multi-class subtyping\nproblems. CLAM is a deep-learning-based weakly-supervised method that uses\nattention-based learning to automatically identify sub-regions of high\ndiagnostic value in order to accurately classify the whole slide, while also\nutilizing instance-level clustering over the representative regions identified\nto constrain and refine the feature space. In three separate analyses, we\ndemonstrate the data efficiency and adaptability of CLAM and its superior\nperformance over standard weakly-supervised classification. We demonstrate that\nCLAM models are interpretable and can be used to identify well-known and new\nmorphological features. We further show that models trained using CLAM are\nadaptable to independent test cohorts, cell phone microscopy images, and\nbiopsies. CLAM is a general-purpose and adaptable method that can be used for a\nvariety of different computational pathology tasks in both clinical and\nresearch settings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 23:00:13 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 02:03:49 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Lu", "Ming Y.", ""], ["Williamson", "Drew F. K.", ""], ["Chen", "Tiffany Y.", ""], ["Chen", "Richard J.", ""], ["Barbieri", "Matteo", ""], ["Mahmood", "Faisal", ""]]}, {"id": "2004.09672", "submitter": "Thales Vieira", "authors": "Lucas Massa, Adriano Barbosa, Krerley Oliveira, Thales Vieira", "title": "LRCN-RetailNet: A recurrent neural network architecture for accurate\n  people counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring and analyzing the flow of customers in retail stores is essential\nfor a retailer to better comprehend customers' behavior and support\ndecision-making. Nevertheless, not much attention has been given to the\ndevelopment of novel technologies for automatic people counting. We introduce\nLRCN-RetailNet: a recurrent neural network architecture capable of learning a\nnon-linear regression model and accurately predicting the people count from\nvideos captured by low-cost surveillance cameras. The input video format\nfollows the recently proposed RGBP image format, which is comprised of color\nand people (foreground) information. Our architecture is capable of considering\ntwo relevant aspects: spatial features extracted through convolutional layers\nfrom the RGBP images; and the temporal coherence of the problem, which is\nexploited by recurrent layers. We show that, through a supervised learning\napproach, the trained models are capable of predicting the people count with\nhigh accuracy. Additionally, we present and demonstrate that a straightforward\nmodification of the methodology is effective to exclude salespeople from the\npeople count. Comprehensive experiments were conducted to validate, evaluate\nand compare the proposed architecture. Results corroborated that LRCN-RetailNet\nremarkably outperforms both the previous RetailNet architecture, which was\nlimited to evaluating a single image per iteration; and a state-of-the-art\nneural network for object detection. Finally, computational performance\nexperiments confirmed that the entire methodology is effective to estimate\npeople count in real-time.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 23:25:37 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 16:21:12 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Massa", "Lucas", ""], ["Barbosa", "Adriano", ""], ["Oliveira", "Krerley", ""], ["Vieira", "Thales", ""]]}, {"id": "2004.09681", "submitter": "Evelyn Fan", "authors": "Yingruo Fan, Jacqueline C.K. Lam, Victor O.K. Li", "title": "Facial Action Unit Intensity Estimation via Semantic Correspondence\n  Learning with Dynamic Graph Convolution", "comments": "Accepted at AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensity estimation of facial action units (AUs) is challenging due to\nsubtle changes in the person's facial appearance. Previous approaches mainly\nrely on probabilistic models or predefined rules for modeling co-occurrence\nrelationships among AUs, leading to limited generalization. In contrast, we\npresent a new learning framework that automatically learns the latent\nrelationships of AUs via establishing semantic correspondences between feature\nmaps. In the heatmap regression-based network, feature maps preserve rich\nsemantic information associated with AU intensities and locations. Moreover,\nthe AU co-occurring pattern can be reflected by activating a set of feature\nchannels, where each channel encodes a specific visual pattern of AU. This\nmotivates us to model the correlation among feature channels, which implicitly\nrepresents the co-occurrence relationship of AU intensity levels. Specifically,\nwe introduce a semantic correspondence convolution (SCC) module to dynamically\ncompute the correspondences from deep and low resolution feature maps, and thus\nenhancing the discriminability of features. The experimental results\ndemonstrate the effectiveness and the superior performance of our method on two\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 23:55:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Fan", "Yingruo", ""], ["Lam", "Jacqueline C. K.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "2004.09694", "submitter": "Wei Zhu", "authors": "Wei Zhu, Haofu Liao, Wenbin Li, Weijian Li, Jiebo Luo", "title": "Alleviating the Incompatibility between Cross Entropy Loss and Episode\n  Training for Few-shot Skin Disease Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin disease classification from images is crucial to dermatological\ndiagnosis. However, identifying skin lesions involves a variety of aspects in\nterms of size, color, shape, and texture. To make matters worse, many\ncategories only contain very few samples, posing great challenges to\nconventional machine learning algorithms and even human experts. Inspired by\nthe recent success of Few-Shot Learning (FSL) in natural image classification,\nwe propose to apply FSL to skin disease identification to address the extreme\nscarcity of training sample problem. However, directly applying FSL to this\ntask does not work well in practice, and we find that the problem can be\nlargely attributed to the incompatibility between Cross Entropy (CE) and\nepisode training, which are both commonly used in FSL. Based on a detailed\nanalysis, we propose the Query-Relative (QR) loss, which proves superior to CE\nunder episode training and is closely related to recently proposed mutual\ninformation estimation. Moreover, we further strengthen the proposed QR loss\nwith a novel adaptive hard margin strategy. Comprehensive experiments validate\nthe effectiveness of the proposed FSL scheme and the possibility to diagnosis\nrare skin disease with a few labeled samples.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 00:57:11 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhu", "Wei", ""], ["Liao", "Haofu", ""], ["Li", "Wenbin", ""], ["Li", "Weijian", ""], ["Luo", "Jiebo", ""]]}, {"id": "2004.09695", "submitter": "Marco Bertini", "authors": "Federico Vaccaro, Marco Bertini, Tiberio Uricchio, Alberto Del Bimbo", "title": "Image Retrieval using Multi-scale CNN Features Pooling", "comments": "Accepted at ICMR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of image retrieval by learning images\nrepresentation based on the activations of a Convolutional Neural Network. We\npresent an end-to-end trainable network architecture that exploits a novel\nmulti-scale local pooling based on NetVLAD and a triplet mining procedure based\non samples difficulty to obtain an effective image representation. Extensive\nexperiments show that our approach is able to reach state-of-the-art results on\nthree standard datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 00:57:52 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 11:19:31 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Vaccaro", "Federico", ""], ["Bertini", "Marco", ""], ["Uricchio", "Tiberio", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2004.09722", "submitter": "Baichuan Huang", "authors": "Baichuan Huang, Hongwei Yi, Can Huang, Yijia He, Jingbin Liu, Xiao Liu", "title": "M^3VSNet: Unsupervised Multi-metric Multi-view Stereo Network", "comments": "Welcome to communicate with the author by the repo\n  https://github.com/whubaichuan/M3VSNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present Multi-view stereo (MVS) methods with supervised learning-based\nnetworks have an impressive performance comparing with traditional MVS methods.\nHowever, the ground-truth depth maps for training are hard to be obtained and\nare within limited kinds of scenarios. In this paper, we propose a novel\nunsupervised multi-metric MVS network, named M^3VSNet, for dense point cloud\nreconstruction without any supervision. To improve the robustness and\ncompleteness of point cloud reconstruction, we propose a novel multi-metric\nloss function that combines pixel-wise and feature-wise loss function to learn\nthe inherent constraints from different perspectives of matching\ncorrespondences. Besides, we also incorporate the normal-depth consistency in\nthe 3D point cloud format to improve the accuracy and continuity of the\nestimated depth maps. Experimental results show that M3VSNet establishes the\nstate-of-the-arts unsupervised method and achieves comparable performance with\nprevious supervised MVSNet on the DTU dataset and demonstrates the powerful\ngeneralization ability on the Tanks and Temples benchmark with effective\nimprovement. Our code is available at https://github.com/whubaichuan/M3VSNet.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:45:25 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 11:29:16 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Huang", "Baichuan", ""], ["Yi", "Hongwei", ""], ["Huang", "Can", ""], ["He", "Yijia", ""], ["Liu", "Jingbin", ""], ["Liu", "Xiao", ""]]}, {"id": "2004.09725", "submitter": "Bj\\\"orn L\\\"utjens", "authors": "Simona Santamaria, David Dao, Bj\\\"orn L\\\"utjens, Ce Zhang", "title": "TrueBranch: Metric Learning-based Verification of Forest Conservation\n  Projects", "comments": "*Authors have contributed equally. Published as Spotlight\n  Presentation at ICLR 2020 Workshop on Tackling Climate Change with Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International stakeholders increasingly invest in offsetting carbon\nemissions, for example, via issuing Payments for Ecosystem Services (PES) to\nforest conservation projects. Issuing trusted payments requires a transparent\nmonitoring, reporting, and verification (MRV) process of the ecosystem services\n(e.g., carbon stored in forests). The current MRV process, however, is either\ntoo expensive (on-ground inspection of forest) or inaccurate (satellite).\nRecent works propose low-cost and accurate MRV via automatically determining\nforest carbon from drone imagery, collected by the landowners. The automation\nof MRV, however, opens up the possibility that landowners report untruthful\ndrone imagery. To be robust against untruthful reporting, we propose\nTrueBranch, a metric learning-based algorithm that verifies the truthfulness of\ndrone imagery from forest conservation projects. TrueBranch aims to detect\nuntruthfully reported drone imagery by matching it with public satellite\nimagery. Preliminary results suggest that nominal distance metrics are not\nsufficient to reliably detect untruthfully reported imagery. TrueBranch\nleverages metric learning to create a feature embedding in which truthfully and\nuntruthfully collected imagery is easily distinguishable by distance\nthresholding.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:52:27 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Santamaria", "Simona", ""], ["Dao", "David", ""], ["L\u00fctjens", "Bj\u00f6rn", ""], ["Zhang", "Ce", ""]]}, {"id": "2004.09750", "submitter": "Yun Liu", "authors": "Yu Qiu and Yun Liu and Shijie Li and Jing Xu", "title": "MiniSeg: An Extremely Minimum Network for Efficient COVID-19\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid spread of the new pandemic, i.e., COVID-19, has severely threatened\nglobal health. Deep-learning-based computer-aided screening, e.g., COVID-19\ninfected CT area segmentation, has attracted much attention. However, the\npublicly available COVID-19 training data are limited, easily causing\noverfitting for traditional deep learning methods that are usually data-hungry\nwith millions of parameters. On the other hand, fast training/testing and low\ncomputational cost are also necessary for quick deployment and development of\nCOVID-19 screening systems, but traditional deep learning methods are usually\ncomputationally intensive. To address the above problems, we propose MiniSeg, a\nlightweight deep learning model for efficient COVID-19 segmentation. Compared\nwith traditional segmentation methods, MiniSeg has several significant\nstrengths: i) it only has 83K parameters and is thus not easy to overfit; ii)\nit has high computational efficiency and is thus convenient for practical\ndeployment; iii) it can be fast retrained by other users using their private\nCOVID-19 data for further improving performance. In addition, we build a\ncomprehensive COVID-19 segmentation benchmark for comparing MiniSeg to\ntraditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 04:51:37 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 06:28:10 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 14:47:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Qiu", "Yu", ""], ["Liu", "Yun", ""], ["Li", "Shijie", ""], ["Xu", "Jing", ""]]}, {"id": "2004.09754", "submitter": "Mang Tik Chiu", "authors": "Mang Tik Chiu, Xingqian Xu, Kai Wang, Jennifer Hobbs, Naira\n  Hovakimyan, Thomas S. Huang, Honghui Shi, Yunchao Wei, Zilong Huang,\n  Alexander Schwing, Robert Brunner, Ivan Dozier, Wyatt Dozier, Karen\n  Ghandilyan, David Wilson, Hyunseong Park, Junhee Kim, Sungho Kim, Qinghui\n  Liu, Michael C. Kampffmeyer, Robert Jenssen, Arnt B. Salberg, Alexandre\n  Barbosa, Rodrigo Trevisan, Bingchen Zhao, Shaozuo Yu, Siwei Yang, Yin Wang,\n  Hao Sheng, Xiao Chen, Jingyi Su, Ram Rajagopal, Andrew Ng, Van Thong Huynh,\n  Soo-Hyung Kim, In-Seop Na, Ujjwal Baid, Shubham Innani, Prasad Dutande,\n  Bhakti Baheti, Sanjay Talbar, Jianyu Tang", "title": "The 1st Agriculture-Vision Challenge: Methods and Results", "comments": "CVPR 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first Agriculture-Vision Challenge aims to encourage research in\ndeveloping novel and effective algorithms for agricultural pattern recognition\nfrom aerial images, especially for the semantic segmentation task associated\nwith our challenge dataset. Around 57 participating teams from various\ncountries compete to achieve state-of-the-art in aerial agriculture semantic\nsegmentation. The Agriculture-Vision Challenge Dataset was employed, which\ncomprises of 21,061 aerial and multi-spectral farmland images. This paper\nprovides a summary of notable methods and results in the challenge. Our\nsubmission server and leaderboard will continue to open for researchers that\nare interested in this challenge dataset and task; the link can be found here.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 05:02:31 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 17:24:31 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Chiu", "Mang Tik", ""], ["Xu", "Xingqian", ""], ["Wang", "Kai", ""], ["Hobbs", "Jennifer", ""], ["Hovakimyan", "Naira", ""], ["Huang", "Thomas S.", ""], ["Shi", "Honghui", ""], ["Wei", "Yunchao", ""], ["Huang", "Zilong", ""], ["Schwing", "Alexander", ""], ["Brunner", "Robert", ""], ["Dozier", "Ivan", ""], ["Dozier", "Wyatt", ""], ["Ghandilyan", "Karen", ""], ["Wilson", "David", ""], ["Park", "Hyunseong", ""], ["Kim", "Junhee", ""], ["Kim", "Sungho", ""], ["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael C.", ""], ["Jenssen", "Robert", ""], ["Salberg", "Arnt B.", ""], ["Barbosa", "Alexandre", ""], ["Trevisan", "Rodrigo", ""], ["Zhao", "Bingchen", ""], ["Yu", "Shaozuo", ""], ["Yang", "Siwei", ""], ["Wang", "Yin", ""], ["Sheng", "Hao", ""], ["Chen", "Xiao", ""], ["Su", "Jingyi", ""], ["Rajagopal", "Ram", ""], ["Ng", "Andrew", ""], ["Huynh", "Van Thong", ""], ["Kim", "Soo-Hyung", ""], ["Na", "In-Seop", ""], ["Baid", "Ujjwal", ""], ["Innani", "Shubham", ""], ["Dutande", "Prasad", ""], ["Baheti", "Bhakti", ""], ["Talbar", "Sanjay", ""], ["Tang", "Jianyu", ""]]}, {"id": "2004.09760", "submitter": "Hao Xue", "authors": "Hao Xue, Du. Q. Huynh and Mark Reynolds", "title": "Take a NAP: Non-Autoregressive Prediction for Pedestrian Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is a challenging task as there are three\nproperties of human movement behaviors which need to be addressed, namely, the\nsocial influence from other pedestrians, the scene constraints, and the\nmultimodal (multiroute) nature of predictions. Although existing methods have\nexplored these key properties, the prediction process of these methods is\nautoregressive. This means they can only predict future locations sequentially.\nIn this paper, we present NAP, a non-autoregressive method for trajectory\nprediction. Our method comprises specifically designed feature encoders and a\nlatent variable generator to handle the three properties above. It also has a\ntime-agnostic context generator and a time-specific context generator for\nnon-autoregressive prediction. Through extensive experiments that compare NAP\nagainst several recent methods, we show that NAP has state-of-the-art\ntrajectory prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 05:32:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Xue", "Hao", ""], ["Huynh", "Du. Q.", ""], ["Reynolds", "Mark", ""]]}, {"id": "2004.09769", "submitter": "Junshu Tang", "authors": "Junshu Tang, Zhiwen Shao, Lizhuang Ma", "title": "Fine-Grained Expression Manipulation via Structured Latent Space", "comments": null, "journal-ref": null, "doi": "10.1109/ICME46284.2020.9102852", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained facial expression manipulation is a challenging problem, as\nfine-grained expression details are difficult to be captured. Most existing\nexpression manipulation methods resort to discrete expression labels, which\nmainly edit global expressions and ignore the manipulation of fine details. To\ntackle this limitation, we propose an end-to-end expression-guided generative\nadversarial network (EGGAN), which utilizes structured latent codes and\ncontinuous expression labels as input to generate images with expected\nexpressions. Specifically, we adopt an adversarial autoencoder to map a source\nimage into a structured latent space. Then, given the source latent code and\nthe target expression label, we employ a conditional GAN to generate a new\nimage with the target expression. Moreover, we introduce a perceptual loss and\na multi-scale structural similarity loss to preserve identity and global shape\nduring generation. Extensive experiments show that our method can manipulate\nfine-grained expressions, and generate continuous intermediate expressions\nbetween source and target expressions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 06:18:34 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 08:11:12 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tang", "Junshu", ""], ["Shao", "Zhiwen", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2004.09776", "submitter": "Moritz Einfalt", "authors": "Moritz Einfalt, Rainer Lienhart", "title": "Decoupling Video and Human Motion: Towards Practical Event Detection in\n  Athlete Recordings", "comments": "Accepted at 2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of motion event detection in athlete\nrecordings from individual sports. In contrast to recent end-to-end approaches,\nwe propose to use 2D human pose sequences as an intermediate representation\nthat decouples human motion from the raw video information. Combined with\ndomain-adapted athlete tracking, we describe two approaches to event detection\non pose sequences and evaluate them in complementary domains: swimming and\nathletics. For swimming, we show how robust decision rules on pose statistics\ncan detect different motion events during swim starts, with a F1 score of over\n91% despite limited data. For athletics, we use a convolutional sequence model\nto infer stride-related events in long and triple jump recordings, leading to\nhighly accurate detections with 96% in F1 score at only +/- 5ms temporal\ndeviation. Our approach is not limited to these domains and shows the\nflexibility of pose-based motion event detection.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 07:06:12 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 15:52:25 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Einfalt", "Moritz", ""], ["Lienhart", "Rainer", ""]]}, {"id": "2004.09788", "submitter": "Jinyoung Kim Dr.", "authors": "Jinyoung Kim, Remi Patriat, Jordan Kaplan, Oren Solomon, Noam Harel", "title": "Deep Cerebellar Nuclei Segmentation via Semi-Supervised Deep\n  Context-Aware Learning from 7T Diffusion MRI", "comments": "56 pages (one column), 13 figures, 5 tables, supplementary materials,\n  Accepted for publication in IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep cerebellar nuclei are a key structure of the cerebellum that are\ninvolved in processing motor and sensory information. It is thus a crucial step\nto accurately segment deep cerebellar nuclei for the understanding of the\ncerebellum system and its utility in deep brain stimulation treatment. However,\nit is challenging to clearly visualize such small nuclei under standard\nclinical magnetic resonance imaging (MRI) protocols and therefore precise\nsegmentation is not feasible. Recent advances in 7 Tesla (T) MRI technology and\ngreat potential of deep neural networks facilitate automatic patient-specific\nsegmentation. In this paper, we propose a novel deep learning framework\n(referred to as DCN-Net) for fast, accurate, and robust patient-specific\nsegmentation of deep cerebellar dentate and interposed nuclei on 7T diffusion\nMRI. DCN-Net effectively encodes contextual information on the patch images\nwithout consecutive pooling operations and adding complexity via proposed\ndilated dense blocks. During the end-to-end training, label probabilities of\ndentate and interposed nuclei are independently learned with a hybrid loss,\nhandling highly imbalanced data. Finally, we utilize self-training strategies\nto cope with the problem of limited labeled data. To this end, auxiliary\ndentate and interposed nuclei labels are created on unlabeled data by using\nDCN-Net trained on manual labels. We validate the proposed framework using 7T\nB0 MRIs from 60 subjects. Experimental results demonstrate that DCN-Net\nprovides better segmentation than atlas-based deep cerebellar nuclei\nsegmentation tools and other state-of-the-art deep neural networks in terms of\naccuracy and consistency. We further prove the effectiveness of the proposed\ncomponents within DCN-Net in dentate and interposed nuclei segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 07:30:07 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 06:26:20 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 01:47:33 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Kim", "Jinyoung", ""], ["Patriat", "Remi", ""], ["Kaplan", "Jordan", ""], ["Solomon", "Oren", ""], ["Harel", "Noam", ""]]}, {"id": "2004.09795", "submitter": "Long Chen", "authors": "Long Chen, Martin Strauch, Matthias Daub, Xiaochen Jiang, Marcus\n  Jansen, Hans-Georg Luigs, Susanne Schultz-Kuhlmann, Stefan Kr\\\"ussel, Dorif\n  Merhof", "title": "A CNN Framenwork Based on Line Annotations for Detecting Nematodes in\n  Microscopic Images", "comments": "ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant parasitic nematodes cause damage to crop plants on a global scale.\nRobust detection on image data is a prerequisite for monitoring such nematodes,\nas well as for many biological studies involving the nematode C. elegans, a\ncommon model organism. Here, we propose a framework for detecting worm-shaped\nobjects in microscopic images that is based on convolutional neural networks\n(CNNs). We annotate nematodes with curved lines along the body, which is more\nsuitable for worm-shaped objects than bounding boxes. The trained model\npredicts worm skeletons and body endpoints. The endpoints serve to untangle the\nskeletons from which segmentation masks are reconstructed by estimating the\nbody width at each location along the skeleton. With light-weight backbone\nnetworks, we achieve 75.85 % precision, 73.02 % recall on a potato cyst\nnematode data set and 84.20 % precision, 85.63 % recall on a public C. elegans\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 07:48:02 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Long", ""], ["Strauch", "Martin", ""], ["Daub", "Matthias", ""], ["Jiang", "Xiaochen", ""], ["Jansen", "Marcus", ""], ["Luigs", "Hans-Georg", ""], ["Schultz-Kuhlmann", "Susanne", ""], ["Kr\u00fcssel", "Stefan", ""], ["Merhof", "Dorif", ""]]}, {"id": "2004.09802", "submitter": "Qi Li", "authors": "Qi Li, Hanlin Mo, Jinghan Zhao, Hongxiang Hao and Hua Li", "title": "Spatio-Temporal Dual Affine Differential Invariant for Skeleton-based\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dynamics of human skeletons have significant information for the task of\naction recognition. The similarity between trajectories of corresponding joints\nis an indicating feature of the same action, while this similarity may subject\nto some distortions that can be modeled as the combination of spatial and\ntemporal affine transformations. In this work, we propose a novel feature\ncalled spatio-temporal dual affine differential invariant (STDADI).\nFurthermore, in order to improve the generalization ability of neural networks,\na channel augmentation method is proposed. On the large scale action\nrecognition dataset NTU-RGB+D, and its extended version NTU-RGB+D 120, it\nachieves remarkable improvements over previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:00:45 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Qi", ""], ["Mo", "Hanlin", ""], ["Zhao", "Jinghan", ""], ["Hao", "Hongxiang", ""], ["Li", "Hua", ""]]}, {"id": "2004.09803", "submitter": "Subhashis Banerjee", "authors": "Arpan Mangal, Surya Kalia, Harish Rajgopal, Krithika Rangarajan, Vinay\n  Namboodiri, Subhashis Banerjee, Chetan Arora", "title": "CovidAID: COVID-19 Detection Using Chest X-Ray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential increase in COVID-19 patients is overwhelming healthcare\nsystems across the world. With limited testing kits, it is impossible for every\npatient with respiratory illness to be tested using conventional techniques\n(RT-PCR). The tests also have long turn-around time, and limited sensitivity.\nDetecting possible COVID-19 infections on Chest X-Ray may help quarantine high\nrisk patients while test results are awaited. X-Ray machines are already\navailable in most healthcare systems, and with most modern X-Ray systems\nalready digitized, there is no transportation time involved for the samples\neither. In this work we propose the use of chest X-Ray to prioritize the\nselection of patients for further RT-PCR testing. This may be useful in an\ninpatient setting where the present systems are struggling to decide whether to\nkeep the patient in the ward along with other patients or isolate them in\nCOVID-19 areas. It would also help in identifying patients with high likelihood\nof COVID with a false negative RT-PCR who would need repeat testing. Further,\nwe propose the use of modern AI techniques to detect the COVID-19 patients\nusing X-Ray images in an automated manner, particularly in settings where\nradiologists are not available, and help make the proposed testing technology\nscalable. We present CovidAID: COVID-19 AI Detector, a novel deep neural\nnetwork based model to triage patients for appropriate testing. On the publicly\navailable covid-chestxray-dataset [2], our model gives 90.5% accuracy with 100%\nsensitivity (recall) for the COVID-19 infection. We significantly improve upon\nthe results of Covid-Net [10] on the same dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:02:52 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Mangal", "Arpan", ""], ["Kalia", "Surya", ""], ["Rajgopal", "Harish", ""], ["Rangarajan", "Krithika", ""], ["Namboodiri", "Vinay", ""], ["Banerjee", "Subhashis", ""], ["Arora", "Chetan", ""]]}, {"id": "2004.09805", "submitter": "Anirudh Som", "authors": "Hongjun Choi, Anirudh Som and Pavan Turaga", "title": "AMC-Loss: Angular Margin Contrastive Loss for Improved Explainability in\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning architectures for classification problems involve the\ncross-entropy loss sometimes assisted with auxiliary loss functions like center\nloss, contrastive loss and triplet loss. These auxiliary loss functions\nfacilitate better discrimination between the different classes of interest.\nHowever, recent studies hint at the fact that these loss functions do not take\ninto account the intrinsic angular distribution exhibited by the low-level and\nhigh-level feature representations. This results in less compactness between\nsamples from the same class and unclear boundary separations between data\nclusters of different classes. In this paper, we address this issue by\nproposing the use of geometric constraints, rooted in Riemannian geometry.\nSpecifically, we propose Angular Margin Contrastive Loss (AMC-Loss), a new loss\nfunction to be used along with the traditional cross-entropy loss. The AMC-Loss\nemploys the discriminative angular distance metric that is equivalent to\ngeodesic distance on a hypersphere manifold such that it can serve a clear\ngeometric interpretation. We demonstrate the effectiveness of AMC-Loss by\nproviding quantitative and qualitative results. We find that although the\nproposed geometrically constrained loss-function improves quantitative results\nmodestly, it has a qualitatively surprisingly beneficial effect on increasing\nthe interpretability of deep-net decisions as seen by the visual explanations\ngenerated by techniques such as the Grad-CAM. Our code is available at\nhttps://github.com/hchoi71/AMC-Loss.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:03:14 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Choi", "Hongjun", ""], ["Som", "Anirudh", ""], ["Turaga", "Pavan", ""]]}, {"id": "2004.09811", "submitter": "Yuanxin Ye", "authors": "Bai Zhu, Yuanxin Ye, Chao Yang, Liang Zhou, Huiyu Liu, Yungang Cao", "title": "Fast and Robust Registration of Aerial Images and LiDAR data Based on\n  Structrual Features and 3D Phase Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-Registration of aerial imagery and Light Detection and Ranging (LiDAR)\ndata is quilt challenging because the different imaging mechanism causes\nsignificant geometric and radiometric distortions between such data. To tackle\nthe problem, this paper proposes an automatic registration method based on\nstructural features and three-dimension (3D) phase correlation. In the proposed\nmethod, the LiDAR point cloud data is first transformed into the intensity map,\nwhich is used as the reference image. Then, we employ the Fast operator to\nextract uniformly distributed interest points in the aerial image by a\npartition strategy and perform a local geometric correction by using the\ncollinearity equation to eliminate scale and rotation difference between\nimages. Subsequently, a robust structural feature descriptor is build based on\ndense gradient features, and the 3D phase correlation is used to detect control\npoints (CPs) between aerial images and LiDAR data in the frequency domain,\nwhere the image matching is accelerated by the 3D Fast Fourier Transform (FFT).\nFinally, the obtained CPs are employed to correct the exterior orientation\nelements, which is used to achieve co-registration of aerial images and LiDAR\ndata. Experiments with two datasets of aerial images and LiDAR data show that\nthe proposed method is much faster and more robust than state of the art\nmethods\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:19:56 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhu", "Bai", ""], ["Ye", "Yuanxin", ""], ["Yang", "Chao", ""], ["Zhou", "Liang", ""], ["Liu", "Huiyu", ""], ["Cao", "Yungang", ""]]}, {"id": "2004.09821", "submitter": "Long Chen", "authors": "Long Chen, Martin Strauch, Dorit Merhof", "title": "Instance Segmentation of Biomedical Images with an Object-aware\n  Embedding Learned with Local Constraints", "comments": "MICCAI 2019", "journal-ref": "vol 11764, pp 451-459, MICCAI 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic instance segmentation is a problem that occurs in many biomedical\napplications. State-of-the-art approaches either perform semantic segmentation\nor refine object bounding boxes obtained from detection methods. Both suffer\nfrom crowded objects to varying degrees, merging adjacent objects or\nsuppressing a valid object. In this work, we assign an embedding vector to each\npixel through a deep neural network. The network is trained to output embedding\nvectors of similar directions for pixels from the same object, while adjacent\nobjects are orthogonal in the embedding space, which effectively avoids the\nfusion of objects in a crowd. Our method yields state-of-the-art results even\nwith a light-weighted backbone network on a cell segmentation (BBBC006 +\nDSB2018) and a leaf segmentation data set (CVPPP2017). The code and model\nweights are public available.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:33:29 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Long", ""], ["Strauch", "Martin", ""], ["Merhof", "Dorit", ""]]}, {"id": "2004.09829", "submitter": "Jihua Zhu", "authors": "Jihua Zhu, Jie Hu, Huimin Lu, Badong Chen, Zhongyu Li", "title": "Robust Motion Averaging under Maximum Correntropy Criterion", "comments": null, "journal-ref": "IEEE ICRA 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the motion averaging method has been introduced as an effective\nmeans to solve the multi-view registration problem. This method aims to recover\nglobal motions from a set of relative motions, where the original method is\nsensitive to outliers due to using the Frobenius norm error in the\noptimization. Accordingly, this paper proposes a novel robust motion averaging\nmethod based on the maximum correntropy criterion (MCC). Specifically, the\ncorrentropy measure is used instead of utilizing Frobenius norm error to\nimprove the robustness of motion averaging against outliers. According to the\nhalf-quadratic technique, the correntropy measure based optimization problem\ncan be solved by the alternating minimization procedure, which includes\noperations of weight assignment and weighted motion averaging. Further, we\ndesign a selection strategy of adaptive kernel width to take advantage of\ncorrentropy. Experimental results on benchmark data sets illustrate that the\nnew method has superior performance on accuracy and robustness for multi-view\nregistration.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:52:38 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 09:07:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhu", "Jihua", ""], ["Hu", "Jie", ""], ["Lu", "Huimin", ""], ["Chen", "Badong", ""], ["Li", "Zhongyu", ""]]}, {"id": "2004.09832", "submitter": "Long Chen", "authors": "Long Chen, Dorit Merhof", "title": "MixNet: Multi-modality Mix Network for Brain Segmentation", "comments": "BrainLes, MICCAI 2018", "journal-ref": "pp 267-376, MICCAI 2018 Brainlesion: Glioma, Multiple Sclerosis,\n  Stroke and Traumatic Brain Injuries", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated brain structure segmentation is important to many clinical\nquantitative analysis and diagnoses. In this work, we introduce MixNet, a 2D\nsemantic-wise deep convolutional neural network to segment brain structure in\nmulti-modality MRI images. The network is composed of our modified deep\nresidual learning units. In the unit, we replace the traditional convolution\nlayer with the dilated convolutional layer, which avoids the use of pooling\nlayers and deconvolutional layers, reducing the number of network parameters.\nFinal predictions are made by aggregating information from multiple scales and\nmodalities. A pyramid pooling module is used to capture spatial information of\nthe anatomical structures at the output end. In addition, we test three\narchitectures (MixNetv1, MixNetv2 and MixNetv3) which fuse the modalities\ndifferently to see the effect on the results. Our network achieves the\nstate-of-the-art performance. MixNetv2 was submitted to the MRBrainS challenge\nat MICCAI 2018 and won the 3rd place in the 3-label task. On the MRBrainS2018\ndataset, which includes subjects with a variety of pathologies, the overall DSC\n(Dice Coefficient) of 84.7% (gray matter), 87.3% (white matter) and 83.4%\n(cerebrospinal fluid) were obtained with only 7 subjects as training data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:55:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Long", ""], ["Merhof", "Dorit", ""]]}, {"id": "2004.09834", "submitter": "Gaetano Scebba", "authors": "Gaetano Scebba, Giulia Da Poian, and Walter Karlen", "title": "Multispectral Video Fusion for Non-contact Monitoring of Respiratory\n  Rate and Apnea", "comments": null, "journal-ref": null, "doi": "10.1109/TBME.2020.2993649", "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous monitoring of respiratory activity is desirable in many clinical\napplications to detect respiratory events. Non-contact monitoring of\nrespiration can be achieved with near- and far-infrared spectrum cameras.\nHowever, current technologies are not sufficiently robust to be used in\nclinical applications. For example, they fail to estimate an accurate\nrespiratory rate (RR) during apnea. We present a novel algorithm based on\nmultispectral data fusion that aims at estimating RR also during apnea. The\nalgorithm independently addresses the RR estimation and apnea detection tasks.\nRespiratory information is extracted from multiple sources and fed into an RR\nestimator and an apnea detector whose results are fused into a final\nrespiratory activity estimation. We evaluated the system retrospectively using\ndata from 30 healthy adults who performed diverse controlled breathing tasks\nwhile lying supine in a dark room and reproduced central and obstructive apneic\nevents. Combining multiple respiratory information from multispectral cameras\nimproved the root mean square error (RMSE) accuracy of the RR estimation from\nup to 4.64 monospectral data down to 1.60 breaths/min. The median F1 scores for\nclassifying obstructive (0.75 to 0.86) and central apnea (0.75 to 0.93) also\nimproved. Furthermore, the independent consideration of apnea detection led to\na more robust system (RMSE of 4.44 vs. 7.96 breaths/min). Our findings may\nrepresent a step towards the use of cameras for vital sign monitoring in\nmedical applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:07:09 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Scebba", "Gaetano", ""], ["Da Poian", "Giulia", ""], ["Karlen", "Walter", ""]]}, {"id": "2004.09845", "submitter": "Xueying Shi", "authors": "Xueying Shi, Yueming Jin, Qi Dou, Pheng-Ann Heng", "title": "LRTD: Long-Range Temporal Dependency based Active Learning for Surgical\n  Workflow Recognition", "comments": "Accepted as a conference paper in IPCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical workflow recognition in video is an essentially\nfundamental yet challenging problem for developing computer-assisted and\nrobotic-assisted surgery. Existing approaches with deep learning have achieved\nremarkable performance on analysis of surgical videos, however, heavily relying\non large-scale labelled datasets. Unfortunately, the annotation is not often\navailable in abundance, because it requires the domain knowledge of surgeons.\nIn this paper, we propose a novel active learning method for cost-effective\nsurgical video analysis. Specifically, we propose a non-local recurrent\nconvolutional network (NL-RCNet), which introduces non-local block to capture\nthe long-range temporal dependency (LRTD) among continuous frames. We then\nformulate an intra-clip dependency score to represent the overall dependency\nwithin this clip. By ranking scores among clips in unlabelled data pool, we\nselect the clips with weak dependencies to annotate, which indicates the most\ninformative ones to better benefit network training. We validate our approach\non a large surgical video dataset (Cholec80) by performing surgical workflow\nrecognition task. By using our LRTD based selection strategy, we can outperform\nother state-of-the-art active learning methods. Using only up to 50% of\nsamples, our approach can exceed the performance of full-data training.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:21:22 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 05:57:47 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Shi", "Xueying", ""], ["Jin", "Yueming", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2004.09862", "submitter": "Pengcheng Wang", "authors": "Pengcheng Wang, Zihao Wang, Zhilong Ji, Xiao Liu, Songfan Yang and\n  Zhongqin Wu", "title": "TAL EmotioNet Challenge 2020 Rethinking the Model Chosen Problem in\n  Multi-Task Learning", "comments": "4 pages, 2 figures, 3 tables, CVPRW2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces our approach to the EmotioNet Challenge 2020. We pose\nthe AU recognition problem as a multi-task learning problem, where the\nnon-rigid facial muscle motion (mainly the first 17 AUs) and the rigid head\nmotion (the last 6 AUs) are modeled separately. The co-occurrence of the\nexpression features and the head pose features are explored. We observe that\ndifferent AUs converge at various speed. By choosing the optimal checkpoint for\neach AU, the recognition results are improved. We are able to obtain a final\nscore of 0.746 in validation set and 0.7306 in the test set of the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:39:38 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wang", "Pengcheng", ""], ["Wang", "Zihao", ""], ["Ji", "Zhilong", ""], ["Liu", "Xiao", ""], ["Yang", "Songfan", ""], ["Wu", "Zhongqin", ""]]}, {"id": "2004.09870", "submitter": "Tashin Ahmed", "authors": "Tashin Ahmed, Chowdhury Rafeed Rahman, Md. Faysal Mahmud Abid", "title": "Rice grain disease identification using dual phase convolutional neural\n  network based system aimed at small dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Convolutional neural networks (CNNs) are widely used for plant\ndisease detection, they require a large number of training samples when dealing\nwith wide variety of heterogeneous background. In this work, a CNN based dual\nphase method has been proposed which can work effectively on small rice grain\ndisease dataset with heterogeneity. At the first phase, Faster RCNN method is\napplied for cropping out the significant portion (rice grain) from the image.\nThis initial phase results in a secondary dataset of rice grains devoid of\nheterogeneous background. Disease classification is performed on such derived\nand simplified samples using CNN architecture. Comparison of the dual phase\napproach with straight forward application of CNN on the small grain dataset\nshows the effectiveness of the proposed method which provides a 5 fold cross\nvalidation accuracy of 88.07%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:56:27 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 18:42:32 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ahmed", "Tashin", ""], ["Rahman", "Chowdhury Rafeed", ""], ["Abid", "Md. Faysal Mahmud", ""]]}, {"id": "2004.09927", "submitter": "Nikolay Falaleev", "authors": "Roman Voeikov, Nikolay Falaleev and Ruslan Baikulov", "title": "TTNet: Real-time temporal and spatial video analysis of table tennis", "comments": "6th International Workshop on Computer Vision in Sports (CVsports) at\n  CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network TTNet aimed at real-time processing of\nhigh-resolution table tennis videos, providing both temporal (events spotting)\nand spatial (ball detection and semantic segmentation) data. This approach\ngives core information for reasoning score updates by an auto-referee system.\n  We also publish a multi-task dataset OpenTTGames with videos of table tennis\ngames in 120 fps labeled with events, semantic segmentation masks, and ball\ncoordinates for evaluation of multi-task approaches, primarily oriented on\nspotting of quick events and small objects tracking. TTNet demonstrated 97.0%\naccuracy in game events spotting along with 2 pixels RMSE in ball detection\nwith 97.5% accuracy on the test part of the presented dataset.\n  The proposed network allows the processing of downscaled full HD videos with\ninference time below 6 ms per input tensor on a machine with a single\nconsumer-grade GPU. Thus, we are contributing to the development of real-time\nmulti-task deep learning applications and presenting approach, which is\npotentially capable of substituting manual data collection by sports scouts,\nproviding support for referees' decision-making, and gathering extra\ninformation about the game process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 11:57:51 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Voeikov", "Roman", ""], ["Falaleev", "Nikolay", ""], ["Baikulov", "Ruslan", ""]]}, {"id": "2004.09965", "submitter": "Guy Shacht", "authors": "Guy Shacht, Sharon Fogel, Dov Danon, Daniel Cohen-Or and Ilya\n  Leizerson", "title": "Single Pair Cross-Modality Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-visual imaging sensors are widely used in the industry for different\npurposes. Those sensors are more expensive than visual (RGB) sensors, and\nusually produce images with lower resolution. To this end, Cross-Modality\nSuper-Resolution methods were introduced, where an RGB image of a\nhigh-resolution assists in increasing the resolution of the low-resolution\nmodality. However, fusing images from different modalities is not a trivial\ntask; the output must be artifact-free and remain loyal to the characteristics\nof the target modality. Moreover, the input images are never perfectly aligned,\nwhich results in further artifacts during the fusion process.\n  We present CMSR, a deep network for Cross-Modality Super-Resolution, which\nunlike previous methods, is designed to deal with weakly aligned images. The\nnetwork is trained on the two input images only, learns their internal\nstatistics and correlations, and applies them to up-sample the target modality.\nCMSR contains an internal transformer that is trained on-the-fly together with\nthe up-sampling process itself, without explicit supervision. We show that CMSR\nsucceeds to increase the resolution of the input image, gaining valuable\ninformation from its RGB counterpart, yet in a conservative way, without\nintroducing artifacts or irrelevant details.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:57:51 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:54:55 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 09:42:09 GMT"}, {"version": "v4", "created": "Fri, 22 Jan 2021 16:17:27 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Shacht", "Guy", ""], ["Fogel", "Sharon", ""], ["Danon", "Dov", ""], ["Cohen-Or", "Daniel", ""], ["Leizerson", "Ilya", ""]]}, {"id": "2004.09989", "submitter": "Renato Baptista", "authors": "Renato Baptista, Alexandre Saint, Kassem Al Ismaeil, Djamila Aouada", "title": "Towards Generalization of 3D Human Pose Estimation In The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose 3DBodyTex.Pose, a dataset that addresses the task\nof 3D human pose estimation in-the-wild. Generalization to in-the-wild images\nremains limited due to the lack of adequate datasets. Existent ones are usually\ncollected in indoor controlled environments where motion capture systems are\nused to obtain the 3D ground-truth annotations of humans. 3DBodyTex.Pose offers\nhigh quality and rich data containing 405 different real subjects in various\nclothing and poses, and 81k image samples with ground-truth 2D and 3D pose\nannotations. These images are generated from 200 viewpoints among which 70\nchallenging extreme viewpoints. This data was created starting from high\nresolution textured 3D body scans and by incorporating various realistic\nbackgrounds. Retraining a state-of-the-art 3D pose estimation approach using\ndata augmented with 3DBodyTex.Pose showed promising improvement in the overall\nperformance, and a sensible decrease in the per joint position error when\ntesting on challenging viewpoints. The 3DBodyTex.Pose is expected to offer the\nresearch community with new possibilities for generalizing 3D pose estimation\nfrom monocular in-the-wild images.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:31:58 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Baptista", "Renato", ""], ["Saint", "Alexandre", ""], ["Ismaeil", "Kassem Al", ""], ["Aouada", "Djamila", ""]]}, {"id": "2004.09995", "submitter": "Zhongpai Gao", "authors": "Zhongpai Gao, Junchi Yan, Guangtao Zhai, Juyong Zhang, Yiyan Yang,\n  Xiaokang Yang", "title": "Learning Local Neighboring Structure for Robust 3D Shape Representation", "comments": null, "journal-ref": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh is a powerful data structure for 3D shapes. Representation learning for\n3D meshes is important in many computer vision and graphics applications. The\nrecent success of convolutional neural networks (CNNs) for structured data\n(e.g., images) suggests the value of adapting insight from CNN for 3D shapes.\nHowever, 3D shape data are irregular since each node's neighbors are unordered.\nVarious graph neural networks for 3D shapes have been developed with isotropic\nfilters or predefined local coordinate systems to overcome the node\ninconsistency on graphs. However, isotropic filters or predefined local\ncoordinate systems limit the representation power. In this paper, we propose a\nlocal structure-aware anisotropic convolutional operation (LSA-Conv) that\nlearns adaptive weighting matrices for each node according to the local\nneighboring structure and performs shared anisotropic filters. In fact, the\nlearnable weighting matrix is similar to the attention matrix in the random\nsynthesizer -- a new Transformer model for natural language processing (NLP).\nComprehensive experiments demonstrate that our model produces significant\nimprovement in 3D shape reconstruction compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:40:03 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 16:39:17 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 13:32:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gao", "Zhongpai", ""], ["Yan", "Junchi", ""], ["Zhai", "Guangtao", ""], ["Zhang", "Juyong", ""], ["Yang", "Yiyan", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2004.10016", "submitter": "Mohammad Reza Loghmani", "authors": "Mohammad Reza Loghmani, Luca Robbiano, Mirco Planamente, Kiru Park,\n  Barbara Caputo and Markus Vincze", "title": "Unsupervised Domain Adaptation through Inter-modal Rotation for RGB-D\n  Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (DA) exploits the supervision of a label-rich\nsource dataset to make predictions on an unlabeled target dataset by aligning\nthe two data distributions. In robotics, DA is used to take advantage of\nautomatically generated synthetic data, that come with \"free\" annotation, to\nmake effective predictions on real data. However, existing DA methods are not\ndesigned to cope with the multi-modal nature of RGB-D data, which are widely\nused in robotic vision. We propose a novel RGB-D DA method that reduces the\nsynthetic-to-real domain shift by exploiting the inter-modal relation between\nthe RGB and depth image. Our method consists of training a convolutional neural\nnetwork to solve, in addition to the main recognition task, the pretext task of\npredicting the relative rotation between the RGB and depth image. To evaluate\nour method and encourage further research in this area, we define two benchmark\ndatasets for object categorization and instance recognition. With extensive\nexperiments, we show the benefits of leveraging the inter-modal relations for\nRGB-D DA.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:53:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Loghmani", "Mohammad Reza", ""], ["Robbiano", "Luca", ""], ["Planamente", "Mirco", ""], ["Park", "Kiru", ""], ["Caputo", "Barbara", ""], ["Vincze", "Markus", ""]]}, {"id": "2004.10024", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Haofu Liao, Lele Chen, Wei Xiong, Tianlang Chen, Jiebo\n  Luo", "title": "Example-Guided Image Synthesis across Arbitrary Scenes using Masked\n  Spatial-Channel Attention and Self-Supervision", "comments": "24 pages. arXiv admin note: substantial text overlap with\n  arXiv:1911.12362", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Example-guided image synthesis has recently been attempted to synthesize an\nimage from a semantic label map and an exemplary image. In the task, the\nadditional exemplar image provides the style guidance that controls the\nappearance of the synthesized output. Despite the controllability advantage,\nthe existing models are designed on datasets with specific and roughly aligned\nobjects. In this paper, we tackle a more challenging and general task, where\nthe exemplar is an arbitrary scene image that is semantically different from\nthe given label map. To this end, we first propose a Masked Spatial-Channel\nAttention (MSCA) module which models the correspondence between two arbitrary\nscenes via efficient decoupled attention. Next, we propose an end-to-end\nnetwork for joint global and local feature alignment and synthesis. Finally, we\npropose a novel self-supervision task to enable training. Experiments on the\nlarge-scale and more diverse COCO-stuff dataset show significant improvements\nover the existing methods. Moreover, our approach provides interpretability and\ncan be readily extended to other content manipulation tasks including style and\nspatial interpolation or extrapolation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 18:17:40 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zheng", "Haitian", ""], ["Liao", "Haofu", ""], ["Chen", "Lele", ""], ["Xiong", "Wei", ""], ["Chen", "Tianlang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2004.10043", "submitter": "Shurun Wang", "authors": "Shurun Wang, Shiqi Wang, Wenhan Yang, Xinfeng Zhang, Shanshe Wang,\n  Siwei Ma, Wen Gao", "title": "Towards Analysis-friendly Face Representation with Scalable Feature and\n  Texture Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It plays a fundamental role to compactly represent the visual information\ntowards the optimization of the ultimate utility in myriad visual data centered\napplications. With numerous approaches proposed to efficiently compress the\ntexture and visual features serving human visual perception and machine\nintelligence respectively, much less work has been dedicated to studying the\ninteractions between them. Here we investigate the integration of feature and\ntexture compression, and show that a universal and collaborative visual\ninformation representation can be achieved in a hierarchical way. In\nparticular, we study the feature and texture compression in a scalable coding\nframework, where the base layer serves as the deep learning feature and\nenhancement layer targets to perfectly reconstruct the texture. Based on the\nstrong generative capability of deep neural networks, the gap between the base\nfeature layer and enhancement layer is further filled with the feature level\ntexture reconstruction, aiming to further construct texture representation from\nfeature. As such, the residuals between the original and reconstructed texture\ncould be further conveyed in the enhancement layer. To improve the efficiency\nof the proposed framework, the base layer neural network is trained in a\nmulti-task manner such that the learned features enjoy both high quality\nreconstruction and high accuracy analysis. We further demonstrate the framework\nand optimization strategies in face image compression, and promising coding\nperformance has been achieved in terms of both rate-fidelity and rate-accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:32:49 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 16:40:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Shurun", ""], ["Wang", "Shiqi", ""], ["Yang", "Wenhan", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2004.10068", "submitter": "Shenghan Wang", "authors": "Shenghan Wang, Yipeng Liu, Lanlan Feng, Ce Zhu", "title": "Frequency-Weighted Robust Tensor Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust tensor principal component analysis (RTPCA) can separate the low-rank\ncomponent and sparse component from multidimensional data, which has been used\nsuccessfully in several image applications. Its performance varies with\ndifferent kinds of tensor decompositions, and the tensor singular value\ndecomposition (t-SVD) is a popularly selected one. The standard t-SVD takes the\ndiscrete Fourier transform to exploit the residual in the 3rd mode in the\ndecomposition. When minimizing the tensor nuclear norm related to t-SVD, all\nthe frontal slices in frequency domain are optimized equally. In this paper, we\nincorporate frequency component analysis into t-SVD to enhance the RTPCA\nperformance. Specially, different frequency bands are unequally weighted with\nrespect to the corresponding physical meanings, and the frequency-weighted\ntensor nuclear norm can be obtained. Accordingly we rigorously deduce the\nfrequency-weighted tensor singular value threshold operator, and apply it for\nlow rank approximation subproblem in RTPCA. The newly obtained\nfrequency-weighted RTPCA can be solved by alternating direction method of\nmultipliers, and it is the first time that frequency analysis is taken in\ntensor principal component analysis. Numerical experiments on synthetic 3D\ndata, color image denoising and background modeling verify that the proposed\nmethod outperforms the state-of-the-art algorithms both in accuracy and\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:58:21 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:06:22 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Shenghan", ""], ["Liu", "Yipeng", ""], ["Feng", "Lanlan", ""], ["Zhu", "Ce", ""]]}, {"id": "2004.10076", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan and Erik B Dam", "title": "Tensor Networks for Medical Image Classification", "comments": "Accepted for publication at International Conference on Medical\n  Imaging with Deep Learning (MIDL), 2020. Reviews on Openreview here:\n  https://openreview.net/forum?id=jjk6bxk07G", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of machine learning tools like neural networks\nacross several domains, interesting connections and comparisons to concepts\nfrom other domains are coming to light. In this work, we focus on the class of\nTensor Networks, which has been a work horse for physicists in the last two\ndecades to analyse quantum many-body systems. Building on the recent interest\nin tensor networks for machine learning, we extend the Matrix Product State\ntensor networks (which can be interpreted as linear classifiers operating in\nexponentially high dimensional spaces) to be useful in medical image analysis\ntasks. We focus on classification problems as a first step where we motivate\nthe use of tensor networks and propose adaptions for 2D images using classical\nimage domain concepts such as local orderlessness of images. With the proposed\nlocally orderless tensor network model (LoTeNet), we show that tensor networks\nare capable of attaining performance that is comparable to state-of-the-art\ndeep learning methods. We evaluate the model on two publicly available medical\nimaging datasets and show performance improvements with fewer model\nhyperparameters and lesser computational resources compared to relevant\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:02:58 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Dam", "Erik B", ""]]}, {"id": "2004.10078", "submitter": "Zhonghao Zhang", "authors": "Zhonghao Zhang, Yipeng Liu, Jiani Liu, Fei Wen, Ce Zhu", "title": "AMP-Net: Denoising based Deep Unfolding for Compressive Image Sensing", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2021", "doi": "10.1109/TIP.2020.3044472", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most compressive sensing (CS) reconstruction methods can be divided into two\ncategories, i.e. model-based methods and classical deep network methods. By\nunfolding the iterative optimization algorithm for model-based methods onto\nnetworks, deep unfolding methods have the good interpretation of model-based\nmethods and the high speed of classical deep network methods. In this paper, to\nsolve the visual image CS problem, we propose a deep unfolding model dubbed\nAMP-Net. Rather than learning regularization terms, it is established by\nunfolding the iterative denoising process of the well-known approximate message\npassing algorithm. Furthermore, AMP-Net integrates deblocking modules in order\nto eliminate the blocking artifacts that usually appear in CS of visual images.\nIn addition, the sampling matrix is jointly trained with other network\nparameters to enhance the reconstruction performance. Experimental results show\nthat the proposed AMP-Net has better reconstruction accuracy than other\nstate-of-the-art methods with high reconstruction speed and a small number of\nnetwork parameters.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:04:17 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 02:59:40 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhang", "Zhonghao", ""], ["Liu", "Yipeng", ""], ["Liu", "Jiani", ""], ["Wen", "Fei", ""], ["Zhu", "Ce", ""]]}, {"id": "2004.10114", "submitter": "Marcel Bengs", "authors": "Marcel Bengs and Nils Gessert and Matthias Schl\\\"uter and Alexander\n  Schlaefer", "title": "Spatio-Temporal Deep Learning Methods for Motion Estimation Using 4D OCT\n  Image Data", "comments": "Accepted for publication in the International Journal of Computer\n  Assisted Radiology and Surgery (IJCARS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Localizing structures and estimating the motion of a specific target\nregion are common problems for navigation during surgical interventions.\nOptical coherence tomography (OCT) is an imaging modality with a high spatial\nand temporal resolution that has been used for intraoperative imaging and also\nfor motion estimation, for example, in the context of ophthalmic surgery or\ncochleostomy. Recently, motion estimation between a template and a moving OCT\nimage has been studied with deep learning methods to overcome the shortcomings\nof conventional, feature-based methods.\n  Methods. We investigate whether using a temporal stream of OCT image volumes\ncan improve deep learning-based motion estimation performance. For this\npurpose, we design and evaluate several 3D and 4D deep learning methods and we\npropose a new deep learning approach. Also, we propose a temporal\nregularization strategy at the model output.\n  Results. Using a tissue dataset without additional markers, our deep learning\nmethods using 4D data outperform previous approaches. The best performing 4D\narchitecture achieves an correlation coefficient (aCC) of 98.58% compared to\n85.0% of a previous 3D deep learning method. Also, our temporal regularization\nstrategy at the output further improves 4D model performance to an aCC of\n99.06%. In particular, our 4D method works well for larger motion and is robust\ntowards image rotations and motion distortions.\n  Conclusions. We propose 4D spatio-temporal deep learning for OCT-based motion\nestimation. On a tissue dataset, we find that using 4D information for the\nmodel input improves performance while maintaining reasonable inference times.\nOur regularization strategy demonstrates that additional temporal information\nis also beneficial at the model output.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:43:01 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bengs", "Marcel", ""], ["Gessert", "Nils", ""], ["Schl\u00fcter", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2004.10121", "submitter": "Marcel Bengs", "authors": "Marcel Bengs and Nils Gessert and Alexander Schlaefer", "title": "A Deep Learning Approach for Motion Forecasting Using 4D OCT Data", "comments": "Accepted for publication at MIDL 2020:\n  https://openreview.net/forum?id=WVd56kgRV", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/WVd56kgRV", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting motion of a specific target object is a common problem for\nsurgical interventions, e.g. for localization of a target region, guidance for\nsurgical interventions, or motion compensation. Optical coherence tomography\n(OCT) is an imaging modality with a high spatial and temporal resolution.\nRecently, deep learning methods have shown promising performance for OCT-based\nmotion estimation based on two volumetric images. We extend this approach and\ninvestigate whether using a time series of volumes enables motion forecasting.\nWe propose 4D spatio-temporal deep learning for end-to-end motion forecasting\nand estimation using a stream of OCT volumes. We design and evaluate five\ndifferent 3D and 4D deep learning methods using a tissue data set. Our best\nperforming 4D method achieves motion forecasting with an overall average\ncorrelation coefficient of 97.41%, while also improving motion estimation\nperformance by a factor of 2.5 compared to a previous 3D approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:59:53 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 10:53:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Bengs", "Marcel", ""], ["Gessert", "Nils", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2004.10129", "submitter": "Xiao Liu", "authors": "Xiao Liu, Sotirios A Tsaftaris", "title": "Have you forgotten? A method to assess if machine learning models have\n  forgotten data", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of deep learning, aggregation of data from several sources is a\ncommon approach to ensuring data diversity. Let us consider a scenario where\nseveral providers contribute data to a consortium for the joint development of\na classification model (hereafter the target model), but, now one of the\nproviders decides to leave. This provider requests that their data (hereafter\nthe query dataset) be removed from the databases but also that the model\n`forgets' their data. In this paper, for the first time, we want to address the\nchallenging question of whether data have been forgotten by a model. We assume\nknowledge of the query dataset and the distribution of a model's output. We\nestablish statistical methods that compare the target's outputs with outputs of\nmodels trained with different datasets. We evaluate our approach on several\nbenchmark datasets (MNIST, CIFAR-10 and SVHN) and on a cardiac pathology\ndiagnosis task using data from the Automated Cardiac Diagnosis Challenge\n(ACDC). We hope to encourage studies on what information a model retains and\ninspire extensions in more complex settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 16:13:45 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:50:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Xiao", ""], ["Tsaftaris", "Sotirios A", ""]]}, {"id": "2004.10130", "submitter": "Takato Yasuno", "authors": "Takato Yasuno, Masazumi Amakata, Masahiro Okano", "title": "Natural Disaster Classification using Aerial Photography Explainable for\n  Typhoon Damaged Feature", "comments": "10pages, 5figures", "journal-ref": "ICPR2020 Workshops and Challenges (forthcoming on 10 January 2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years, typhoon damages has become social problem owing to climate\nchange. In 9 September 2019, Typhoon Faxai passed on the Chiba in Japan, whose\ndamages included with electric provision stop because of strong wind recorded\non the maximum 45 meter per second. A large amount of tree fell down, and the\nneighbor electric poles also fell down at the same time. These disaster\nfeatures have caused that it took 18 days for recovery longer than past ones.\nImmediate responses are important for faster recovery. As long as we can,\naerial survey for global screening of devastated region would be required for\ndecision support to respond where to recover ahead. This paper proposes a\npractical method to visualize the damaged areas focused on the typhoon disaster\nfeatures using aerial photography. This method can classify eight classes which\ncontains land covers without damages and areas with disaster. Using target\nfeature class probabilities, we can visualize disaster feature map to scale a\ncolor range. Furthermore, we can realize explainable map on each unit grid\nimages to compute the convolutional activation map using Grad-CAM. We\ndemonstrate case studies applied to aerial photographs recorded at the Chiba\nregion after typhoon.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 16:21:52 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 10:46:54 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 16:43:35 GMT"}, {"version": "v4", "created": "Fri, 23 Oct 2020 09:06:28 GMT"}, {"version": "v5", "created": "Mon, 16 Nov 2020 14:29:26 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yasuno", "Takato", ""], ["Amakata", "Masazumi", ""], ["Okano", "Masahiro", ""]]}, {"id": "2004.10141", "submitter": "Rami Ben-Ari", "authors": "Rami Ben-Ari, Mor Shpigel, Ophir Azulai, Udi Barzelay and Daniel\n  Rotman", "title": "TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition", "comments": null, "journal-ref": "Published in Learning from Limited and Imperfect Data (L2ID)\n  Workshop - CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classification of new class entities requires collecting and annotating\nhundreds or thousands of samples that is often prohibitively costly. Few-shot\nlearning suggests learning to classify new classes using just a few examples.\nOnly a small number of studies address the challenge of few-shot learning on\nspatio-temporal patterns such as videos. In this paper, we present the Temporal\nAware Embedding Network (TAEN) for few-shot action recognition, that learns to\nrepresent actions, in a metric space as a trajectory, conveying both short term\nsemantics and longer term connectivity between action parts. We demonstrate the\neffectiveness of TAEN on two few shot tasks, video classification and temporal\naction detection and evaluate our method on the Kinetics-400 and on ActivityNet\n1.2 few-shot benchmarks. With training of just a few fully connected layers we\nreach comparable results to prior art on both few shot video classification and\ntemporal detection tasks, while reaching state-of-the-art in certain scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 16:32:10 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 10:54:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ben-Ari", "Rami", ""], ["Shpigel", "Mor", ""], ["Azulai", "Ophir", ""], ["Barzelay", "Udi", ""], ["Rotman", "Daniel", ""]]}, {"id": "2004.10159", "submitter": "Marcel Bengs", "authors": "Marcel Bengs and Stephan Westermann and Nils Gessert and Dennis Eggert\n  and Andreas O. H. Gerstner and Nina A. Mueller and Christian Betz and Wiebke\n  Laffers and Alexander Schlaefer", "title": "Spatio-spectral deep learning methods for in-vivo hyperspectral\n  laryngeal cancer detection", "comments": "Accepted at SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of head and neck tumors is crucial for patient survival.\nOften, diagnoses are made based on endoscopic examination of the larynx\nfollowed by biopsy and histological analysis, leading to a high inter-observer\nvariability due to subjective assessment. In this regard, early non-invasive\ndiagnostics independent of the clinician would be a valuable tool. A recent\nstudy has shown that hyperspectral imaging (HSI) can be used for non-invasive\ndetection of head and neck tumors, as precancerous or cancerous lesions show\nspecific spectral signatures that distinguish them from healthy tissue.\nHowever, HSI data processing is challenging due to high spectral variations,\nvarious image interferences, and the high dimensionality of the data.\nTherefore, performance of automatic HSI analysis has been limited and so far,\nmostly ex-vivo studies have been presented with deep learning. In this work, we\nanalyze deep learning techniques for in-vivo hyperspectral laryngeal cancer\ndetection. For this purpose we design and evaluate convolutional neural\nnetworks (CNNs) with 2D spatial or 3D spatio-spectral convolutions combined\nwith a state-of-the-art Densenet architecture. For evaluation, we use an\nin-vivo data set with HSI of the oral cavity or oropharynx. Overall, we present\nmultiple deep learning techniques for in-vivo laryngeal cancer detection based\non HSI and we show that jointly learning from the spatial and spectral domain\nimproves classification accuracy notably. Our 3D spatio-spectral Densenet\nachieves an average accuracy of 81%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:07:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bengs", "Marcel", ""], ["Westermann", "Stephan", ""], ["Gessert", "Nils", ""], ["Eggert", "Dennis", ""], ["Gerstner", "Andreas O. H.", ""], ["Mueller", "Nina A.", ""], ["Betz", "Christian", ""], ["Laffers", "Wiebke", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2004.10162", "submitter": "Sanchari Sen", "authors": "Sanchari Sen, Balaraman Ravindran, Anand Raghunathan", "title": "EMPIR: Ensembles of Mixed Precision Deep Networks for Increased\n  Robustness against Adversarial Attacks", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their\nadoption in safety-critical applications such as self-driving cars, drones, and\nhealthcare. Notably, DNNs are vulnerable to adversarial attacks in which small\ninput perturbations can produce catastrophic misclassifications. In this work,\nwe propose EMPIR, ensembles of quantized DNN models with different numerical\nprecisions, as a new approach to increase robustness against adversarial\nattacks. EMPIR is based on the observation that quantized neural networks often\ndemonstrate much higher robustness to adversarial attacks than full precision\nnetworks, but at the cost of a substantial loss in accuracy on the original\n(unperturbed) inputs. EMPIR overcomes this limitation to achieve the 'best of\nboth worlds', i.e., the higher unperturbed accuracies of the full precision\nmodels combined with the higher robustness of the low precision models, by\ncomposing them in an ensemble. Further, as low precision DNN models have\nsignificantly lower computational and storage requirements than full precision\nmodels, EMPIR models only incur modest compute and memory overheads compared to\na single full-precision model (<25% in our evaluations). We evaluate EMPIR\nacross a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10\nand ImageNet) and under 4 different adversarial attacks. Our results indicate\nthat EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5%\nfor the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets\nrespectively, when compared to single full-precision models, without\nsacrificing accuracy on the unperturbed inputs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:17:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sen", "Sanchari", ""], ["Ravindran", "Balaraman", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2004.10165", "submitter": "Marcel Bengs", "authors": "Marcel Bengs and Nils Gessert and Alexander Schlaefer", "title": "4D Spatio-Temporal Deep Learning with 4D fMRI Data for Autism Spectrum\n  Disorder Classification", "comments": "Accepted at MIDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder (ASD) is associated with behavioral and\ncommunication problems. Often, functional magnetic resonance imaging (fMRI) is\nused to detect and characterize brain changes related to the disorder.\nRecently, machine learning methods have been employed to reveal new patterns by\ntrying to classify ASD from spatio-temporal fMRI images. Typically, these\nmethods have either focused on temporal or spatial information processing.\nInstead, we propose a 4D spatio-temporal deep learning approach for ASD\nclassification where we jointly learn from spatial and temporal data. We employ\n4D convolutional neural networks and convolutional-recurrent models which\noutperform a previous approach with an F1-score of 0.71 compared to an F1-score\nof 0.65.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:19:06 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bengs", "Marcel", ""], ["Gessert", "Nils", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2004.10190", "submitter": "Ryan Julian", "authors": "Ryan Julian, Benjamin Swanson, Gaurav S. Sukhatme, Sergey Levine,\n  Chelsea Finn, and Karol Hausman", "title": "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic\n  Reinforcement Learning", "comments": "8.5 pages, 9 figures. See video overview and experiments at\n  https://youtu.be/pPDVewcSpdc and project website at\n  https://ryanjulian.me/continual-fine-tuning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the great promises of robot learning systems is that they will be able\nto learn from their mistakes and continuously adapt to ever-changing\nenvironments. Despite this potential, most of the robot learning systems today\nare deployed as a fixed policy and they are not being adapted after their\ndeployment. Can we efficiently adapt previously learned behaviors to new\nenvironments, objects and percepts in the real world? In this paper, we present\na method and empirical evidence towards a robot learning framework that\nfacilitates continuous adaption. In particular, we demonstrate how to adapt\nvision-based robotic manipulation policies to new variations by fine-tuning via\noff-policy reinforcement learning, including changes in background, object\nshape and appearance, lighting conditions, and robot morphology. Further, this\nadaptation uses less than 0.2% of the data necessary to learn the task from\nscratch. We find that our approach of adapting pre-trained policies leads to\nsubstantial performance gains over the course of fine-tuning, and that\npre-training via RL is essential: training from scratch or adapting from\nsupervised ImageNet features are both unsuccessful with such small amounts of\ndata. We also find that these positive results hold in a limited continual\nlearning setting, in which we repeatedly fine-tune a single lineage of policies\nusing data from a succession of new tasks. Our empirical conclusions are\nconsistently supported by experiments on simulated manipulation tasks, and by\n52 unique fine-tuning experiments on a real robotic grasping system pre-trained\non 580,000 grasps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:57:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 13:43:53 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Julian", "Ryan", ""], ["Swanson", "Benjamin", ""], ["Sukhatme", "Gaurav S.", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""], ["Hausman", "Karol", ""]]}, {"id": "2004.10221", "submitter": "Benjamin Billot", "authors": "Benjamin Billot, Eleanor D. Robinson, Adrian V. Dalca, Juan Eugenio\n  Iglesias", "title": "Partial Volume Segmentation of Brain MRI Scans of any Resolution and\n  Contrast", "comments": "12 pages, 7 figures", "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention (MICCAI) 2020, pp. 177-187", "doi": "10.1007/978-3-030-59728-3_18", "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial voluming (PV) is arguably the last crucial unsolved problem in\nBayesian segmentation of brain MRI with probabilistic atlases. PV occurs when\nvoxels contain multiple tissue classes, giving rise to image intensities that\nmay not be representative of any one of the underlying classes. PV is\nparticularly problematic for segmentation when there is a large resolution gap\nbetween the atlas and the test scan, e.g., when segmenting clinical scans with\nthick slices, or when using a high-resolution atlas. In this work, we present\nPV-SynthSeg, a convolutional neural network (CNN) that tackles this problem by\ndirectly learning a mapping between (possibly multi-modal) low resolution (LR)\nscans and underlying high resolution (HR) segmentations. PV-SynthSeg simulates\nLR images from HR label maps with a generative model of PV, and can be trained\nto segment scans of any desired target contrast and resolution, even for\npreviously unseen modalities where neither images nor segmentations are\navailable at training. PV-SynthSeg does not require any preprocessing, and runs\nin seconds. We demonstrate the accuracy and flexibility of the method with\nextensive experiments on three datasets and 2,680 scans. The code is available\nat https://github.com/BBillot/SynthSeg.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:04:44 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 12:01:53 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 12:56:24 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Billot", "Benjamin", ""], ["Robinson", "Eleanor D.", ""], ["Dalca", "Adrian V.", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "2004.10258", "submitter": "Shiyang Yan", "authors": "Shiyang Yan, Yang Hua, Neil Robertson", "title": "ParaCNN: Visual Paragraph Generation via Adversarial Twin Contextual\n  CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image description generation plays an important role in many real-world\napplications, such as image retrieval, automatic navigation, and disabled\npeople support. A well-developed task of image description generation is image\ncaptioning, which usually generates a short captioning sentence and thus\nneglects many of fine-grained properties, e.g., the information of subtle\nobjects and their relationships. In this paper, we study the visual paragraph\ngeneration, which can describe the image with a long paragraph containing rich\ndetails. Previous research often generates the paragraph via a hierarchical\nRecurrent Neural Network (RNN)-like model, which has complex memorising,\nforgetting and coupling mechanism. Instead, we propose a novel pure CNN model,\nParaCNN, to generate visual paragraph using hierarchical CNN architecture with\ncontextual information between sentences within one paragraph. The ParaCNN can\ngenerate an arbitrary length of a paragraph, which is more applicable in many\nreal-world applications. Furthermore, to enable the ParaCNN to model paragraph\ncomprehensively, we also propose an adversarial twin net training scheme.\nDuring training, we force the forwarding network's hidden features to be close\nto that of the backwards network by using adversarial training. During testing,\nwe only use the forwarding network, which already includes the knowledge of the\nbackwards network, to generate a paragraph. We conduct extensive experiments on\nthe Stanford Visual Paragraph dataset and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:54:18 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yan", "Shiyang", ""], ["Hua", "Yang", ""], ["Robertson", "Neil", ""]]}, {"id": "2004.10282", "submitter": "Malte Hoffmann", "authors": "Malte Hoffmann, Benjamin Billot, Juan Eugenio Iglesias, Bruce Fischl,\n  Adrian V. Dalca", "title": "Learning image registration without images", "comments": "17 pages, 12 figures, deformable image registration, contrast\n  invariance, training without data, expanded analyses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning strategy for contrast-invariant image registration\nwithout requiring imaging data. While classical registration methods accurately\nestimate the spatial correspondence between images, they solve a costly\noptimization problem for every image pair. Learning-based techniques are fast\nat test time, but can only register images with image contrast and geometric\ncontent that are similar to those available during training. We focus on\nremoving this image-data dependency of learning methods. Our approach leverages\na generative model for diverse label maps and images that exposes networks to a\nwide range of variability during training, forcing them to learn features\ninvariant to image type (contrast). This strategy results in powerful networks\ntrained to generalize to a broad array of real input images. We present\nextensive experiments, with a focus on 3D neuroimaging, showing that this\nstrategy enables robust registration of arbitrary image contrasts without the\nneed to retrain for new modalities. We demonstrate registration accuracy that\nmost often surpasses the state of the art both within and across modalities,\nusing a single model. Critically, we show that input labels from which we\nsynthesize images need not be of actual anatomy: training on randomly generated\ngeometric shapes also results in competitive registration performance, albeit\nslightly less accurate, while alleviating the dependency on real data of any\nkind. Our code is available at: http://voxelmorph.csail.mit.edu\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:29:39 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 18:24:37 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Hoffmann", "Malte", ""], ["Billot", "Benjamin", ""], ["Iglesias", "Juan Eugenio", ""], ["Fischl", "Bruce", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2004.10289", "submitter": "Aysegul Dundar", "authors": "Aysegul Dundar, Karan Sapra, Guilin Liu, Andrew Tao, Bryan Catanzaro", "title": "Panoptic-based Image Synthesis", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image synthesis for generating photorealistic images serves\nvarious applications for content editing to content generation. Previous\nconditional image synthesis algorithms mostly rely on semantic maps, and often\nfail in complex environments where multiple instances occlude each other. We\npropose a panoptic aware image synthesis network to generate high fidelity and\nphotorealistic images conditioned on panoptic maps which unify semantic and\ninstance information. To achieve this, we efficiently use panoptic maps in\nconvolution and upsampling layers. We show that with the proposed changes to\nthe generator, we can improve on the previous state-of-the-art methods by\ngenerating images in complex instance interaction environments in higher\nfidelity and tiny objects in more details. Furthermore, our proposed method\nalso outperforms the previous state-of-the-art methods in metrics of mean IoU\n(Intersection over Union), and detAP (Detection Average Precision).\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:40:53 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Dundar", "Aysegul", ""], ["Sapra", "Karan", ""], ["Liu", "Guilin", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2004.10290", "submitter": "Jianping Lin", "authors": "Jianping Lin, Dong Liu, Houqiang Li, Feng Wu", "title": "M-LVC: Multiple Frames Prediction for Learned Video Compression", "comments": "Accepted to appear in CVPR2020; camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end learned video compression scheme for low-latency\nscenarios. Previous methods are limited in using the previous one frame as\nreference. Our method introduces the usage of the previous multiple frames as\nreferences. In our scheme, the motion vector (MV) field is calculated between\nthe current frame and the previous one. With multiple reference frames and\nassociated multiple MV fields, our designed network can generate more accurate\nprediction of the current frame, yielding less residual. Multiple reference\nframes also help generate MV prediction, which reduces the coding cost of MV\nfield. We use two deep auto-encoders to compress the residual and the MV,\nrespectively. To compensate for the compression error of the auto-encoders, we\nfurther design a MV refinement network and a residual refinement network,\ntaking use of the multiple reference frames as well. All the modules in our\nscheme are jointly optimized through a single rate-distortion loss function. We\nuse a step-by-step training strategy to optimize the entire scheme.\nExperimental results show that the proposed method outperforms the existing\nlearned video compression methods for low-latency mode. Our method also\nperforms better than H.265 in both PSNR and MS-SSIM. Our code and models are\npublicly available.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 20:42:02 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Lin", "Jianping", ""], ["Liu", "Dong", ""], ["Li", "Houqiang", ""], ["Wu", "Feng", ""]]}, {"id": "2004.10299", "submitter": "Luiz Gustavo Hafemann", "authors": "Ryan Sanford, Siavash Gorji, Luiz G. Hafemann, Bahareh Pourbabaee,\n  Mehrsan Javan", "title": "Group Activity Detection from Trajectory and Video Data in Soccer", "comments": "Accepted to the 6th International Workshop on Computer Vision in\n  Sports (CVsports) at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group activity detection in soccer can be done by using either video data or\nplayer and ball trajectory data. In current soccer activity datasets,\nactivities are labelled as atomic events without a duration. Given that the\nstate-of-the-art activity detection methods are not well-defined for atomic\nactions, these methods cannot be used. In this work, we evaluated the\neffectiveness of activity recognition models for detecting such events, by\nusing an intuitive non-maximum suppression process and evaluation metrics. We\nalso considered the problem of explicitly modeling interactions between players\nand ball. For this, we propose self-attention models to learn and extract\nrelevant information from a group of soccer players for activity detection from\nboth trajectory and video data. We conducted an extensive study on the use of\nvisual features and trajectory data for group activity detection in sports\nusing a large scale soccer dataset provided by Sportlogiq. Our results show\nthat most events can be detected using either vision or trajectory-based\napproaches with a temporal resolution of less than 0.5 seconds, and that each\napproach has unique challenges.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 21:11:30 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sanford", "Ryan", ""], ["Gorji", "Siavash", ""], ["Hafemann", "Luiz G.", ""], ["Pourbabaee", "Bahareh", ""], ["Javan", "Mehrsan", ""]]}, {"id": "2004.10313", "submitter": "Baihan Lin", "authors": "Baihan Lin", "title": "Keep It Real: a Window to Real Reality in Virtual Reality", "comments": "IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new interaction paradigm in the virtual reality (VR)\nenvironments, which consists of a virtual mirror or window projected onto a\nvirtual surface, representing the correct perspective geometry of a mirror or\nwindow reflecting the real world. This technique can be applied to various\nvideos, live streaming apps, augmented and virtual reality settings to provide\nan interactive and immersive user experience. To support such a\nperspective-accurate representation, we implemented computer vision algorithms\nfor feature detection and correspondence matching. To constrain the solutions,\nwe incorporated an automatically tuning scaling factor upon the homography\ntransform matrix such that each image frame follows a smooth transition with\nthe user in sight. The system is a real-time rendering framework where users\ncan engage their real-life presence with the virtual space.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 21:33:14 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 07:23:51 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 07:02:43 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lin", "Baihan", ""]]}, {"id": "2004.10314", "submitter": "Jan Sedmidubsky", "authors": "Jan Sedmidubsky and Pavel Zezula", "title": "Combining Deep Learning Classifiers for 3D Action Recognition", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular task of 3D human action recognition is almost exclusively solved\nby training deep-learning classifiers. To achieve a high recognition accuracy,\nthe input 3D actions are often pre-processed by various normalization or\naugmentation techniques. However, it is not computationally feasible to train a\nclassifier for each possible variant of training data in order to select the\nbest-performing subset of pre-processing techniques for a given dataset. In\nthis paper, we propose to train an independent classifier for each available\npre-processing technique and fuse the classification results based on a strict\nmajority vote rule. Together with a proposed evaluation procedure, we can very\nefficiently determine the best combination of normalization and augmentation\ntechniques for a specific dataset. For the best-performing combination, we can\nretrospectively apply the normalized/augmented variants of input data to train\nonly a single classifier. This also allows us to decide whether it is better to\ntrain a single model, or rather a set of independent classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 21:36:25 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sedmidubsky", "Jan", ""], ["Zezula", "Pavel", ""]]}, {"id": "2004.10327", "submitter": "Qinghui Liu", "authors": "Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-B{\\o}rre\n  Salberg", "title": "Multi-view Self-Constructing Graph Convolutional Networks with Adaptive\n  Class Weighting Loss for Semantic Segmentation", "comments": "7-page, MSCG-Net, CVPRW-2020", "journal-ref": null, "doi": null, "report-no": "2004.10327", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture called the Multi-view Self-Constructing Graph\nConvolutional Networks (MSCG-Net) for semantic segmentation. Building on the\nrecently proposed Self-Constructing Graph (SCG) module, which makes use of\nlearnable latent variables to self-construct the underlying graphs directly\nfrom the input features without relying on manually built prior knowledge\ngraphs, we leverage multiple views in order to explicitly exploit the\nrotational invariance in airborne images. We further develop an adaptive class\nweighting loss to address the class imbalance. We demonstrate the effectiveness\nand flexibility of the proposed method on the Agriculture-Vision challenge\ndataset and our model achieves very competitive results (0.547 mIoU) with much\nfewer parameters and at a lower computational cost compared to related pure-CNN\nbased work. Code will be available at: github.com/samleoqh/MSCG-Net\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 22:18:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""], ["Salberg", "Arnt-B\u00f8rre", ""]]}, {"id": "2004.10335", "submitter": "Isidoros Marougkas Mr.", "authors": "Isidoros Marougkas, Petros Koutras, Nikos Kardaris, Georgios Retsinas,\n  Georgia Chalvatzaki, and Petros Maragos", "title": "How to track your dragon: A Multi-Attentional Framework for real-time\n  RGB-D 6-DOF Object Pose Tracking", "comments": "14 pages, accepted at the 6th Workshop on Recovering 6D Object Pose\n  of the ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel multi-attentional convolutional architecture to tackle the\nproblem of real-time RGB-D 6D object pose tracking of single, known objects.\nSuch a problem poses multiple challenges originating both from the objects'\nnature and their interaction with their environment, which previous approaches\nhave failed to fully address. The proposed framework encapsulates methods for\nbackground clutter and occlusion handling by integrating multiple parallel soft\nspatial attention modules into a multitask Convolutional Neural Network (CNN)\narchitecture. Moreover, we consider the special geometrical properties of both\nthe object's 3D model and the pose space, and we use a more sophisticated\napproach for data augmentation during training. The provided experimental\nresults confirm the effectiveness of the proposed multi-attentional\narchitecture, as it improves the State-of-the-Art (SoA) tracking performance by\nan average score of 34.03% for translation and 40.01% for rotation, when tested\non the most complete dataset designed, up to date,for the problem of RGB-D\nobject tracking.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:00:06 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 22:08:21 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 11:33:55 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Marougkas", "Isidoros", ""], ["Koutras", "Petros", ""], ["Kardaris", "Nikos", ""], ["Retsinas", "Georgios", ""], ["Chalvatzaki", "Georgia", ""], ["Maragos", "Petros", ""]]}, {"id": "2004.10340", "submitter": "Sara Beery", "authors": "Sara Beery, Elijah Cole, Arvi Gjoka", "title": "The iWildCam 2020 Competition Dataset", "comments": "Fine-Grained Visual Categorization Workshop at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera traps enable the automatic collection of large quantities of image\ndata. Biologists all over the world use camera traps to monitor animal\npopulations. We have recently been making strides towards automatic species\nclassification in camera trap images. However, as we try to expand the\ngeographic scope of these models we are faced with an interesting question: how\ndo we train models that perform well on new (unseen during training) camera\ntrap locations? Can we leverage data from other modalities, such as citizen\nscience data and remote sensing data? In order to tackle this problem, we have\nprepared a challenge where the training data and test data are from different\ncameras spread across the globe. For each camera, we provide a series of remote\nsensing imagery that is tied to the location of the camera. We also provide\ncitizen science imagery from the set of species seen in our data. The challenge\nis to correctly classify species in the test camera traps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:25:13 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Beery", "Sara", ""], ["Cole", "Elijah", ""], ["Gjoka", "Arvi", ""]]}, {"id": "2004.10349", "submitter": "Ahmed Sabir", "authors": "Ahmed Sabir, Francesc Moreno-Noguer and Llu\\'is Padr\\'o", "title": "Textual Visual Semantic Dataset for Text Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Spotting in the wild consists of detecting and recognizing text\nappearing in images (e.g. signboards, traffic signals or brands in clothing or\nobjects). This is a challenging problem due to the complexity of the context\nwhere texts appear (uneven backgrounds, shading, occlusions, perspective\ndistortions, etc.). Only a few approaches try to exploit the relation between\ntext and its surrounding environment to better recognize text in the scene. In\nthis paper, we propose a visual context dataset for Text Spotting in the wild,\nwhere the publicly available dataset COCO-text [Veit et al. 2016] has been\nextended with information about the scene (such as objects and places appearing\nin the image) to enable researchers to include semantic relations between texts\nand scene in their Text Spotting systems, and to offer a common framework for\nsuch approaches. For each text in an image, we extract three kinds of context\ninformation: objects in the scene, image location label and a textual image\ndescription (caption). We use state-of-the-art out-of-the-box available tools\nto extract this additional information. Since this information has textual\nform, it can be used to leverage text similarity or semantic relation methods\ninto Text Spotting systems, either as a post-processing or in an end-to-end\ntraining strategy. Our data is publicly available at https://git.io/JeZTb.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:58:16 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sabir", "Ahmed", ""], ["Moreno-Noguer", "Francesc", ""], ["Padr\u00f3", "Llu\u00eds", ""]]}, {"id": "2004.10362", "submitter": "Sudhakar Kumawat", "authors": "Manisha Verma, Sudhakar Kumawat, Yuta Nakashima, Shanmuganathan Raman", "title": "Yoga-82: A New Dataset for Fine-grained Classification of Human Poses", "comments": "Accepted CVPR Workshops 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is a well-known problem in computer vision to locate\njoint positions. Existing datasets for the learning of poses are observed to be\nnot challenging enough in terms of pose diversity, object occlusion, and\nviewpoints. This makes the pose annotation process relatively simple and\nrestricts the application of the models that have been trained on them. To\nhandle more variety in human poses, we propose the concept of fine-grained\nhierarchical pose classification, in which we formulate the pose estimation as\na classification task, and propose a dataset, Yoga-82, for large-scale yoga\npose recognition with 82 classes. Yoga-82 consists of complex poses where fine\nannotations may not be possible. To resolve this, we provide hierarchical\nlabels for yoga poses based on the body configuration of the pose. The dataset\ncontains a three-level hierarchy including body positions, variations in body\npositions, and the actual pose names. We present the classification accuracy of\nthe state-of-the-art convolutional neural network architectures on Yoga-82. We\nalso present several hierarchical variants of DenseNet in order to utilize the\nhierarchical labels.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 01:43:44 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Verma", "Manisha", ""], ["Kumawat", "Sudhakar", ""], ["Nakashima", "Yuta", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2004.10365", "submitter": "Reza Pourreza", "authors": "Reza Pourreza, Nasser Kehtarnavaz", "title": "Automatic exposure selection and fusion for high-dynamic-range\n  photography via smartphones", "comments": null, "journal-ref": "Signal, Image and Video Processing volume 11, pages 1437-1444\n  (2017)", "doi": "10.1007/s11760-017-1104-9", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dynamic-range (HDR) photography involves fusing a bracket of images\ntaken at different exposure settings in order to compensate for the low dynamic\nrange of digital cameras such as the ones used in smartphones. In this paper, a\nmethod for automatically selecting the exposure settings of such images is\nintroduced based on the camera characteristic function. In addition, a new\nfusion method is introduced based on an optimization formulation and weighted\naveraging. Both of these methods are implemented on a smartphone platform as an\nHDR app to demonstrate the practicality of the introduced methods. Comparison\nresults with several existing methods are presented indicating the\neffectiveness as well as the computational efficiency of the introduced\nsolution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 01:48:17 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Pourreza", "Reza", ""], ["Kehtarnavaz", "Nasser", ""]]}, {"id": "2004.10375", "submitter": "Wanhua Li", "authors": "Wanhua Li, Yingqiang Zhang, Kangchen Lv, Jiwen Lu, Jianjiang Feng, and\n  Jie Zhou", "title": "Graph-based Kinship Reasoning Network", "comments": "Accepted to ICME 2020(IEEE International Conference on Multimedia &\n  Expo 2020) as an Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a graph-based kinship reasoning (GKR) network for\nkinship verification, which aims to effectively perform relational reasoning on\nthe extracted features of an image pair. Unlike most existing methods which\nmainly focus on how to learn discriminative features, our method considers how\nto compare and fuse the extracted feature pair to reason about the kin\nrelations. The proposed GKR constructs a star graph called kinship relational\ngraph where each peripheral node represents the information comparison in one\nfeature dimension and the central node is used as a bridge for information\ncommunication among peripheral nodes. Then the GKR performs relational\nreasoning on this graph with recursive message passing. Extensive experimental\nresults on the KinFaceW-I and KinFaceW-II datasets show that the proposed GKR\noutperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 02:55:38 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Li", "Wanhua", ""], ["Zhang", "Yingqiang", ""], ["Lv", "Kangchen", ""], ["Lu", "Jiwen", ""], ["Feng", "Jianjiang", ""], ["Zhou", "Jie", ""]]}, {"id": "2004.10382", "submitter": "Bahareh Rahmani", "authors": "J. Wilkins, M. V. Nguyen, B. Rahmani", "title": "Image Processing Failure and Deep Learning Success in Lawn Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lawn area measurement is an application of image processing and deep\nlearning. Researchers have been used hierarchical networks, segmented images\nand many other methods to measure lawn area. Methods effectiveness and accuracy\nvaries. In this project Image processing and deep learning methods has been\ncompared to find the best way to measure the lawn area. Three Image processing\nmethods using OpenCV has been compared to Convolutional Neural network which is\none of the most famous and effective deep learning methods. We used Keras and\nTensorFlow to estimate the lawn area. Convolutional Neural Network or shortly\nCNN shows very high accuracy (94-97%) for this purpose. In image processing\nmethods, Thresholding with 80-87% accuracy and Edge detection are effective\nmethods to measure the lawn area but Contouring with 26-31% accuracy does not\ncalculate the lawn area successfully. We may conclude that deep learning\nmethods especially CNN could be the best detective method comparing to image\nprocessing and other deep learning techniques.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 03:20:16 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Wilkins", "J.", ""], ["Nguyen", "M. V.", ""], ["Rahmani", "B.", ""]]}, {"id": "2004.10402", "submitter": "Jianhua Sun", "authors": "Jianhua Sun, Qinhong Jiang, Cewu Lu", "title": "Recursive Social Behavior Graph for Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social interaction is an important topic in human trajectory prediction to\ngenerate plausible paths. In this paper, we present a novel insight of\ngroup-based social interaction model to explore relationships among\npedestrians. We recursively extract social representations supervised by\ngroup-based annotations and formulate them into a social behavior graph, called\nRecursive Social Behavior Graph. Our recursive mechanism explores the\nrepresentation power largely. Graph Convolutional Neural Network then is used\nto propagate social interaction information in such a graph. With the guidance\nof Recursive Social Behavior Graph, we surpass state-of-the-art method on ETH\nand UCY dataset for 11.1% in ADE and 10.8% in FDE in average, and successfully\npredict complex social behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 06:01:48 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sun", "Jianhua", ""], ["Jiang", "Qinhong", ""], ["Lu", "Cewu", ""]]}, {"id": "2004.10447", "submitter": "Yu Zhang", "authors": "Qingxu Fu, Xiaoguang Di, and Yu Zhang", "title": "Learning an Adaptive Model for Extreme Low-light Raw Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light images suffer from severe noise and low illumination. Current deep\nlearning models that are trained with real-world images have excellent noise\nreduction, but a ratio parameter must be chosen manually to complete the\nenhancement pipeline. In this work, we propose an adaptive low-light raw image\nenhancement network to avoid parameter-handcrafting and to improve image\nquality. The proposed method can be divided into two sub-models: Brightness\nPrediction (BP) and Exposure Shifting (ES). The former is designed to control\nthe brightness of the resulting image by estimating a guideline exposure time\n$t_1$. The latter learns to approximate an exposure-shifting operator $ES$,\nconverting a low-light image with real exposure time $t_0$ to a noise-free\nimage with guideline exposure time $t_1$. Additionally, structural similarity\n(SSIM) loss and Image Enhancement Vector (IEV) are introduced to promote image\nquality, and a new Campus Image Dataset (CID) is proposed to overcome the\nlimitations of the existing datasets and to supervise the training of the\nproposed model. Using the proposed model, we can achieve high-quality low-light\nimage enhancement from a single raw image. In quantitative tests, it is shown\nthat the proposed method has the lowest Noise Level Estimation (NLE) score\ncompared with the state-of-the-art low-light algorithms, suggesting a superior\ndenoising performance. Furthermore, those tests illustrate that the proposed\nmethod is able to adaptively control the global image brightness according to\nthe content of the image scene. Lastly, the potential application in video\nprocessing is briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:01:07 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Fu", "Qingxu", ""], ["Di", "Xiaoguang", ""], ["Zhang", "Yu", ""]]}, {"id": "2004.10448", "submitter": "Luca Guarnera", "authors": "Luca Guarnera (1 and 2), Oliver Giudice (1), Sebastiano Battiato (1\n  and 2) ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University\n  of Catania)", "title": "DeepFake Detection by Analyzing Convolutional Traces", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deepfake phenomenon has become very popular nowadays thanks to the\npossibility to create incredibly realistic images using deep learning tools,\nbased mainly on ad-hoc Generative Adversarial Networks (GAN). In this work we\nfocus on the analysis of Deepfakes of human faces with the objective of\ncreating a new detection method able to detect a forensics trace hidden in\nimages: a sort of fingerprint left in the image generation process. The\nproposed technique, by means of an Expectation Maximization (EM) algorithm,\nextracts a set of local features specifically addressed to model the underlying\nconvolutional generative process. Ad-hoc validation has been employed through\nexperimental tests with naive classifiers on five different architectures\n(GDWCT, STARGAN, ATTGAN, STYLEGAN, STYLEGAN2) against the CELEBA dataset as\nground-truth for non-fakes. Results demonstrated the effectiveness of the\ntechnique in distinguishing the different architectures and the corresponding\ngeneration process.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:02:55 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Guarnera", "Luca", "", "1 and 2"], ["Giudice", "Oliver", "", "1\n  and 2"], ["Battiato", "Sebastiano", "", "1\n  and 2"]]}, {"id": "2004.10469", "submitter": "Yijun Quan", "authors": "Yijun Quan, Chang-Tsun Li, Yujue Zhou and Li Li", "title": "Warwick Image Forensics Dataset for Device Fingerprinting In Multimedia\n  Forensics", "comments": "Paper accepted to IEEE International Conference on Multimedia and\n  Expo 2020 (ICME 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device fingerprints like sensor pattern noise (SPN) are widely used for\nprovenance analysis and image authentication. Over the past few years, the\nrapid advancement in digital photography has greatly reshaped the pipeline of\nimage capturing process on consumer-level mobile devices. The flexibility of\ncamera parameter settings and the emergence of multi-frame photography\nalgorithms, especially high dynamic range (HDR) imaging, bring new challenges\nto device fingerprinting. The subsequent study on these topics requires a new\npurposefully built image dataset. In this paper, we present the Warwick Image\nForensics Dataset, an image dataset of more than 58,600 images captured using\n14 digital cameras with various exposure settings. Special attention to the\nexposure settings allows the images to be adopted by different multi-frame\ncomputational photography algorithms and for subsequent device fingerprinting.\nThe dataset is released as an open-source, free for use for the digital\nforensic community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:54:27 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 10:55:05 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Quan", "Yijun", ""], ["Li", "Chang-Tsun", ""], ["Zhou", "Yujue", ""], ["Li", "Li", ""]]}, {"id": "2004.10476", "submitter": "Yaoming Cai", "authors": "Yaoming Cai, Zijia Zhang, Zhihua Cai, Xiaobo Liu, Xinwei Jiang, and\n  Qin Yan", "title": "Graph Convolutional Subspace Clustering: A Robust Subspace Clustering\n  Framework for Hyperspectral Image", "comments": "This paper is submitted to IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2020.3018135", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) clustering is a challenging task due to the high\ncomplexity of HSI data. Subspace clustering has been proven to be powerful for\nexploiting the intrinsic relationship between data points. Despite the\nimpressive performance in the HSI clustering, traditional subspace clustering\nmethods often ignore the inherent structural information among data. In this\npaper, we revisit the subspace clustering with graph convolution and present a\nnovel subspace clustering framework called Graph Convolutional Subspace\nClustering (GCSC) for robust HSI clustering. Specifically, the framework\nrecasts the self-expressiveness property of the data into the non-Euclidean\ndomain, which results in a more robust graph embedding dictionary. We show that\ntraditional subspace clustering models are the special forms of our framework\nwith the Euclidean data. Basing on the framework, we further propose two novel\nsubspace clustering models by using the Frobenius norm, namely Efficient GCSC\n(EGCSC) and Efficient Kernel GCSC (EKGCSC). Both models have a globally optimal\nclosed-form solution, which makes them easier to implement, train, and apply in\npractice. Extensive experiments on three popular HSI datasets demonstrate that\nEGCSC and EKGCSC can achieve state-of-the-art clustering performance and\ndramatically outperforms many existing methods with significant margins.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 10:09:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Cai", "Yaoming", ""], ["Zhang", "Zijia", ""], ["Cai", "Zhihua", ""], ["Liu", "Xiaobo", ""], ["Jiang", "Xinwei", ""], ["Yan", "Qin", ""]]}, {"id": "2004.10484", "submitter": "Gary Goh", "authors": "Gary S. W. Goh, Sebastian Lapuschkin, Leander Weber, Wojciech Samek,\n  Alexander Binder", "title": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural\n  Network Attribution", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated gradients as an attribution method for deep neural network models\noffers simple implementability. However, it also suffers from noisiness of\nexplanations, which affects the ease of interpretability. In this paper, we\npresent Smooth Integrated Gradients as a statistically improved attribution\nmethod inspired by Taylor's theorem, which does not require a fixed baseline to\nbe chosen. We apply both methods to the image classification problem, using the\nILSVRC2012 ImageNet object recognition dataset, and a couple of pretrained\nimage models to generate attribution maps of their predictions. These\nattribution maps are visualized by saliency maps which can be evaluated\nqualitatively. We also empirically evaluate them using quantitative metrics\nsuch as perturbations-based score drops and multi-scaled total variance. We\nfurther propose adaptive noising to optimize for the noise scale hyperparameter\nvalue in our proposed method. From our experiments, we find that the Smooth\nIntegrated Gradients approach together with adaptive noising is able to\ngenerate better quality saliency maps with lesser noise and higher sensitivity\nto the relevant points in the input space.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 10:43:19 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Goh", "Gary S. W.", ""], ["Lapuschkin", "Sebastian", ""], ["Weber", "Leander", ""], ["Samek", "Wojciech", ""], ["Binder", "Alexander", ""]]}, {"id": "2004.10495", "submitter": "Dong Wang", "authors": "Dong Wang, Xiaoqian Qin, Fengyi Song, Li Cheng", "title": "Stabilizing Training of Generative Adversarial Nets via Langevin Stein\n  Variational Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs), famous for the capability of learning\ncomplex underlying data distribution, are however known to be tricky in the\ntraining process, which would probably result in mode collapse or performance\ndeterioration. Current approaches of dealing with GANs' issues almost utilize\nsome practical training techniques for the purpose of regularization, which on\nthe other hand undermines the convergence and theoretical soundness of GAN. In\nthis paper, we propose to stabilize GAN training via a novel particle-based\nvariational inference -- Langevin Stein variational gradient descent (LSVGD),\nwhich not only inherits the flexibility and efficiency of original SVGD but\naims to address its instability issues by incorporating an extra disturbance\ninto the update dynamics. We further demonstrate that by properly adjusting the\nnoise variance, LSVGD simulates a Langevin process whose stationary\ndistribution is exactly the target distribution. We also show that LSVGD\ndynamics has an implicit regularization which is able to enhance particles'\nspread-out and diversity. At last we present an efficient way of applying\nparticle-based variational inference on a general GAN training procedure no\nmatter what loss function is adopted. Experimental results on one synthetic\ndataset and three popular benchmark datasets -- Cifar-10, Tiny-ImageNet and\nCelebA validate that LSVGD can remarkably improve the performance and stability\nof various GAN models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 11:20:04 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Wang", "Dong", ""], ["Qin", "Xiaoqian", ""], ["Song", "Fengyi", ""], ["Cheng", "Li", ""]]}, {"id": "2004.10497", "submitter": "Sudeep Katakol", "authors": "Sudeep Katakol, Basem Elbarashy, Luis Herranz, Joost van de Weijer,\n  and Antonio M. Lopez", "title": "Distributed Learning and Inference with Compressed Images", "comments": "Accepted for publication in IEEE Transactions on Image Processing; 15\n  pages, 15 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3058545", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision requires processing large amounts of data, both while\ntraining the model and/or during inference, once the model is deployed.\nScenarios where images are captured and processed in physically separated\nlocations are increasingly common (e.g. autonomous vehicles, cloud computing).\nIn addition, many devices suffer from limited resources to store or transmit\ndata (e.g. storage space, channel capacity). In these scenarios, lossy image\ncompression plays a crucial role to effectively increase the number of images\ncollected under such constraints. However, lossy compression entails some\nundesired degradation of the data that may harm the performance of the\ndownstream analysis task at hand, since important semantic information may be\nlost in the process. Moreover, we may only have compressed images at training\ntime but are able to use original images at inference time, or vice versa, and\nin such a case, the downstream model suffers from covariate shift. In this\npaper, we analyze this phenomenon, with a special focus on vision-based\nperception for autonomous driving as a paradigmatic scenario. We see that loss\nof semantic information and covariate shift do indeed exist, resulting in a\ndrop in performance that depends on the compression rate. In order to address\nthe problem, we propose dataset restoration, based on image restoration with\ngenerative adversarial networks (GANs). Our method is agnostic to both the\nparticular image compression method and the downstream task; and has the\nadvantage of not adding additional cost to the deployed models, which is\nparticularly important in resource-limited devices. The presented experiments\nfocus on semantic segmentation as a challenging use case, cover a broad range\nof compression rates and diverse datasets, and show how our method is able to\nsignificantly alleviate the negative effects of compression on the downstream\nvisual task.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 11:20:53 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 11:45:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Katakol", "Sudeep", ""], ["Elbarashy", "Basem", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "2004.10507", "submitter": "Sushmita Mitra Prof.", "authors": "Sanhita Basu, Sushmita Mitra, Nilanjan Saha", "title": "Deep Learning for Screening COVID-19 using Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever increasing demand for screening millions of prospective \"novel\ncoronavirus\" or COVID-19 cases, and due to the emergence of high false\nnegatives in the commonly used PCR tests, the necessity for probing an\nalternative simple screening mechanism of COVID-19 using radiological images\n(like chest X-Rays) assumes importance. In this scenario, machine learning (ML)\nand deep learning (DL) offer fast, automated, effective strategies to detect\nabnormalities and extract key features of the altered lung parenchyma, which\nmay be related to specific signatures of the COVID-19 virus. However, the\navailable COVID-19 datasets are inadequate to train deep neural networks.\nTherefore, we propose a new concept called domain extension transfer learning\n(DETL). We employ DETL, with pre-trained deep convolutional neural network, on\na related large chest X-Ray dataset that is tuned for classifying between four\nclasses \\textit{viz.} $normal$, $pneumonia$, $other\\_disease$, and $Covid-19$.\nA 5-fold cross validation is performed to estimate the feasibility of using\nchest X-Rays to diagnose COVID-19. The initial results show promise, with the\npossibility of replication on bigger and more diverse data sets. The overall\naccuracy was measured as $90.13\\% \\pm 0.14$. In order to get an idea about the\nCOVID-19 detection transparency, we employed the concept of Gradient Class\nActivation Map (Grad-CAM) for detecting the regions where the model paid more\nattention during the classification. This was found to strongly correlate with\nclinical findings, as validated by experts.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 11:41:50 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 05:44:02 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 04:31:56 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 20:17:35 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Basu", "Sanhita", ""], ["Mitra", "Sushmita", ""], ["Saha", "Nilanjan", ""]]}, {"id": "2004.10518", "submitter": "Fatemeh Ziaeetabar", "authors": "Fatemeh Ziaeetabar, Jennifer Pomp, Stefan Pfeiffer, Nadiya El-Sourani,\n  Ricarda I. Schubotz, Minija Tamosiunaite and Florentin W\\\"org\\\"otter", "title": "Human and Machine Action Prediction Independent of Object Information", "comments": "This paper includes 31 pages, 11 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting other people's action is key to successful social interactions,\nenabling us to adjust our own behavior to the consequence of the others' future\nactions. Studies on action recognition have focused on the importance of\nindividual visual features of objects involved in an action and its context.\nHumans, however, recognize actions on unknown objects or even when objects are\nimagined (pantomime). Other cues must thus compensate the lack of recognizable\nvisual object features. Here, we focus on the role of inter-object relations\nthat change during an action. We designed a virtual reality setup and tested\nrecognition speed for 10 different manipulation actions on 50 subjects. All\nobjects were abstracted by emulated cubes so the actions could not be inferred\nusing object information. Instead, subjects had to rely only on the information\nthat comes from the changes in the spatial relations that occur between those\ncubes. In spite of these constraints, our results show the subjects were able\nto predict actions in, on average, less than 64% of the action's duration. We\nemployed a computational model -an enriched Semantic Event Chain (eSEC)-\nincorporating the information of spatial relations, specifically (a) objects'\ntouching/untouching, (b) static spatial relations between objects and (c)\ndynamic spatial relations between objects. Trained on the same actions as those\nobserved by subjects, the model successfully predicted actions even better than\nhumans. Information theoretical analysis shows that eSECs optimally use\nindividual cues, whereas humans presumably mostly rely on a mixed-cue strategy,\nwhich takes longer until recognition. Providing a better cognitive basis of\naction recognition may, on one hand improve our understanding of related human\npathologies and, on the other hand, also help to build robots for conflict-free\nhuman-robot cooperation. Our results open new avenues here.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:13:25 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Ziaeetabar", "Fatemeh", ""], ["Pomp", "Jennifer", ""], ["Pfeiffer", "Stefan", ""], ["El-Sourani", "Nadiya", ""], ["Schubotz", "Ricarda I.", ""], ["Tamosiunaite", "Minija", ""], ["W\u00f6rg\u00f6tter", "Florentin", ""]]}, {"id": "2004.10534", "submitter": "Diego Thomas", "authors": "Hayato Onizuka, Zehra Hayirci, Diego Thomas, Akihiro Sugimoto, Hideaki\n  Uchiyama, Rin-ichiro Taniguchi", "title": "TetraTSDF: 3D human reconstruction from a single image with a\n  tetrahedral outer shell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D shape of a person from its 2D appearance is ill-posed due\nto ambiguities. Nevertheless, with the help of convolutional neural networks\n(CNN) and prior knowledge on the 3D human body, it is possible to overcome such\nambiguities to recover detailed 3D shapes of human bodies from single images.\nCurrent solutions, however, fail to reconstruct all the details of a person\nwearing loose clothes. This is because of either (a) huge memory requirement\nthat cannot be maintained even on modern GPUs or (b) the compact 3D\nrepresentation that cannot encode all the details. In this paper, we propose\nthe tetrahedral outer shell volumetric truncated signed distance function\n(TetraTSDF) model for the human body, and its corresponding part connection\nnetwork (PCN) for 3D human body shape regression. Our proposed model is\ncompact, dense, accurate, and yet well suited for CNN-based regression task.\nOur proposed PCN allows us to learn the distribution of the TSDF in the\ntetrahedral volume from a single image in an end-to-end manner. Results show\nthat our proposed method allows to reconstruct detailed shapes of humans\nwearing loose clothes from single RGB images.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:47:24 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Onizuka", "Hayato", ""], ["Hayirci", "Zehra", ""], ["Thomas", "Diego", ""], ["Sugimoto", "Akihiro", ""], ["Uchiyama", "Hideaki", ""], ["Taniguchi", "Rin-ichiro", ""]]}, {"id": "2004.10547", "submitter": "Shuting He", "authors": "Shuting He, Hao Luo, Weihua Chen, Miao Zhang, Yuqi Zhang, Fan Wang,\n  Hao Li and Wei Jiang", "title": "Multi-Domain Learning and Identity Mining for Vehicle Re-Identification", "comments": "Solution for AI City Challenge, CVPR2020 Workshop. Codes are at\n  https://github.com/heshuting555/AICITY2020_DMT_VehicleReID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces our solution for the Track2 in AI City Challenge 2020\n(AICITY20). The Track2 is a vehicle re-identification (ReID) task with both the\nreal-world data and synthetic data. Our solution is based on a strong baseline\nwith bag of tricks (BoT-BS) proposed in person ReID. At first, we propose a\nmulti-domain learning method to joint the real-world and synthetic data to\ntrain the model. Then, we propose the Identity Mining method to automatically\ngenerate pseudo labels for a part of the testing data, which is better than the\nk-means clustering. The tracklet-level re-ranking strategy with weighted\nfeatures is also used to post-process the results. Finally, with multiple-model\nensemble, our method achieves 0.7322 in the mAP score which yields third place\nin the competition. The codes are available at\nhttps://github.com/heshuting555/AICITY2020_DMT_VehicleReID.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:03:52 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 14:08:35 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["He", "Shuting", ""], ["Luo", "Hao", ""], ["Chen", "Weihua", ""], ["Zhang", "Miao", ""], ["Zhang", "Yuqi", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""], ["Jiang", "Wei", ""]]}, {"id": "2004.10557", "submitter": "Diego Thomas", "authors": "Diego Thomas", "title": "Real-time Simultaneous 3D Head Modeling and Facial Motion Capture with\n  an RGB-D camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to build in real-time animated 3D head models using a\nconsumer-grade RGB-D camera. Our proposed method is the first one to provide\nsimultaneously comprehensive facial motion tracking and a detailed 3D model of\nthe user's head. Anyone's head can be instantly reconstructed and his facial\nmotion captured without requiring any training or pre-scanning. The user starts\nfacing the camera with a neutral expression in the first frame, but is free to\nmove, talk and change his face expression as he wills otherwise. The facial\nmotion is captured using a blendshape animation model while geometric details\nare captured using a Deviation image mapped over the template mesh. We\ncontribute with an efficient algorithm to grow and refine the deforming 3D\nmodel of the head on-the-fly and in real-time. We demonstrate robust and\nhigh-fidelity simultaneous facial motion capture and 3D head modeling results\non a wide range of subjects with various head poses and facial expressions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:22:21 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Thomas", "Diego", ""]]}, {"id": "2004.10566", "submitter": "Ignacio Rocco", "authors": "Ignacio Rocco, Relja Arandjelovi\\'c, Josef Sivic", "title": "Efficient Neighbourhood Consensus Networks via Submanifold Sparse\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we target the problem of estimating accurately localised\ncorrespondences between a pair of images. We adopt the recent Neighbourhood\nConsensus Networks that have demonstrated promising performance for difficult\ncorrespondence problems and propose modifications to overcome their main\nlimitations: large memory consumption, large inference time and poorly\nlocalised correspondences. Our proposed modifications can reduce the memory\nfootprint and execution time more than $10\\times$, with equivalent results.\nThis is achieved by sparsifying the correlation tensor containing tentative\nmatches, and its subsequent processing with a 4D CNN using submanifold sparse\nconvolutions. Localisation accuracy is significantly improved by processing the\ninput images in higher resolution, which is possible due to the reduced memory\nfootprint, and by a novel two-stage correspondence relocalisation module. The\nproposed Sparse-NCNet method obtains state-of-the-art results on the HPatches\nSequences and InLoc visual localisation benchmarks, and competitive results in\nthe Aachen Day-Night benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:37:36 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Rocco", "Ignacio", ""], ["Arandjelovi\u0107", "Relja", ""], ["Sivic", "Josef", ""]]}, {"id": "2004.10568", "submitter": "Markus Nagel", "authors": "Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos,\n  Tijmen Blankevoort", "title": "Up or Down? Adaptive Rounding for Post-Training Quantization", "comments": "Published as a conference paper at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When quantizing neural networks, assigning each floating-point weight to its\nnearest fixed-point value is the predominant approach. We find that, perhaps\nsurprisingly, this is not the best we can do. In this paper, we propose\nAdaRound, a better weight-rounding mechanism for post-training quantization\nthat adapts to the data and the task loss. AdaRound is fast, does not require\nfine-tuning of the network, and only uses a small amount of unlabelled data. We\nstart by theoretically analyzing the rounding problem for a pre-trained neural\nnetwork. By approximating the task loss with a Taylor series expansion, the\nrounding task is posed as a quadratic unconstrained binary optimization\nproblem. We simplify this to a layer-wise local loss and propose to optimize\nthis loss with a soft relaxation. AdaRound not only outperforms\nrounding-to-nearest by a significant margin but also establishes a new\nstate-of-the-art for post-training quantization on several networks and tasks.\nWithout fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4\nbits while staying within an accuracy loss of 1%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:44:28 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 09:51:23 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Nagel", "Markus", ""], ["Amjad", "Rana Ali", ""], ["van Baalen", "Mart", ""], ["Louizos", "Christos", ""], ["Blankevoort", "Tijmen", ""]]}, {"id": "2004.10605", "submitter": "Ioan-Adrian Cosma Mr.", "authors": "Adrian Cosma, Mihai Ghidoveanu, Michael Panaitescu-Liess and Marius\n  Popescu", "title": "Self-Supervised Representation Learning on Document Images", "comments": "15 pages, 5 figures. Accepted at DAS 2020: IAPR International\n  Workshop on Document Analysis Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyses the impact of self-supervised pre-training on document\nimages in the context of document image classification. While previous\napproaches explore the effect of self-supervision on natural images, we show\nthat patch-based pre-training performs poorly on document images because of\ntheir different structural properties and poor intra-sample semantic\ninformation. We propose two context-aware alternatives to improve performance\non the Tobacco-3482 image classification task. We also propose a novel method\nfor self-supervision, which makes use of the inherent multi-modality of\ndocuments (image and text), which performs better than other popular\nself-supervised methods, including supervised ImageNet pre-training, on\ndocument image classification scenarios with a limited amount of data.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 10:14:06 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 08:48:48 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Cosma", "Adrian", ""], ["Ghidoveanu", "Mihai", ""], ["Panaitescu-Liess", "Michael", ""], ["Popescu", "Marius", ""]]}, {"id": "2004.10634", "submitter": "Hao Su", "authors": "Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Jiahe Cui, and Ji Wan", "title": "MangaGAN: Unpaired Photo-to-Manga Translation Based on The Methodology\n  of Manga Drawing", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manga is a world popular comic form originated in Japan, which typically\nemploys black-and-white stroke lines and geometric exaggeration to describe\nhumans' appearances, poses, and actions. In this paper, we propose MangaGAN,\nthe first method based on Generative Adversarial Network (GAN) for unpaired\nphoto-to-manga translation. Inspired by how experienced manga artists draw\nmanga, MangaGAN generates the geometric features of manga face by a designed\nGAN model and delicately translates each facial region into the manga domain by\na tailored multi-GANs architecture. For training MangaGAN, we construct a new\ndataset collected from a popular manga work, containing manga facial features,\nlandmarks, bodies, and so on. Moreover, to produce high-quality manga faces, we\nfurther propose a structural smoothing loss to smooth stroke-lines and avoid\nnoisy pixels, and a similarity preserving module to improve the similarity\nbetween domains of photo and manga. Extensive experiments show that MangaGAN\ncan produce high-quality manga faces which preserve both the facial similarity\nand a popular manga style, and outperforms other related state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:23:42 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 17:21:42 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Su", "Hao", ""], ["Niu", "Jianwei", ""], ["Liu", "Xuefeng", ""], ["Li", "Qingfeng", ""], ["Cui", "Jiahe", ""], ["Wan", "Ji", ""]]}, {"id": "2004.10641", "submitter": "Sara Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassasni, Michal J.\n  Wesolowski, Kevin A. Schneider, Ralph Deters", "title": "Automatic Detection of Coronavirus Disease (COVID-19) in X-ray and CT\n  Images: A Machine Learning-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The newly identified Coronavirus pneumonia, subsequently termed COVID-19, is\nhighly transmittable and pathogenic with no clinically approved antiviral drug\nor vaccine available for treatment. The most common symptoms of COVID-19 are\ndry cough, sore throat, and fever. Symptoms can progress to a severe form of\npneumonia with critical complications, including septic shock, pulmonary edema,\nacute respiratory distress syndrome and multi-organ failure. While medical\nimaging is not currently recommended in Canada for primary diagnosis of\nCOVID-19, computer-aided diagnosis systems could assist in the early detection\nof COVID-19 abnormalities and help to monitor the progression of the disease,\npotentially reduce mortality rates. In this study, we compare popular deep\nlearning-based feature extraction frameworks for automatic COVID-19\nclassification. To obtain the most accurate feature, which is an essential\ncomponent of learning, MobileNet, DenseNet, Xception, ResNet, InceptionV3,\nInceptionResNetV2, VGGNet, NASNet were chosen amongst a pool of deep\nconvolutional neural networks. The extracted features were then fed into\nseveral machine learning classifiers to classify subjects as either a case of\nCOVID-19 or a control. This approach avoided task-specific data pre-processing\nmethods to support a better generalization ability for unseen data. The\nperformance of the proposed method was validated on a publicly available\nCOVID-19 dataset of chest X-ray and CT images. The DenseNet121 feature\nextractor with Bagging tree classifier achieved the best performance with 99%\nclassification accuracy. The second-best learner was a hybrid of the a ResNet50\nfeature extractor trained by LightGBM with an accuracy of 98%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:34:45 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["Kassasni", "Peyman Hosseinzadeh", ""], ["Wesolowski", "Michal J.", ""], ["Schneider", "Kevin A.", ""], ["Deters", "Ralph", ""]]}, {"id": "2004.10664", "submitter": "Tongxue Zhou", "authors": "Tongxue Zhou, Su Ruan, St\\'ephane Canu", "title": "A review: Deep learning for medical image segmentation using\n  multi-modality fusion", "comments": "26 pages, 8 figures", "journal-ref": "Array, Volumes 3-4, September-December 2019, Article 100004", "doi": "10.1016/j.array.2019.100004", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality is widely used in medical imaging, because it can provide\nmultiinformation about a target (tumor, organ or tissue). Segmentation using\nmultimodality consists of fusing multi-information to improve the segmentation.\nRecently, deep learning-based approaches have presented the state-of-the-art\nperformance in image classification, segmentation, object detection and\ntracking tasks. Due to their self-learning and generalization ability over\nlarge amounts of data, deep learning recently has also gained great interest in\nmulti-modal medical image segmentation. In this paper, we give an overview of\ndeep learning-based approaches for multi-modal medical image segmentation task.\nFirstly, we introduce the general principle of deep learning and multi-modal\nmedical image segmentation. Secondly, we present different deep learning\nnetwork architectures, then analyze their fusion strategies and compare their\nresults. The earlier fusion is commonly used, since it's simple and it focuses\non the subsequent segmentation network architecture. However, the later fusion\ngives more attention on fusion strategy to learn the complex relationship\nbetween different modalities. In general, compared to the earlier fusion, the\nlater fusion can give more accurate result if the fusion method is effective\nenough. We also discuss some common problems in medical image segmentation.\nFinally, we summarize and provide some perspectives on the future research.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 16:00:53 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:33:31 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhou", "Tongxue", ""], ["Ruan", "Su", ""], ["Canu", "St\u00e9phane", ""]]}, {"id": "2004.10681", "submitter": "Lokender Tiwari", "authors": "Lokender Tiwari, Pan Ji, Quoc-Huy Tran, Bingbing Zhuang, Saket Anand,\n  Manmohan Chandraker", "title": "Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction", "comments": "ECCV 2020, Project Page:\n  https://lokender.github.io/self-improving-SLAM.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical monocular Simultaneous Localization And Mapping (SLAM) and the\nrecently emerging convolutional neural networks (CNNs) for monocular depth\nprediction represent two largely disjoint approaches towards building a 3D map\nof the surrounding environment. In this paper, we demonstrate that the coupling\nof these two by leveraging the strengths of each mitigates the other's\nshortcomings. Specifically, we propose a joint narrow and wide baseline based\nself-improving framework, where on the one hand the CNN-predicted depth is\nleveraged to perform pseudo RGB-D feature-based SLAM, leading to better\naccuracy and robustness than the monocular RGB SLAM baseline. On the other\nhand, the bundle-adjusted 3D scene structures and camera poses from the more\nprincipled geometric SLAM are injected back into the depth network through\nnovel wide baseline losses proposed for improving the depth prediction network,\nwhich then continues to contribute towards better pose and 3D structure\nestimation in the next iteration. We emphasize that our framework only requires\nunlabeled monocular videos in both training and inference stages, and yet is\nable to outperform state-of-the-art self-supervised monocular and stereo depth\nprediction networks (e.g, Monodepth2) and feature-based monocular SLAM system\n(i.e, ORB-SLAM). Extensive experiments on KITTI and TUM RGB-D datasets verify\nthe superiority of our self-improving geometry-CNN framework.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 16:31:59 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 17:35:36 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 05:15:31 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Tiwari", "Lokender", ""], ["Ji", "Pan", ""], ["Tran", "Quoc-Huy", ""], ["Zhuang", "Bingbing", ""], ["Anand", "Saket", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2004.10694", "submitter": "Zhao Zhong", "authors": "Yikang Zhang, Jian Zhang, Qiang Wang, Zhao Zhong", "title": "DyNet: Dynamic Convolution for Accelerating Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution operator is the core of convolutional neural networks (CNNs) and\noccupies the most computation cost. To make CNNs more efficient, many methods\nhave been proposed to either design lightweight networks or compress models.\nAlthough some efficient network structures have been proposed, such as\nMobileNet or ShuffleNet, we find that there still exists redundant information\nbetween convolution kernels. To address this issue, we propose a novel dynamic\nconvolution method to adaptively generate convolution kernels based on image\ncontents. To demonstrate the effectiveness, we apply dynamic convolution on\nmultiple state-of-the-art CNNs. On one hand, we can reduce the computation cost\nremarkably while maintaining the performance. For\nShuffleNetV2/MobileNetV2/ResNet18/ResNet50, DyNet can reduce\n37.0/54.7/67.2/71.3% FLOPs without loss of accuracy. On the other hand, the\nperformance can be largely boosted if the computation cost is maintained. Based\non the architecture MobileNetV3-Small/Large, DyNet achieves 70.3/77.1% Top-1\naccuracy on ImageNet with an improvement of 2.9/1.9%. To verify the\nscalability, we also apply DyNet on segmentation task, the results show that\nDyNet can reduce 69.3% FLOPs while maintaining Mean IoU on segmentation task.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 16:58:05 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Zhang", "Yikang", ""], ["Zhang", "Jian", ""], ["Wang", "Qiang", ""], ["Zhong", "Zhao", ""]]}, {"id": "2004.10703", "submitter": "Arun Maiya", "authors": "Arun S. Maiya", "title": "ktrain: A Low-Code Library for Augmented Machine Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ktrain, a low-code Python library that makes machine learning more\naccessible and easier to apply. As a wrapper to TensorFlow and many other\nlibraries (e.g., transformers, scikit-learn, stellargraph), it is designed to\nmake sophisticated, state-of-the-art machine learning models simple to build,\ntrain, inspect, and apply by both beginners and experienced practitioners.\nFeaturing modules that support text data (e.g., text classification, sequence\ntagging, open-domain question-answering), vision data (e.g., image\nclassification), graph data (e.g., node classification, link prediction), and\ntabular data, ktrain presents a simple unified interface enabling one to\nquickly solve a wide range of tasks in as little as three or four \"commands\" or\nlines of code.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 14:18:20 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 11:48:35 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 15:50:12 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 21:25:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Maiya", "Arun S.", ""]]}, {"id": "2004.10705", "submitter": "Stanis{\\l}aw Ka\\'zmierczak", "authors": "Stanis{\\l}aw Ka\\'zmierczak, Jacek Ma\\'ndziuk", "title": "A Committee of Convolutional Neural Networks for Image Classication in\n  the Concurrent Presence of Feature and Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has become a ubiquitous task. Models trained on good\nquality data achieve accuracy which in some application domains is already\nabove human-level performance. Unfortunately, real-world data are quite often\ndegenerated by the noise existing in features and/or labels. There are quite\nmany papers that handle the problem of either feature or label noise,\nseparately. However, to the best of our knowledge, this piece of research is\nthe first attempt to address the problem of concurrent occurrence of both types\nof noise. Basing on the MNIST, CIFAR-10 and CIFAR-100 datasets, we\nexperimentally proved that the difference by which committees beat single\nmodels increases along with noise level, no matter it is an attribute or label\ndisruption. Thus, it makes ensembles legitimate to be applied to noisy images\nwith noisy labels. The aforementioned committees' advantage over single models\nis positively correlated with dataset difficulty level as well. We propose\nthree committee selection algorithms that outperform a strong baseline\nalgorithm which relies on an ensemble of individual (nonassociated) best\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 00:22:11 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:54:13 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ka\u017amierczak", "Stanis\u0142aw", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2004.10734", "submitter": "Ivan Ezhov", "authors": "Ahmad B Qasim, Ivan Ezhov, Suprosanna Shit, Oliver Schoppe, Johannes C\n  Paetzold, Anjany Sekuboyina, Florian Kofler, Jana Lipkova, Hongwei Li, Bjoern\n  Menze", "title": "Red-GAN: Attacking class imbalance via conditioned generation. Yet\n  another perspective on medical image synthesis for skin lesion dermoscopy and\n  brain tumor MRI", "comments": null, "journal-ref": "Published in Proceedings of the 3rd edition of Medical Imaging\n  with Deep Learning, Montr\\'eal, Canada, PMLR 121, 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting learning algorithms under scarce data regimes is a limitation and\na reality of the medical imaging field. In an attempt to mitigate the problem,\nwe propose a data augmentation protocol based on generative adversarial\nnetworks. We condition the networks at a pixel-level (segmentation mask) and at\na global-level information (acquisition environment or lesion type). Such\nconditioning provides immediate access to the image-label pairs while\ncontrolling global class specific appearance of the synthesized images. To\nstimulate synthesis of the features relevant for the segmentation task, an\nadditional passive player in a form of segmentor is introduced into the\nadversarial game. We validate the approach on two medical datasets: BraTS,\nISIC. By controlling the class distribution through injection of synthetic\nimages into the training set we achieve control over the accuracy levels of the\ndatasets' classes.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:38:48 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:44:11 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 23:03:24 GMT"}, {"version": "v4", "created": "Sun, 28 Mar 2021 00:15:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Qasim", "Ahmad B", ""], ["Ezhov", "Ivan", ""], ["Shit", "Suprosanna", ""], ["Schoppe", "Oliver", ""], ["Paetzold", "Johannes C", ""], ["Sekuboyina", "Anjany", ""], ["Kofler", "Florian", ""], ["Lipkova", "Jana", ""], ["Li", "Hongwei", ""], ["Menze", "Bjoern", ""]]}, {"id": "2004.10774", "submitter": "Waqas Sultani", "authors": "Waqas Sultani, Qazi Ammar Arshad, Chen Chen", "title": "Action recognition in real-world videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of human action recognition is to temporally or spatially localize\nthe human action of interest in video sequences. Temporal localization (i.e.\nindicating the start and end frames of the action in a video) is referred to as\nframe-level detection. Spatial localization, which is more challenging, means\nto identify the pixels within each action frame that correspond to the action.\nThis setting is usually referred to as pixel-level detection. In this chapter,\nwe are using action, activity, event interchangeably.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 18:02:50 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Sultani", "Waqas", ""], ["Arshad", "Qazi Ammar", ""], ["Chen", "Chen", ""]]}, {"id": "2004.10780", "submitter": "Juan Castorena", "authors": "Manish Bhattarai, Diane Oyen, Juan Castorena, Liping Yang, Brendt\n  Wohlberg", "title": "Diagram Image Retrieval using Sketch-Based Deep Learning and Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution of the complex problem of image retrieval for diagram images has\nyet to be reached. Deep learning methods continue to excel in the fields of\nobject detection and image classification applied to natural imagery. However,\nthe application of such methodologies applied to binary imagery remains limited\ndue to lack of crucial features such as textures,color and intensity\ninformation. This paper presents a deep learning based method for image-based\nsearch for binary patent images by taking advantage of existing large natural\nimage repositories for image search and sketch-based methods (Sketches are not\nidentical to diagrams, but they do share some characteristics; for example,\nboth imagery types are gray scale (binary), composed of contours, and are\nlacking in texture).\n  We begin by using deep learning to generate sketches from natural images for\nimage retrieval and then train a second deep learning model on the sketches. We\nthen use our small set of manually labeled patent diagram images via transfer\nlearning to adapt the image search from sketches of natural images to diagrams.\nOur experiment results show the effectiveness of deep learning with transfer\nlearning for detecting near-identical copies in patent images and querying\nsimilar images based on content.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 18:27:46 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Bhattarai", "Manish", ""], ["Oyen", "Diane", ""], ["Castorena", "Juan", ""], ["Yang", "Liping", ""], ["Wohlberg", "Brendt", ""]]}, {"id": "2004.10792", "submitter": "Sara Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J.\n  Wesolowski, Kevin A. Schneider, Ralph Deters", "title": "Automatic Polyp Segmentation Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer is the third most common cancer-related death after lung\ncancer and breast cancer worldwide. The risk of developing colorectal cancer\ncould be reduced by early diagnosis of polyps during a colonoscopy.\nComputer-aided diagnosis systems have the potential to be applied for polyp\nscreening and reduce the number of missing polyps. In this paper, we compare\nthe performance of different deep learning architectures as feature extractors,\ni.e. ResNet, DenseNet, InceptionV3, InceptionResNetV2 and SE-ResNeXt in the\nencoder part of a U-Net architecture. We validated the performance of presented\nensemble models on the CVC-Clinic (GIANA 2018) dataset. The DenseNet169 feature\nextractor combined with U-Net architecture outperformed the other counterparts\nand achieved an accuracy of 99.15\\%, Dice similarity coefficient of 90.87%, and\nJaccard index of 83.82%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 18:54:29 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["Kassani", "Peyman Hosseinzadeh", ""], ["Wesolowski", "Michal J.", ""], ["Schneider", "Kevin A.", ""], ["Deters", "Ralph", ""]]}, {"id": "2004.10796", "submitter": "Jae Sung Park", "authors": "Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi,\n  Yejin Choi", "title": "VisualCOMET: Reasoning about the Dynamic Context of a Still Image", "comments": "Project Page: http://visualcomet.xyz (ECCV 2020 Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even from a single frame of a still image, people can reason about the\ndynamic story of the image before, after, and beyond the frame. For example,\ngiven an image of a man struggling to stay afloat in water, we can reason that\nthe man fell into the water sometime in the past, the intent of that man at the\nmoment is to stay alive, and he will need help in the near future or else he\nwill get washed away. We propose VisualComet, the novel framework of visual\ncommonsense reasoning tasks to predict events that might have happened before,\nevents that might happen next, and the intents of the people at present. To\nsupport research toward visual commonsense reasoning, we introduce the first\nlarge-scale repository of Visual Commonsense Graphs that consists of over 1.4\nmillion textual descriptions of visual commonsense inferences carefully\nannotated over a diverse set of 60,000 images, each paired with short video\nsummaries of before and after. In addition, we provide person-grounding (i.e.,\nco-reference links) between people appearing in the image and people mentioned\nin the textual commonsense descriptions, allowing for tighter integration\nbetween images and text. We establish strong baseline performances on this task\nand demonstrate that integration between visual and textual commonsense\nreasoning is the key and wins over non-integrative alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:02:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:37:10 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 13:11:10 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Park", "Jae Sung", ""], ["Bhagavatula", "Chandra", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "2004.10862", "submitter": "Kishan Parshotam", "authors": "Kishan Parshotam and Mert Kilickaya", "title": "Continual Learning of Object Instances", "comments": "Accepted to CVPR 2020: Workshop on Continual Learning in Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose continual instance learning - a method that applies the concept of\ncontinual learning to the task of distinguishing instances of the same object\ncategory. We specifically focus on the car object, and incrementally learn to\ndistinguish car instances from each other with metric learning. We begin our\npaper by evaluating current techniques. Establishing that catastrophic\nforgetting is evident in existing methods, we then propose two remedies.\nFirstly, we regularise metric learning via Normalised Cross-Entropy. Secondly,\nwe augment existing models with synthetic data transfer. Our extensive\nexperiments on three large-scale datasets, using two different architectures\nfor five different continual learning methods, reveal that Normalised\ncross-entropy and synthetic transfer leads to less forgetting in existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 21:09:02 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Parshotam", "Kishan", ""], ["Kilickaya", "Mert", ""]]}, {"id": "2004.10884", "submitter": "Martin Chatton", "authors": "Martin Chatton", "title": "Microscopy Image Restoration using Deep Learning on W2S", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage deep learning techniques to jointly denoise and super-resolve\nbiomedical images acquired with fluorescence microscopy. We develop a deep\nlearning algorithm based on the networks and method described in the recent W2S\npaper to solve a joint denoising and super-resolution problem. Specifically, we\naddress the restoration of SIM images from widefield images. Our TensorFlow\nmodel is trained on the W2S dataset of cell images and is made accessible\nonline in this repository: https://github.com/mchatton/w2s-tensorflow. On test\nimages, the model shows a visually-convincing denoising and increases the\nresolution by a factor of two compared to the input image. For a 512 $\\times$\n512 image, the inference takes less than 1 second on a Titan X GPU and about 15\nseconds on a common CPU. We further present the results of different variations\nof losses used in training.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 22:14:19 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Chatton", "Martin", ""]]}, {"id": "2004.10904", "submitter": "Zhengqin Li", "authors": "Zhengqin Li, Yu-Ying Yeh, Manmohan Chandraker", "title": "Through the Looking Glass: Neural 3D Reconstruction of Transparent\n  Shapes", "comments": "Accepted by CVPR 2020 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D shape of transparent objects using a small number of\nunconstrained natural images is an ill-posed problem. Complex light paths\ninduced by refraction and reflection have prevented both traditional and deep\nmultiview stereo from solving this challenge. We propose a physically-based\nnetwork to recover 3D shape of transparent objects using a few images acquired\nwith a mobile phone camera, under a known but arbitrary environment map. Our\nnovel contributions include a normal representation that enables the network to\nmodel complex light transport through local computation, a rendering layer that\nmodels refractions and reflections, a cost volume specifically designed for\nnormal refinement of transparent shapes and a feature mapping based on\npredicted normals for 3D point cloud reconstruction. We render a synthetic\ndataset to encourage the model to learn refractive light transport across\ndifferent views. Our experiments show successful recovery of high-quality 3D\ngeometry for complex transparent shapes using as few as 5-12 natural images.\nCode and data are publicly released.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 23:51:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 05:54:49 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Li", "Zhengqin", ""], ["Yeh", "Yu-Ying", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2004.10924", "submitter": "Lucas Tabelini Torres", "authors": "Lucas Tabelini, Rodrigo Berriel, Thiago M. Paix\\~ao, Claudine Badue,\n  Alberto F. De Souza and Thiago Oliveira-Santos", "title": "PolyLaneNet: Lane Estimation via Deep Polynomial Regression", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main factors that contributed to the large advances in autonomous\ndriving is the advent of deep learning. For safer self-driving vehicles, one of\nthe problems that has yet to be solved completely is lane detection. Since\nmethods for this task have to work in real-time (+30 FPS), they not only have\nto be effective (i.e., have high accuracy) but they also have to be efficient\n(i.e., fast). In this work, we present a novel method for lane detection that\nuses as input an image from a forward-looking camera mounted in the vehicle and\noutputs polynomials representing each lane marking in the image, via deep\npolynomial regression. The proposed method is shown to be competitive with\nexisting state-of-the-art methods in the TuSimple dataset while maintaining its\nefficiency (115 FPS). Additionally, extensive qualitative results on two\nadditional public datasets are presented, alongside with limitations in the\nevaluation metrics used by recent works for lane detection. Finally, we provide\nsource code and trained models that allow others to replicate all the results\nshown in this paper, which is surprisingly rare in state-of-the-art lane\ndetection methods. The full source code and pretrained models are available at\nhttps://github.com/lucastabelini/PolyLaneNet.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 01:23:02 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 17:02:54 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Tabelini", "Lucas", ""], ["Berriel", "Rodrigo", ""], ["Paix\u00e3o", "Thiago M.", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2004.10934", "submitter": "Alexey Bochkovskiy", "authors": "Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao", "title": "YOLOv4: Optimal Speed and Accuracy of Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a huge number of features which are said to improve Convolutional\nNeural Network (CNN) accuracy. Practical testing of combinations of such\nfeatures on large datasets, and theoretical justification of the result, is\nrequired. Some features operate on certain models exclusively and for certain\nproblems exclusively, or only for small-scale datasets; while some features,\nsuch as batch-normalization and residual-connections, are applicable to the\nmajority of models, tasks, and datasets. We assume that such universal features\ninclude Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections\n(CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT)\nand Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation,\nMosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and\ncombine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50)\nfor the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source\ncode is at https://github.com/AlexeyAB/darknet\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 02:10:02 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Bochkovskiy", "Alexey", ""], ["Wang", "Chien-Yao", ""], ["Liao", "Hong-Yuan Mark", ""]]}, {"id": "2004.10936", "submitter": "Siyu Liao", "authors": "Chunhua Deng, Siyu Liao, Yi Xie, Keshab K. Parhi, Xuehai Qian, Bo Yuan", "title": "PERMDNN: Efficient Compressed DNN Architecture with Permuted Diagonal\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) has emerged as the most important and popular\nartificial intelligent (AI) technique. The growth of model size poses a key\nenergy efficiency challenge for the underlying computing platform. Thus, model\ncompression becomes a crucial problem. However, the current approaches are\nlimited by various drawbacks. Specifically, network sparsification approach\nsuffers from irregularity, heuristic nature and large indexing overhead. On the\nother hand, the recent structured matrix-based approach (i.e., CirCNN) is\nlimited by the relatively complex arithmetic computation (i.e., FFT), less\nflexible compression ratio, and its inability to fully utilize input sparsity.\nTo address these drawbacks, this paper proposes PermDNN, a novel approach to\ngenerate and execute hardware-friendly structured sparse DNN models using\npermuted diagonal matrices. Compared with unstructured sparsification approach,\nPermDNN eliminates the drawbacks of indexing overhead, non-heuristic\ncompression effects and time-consuming retraining. Compared with circulant\nstructure-imposing approach, PermDNN enjoys the benefits of higher reduction in\ncomputational complexity, flexible compression ratio, simple arithmetic\ncomputation and full utilization of input sparsity. We propose PermDNN\narchitecture, a multi-processing element (PE) fully-connected (FC)\nlayer-targeted computing engine. The entire architecture is highly scalable and\nflexible, and hence it can support the needs of different applications with\ndifferent model configurations. We implement a 32-PE design using CMOS 28nm\ntechnology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout,\n5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on\ndifferent workloads. Compared with CirCNN, PermDNN achieves 11.51x higher\nthroughput and 3.89x better energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 02:26:40 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Deng", "Chunhua", ""], ["Liao", "Siyu", ""], ["Xie", "Yi", ""], ["Parhi", "Keshab K.", ""], ["Qian", "Xuehai", ""], ["Yuan", "Bo", ""]]}, {"id": "2004.10943", "submitter": "Luis Felipe De Araujo Zeni", "authors": "Luis Felipe Zeni and Claudio Jung", "title": "Distilling Knowledge from Refinement in Multiple Instance Detection\n  Networks", "comments": "published at CVPR 2020 Deepvision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD) aims to tackle the object detection\nproblem using only labeled image categories as supervision. A common approach\nused in WSOD to deal with the lack of localization information is Multiple\nInstance Learning, and in recent years methods started adopting Multiple\nInstance Detection Networks (MIDN), which allows training in an end-to-end\nfashion. In general, these methods work by selecting the best instance from a\npool of candidates and then aggregating other instances based on similarity. In\nthis work, we claim that carefully selecting the aggregation criteria can\nconsiderably improve the accuracy of the learned detector. We start by\nproposing an additional refinement step to an existing approach (OICR), which\nwe call refinement knowledge distillation. Then, we present an adaptive\nsupervision aggregation function that dynamically changes the aggregation\ncriteria for selecting boxes related to one of the ground-truth classes,\nbackground, or even ignored during the generation of each refinement module\nsupervision. Experiments in Pascal VOC 2007 demonstrate that our Knowledge\nDistillation and smooth aggregation function significantly improves the\nperformance of OICR in the weakly supervised object detection and weakly\nsupervised object localization tasks. These improvements make the Boosted-OICR\ncompetitive again versus other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 02:49:40 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zeni", "Luis Felipe", ""], ["Jung", "Claudio", ""]]}, {"id": "2004.10955", "submitter": "Xide Xia", "authors": "Xide Xia, Meng Zhang, Tianfan Xue, Zheng Sun, Hui Fang, Brian Kulis,\n  and Jiawen Chen", "title": "Joint Bilateral Learning for Real-time Universal Photorealistic Style\n  Transfer", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic style transfer is the task of transferring the artistic style\nof an image onto a content target, producing a result that is plausibly taken\nwith a camera. Recent approaches, based on deep neural networks, produce\nimpressive results but are either too slow to run at practical resolutions, or\nstill contain objectionable artifacts. We propose a new end-to-end model for\nphotorealistic style transfer that is both fast and inherently generates\nphotorealistic results. The core of our approach is a feed-forward neural\nnetwork that learns local edge-aware affine transforms that automatically obey\nthe photorealism constraint. When trained on a diverse set of images and a\nvariety of styles, our model can robustly apply style transfer to an arbitrary\npair of input images. Compared to the state of the art, our method produces\nvisually superior results and is three orders of magnitude faster, enabling\nreal-time performance at 4K on a mobile phone. We validate our method with\nablation and user studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 03:31:24 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 13:02:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Xia", "Xide", ""], ["Zhang", "Meng", ""], ["Xue", "Tianfan", ""], ["Sun", "Zheng", ""], ["Fang", "Hui", ""], ["Kulis", "Brian", ""], ["Chen", "Jiawen", ""]]}, {"id": "2004.10956", "submitter": "Xiaoyu Tao", "authors": "Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei,\n  Yihong Gong", "title": "Few-Shot Class-Incremental Learning", "comments": "Accepted by CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to incrementally learn new classes is crucial to the development\nof real-world artificial intelligence systems. In this paper, we focus on a\nchallenging but practical few-shot class-incremental learning (FSCIL) problem.\nFSCIL requires CNN models to incrementally learn new classes from very few\nlabelled samples, without forgetting the previously learned ones. To address\nthis problem, we represent the knowledge using a neural gas (NG) network, which\ncan learn and preserve the topology of the feature manifold formed by different\nclasses. On this basis, we propose the TOpology-Preserving knowledge\nInCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old\nclasses by stabilizing NG's topology and improves the representation learning\nfor few-shot new classes by growing and adapting NG to new training samples.\nComprehensive experimental results demonstrate that our proposed method\nsignificantly outperforms other state-of-the-art class-incremental learning\nmethods on CIFAR100, miniImageNet, and CUB200 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 03:38:33 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 02:12:32 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Tao", "Xiaoyu", ""], ["Hong", "Xiaopeng", ""], ["Chang", "Xinyuan", ""], ["Dong", "Songlin", ""], ["Wei", "Xing", ""], ["Gong", "Yihong", ""]]}, {"id": "2004.10959", "submitter": "Jingwei Song", "authors": "Jingwei Song, Shaobo Xia, Jun Wang, Mitesh Patel, and Dong Chen", "title": "Uncertainty Quantification for Hyperspectral Image Denoising Frameworks\n  based on Low-rank Matrix Approximation", "comments": "Accepted for publication by IEEE Transactions on Geoscience and\n  Remote Sensing. IEEE Transactions on Geoscience and Remote Sensing (TGRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding-window based low-rank matrix approximation (LRMA) is a technique\nwidely used in hyperspectral images (HSIs) denoising or completion. However,\nthe uncertainty quantification of the restored HSI has not been addressed to\ndate. Accurate uncertainty quantification of the denoised HSI facilitates to\napplications such as multi-source or multi-scale data fusion, data\nassimilation, and product uncertainty quantification, since these applications\nrequire an accurate approach to describe the statistical distributions of the\ninput data. Therefore, we propose a prior-free closed-form element-wise\nuncertainty quantification method for LRMA-based HSI restoration. Our\nclosed-form algorithm overcomes the difficulty of the HSI patch mixing problem\ncaused by the sliding-window strategy used in the conventional LRMA process.\nThe proposed approach only requires the uncertainty of the observed HSI and\nprovides the uncertainty result relatively rapidly and with similar\ncomputational complexity as the LRMA technique. We conduct extensive\nexperiments to validate the estimation accuracy of the proposed closed-form\nuncertainty approach. The method is robust to at least 10% random impulse noise\nat the cost of 10-20% of additional processing time compared to the LRMA. The\nexperiments indicate that the proposed closed-form uncertainty quantification\nmethod is more applicable to real-world applications than the baseline Monte\nCarlo test, which is computationally expensive. The code is available in the\nattachment and will be released after the acceptance of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 03:56:30 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 10:02:10 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 09:32:41 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Song", "Jingwei", ""], ["Xia", "Shaobo", ""], ["Wang", "Jun", ""], ["Patel", "Mitesh", ""], ["Chen", "Dong", ""]]}, {"id": "2004.10963", "submitter": "Yueming Yin", "authors": "Yueming Yin, Zhen Yang, Haifeng Hu and Xiaofu Wu", "title": "Metric-Learning-Assisted Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain alignment (DA) has been widely used in unsupervised domain adaptation.\nMany existing DA methods assume that a low source risk, together with the\nalignment of distributions of source and target, means a low target risk. In\nthis paper, we show that this does not always hold. We thus propose a novel\nmetric-learning-assisted domain adaptation (MLA-DA) method, which employs a\nnovel triplet loss for helping better feature alignment. We explore the\nrelationship between the second largest probability of a target sample's\nprediction and its distance to the decision boundary. Based on the\nrelationship, we propose a novel mechanism to adaptively adjust the margin in\nthe triplet loss according to target predictions. Experimental results show\nthat the use of proposed triplet loss can achieve clearly better results. We\nalso demonstrate the performance improvement of MLA-DA on all four standard\nbenchmarks compared with the state-of-the-art unsupervised domain adaptation\nmethods. Furthermore, MLA-DA shows stable performance in robust experiments.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 04:20:02 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 12:22:47 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 09:41:08 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Yin", "Yueming", ""], ["Yang", "Zhen", ""], ["Hu", "Haifeng", ""], ["Wu", "Xiaofu", ""]]}, {"id": "2004.10966", "submitter": "Tasmia Tasrin", "authors": "Tasmia Tasrin, Md Sultan Al Nahian and Brent Harrison", "title": "Visual Question Answering Using Semantic Information from Image\n  Descriptions", "comments": "6 pages, 5 figures, The 34th International FLAIRS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a deep neural architecture that uses an attention\nmechanism which utilizes region based image features, the natural language\nquestion asked, and semantic knowledge extracted from the regions of an image\nto produce open-ended answers for questions asked in a visual question\nanswering (VQA) task. The combination of both region based features and region\nbased textual information about the image bolsters a model to more accurately\nrespond to questions and potentially do so with less required training data. We\nevaluate our proposed architecture on a VQA task against a strong baseline and\nshow that our method achieves excellent results on this task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 04:35:04 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 18:09:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tasrin", "Tasmia", ""], ["Nahian", "Md Sultan Al", ""], ["Harrison", "Brent", ""]]}, {"id": "2004.10977", "submitter": "Hao Yan", "authors": "Hao Yan, Marco Grasso, Kamran Paynabar, and Bianca Maria Colosimo", "title": "Real-time Detection of Clustered Events in Video-imaging data with\n  Applications to Additive Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of video-imaging data for in-line process monitoring applications has\nbecome more and more popular in the industry. In this framework,\nspatio-temporal statistical process monitoring methods are needed to capture\nthe relevant information content and signal possible out-of-control states.\nVideo-imaging data are characterized by a spatio-temporal variability structure\nthat depends on the underlying phenomenon, and typical out-of-control patterns\nare related to the events that are localized both in time and space. In this\npaper, we propose an integrated spatio-temporal decomposition and regression\napproach for anomaly detection in video-imaging data. Out-of-control events are\ntypically sparse spatially clustered and temporally consistent. Therefore, the\ngoal is to not only detect the anomaly as quickly as possible (\"when\") but also\nlocate it (\"where\"). The proposed approach works by decomposing the original\nspatio-temporal data into random natural events, sparse spatially clustered and\ntemporally consistent anomalous events, and random noise. Recursive estimation\nprocedures for spatio-temporal regression are presented to enable the real-time\nimplementation of the proposed methodology. Finally, a likelihood ratio test\nprocedure is proposed to detect when and where the hotspot happens. The\nproposed approach was applied to the analysis of video-imaging data to detect\nand locate local over-heating phenomena (\"hotspots\") during the layer-wise\nprocess in a metal additive manufacturing process.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 05:32:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yan", "Hao", ""], ["Grasso", "Marco", ""], ["Paynabar", "Kamran", ""], ["Colosimo", "Bianca Maria", ""]]}, {"id": "2004.10987", "submitter": "Qingsen Yan", "authors": "Qingsen Yan, Bo Wang, Dong Gong, Chuan Luo, Wei Zhao, Jianhu Shen,\n  Qinfeng Shi, Shuo Jin, Liang Zhang and Zheng You", "title": "COVID-19 Chest CT Image Segmentation -- A Deep Convolutional Neural\n  Network Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel coronavirus disease 2019 (COVID-19) was detected and has spread\nrapidly across various countries around the world since the end of the year\n2019, Computed Tomography (CT) images have been used as a crucial alternative\nto the time-consuming RT-PCR test. However, pure manual segmentation of CT\nimages faces a serious challenge with the increase of suspected cases,\nresulting in urgent requirements for accurate and automatic segmentation of\nCOVID-19 infections. Unfortunately, since the imaging characteristics of the\nCOVID-19 infection are diverse and similar to the backgrounds, existing medical\nimage segmentation methods cannot achieve satisfactory performance. In this\nwork, we try to establish a new deep convolutional neural network tailored for\nsegmenting the chest CT images with COVID-19 infections. We firstly maintain a\nlarge and new chest CT image dataset consisting of 165,667 annotated chest CT\nimages from 861 patients with confirmed COVID-19. Inspired by the observation\nthat the boundary of the infected lung can be enhanced by adjusting the global\nintensity, in the proposed deep CNN, we introduce a feature variation block\nwhich adaptively adjusts the global properties of the features for segmenting\nCOVID-19 infection. The proposed FV block can enhance the capability of feature\nrepresentation effectively and adaptively for diverse cases. We fuse features\nat different scales by proposing Progressive Atrous Spatial Pyramid Pooling to\nhandle the sophisticated infection areas with diverse appearance and shapes. We\nconducted experiments on the data collected in China and Germany and show that\nthe proposed deep CNN can produce impressive performance effectively.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 06:09:16 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 01:45:16 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yan", "Qingsen", ""], ["Wang", "Bo", ""], ["Gong", "Dong", ""], ["Luo", "Chuan", ""], ["Zhao", "Wei", ""], ["Shen", "Jianhu", ""], ["Shi", "Qinfeng", ""], ["Jin", "Shuo", ""], ["Zhang", "Liang", ""], ["You", "Zheng", ""]]}, {"id": "2004.10998", "submitter": "Jun Wan", "authors": "Ajian Liu, Xuan Li, Jun Wan, Sergio Escalera, Hugo Jair Escalante,\n  Meysam Madadi, Yi Jin, Zhuoyuan Wu, Xiaogang Yu, Zichang Tan, Qi Yuan, Ruikun\n  Yang, Benjia Zhou, Guodong Guo, Stan Z. Li", "title": "Cross-ethnicity Face Anti-spoofing Recognition Challenge: A Review", "comments": "18 figures, 6 tables, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is critical to prevent face recognition systems from a\nsecurity breach. The biometrics community has %possessed achieved impressive\nprogress recently due the excellent performance of deep neural networks and the\navailability of large datasets. Although ethnic bias has been verified to\nseverely affect the performance of face recognition systems, it still remains\nan open research problem in face anti-spoofing. Recently, a multi-ethnic face\nanti-spoofing dataset, CASIA-SURF CeFA, has been released with the goal of\nmeasuring the ethnic bias. It is the largest up to date cross-ethnicity face\nanti-spoofing dataset covering $3$ ethnicities, $3$ modalities, $1,607$\nsubjects, 2D plus 3D attack types, and the first dataset including explicit\nethnic labels among the recently released datasets for face anti-spoofing. We\norganized the Chalearn Face Anti-spoofing Attack Detection Challenge which\nconsists of single-modal (e.g., RGB) and multi-modal (e.g., RGB, Depth,\nInfrared (IR)) tracks around this novel resource to boost research aiming to\nalleviate the ethnic bias. Both tracks have attracted $340$ teams in the\ndevelopment stage, and finally 11 and 8 teams have submitted their codes in the\nsingle-modal and multi-modal face anti-spoofing recognition challenges,\nrespectively. All the results were verified and re-ran by the organizing team,\nand the results were used for the final ranking. This paper presents an\noverview of the challenge, including its design, evaluation protocol and a\nsummary of results. We analyze the top ranked solutions and draw conclusions\nderived from the competition. In addition we outline future work directions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 06:43:08 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Liu", "Ajian", ""], ["Li", "Xuan", ""], ["Wan", "Jun", ""], ["Escalera", "Sergio", ""], ["Escalante", "Hugo Jair", ""], ["Madadi", "Meysam", ""], ["Jin", "Yi", ""], ["Wu", "Zhuoyuan", ""], ["Yu", "Xiaogang", ""], ["Tan", "Zichang", ""], ["Yuan", "Qi", ""], ["Yang", "Ruikun", ""], ["Zhou", "Benjia", ""], ["Guo", "Guodong", ""], ["Li", "Stan Z.", ""]]}, {"id": "2004.10999", "submitter": "Zilin Wang", "authors": "Zengyuan Guo, Zilin Wang, Zhihui Wang, Wanli Ouyang, Haojie Li, Wen\n  Gao", "title": "Location-Aware Feature Selection Text Detection Network", "comments": "10 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression-based text detection methods have already achieved promising\nperformances with simple network structure and high efficiency. However, they\nare behind in accuracy comparing with recent segmentation-based text detectors.\nIn this work, we discover that one important reason to this case is that\nregression-based methods usually utilize a fixed feature selection way, i.e.\nselecting features in a single location or in neighbor regions, to predict\ncomponents of the bounding box, such as the distances to the boundaries or the\nrotation angle. The features selected through this way sometimes are not the\nbest choices for predicting every component of a text bounding box and thus\ndegrade the accuracy performance. To address this issue, we propose a novel\nLocation-Aware feature Selection text detection Network (LASNet). LASNet\nselects suitable features from different locations to separately predict the\nfive components of a bounding box and gets the final bounding box through the\ncombination of these components. Specifically, instead of using the\nclassification score map to select one feature for predicting the whole\nbounding box as most of the existing methods did, the proposed LASNet first\nlearn five new confidence score maps to indicate the prediction accuracy of the\nbounding box components, respectively. Then, a Location-Aware Feature Selection\nmechanism (LAFS) is designed to weightily fuse the top-$K$ prediction results\nfor each component according to their confidence score, and to combine the all\nfive fused components into a final bounding box. As a result, LASNet predicts\nthe more accurate bounding boxes by using a learnable feature selection way.\nThe experimental results demonstrate that our LASNet achieves state-of-the-art\nperformance with single-model and single-scale testing, outperforming all\nexisting regression-based detectors.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 06:45:26 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 02:26:21 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Guo", "Zengyuan", ""], ["Wang", "Zilin", ""], ["Wang", "Zhihui", ""], ["Ouyang", "Wanli", ""], ["Li", "Haojie", ""], ["Gao", "Wen", ""]]}, {"id": "2004.11001", "submitter": "Michael Gadermayr", "authors": "Michael Gadermayr, Maximilian Tschuchnig, Laxmi Gupta, Dorit Merhof,\n  Nils Kr\\\"amer, Daniel Truhn, Burkhard Gess", "title": "An Asymmetric Cycle-Consistency Loss for Dealing with Many-to-One\n  Mappings in Image Translation: A Study on Thigh MR Scans", "comments": "Presented at IEEE ISBI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks using a cycle-consistency loss facilitate\nunpaired training of image-translation models and thereby exhibit a very high\npotential in manifold medical applications. However, the fact that images in\none domain potentially map to more than one image in another domain (e.g. in\ncase of pathological changes) exhibits a major challenge for training the\nnetworks. In this work, we offer a solution to improve the training process in\ncase of many-to-one mappings by modifying the cycle-consistency loss. We show\nformally and empirically that the proposed method improves the performance\nsignificantly without radically changing the architecture and without\nincreasing the overall complexity. We evaluate our method on thigh MRI scans\nwith the final goal of segmenting the muscle in fat-infiltrated patients' data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 06:59:58 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 06:00:07 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 18:06:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gadermayr", "Michael", ""], ["Tschuchnig", "Maximilian", ""], ["Gupta", "Laxmi", ""], ["Merhof", "Dorit", ""], ["Kr\u00e4mer", "Nils", ""], ["Truhn", "Daniel", ""], ["Gess", "Burkhard", ""]]}, {"id": "2004.11020", "submitter": "Namhyuk Ahn", "authors": "Namhyuk Ahn and Jaejun Yoo and Kyung-Ah Sohn", "title": "SimUSR: A Simple but Strong Baseline for Unsupervised Image\n  Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle a fully unsupervised super-resolution problem, i.e.,\nneither paired images nor ground truth HR images. We assume that low resolution\n(LR) images are relatively easy to collect compared to high resolution (HR)\nimages. By allowing multiple LR images, we build a set of pseudo pairs by\ndenoising and downsampling LR images and cast the original unsupervised problem\ninto a supervised learning problem but in one level lower. Though this line of\nstudy is easy to think of and thus should have been investigated prior to any\ncomplicated unsupervised methods, surprisingly, there are currently none. Even\nmore, we show that this simple method outperforms the state-of-the-art\nunsupervised method with a dramatically shorter latency at runtime, and\nsignificantly reduces the gap to the HR supervised models. We submitted our\nmethod in NTIRE 2020 super-resolution challenge and won 1st in PSNR, 2nd in\nSSIM, and 13th in LPIPS. This simple method should be used as the baseline to\nbeat in the future, especially when multiple LR images are allowed during the\ntraining phase. However, even in the zero-shot condition, we argue that this\nmethod can serve as a useful baseline to see the gap between supervised and\nunsupervised frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:27:41 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ahn", "Namhyuk", ""], ["Yoo", "Jaejun", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "2004.11021", "submitter": "Shrey Dabhi", "authors": "Shrey Dabhi, Kartavya Soni, Utkarsh Patel, Priyanka Sharma and\n  Manojkumar Parmar", "title": "Virtual SAR: A Synthetic Dataset for Deep Learning based Speckle Noise\n  Reduction Algorithms", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Radar (SAR) images contain a huge amount of information,\nhowever, the number of practical use-cases is limited due to the presence of\nspeckle noise in them. In recent years, deep learning based techniques have\nbrought significant improvement in the domain of denoising and image\nrestoration. However, further research has been hampered by the lack of\navailability of data suitable for training deep neural network based systems.\nWith this paper, we propose a standard way of generating synthetic data for the\ntraining of speckle reduction algorithms and demonstrate a use-case to advance\nresearch in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 08:27:45 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Dabhi", "Shrey", ""], ["Soni", "Kartavya", ""], ["Patel", "Utkarsh", ""], ["Sharma", "Priyanka", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2004.11051", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Laura Sevilla-Lara, Ernest Mwebaze, Dina Machuve,\n  Hamed Alemohammad, David Guerena", "title": "Proceedings of the ICLR Workshop on Computer Vision for Agriculture\n  (CV4A) 2020", "comments": "14 papers accepted, 4 as oral, 10 as spotlights", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the proceedings of the Computer Vision for Agriculture (CV4A)\nWorkshop that was held in conjunction with the International Conference on\nLearning Representations (ICLR) 2020.\n  The Computer Vision for Agriculture (CV4A) 2020 workshop was scheduled to be\nheld in Addis Ababa, Ethiopia, on April 26th, 2020. It was held virtually that\nsame day due to the COVID-19 pandemic. The workshop was held in conjunction\nwith the International Conference on Learning Representations (ICLR) 2020.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:11:52 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 12:33:59 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 16:50:58 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Sevilla-Lara", "Laura", ""], ["Mwebaze", "Ernest", ""], ["Machuve", "Dina", ""], ["Alemohammad", "Hamed", ""], ["Guerena", "David", ""]]}, {"id": "2004.11072", "submitter": "Marvin Klingner", "authors": "Marvin Klingner, Andreas B\\\"ar, Tim Fingscheidt", "title": "Improved Noise and Attack Robustness for Semantic Segmentation by Using\n  Multi-Task Training with Self-Supervised Depth Estimation", "comments": "CVPR 2020 Workshop on Safe Artificial Intelligence for Automated\n  Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current approaches for neural network training often aim at improving\nperformance, less focus is put on training methods aiming at robustness towards\nvarying noise conditions or directed attacks by adversarial examples. In this\npaper, we propose to improve robustness by a multi-task training, which extends\nsupervised semantic segmentation by a self-supervised monocular depth\nestimation on unlabeled videos. This additional task is only performed during\ntraining to improve the semantic segmentation model's robustness at test time\nunder several input perturbations. Moreover, we even find that our joint\ntraining approach also improves the performance of the model on the original\n(supervised) semantic segmentation task. Our evaluation exhibits a particular\nnovelty in that it allows to mutually compare the effect of input noises and\nadversarial attacks on the robustness of the semantic segmentation. We show the\neffectiveness of our method on the Cityscapes dataset, where our multi-task\ntraining approach consistently outperforms the single-task semantic\nsegmentation baseline in terms of both robustness vs. noise and in terms of\nadversarial attacks, without the need for depth labels in training.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:03:56 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Klingner", "Marvin", ""], ["B\u00e4r", "Andreas", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2004.11075", "submitter": "Jonas Geiping", "authors": "Jonas Geiping, Fjedor Gaede, Hartmut Bauermeister and Michael Moeller", "title": "Fast Convex Relaxations using Graph Discretizations", "comments": "20 pages, 9 figures. BMVC 2020 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching and partitioning problems are fundamentals of computer vision\napplications with examples in multilabel segmentation, stereo estimation and\noptical-flow computation. These tasks can be posed as non-convex energy\nminimization problems and solved near-globally optimal by recent convex lifting\napproaches. Yet, applying these techniques comes with a significant\ncomputational effort, reducing their feasibility in practical applications. We\ndiscuss spatial discretization of continuous partitioning problems into a graph\nstructure, generalizing discretization onto a Cartesian grid. This setup allows\nus to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit,\nmassively decreasing the computational effort for lifted partitioning problems\ncompared to a Cartesian grid, while optimal energy values remain similar: The\nglobal matching is still solved near-globally optimal. We discuss this\nmethodology in detail and show examples in multi-label segmentation by minimal\npartitions and stereo estimation, where we demonstrate that the proposed graph\ndiscretization can reduce runtime as well as memory consumption of convex\nrelaxations of matching problems by up to a factor of 10.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:14:38 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 11:42:38 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Geiping", "Jonas", ""], ["Gaede", "Fjedor", ""], ["Bauermeister", "Hartmut", ""], ["Moeller", "Michael", ""]]}, {"id": "2004.11076", "submitter": "Wang Zejin", "authors": "Zejin Wang, Guoqing Li, Xi Chen, Hua Han", "title": "DAN: A Deformation-Aware Network for Consecutive Biomedical Image\n  Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuity of biological tissue between consecutive biomedical images\nmakes it possible for the video interpolation algorithm, to recover large area\ndefects and tears that are common in biomedical images. However, noise and blur\ndifferences, large deformation, and drift between biomedical images, make the\ntask challenging. To address the problem, this paper introduces a\ndeformation-aware network to synthesize each pixel in accordance with the\ncontinuity of biological tissue. First, we develop a deformation-aware layer\nfor consecutive biomedical images interpolation that implicitly adopting global\nperceptual deformation. Second, we present an adaptive style-balance loss to\ntake the style differences of consecutive biomedical images such as blur and\nnoise into consideration. Guided by the deformation-aware module, we synthesize\neach pixel from a global domain adaptively which further improves the\nperformance of pixel synthesis. Quantitative and qualitative experiments on the\nbenchmark dataset show that the proposed method is superior to the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:14:44 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Wang", "Zejin", ""], ["Li", "Guoqing", ""], ["Chen", "Xi", ""], ["Han", "Hua", ""]]}, {"id": "2004.11085", "submitter": "Raphael Memmesheimer", "authors": "Raphael Memmesheimer, Nick Theisen, Dietrich Paulus", "title": "SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action\n  Recognition", "comments": "8 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing an activity with a single reference sample using metric learning\napproaches is a promising research field. The majority of few-shot methods\nfocus on object recognition or face-identification. We propose a metric\nlearning approach to reduce the action recognition problem to a nearest\nneighbor search in embedding space. We encode signals into images and extract\nfeatures using a deep residual CNN. Using triplet loss, we learn a feature\nembedding. The resulting encoder transforms features into an embedding space in\nwhich closer distances encode similar actions while higher distances encode\ndifferent actions. Our approach is based on a signal level formulation and\nremains flexible across a variety of modalities. It further outperforms the\nbaseline on the large scale NTU RGB+D 120 dataset for the One-Shot action\nrecognition protocol by 5.6%. With just 60% of the training data, our approach\nstill outperforms the baseline approach by 3.7%. With 40% of the training data,\nour approach performs comparably well to the second follow up. Further, we show\nthat our approach generalizes well in experiments on the UTD-MHAD dataset for\ninertial, skeleton and fused data and the Simitate dataset for motion capturing\ndata. Furthermore, our inter-joint and inter-sensor experiments suggest good\ncapabilities on previously unseen setups.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:28:27 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:58:35 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 12:37:18 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 13:16:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Memmesheimer", "Raphael", ""], ["Theisen", "Nick", ""], ["Paulus", "Dietrich", ""]]}, {"id": "2004.11114", "submitter": "Patrick McClure", "authors": "Patrick McClure, Dustin Moraczewski, Ka Chun Lam, Adam Thomas,\n  Francisco Pereira", "title": "Improving the Interpretability of fMRI Decoding using Deep Neural\n  Networks and Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are being increasingly used to make predictions\nfrom functional magnetic resonance imaging (fMRI) data. However, they are\nwidely seen as uninterpretable \"black boxes\", as it can be difficult to\ndiscover what input information is used by the DNN in the process, something\nimportant in both cognitive neuroscience and clinical applications. A saliency\nmap is a common approach for producing interpretable visualizations of the\nrelative importance of input features for a prediction. However, methods for\ncreating maps often fail due to DNNs being sensitive to input noise, or by\nfocusing too much on the input and too little on the model. It is also\nchallenging to evaluate how well saliency maps correspond to the truly relevant\ninput information, as ground truth is not always available. In this paper, we\nreview a variety of methods for producing gradient-based saliency maps, and\npresent a new adversarial training method we developed to make DNNs robust to\ninput noise, with the goal of improving interpretability. We introduce two\nquantitative evaluation procedures for saliency map methods in fMRI, applicable\nwhenever a DNN or linear model is being trained to decode some information from\nimaging data. We evaluate the procedures using a synthetic dataset where the\ncomplex activation structure is known, and on saliency maps produced for DNN\nand linear models for task decoding in the Human Connectome Project (HCP)\ndataset. Our key finding is that saliency maps produced with different methods\nvary widely in interpretability, in both in synthetic and HCP fMRI data.\nStrikingly, even when DNN and linear models decode at comparable levels of\nperformance, DNN saliency maps score higher on interpretability than linear\nmodel saliency maps (derived via weights or gradient). Finally, saliency maps\nproduced with our adversarial training method outperform those from other\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 12:56:24 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 16:15:40 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 16:01:57 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["McClure", "Patrick", ""], ["Moraczewski", "Dustin", ""], ["Lam", "Ka Chun", ""], ["Thomas", "Adam", ""], ["Pereira", "Francisco", ""]]}, {"id": "2004.11138", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky, Wenke Lee", "title": "The Creation and Detection of Deepfakes: A Survey", "comments": null, "journal-ref": "ACM Computing Surveys (CSUR), 2020, preprint", "doi": "10.1145/3425780", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative deep learning algorithms have progressed to a point where it is\ndifficult to tell the difference between what is real and what is fake. In\n2018, it was discovered how easy it is to use this technology for unethical and\nmalicious applications, such as the spread of misinformation, impersonation of\npolitical leaders, and the defamation of innocent individuals. Since then,\nthese `deepfakes' have advanced significantly.\n  In this paper, we explore the creation and detection of deepfakes and provide\nan in-depth view of how these architectures work. The purpose of this survey is\nto provide the reader with a deeper understanding of (1) how deepfakes are\ncreated and detected, (2) the current trends and advancements in this domain,\n(3) the shortcomings of the current defense solutions, and (4) the areas which\nrequire further research and attention.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:35:49 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 22:32:42 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 22:44:33 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mirsky", "Yisroel", ""], ["Lee", "Wenke", ""]]}, {"id": "2004.11168", "submitter": "Fernando Alonso-Fernandez", "authors": "Nathalie Tkauc, Thao Tran, Kevin Hernandez-Diaz, Fernando\n  Alonso-Fernandez", "title": "Cloud-Based Face and Speech Recognition for Access Control Applications", "comments": "Published at Proc. 6th International Workshop on Security and Privacy\n  in the Cloud, SPC, in conjunction with IEEE Conference on Communications and\n  Network Security, CNS, Avignon, France, 29 June - 1 July 2020", "journal-ref": "Proc. 6th International Workshop on Security and Privacy in the\n  Cloud, SPC, in conjunction with IEEE Conference on Communications and Network\n  Security, CNS, Avignon, France, 29 June - 1 July 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the implementation of a system to recognize employees\nand visitors wanting to gain access to a physical office through face images\nand speech-to-text recognition. The system helps employees to unlock the\nentrance door via face recognition without the need of tag-keys or cards. To\nprevent spoofing attacks and increase security, a randomly generated code is\nsent to the employee, who then has to type it into the screen. On the other\nhand, visitors and delivery persons are provided with a speech-to-text service\nwhere they utter the name of the employee that they want to meet, and the\nsystem then sends a notification to the right employee automatically. The\nhardware of the system is constituted by two Raspberry Pi, a 7-inch LCD-touch\ndisplay, a camera, and a sound card with a microphone and speaker. To carry out\nface recognition and speech-to-text conversion, the cloud-based platforms\nAmazon Web Services and the Google Speech-to-Text API service are used\nrespectively. The two-step face authentication mechanism for employees provides\nan increased level of security and protection against spoofing attacks without\nthe need of carrying key-tags or access cards, while disturbances by visitors\nor couriers are minimized by notifying their arrival to the right employee,\nwithout disturbing other co-workers by means of ring-bells.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:57:55 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 13:07:24 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Tkauc", "Nathalie", ""], ["Tran", "Thao", ""], ["Hernandez-Diaz", "Kevin", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "2004.11178", "submitter": "Artur Jordao", "authors": "Artur Jordao, Fernando Akio, Maiko Lie and William Robson Schwartz", "title": "Stage-Wise Neural Architecture Search", "comments": "Accepted for publication at International Conference on Pattern\n  Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern convolutional networks such as ResNet and NASNet have achieved\nstate-of-the-art results in many computer vision applications. These\narchitectures consist of stages, which are sets of layers that operate on\nrepresentations in the same resolution. It has been demonstrated that\nincreasing the number of layers in each stage improves the prediction ability\nof the network. However, the resulting architecture becomes computationally\nexpensive in terms of floating point operations, memory requirements and\ninference time. Thus, significant human effort is necessary to evaluate\ndifferent trade-offs between depth and performance. To handle this problem,\nrecent works have proposed to automatically design high-performance\narchitectures, mainly by means of neural architecture search (NAS). Current NAS\nstrategies analyze a large set of possible candidate architectures and, hence,\nrequire vast computational resources and take many GPUs days. Motivated by\nthis, we propose a NAS approach to efficiently design accurate and low-cost\nconvolutional architectures and demonstrate that an efficient strategy for\ndesigning these architectures is to learn the depth stage-by-stage. For this\npurpose, our approach increases depth incrementally in each stage taking into\naccount its importance, such that stages with low importance are kept shallow\nwhile stages with high importance become deeper. We conduct experiments on the\nCIFAR and different versions of ImageNet datasets, where we show that\narchitectures discovered by our approach achieve better accuracy and efficiency\nthan human-designed architectures. Additionally, we show that architectures\ndiscovered on CIFAR-10 can be successfully transferred to large datasets.\nCompared to previous NAS approaches, our method is substantially more\nefficient, as it evaluates one order of magnitude fewer models and yields\narchitectures on par with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:16:39 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 21:59:00 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Jordao", "Artur", ""], ["Akio", "Fernando", ""], ["Lie", "Maiko", ""], ["Schwartz", "William Robson", ""]]}, {"id": "2004.11186", "submitter": "Riku Murai", "authors": "Riku Murai, Sajad Saeedi, Paul H. J. Kelly", "title": "BIT-VO: Visual Odometry at 300 FPS using Binary Features from the Focal\n  Plane", "comments": "8 pages, 16 figures", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9341151", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focal-plane Sensor-processor (FPSP) is a next-generation camera technology\nwhich enables every pixel on the sensor chip to perform computation in\nparallel, on the focal plane where the light intensity is captured. SCAMP-5 is\na general-purpose FPSP used in this work and it carries out computations in the\nanalog domain before analog to digital conversion. By extracting features from\nthe image on the focal plane, data which is digitized and transferred is\nreduced. As a consequence, SCAMP-5 offers a high frame rate while maintaining\nlow energy consumption. Here, we present BIT-VO, which is, to the best of our\nknowledge, the first 6 Degrees of Freedom visual odometry algorithm which\nutilises the FPSP. Our entire system operates at 300 FPS in a natural scene,\nusing binary edges and corner features detected by the SCAMP-5.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:26:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Murai", "Riku", ""], ["Saeedi", "Sajad", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2004.11187", "submitter": "Fernando Alonso-Fernandez", "authors": "Felix Nilsson, Jens Jakobsen, Fernando Alonso-Fernandez", "title": "Detection and Classification of Industrial Signal Lights for Factory\n  Floors", "comments": "Published at Proc International Conference on Intelligent Systems and\n  Computer Vision, ISCV, Fez, Morocco, 9-11 June 2020", "journal-ref": "Proc International Conference on Intelligent Systems and Computer\n  Vision, ISCV, Fez, Morocco, 9-11 June 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial manufacturing has developed during the last decades from a\nlabor-intensive manual control of machines to a fully-connected automated\nprocess. The next big leap is known as industry 4.0, or smart manufacturing.\nWith industry 4.0 comes increased integration between IT systems and the\nfactory floor from the customer order system to final delivery of the product.\nOne benefit of this integration is mass production of individually customized\nproducts. However, this has proven challenging to implement into existing\nfactories, considering that their lifetime can be up to 30 years. The single\nmost important parameter to measure in a factory is the operating hours of each\nmachine. Operating hours can be affected by machine maintenance as well as\nre-configuration for different products. For older machines without\nconnectivity, the operating state is typically indicated by signal lights of\ngreen, yellow and red colours. Accordingly, the goal is to develop a solution\nwhich can measure the operational state using the input from a video camera\ncapturing a factory floor. Using methods commonly employed for traffic light\nrecognition in autonomous cars, a system with an accuracy of over 99% in the\nspecified conditions is presented. It is believed that if more diverse video\ndata becomes available, a system with high reliability that generalizes well\ncould be developed using a similar methodology.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 14:26:39 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 14:31:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nilsson", "Felix", ""], ["Jakobsen", "Jens", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "2004.11210", "submitter": "Stanley Chan", "authors": "Nicholas Chimitt and Stanley H. Chan", "title": "Simulating Anisoplanatic Turbulence by Sampling Inter-modal and\n  Spatially Correlated Zernike Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics astro-ph.IM cs.CV eess.IV physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating atmospheric turbulence is an essential task for evaluating\nturbulence mitigation algorithms and training learning-based methods. Advanced\nnumerical simulators for atmospheric turbulence are available, but they require\nevaluating wave propagation which is computationally expensive. In this paper,\nwe present a propagation-free method for simulating imaging through turbulence.\nThe key idea behind our work is a new method to draw inter-modal and spatially\ncorrelated Zernike coefficients. By establishing the equivalence between the\nangle-of-arrival correlation by Basu, McCrae and Fiorino (2015) and the\nmulti-aperture correlation by Chanan (1992), we show that the Zernike\ncoefficients can be drawn according to a covariance matrix defining the\ncorrelations. We propose fast and scalable sampling strategies to draw these\nsamples. The new method allows us to compress the wave propagation problem into\na sampling problem, hence making the new simulator significantly faster than\nexisting ones. Experimental results show that the simulator has an excellent\nmatch with the theory and real turbulence data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 15:05:39 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 03:03:08 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chimitt", "Nicholas", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2004.11233", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda", "title": "QUANOS- Adversarial Noise Sensitivity Driven Hybrid Quantization of\n  Neural Networks", "comments": "Accepted in ACM/IEEE International Symposium on Low Power Electronics\n  and Design (ISLPED), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial\nattacks, wherein, a model gets fooled by applying slight perturbations on the\ninput. With the advent of Internet-of-Things and the necessity to enable\nintelligence in embedded devices, low-power and secure hardware implementation\nof DNNs is vital. In this paper, we investigate the use of quantization to\npotentially resist adversarial attacks. Several recent studies have reported\nremarkable results in reducing the energy requirement of a DNN through\nquantization. However, no prior work has considered the relationship between\nadversarial sensitivity of a DNN and its effect on quantization. We propose\nQUANOS- a framework that performs layer-specific hybrid quantization based on\nAdversarial Noise Sensitivity (ANS). We identify a novel noise stability metric\n(ANS) for DNNs, i.e., the sensitivity of each layer's computation to\nadversarial noise. ANS allows for a principled way of determining optimal\nbit-width per layer that incurs adversarial robustness as well as\nenergy-efficiency with minimal loss in accuracy. Essentially, QUANOS assigns\nlayer significance based on its contribution to adversarial perturbation and\naccordingly scales the precision of the layers. A key advantage of QUANOS is\nthat it does not rely on a pre-trained model and can be applied in the initial\nstages of training. We evaluate the benefits of QUANOS on precision scalable\nMultiply and Accumulate (MAC) hardware architectures with data gating and\nsubword parallelism capabilities. Our experiments on CIFAR10, CIFAR100 datasets\nshow that QUANOS outperforms homogenously quantized 8-bit precision baseline in\nterms of adversarial robustness (3%-4% higher) while yielding improved\ncompression (>5x) and energy savings (>2x) at iso-accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:56:31 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 13:14:58 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Panda", "Priyadarshini", ""]]}, {"id": "2004.11245", "submitter": "Claire Voreiter", "authors": "Claire Voreiter (OBELIX), Jean-Christophe Burnel (OBELIX), Pierre\n  Lassalle (CNES), Marc Spigai (TAS), Romain Hugues (TAS), Nicolas Courty (FT\n  R&D, OBELIX)", "title": "A Cycle GAN Approach for Heterogeneous Domain Adaptation in Land Use\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of remote sensing and more specifically in Earth Observation,\nnew data are available every day, coming from different sensors. Leveraging on\nthose data in classification tasks comes at the price of intense labelling\ntasks that are not realistic in operational settings. While domain adaptation\ncould be useful to counterbalance this problem, most of the usual methods\nassume that the data to adapt are comparable (they belong to the same metric\nspace), which is not the case when multiple sensors are at stake. Heterogeneous\ndomain adaptation methods are a particular solution to this problem. We present\na novel method to deal with such cases, based on a modified cycleGAN version\nthat incorporates classification losses and a metric space alignment term. We\ndemonstrate its power on a land use classification tasks, with images from both\nGoogle Earth and Sentinel-2.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 08:16:18 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Voreiter", "Claire", "", "OBELIX"], ["Burnel", "Jean-Christophe", "", "OBELIX"], ["Lassalle", "Pierre", "", "CNES"], ["Spigai", "Marc", "", "TAS"], ["Hugues", "Romain", "", "TAS"], ["Courty", "Nicolas", "", "FT\n  R&D, OBELIX"]]}, {"id": "2004.11246", "submitter": "Aythami Morales", "authors": "Ignacio Serna, Aythami Morales, Julian Fierrez, Manuel Cebrian, Nick\n  Obradovich, and Iyad Rahwan", "title": "SensitiveLoss: Improving Accuracy and Fairness of Face Representations\n  with Discrimination-Aware Deep Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.01842", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a discrimination-aware learning method to improve both accuracy\nand fairness of biased face recognition algorithms. The most popular face\nrecognition benchmarks assume a distribution of subjects without paying much\nattention to their demographic attributes. In this work, we perform a\ncomprehensive discrimination-aware experimentation of deep learning-based face\nrecognition. We also propose a general formulation of algorithmic\ndiscrimination with application to face biometrics. The experiments include\ntree popular face recognition models and three public databases composed of\n64,000 identities from different demographic groups characterized by gender and\nethnicity. We experimentally show that learning processes based on the most\nused face databases have led to popular pre-trained deep face models that\npresent a strong algorithmic discrimination. We finally propose a\ndiscrimination-aware learning method, Sensitive Loss, based on the popular\ntriplet loss function and a sensitive triplet generator. Our approach works as\nan add-on to pre-trained networks and is used to improve their performance in\nterms of average accuracy and fairness. The method shows results comparable to\nstate-of-the-art de-biasing networks and represents a step forward to prevent\ndiscriminatory effects by automatic systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 10:32:16 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 16:22:01 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Serna", "Ignacio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Cebrian", "Manuel", ""], ["Obradovich", "Nick", ""], ["Rahwan", "Iyad", ""]]}, {"id": "2004.11250", "submitter": "Pu Zhao", "authors": "Wei Niu, Pu Zhao, Zheng Zhan, Xue Lin, Yanzhi Wang, Bin Ren", "title": "Towards Real-Time DNN Inference on Mobile Platforms with Model Pruning\n  and Compiler Optimization", "comments": "accepted by the IJCAI-PRICAI 2020 Demonstrations Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-end mobile platforms rapidly serve as primary computing devices for a\nwide range of Deep Neural Network (DNN) applications. However, the constrained\ncomputation and storage resources on these devices still pose significant\nchallenges for real-time DNN inference executions. To address this problem, we\npropose a set of hardware-friendly structured model pruning and compiler\noptimization techniques to accelerate DNN executions on mobile devices. This\ndemo shows that these optimizations can enable real-time mobile execution of\nmultiple DNN applications, including style transfer, DNN coloring and super\nresolution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 03:18:23 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Niu", "Wei", ""], ["Zhao", "Pu", ""], ["Zhan", "Zheng", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""]]}, {"id": "2004.11252", "submitter": "Edson Bollis Mst", "authors": "Edson Bollis, Helio Pedrini, and Sandra Avila", "title": "Weakly Supervised Learning Guided by Activation Mapping Applied to a\n  Novel Citrus Pest Benchmark", "comments": "Accepted to The 1st International Workshop on Agriculture-Vision\n  Workshop - CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pests and diseases are relevant factors for production losses in agriculture\nand, therefore, promote a huge investment in the prevention and detection of\nits causative agents. In many countries, Integrated Pest Management is the most\nwidely used process to prevent and mitigate the damages caused by pests and\ndiseases in citrus crops. However, its results are credited by humans who\nvisually inspect the orchards in order to identify the disease symptoms,\ninsects and mite pests. In this context, we design a weakly supervised learning\nprocess guided by saliency maps to automatically select regions of interest in\nthe images, significantly reducing the annotation task. In addition, we create\na large citrus pest benchmark composed of positive samples (six classes of mite\nspecies) and negative samples. Experiments conducted on two large datasets\ndemonstrate that our results are very promising for the problem of pest and\ndisease classification in the agriculture field.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 01:26:50 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Bollis", "Edson", ""], ["Pedrini", "Helio", ""], ["Avila", "Sandra", ""]]}, {"id": "2004.11253", "submitter": "S. M. Kamrul Hasan", "authors": "S. M. Kamrul Hasan, Cristian A. Linte", "title": "L-CO-Net: Learned Condensation-Optimization Network for Clinical\n  Parameter Estimation from Cardiac Cine MRI", "comments": "6 pages, 5 figures, IEEE Conference. arXiv admin note: text overlap\n  with arXiv:2004.02249", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we implement a fully convolutional segmenter featuring both a\nlearned group structure and a regularized weight-pruner to reduce the high\ncomputational cost in volumetric image segmentation. We validated our framework\non the ACDC dataset featuring one healthy and four pathology groups imaged\nthroughout the cardiac cycle. Our technique achieved Dice scores of 96.8% (LV\nblood-pool), 93.3% (RV blood-pool) and 90.0% (LV Myocardium) with five-fold\ncross-validation and yielded similar clinical parameters as those estimated\nfrom the ground truth segmentation data. Based on these results, this technique\nhas the potential to become an efficient and competitive cardiac image\nsegmentation tool that may be used for cardiac computer-aided diagnosis,\nplanning, and guidance applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 23:59:07 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Hasan", "S. M. Kamrul", ""], ["Linte", "Cristian A.", ""]]}, {"id": "2004.11273", "submitter": "Jianhe Yuan", "authors": "Jianhe Yuan and Zhihai He", "title": "Ensemble Generative Cleaning with Feedback Loops for Defending\n  Adversarial Attacks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective defense of deep neural networks against adversarial attacks remains\na challenging problem, especially under powerful white-box attacks. In this\npaper, we develop a new method called ensemble generative cleaning with\nfeedback loops (EGC-FL) for effective defense of deep neural networks. The\nproposed EGC-FL method is based on two central ideas. First, we introduce a\ntransformed deadzone layer into the defense network, which consists of an\northonormal transform and a deadzone-based activation function, to destroy the\nsophisticated noise pattern of adversarial attacks. Second, by constructing a\ngenerative cleaning network with a feedback loop, we are able to generate an\nensemble of diverse estimations of the original clean image. We then learn a\nnetwork to fuse this set of diverse estimations together to restore the\noriginal image. Our extensive experimental results demonstrate that our\napproach improves the state-of-art by large margins in both white-box and\nblack-box attacks. It significantly improves the classification accuracy for\nwhite-box PGD attacks upon the second best method by more than 29% on the SVHN\ndataset and more than 39% on the challenging CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:01:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yuan", "Jianhe", ""], ["He", "Zhihai", ""]]}, {"id": "2004.11296", "submitter": "Anand S", "authors": "S.Anand, K.Nagajothi, K.Nithya", "title": "Edge Detection using Stationary Wavelet Transform, HMM, and EM algorithm", "comments": "07 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stationary Wavelet Transform (SWT) is an efficient tool for edge analysis.\nThis paper a new edge detection technique using SWT based Hidden Markov Model\n(WHMM) along with the expectation-maximization (EM) algorithm is proposed. The\nSWT coefficients contain a hidden state and they indicate the SWT coefficient\nfits into an edge model or not. Laplacian and Gaussian model is used to check\nthe information of the state is an edge or no edge. This model is trained by an\nEM algorithm and the Viterbi algorithm is employed to recover the state. This\nalgorithm can be applied to noisy images efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:27:26 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Anand", "S.", ""], ["Nagajothi", "K.", ""], ["Nithya", "K.", ""]]}, {"id": "2004.11300", "submitter": "Luca Mariot", "authors": "Domagoj Jakobovic, Luca Manzoni, Luca Mariot, Stjepan Picek, Mauro\n  Castelli", "title": "CoInGP: Convolutional Inpainting with Genetic Programming", "comments": "21 pages, 8 figures, updated pre-print accepted at GECCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of Genetic Programming (GP) as a convolutional\npredictor for missing pixels in images. The training phase is performed by\nsweeping a sliding window over an image, where the pixels on the border\nrepresent the inputs of a GP tree. The output of the tree is taken as the\npredicted value for the central pixel. We consider two topologies for the\nsliding window, namely the Moore and the Von Neumann neighborhood. The best GP\ntree scoring the lowest prediction error over the training set is then used to\npredict the pixels in the test set. We experimentally assess our approach\nthrough two experiments. In the first one, we train a GP tree over a subset of\n1000 complete images from the MNIST dataset. The results show that GP can learn\nthe distribution of the pixels with respect to a simple baseline predictor,\nwith no significant differences observed between the two neighborhoods. In the\nsecond experiment, we train a GP convolutional predictor on two degraded\nimages, removing around 20% of their pixels. In this case, we observe that the\nMoore neighborhood works better, although the Von Neumann neighborhood allows\nfor a larger training set.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 16:31:58 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 10:23:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jakobovic", "Domagoj", ""], ["Manzoni", "Luca", ""], ["Mariot", "Luca", ""], ["Picek", "Stjepan", ""], ["Castelli", "Mauro", ""]]}, {"id": "2004.11336", "submitter": "Ana Martinazzo", "authors": "Ana Martinazzo, Mateus Espadoto, Nina S. T. Hirata", "title": "Self-supervised Learning for Astronomical Image Classification", "comments": "Accepted for ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Astronomy, a huge amount of image data is generated daily by photometric\nsurveys, which scan the sky to collect data from stars, galaxies and other\ncelestial objects. In this paper, we propose a technique to leverage unlabeled\nastronomical images to pre-train deep convolutional neural networks, in order\nto learn a domain-specific feature extractor which improves the results of\nmachine learning techniques in setups with small amounts of labeled data\navailable. We show that our technique produces results which are in many cases\nbetter than using ImageNet pre-training.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:32:19 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 13:49:19 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Martinazzo", "Ana", ""], ["Espadoto", "Mateus", ""], ["Hirata", "Nina S. T.", ""]]}, {"id": "2004.11362", "submitter": "Aaron Maschinot", "authors": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian,\n  Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan", "title": "Supervised Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning applied to self-supervised representation learning has\nseen a resurgence in recent years, leading to state of the art performance in\nthe unsupervised training of deep image models. Modern batch contrastive\napproaches subsume or significantly outperform traditional contrastive losses\nsuch as triplet, max-margin and the N-pairs loss. In this work, we extend the\nself-supervised batch contrastive approach to the fully-supervised setting,\nallowing us to effectively leverage label information. Clusters of points\nbelonging to the same class are pulled together in embedding space, while\nsimultaneously pushing apart clusters of samples from different classes. We\nanalyze two possible versions of the supervised contrastive (SupCon) loss,\nidentifying the best-performing formulation of the loss. On ResNet-200, we\nachieve top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above\nthe best number reported for this architecture. We show consistent\noutperformance over cross-entropy on other datasets and two ResNet variants.\nThe loss shows benefits for robustness to natural corruptions and is more\nstable to hyperparameter settings such as optimizers and data augmentations.\nOur loss function is simple to implement, and reference TensorFlow code is\nreleased at https://t.ly/supcon.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:58:56 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 15:51:35 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 15:16:53 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 22:25:05 GMT"}, {"version": "v5", "created": "Wed, 10 Mar 2021 19:11:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Khosla", "Prannay", ""], ["Teterwak", "Piotr", ""], ["Wang", "Chen", ""], ["Sarna", "Aaron", ""], ["Tian", "Yonglong", ""], ["Isola", "Phillip", ""], ["Maschinot", "Aaron", ""], ["Liu", "Ce", ""], ["Krishnan", "Dilip", ""]]}, {"id": "2004.11364", "submitter": "Richard Tucker", "authors": "Richard Tucker and Noah Snavely", "title": "Single-View View Synthesis with Multiplane Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent strand of work in view synthesis uses deep learning to generate\nmultiplane images (a camera-centric, layered 3D representation) given two or\nmore input images at known viewpoints. We apply this representation to\nsingle-view view synthesis, a problem which is more challenging but has\npotentially much wider application. Our method learns to predict a multiplane\nimage directly from a single image input, and we introduce scale-invariant view\nsynthesis for supervision, enabling us to train on online video. We show this\napproach is applicable to several different datasets, that it additionally\ngenerates reasonable depth maps, and that it learns to fill in content behind\nthe edges of foreground objects in background layers.\n  Project page at https://single-view-mpi.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:59:19 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Tucker", "Richard", ""], ["Snavely", "Noah", ""]]}, {"id": "2004.11373", "submitter": "Jun Xu", "authors": "Ying-Jun Du, Jun Xu, Xian-Tong Zhen, Ming-Ming Cheng, Ling Shao", "title": "Conditional Variational Image Deraining", "comments": "14pages, 11 figures, 5 tables, newly accepted by TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.2990606", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deraining is an important yet challenging image processing task. Though\ndeterministic image deraining methods are developed with encouraging\nperformance, they are infeasible to learn flexible representations for\nprobabilistic inference and diverse predictions. Besides, rain intensity varies\nboth in spatial locations and across color channels, making this task more\ndifficult. In this paper, we propose a Conditional Variational Image Deraining\n(CVID) network for better deraining performance, leveraging the exclusive\ngenerative ability of Conditional Variational Auto-Encoder (CVAE) on providing\ndiverse predictions for the rainy image. To perform spatially adaptive\nderaining, we propose a spatial density estimation (SDE) module to estimate a\nrain density map for each image. Since rain density varies across different\ncolor channels, we also propose a channel-wise (CW) deraining scheme.\nExperiments on synthesized and real-world datasets show that the proposed CVID\nnetwork achieves much better performance than previous deterministic methods on\nimage deraining. Extensive ablation studies validate the effectiveness of the\nproposed SDE module and CW scheme in our CVID network. The code is available at\n\\url{https://github.com/Yingjun-Du/VID}.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:51:38 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 15:29:51 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Du", "Ying-Jun", ""], ["Xu", "Jun", ""], ["Zhen", "Xian-Tong", ""], ["Cheng", "Ming-Ming", ""], ["Shao", "Ling", ""]]}, {"id": "2004.11435", "submitter": "Clemens Seibold", "authors": "Clemens Seibold, Anna Hilsmann, Peter Eisert", "title": "Style Your Face Morph and Improve Your Face Morphing Attack Detector", "comments": "Published at BIOSIG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A morphed face image is a synthetically created image that looks so similar\nto the faces of two subjects that both can use it for verification against a\nbiometric verification system. It can be easily created by aligning and\nblending face images of the two subjects. In this paper, we propose a style\ntransfer based method that improves the quality of morphed face images. It\ncounters the image degeneration during the creation of morphed face images\ncaused by blending. We analyze different state of the art face morphing attack\ndetection systems regarding their performance against our improved morphed face\nimages and other methods that improve the image quality. All detection systems\nperform significantly worse, when first confronted with our improved morphed\nface images. Most of them can be enhanced by adding our quality improved morphs\nto the training data, which further improves the robustness against other means\nof quality improvement.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 19:29:07 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Seibold", "Clemens", ""], ["Hilsmann", "Anna", ""], ["Eisert", "Peter", ""]]}, {"id": "2004.11443", "submitter": "Guru Swaroop Bennabhaktula", "authors": "Guru Swaroop Bennabhaktula, Enrique Alegre, Dimka Karastoyanova and\n  George Azzopardi", "title": "Device-based Image Matching with Similarity Learning by Convolutional\n  Neural Networks that Exploit the Underlying Camera Sensor Pattern Noise", "comments": "7 pages, 4 figures, conference paper", "journal-ref": "In Proceedings of the 9th International Conference on Pattern\n  Recognition Applications and Methods - Volume 1: ICPRAM, 578-584, 2020", "doi": "10.5220/0009155505780584", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenging problems in digital image forensics is the capability\nto identify images that are captured by the same camera device. This knowledge\ncan help forensic experts in gathering intelligence about suspects by analyzing\ndigital images. In this paper, we propose a two-part network to quantify the\nlikelihood that a given pair of images have the same source camera, and we\nevaluated it on the benchmark Dresden data set containing 1851 images from 31\ndifferent cameras. To the best of our knowledge, we are the first ones\naddressing the challenge of device-based image matching. Though the proposed\napproach is not yet forensics ready, our experiments show that this direction\nis worth pursuing, achieving at this moment 85 percent accuracy. This ongoing\nwork is part of the EU-funded project 4NSEEK concerned with forensics against\nchild sexual abuse.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 20:03:40 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Bennabhaktula", "Guru Swaroop", ""], ["Alegre", "Enrique", ""], ["Karastoyanova", "Dimka", ""], ["Azzopardi", "George", ""]]}, {"id": "2004.11449", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, R\\'emi Lebret, Didier Orel, Philippe Sordet, Karl Aberer", "title": "Upgrading the Newsroom: An Automated Image Selection System for News\n  Articles", "comments": "Accepted to ACM Transactions on Multimedia Computing Communications\n  and Applications (ACM TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an automated image selection system to assist photo editors in\nselecting suitable images for news articles. The system fuses multiple textual\nsources extracted from news articles and accepts multilingual inputs. It is\nequipped with char-level word embeddings to help both modeling morphologically\nrich languages, e.g. German, and transferring knowledge across nearby\nlanguages. The text encoder adopts a hierarchical self-attention mechanism to\nattend more to both keywords within a piece of text and informative components\nof a news article. We extensively experiment with our system on a large-scale\ntext-image database containing multimodal multilingual news articles collected\nfrom Swiss local news media websites. The system is compared with multiple\nbaselines with ablation studies and is shown to beat existing text-image\nretrieval methods in a weakly-supervised learning setting. Besides, we also\noffer insights on the advantage of using multiple textual sources and\nmultilingual data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 20:29:26 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Liu", "Fangyu", ""], ["Lebret", "R\u00e9mi", ""], ["Orel", "Didier", ""], ["Sordet", "Philippe", ""], ["Aberer", "Karl", ""]]}, {"id": "2004.11457", "submitter": "Alceu Emanuel Bissoto", "authors": "Alceu Bissoto, Eduardo Valle, Sandra Avila", "title": "Debiasing Skin Lesion Datasets and Models? Not So Fast", "comments": "Accepted to the ISIC Skin Image Analysis Workshop @ CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven models are now deployed in a plethora of real-world applications\n- including automated diagnosis - but models learned from data risk learning\nbiases from that same data. When models learn spurious correlations not found\nin real-world situations, their deployment for critical tasks, such as medical\ndecisions, can be catastrophic. In this work we address this issue for\nskin-lesion classification models, with two objectives: finding out what are\nthe spurious correlations exploited by biased networks, and debiasing the\nmodels by removing such spurious correlations from them. We perform a\nsystematic integrated analysis of 7 visual artifacts (which are possible\nsources of biases exploitable by networks), employ a state-of-the-art technique\nto prevent the models from learning spurious correlations, and propose datasets\nto test models for the presence of bias. We find out that, despite interesting\nresults that point to promising future research, current debiasing methods are\nnot ready to solve the bias issue for skin-lesion models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:07:49 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Bissoto", "Alceu", ""], ["Valle", "Eduardo", ""], ["Avila", "Sandra", ""]]}, {"id": "2004.11465", "submitter": "Xuanyu Yin", "authors": "Xuanyu YIN, Yoko SASAKI, Weimin WANG, Kentaro SHIMIZU", "title": "YOLO and K-Means Based 3D Object Detection Method on Image and Point\n  Cloud", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar based 3D object detection and classification tasks are essential for\nautomated driving(AD). A Lidar sensor can provide the 3D point coud data\nreconstruction of the surrounding environment. But the detection in 3D point\ncloud still needs a strong algorithmic challenge. This paper consists of three\nparts.(1)Lidar-camera calib. (2)YOLO, based detection and PointCloud\nextraction, (3) k-means based point cloud segmentation. In our research, Camera\ncan capture the image to make the Real-time 2D Object Detection by using YOLO,\nI transfer the bounding box to node whose function is making 3d object\ndetection on point cloud data from Lidar. By comparing whether 2D coordinate\ntransferred from the 3D point is in the object bounding box or not, and doing a\nk-means clustering can achieve High-speed 3D object recognition function in\nGPU.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 03:08:46 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["YIN", "Xuanyu", ""], ["SASAKI", "Yoko", ""], ["WANG", "Weimin", ""], ["SHIMIZU", "Kentaro", ""]]}, {"id": "2004.11475", "submitter": "Aayush Rana", "authors": "Mamshad Nayeem Rizve, Ugur Demir, Praveen Tirupattur, Aayush Jung\n  Rana, Kevin Duarte, Ishan Dave, Yogesh Singh Rawat, Mubarak Shah", "title": "Gabriella: An Online System for Real-Time Activity Detection in\n  Untrimmed Security Videos", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Activity detection in security videos is a difficult problem due to multiple\nfactors such as large field of view, presence of multiple activities, varying\nscales and viewpoints, and its untrimmed nature. The existing research in\nactivity detection is mainly focused on datasets, such as UCF-101, JHMDB,\nTHUMOS, and AVA, which partially address these issues. The requirement of\nprocessing the security videos in real-time makes this even more challenging.\nIn this work we propose Gabriella, a real-time online system to perform\nactivity detection on untrimmed security videos. The proposed method consists\nof three stages: tubelet extraction, activity classification, and online\ntubelet merging. For tubelet extraction, we propose a localization network\nwhich takes a video clip as input and spatio-temporally detects potential\nforeground regions at multiple scales to generate action tubelets. We propose a\nnovel Patch-Dice loss to handle large variations in actor size. Our online\nprocessing of videos at a clip level drastically reduces the computation time\nin detecting activities. The detected tubelets are assigned activity class\nscores by the classification network and merged together using our proposed\nTubelet-Merge Action-Split (TMAS) algorithm to form the final action\ndetections. The TMAS algorithm efficiently connects the tubelets in an online\nfashion to generate action detections which are robust against varying length\nactivities. We perform our experiments on the VIRAT and MEVA (Multiview\nExtended Video with Activities) datasets and demonstrate the effectiveness of\nthe proposed approach in terms of speed (~100 fps) and performance with\nstate-of-the-art results. The code and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 22:20:10 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 17:45:25 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Rizve", "Mamshad Nayeem", ""], ["Demir", "Ugur", ""], ["Tirupattur", "Praveen", ""], ["Rana", "Aayush Jung", ""], ["Duarte", "Kevin", ""], ["Dave", "Ishan", ""], ["Rawat", "Yogesh Singh", ""], ["Shah", "Mubarak", ""]]}, {"id": "2004.11479", "submitter": "David Prokop", "authors": "david Prokop, Joseph Babigumira, Ashleigh Lewis", "title": "Pill Identification using a Mobile Phone App for Assessing Medication\n  Adherence and Post-Market Drug Surveillance", "comments": "12 pages, 1 photo, 6 tables, 3 charts, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Medication non-adherence is an important factor in clinical\npractice and research methodology. There have been many methods of measuring\nadherence yet no recognized standard for adherence. Here we conduct a software\nstudy of the usefulness and efficacy of a mobile phone app to measure\nmedication adherence using photographs taken by a phone app of medications and\nself-reported health measures.\n  Results: The participants were asked by the app 'would help to keep track of\nyour medication', their response indicated 92.9% felt the app 'would you use\nthis app every day' to improve their medication adherence. The subjects were\nalso asked by the app if they 'would photograph their pills on a daily basis'.\nSubject responses indicated 63% would use the app on a daily basis. By using\nthe data collected, we determined that subjects who used the app on daily basis\nwere more likely to adhere to the prescribed regimen.\n  Conclusions: Pill photographs are a useful measure of adherence, allowing\nmore accurate time measures and more frequent adherence assessment. Given the\nubiquity of mobile telephone use, and the relative ease of this adherence\nmeasurement method, we believe it is a useful and cost-effective approach.\nHowever we feel the 'manual' nature of using the phone for taking a photograph\nof a pill has individual variability and an 'automatic' method is needed to\nreduce data inconsistency.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 22:24:01 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Prokop", "david", ""], ["Babigumira", "Joseph", ""], ["Lewis", "Ashleigh", ""]]}, {"id": "2004.11482", "submitter": "Roman Solovyev A", "authors": "Roman Solovyev", "title": "Roof material classification from aerial imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an algorithm for classification of roof materials using\naerial photographs. Main advantages of the algorithm are proposed methods to\nimprove prediction accuracy. Proposed methods includes: method of converting\nImageNet weights of neural networks for using multi-channel images; special set\nof features of second level models that are used in addition to specific\npredictions of neural networks; special set of image augmentations that improve\ntraining accuracy. In addition, complete flow for solving this problem is\nproposed. The following content is available in open access: solution code,\nweight sets and architecture of the used neural networks. The proposed solution\nachieved second place in the competition \"Open AI Caribbean Challenge\".\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 22:47:50 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Solovyev", "Roman", ""]]}, {"id": "2004.11488", "submitter": "Ninghao Liu", "authors": "Ninghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, Xia Hu", "title": "Adversarial Attacks and Defenses: An Interpretation Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in a wide spectrum of applications, machine\nlearning models, especially deep neural networks, have been shown to be\nvulnerable to adversarial attacks. Attackers add carefully-crafted\nperturbations to input, where the perturbations are almost imperceptible to\nhumans, but can cause models to make wrong predictions. Techniques to protect\nmodels against adversarial input are called adversarial defense methods.\nAlthough many approaches have been proposed to study adversarial attacks and\ndefenses in different scenarios, an intriguing and crucial challenge remains\nthat how to really understand model vulnerability? Inspired by the saying that\n\"if you know yourself and your enemy, you need not fear the battles\", we may\ntackle the aforementioned challenge after interpreting machine learning models\nto open the black-boxes. The goal of model interpretation, or interpretable\nmachine learning, is to extract human-understandable terms for the working\nmechanism of models. Recently, some approaches start incorporating\ninterpretation into the exploration of adversarial attacks and defenses.\nMeanwhile, we also observe that many existing methods of adversarial attacks\nand defenses, although not explicitly claimed, can be understood from the\nperspective of interpretation. In this paper, we review recent work on\nadversarial attacks and defenses, particularly from the perspective of machine\nlearning interpretation. We categorize interpretation into two types,\nfeature-level interpretation and model-level interpretation. For each type of\ninterpretation, we elaborate on how it could be used for adversarial attacks\nand defenses. We then briefly illustrate additional correlations between\ninterpretation and adversaries. Finally, we discuss the challenges and future\ndirections along tackling adversary issues with interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 23:19:00 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 15:43:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Liu", "Ninghao", ""], ["Du", "Mengnan", ""], ["Guo", "Ruocheng", ""], ["Liu", "Huan", ""], ["Hu", "Xia", ""]]}, {"id": "2004.11498", "submitter": "Anthony Ortiz", "authors": "Kolya Malkin, Anthony Ortiz, Caleb Robinson and Nebojsa Jojic", "title": "Mining self-similarity: Label super-resolution with epitomic\n  representations", "comments": "Submitted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that simple patch-based models, such as epitomes, can have superior\nperformance to the current state of the art in semantic segmentation and label\nsuper-resolution, which uses deep convolutional neural networks. We derive a\nnew training algorithm for epitomes which allows, for the first time, learning\nfrom very large data sets and derive a label super-resolution algorithm as a\nstatistical inference algorithm over epitomic representations. We illustrate\nour methods on land cover mapping and medical image analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 00:42:59 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Malkin", "Kolya", ""], ["Ortiz", "Anthony", ""], ["Robinson", "Caleb", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "2004.11500", "submitter": "Jiahua Dong", "authors": "Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, Xiaowei Xu", "title": "What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic\n  Lesions Segmentation", "comments": "This paper is accepted by IEEE Conference on Computer Vision and\n  Pattern Recognition 2020 (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation has attracted growing research attention on\nsemantic segmentation. However, 1) most existing models cannot be directly\napplied into lesions transfer of medical images, due to the diverse appearances\nof same lesion among different datasets; 2) equal attention has been paid into\nall semantic representations instead of neglecting irrelevant knowledge, which\nleads to negative transfer of untransferable knowledge. To address these\nchallenges, we develop a new unsupervised semantic transfer model including two\ncomplementary modules (i.e., T_D and T_F ) for endoscopic lesions segmentation,\nwhich can alternatively determine where and how to explore transferable\ndomain-invariant knowledge between labeled source lesions dataset (e.g.,\ngastroscope) and unlabeled target diseases dataset (e.g., enteroscopy).\nSpecifically, T_D focuses on where to translate transferable visual information\nof medical lesions via residual transferability-aware bottleneck, while\nneglecting untransferable visual characterizations. Furthermore, T_F highlights\nhow to augment transferable semantic features of various lesions and\nautomatically ignore untransferable representations, which explores\ndomain-invariant knowledge and in return improves the performance of T_D. To\nthe end, theoretical analysis and extensive experiments on medical endoscopic\ndataset and several non-medical public datasets well demonstrate the\nsuperiority of our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 00:57:05 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Dong", "Jiahua", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""], ["Zhong", "Bineng", ""], ["Xu", "Xiaowei", ""]]}, {"id": "2004.11506", "submitter": "Tao Wang", "authors": "Tao Wang, Junsong Wang, Chang Xu and Chao Xue", "title": "Automatic low-bit hybrid quantization of neural networks through meta\n  learning", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization is a widely used technique to compress and accelerate deep\nneural network (DNN) inference, especially when deploying to edge or IoT\ndevices with limited computation capacity and power consumption budget. The\nuniform bit width quantization across all the layers is usually sub-optimal and\nthe exploration of hybrid quantization for different layers is vital for\nefficient deep compression. In this paper, we employ the meta learning method\nto automatically realize low-bit hybrid quantization of neural networks. A\nMetaQuantNet, together with a Quantization function, are trained to generate\nthe quantized weights for the target DNN. Then, we apply a genetic algorithm to\nsearch the best hybrid quantization policy that meets compression constraints.\nWith the best searched quantization policy, we subsequently retrain or finetune\nto further improve the performance of the quantized target network. Extensive\nexperiments demonstrate the performance of searched hybrid quantization scheme\nsurpass that of uniform bitwidth counterpart. Compared to the existing\nreinforcement learning (RL) based hybrid quantization search approach that\nrelies on tedious explorations, our meta learning approach is more efficient\nand effective for any compression requirements since the MetaQuantNet only\nneeds be trained once.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 02:01:26 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wang", "Tao", ""], ["Wang", "Junsong", ""], ["Xu", "Chang", ""], ["Xue", "Chao", ""]]}, {"id": "2004.11514", "submitter": "Brian Hutchinson", "authors": "Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda\n  Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor", "title": "Systematic Evaluation of Backdoor Data Poisoning Attacks on Image\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor data poisoning attacks have recently been demonstrated in computer\nvision research as a potential safety risk for machine learning (ML) systems.\nTraditional data poisoning attacks manipulate training data to induce\nunreliability of an ML model, whereas backdoor data poisoning attacks maintain\nsystem performance unless the ML model is presented with an input containing an\nembedded \"trigger\" that provides a predetermined response advantageous to the\nadversary. Our work builds upon prior backdoor data-poisoning research for ML\nimage classifiers and systematically assesses different experimental conditions\nincluding types of trigger patterns, persistence of trigger patterns during\nretraining, poisoning strategies, architectures (ResNet-50, NasNet,\nNasNet-Mobile), datasets (Flowers, CIFAR-10), and potential defensive\nregularization techniques (Contrastive Loss, Logit Squeezing, Manifold Mixup,\nSoft-Nearest-Neighbors Loss). Experiments yield four key findings. First, the\nsuccess rate of backdoor poisoning attacks varies widely, depending on several\nfactors, including model architecture, trigger pattern and regularization\ntechnique. Second, we find that poisoned models are hard to detect through\nperformance inspection alone. Third, regularization typically reduces backdoor\nsuccess rate, although it can have no effect or even slightly increase it,\ndepending on the form of regularization. Finally, backdoors inserted through\ndata poisoning can be rendered ineffective after just a few epochs of\nadditional training on a small set of clean data without affecting the model's\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 02:58:22 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Truong", "Loc", ""], ["Jones", "Chace", ""], ["Hutchinson", "Brian", ""], ["August", "Andrew", ""], ["Praggastis", "Brenda", ""], ["Jasper", "Robert", ""], ["Nichols", "Nicole", ""], ["Tuor", "Aaron", ""]]}, {"id": "2004.11540", "submitter": "Christopher Choy", "authors": "Christopher Choy, Wei Dong, Vladlen Koltun", "title": "Deep Global Registration", "comments": "Accepted for CVPR'20 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Deep Global Registration, a differentiable framework for pairwise\nregistration of real-world 3D scans. Deep global registration is based on three\nmodules: a 6-dimensional convolutional network for correspondence confidence\nprediction, a differentiable Weighted Procrustes algorithm for closed-form pose\nestimation, and a robust gradient-based SE(3) optimizer for pose refinement.\nExperiments demonstrate that our approach outperforms state-of-the-art methods,\nboth learning-based and classical, on real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 05:47:32 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 08:59:52 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Choy", "Christopher", ""], ["Dong", "Wei", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2004.11545", "submitter": "Seyed Iman Mirzadeh", "authors": "Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Hassan Ghasemzadeh", "title": "Dropout as an Implicit Gating Mechanism For Continual Learning", "comments": "CVPR 2020 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural networks have demonstrated an outstanding ability to\nachieve complex learning tasks across various domains. However, they suffer\nfrom the \"catastrophic forgetting\" problem when they face a sequence of\nlearning tasks, where they forget the old ones as they learn new tasks. This\nproblem is also highly related to the \"stability-plasticity dilemma\". The more\nplastic the network, the easier it can learn new tasks, but the faster it also\nforgets previous ones. Conversely, a stable network cannot learn new tasks as\nfast as a very plastic network. However, it is more reliable to preserve the\nknowledge it has learned from the previous tasks. Several solutions have been\nproposed to overcome the forgetting problem by making the neural network\nparameters more stable, and some of them have mentioned the significance of\ndropout in continual learning. However, their relationship has not been\nsufficiently studied yet. In this paper, we investigate this relationship and\nshow that a stable network with dropout learns a gating mechanism such that for\ndifferent tasks, different paths of the network are active. Our experiments\nshow that the stability achieved by this implicit gating plays a very critical\nrole in leading to performance comparable to or better than other involved\ncontinual learning algorithms to overcome catastrophic forgetting.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 06:04:15 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mirzadeh", "Seyed-Iman", ""], ["Farajtabar", "Mehrdad", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2004.11563", "submitter": "Xuequan Lu", "authors": "Dening Lu, Xuequan Lu, Yangxing Sun, Jun Wang", "title": "Deep Feature-preserving Normal Estimation for Point Cloud Filtering", "comments": "accepted to Computer Aided Design (Symposium on Solid and Physical\n  Modeling 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud filtering, the main bottleneck of which is removing noise\n(outliers) while preserving geometric features, is a fundamental problem in 3D\nfield. The two-step schemes involving normal estimation and position update\nhave been shown to produce promising results. Nevertheless, the current normal\nestimation methods including optimization ones and deep learning ones, often\neither have limited automation or cannot preserve sharp features. In this\npaper, we propose a novel feature-preserving normal estimation method for point\ncloud filtering with preserving geometric features. It is a learning method and\nthus achieves automatic prediction for normals. For training phase, we first\ngenerate patch based samples which are then fed to a classification network to\nclassify feature and non-feature points. We finally train the samples of\nfeature and non-feature points separately, to achieve decent results. Regarding\ntesting, given a noisy point cloud, its normals can be automatically estimated.\nFor further point cloud filtering, we iterate the above normal estimation and a\ncurrent position update algorithm for a few times. Various experiments\ndemonstrate that our method outperforms state-of-the-art normal estimation\nmethods and point cloud filtering techniques, in terms of both quality and\nquantity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:05:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Lu", "Dening", ""], ["Lu", "Xuequan", ""], ["Sun", "Yangxing", ""], ["Wang", "Jun", ""]]}, {"id": "2004.11577", "submitter": "Gabriele Facciolo", "authors": "Qiyu Jin and Gabriele Facciolo and Jean-Michel Morel", "title": "A Review of an Old Dilemma: Demosaicking First, or Denoising First?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising and demosaicking are the most important early stages in\ndigital camera pipelines. They constitute a severely ill-posed problem that\naims at reconstructing a full color image from a noisy color filter array (CFA)\nimage. In most of the literature, denoising and demosaicking are treated as two\nindependent problems, without considering their interaction, or asking which\nshould be applied first. Several recent works have started addressing them\njointly in works that involve heavy weight CNNs, thus incompatible with low\npower portable imaging devices. Hence, the question of how to combine denoising\nand demosaicking to reconstruct full color images remains very relevant: Is\ndenoising to be applied first, or should that be demosaicking first? In this\npaper, we review the main variants of these strategies and carry-out an\nextensive evaluation to find the best way to reconstruct full color images from\na noisy mosaic. We conclude that demosaicking should applied first, followed by\ndenoising. Yet we prove that this requires an adaptation of classic denoising\nalgorithms to demosaicked noise, which we justify and specify.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:32:17 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Jin", "Qiyu", ""], ["Facciolo", "Gabriele", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "2004.11598", "submitter": "Sicheng Xu", "authors": "Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, Yu Deng, Yunde Jia,\n  Xin Tong", "title": "Deep 3D Portrait from a Single Image", "comments": "Accepted by CVPR2020; Code: https://github.com/sicxu/Deep3dPortrait", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a learning-based approach for recovering the 3D\ngeometry of human head from a single portrait image. Our method is learned in\nan unsupervised manner without any ground-truth 3D data.\n  We represent the head geometry with a parametric 3D face model together with\na depth map for other head regions including hair and ear. A two-step geometry\nlearning scheme is proposed to learn 3D head reconstruction from in-the-wild\nface images, where we first learn face shape on single images using\nself-reconstruction and then learn hair and ear geometry using pairs of images\nin a stereo-matching fashion. The second step is based on the output of the\nfirst to not only improve the accuracy but also ensure the consistency of\noverall head geometry.\n  We evaluate the accuracy of our method both in 3D and with pose manipulation\ntasks on 2D images. We alter pose based on the recovered geometry and apply a\nrefinement network trained with adversarial learning to ameliorate the\nreprojected images and translate them to the real image domain. Extensive\nevaluations and comparison with previous methods show that our new method can\nproduce high-fidelity 3D head geometry and head pose manipulation results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 08:55:37 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Xu", "Sicheng", ""], ["Yang", "Jiaolong", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Deng", "Yu", ""], ["Jia", "Yunde", ""], ["Tong", "Xin", ""]]}, {"id": "2004.11612", "submitter": "Tomasz Kryjak", "authors": "Krzysztof Blachut and Hubert Szolc and Mateusz Wasala and Tomasz\n  Kryjak and Marek Gorgon", "title": "Vision based hardware-software real-time control system for autonomous\n  landing of an UAV", "comments": "7 pages, 9 figures, submitted to MMAR 2020 conference", "journal-ref": null, "doi": "10.1007/978-3-030-59006-2_2", "report-no": null, "categories": "cs.CV cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a vision based hardware-software control system\nenabling autonomous landing of a multirotor unmanned aerial vehicle (UAV). It\nallows the detection of a marked landing pad in real-time for a 1280 x 720 @ 60\nfps video stream. In addition, a LiDAR sensor is used to measure the altitude\nabove ground. A heterogeneous Zynq SoC device is used as the computing\nplatform. The solution was tested on a number of sequences and the landing pad\nwas detected with 96% accuracy. This research shows that a reprogrammable\nheterogeneous computing system is a good solution for UAVs because it enables\nreal-time data stream processing with relatively low energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:24:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Blachut", "Krzysztof", ""], ["Szolc", "Hubert", ""], ["Wasala", "Mateusz", ""], ["Kryjak", "Tomasz", ""], ["Gorgon", "Marek", ""]]}, {"id": "2004.11623", "submitter": "Maarten Vandersteegen", "authors": "Maarten Vandersteegen, Wouter Reusen, Kristof Van Beeck Toon Goedeme", "title": "Low-latency hand gesture recognition with a low resolution thermal\n  imager", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using hand gestures to answer a call or to control the radio while driving a\ncar, is nowadays an established feature in more expensive cars. High resolution\ntime-of-flight cameras and powerful embedded processors usually form the heart\nof these gesture recognition systems. This however comes with a price tag. We\ntherefore investigate the possibility to design an algorithm that predicts hand\ngestures using a cheap low-resolution thermal camera with only 32x24 pixels,\nwhich is light-weight enough to run on a low-cost processor. We recorded a new\ndataset of over 1300 video clips for training and evaluation and propose a\nlight-weight low-latency prediction algorithm. Our best model achieves 95.9%\nclassification accuracy and 83% mAP detection accuracy while its processing\npipeline has a latency of only one frame.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:43:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Vandersteegen", "Maarten", ""], ["Reusen", "Wouter", ""], ["Goedeme", "Kristof Van Beeck Toon", ""]]}, {"id": "2004.11624", "submitter": "Changhui Liang", "authors": "Chang-Hui Liang, Wan-Lei Zhao, Run-Qing Chen", "title": "Dynamic Sampling for Deep Metric Learning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning maps visually similar images onto nearby locations and\nvisually dissimilar images apart from each other in an embedding manifold. The\nlearning process is mainly based on the supplied image negative and positive\ntraining pairs. In this paper, a dynamic sampling strategy is proposed to\norganize the training pairs in an easy-to-hard order to feed into the network.\nIt allows the network to learn general boundaries between categories from the\neasy training pairs at its early stages and finalize the details of the model\nmainly relying on the hard training samples in the later. Compared to the\nexisting training sample mining approaches, the hard samples are mined with\nlittle harm to the learned general model. This dynamic sampling strategy is\nformularized as two simple terms that are compatible with various loss\nfunctions. Consistent performance boost is observed when it is integrated with\nseveral popular loss functions on fashion search, fine-grained classification,\nand person re-identification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:47:23 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 01:29:01 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Liang", "Chang-Hui", ""], ["Zhao", "Wan-Lei", ""], ["Chen", "Run-Qing", ""]]}, {"id": "2004.11627", "submitter": "Zhongzhan Huang", "authors": "Zhongzhan Huang, Wenqi Shao, Xinjiang Wang, Ping Luo", "title": "Convolution-Weight-Distribution Assumption: Rethinking the Criteria of\n  Channel Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning is a popular technique for compressing convolutional neural\nnetworks (CNNs), where various pruning criteria have been proposed to remove\nthe redundant filters. From our comprehensive experiments, we found two blind\nspots in the study of pruning criteria: (1) Similarity: There are some strong\nsimilarities among several primary pruning criteria that are widely cited and\ncompared. According to these criteria, the ranks of filters'Importance Score\nare almost identical, resulting in similar pruned structures. (2)\nApplicability: The filters'Importance Score measured by some pruning criteria\nare too close to distinguish the network redundancy well. In this paper, we\nanalyze these two blind spots on different types of pruning criteria with\nlayer-wise pruning or global pruning. The analyses are based on the empirical\nexperiments and our assumption (Convolutional Weight Distribution Assumption)\nthat the well-trained convolutional filters each layer approximately follow a\nGaussian-alike distribution. This assumption has been verified through\nsystematic and extensive statistical tests.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:54:21 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:36:14 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Huang", "Zhongzhan", ""], ["Shao", "Wenqi", ""], ["Wang", "Xinjiang", ""], ["Luo", "Ping", ""]]}, {"id": "2004.11639", "submitter": "Alessandro Ortis", "authors": "Alessandro Ortis and Giovanni Maria Farinella and Sebastiano Battiato", "title": "Survey on Visual Sentiment Analysis", "comments": "This paper is a postprint of a paper accepted by IET Image Processing\n  and is subject to Institution of Engineering and Technology Copyright. When\n  the final version is published, the copy of record will be available at the\n  IET Digital Library", "journal-ref": null, "doi": "10.1049/iet-ipr.2019.1270", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Sentiment Analysis aims to understand how images affect people, in\nterms of evoked emotions. Although this field is rather new, a broad range of\ntechniques have been developed for various data sources and problems, resulting\nin a large body of research. This paper reviews pertinent publications and\ntries to present an exhaustive overview of the field. After a description of\nthe task and the related applications, the subject is tackled under different\nmain headings. The paper also describes principles of design of general Visual\nSentiment Analysis systems from three main points of view: emotional models,\ndataset definition, feature design. A formalization of the problem is\ndiscussed, considering different levels of granularity, as well as the\ncomponents that can affect the sentiment toward an image in different ways. To\nthis aim, this paper considers a structured formalization of the problem which\nis usually used for the analysis of text, and discusses it's suitability in the\ncontext of Visual Sentiment Analysis. The paper also includes a description of\nnew challenges, the evaluation from the viewpoint of progress toward more\nsophisticated systems and related practical applications, as well as a summary\nof the insights resulting from this study.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 10:15:22 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 11:09:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ortis", "Alessandro", ""], ["Farinella", "Giovanni Maria", ""], ["Battiato", "Sebastiano", ""]]}, {"id": "2004.11647", "submitter": "Artem Filatov", "authors": "Artem Filatov, Andrey Rykov, Viacheslav Murashkin", "title": "Any Motion Detector: Learning Class-agnostic Scene Dynamics from a\n  Sequence of LiDAR Point Clouds", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and motion parameters estimation are crucial tasks for\nself-driving vehicle safe navigation in a complex urban environment. In this\nwork we propose a novel real-time approach of temporal context aggregation for\nmotion detection and motion parameters estimation based on 3D point cloud\nsequence. We introduce an ego-motion compensation layer to achieve real-time\ninference with performance comparable to a naive odometric transform of the\noriginal point cloud sequence. Not only is the proposed architecture capable of\nestimating the motion of common road participants like vehicles or pedestrians\nbut also generalizes to other object categories which are not present in\ntraining data. We also conduct an in-deep analysis of different temporal\ncontext aggregation strategies such as recurrent cells and 3D convolutions.\nFinally, we provide comparison results of our state-of-the-art model with\nexisting solutions on KITTI Scene Flow dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 10:40:07 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Filatov", "Artem", ""], ["Rykov", "Andrey", ""], ["Murashkin", "Viacheslav", ""]]}, {"id": "2004.11657", "submitter": "Martin G\\\"unther", "authors": "Till Grenzd\\\"orffer, Martin G\\\"unther and Joachim Hertzberg", "title": "YCB-M: A Multi-Camera RGB-D Dataset for Object Recognition and 6DoF Pose\n  Estimation", "comments": "Published at ICRA-2020", "journal-ref": null, "doi": "10.1109/ICRA40945.2020.9197426", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a great variety of 3D cameras have been introduced in recent years,\nmost publicly available datasets for object recognition and pose estimation\nfocus on one single camera. In this work, we present a dataset of 32 scenes\nthat have been captured by 7 different 3D cameras, totaling 49,294 frames. This\nallows evaluating the sensitivity of pose estimation algorithms to the\nspecifics of the used camera and the development of more robust algorithms that\nare more independent of the camera model. Vice versa, our dataset enables\nresearchers to perform a quantitative comparison of the data from several\ndifferent cameras and depth sensing technologies and evaluate their algorithms\nbefore selecting a camera for their specific task. The scenes in our dataset\ncontain 20 different objects from the common benchmark YCB object and model set\n[1], [2]. We provide full ground truth 6DoF poses for each object, per-pixel\nsegmentation, 2D and 3D bounding boxes and a measure of the amount of occlusion\nof each object. We have also performed an initial evaluation of the cameras\nusing our dataset on a state-of-the-art object recognition and pose estimation\nsystem [3].\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 11:14:04 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 07:58:18 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Grenzd\u00f6rffer", "Till", ""], ["G\u00fcnther", "Martin", ""], ["Hertzberg", "Joachim", ""]]}, {"id": "2004.11660", "submitter": "Yu Deng", "authors": "Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong", "title": "Disentangled and Controllable Face Image Generation via 3D\n  Imitative-Contrastive Learning", "comments": "Accepted by CVPR2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DiscoFaceGAN, an approach for face image generation of virtual\npeople with disentangled, precisely-controllable latent representations for\nidentity of non-existing people, expression, pose, and illumination. We embed\n3D priors into adversarial learning and train the network to imitate the image\nformation of an analytic 3D face deformation and rendering process. To deal\nwith the generation freedom induced by the domain gap between real and rendered\nfaces, we further introduce contrastive learning to promote disentanglement by\ncomparing pairs of generated images. Experiments show that through our\nimitative-contrastive learning, the factor variations are very well\ndisentangled and the properties of a generated face can be precisely\ncontrolled. We also analyze the learned latent space and present several\nmeaningful properties supporting factor disentanglement. Our method can also be\nused to embed real images into the disentangled latent space. We hope our\nmethod could provide new understandings of the relationship between physical\nproperties and deep image synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 11:21:13 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 04:32:25 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Deng", "Yu", ""], ["Yang", "Jiaolong", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Tong", "Xin", ""]]}, {"id": "2004.11676", "submitter": "Narinder Punn", "authors": "Narinder Singh Punn and Sonali Agarwal", "title": "Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray\n  images using fine-tuned deep neural networks", "comments": null, "journal-ref": "Appl Intell (2020)", "doi": "10.1007/s10489-020-01900-3", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that\nresembles pneumonia. The current diagnostic procedure of COVID-19 follows\nreverse-transcriptase polymerase chain reaction (RT-PCR) based approach which\nhowever is less sensitive to identify the virus at the initial stage. Hence, a\nmore robust and alternate diagnosis technique is desirable. Recently, with the\nrelease of publicly available datasets of corona positive patients comprising\nof computed tomography (CT) and chest X-ray (CXR) imaging; scientists,\nresearchers and healthcare experts are contributing for faster and automated\ndiagnosis of COVID-19 by identifying pulmonary infections using deep learning\napproaches to achieve better cure and treatment. These datasets have limited\nsamples concerned with the positive COVID-19 cases, which raise the challenge\nfor unbiased learning. Following from this context, this article presents the\nrandom oversampling and weighted class loss function approach for unbiased\nfine-tuned learning (transfer learning) in various state-of-the-art deep\nlearning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2,\nDenseNet169, and NASNetLarge to perform binary classification (as normal and\nCOVID-19 cases) and also multi-class classification (as COVID-19, pneumonia,\nand normal case) of posteroanterior CXR images. Accuracy, precision, recall,\nloss, and area under the curve (AUC) are utilized to evaluate the performance\nof the models. Considering the experimental results, the performance of each\nmodel is scenario dependent; however, NASNetLarge displayed better scores in\ncontrast to other architectures, which is further compared with other recently\nproposed approaches. This article also added the visual explanation to\nillustrate the basis of model classification and perception of COVID-19 in CXR\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 10:24:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 04:15:59 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 14:56:37 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 08:44:20 GMT"}, {"version": "v5", "created": "Tue, 21 Jul 2020 10:27:36 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Punn", "Narinder Singh", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2004.11678", "submitter": "Ylva Jansson", "authors": "Lukas Finnveden, Ylva Jansson and Tony Lindeberg", "title": "Understanding when spatial transformer networks do not support\n  invariance, and what to do about it", "comments": "13 pages, 7 figures, 6 tables", "journal-ref": "Shortened version in International Conference on Pattern\n  Recognition (ICPR 2020), pages 3427--3434, Jan 2021", "doi": "10.1109/ICPR48806.2021.9412997", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial transformer networks (STNs) were designed to enable convolutional\nneural networks (CNNs) to learn invariance to image transformations. STNs were\noriginally proposed to transform CNN feature maps as well as input images. This\nenables the use of more complex features when predicting transformation\nparameters. However, since STNs perform a purely spatial transformation, they\ndo not, in the general case, have the ability to align the feature maps of a\ntransformed image with those of its original. STNs are therefore unable to\nsupport invariance when transforming CNN feature maps. We present a simple\nproof for this and study the practical implications, showing that this\ninability is coupled with decreased classification accuracy. We therefore\ninvestigate alternative STN architectures that make use of complex features. We\nfind that while deeper localization networks are difficult to train,\nlocalization networks that share parameters with the classification network\nremain stable as they grow deeper, which allows for higher classification\naccuracy on difficult datasets. Finally, we explore the interaction between\nlocalization network complexity and iterative image alignment.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:20:35 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 08:38:07 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 13:29:50 GMT"}, {"version": "v4", "created": "Wed, 23 Dec 2020 11:48:25 GMT"}, {"version": "v5", "created": "Tue, 18 May 2021 09:14:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Finnveden", "Lukas", ""], ["Jansson", "Ylva", ""], ["Lindeberg", "Tony", ""]]}, {"id": "2004.11691", "submitter": "Peter Wakeford", "authors": "Peter Robert Wakeford, Enrico Pellegrini, Gavin Robertson, Michael\n  Verhoek, Alan Duncan Fleming, Jano van Hemert and Ik Siong Heng", "title": "Optic disc and fovea localisation in ultra-widefield scanning laser\n  ophthalmoscope images captured in multiple modalities", "comments": "Submitted to the 23rd Conference on Medical Image Understanding and\n  Analysis (MIUA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional neural network for localising the centres of the\noptic disc (OD) and fovea in ultra-wide field of view scanning laser\nophthalmoscope (UWFoV-SLO) images of the retina. Images captured in both\nreflectance and autofluorescence (AF) modes, and central pole and eyesteered\ngazes, were used. The method achieved an OD localisation accuracy of 99.4%\nwithin one OD radius, and fovea localisation accuracy of 99.1% within one OD\nradius on a test set comprising of 1790 images. The performance of fovea\nlocalisation in AF images was comparable to the variation between human\nannotators at this task. The laterality of the image (whether the image is of\nthe left or right eye) was inferred from the OD and fovea coordinates with an\naccuracy of 99.9%\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:29:10 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wakeford", "Peter Robert", ""], ["Pellegrini", "Enrico", ""], ["Robertson", "Gavin", ""], ["Verhoek", "Michael", ""], ["Fleming", "Alan Duncan", ""], ["van Hemert", "Jano", ""], ["Heng", "Ik Siong", ""]]}, {"id": "2004.11693", "submitter": "Arunava Chakravarty", "authors": "Arka Mitra, Arunava Chakravarty, Nirmalya Ghosh, Tandra Sarkar,\n  Ramanathan Sethuraman, Debdoot Sheet", "title": "A Systematic Search over Deep Convolutional Neural Network Architectures\n  for Screening Chest Radiographs", "comments": "accepted in EMBC 2020, 4 pages+2 page Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiographs are primarily employed for the screening of pulmonary and\ncardio-/thoracic conditions. Being undertaken at primary healthcare centers,\nthey require the presence of an on-premise reporting Radiologist, which is a\nchallenge in low and middle income countries. This has inspired the development\nof machine learning based automation of the screening process. While recent\nefforts demonstrate a performance benchmark using an ensemble of deep\nconvolutional neural networks (CNN), our systematic search over multiple\nstandard CNN architectures identified single candidate CNN models whose\nclassification performances were found to be at par with ensembles. Over 63\nexperiments spanning 400 hours, executed on a 11:3 FP32 TensorTFLOPS compute\nsystem, we found the Xception and ResNet-18 architectures to be consistent\nperformers in identifying co-existing disease conditions with an average AUC of\n0.87 across nine pathologies. We conclude on the reliability of the models by\nassessing their saliency maps generated using the randomized input sampling for\nexplanation (RISE) method and qualitatively validating them against manual\nannotations locally sourced from an experienced Radiologist. We also draw a\ncritical note on the limitations of the publicly available CheXpert dataset\nprimarily on account of disparity in class distribution in training vs. testing\nsets, and unavailability of sufficient samples for few classes, which hampers\nquantitative reporting due to sample insufficiency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:30:40 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mitra", "Arka", ""], ["Chakravarty", "Arunava", ""], ["Ghosh", "Nirmalya", ""], ["Sarkar", "Tandra", ""], ["Sethuraman", "Ramanathan", ""], ["Sheet", "Debdoot", ""]]}, {"id": "2004.11721", "submitter": "Arunava Chakravarty", "authors": "Arunava Chakravarty, Tandra Sarkar, Nirmalya Ghosh, Ramanathan\n  Sethuraman, Debdoot Sheet", "title": "Learning Decision Ensemble using a Graph Neural Network for Comorbidity\n  Aware Chest Radiograph Screening", "comments": "accepted in EMBC 2020, 4pg+2pg Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiographs are primarily employed for the screening of cardio,\nthoracic and pulmonary conditions. Machine learning based automated solutions\nare being developed to reduce the burden of routine screening on Radiologists,\nallowing them to focus on critical cases. While recent efforts demonstrate the\nuse of ensemble of deep convolutional neural networks(CNN), they do not take\ndisease comorbidity into consideration, thus lowering their screening\nperformance. To address this issue, we propose a Graph Neural Network (GNN)\nbased solution to obtain ensemble predictions which models the dependencies\nbetween different diseases. A comprehensive evaluation of the proposed method\ndemonstrated its potential by improving the performance over standard\nensembling technique across a wide range of ensemble constructions. The best\nperformance was achieved using the GNN ensemble of DenseNet121 with an average\nAUC of 0.821 across thirteen disease comorbidities.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:57:50 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Chakravarty", "Arunava", ""], ["Sarkar", "Tandra", ""], ["Ghosh", "Nirmalya", ""], ["Sethuraman", "Ramanathan", ""], ["Sheet", "Debdoot", ""]]}, {"id": "2004.11726", "submitter": "Arunava Chakravarty", "authors": "Sarath Chandra K, Arunava Chakravarty, Nirmalya Ghosh, Tandra Sarkar,\n  Ramanathan Sethuraman, Debdoot Sheet", "title": "A Two-Stage Multiple Instance Learning Framework for the Detection of\n  Breast Cancer in Mammograms", "comments": "accepted in EMBC 2020, 4 pg+1 pg Supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammograms are commonly employed in the large scale screening of breast\ncancer which is primarily characterized by the presence of malignant masses.\nHowever, automated image-level detection of malignancy is a challenging task\ngiven the small size of the mass regions and difficulty in discriminating\nbetween malignant, benign mass and healthy dense fibro-glandular tissue. To\naddress these issues, we explore a two-stage Multiple Instance Learning (MIL)\nframework. A Convolutional Neural Network (CNN) is trained in the first stage\nto extract local candidate patches in the mammograms that may contain either a\nbenign or malignant mass. The second stage employs a MIL strategy for an image\nlevel benign vs. malignant classification. A global image-level feature is\ncomputed as a weighted average of patch-level features learned using a CNN. Our\nmethod performed well on the task of localization of masses with an average\nPrecision/Recall of 0.76/0.80 and acheived an average AUC of 0.91 on the\nimagelevel classification task using a five-fold cross-validation on the\nINbreast dataset. Restricting the MIL only to the candidate patches extracted\nin Stage 1 led to a significant improvement in classification performance in\ncomparison to a dense extraction of patches from the entire mammogram.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:06:47 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["K", "Sarath Chandra", ""], ["Chakravarty", "Arunava", ""], ["Ghosh", "Nirmalya", ""], ["Sarkar", "Tandra", ""], ["Sethuraman", "Ramanathan", ""], ["Sheet", "Debdoot", ""]]}, {"id": "2004.11744", "submitter": "Qing Yang", "authors": "Qing Yang, Xia Zhu, Jong-Kae Fwu, Yun Ye, Ganmei You, and Yuan Zhu", "title": "PipeNet: Selective Modal Pipeline of Fusion Network for Multi-Modal Face\n  Anti-Spoofing", "comments": "Accepted to appear in CVPR2020 WMF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face anti-spoofing has become an increasingly important and critical security\nfeature for authentication systems, due to rampant and easily launchable\npresentation attacks. Addressing the shortage of multi-modal face dataset,\nCASIA recently released the largest up-to-date CASIA-SURF Cross-ethnicity Face\nAnti-spoofing(CeFA) dataset, covering 3 ethnicities, 3 modalities, 1607\nsubjects, and 2D plus 3D attack types in four protocols, and focusing on the\nchallenge of improving the generalization capability of face anti-spoofing in\ncross-ethnicity and multi-modal continuous data. In this paper, we propose a\nnovel pipeline-based multi-stream CNN architecture called PipeNet for\nmulti-modal face anti-spoofing. Unlike previous works, Selective Modal Pipeline\n(SMP) is designed to enable a customized pipeline for each data modality to\ntake full advantage of multi-modal data. Limited Frame Vote (LFV) is designed\nto ensure stable and accurate prediction for video classification. The proposed\nmethod wins the third place in the final ranking of Chalearn Multi-modal\nCross-ethnicity Face Anti-spoofing Recognition Challenge@CVPR2020. Our final\nsubmission achieves the Average Classification Error Rate (ACER) of 2.21 with\nStandard Deviation of 1.26 on the test set.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:46:00 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Yang", "Qing", ""], ["Zhu", "Xia", ""], ["Fwu", "Jong-Kae", ""], ["Ye", "Yun", ""], ["You", "Ganmei", ""], ["Zhu", "Yuan", ""]]}, {"id": "2004.11757", "submitter": "Xi Li", "authors": "Zequn Qin, Huanyu Wang, and Xi Li", "title": "Ultra Fast Structure-aware Deep Lane Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern methods mainly regard lane detection as a problem of pixel-wise\nsegmentation, which is struggling to address the problem of challenging\nscenarios and speed. Inspired by human perception, the recognition of lanes\nunder severe occlusion and extreme lighting conditions is mainly based on\ncontextual and global information. Motivated by this observation, we propose a\nnovel, simple, yet effective formulation aiming at extremely fast speed and\nchallenging scenarios. Specifically, we treat the process of lane detection as\na row-based selecting problem using global features. With the help of row-based\nselecting, our formulation could significantly reduce the computational cost.\nUsing a large receptive field on global features, we could also handle the\nchallenging scenarios. Moreover, based on the formulation, we also propose a\nstructural loss to explicitly model the structure of lanes. Extensive\nexperiments on two lane detection benchmark datasets show that our method could\nachieve the state-of-the-art performance in terms of both speed and accuracy. A\nlight-weight version could even achieve 300+ frames per second with the same\nresolution, which is at least 4x faster than previous state-of-the-art methods.\nOur code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:58:49 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 07:50:42 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 04:01:50 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 02:59:50 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Qin", "Zequn", ""], ["Wang", "Huanyu", ""], ["Li", "Xi", ""]]}, {"id": "2004.11765", "submitter": "Bo Li", "authors": "Bo Li and Viktor Larsson", "title": "GAPS: Generator for Automatic Polynomial Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MS cs.RO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal problems in computer vision raise the demand of generating efficient\nautomatic solvers for polynomial equation systems. Given a polynomial system\nrepeated with different coefficient instances, the traditional Gr\\\"obner basis\nor normal form based solution is very inefficient. Fortunately the Gr\\\"obner\nbasis of a same polynomial system with different coefficients is found to share\nconsistent inner structure. By precomputing such structures offline, Gr\\\"obner\nbasis as well as the polynomial system solutions can be solved automatically\nand efficiently online. In the past decade, several tools have been released to\ngenerate automatic solvers for a general minimal problems. The most recent tool\nautogen from Larsson et al. is a representative of these tools with\nstate-of-the-art performance in solver efficiency. GAPS wraps and improves\nautogen with more user-friendly interface, more functionality and better\nstability. We demonstrate in this report the main approach and enhancement\nfeatures of GAPS. A short tutorial of the software is also included.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:11:28 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Li", "Bo", ""], ["Larsson", "Viktor", ""]]}, {"id": "2004.11783", "submitter": "Barry De Bruin", "authors": "Barry de Bruin, Zoran Zivkovic, Henk Corporaal", "title": "Quantization of Deep Neural Networks for Accumulator-constrained\n  Processors", "comments": "20 pages, 13 figures", "journal-ref": "Microprocessors and Microsystems Volume 72, February 2020, 102872", "doi": "10.1016/j.micpro.2019.102872", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an Artificial Neural Network (ANN) quantization methodology for\nplatforms without wide accumulation registers. This enables fixed-point model\ndeployment on embedded compute platforms that are not specifically designed for\nlarge kernel computations (i.e. accumulator-constrained processors). We\nformulate the quantization problem as a function of accumulator size, and aim\nto maximize the model accuracy by maximizing bit width of input data and\nweights. To reduce the number of configurations to consider, only solutions\nthat fully utilize the available accumulator bits are being tested. We\ndemonstrate that 16-bit accumulators are able to obtain a classification\naccuracy within 1\\% of the floating-point baselines on the CIFAR-10 and\nILSVRC2012 image classification benchmarks. Additionally, a near-optimal\n$2\\times$ speedup is obtained on an ARM processor, by exploiting 16-bit\naccumulators for image classification on the All-CNN-C and AlexNet networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:47:14 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["de Bruin", "Barry", ""], ["Zivkovic", "Zoran", ""], ["Corporaal", "Henk", ""]]}, {"id": "2004.11784", "submitter": "Dahlia Urbach", "authors": "Dahlia Urbach, Yizhak Ben-Shabat, Michael Lindenbaum", "title": "DPDist : Comparing Point Clouds Using Deep Point Cloud Distance", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58621-8_32", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new deep learning method for point cloud comparison. Our\napproach, named Deep Point Cloud Distance (DPDist), measures the distance\nbetween the points in one cloud and the estimated surface from which the other\npoint cloud is sampled. The surface is estimated locally and efficiently using\nthe 3D modified Fisher vector representation. The local representation reduces\nthe complexity of the surface, enabling efficient and effective learning, which\ngeneralizes well between object categories. We test the proposed distance in\nchallenging tasks, such as similar object comparison and registration, and show\nthat it provides significant improvements over commonly used distances such as\nChamfer distance, Earth mover's distance, and others.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:47:44 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 07:46:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Urbach", "Dahlia", ""], ["Ben-Shabat", "Yizhak", ""], ["Lindenbaum", "Michael", ""]]}, {"id": "2004.11803", "submitter": "Larissa Triess", "authors": "Larissa T. Triess, David Peter, Christoph B. Rist, J. Marius Z\\\"ollner", "title": "Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental\n  Study", "comments": "Accepted at IEEE Intelligent Vehicles Symposium (IV) 2020. The code\n  can be found here: http://ltriess.github.io/scan-semseg", "journal-ref": null, "doi": "10.1109/IV47402.2020.9304631", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles need to have a semantic understanding of the\nthree-dimensional world around them in order to reason about their environment.\nState of the art methods use deep neural networks to predict semantic classes\nfor each point in a LiDAR scan. A powerful and efficient way to process LiDAR\nmeasurements is to use two-dimensional, image-like projections. In this work,\nwe perform a comprehensive experimental study of image-based semantic\nsegmentation architectures for LiDAR point clouds. We demonstrate various\ntechniques to boost the performance and to improve runtime as well as memory\nconstraints.\n  First, we examine the effect of network size and suggest that much faster\ninference times can be achieved at a very low cost to accuracy. Next, we\nintroduce an improved point cloud projection technique that does not suffer\nfrom systematic occlusions. We use a cyclic padding mechanism that provides\ncontext at the horizontal field-of-view boundaries. In a third part, we perform\nexperiments with a soft Dice loss function that directly optimizes for the\nintersection-over-union metric. Finally, we propose a new kind of convolution\nlayer with a reduced amount of weight-sharing along one of the two spatial\ndimensions, addressing the large difference in appearance along the vertical\naxis of a LiDAR scan. We propose a final set of the above methods with which\nthe model achieves an increase of 3.2% in mIoU segmentation performance over\nthe baseline while requiring only 42% of the original inference time.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:08:12 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 07:12:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Triess", "Larissa T.", ""], ["Peter", "David", ""], ["Rist", "Christoph B.", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "2004.11804", "submitter": "Jana Obernosterer", "authors": "Nika Dogonadze, Jana Obernosterer, Ji Hou", "title": "Deep Face Forgery Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid progress in deep learning is continuously making it easier and cheaper\nto generate video forgeries. Hence, it becomes very important to have a\nreliable way of detecting these forgeries. This paper describes such an\napproach for various tampering scenarios. The problem is modelled as a\nper-frame binary classification task. We propose to use transfer learning from\nface recognition task to improve tampering detection on many different facial\nmanipulation scenarios. Furthermore, in low resolution settings, where single\nframe detection performs poorly, we try to make use of neighboring frames for\nmiddle frame classification. We evaluate both approaches on the public\nFaceForensics benchmark, achieving state of the art accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:13:04 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Dogonadze", "Nika", ""], ["Obernosterer", "Jana", ""], ["Hou", "Ji", ""]]}, {"id": "2004.11805", "submitter": "Sergey Tarasenko", "authors": "Sergey Tarasenko and Fumihiko Takahashi", "title": "Applications of the Streaming Networks", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recently Streaming Networks (STnets) have been introduced as a mechanism\nof robust noise-corrupted images classification. STnets is a family of\nconvolutional neural networks, which consists of multiple neural networks\n(streams), which have different inputs and their outputs are concatenated and\nfed into a single joint classifier. The original paper has illustrated how\nSTnets can successfully classify images from Cifar10, EuroSat and UCmerced\ndatasets, when images were corrupted with various levels of random zero noise.\nIn this paper, we demonstrate that STnets are capable of high accuracy\nclassification of images corrupted with Gaussian noise, fog, snow, etc.\n(Cifar10 corrupted dataset) and low light images (subset of Carvana dataset).\nWe also introduce a new type of STnets called Hybrid STnets. Thus, we\nillustrate that STnets is a universal tool of image classification when\noriginal training dataset is corrupted with noise or other transformations,\nwhich lead to information loss from original images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 08:13:17 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Tarasenko", "Sergey", ""], ["Takahashi", "Fumihiko", ""]]}, {"id": "2004.11814", "submitter": "Feng Li", "authors": "Feng Li, Runming Cong, Huihui Bai, and Yifan He", "title": "Deep Interleaved Network for Image Super-Resolution With Asymmetric\n  Co-Attention", "comments": "Accepted by the IJCAI-PRICAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (CNN) based image super-resolution\n(SR) have shown significant success in the literature. However, these methods\nare implemented as single-path stream to enrich feature maps from the input for\nthe final prediction, which fail to fully incorporate former low-level features\ninto later high-level features. In this paper, to tackle this problem, we\npropose a deep interleaved network (DIN) to learn how information at different\nstates should be combined for image SR where shallow information guides deep\nrepresentative features prediction. Our DIN follows a multi-branch pattern\nallowing multiple interconnected branches to interleave and fuse at different\nstates. Besides, the asymmetric co-attention (AsyCA) is proposed and attacked\nto the interleaved nodes to adaptively emphasize informative features from\ndifferent states and improve the discriminative ability of networks. Extensive\nexperiments demonstrate the superiority of our proposed DIN in comparison with\nthe state-of-the-art SR methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:49:18 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Li", "Feng", ""], ["Cong", "Runming", ""], ["Bai", "Huihui", ""], ["He", "Yifan", ""]]}, {"id": "2004.11819", "submitter": "Junhee Kim", "authors": "Younghwan Na, Jun Hee Kim, Kyungsu Lee, Juhum Park, Jae Youn Hwang,\n  Jihwan P. Choi", "title": "Domain Adaptive Transfer Attack (DATA)-based Segmentation Networks for\n  Building Extraction from Aerial Images", "comments": "11pages, 12 figures", "journal-ref": null, "doi": "10.1109/TGRS.2020.3010055", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation models based on convolutional neural networks (CNNs)\nhave gained much attention in relation to remote sensing and have achieved\nremarkable performance for the extraction of buildings from high-resolution\naerial images. However, the issue of limited generalization for unseen images\nremains. When there is a domain gap between the training and test datasets,\nCNN-based segmentation models trained by a training dataset fail to segment\nbuildings for the test dataset. In this paper, we propose segmentation networks\nbased on a domain adaptive transfer attack (DATA) scheme for building\nextraction from aerial images. The proposed system combines the domain transfer\nand adversarial attack concepts. Based on the DATA scheme, the distribution of\nthe input images can be shifted to that of the target images while turning\nimages into adversarial examples against a target network. Defending\nadversarial examples adapted to the target domain can overcome the performance\ndegradation due to the domain gap and increase the robustness of the\nsegmentation model. Cross-dataset experiments and the ablation study are\nconducted for the three different datasets: the Inria aerial image labeling\ndataset, the Massachusetts building dataset, and the WHU East Asia dataset.\nCompared to the performance of the segmentation network without the DATA\nscheme, the proposed method shows improvements in the overall IoU. Moreover, it\nis verified that the proposed method outperforms even when compared to feature\nadaptation (FA) and output space adaptation (OSA).\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 06:17:13 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 06:12:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Na", "Younghwan", ""], ["Kim", "Jun Hee", ""], ["Lee", "Kyungsu", ""], ["Park", "Juhum", ""], ["Hwang", "Jae Youn", ""], ["Choi", "Jihwan P.", ""]]}, {"id": "2004.11820", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard Hovy", "title": "Decoupling Global and Local Representations via Invertible Generative\n  Flows", "comments": "Camera-ready at ICLR 2021. 23 pages (plus appendix), 16 figures, 5\n  tables. Due to arxiv size constraints, this version is using downscaled\n  images. Please download the full-resolution version from\n  https://vixra.org/abs/2004.0222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new generative model that is capable of\nautomatically decoupling global and local representations of images in an\nentirely unsupervised setting, by embedding a generative flow in the VAE\nframework to model the decoder. Specifically, the proposed model utilizes the\nvariational auto-encoding framework to learn a (low-dimensional) vector of\nlatent variables to capture the global information of an image, which is fed as\na conditional input to a flow-based invertible decoder with architecture\nborrowed from style transfer literature. Experimental results on standard image\nbenchmarks demonstrate the effectiveness of our model in terms of density\nestimation, image generation and unsupervised representation learning.\nImportantly, this work demonstrates that with only architectural inductive\nbiases, a generative model with a likelihood-based objective is capable of\nlearning decoupled representations, requiring no explicit supervision. The code\nfor our model is available at https://github.com/XuezheMax/wolf.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 03:18:13 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 20:17:34 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ma", "Xuezhe", ""], ["Kong", "Xiang", ""], ["Zhang", "Shanghang", ""], ["Hovy", "Eduard", ""]]}, {"id": "2004.11822", "submitter": "Yu Cheng", "authors": "Yu Cheng, Bo Yang, Bo Wang, Robby T. Tan", "title": "3D Human Pose Estimation using Spatio-Temporal Networks with Explicit\n  Occlusion Training", "comments": "8 pages, AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D poses from a monocular video is still a challenging task,\ndespite the significant progress that has been made in recent years. Generally,\nthe performance of existing methods drops when the target person is too\nsmall/large, or the motion is too fast/slow relative to the scale and speed of\nthe training data. Moreover, to our knowledge, many of these methods are not\ndesigned or trained under severe occlusion explicitly, making their performance\non handling occlusion compromised. Addressing these problems, we introduce a\nspatio-temporal network for robust 3D human pose estimation. As humans in\nvideos may appear in different scales and have various motion speeds, we apply\nmulti-scale spatial features for 2D joints or keypoints prediction in each\nindividual frame, and multi-stride temporal convolutional net-works (TCNs) to\nestimate 3D joints or keypoints. Furthermore, we design a spatio-temporal\ndiscriminator based on body structures as well as limb motions to assess\nwhether the predicted pose forms a valid pose and a valid movement. During\ntraining, we explicitly mask out some keypoints to simulate various occlusion\ncases, from minor to severe occlusion, so that our network can learn better and\nbecomes robust to various degrees of occlusion. As there are limited 3D\nground-truth data, we further utilize 2D video data to inject a semi-supervised\nlearning capability to our network. Experiments on public datasets validate the\neffectiveness of our method, and our ablation studies show the strengths of our\nnetwork\\'s individual submodules.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:12:12 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Cheng", "Yu", ""], ["Yang", "Bo", ""], ["Wang", "Bo", ""], ["Tan", "Robby T.", ""]]}, {"id": "2004.11823", "submitter": "Amil Khanzada", "authors": "Amil Khanzada, Charles Bai, Ferhat Turker Celepcikay", "title": "Facial Expression Recognition with Deep Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most universal ways that people communicate is through facial\nexpressions. In this paper, we take a deep dive, implementing multiple deep\nlearning models for facial expression recognition (FER). Our goals are twofold:\nwe aim not only to maximize accuracy, but also to apply our results to the\nreal-world. By leveraging numerous techniques from recent research, we\ndemonstrate a state-of-the-art 75.8% accuracy on the FER2013 test set,\noutperforming all existing publications. Additionally, we showcase a mobile web\napp which runs our FER models on-device in real time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:12:49 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Khanzada", "Amil", ""], ["Bai", "Charles", ""], ["Celepcikay", "Ferhat Turker", ""]]}, {"id": "2004.11824", "submitter": "Alex Levering", "authors": "Alex Levering, Martin Tomko, Devis Tuia, Kourosh Khoshelham", "title": "Detecting Unsigned Physical Road Incidents from Driver-View Images", "comments": "Preprint to T-IV paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Safety on roads is of uttermost importance, especially in the context of\nautonomous vehicles. A critical need is to detect and communicate disruptive\nincidents early and effectively. In this paper we propose a system based on an\noff-the-shelf deep neural network architecture that is able to detect and\nrecognize types of unsigned (non-placarded, such as traffic signs), physical\n(visible in images) road incidents. We develop a taxonomy for unsigned physical\nincidents to provide a means of organizing and grouping related incidents.\nAfter selecting eight target types of incidents, we collect a dataset of twelve\nthousand images gathered from publicly-available web sources. We subsequently\nfine-tune a convolutional neural network to recognize the eight types of road\nincidents. The proposed model is able to recognize incidents with a high level\nof accuracy (higher than 90%). We further show that while our system\ngeneralizes well across spatial context by training a classifier on\ngeostratified data in the United Kingdom (with an accuracy of over 90%), the\ntranslation to visually less similar environments requires spatially\ndistributed data collection.\n  Note: this is a pre-print version of work accepted in IEEE Transactions on\nIntelligent Vehicles (T-IV;in press). The paper is currently in production, and\nthe DOI link will be added soon.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:02:17 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Levering", "Alex", ""], ["Tomko", "Martin", ""], ["Tuia", "Devis", ""], ["Khoshelham", "Kourosh", ""]]}, {"id": "2004.11838", "submitter": "Firoj Alam", "authors": "Ferda Ofli, Firoj Alam and Muhammad Imran", "title": "Analysis of Social Media Data using Multimodal Deep Learning for\n  Disaster Response", "comments": "Accepted in ISCRAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia content in social media platforms provides significant information\nduring disaster events. The types of information shared include reports of\ninjured or deceased people, infrastructure damage, and missing or found people,\namong others. Although many studies have shown the usefulness of both text and\nimage content for disaster response purposes, the research has been mostly\nfocused on analyzing only the text modality in the past. In this paper, we\npropose to use both text and image modalities of social media data to learn a\njoint representation using state-of-the-art deep learning techniques.\nSpecifically, we utilize convolutional neural networks to define a multimodal\ndeep learning architecture with a modality-agnostic shared representation.\nExtensive experiments on real-world disaster datasets show that the proposed\nmultimodal architecture yields better performance than models trained using a\nsingle modality (e.g., either text or image).\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 19:36:11 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ofli", "Ferda", ""], ["Alam", "Firoj", ""], ["Imran", "Muhammad", ""]]}, {"id": "2004.11848", "submitter": "Chao Zhou", "authors": "Xinting Yang, Song Zhang, Jintao Liu, Qinfeng Gao, Shuanglin Dong,\n  Chao Zhou", "title": "Deep learning for smart fish farming: applications, opportunities and\n  challenges", "comments": "43 pages, 7 figures", "journal-ref": "Reviews in aquaculture,2020", "doi": "10.1111/raq.12464", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid emergence of deep learning (DL) technology, it has been\nsuccessfully used in various fields including aquaculture. This change can\ncreate new opportunities and a series of challenges for information and data\nprocessing in smart fish farming. This paper focuses on the applications of DL\nin aquaculture, including live fish identification, species classification,\nbehavioral analysis, feeding decision-making, size or biomass estimation, water\nquality prediction. In addition, the technical details of DL methods applied to\nsmart fish farming are also analyzed, including data, algorithms, computing\npower, and performance. The results of this review show that the most\nsignificant contribution of DL is the ability to automatically extract\nfeatures. However, challenges still exist; DL is still in an era of weak\nartificial intelligence. A large number of labeled data are needed for\ntraining, which has become a bottleneck restricting further DL applications in\naquaculture. Nevertheless, DL still offers breakthroughs in the handling of\ncomplex data in aquaculture. In brief, our purpose is to provide researchers\nand practitioners with a better understanding of the current state of the art\nof DL in aquaculture, which can provide strong support for the implementation\nof smart fish farming.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:07:27 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 11:11:22 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Yang", "Xinting", ""], ["Zhang", "Song", ""], ["Liu", "Jintao", ""], ["Gao", "Qinfeng", ""], ["Dong", "Shuanglin", ""], ["Zhou", "Chao", ""]]}, {"id": "2004.11853", "submitter": "Moi Hoon Yap", "authors": "Bill Cassidy and Neil D. Reeves and Pappachan Joseph and David\n  Gillespie and Claire O'Shea and Satyan Rajbhandari and Arun G. Maiya and Eibe\n  Frank and Andrew Boulton and David Armstrong and Bijan Najafi and Justina Wu\n  and Moi Hoon Yap", "title": "DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection", "comments": "16 pages, 8 figures", "journal-ref": "touchREVIEWS in Endocrinology, 17(1):5-11 (2021)", "doi": "10.17925/EE.2021.1.1.5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every 20 seconds, a limb is amputated somewhere in the world due to diabetes.\nThis is a global health problem that requires a global solution. The MICCAI\nchallenge discussed in this paper, which concerns the automated detection of\ndiabetic foot ulcers using machine learning techniques, will accelerate the\ndevelopment of innovative healthcare technology to address this unmet medical\nneed. In an effort to improve patient care and reduce the strain on healthcare\nsystems, recent research has focused on the creation of cloud-based detection\nalgorithms. These can be consumed as a service by a mobile app that patients\n(or a carer, partner or family member) could use themselves at home to monitor\ntheir condition and to detect the appearance of a diabetic foot ulcer (DFU).\nCollaborative work between Manchester Metropolitan University, Lancashire\nTeaching Hospital and the Manchester University NHS Foundation Trust has\ncreated a repository of 4,000 DFU images for the purpose of supporting research\ntoward more advanced methods of DFU detection. Based on a joint effort\ninvolving the lead scientists of the UK, US, India and New Zealand, this\nchallenge will solicit original work, and promote interactions between\nresearchers and interdisciplinary collaborations. This paper presents a dataset\ndescription and analysis, assessment methods, benchmark algorithms and initial\nevaluation results. It facilitates the challenge by providing useful insights\ninto state-of-the-art and ongoing research. This grand challenge takes on even\ngreater urgency in a peri and post-pandemic period, where stresses on resource\nutilization will increase the need for technology that allows people to remain\nactive, healthy and intact in their home.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:56:48 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 16:50:04 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 12:20:32 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cassidy", "Bill", ""], ["Reeves", "Neil D.", ""], ["Joseph", "Pappachan", ""], ["Gillespie", "David", ""], ["O'Shea", "Claire", ""], ["Rajbhandari", "Satyan", ""], ["Maiya", "Arun G.", ""], ["Frank", "Eibe", ""], ["Boulton", "Andrew", ""], ["Armstrong", "David", ""], ["Najafi", "Bijan", ""], ["Wu", "Justina", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2004.11855", "submitter": "Sonaal Kant", "authors": "Sonaal Kant", "title": "Learning Gaussian Maps for Dense Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a famous branch of research in computer vision, many\nstate of the art object detection algorithms have been introduced in the recent\npast, but how good are those object detectors when it comes to dense object\ndetection? In this paper we review common and highly accurate object detection\nmethods on the scenes where numerous similar looking objects are placed in\nclose proximity with each other. We also show that, multi-task learning of\ngaussian maps along with classification and bounding box regression gives us a\nsignificant boost in accuracy over the baseline. We introduce Gaussian Layer\nand Gaussian Decoder in the existing RetinaNet network for better accuracy in\ndense scenes, with the same computational cost as the RetinaNet. We show the\ngain of 6\\% and 5\\% in mAP with respect to baseline RetinaNet. Our method also\nachieves the state of the art accuracy on the SKU110K \\cite{sku110k} dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:01:25 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 09:51:10 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Kant", "Sonaal", ""]]}, {"id": "2004.11883", "submitter": "Duy-Kien Nguyen", "authors": "Duy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen", "title": "MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on visual counting, which aims to predict the number of\noccurrences given a natural image and a query (e.g. a question or a category).\nUnlike most prior works that use explicit, symbolic models which can be\ncomputationally expensive and limited in generalization, we propose a simple\nand effective alternative by revisiting modulated convolutions that fuse the\nquery and the image locally. Following the design of residual bottleneck, we\ncall our method MoVie, short for Modulated conVolutional bottlenecks. Notably,\nMoVie reasons implicitly and holistically and only needs a single forward-pass\nduring inference. Nevertheless, MoVie showcases strong performance for\ncounting: 1) advancing the state-of-the-art on counting-specific VQA tasks\nwhile being more efficient; 2) outperforming prior-art on difficult benchmarks\nlike COCO for common object counting; 3) helped us secure the first place of\n2020 VQA challenge when integrated as a module for 'number' related questions\nin generic VQA models. Finally, we show evidence that modulated convolutions\nsuch as MoVie can serve as a general mechanism for reasoning tasks beyond\ncounting.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:49:56 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:31:18 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 06:50:48 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Nguyen", "Duy-Kien", ""], ["Goswami", "Vedanuj", ""], ["Chen", "Xinlei", ""]]}, {"id": "2004.11909", "submitter": "Diego Alberto Mercado-Ravell Dr.", "authors": "Marichelo Garcia-Venegas, Diego A. Mercado-Ravell and Carlos A.\n  Carballo-Monsivais", "title": "On the safety of vulnerable road users by cyclist orientation detection\n  using Deep Learning", "comments": "\"This paper is a preprint of a paper submitted to IET Intelligent\n  Transport Systems. If accepted, the copy of record will be available at the\n  IET Digital Library\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, orientation detection using Deep Learning is acknowledged for a\nparticularly vulnerable class of road users,the cyclists. Knowing the cyclists'\norientation is of great relevance since it provides a good notion about their\nfuture trajectory, which is crucial to avoid accidents in the context of\nintelligent transportation systems. Using Transfer Learning with pre-trained\nmodels and TensorFlow, we present a performance comparison between the main\nalgorithms reported in the literature for object detection,such as SSD, Faster\nR-CNN and R-FCN along with MobilenetV2, InceptionV2, ResNet50, ResNet101\nfeature extractors. Moreover, we propose multi-class detection with eight\ndifferent classes according to orientations. To do so, we introduce a new\ndataset called \"Detect-Bike\", containing 20,229 cyclist instances over 11,103\nimages, which has been labeled based on cyclist's orientation. Then, the same\nDeep Learning methods used for detection are trained to determine the target's\nheading. Our experimental results and vast evaluation showed satisfactory\nperformance of all of the studied methods for the cyclists and their\norientation detection, especially using Faster R-CNN with ResNet50 proved to be\nprecise but significantly slower. Meanwhile, SSD using InceptionV2 provided\ngood trade-off between precision and execution time, and is to be preferred for\nreal-time embedded applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 18:10:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Garcia-Venegas", "Marichelo", ""], ["Mercado-Ravell", "Diego A.", ""], ["Carballo-Monsivais", "Carlos A.", ""]]}, {"id": "2004.11958", "submitter": "Awais Khan", "authors": "Ranjita Thapa (1), Noah Snavely (2), Serge Belongie (2), Awais Khan\n  (1) ((1) Plant Pathology and Plant-Microbe Biology Section, Cornell\n  University, Geneva, NY, (2) Cornell Tech)", "title": "The Plant Pathology 2020 challenge dataset to classify foliar disease of\n  apples", "comments": "11 pages, 5 figures, Kaggle competition website:\n  https://www.kaggle.com/c/plant-pathology-2020-fgvc7, CVPR fine-grained visual\n  categorization website: https://sites.google.com/view/fgvc7/competitions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apple orchards in the U.S. are under constant threat from a large number of\npathogens and insects. Appropriate and timely deployment of disease management\ndepends on early disease detection. Incorrect and delayed diagnosis can result\nin either excessive or inadequate use of chemicals, with increased production\ncosts, environmental, and health impacts. We have manually captured 3,651\nhigh-quality, real-life symptom images of multiple apple foliar diseases, with\nvariable illumination, angles, surfaces, and noise. A subset, expert-annotated\nto create a pilot dataset for apple scab, cedar apple rust, and healthy leaves,\nwas made available to the Kaggle community for 'Plant Pathology Challenge';\npart of the Fine-Grained Visual Categorization (FGVC) workshop at CVPR 2020\n(Computer Vision and Pattern Recognition). We also trained an off-the-shelf\nconvolutional neural network (CNN) on this data for disease classification and\nachieved 97% accuracy on a held-out test set. This dataset will contribute\ntowards development and deployment of machine learning-based automated plant\ndisease classification algorithms to ultimately realize fast and accurate\ndisease detection. We will continue to add images to the pilot dataset for a\nlarger, more comprehensive expert-annotated dataset for future Kaggle\ncompetitions and to explore more advanced methods for disease classification\nand quantification.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:36:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Thapa", "Ranjita", ""], ["Snavely", "Noah", ""], ["Belongie", "Serge", ""], ["Khan", "Awais", ""]]}, {"id": "2004.11966", "submitter": "Nima Tajbakhsh", "authors": "Gaurav Fotedar, Nima Tajbakhsh, Shilpa Ananth, and Xiaowei Ding", "title": "Extreme Consistency: Overcoming Annotation Scarcity and Domain Shifts", "comments": "submitted for peer-review on March 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning has proved effective for medical image analysis. However,\nit can utilize only the small labeled portion of data; it fails to leverage the\nlarge amounts of unlabeled data that is often available in medical image\ndatasets. Supervised models are further handicapped by domain shifts, when the\nlabeled dataset, despite being large enough, fails to cover different protocols\nor ethnicities. In this paper, we introduce \\emph{extreme consistency}, which\novercomes the above limitations, by maximally leveraging unlabeled data from\nthe same or a different domain in a teacher-student semi-supervised paradigm.\nExtreme consistency is the process of sending an extreme transformation of a\ngiven image to the student network and then constraining its prediction to be\nconsistent with the teacher network's prediction for the untransformed image.\nThe extreme nature of our consistency loss distinguishes our method from\nrelated works that yield suboptimal performance by exercising only mild\nprediction consistency. Our method is 1) auto-didactic, as it requires no extra\nexpert annotations; 2) versatile, as it handles both domain shift and limited\nannotation problems; 3) generic, as it is readily applicable to classification,\nsegmentation, and detection tasks; and 4) simple to implement, as it requires\nno adversarial training. We evaluate our method for the tasks of lesion and\nretinal vessel segmentation in skin and fundus images. Our experiments\ndemonstrate a significant performance gain over both modern supervised networks\nand recent semi-supervised models. This performance is attributed to the strong\nregularization enforced by extreme consistency, which enables the student\nnetwork to learn how to handle extreme variants of both labeled and unlabeled\nimages. This enhances the network's ability to tackle the inevitable same- and\ncross-domain data variability during inference.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:32:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Fotedar", "Gaurav", ""], ["Tajbakhsh", "Nima", ""], ["Ananth", "Shilpa", ""], ["Ding", "Xiaowei", ""]]}, {"id": "2004.11967", "submitter": "Antreas Antoniou Mr", "authors": "Antreas Antoniou, Massimiliano Patacchiola, Mateusz Ochal and Amos\n  Storkey", "title": "Defining Benchmarks for Continual Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both few-shot and continual learning have seen substantial progress in the\nlast years due to the introduction of proper benchmarks. That being said, the\nfield has still to frame a suite of benchmarks for the highly desirable setting\nof continual few-shot learning, where the learner is presented a number of\nfew-shot tasks, one after the other, and then asked to perform well on a\nvalidation set stemming from all previously seen tasks. Continual few-shot\nlearning has a small computational footprint and is thus an excellent setting\nfor efficient investigation and experimentation. In this paper we first define\na theoretical framework for continual few-shot learning, taking into account\nrecent literature, then we propose a range of flexible benchmarks that unify\nthe evaluation criteria and allows exploring the problem from multiple\nperspectives. As part of the benchmark, we introduce a compact variant of\nImageNet, called SlimageNet64, which retains all original 1000 classes but only\ncontains 200 instances of each one (a total of 200K data-points) downscaled to\n64 x 64 pixels. We provide baselines for the proposed benchmarks using a number\nof popular few-shot learning algorithms, as a result, exposing previously\nunknown strengths and weaknesses of those algorithms in continual and\ndata-limited settings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:41:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Antoniou", "Antreas", ""], ["Patacchiola", "Massimiliano", ""], ["Ochal", "Mateusz", ""], ["Storkey", "Amos", ""]]}, {"id": "2004.11968", "submitter": "Edgar Avalos", "authors": "Edgar Avalos, Kazuto Akagi and Yasumasa Nishiura", "title": "Visible fingerprint of X-ray images of epoxy resins using singular value\n  decomposition of deep learning features", "comments": "43 pages, 16 figures", "journal-ref": "COMMAT Volume 186, January 2021, 109996", "doi": "10.1016/j.commatsci.2020.109996", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the process variables of epoxy resins alter their mechanical\nproperties, the visual identification of the characteristic features of X-ray\nimages of samples of these materials is challenging. To facilitate the\nidentification, we approximate the magnitude of the gradient of the intensity\nfield of the X-ray images of different kinds of epoxy resins and then we use\ndeep learning to discover the most representative features of the transformed\nimages. In this solution of the inverse problem to finding characteristic\nfeatures to discriminate samples of heterogeneous materials, we use the\neigenvectors obtained from the singular value decomposition of all the channels\nof the feature maps of the early layers in a convolutional neural network.\nWhile the strongest activated channel gives a visual representation of the\ncharacteristic features, often these are not robust enough in some practical\nsettings. On the other hand, the left singular vectors of the matrix\ndecomposition of the feature maps, barely change when variables such as the\ncapacity of the network or network architecture change. High classification\naccuracy and robustness of characteristic features are presented in this work.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:44:08 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 03:46:45 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Avalos", "Edgar", ""], ["Akagi", "Kazuto", ""], ["Nishiura", "Yasumasa", ""]]}, {"id": "2004.11969", "submitter": "Xin Li", "authors": "Xin Li, Yijia He, Jinlong Lin, Xiao Liu", "title": "Leveraging Planar Regularities for Point Line Visual-Inertial Odometry", "comments": "Accepted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With monocular Visual-Inertial Odometry (VIO) system, 3D point cloud and\ncamera motion can be estimated simultaneously. Because pure sparse 3D points\nprovide a structureless representation of the environment, generating 3D mesh\nfrom sparse points can further model the environment topology and produce dense\nmapping. To improve the accuracy of 3D mesh generation and localization, we\npropose a tightly-coupled monocular VIO system, PLP-VIO, which exploits point\nfeatures and line features as well as plane regularities. The co-planarity\nconstraints are used to leverage additional structure information for the more\naccurate estimation of 3D points and spatial lines in state estimator. To\ndetect plane and 3D mesh robustly, we combine both the line features with point\nfeatures in the detection method. The effectiveness of the proposed method is\nverified on both synthetic data and public datasets and is compared with other\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 18:20:00 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 07:06:17 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Li", "Xin", ""], ["He", "Yijia", ""], ["Lin", "Jinlong", ""], ["Liu", "Xiao", ""]]}, {"id": "2004.11970", "submitter": "Anis Koubaa", "authors": "Alam Noor, Bilel Benjdira, Adel Ammar, Anis Koubaa", "title": "DriftNet: Aggressive Driving Behavior Classification using 3D\n  EfficientNet Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": "RIOTU-TR-2020-04", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aggressive driving (i.e., car drifting) is a dangerous behavior that puts\nhuman safety and life into a significant risk. This behavior is considered as\nan anomaly concerning the regular traffic in public transportation roads.\nRecent techniques in deep learning proposed new approaches for anomaly\ndetection in different contexts such as pedestrian monitoring, street fighting,\nand threat detection. In this paper, we propose a new anomaly detection\nframework applied to the detection of aggressive driving behavior. Our\ncontribution consists in the development of a 3D neural network architecture,\nbased on the state-of-the-art EfficientNet 2D image classifier, for the\naggressive driving detection in videos. We propose an EfficientNet3D CNN\nfeature extractor for video analysis, and we compare it with existing feature\nextractors. We also created a dataset of car drifting in Saudi Arabian context\nhttps://www.youtube.com/watch?v=vLzgye1-d1k . To the best of our knowledge,\nthis is the first work that addresses the problem of aggressive driving\nbehavior using deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:36:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Noor", "Alam", ""], ["Benjdira", "Bilel", ""], ["Ammar", "Adel", ""], ["Koubaa", "Anis", ""]]}, {"id": "2004.11981", "submitter": "Aleksandra \\'Ciprijanovi\\'c", "authors": "A. \\'Ciprijanovi\\'c, G. F. Snyder, B. Nord, J. E. G. Peek", "title": "DeepMerge: Classifying High-redshift Merging Galaxies with Deep Neural\n  Networks", "comments": "17 pages, 8 figures, submitted to Astronomy & Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate and demonstrate the use of convolutional neural networks\n(CNNs) for the task of distinguishing between merging and non-merging galaxies\nin simulated images, and for the first time at high redshifts (i.e. $z=2$). We\nextract images of merging and non-merging galaxies from the Illustris-1\ncosmological simulation and apply observational and experimental noise that\nmimics that from the Hubble Space Telescope; the data without noise form a\n\"pristine\" data set and that with noise form a \"noisy\" data set. The test set\nclassification accuracy of the CNN is $79\\%$ for pristine and $76\\%$ for noisy.\nThe CNN outperforms a Random Forest classifier, which was shown to be superior\nto conventional one- or two-dimensional statistical methods (Concentration,\nAsymmetry, the Gini, $M_{20}$ statistics etc.), which are commonly used when\nclassifying merging galaxies. We also investigate the selection effects of the\nclassifier with respect to merger state and star formation rate, finding no\nbias. Finally, we extract Grad-CAMs (Gradient-weighted Class Activation\nMapping) from the results to further assess and interrogate the fidelity of the\nclassification model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:36:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["\u0106iprijanovi\u0107", "A.", ""], ["Snyder", "G. F.", ""], ["Nord", "B.", ""], ["Peek", "J. E. G.", ""]]}, {"id": "2004.11985", "submitter": "Nina Varney", "authors": "Nina Varney, Vijayan K. Asari and Quinn Graehling", "title": "DALES: A Large-scale Aerial LiDAR Data Set for Semantic Segmentation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Dayton Annotated LiDAR Earth Scan (DALES) data set, a new\nlarge-scale aerial LiDAR data set with over a half-billion hand-labeled points\nspanning 10 square kilometers of area and eight object categories. Large\nannotated point cloud data sets have become the standard for evaluating deep\nlearning methods. However, most of the existing data sets focus on data\ncollected from a mobile or terrestrial scanner with few focusing on aerial\ndata. Point cloud data collected from an Aerial Laser Scanner (ALS) presents a\nnew set of challenges and applications in areas such as 3D urban modeling and\nlarge-scale surveillance. DALES is the most extensive publicly available ALS\ndata set with over 400 times the number of points and six times the resolution\nof other currently available annotated aerial point cloud data sets. This data\nset gives a critical number of expert verified hand-labeled points for the\nevaluation of new 3D deep learning algorithms, helping to expand the focus of\ncurrent algorithms to aerial data. We describe the nature of our data,\nannotation workflow, and provide a benchmark of current state-of-the-art\nalgorithm performance on the DALES data set.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 20:05:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Varney", "Nina", ""], ["Asari", "Vijayan K.", ""], ["Graehling", "Quinn", ""]]}, {"id": "2004.11989", "submitter": "Subhradeep Kayal", "authors": "Subhradeep Kayal and Florian Dubost and Harm A. W. M. Tiddens and\n  Marleen de Bruijne", "title": "Spectral Data Augmentation Techniques to quantify Lung Pathology from\n  CT-images", "comments": "5 pages including references, accepted as Oral presentation at IEEE\n  ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is of paramount importance in biomedical image processing\ntasks, characterized by inadequate amounts of labelled data, to best use all of\nthe data that is present. In-use techniques range from intensity\ntransformations and elastic deformations, to linearly combining existing data\npoints to make new ones. In this work, we propose the use of spectral\ntechniques for data augmentation, using the discrete cosine and wavelet\ntransforms. We empirically evaluate our approaches on a CT texture analysis\ntask to detect abnormal lung-tissue in patients with cystic fibrosis. Empirical\nexperiments show that the proposed spectral methods perform favourably as\ncompared to the existing methods. When used in combination with existing\nmethods, our proposed approach can increase the relative minor class\nsegmentation performance by 44.1% over a simple replication baseline.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:57:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kayal", "Subhradeep", ""], ["Dubost", "Florian", ""], ["Tiddens", "Harm A. W. M.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2004.11992", "submitter": "Bram Wallace", "authors": "Bram Wallace, Bharath Hariharan", "title": "Extending and Analyzing Self-Supervised Learning Across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning has achieved impressive results in\nrecent years, with experiments primarily coming on ImageNet or other similarly\nlarge internet imagery datasets. There has been little to no work with these\nmethods on other smaller domains, such as satellite, textural, or biological\nimagery. We experiment with several popular methods on an unprecedented variety\nof domains. We discover, among other findings, that Rotation is by far the most\nsemantically meaningful task, with much of the performance of Jigsaw and\nInstance Discrimination being attributable to the nature of their induced\ndistribution rather than semantic understanding. Additionally, there are\nseveral areas, such as fine-grain classification, where all tasks underperform.\nWe quantitatively and qualitatively diagnose the reasons for these failures and\nsuccesses via novel experiments studying pretext generalization, random\nlabelings, and implicit dimensionality. Code and models are available at\nhttps://github.com/BramSW/Extending_SSRL_Across_Domains/.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:18:02 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 16:13:46 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wallace", "Bram", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2004.11995", "submitter": "Oliver Scheel", "authors": "Oliver Scheel, Loren Schwarz, Nassir Navab, Federico Tombari", "title": "Explicit Domain Adaptation with Loosely Coupled Samples", "comments": "Submitted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is an important field of machine learning in general, and\nparticularly in the context of fully autonomous driving, which needs to be\nsolved simultaneously for many different domains, such as changing weather\nconditions and country-specific driving behaviors. Traditional transfer\nlearning methods often focus on image data and are black-box models. In this\nwork we propose a transfer learning framework, core of which is learning an\nexplicit mapping between domains. Due to its interpretability, this is\nbeneficial for safety-critical applications, like autonomous driving. We show\nits general applicability by considering image classification problems and then\nmove on to time-series data, particularly predicting lane changes. In our\nevaluation we adapt a pre-trained model to a dataset exhibiting different\ndriving and sensory characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:23:45 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Scheel", "Oliver", ""], ["Schwarz", "Loren", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2004.12000", "submitter": "Egor Burkov", "authors": "Egor Burkov, Igor Pasechnik, Artur Grigorev, Victor Lempitsky", "title": "Neural Head Reenactment with Latent Pose Descriptors", "comments": null, "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR). pp. 13783-13792", "doi": "10.1109/CVPR42600.2020.01380", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural head reenactment system, which is driven by a latent pose\nrepresentation and is capable of predicting the foreground segmentation\nalongside the RGB image. The latent pose representation is learned as a part of\nthe entire reenactment system, and the learning process is based solely on\nimage reconstruction losses. We show that despite its simplicity, with a large\nand diverse enough training dataset, such learning successfully decomposes pose\nfrom identity. The resulting system can then reproduce mimics of the driving\nperson and, furthermore, can perform cross-person reenactment. Additionally, we\nshow that the learned descriptors are useful for other pose-related tasks, such\nas keypoint prediction and pose-based retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 21:37:52 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 13:37:15 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Burkov", "Egor", ""], ["Pasechnik", "Igor", ""], ["Grigorev", "Artur", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2004.12027", "submitter": "Daniel Mas Montserrat", "authors": "Daniel Mas Montserrat, Hanxiang Hao, S. K. Yarlagadda, Sriram\n  Baireddy, Ruiting Shao, J\\'anos Horv\\'ath, Emily Bartusiak, Justin Yang,\n  David G\\\"uera, Fengqing Zhu, Edward J. Delp", "title": "Deepfakes Detection with Automatic Face Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Altered and manipulated multimedia is increasingly present and widely\ndistributed via social media platforms. Advanced video manipulation tools\nenable the generation of highly realistic-looking altered multimedia. While\nmany methods have been presented to detect manipulations, most of them fail\nwhen evaluated with data outside of the datasets used in research environments.\nIn order to address this problem, the Deepfake Detection Challenge (DFDC)\nprovides a large dataset of videos containing realistic manipulations and an\nevaluation system that ensures that methods work quickly and accurately, even\nwhen faced with challenging data. In this paper, we introduce a method based on\nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs) that\nextracts visual and temporal features from faces present in videos to\naccurately detect manipulations. The method is evaluated with the DFDC dataset,\nproviding competitive results compared to other techniques.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:47:42 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 19:44:49 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Montserrat", "Daniel Mas", ""], ["Hao", "Hanxiang", ""], ["Yarlagadda", "S. K.", ""], ["Baireddy", "Sriram", ""], ["Shao", "Ruiting", ""], ["Horv\u00e1th", "J\u00e1nos", ""], ["Bartusiak", "Emily", ""], ["Yang", "Justin", ""], ["G\u00fcera", "David", ""], ["Zhu", "Fengqing", ""], ["Delp", "Edward J.", ""]]}, {"id": "2004.12031", "submitter": "Zakaria Aldeneh", "authors": "Zakaria Aldeneh, Anushree Prasanna Kumar, Barry-John Theobald, Erik\n  Marchi, Sachin Kajarekar, Devang Naik, Ahmed Hussen Abdelaziz", "title": "On the Role of Visual Cues in Audiovisual Speech Enhancement", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an introspection of an audiovisual speech enhancement model. In\nparticular, we focus on interpreting how a neural audiovisual speech\nenhancement model uses visual cues to improve the quality of the target speech\nsignal. We show that visual cues provide not only high-level information about\nspeech activity, i.e., speech/silence, but also fine-grained visual information\nabout the place of articulation. One byproduct of this finding is that the\nlearned visual embeddings can be used as features for other visual speech\napplications. We demonstrate the effectiveness of the learned visual embeddings\nfor classifying visemes (the visual analogy to phonemes). Our results provide\ninsight into important aspects of audiovisual speech enhancement and\ndemonstrate how such models can be used for self-supervision tasks for visual\nspeech applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:00:03 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:27:47 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 17:11:24 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 15:56:42 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Aldeneh", "Zakaria", ""], ["Kumar", "Anushree Prasanna", ""], ["Theobald", "Barry-John", ""], ["Marchi", "Erik", ""], ["Kajarekar", "Sachin", ""], ["Naik", "Devang", ""], ["Abdelaziz", "Ahmed Hussen", ""]]}, {"id": "2004.12032", "submitter": "Sang Hun Lee", "authors": "Sangrok Lee, Eunsoo Park, Hongsuk Yi, Sang Hun Lee", "title": "StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle\n  Re-Identification", "comments": "7 pages, 2 figures, CVPR Workshop Paper (Revised)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicle re-identification aims to obtain the same vehicles from vehicle\nimages. This is challenging but essential for analyzing and predicting traffic\nflow in the city. Although deep learning methods have achieved enormous\nprogress for this task, their large data requirement is a critical shortcoming.\nTherefore, we propose a synthetic-to-real domain adaptation network (StRDAN)\nframework, which can be trained with inexpensive large-scale synthetic and real\ndata to improve performance. The StRDAN training method combines domain\nadaptation and semi-supervised learning methods and their associated losses.\nStRDAN offers significant improvement over the baseline model, which can only\nbe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%\nand 12.9% improved mean average precision, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:00:30 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 07:44:10 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lee", "Sangrok", ""], ["Park", "Eunsoo", ""], ["Yi", "Hongsuk", ""], ["Lee", "Sang Hun", ""]]}, {"id": "2004.12040", "submitter": "Yomna Safaa El-Din MSc.", "authors": "Yomna Safaa El-Din, Mohamed N. Moustafa, Hani Mahdi", "title": "Deep convolutional neural networks for face and iris presentation attack\n  detection: Survey and case study", "comments": "A preprint of a paper accepted by IET Biometrics journal and is\n  subject to Institution of Engineering and Technology Copyright", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric presentation attack detection is gaining increasing attention.\nUsers of mobile devices find it more convenient to unlock their smart\napplications with finger, face or iris recognition instead of passwords. In\nthis paper, we survey the approaches presented in the recent literature to\ndetect face and iris presentation attacks. Specifically, we investigate the\neffectiveness of fine tuning very deep convolutional neural networks to the\ntask of face and iris antispoofing. We compare two different fine tuning\napproaches on six publicly available benchmark datasets. Results show the\neffectiveness of these deep models in learning discriminative features that can\ntell apart real from fake biometric images with very low error rate.\nCross-dataset evaluation on face PAD showed better generalization than state of\nthe art. We also performed cross-dataset testing on iris PAD datasets in terms\nof equal error rate which was not reported in literature before. Additionally,\nwe propose the use of a single deep network trained to detect both face and\niris attacks. We have not noticed accuracy degradation compared to networks\ntrained for only one biometric separately. Finally, we analyzed the learned\nfeatures by the network, in correlation with the image frequency components, to\njustify its prediction decision.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 02:06:19 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:51:10 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["El-Din", "Yomna Safaa", ""], ["Moustafa", "Mohamed N.", ""], ["Mahdi", "Hani", ""]]}, {"id": "2004.12051", "submitter": "Hengkai Guo", "authors": "Sicong Du, Hengkai Guo, Yao Chen, Yilun Lin, Xiangbing Meng, Linfu\n  Wen, Fei-Yue Wang", "title": "GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM\n  Initialization", "comments": "Revised some minor errors. Accepted by ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initialization is essential to monocular Simultaneous Localization and\nMapping (SLAM) problems. This paper focuses on a novel initialization method\nfor monocular SLAM based on planar features. The algorithm starts by homography\nestimation in a sliding window. It then proceeds to a global plane optimization\n(GPO) to obtain camera poses and the plane normal. 3D points can be recovered\nusing planar constraints without triangulation. The proposed method fully\nexploits the plane information from multiple frames and avoids the ambiguities\nin homography decomposition. We validate our algorithm on the collected\nchessboard dataset against baseline implementations and present extensive\nanalysis. Experimental results show that our method outperforms the fine-tuned\nbaselines in both accuracy and real-time.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 03:57:50 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 01:59:57 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Du", "Sicong", ""], ["Guo", "Hengkai", ""], ["Chen", "Yao", ""], ["Lin", "Yilun", ""], ["Meng", "Xiangbing", ""], ["Wen", "Linfu", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "2004.12059", "submitter": "Di Zhuang", "authors": "Di Zhuang, Nam Nguyen, Keyu Chen, J. Morris Chang", "title": "SAIA: Split Artificial Intelligence Architecture for Mobile Healthcare\n  System", "comments": "17 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the advancement of deep learning (DL), the Internet of Things and cloud\ncomputing techniques for biomedical and healthcare problems, mobile healthcare\nsystems have received unprecedented attention. Since DL techniques usually\nrequire enormous amount of computation, most of them cannot be directly\ndeployed on the resource-constrained mobile and IoT devices. Hence, most of the\nmobile healthcare systems leverage the cloud computing infrastructure, where\nthe data collected by the mobile and IoT devices would be transmitted to the\ncloud computing platforms for analysis. However, in the contested environments,\nrelying on the cloud might not be practical at all times. For instance, the\nsatellite communication might be denied or disrupted. We propose SAIA, a Split\nArtificial Intelligence Architecture for mobile healthcare systems. Unlike\ntraditional approaches for artificial intelligence (AI) which solely exploits\nthe computational power of the cloud server, SAIA could not only relies on the\ncloud computing infrastructure while the wireless communication is available,\nbut also utilizes the lightweight AI solutions that work locally on the client\nside, hence, it can work even when the communication is impeded. In SAIA, we\npropose a meta-information based decision unit, that could tune whether a\nsample captured by the client should be operated by the embedded AI (i.e.,\nkeeping on the client) or the networked AI (i.e., sending to the server), under\ndifferent conditions. In our experimental evaluation, extensive experiments\nhave been conducted on two popular healthcare datasets. Our results show that\nSAIA consistently outperforms its baselines in terms of both effectiveness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 05:06:51 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 05:00:59 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhuang", "Di", ""], ["Nguyen", "Nam", ""], ["Chen", "Keyu", ""], ["Chang", "J. Morris", ""]]}, {"id": "2004.12064", "submitter": "Di Zhuang", "authors": "Di Zhuang, Keyu Chen, J. Morris Chang", "title": "CS-AF: A Cost-sensitive Multi-classifier Active Fusion Framework for\n  Skin Lesion Classification", "comments": "16 pages, 8 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved the state-of-the-art\nperformance in skin lesion analysis. Compared with single CNN classifier,\ncombining the results of multiple classifiers via fusion approaches shows to be\nmore effective and robust. Since the skin lesion datasets are usually limited\nand statistically biased, while designing an effective fusion approach, it is\nimportant to consider not only the performance of each classifier on the\ntraining/validation dataset, but also the relative discriminative power (e.g.,\nconfidence) of each classifier regarding an individual sample in the testing\nphase, which calls for an active fusion approach. Furthermore, in skin lesion\nanalysis, the data of certain classes (e.g., the benign lesions) is usually\nabundant making them an over-represented majority, while the data of some other\nclasses (e.g., the cancerous lesions) is deficient, making them an\nunderrepresented minority. It is more crucial to precisely identify the samples\nfrom an underrepresented (i.e., in terms of the amount of data) but more\nimportant minority class (e.g., certain cancerous lesion). In other words,\nmisclassifying a more severe lesion to a benign or less severe lesion should\nhave relative more cost (e.g., money, time and even lives). To address such\nchallenges, we present CS-AF, a cost-sensitive multi-classifier active fusion\nframework for skin lesion classification. In the experimental evaluation, we\nprepared 96 base classifiers (of 12 CNN architectures) on the ISIC research\ndatasets. Our experimental results show that our framework consistently\noutperforms the static fusion competitors.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 05:48:14 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 04:37:03 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Zhuang", "Di", ""], ["Chen", "Keyu", ""], ["Chang", "J. Morris", ""]]}, {"id": "2004.12070", "submitter": "Zhou Yu", "authors": "Zhou Yu, Yuhao Cui, Jun Yu, Meng Wang, Dacheng Tao, Qi Tian", "title": "Deep Multimodal Neural Architecture Search", "comments": "Accept to ACM MM2020, code available at\n  https://github.com/MILVLG/mmnas/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective neural networks is fundamentally important in deep\nmultimodal learning. Most existing works focus on a single task and design\nneural architectures manually, which are highly task-specific and hard to\ngeneralize to different tasks. In this paper, we devise a generalized deep\nmultimodal neural architecture search (MMnas) framework for various multimodal\nlearning tasks. Given multimodal input, we first define a set of primitive\noperations, and then construct a deep encoder-decoder based unified backbone,\nwhere each encoder or decoder block corresponds to an operation searched from a\npredefined operation pool. On top of the unified backbone, we attach\ntask-specific heads to tackle different multimodal learning tasks. By using a\ngradient-based NAS algorithm, the optimal architectures for different tasks are\nlearned efficiently. Extensive ablation studies, comprehensive analysis, and\ncomparative experimental results show that the obtained MMnasNet significantly\noutperforms existing state-of-the-art approaches across three multimodal\nlearning tasks (over five datasets), including visual question answering,\nimage-text matching, and visual grounding.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 07:00:32 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 03:28:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yu", "Zhou", ""], ["Cui", "Yuhao", ""], ["Yu", "Jun", ""], ["Wang", "Meng", ""], ["Tao", "Dacheng", ""], ["Tian", "Qi", ""]]}, {"id": "2004.12084", "submitter": "Jannis Born", "authors": "Jannis Born, Gabriel Br\\\"andle, Manuel Cossio, Marion Disdier, Julie\n  Goulet, J\\'er\\'emie Roulin, Nina Wiedemann", "title": "POCOVID-Net: Automatic Detection of COVID-19 From a New Lung Ultrasound\n  Imaging Dataset (POCUS)", "comments": "7 pages, 4 figures", "journal-ref": "ISMB TransMed COSI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of COVID-19 into a global pandemic, there is an\never more urgent need for cheap, fast and reliable tools that can assist\nphysicians in diagnosing COVID-19. Medical imaging such as CT can take a key\nrole in complementing conventional diagnostic tools from molecular biology,\nand, using deep learning techniques, several automatic systems were\ndemonstrated promising performances using CT or X-ray data. Here, we advocate a\nmore prominent role of point-of-care ultrasound imaging to guide COVID-19\ndetection. Ultrasound is non-invasive and ubiquitous in medical facilities\naround the globe. Our contribution is threefold. First, we gather a lung\nultrasound (POCUS) dataset consisting of 1103 images (654 COVID-19, 277\nbacterial pneumonia and 172 healthy controls), sampled from 64 videos. This\ndataset was assembled from various online sources, processed specifically for\ndeep learning models and is intended to serve as a starting point for an\nopen-access initiative. Second, we train a deep convolutional neural network\n(POCOVID-Net) on this 3-class dataset and achieve an accuracy of 89% and, by a\nmajority vote, a video accuracy of 92% . For detecting COVID-19 in particular,\nthe model performs with a sensitivity of 0.96, a specificity of 0.79 and\nF1-score of 0.92 in a 5-fold cross validation. Third, we provide an open-access\nweb service (POCOVIDScreen) that is available at: https://pocovidscreen.org.\nThe website deploys the predictive model, allowing to perform predictions on\nultrasound lung images. In addition, it grants medical staff the option to\n(bulk) upload their own screenings in order to contribute to the growing public\ndatabase of pathological lung ultrasound images.\n  Dataset and code are available from:\nhttps://github.com/jannisborn/covid19_pocus_ultrasound.\n  NOTE: This preprint is superseded by our paper in Applied Sciences:\nhttps://doi.org/10.3390/app11020672\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 08:41:24 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:20:23 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 11:30:39 GMT"}, {"version": "v4", "created": "Sun, 24 Jan 2021 13:37:44 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Born", "Jannis", ""], ["Br\u00e4ndle", "Gabriel", ""], ["Cossio", "Manuel", ""], ["Disdier", "Marion", ""], ["Goulet", "Julie", ""], ["Roulin", "J\u00e9r\u00e9mie", ""], ["Wiedemann", "Nina", ""]]}, {"id": "2004.12087", "submitter": "Manman Deng", "authors": "Luhong Diao (1,2), Jinying Gao1 (1,2), Manman Deng (1,2) ((1) Beijing\n  Institute for Scientific and Engineering Computing, Beijing University of\n  Technology, Beijing, China.(2) College of Applied Sciences, Beijing\n  University of Technology, Beijing, China.)", "title": "Clustering by Constructing Hyper-Planes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a kind of basic machine learning method, clustering algorithms group data\npoints into different categories based on their similarity or distribution. We\npresent a clustering algorithm by finding hyper-planes to distinguish the data\npoints. It relies on the marginal space between the points. Then we combine\nthese hyper-planes to determine centers and numbers of clusters. Because the\nalgorithm is based on linear structures, it can approximate the distribution of\ndatasets accurately and flexibly. To evaluate its performance, we compared it\nwith some famous clustering algorithms by carrying experiments on different\nkinds of benchmark datasets. It outperforms other methods clearly.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 08:52:21 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Diao", "Luhong", ""], ["Gao1", "Jinying", ""], ["Deng", "Manman", ""]]}, {"id": "2004.12103", "submitter": "Waris Quamer", "authors": "Suyash Shandilya, Waris Quamer", "title": "How to read faces without looking at them", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face reading is the most intuitive aspect of emotion recognition.\nUnfortunately, digital analysis of facial expression requires digitally\nrecording personal faces. As emotional analysis is particularly required in a\nmore poised scenario, capturing faces becomes a gross violation of privacy. In\nthis paper, we use the concept of compressive analysis to conceptualise a\nsystem which compressively acquires faces in order to ascertain unusable\nreconstruction, while allowing for acceptable (and adjustable) accuracy in\ninference.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 10:17:38 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Shandilya", "Suyash", ""], ["Quamer", "Waris", ""]]}, {"id": "2004.12104", "submitter": "Deniz Engin", "authors": "Deniz Engin, Alperen Kantarc{\\i}, Se\\c{c}il Arslan, Haz{\\i}m Kemal\n  Ekenel", "title": "Offline Signature Verification on Real-World Documents", "comments": "CVPR 2020 Biometrics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on offline signature verification has explored a large variety of\nmethods on multiple signature datasets, which are collected under controlled\nconditions. However, these datasets may not fully reflect the characteristics\nof the signatures in some practical use cases. Real-world signatures extracted\nfrom the formal documents may contain different types of occlusions, for\nexample, stamps, company seals, ruling lines, and signature boxes. Moreover,\nthey may have very high intra-class variations, where even genuine signatures\nresemble forgeries. In this paper, we address a real-world writer independent\noffline signature verification problem, in which, a bank's customers'\ntransaction request documents that contain their occluded signatures are\ncompared with their clean reference signatures. Our proposed method consists of\ntwo main components, a stamp cleaning method based on CycleGAN and signature\nrepresentation based on CNNs. We extensively evaluate different verification\nsetups, fine-tuning strategies, and signature representation approaches to have\na thorough analysis of the problem. Moreover, we conduct a human evaluation to\nshow the challenging nature of the problem. We run experiments both on our\ncustom dataset, as well as on the publicly available Tobacco-800 dataset. The\nexperimental results validate the difficulty of offline signature verification\non real-world documents. However, by employing the stamp cleaning process, we\nimprove the signature verification performance significantly.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 10:28:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Engin", "Deniz", ""], ["Kantarc\u0131", "Alperen", ""], ["Arslan", "Se\u00e7il", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "2004.12150", "submitter": "Xiaozheng Xie", "authors": "Xiaozheng Xie, Jianwei Niu, Xuefeng Liu, Zhengsu Chen, Shaojie Tang\n  and Shui Yu", "title": "A Survey on Incorporating Domain Knowledge into Deep Learning for\n  Medical Image Analysis", "comments": "27 pages, 18 figures", "journal-ref": "Medical Image Analysis 2021", "doi": "10.1016/j.media.2021.101985", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models like CNNs have achieved great success in\nmedical image analysis, the small size of medical datasets remains a major\nbottleneck in this area. To address this problem, researchers have started\nlooking for external information beyond current available medical datasets.\nTraditional approaches generally leverage the information from natural images\nvia transfer learning. More recent works utilize the domain knowledge from\nmedical doctors, to create networks that resemble how medical doctors are\ntrained, mimic their diagnostic patterns, or focus on the features or areas\nthey pay particular attention to. In this survey, we summarize the current\nprogress on integrating medical domain knowledge into deep learning models for\nvarious tasks, such as disease diagnosis, lesion, organ and abnormality\ndetection, lesion and organ segmentation. For each task, we systematically\ncategorize different kinds of medical domain knowledge that have been utilized\nand their corresponding integrating methods. We also provide current challenges\nand directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:27:47 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 05:25:11 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 09:51:27 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 06:55:26 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Xie", "Xiaozheng", ""], ["Niu", "Jianwei", ""], ["Liu", "Xuefeng", ""], ["Chen", "Zhengsu", ""], ["Tang", "Shaojie", ""], ["Yu", "Shui", ""]]}, {"id": "2004.12152", "submitter": "Briti Gangopadhyay", "authors": "Briti Gangopadhyay, Somnath Hazra and Pallab Dasgupta", "title": "Semi-Lexical Languages -- A Formal Basis for Unifying Machine Learning\n  and Symbolic Reasoning in Computer Vision", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human vision is able to compensate imperfections in sensory inputs from the\nreal world by reasoning based on prior knowledge about the world. Machine\nlearning has had a significant impact on computer vision due to its inherent\nability in handling imprecision, but the absence of a reasoning framework based\non domain knowledge limits its ability to interpret complex scenarios. We\npropose semi-lexical languages as a formal basis for dealing with imperfect\ntokens provided by the real world. The power of machine learning is used to map\nthe imperfect tokens into the alphabet of the language and symbolic reasoning\nis used to determine the membership of input in the language. Semi-lexical\nlanguages also have bindings that prevent the variations in which a\nsemi-lexical token is interpreted in different parts of the input, thereby\nleaning on deduction to enhance the quality of recognition of individual\ntokens. We present case studies that demonstrate the advantage of using such a\nframework over pure machine learning and pure symbolic methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:37:24 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 12:20:08 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gangopadhyay", "Briti", ""], ["Hazra", "Somnath", ""], ["Dasgupta", "Pallab", ""]]}, {"id": "2004.12165", "submitter": "Andras Palffy", "authors": "Andras Palffy, Jiaao Dong, Julian F. P. Kooij and Dariu M. Gavrila", "title": "CNN based Road User Detection using the 3D Radar Cube", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters (RAL), vol. 5, nr. 2, pp.\n  1263-1270, 2020", "doi": "10.1109/LRA.2020.2967272", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a novel radar based, single-frame, multi-class detection\nmethod for moving road users (pedestrian, cyclist, car), which utilizes\nlow-level radar cube data. The method provides class information both on the\nradar target- and object-level. Radar targets are classified individually after\nextending the target features with a cropped block of the 3D radar cube around\ntheir positions, thereby capturing the motion of moving parts in the local\nvelocity distribution. A Convolutional Neural Network (CNN) is proposed for\nthis classification step. Afterwards, object proposals are generated with a\nclustering step, which not only considers the radar targets' positions and\nvelocities, but their calculated class scores as well. In experiments on a\nreal-life dataset we demonstrate that our method outperforms the\nstate-of-the-art methods both target- and object-wise by reaching an average of\n0.70 (baseline: 0.68) target-wise and 0.56 (baseline: 0.48) object-wise F1\nscore. Furthermore, we examine the importance of the used features in an\nablation study.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 15:07:03 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 10:06:15 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Palffy", "Andras", ""], ["Dong", "Jiaao", ""], ["Kooij", "Julian F. P.", ""], ["Gavrila", "Dariu M.", ""]]}, {"id": "2004.12170", "submitter": "Fatemeh Azimi", "authors": "Fatemeh Azimi, Benjamin Bischke, Sebastian Palacio, Federico Raue,\n  Joern Hees, Andreas Dengel", "title": "Revisiting Sequence-to-Sequence Video Object Segmentation with\n  Multi-Task Loss and Skip-Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video Object Segmentation (VOS) is an active research area of the visual\ndomain. One of its fundamental sub-tasks is semi-supervised / one-shot\nlearning: given only the segmentation mask for the first frame, the task is to\nprovide pixel-accurate masks for the object over the rest of the sequence.\nDespite much progress in the last years, we noticed that many of the existing\napproaches lose objects in longer sequences, especially when the object is\nsmall or briefly occluded. In this work, we build upon a sequence-to-sequence\napproach that employs an encoder-decoder architecture together with a memory\nmodule for exploiting the sequential data. We further improve this approach by\nproposing a model that manipulates multi-scale spatio-temporal information\nusing memory-equipped skip connections. Furthermore, we incorporate an\nauxiliary task based on distance classification which greatly enhances the\nquality of edges in segmentation masks. We compare our approach to the state of\nthe art and show considerable improvement in the contour accuracy metric and\nthe overall segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 15:38:09 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Azimi", "Fatemeh", ""], ["Bischke", "Benjamin", ""], ["Palacio", "Sebastian", ""], ["Raue", "Federico", ""], ["Hees", "Joern", ""], ["Dengel", "Andreas", ""]]}, {"id": "2004.12178", "submitter": "Xinchi Zhou", "authors": "Dongzhan Zhou, Xinchi Zhou, Hongwen Zhang, Shuai Yi, Wanli Ouyang", "title": "Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general and efficient pre-training paradigm,\nMontage pre-training, for object detection. Montage pre-training needs only the\ntarget detection dataset while taking only 1/4 computational resources compared\nto the widely adopted ImageNet pre-training.To build such an efficient\nparadigm, we reduce the potential redundancy by carefully extracting useful\nsamples from the original images, assembling samples in a Montage manner as\ninput, and using an ERF-adaptive dense classification strategy for model\npre-training. These designs include not only a new input pattern to improve the\nspatial utilization but also a novel learning objective to expand the effective\nreceptive field of the pretrained model. The efficiency and effectiveness of\nMontage pre-training are validated by extensive experiments on the MS-COCO\ndataset, where the results indicate that the models using Montage pre-training\nare able to achieve on-par or even better detection performances compared with\nthe ImageNet pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:09:46 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 09:14:45 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhou", "Dongzhan", ""], ["Zhou", "Xinchi", ""], ["Zhang", "Hongwen", ""], ["Yi", "Shuai", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2004.12186", "submitter": "Daniel Groos", "authors": "Daniel Groos, Heri Ramampiaro, Espen A. F. Ihlen", "title": "EfficientPose: Scalable single-person pose estimation", "comments": "Published in Applied Intelligence Journal (APIN)", "journal-ref": "Applied Intelligence 51 (2021) 2518-2533", "doi": "10.1007/s10489-020-01918-7", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single-person human pose estimation facilitates markerless movement analysis\nin sports, as well as in clinical applications. Still, state-of-the-art models\nfor human pose estimation generally do not meet the requirements of real-life\napplications. The proliferation of deep learning techniques has resulted in the\ndevelopment of many advanced approaches. However, with the progresses in the\nfield, more complex and inefficient models have also been introduced, which\nhave caused tremendous increases in computational demands. To cope with these\ncomplexity and inefficiency challenges, we propose a novel convolutional neural\nnetwork architecture, called EfficientPose, which exploits recently proposed\nEfficientNets in order to deliver efficient and scalable single-person pose\nestimation. EfficientPose is a family of models harnessing an effective\nmulti-scale feature extractor and computationally efficient detection blocks\nusing mobile inverted bottleneck convolutions, while at the same time ensuring\nthat the precision of the pose configurations is still improved. Due to its low\ncomplexity and efficiency, EfficientPose enables real-world applications on\nedge devices by limiting the memory footprint and computational cost. The\nresults from our experiments, using the challenging MPII single-person\nbenchmark, show that the proposed EfficientPose models substantially outperform\nthe widely-used OpenPose model both in terms of accuracy and computational\nefficiency. In particular, our top-performing model achieves state-of-the-art\naccuracy on single-person MPII, with low-complexity ConvNets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:50:46 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 09:27:44 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Groos", "Daniel", ""], ["Ramampiaro", "Heri", ""], ["Ihlen", "Espen A. F.", ""]]}, {"id": "2004.12197", "submitter": "Amine Kechaou", "authors": "Amine Kechaou, Manuel Martinez, Monica Haurilet and Rainer\n  Stiefelhagen", "title": "Detective: An Attentive Recurrent Model for Sparse Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present Detective - an attentive object detector that\nidentifies objects in images in a sequential manner. Our network is based on an\nencoder-decoder architecture, where the encoder is a convolutional neural\nnetwork, and the decoder is a convolutional recurrent neural network coupled\nwith an attention mechanism. At each iteration, our decoder focuses on the\nrelevant parts of the image using an attention mechanism, and then estimates\nthe object's class and the bounding box coordinates. Current object detection\nmodels generate dense predictions and rely on post-processing to remove\nduplicate predictions. Detective is a sparse object detector that generates a\nsingle bounding box per object instance. However, training a sparse object\ndetector is challenging, as it requires the model to reason at the instance\nlevel and not just at the class and spatial levels. We propose a training\nmechanism based on the Hungarian algorithm and a loss that balances the\nlocalization and classification tasks. This allows Detective to achieve\npromising results on the PASCAL VOC object detection dataset. Our experiments\ndemonstrate that sparse object detection is possible and has a great potential\nfor future developments in applications where the order of the objects to be\npredicted is of interest.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:41:52 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kechaou", "Amine", ""], ["Martinez", "Manuel", ""], ["Haurilet", "Monica", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2004.12204", "submitter": "Eduardo Nigri", "authors": "Eduardo Nigri, Nivio Ziviani, Fabio Cappabianco, Augusto Antunes,\n  Adriano Veloso", "title": "Explainable Deep CNNs for MRI-Based Diagnosis of Alzheimer's Disease", "comments": "Accepted for the IEEE International Joint Conference on Neural\n  Networks (IJCNN) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are becoming prominent models for\nsemi-automated diagnosis of Alzheimer's Disease (AD) using brain Magnetic\nResonance Imaging (MRI). Although being highly accurate, deep CNN models lack\ntransparency and interpretability, precluding adequate clinical reasoning and\nnot complying with most current regulatory demands. One popular choice for\nexplaining deep image models is occluding regions of the image to isolate their\ninfluence on the prediction. However, existing methods for occluding patches of\nbrain scans generate images outside the distribution to which the model was\ntrained for, thus leading to unreliable explanations. In this paper, we propose\nan alternative explanation method that is specifically designed for the brain\nscan task. Our method, which we refer to as Swap Test, produces heatmaps that\ndepict the areas of the brain that are most indicative of AD, providing\ninterpretability for the model's decisions in a format understandable to\nclinicians. Experimental results using an axiomatic evaluation show that the\nproposed method is more suitable for explaining the diagnosis of AD using MRI\nwhile the opposite trend was observed when using a typical occlusion test.\nTherefore, we believe our method may address the inherent black-box nature of\ndeep neural networks that are capable of diagnosing AD.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 18:14:49 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Nigri", "Eduardo", ""], ["Ziviani", "Nivio", ""], ["Cappabianco", "Fabio", ""], ["Antunes", "Augusto", ""], ["Veloso", "Adriano", ""]]}, {"id": "2004.12231", "submitter": "Huayu Li", "authors": "Huayu Li, Xiwen Chen, Haiyu Wu, Zaoyi Chi, Christopher Mann, and\n  Abolfazl Razi", "title": "Deep DIH : Statistically Inferred Reconstruction of Digital In-Line\n  Holography by Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital in-line holography is commonly used to reconstruct 3D images from 2D\nholograms for microscopic objects. One of the technical challenges that arise\nin the signal processing stage is removing the twin image that is caused by the\nphase-conjugate wavefront from the recorded holograms. Twin image removal is\ntypically formulated as a non-linear inverse problem due to the irreversible\nscattering process when generating the hologram. Recently, end-to-end deep\nlearning-based methods have been utilized to reconstruct the object wavefront\n(as a surrogate for the 3D structure of the object) directly from a single-shot\nin-line digital hologram. However, massive data pairs are required to train\ndeep learning models for acceptable reconstruction precision. In contrast to\ntypical image processing problems, well-curated datasets for in-line digital\nholography does not exist. Also, the trained model highly influenced by the\nmorphological properties of the object and hence can vary for different\napplications. Therefore, data collection can be prohibitively cumbersome in\npractice as a major hindrance to using deep learning for digital holography. In\nthis paper, we proposed a novel implementation of autoencoder-based deep\nlearning architecture for single-shot hologram reconstruction solely based on\nthe current sample without the need for massive datasets to train the model.\nThe simulations results demonstrate the superior performance of the proposed\nmethod compared to the state of the art single-shot compressive digital in-line\nhologram reconstruction method.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 20:39:25 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 22:08:02 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Li", "Huayu", ""], ["Chen", "Xiwen", ""], ["Wu", "Haiyu", ""], ["Chi", "Zaoyi", ""], ["Mann", "Christopher", ""], ["Razi", "Abolfazl", ""]]}, {"id": "2004.12232", "submitter": "Aditya Aggarwal", "authors": "Aniket Pokale, Aditya Aggarwal, K. Madhava Krishna", "title": "Reconstruct, Rasterize and Backprop: Dense shape and pose estimation\n  from a single image", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new system to obtain dense object reconstructions along\nwith 6-DoF poses from a single image. Geared towards high fidelity\nreconstruction, several recent approaches leverage implicit surface\nrepresentations and deep neural networks to estimate a 3D mesh of an object,\ngiven a single image. However, all such approaches recover only the shape of an\nobject; the reconstruction is often in a canonical frame, unsuitable for\ndownstream robotics tasks. To this end, we leverage recent advances in\ndifferentiable rendering (in particular, rasterization) to close the loop with\n3D reconstruction in camera frame. We demonstrate that our approach---dubbed\nreconstruct, rasterize and backprop (RRB) achieves significantly lower pose\nestimation errors compared to prior art, and is able to recover dense object\nshapes and poses from imagery. We further extend our results to an (offline)\nsetup, where we demonstrate a dense monocular object-centric egomotion\nestimation system.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 20:53:43 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Pokale", "Aniket", ""], ["Aggarwal", "Aditya", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2004.12246", "submitter": "Hangxin Liu", "authors": "Zeyu Zhang, Hangxin Liu, Ziyuan Jiao, Yixin Zhu, Song-Chun Zhu", "title": "Congestion-aware Evacuation Routing using Augmented Reality Devices", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a congestion-aware routing solution for indoor evacuation, which\nproduces real-time individual-customized evacuation routes among multiple\ndestinations while keeping tracks of all evacuees' locations. A population\ndensity map, obtained on-the-fly by aggregating locations of evacuees from\nuser-end Augmented Reality (AR) devices, is used to model the congestion\ndistribution inside a building. To efficiently search the evacuation route\namong all destinations, a variant of A* algorithm is devised to obtain the\noptimal solution in a single pass. In a series of simulated studies, we show\nthat the proposed algorithm is more computationally optimized compared to\nclassic path planning algorithms; it generates a more time-efficient evacuation\nroute for each individual that minimizes the overall congestion. A complete\nsystem using AR devices is implemented for a pilot study in real-world\nenvironments, demonstrating the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 22:54:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhang", "Zeyu", ""], ["Liu", "Hangxin", ""], ["Jiao", "Ziyuan", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2004.12248", "submitter": "Hangxin Liu", "authors": "Tao Yuan, Hangxin Liu, Lifeng Fan, Zilong Zheng, Tao Gao, Yixin Zhu,\n  Song-Chun Zhu", "title": "Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to understand how human (false-)belief--a core socio-cognitive\nability--would affect human interactions with robots, this paper proposes to\nadopt a graphical model to unify the representation of object states, robot\nknowledge, and human (false-)beliefs. Specifically, a parse graph (pg) is\nlearned from a single-view spatiotemporal parsing by aggregating various object\nstates along the time; such a learned representation is accumulated as the\nrobot's knowledge. An inference algorithm is derived to fuse individual pg from\nall robots across multi-views into a joint pg, which affords more effective\nreasoning and inference capability to overcome the errors originated from a\nsingle view. In the experiments, through the joint inference over pg-s, the\nsystem correctly recognizes human (false-)belief in various settings and\nachieves better cross-view accuracy on a challenging small object tracking\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 23:02:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yuan", "Tao", ""], ["Liu", "Hangxin", ""], ["Fan", "Lifeng", ""], ["Zheng", "Zilong", ""], ["Gao", "Tao", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2004.12255", "submitter": "Liangji Fang", "authors": "Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou", "title": "TPNet: Trajectory Proposal Network for Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making accurate motion prediction of the surrounding traffic agents such as\npedestrians, vehicles, and cyclists is crucial for autonomous driving. Recent\ndata-driven motion prediction methods have attempted to learn to directly\nregress the exact future position or its distribution from massive amount of\ntrajectory data. However, it remains difficult for these methods to provide\nmultimodal predictions as well as integrate physical constraints such as\ntraffic rules and movable areas. In this work we propose a novel two-stage\nmotion prediction framework, Trajectory Proposal Network (TPNet). TPNet first\ngenerates a candidate set of future trajectories as hypothesis proposals, then\nmakes the final predictions by classifying and refining the proposals which\nmeets the physical constraints. By steering the proposal generation process,\nsafe and multimodal predictions are realized. Thus this framework effectively\nmitigates the complexity of motion prediction problem while ensuring the\nmultimodal output. Experiments on four large-scale trajectory prediction\ndatasets, i.e. the ETH, UCY, Apollo and Argoverse datasets, show that TPNet\nachieves the state-of-the-art results both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 00:01:49 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 08:04:58 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Fang", "Liangji", ""], ["Jiang", "Qinhong", ""], ["Shi", "Jianping", ""], ["Zhou", "Bolei", ""]]}, {"id": "2004.12260", "submitter": "Charles Herrmann", "authors": "Charles Herrmann, Richard Strong Bowen, Neal Wadhwa, Rahul Garg,\n  Qiurui He, Jonathan T. Barron, Ramin Zabih", "title": "Learning to Autofocus", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autofocus is an important task for digital cameras, yet current approaches\noften exhibit poor performance. We propose a learning-based approach to this\nproblem, and provide a realistic dataset of sufficient size for effective\nlearning. Our dataset is labeled with per-pixel depths obtained from multi-view\nstereo, following \"Learning single camera depth estimation using dual-pixels\".\nUsing this dataset, we apply modern deep classification models and an ordinal\nregression loss to obtain an efficient learning-based autofocus technique. We\ndemonstrate that our approach provides a significant improvement compared with\nprevious learned and non-learned methods: our model reduces the mean absolute\nerror by a factor of 3.6 over the best comparable baseline algorithm. Our\ndataset and code are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 00:39:21 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 14:11:22 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 23:23:12 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Herrmann", "Charles", ""], ["Bowen", "Richard Strong", ""], ["Wadhwa", "Neal", ""], ["Garg", "Rahul", ""], ["He", "Qiurui", ""], ["Barron", "Jonathan T.", ""], ["Zabih", "Ramin", ""]]}, {"id": "2004.12270", "submitter": "James Allworth Mr", "authors": "James Allworth, Lloyd Windrim, Jeffrey Wardman, Daniel Kucharski,\n  James Bennett, Mitch Bryson", "title": "Development of a High Fidelity Simulator for Generalised Photometric\n  Based Space Object Classification using Machine Learning", "comments": "This paper is a pre-print that appeared in Proceedings of 70th\n  International Astronautical Congress (IAC), 2019", "journal-ref": "Proceedings of the 70th International Astronautical Congress, 2019", "doi": null, "report-no": null, "categories": "physics.space-ph astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the initial stages in the development of a deep learning\nclassifier for generalised Resident Space Object (RSO) characterisation that\ncombines high-fidelity simulated light curves with transfer learning to improve\nthe performance of object characterisation models that are trained on real\ndata. The classification and characterisation of RSOs is a significant goal in\nSpace Situational Awareness (SSA) in order to improve the accuracy of orbital\npredictions. The specific focus of this paper is the development of a\nhigh-fidelity simulation environment for generating realistic light curves. The\nsimulator takes in a textured geometric model of an RSO as well as the objects\nephemeris and uses Blender to generate photo-realistic images of the RSO that\nare then processed to extract the light curve. Simulated light curves have been\ncompared with real light curves extracted from telescope imagery to provide\nvalidation for the simulation environment. Future work will involve further\nvalidation and the use of the simulator to generate a dataset of realistic\nlight curves for the purpose of training neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 02:10:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Allworth", "James", ""], ["Windrim", "Lloyd", ""], ["Wardman", "Jeffrey", ""], ["Kucharski", "Daniel", ""], ["Bennett", "James", ""], ["Bryson", "Mitch", ""]]}, {"id": "2004.12274", "submitter": "Baoyu Jing", "authors": "Baoyu Jing, Zeya Wang, Eric Xing", "title": "Show, Describe and Conclude: On Exploiting the Structure Information of\n  Chest X-Ray Reports", "comments": "ACL 2019", "journal-ref": null, "doi": "10.18653/v1/P19-1657", "report-no": null, "categories": "cs.CL cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest X-Ray (CXR) images are commonly used for clinical screening and\ndiagnosis. Automatically writing reports for these images can considerably\nlighten the workload of radiologists for summarizing descriptive findings and\nconclusive impressions. The complex structures between and within sections of\nthe reports pose a great challenge to the automatic report generation.\nSpecifically, the section Impression is a diagnostic summarization over the\nsection Findings; and the appearance of normality dominates each section over\nthat of abnormality. Existing studies rarely explore and consider this\nfundamental structure information. In this work, we propose a novel framework\nthat exploits the structure information between and within report sections for\ngenerating CXR imaging reports. First, we propose a two-stage strategy that\nexplicitly models the relationship between Findings and Impression. Second, we\ndesign a novel cooperative multi-agent system that implicitly captures the\nimbalanced distribution between abnormality and normality. Experiments on two\nCXR report datasets show that our method achieves state-of-the-art performance\nin terms of various evaluation metrics. Our results expose that the proposed\napproach is able to generate high-quality medical reports through integrating\nthe structure information.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 02:29:20 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 17:44:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Jing", "Baoyu", ""], ["Wang", "Zeya", ""], ["Xing", "Eric", ""]]}, {"id": "2004.12276", "submitter": "Menglin Jia", "authors": "Menglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui, Claire Cardie,\n  Bharath Hariharan, Hartwig Adam, Serge Belongie", "title": "Fashionpedia: Ontology, Segmentation, and an Attribute Localization\n  Dataset", "comments": "eccv2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the task of instance segmentation with attribute\nlocalization, which unifies instance segmentation (detect and segment each\nobject instance) and fine-grained visual attribute categorization (recognize\none or multiple attributes). The proposed task requires both localizing an\nobject and describing its properties. To illustrate the various aspects of this\ntask, we focus on the domain of fashion and introduce Fashionpedia as a step\ntoward mapping out the visual aspects of the fashion world. Fashionpedia\nconsists of two parts: (1) an ontology built by fashion experts containing 27\nmain apparel categories, 19 apparel parts, 294 fine-grained attributes and\ntheir relationships; (2) a dataset with everyday and celebrity event fashion\nimages annotated with segmentation masks and their associated per-mask\nfine-grained attributes, built upon the Fashionpedia ontology. In order to\nsolve this challenging task, we propose a novel Attribute-Mask RCNN model to\njointly perform instance segmentation and localized attribute recognition, and\nprovide a novel evaluation metric for the task. We also demonstrate instance\nsegmentation models pre-trained on Fashionpedia achieve better transfer\nlearning performance on other fashion datasets than ImageNet pre-training.\nFashionpedia is available at: https://fashionpedia.github.io/home/index.html.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 02:38:26 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 21:02:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jia", "Menglin", ""], ["Shi", "Mengyun", ""], ["Sirotenko", "Mikhail", ""], ["Cui", "Yin", ""], ["Cardie", "Claire", ""], ["Hariharan", "Bharath", ""], ["Adam", "Hartwig", ""], ["Belongie", "Serge", ""]]}, {"id": "2004.12277", "submitter": "Sheng Shi", "authors": "Sheng Shi, Yangzhou Du and Wei Fan", "title": "An Extension of LIME with Improvement of Interpretability and Fidelity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning makes significant achievements in Artificial Intelligence\n(AI), the lack of transparency has limited its broad application in various\nvertical domains. Explainability is not only a gateway between AI and real\nworld, but also a powerful feature to detect flaw of the models and bias of the\ndata. Local Interpretable Model-agnostic Explanation (LIME) is a\nwidely-accepted technique that explains the prediction of any classifier\nfaithfully by learning an interpretable model locally around the predicted\ninstance. As an extension of LIME, this paper proposes an high-interpretability\nand high-fidelity local explanation method, known as Local Explanation using\nfeature Dependency Sampling and Nonlinear Approximation (LEDSNA). Given an\ninstance being explained, LEDSNA enhances interpretability by feature sampling\nwith intrinsic dependency. Besides, LEDSNA improves the local explanation\nfidelity by approximating nonlinear boundary of local decision. We evaluate our\nmethod with classification tasks in both image domain and text domain.\nExperiments show that LEDSNA's explanation of the back-box model achieves much\nbetter performance than original LIME in terms of interpretability and\nfidelity.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 02:54:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Shi", "Sheng", ""], ["Du", "Yangzhou", ""], ["Fan", "Wei", ""]]}, {"id": "2004.12288", "submitter": "Aji Resindra Widya", "authors": "Aji Resindra Widya, Yusuke Monno, Masatoshi Okutomi, Sho Suzuki,\n  Takuji Gotoda, Kenji Miki", "title": "Stomach 3D Reconstruction Based on Virtual Chromoendoscopic Image\n  Generation", "comments": "Accepted for main conference in EMBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastric endoscopy is a standard clinical process that enables medical\npractitioners to diagnose various lesions inside a patient's stomach. If any\nlesion is found, it is very important to perceive the location of the lesion\nrelative to the global view of the stomach. Our previous research showed that\nthis could be addressed by reconstructing the whole stomach shape from\nchromoendoscopic images using a structure-from-motion (SfM) pipeline, in which\nindigo carmine (IC) blue dye sprayed images were used to increase feature\nmatches for SfM by enhancing stomach surface's textures. However, spraying the\nIC dye to the whole stomach requires additional time, labor, and cost, which is\nnot desirable for patients and practitioners. In this paper, we propose an\nalternative way to achieve whole stomach 3D reconstruction without the need of\nthe IC dye by generating virtual IC-sprayed (VIC) images based on\nimage-to-image style translation trained on unpaired real no-IC and IC-sprayed\nimages. We have specifically investigated the effect of input and output color\nchannel selection for generating the VIC images and found that translating\nno-IC green-channel images to IC-sprayed red-channel images gives the best SfM\nreconstruction result.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 04:55:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Widya", "Aji Resindra", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""], ["Suzuki", "Sho", ""], ["Gotoda", "Takuji", ""], ["Miki", "Kenji", ""]]}, {"id": "2004.12292", "submitter": "Zitong Yu", "authors": "Zitong Yu, Xiaobai Li, Xuesong Niu, Jingang Shi, Guoying Zhao", "title": "AutoHR: A Strong End-to-end Baseline for Remote Heart Rate Measurement\n  with Neural Searching", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.3007086", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photoplethysmography (rPPG), which aims at measuring heart activities\nwithout any contact, has great potential in many applications (e.g., remote\nhealthcare). Existing end-to-end rPPG and heart rate (HR) measurement methods\nfrom facial videos are vulnerable to the less-constrained scenarios (e.g., with\nhead movement and bad illumination). In this letter, we explore the reason why\nexisting end-to-end networks perform poorly in challenging conditions and\nestablish a strong end-to-end baseline (AutoHR) for remote HR measurement with\nneural architecture search (NAS). The proposed method includes three parts: 1)\na powerful searched backbone with novel Temporal Difference Convolution (TDC),\nintending to capture intrinsic rPPG-aware clues between frames; 2) a hybrid\nloss function considering constraints from both time and frequency domains; and\n3) spatio-temporal data augmentation strategies for better representation\nlearning. Comprehensive experiments are performed on three benchmark datasets\nto show our superior performance on both intra- and cross-dataset testing.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 05:43:21 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Yu", "Zitong", ""], ["Li", "Xiaobai", ""], ["Niu", "Xuesong", ""], ["Shi", "Jingang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2004.12314", "submitter": "Zhaohan Xiong", "authors": "Zhaohan Xiong, Qing Xia, Zhiqiang Hu, Ning Huang, Cheng Bian, Yefeng\n  Zheng, Sulaiman Vesal, Nishant Ravikumar, Andreas Maier, Xin Yang, Pheng-Ann\n  Heng, Dong Ni, Caizi Li, Qianqian Tong, Weixin Si, Elodie Puybareau, Younes\n  Khoudli, Thierry Geraud, Chen Chen, Wenjia Bai, Daniel Rueckert, Lingchao Xu,\n  Xiahai Zhuang, Xinzhe Luo, Shuman Jia, Maxime Sermesant, Yashu Liu, Kuanquan\n  Wang, Davide Borra, Alessandro Masci, Cristiana Corsi, Coen de Vente, Mitko\n  Veta, Rashed Karim, Chandrakanth Jayachandran Preetha, Sandy Engelhardt,\n  Menyun Qiao, Yuanyuan Wang, Qian Tao, Marta Nunez-Garcia, Oscar Camara,\n  Nicolo Savioli, Pablo Lamata, Jichao Zhao", "title": "A Global Benchmark of Algorithms for Segmenting Late Gadolinium-Enhanced\n  Cardiac Magnetic Resonance Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of cardiac images, particularly late gadolinium-enhanced\nmagnetic resonance imaging (LGE-MRI) widely used for visualizing diseased\ncardiac structures, is a crucial first step for clinical diagnosis and\ntreatment. However, direct segmentation of LGE-MRIs is challenging due to its\nattenuated contrast. Since most clinical studies have relied on manual and\nlabor-intensive approaches, automatic methods are of high interest,\nparticularly optimized machine learning approaches. To address this, we\norganized the \"2018 Left Atrium Segmentation Challenge\" using 154 3D LGE-MRIs,\ncurrently the world's largest cardiac LGE-MRI dataset, and associated labels of\nthe left atrium segmented by three medical experts, ultimately attracting the\nparticipation of 27 international teams. In this paper, extensive analysis of\nthe submitted algorithms using technical and biological metrics was performed\nby undergoing subgroup analysis and conducting hyper-parameter analysis,\noffering an overall picture of the major design choices of convolutional neural\nnetworks (CNNs) and practical considerations for achieving state-of-the-art\nleft atrium segmentation. Results show the top method achieved a dice score of\n93.2% and a mean surface to a surface distance of 0.7 mm, significantly\noutperforming prior state-of-the-art. Particularly, our analysis demonstrated\nthat double, sequentially used CNNs, in which a first CNN is used for automatic\nregion-of-interest localization and a subsequent CNN is used for refined\nregional segmentation, achieved far superior results than traditional methods\nand pipelines containing single CNNs. This large-scale benchmarking study makes\na significant step towards much-improved segmentation methods for cardiac\nLGE-MRIs, and will serve as an important benchmark for evaluating and comparing\nthe future works in the field.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:49:17 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 09:54:48 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 14:05:14 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Xiong", "Zhaohan", ""], ["Xia", "Qing", ""], ["Hu", "Zhiqiang", ""], ["Huang", "Ning", ""], ["Bian", "Cheng", ""], ["Zheng", "Yefeng", ""], ["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""], ["Yang", "Xin", ""], ["Heng", "Pheng-Ann", ""], ["Ni", "Dong", ""], ["Li", "Caizi", ""], ["Tong", "Qianqian", ""], ["Si", "Weixin", ""], ["Puybareau", "Elodie", ""], ["Khoudli", "Younes", ""], ["Geraud", "Thierry", ""], ["Chen", "Chen", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""], ["Xu", "Lingchao", ""], ["Zhuang", "Xiahai", ""], ["Luo", "Xinzhe", ""], ["Jia", "Shuman", ""], ["Sermesant", "Maxime", ""], ["Liu", "Yashu", ""], ["Wang", "Kuanquan", ""], ["Borra", "Davide", ""], ["Masci", "Alessandro", ""], ["Corsi", "Cristiana", ""], ["de Vente", "Coen", ""], ["Veta", "Mitko", ""], ["Karim", "Rashed", ""], ["Preetha", "Chandrakanth Jayachandran", ""], ["Engelhardt", "Sandy", ""], ["Qiao", "Menyun", ""], ["Wang", "Yuanyuan", ""], ["Tao", "Qian", ""], ["Nunez-Garcia", "Marta", ""], ["Camara", "Oscar", ""], ["Savioli", "Nicolo", ""], ["Lamata", "Pablo", ""], ["Zhao", "Jichao", ""]]}, {"id": "2004.12333", "submitter": "Ramy Ashraf Zeineldin", "authors": "Ramy A. Zeineldin, Mohamed E. Karar, Jan Coburger, Christian R. Wirtz,\n  Oliver Burgert", "title": "DeepSeg: Deep Neural Network Framework for Automatic Brain Tumor\n  Segmentation using Magnetic Resonance FLAIR Images", "comments": "Accepted to International Journal of Computer Assisted Radiology and\n  Surgery", "journal-ref": null, "doi": "10.1007/s11548-020-02186-z", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Gliomas are the most common and aggressive type of brain tumors due\nto their infiltrative nature and rapid progression. The process of\ndistinguishing tumor boundaries from healthy cells is still a challenging task\nin the clinical routine. Fluid-Attenuated Inversion Recovery (FLAIR) MRI\nmodality can provide the physician with information about tumor infiltration.\nTherefore, this paper proposes a new generic deep learning architecture; namely\nDeepSeg for fully automated detection and segmentation of the brain lesion\nusing FLAIR MRI data.\n  Methods: The developed DeepSeg is a modular decoupling framework. It consists\nof two connected core parts based on an encoding and decoding relationship. The\nencoder part is a convolutional neural network (CNN) responsible for spatial\ninformation extraction. The resulting semantic map is inserted into the decoder\npart to get the full resolution probability map. Based on modified U-Net\narchitecture, different CNN models such as Residual Neural Network (ResNet),\nDense Convolutional Network (DenseNet), and NASNet have been utilized in this\nstudy.\n  Results: The proposed deep learning architectures have been successfully\ntested and evaluated on-line based on MRI datasets of Brain Tumor Segmentation\n(BraTS 2019) challenge, including s336 cases as training data and 125 cases for\nvalidation data. The dice and Hausdorff distance scores of obtained\nsegmentation results are about 0.81 to 0.84 and 9.8 to 19.7 correspondingly.\n  Conclusion: This study showed successful feasibility and comparative\nperformance of applying different deep learning models in a new DeepSeg\nframework for automated brain tumor segmentation in FLAIR MR images. The\nproposed DeepSeg is open-source and freely available at\nhttps://github.com/razeineldin/DeepSeg/.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:50:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zeineldin", "Ramy A.", ""], ["Karar", "Mohamed E.", ""], ["Coburger", "Jan", ""], ["Wirtz", "Christian R.", ""], ["Burgert", "Oliver", ""]]}, {"id": "2004.12337", "submitter": "Jaros{\\l}aw Miszczak", "authors": "Mateusz \\.Zarski, Bartosz W\\'ojcik, Jaros{\\l}aw Adam Miszczak", "title": "KrakN: Transfer Learning framework for thin crack detection in\n  infrastructure maintenance", "comments": "23 pages, 15 figures and flowcharts, software available at\n  https://github.com/MatZar01/KrakN, https://doi.org/10.5281/zenodo.3764697,\n  and https://doi.org/10.5281/zenodo.3755452, dataset available from\n  https://doi.org/10.5281/zenodo.3759845", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monitoring the technical condition of infrastructure is a crucial element to\nits maintenance. Currently applied methods are outdated, labour-intensive and\ninaccurate. At the same time, the latest methods using Artificial Intelligence\ntechniques are severely limited in their application due to two main factors --\nlabour-intensive gathering of new datasets and high demand for computing power.\nWe propose to utilize custom made framework -- KrakN, to overcome these\nlimiting factors. It enables the development of unique infrastructure defects\ndetectors on digital images, achieving the accuracy of above 90%. The framework\nsupports semi-automatic creation of new datasets and has modest computing power\nrequirements. It is implemented in the form of a ready-to-use software package\nopenly distributed to the public. Thus, it can be used to immediately implement\nthe methods proposed in this paper in the process of infrastructure management\nby government units, regardless of their financial capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:57:36 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 17:15:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["\u017barski", "Mateusz", ""], ["W\u00f3jcik", "Bartosz", ""], ["Miszczak", "Jaros\u0142aw Adam", ""]]}, {"id": "2004.12344", "submitter": "Prabhu Pradhan", "authors": "Ruchit Rawal, Prabhu Pradhan", "title": "Climate Adaptation: Reliably Predicting from Imbalanced Satellite Data", "comments": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  Workshops: Agriculture-Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The utility of aerial imagery (Satellite, Drones) has become an invaluable\ninformation source for cross-disciplinary applications, especially for crisis\nmanagement. Most of the mapping and tracking efforts are manual which is\nresource-intensive and often lead to delivery delays. Deep Learning methods\nhave boosted the capacity of relief efforts via recognition, detection, and are\nnow being used for non-trivial applications. However the data commonly\navailable is highly imbalanced (similar to other real-life applications) which\nseverely hampers the neural network's capabilities, this reduces robustness and\ntrust. We give an overview on different kinds of techniques being used for\nhandling such extreme settings and present solutions aimed at maximizing\nperformance on minority classes using a diverse set of methods (ranging from\narchitectural tuning to augmentation) which as a combination generalizes for\nall minority classes. We hope to amplify cross-disciplinary efforts by\nenhancing model reliability.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 10:41:08 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rawal", "Ruchit", ""], ["Pradhan", "Prabhu", ""]]}, {"id": "2004.12349", "submitter": "Nevrez Imamoglu", "authors": "Ali Caglayan and Nevrez Imamoglu and Ahmet Burak Can and Ryosuke\n  Nakamura", "title": "When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D\n  Object and Scene Recognition", "comments": "16 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing objects and scenes are two challenging but essential tasks in\nimage understanding. In particular, the use of RGB-D sensors in handling these\ntasks has emerged as an important area of focus for better visual\nunderstanding. Meanwhile, deep neural networks, specifically convolutional\nneural networks (CNNs), have become widespread and have been applied to many\nvisual tasks by replacing hand-crafted features with effective deep features.\nHowever, it is an open problem how to exploit deep features from a multi-layer\nCNN model effectively. In this paper, we propose a novel two-stage framework\nthat extracts discriminative feature representations from multi-modal RGB-D\nimages for object and scene recognition tasks. In the first stage, a pretrained\nCNN model has been employed as a backbone to extract visual features at\nmultiple levels. The second stage maps these features into high level\nrepresentations with a fully randomized structure of recursive neural networks\n(RNNs) efficiently. In order to cope with the high dimensionality of CNN\nactivations, a random weighted pooling scheme has been proposed by extending\nthe idea of randomness in RNNs. Multi-modal fusion has been performed through a\nsoft voting approach by computing weights based on individual recognition\nconfidences (i.e. SVM scores) of RGB and depth streams separately. This\nproduces consistent class label estimation in final RGB-D classification\nperformance. Extensive experiments verify that fully randomized structure in\nRNN stage encodes CNN activations to discriminative solid features\nsuccessfully. Comparative experimental results on the popular Washington RGB-D\nObject and SUN RGB-D Scene datasets show that the proposed approach\nsignificantly outperforms state-of-the-art methods both in object and scene\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 10:58:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Caglayan", "Ali", ""], ["Imamoglu", "Nevrez", ""], ["Can", "Ahmet Burak", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "2004.12352", "submitter": "Michal Heker", "authors": "Michal Heker and Hayit Greenspan", "title": "Joint Liver Lesion Segmentation and Classification via Transfer Learning", "comments": "Accepted to MIDL 2020", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/8gSjgXg5U", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning and joint learning approaches are extensively used to\nimprove the performance of Convolutional Neural Networks (CNNs). In medical\nimaging applications in which the target dataset is typically very small,\ntransfer learning improves feature learning while joint learning has shown\neffectiveness in improving the network's generalization and robustness. In this\nwork, we study the combination of these two approaches for the problem of liver\nlesion segmentation and classification. For this purpose, 332 abdominal CT\nslices containing lesion segmentation and classification of three lesion types\nare evaluated. For feature learning, the dataset of MICCAI 2017 Liver Tumor\nSegmentation (LiTS) Challenge is used. Joint learning shows improvement in both\nsegmentation and classification results. We show that a simple joint framework\noutperforms the commonly used multi-task architecture (Y-Net), achieving an\nimprovement of 10% in classification accuracy, compared to a 3% improvement\nwith Y-Net.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 11:06:23 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 16:08:17 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Heker", "Michal", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2004.12361", "submitter": "Yaniv Benny", "authors": "Yaniv Benny, Tomer Galanti, Sagie Benaim, Lior Wolf", "title": "Evaluation Metrics for Conditional Image Generation", "comments": "To be published in \"INTERNATIONAL JOURNAL OF COMPUTER VISION\"", "journal-ref": null, "doi": "10.1007/s11263-020-01424-w", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new metrics for evaluating generative models in the\nclass-conditional image generation setting. These metrics are obtained by\ngeneralizing the two most popular unconditional metrics: the Inception Score\n(IS) and the Fre'chet Inception Distance (FID). A theoretical analysis shows\nthe motivation behind each proposed metric and links the novel metrics to their\nunconditional counterparts. The link takes the form of a product in the case of\nIS or an upper bound in the FID case. We provide an extensive empirical\nevaluation, comparing the metrics to their unconditional variants and to other\nmetrics, and utilize them to analyze existing generative models, thus providing\nadditional insights about their performance, from unlearned classes to mode\ncollapse.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:15:16 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:36:48 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Benny", "Yaniv", ""], ["Galanti", "Tomer", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "2004.12381", "submitter": "Xiangdong Zhang", "authors": "Xiangdong Zhang, Tengjun Wang, Yun Yang", "title": "Hyperspectral Images Classification Based on Multi-scale Residual\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because hyperspectral remote sensing images contain a lot of redundant\ninformation and the data structure is highly non-linear, leading to low\nclassification accuracy of traditional machine learning methods. The latest\nresearch shows that hyperspectral image classification based on deep\nconvolutional neural network has high accuracy. However, when a small amount of\ndata is used for training, the classification accuracy of deep learning methods\nis greatly reduced. In order to solve the problem of low classification\naccuracy of existing algorithms on small samples of hyperspectral images, a\nmulti-scale residual network is proposed. The multi-scale extraction and fusion\nof spatial and spectral features is realized by adding a branch structure into\nthe residual block and using convolution kernels of different sizes in the\nbranch. The spatial and spectral information contained in hyperspectral images\nare fully utilized to improve the classification accuracy. In addition, in\norder to improve the speed and prevent overfitting, the model uses dynamic\nlearning rate, BN and Dropout strategies. The experimental results show that\nthe overall classification accuracy of this method is 99.07% and 99.96%\nrespectively in the data set of Indian Pines and Pavia University, which is\nbetter than other algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 13:46:52 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 01:56:40 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Xiangdong", ""], ["Wang", "Tengjun", ""], ["Yang", "Yun", ""]]}, {"id": "2004.12385", "submitter": "Qiuling Xu", "authors": "Qiuling Xu, Guanhong Tao, Siyuan Cheng, Xiangyu Zhang", "title": "Towards Feature Space Adversarial Attack", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new adversarial attack to Deep Neural Networks for image\nclassification. Different from most existing attacks that directly perturb\ninput pixels, our attack focuses on perturbing abstract features, more\nspecifically, features that denote styles, including interpretable styles such\nas vivid colors and sharp outlines, and uninterpretable ones. It induces model\nmisclassfication by injecting imperceptible style changes through an\noptimization procedure. We show that our attack can generate adversarial\nsamples that are more natural-looking than the state-of-the-art unbounded\nattacks. The experiment also supports that existing pixel-space adversarial\nattack detection and defense techniques can hardly ensure robustness in the\nstyle related feature space.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 13:56:31 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 03:47:44 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Xu", "Qiuling", ""], ["Tao", "Guanhong", ""], ["Cheng", "Siyuan", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2004.12411", "submitter": "Yazeed Alharbi", "authors": "Yazeed Alharbi, Peter Wonka", "title": "Disentangled Image Generation Through Structured Noise Injection", "comments": "CVPR2020 Oral. Project page:\n  https://github.com/yalharbi/StructuredNoiseInjection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore different design choices for injecting noise into generative\nadversarial networks (GANs) with the goal of disentangling the latent space.\nInstead of traditional approaches, we propose feeding multiple noise codes\nthrough separate fully-connected layers respectively. The aim is restricting\nthe influence of each noise code to specific parts of the generated image. We\nshow that disentanglement in the first layer of the generator network leads to\ndisentanglement in the generated image. Through a grid-based structure, we\nachieve several aspects of disentanglement without complicating the network\narchitecture and without requiring labels. We achieve spatial disentanglement,\nscale-space disentanglement, and disentanglement of the foreground object from\nthe background style allowing fine-grained control over the generated images.\nExamples include changing facial expressions in face images, changing beak\nlength in bird images, and changing car dimensions in car images. This\nempirically leads to better disentanglement scores than state-of-the-art\nmethods on the FFHQ dataset.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 15:15:19 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 15:41:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Alharbi", "Yazeed", ""], ["Wonka", "Peter", ""]]}, {"id": "2004.12427", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Cross-Domain Structure Preserving Projection for Heterogeneous Domain\n  Adaptation", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Domain Adaptation (HDA) addresses the transfer learning\nproblems where data from the source and target domains are of different\nmodalities (e.g., texts and images) or feature dimensions (e.g., features\nextracted with different methods). It is useful for multi-modal data analysis.\nTraditional domain adaptation algorithms assume that the representations of\nsource and target samples reside in the same feature space, hence are likely to\nfail in solving the heterogeneous domain adaptation problem. Contemporary\nstate-of-the-art HDA approaches are usually composed of complex optimization\nobjectives for favourable performance and are therefore computationally\nexpensive and less generalizable. To address these issues, we propose a novel\nCross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an\nextension of the classic LPP to heterogeneous domains, CDSPP aims to learn\ndomain-specific projections to map sample features from source and target\ndomains into a common subspace such that the class consistency is preserved and\ndata distributions are sufficiently aligned. CDSPP is simple and has\ndeterministic solutions by solving a generalized eigenvalue problem. It is\nnaturally suitable for supervised HDA but has also been extended for\nsemi-supervised HDA where the unlabeled target domain samples are available.\nExtensive experiments have been conducted on commonly used benchmark datasets\n(i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for\nHDA as well as the Office-Home dataset firstly introduced for HDA by ourselves\ndue to its significantly larger number of classes than the existing ones (65 vs\n10, 6 and 8). The experimental results of both supervised and semi-supervised\nHDA demonstrate the superior performance of our proposed method against\ncontemporary state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 16:22:28 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 20:59:08 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2004.12432", "submitter": "Chen Yukang", "authors": "Yukang Chen, Peizhen Zhang, Zeming Li, Yanwei Li, Xiangyu Zhang, Lu\n  Qi, Jian Sun, and Jiaya Jia", "title": "Dynamic Scale Training for Object Detection", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Dynamic Scale Training paradigm (abbreviated as DST) to mitigate\nscale variation challenge in object detection. Previous strategies like image\npyramid, multi-scale training, and their variants are aiming at preparing\nscale-invariant data for model optimization. However, the preparation procedure\nis unaware of the following optimization process that restricts their\ncapability in handling the scale variation. Instead, in our paradigm, we use\nfeedback information from the optimization process to dynamically guide the\ndata preparation. The proposed method is surprisingly simple yet obtains\nsignificant gains (2%+ Average Precision on MS COCO dataset), outperforming\nprevious methods. Experimental results demonstrate the efficacy of our proposed\nDST method towards scale variation handling. It could also generalize to\nvarious backbones, benchmarks, and other challenging downstream tasks like\ninstance segmentation. It does not introduce inference overhead and could serve\nas a free lunch for general detection configurations. Besides, it also\nfacilitates efficient training due to fast convergence. Code and models are\navailable at github.com/yukang2017/Stitcher.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 16:48:17 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 05:22:59 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Yukang", ""], ["Zhang", "Peizhen", ""], ["Li", "Zeming", ""], ["Li", "Yanwei", ""], ["Zhang", "Xiangyu", ""], ["Qi", "Lu", ""], ["Sun", "Jian", ""], ["Jia", "Jiaya", ""]]}, {"id": "2004.12436", "submitter": "Meng Cao", "authors": "Meng Cao, Yuexian Zou", "title": "All you need is a second look: Towards Tighter Arbitrary shape text\n  detection", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based scene text detection methods have progressed\nsubstantially over the past years. However, there remain several problems to be\nsolved. Generally, long curve text instances tend to be fragmented because of\nthe limited receptive field size of CNN. Besides, simple representations using\nrectangle or quadrangle bounding boxes fall short when dealing with more\nchallenging arbitrary-shaped texts. In addition, the scale of text instances\nvaries greatly which leads to the difficulty of accurate prediction through a\nsingle segmentation network. To address these problems, we innovatively propose\na two-stage segmentation based arbitrary text detector named \\textit{NASK}\n(\\textbf{N}eed \\textbf{A} \\textbf{S}econd loo\\textbf{K}). Specifically,\n\\textit{NASK} consists of a Text Instance Segmentation network namely\n\\textit{TIS} (\\(1^{st}\\) stage), a Text RoI Pooling module and a Fiducial pOint\neXpression module termed as \\textit{FOX} (\\(2^{nd}\\) stage). Firstly,\n\\textit{TIS} conducts instance segmentation to obtain rectangle text proposals\nwith a proposed Group Spatial and Channel Attention module (\\textit{GSCA}) to\naugment the feature expression. Then, Text RoI Pooling transforms these\nrectangles to the fixed size. Finally, \\textit{FOX} is introduced to\nreconstruct text instances with a more tighter representation using the\npredicted geometrical attributes including text center line, text line\norientation, character scale and character orientation. Experimental results on\ntwo public benchmarks including \\textit{Total-Text} and \\textit{SCUT-CTW1500}\nhave demonstrated that the proposed \\textit{NASK} achieves state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 17:03:41 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Cao", "Meng", ""], ["Zou", "Yuexian", ""]]}, {"id": "2004.12452", "submitter": "Mingming He", "authors": "Sitao Xiang, Yuming Gu, Pengda Xiang, Mingming He, Koki Nagano, Haiwei\n  Chen, Hao Li", "title": "One-Shot Identity-Preserving Portrait Reenactment", "comments": "29 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning-based framework for portrait reenactment from a\nsingle picture of a target (one-shot) and a video of a driving subject.\nExisting facial reenactment methods suffer from identity mismatch and produce\ninconsistent identities when a target and a driving subject are different\n(cross-subject), especially in one-shot settings. In this work, we aim to\naddress identity preservation in cross-subject portrait reenactment from a\nsingle picture. We introduce a novel technique that can disentangle identity\nfrom expressions and poses, allowing identity preserving portrait reenactment\neven when the driver's identity is very different from that of the target. This\nis achieved by a novel landmark disentanglement network (LD-Net), which\npredicts personalized facial landmarks that combine the identity of the target\nwith expressions and poses from a different subject. To handle portrait\nreenactment from unseen subjects, we also introduce a feature dictionary-based\ngenerative adversarial network (FD-GAN), which locally translates 2D landmarks\ninto a personalized portrait, enabling one-shot portrait reenactment under\nlarge pose and expression variations. We validate the effectiveness of our\nidentity disentangling capabilities via an extensive ablation study, and our\nmethod produces consistent identities for cross-subject portrait reenactment.\nOur comprehensive experiments show that our method significantly outperforms\nthe state-of-the-art single-image facial reenactment methods. We will release\nour code and models for academic use.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 18:30:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Xiang", "Sitao", ""], ["Gu", "Yuming", ""], ["Xiang", "Pengda", ""], ["He", "Mingming", ""], ["Nagano", "Koki", ""], ["Chen", "Haiwei", ""], ["Li", "Hao", ""]]}, {"id": "2004.12464", "submitter": "Yuteng Zhu", "authors": "Yuteng Zhu", "title": "Designing a physically-feasible colour filter to make a camera more\n  colorimetric", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previously, a method has been developed to find the best colour filter for a\ngiven camera which results in the new effective camera sensitivities that best\nmeet the Luther condition. That is, the new sensitivities are approximately\nlinearly related to the XYZ colour matching functions. However, with no\nconstraint, the filter derived from this Luther-condition based optimisation\ncan be rather non-smooth and transmit very little light which are impractical\nfor fabrication.\n  In this paper, we extend the Luther-condition filter optimisation method to\nallow us to incorporate both the smoothness and transmittance bounds of the\nrecovered filter which are key practical concerns. Experiments demonstrate that\nwe can find physically realisable filters which are smooth and reasonably\ntransmissive with which the effective \"camera+filter\" becomes significantly\nmore colorimetric.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 19:45:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhu", "Yuteng", ""]]}, {"id": "2004.12482", "submitter": "Damian Kowalczyk", "authors": "Christoffer Riis, Damian Konrad Kowalczyk, Lars Kai Hansen", "title": "On the Limits to Multi-Modal Popularity Prediction on Instagram -- A New\n  Robust, Efficient and Explainable Baseline", "comments": "Presented at ICAART 2021", "journal-ref": "Proceedings of the 13th International Conference on Agents and\n  Artificial Intelligence - Volume 2: ICAART, ISBN 978-989-758-484-8, pages\n  1200-1209, 2021", "doi": "10.5220/0010377112001209", "report-no": null, "categories": "cs.CV cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our global population contributes visual content on platforms like Instagram,\nattempting to express themselves and engage their audiences, at an\nunprecedented and increasing rate. In this paper, we revisit the popularity\nprediction on Instagram. We present a robust, efficient, and explainable\nbaseline for population-based popularity prediction, achieving strong ranking\nperformance. We employ the latest methods in computer vision to maximize the\ninformation extracted from the visual modality. We use transfer learning to\nextract visual semantics such as concepts, scenes, and objects, allowing a new\nlevel of scrutiny in an extensive, explainable ablation study. We inform\nfeature selection towards a robust and scalable model, but also illustrate\nfeature interactions, offering new directions for further inquiry in\ncomputational social science. Our strongest models inform a lower limit to\npopulation-based predictability of popularity on Instagram. The models are\nimmediately applicable to social media monitoring and influencer\nidentification.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 21:21:50 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 13:39:45 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Riis", "Christoffer", ""], ["Kowalczyk", "Damian Konrad", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "2004.12498", "submitter": "Haiyan Wang", "authors": "Haiyan Wang, Xuejian Rong, Liang Yang, Jinglun Feng, Jizhong Xiao,\n  Yingli Tian", "title": "Weakly Supervised Semantic Segmentation in 3D Graph-Structured Point\n  Clouds of Wild Scenes", "comments": "13 pages, 8 figures, Under review as a journal paper at CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deficiency of 3D segmentation labels is one of the main obstacles to\neffective point cloud segmentation, especially for scenes in the wild with\nvarieties of different objects. To alleviate this issue, we propose a novel\ndeep graph convolutional network-based framework for large-scale semantic scene\nsegmentation in point clouds with sole 2D supervision. Different with numerous\npreceding multi-view supervised approaches focusing on single object point\nclouds, we argue that 2D supervision is capable of providing sufficient\nguidance information for training 3D semantic segmentation models of natural\nscene point clouds while not explicitly capturing their inherent structures,\neven with only single view per training sample. Specifically, a Graph-based\nPyramid Feature Network (GPFN) is designed to implicitly infer both global and\nlocal features of point sets and an Observability Network (OBSNet) is\nintroduced to further solve object occlusion problem caused by complicated\nspatial relations of objects in 3D scenes. During the projection process,\nperspective rendering and semantic fusion modules are proposed to provide\nrefined 2D supervision signals for training along with a 2D-3D joint\noptimization strategy. Extensive experimental results demonstrate the\neffectiveness of our 2D supervised framework, which achieves comparable results\nwith the state-of-the-art approaches trained with full 3D labels, for semantic\npoint cloud segmentation on the popular SUNCG synthetic dataset and S3DIS\nreal-world dataset.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:02:23 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 21:14:07 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Haiyan", ""], ["Rong", "Xuejian", ""], ["Yang", "Liang", ""], ["Feng", "Jinglun", ""], ["Xiao", "Jizhong", ""], ["Tian", "Yingli", ""]]}, {"id": "2004.12525", "submitter": "Laurie Bose", "authors": "Laurie Bose, Jianing Chen, Stephen J. Carey, Piotr Dudek, Walterio\n  Mayol-Cuevas", "title": "Fully Embedding Fast Convolutional Networks on Pixel Processor Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method of CNN inference for pixel processor array (PPA)\nvision sensors, designed to take advantage of their massive parallelism and\nanalog compute capabilities. PPA sensors consist of an array of processing\nelements (PEs), with each PE capable of light capture, data storage and\ncomputation, allowing various computer vision processing to be executed\ndirectly upon the sensor device. The key idea behind our approach is storing\nnetwork weights \"in-pixel\" within the PEs of the PPA sensor itself to allow\nvarious computations, such as multiple different image convolutions, to be\ncarried out in parallel. Our approach can perform convolutional layers, max\npooling, ReLu, and a final fully connected layer entirely upon the PPA sensor,\nwhile leaving no untapped computational resources. This is in contrast to\nprevious works that only use a sensor-level processing to sequentially compute\nimage convolutions, and must transfer data to an external digital processor to\ncomplete the computation. We demonstrate our approach on the SCAMP-5 vision\nsystem, performing inference of a MNIST digit classification network at over\n3000 frames per second and over 93% classification accuracy. This is the first\nwork demonstrating CNN inference conducted entirely upon the processor array of\na PPA vision sensor device, requiring no external processing.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:00:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bose", "Laurie", ""], ["Chen", "Jianing", ""], ["Carey", "Stephen J.", ""], ["Dudek", "Piotr", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "2004.12529", "submitter": "Zhongyi Han", "authors": "Zhongyi Han, Xian-Jin Gui, Chaoran Cui, Yilong Yin", "title": "Towards Accurate and Robust Domain Adaptation under Noisy Environments", "comments": "To appear in Proceedings of IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In non-stationary environments, learning machines usually confront the domain\nadaptation scenario where the data distribution does change over time. Previous\ndomain adaptation works have achieved great success in theory and practice.\nHowever, they always lose robustness in noisy environments where the labels and\nfeatures of examples from the source domain become corrupted. In this paper, we\nreport our attempt towards achieving accurate noise-robust domain adaptation.\nWe first give a theoretical analysis that reveals how harmful noises influence\nunsupervised domain adaptation. To eliminate the effect of label noise, we\npropose an offline curriculum learning for minimizing a newly-defined empirical\nsource risk. To reduce the impact of feature noise, we propose a proxy\ndistribution based margin discrepancy. We seamlessly transform our methods into\nan adversarial network that performs efficient joint optimization for them,\nsuccessfully mitigating the negative influence from both data corruption and\ndistribution shift. A series of empirical studies show that our algorithm\nremarkably outperforms state of the art, over 10% accuracy improvements in some\ndomain adaptation tasks under noisy environments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:07:19 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 01:18:01 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Han", "Zhongyi", ""], ["Gui", "Xian-Jin", ""], ["Cui", "Chaoran", ""], ["Yin", "Yilong", ""]]}, {"id": "2004.12535", "submitter": "Jerry Wei", "authors": "Jerry Wei and Arief Suriawinata and Xiaoying Liu and Bing Ren and\n  Mustafa Nasir-Moin and Naofumi Tomita and Jason Wei and Saeed Hassanpour", "title": "Difficulty Translation in Histopathology Images", "comments": "Accepted to 2020 Artificial Intelligence in Medicine (AIME)\n  conference. Invited for long oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unique nature of histopathology images opens the door to domain-specific\nformulations of image translation models. We propose a difficulty translation\nmodel that modifies colorectal histopathology images to be more challenging to\nclassify. Our model comprises a scorer, which provides an output confidence to\nmeasure the difficulty of images, and an image translator, which learns to\ntranslate images from easy-to-classify to hard-to-classify using a training set\ndefined by the scorer. We present three findings. First, generated images were\nindeed harder to classify for both human pathologists and machine learning\nclassifiers than their corresponding source images. Second, image classifiers\ntrained with generated images as augmented data performed better on both easy\nand hard images from an independent test set. Finally, human annotator\nagreement and our model's measure of difficulty correlated strongly, implying\nthat for future work requiring human annotator agreement, the confidence score\nof a machine learning classifier could be used as a proxy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:27:15 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 23:37:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wei", "Jerry", ""], ["Suriawinata", "Arief", ""], ["Liu", "Xiaoying", ""], ["Ren", "Bing", ""], ["Nasir-Moin", "Mustafa", ""], ["Tomita", "Naofumi", ""], ["Wei", "Jason", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2004.12537", "submitter": "Jun Ma", "authors": "Jun Ma, Yixin Wang, Xingle An, Cheng Ge, Ziqi Yu, Jianan Chen,\n  Qiongjie Zhu, Guoqiang Dong, Jian He, Zhiqiang He, Yuntao Zhu, Ziwei Nie,\n  Xiaoping Yang", "title": "Towards Data-Efficient Learning: A Benchmark for COVID-19 CT Lung and\n  Infection Segmentation", "comments": "accepted for publication in Medical Physics", "journal-ref": null, "doi": "10.1002/mp.14676", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Accurate segmentation of lung and infection in COVID-19 CT scans\nplays an important role in the quantitative management of patients. Most of the\nexisting studies are based on large and private annotated datasets that are\nimpractical to obtain from a single institution, especially when radiologists\nare busy fighting the coronavirus disease. Furthermore, it is hard to compare\ncurrent COVID-19 CT segmentation methods as they are developed on different\ndatasets, trained in different settings, and evaluated with different metrics.\nMethods: To promote the development of data-efficient deep learning methods, in\nthis paper, we built three benchmarks for lung and infection segmentation based\non 70 annotated COVID-19 cases, which contain current active research areas,\ne.g., few-shot learning, domain generalization, and knowledge transfer. For a\nfair comparison among different segmentation methods, we also provide standard\ntraining, validation and testing splits, evaluation metrics and, the\ncorresponding code. Results: Based on the state-of-the-art network, we provide\nmore than 40 pre-trained baseline models, which not only serve as\nout-of-the-box segmentation tools but also save computational time for\nresearchers who are interested in COVID-19 lung and infection segmentation. We\nachieve average Dice Similarity Coefficient (DSC) scores of 97.3\\%, 97.7\\%, and\n67.3\\% and average Normalized Surface Dice (NSD) scores of 90.6\\%, 91.4\\%, and\n70.0\\% for left lung, right lung, and infection, respectively. Conclusions: To\nthe best of our knowledge, this work presents the first data-efficient learning\nbenchmark for medical image segmentation and the largest number of pre-trained\nmodels up to now. All these resources are publicly available, and our work lays\nthe foundation for promoting the development of deep learning methods for\nefficient COVID-19 CT segmentation with limited data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 01:31:48 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 11:21:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ma", "Jun", ""], ["Wang", "Yixin", ""], ["An", "Xingle", ""], ["Ge", "Cheng", ""], ["Yu", "Ziqi", ""], ["Chen", "Jianan", ""], ["Zhu", "Qiongjie", ""], ["Dong", "Guoqiang", ""], ["He", "Jian", ""], ["He", "Zhiqiang", ""], ["Zhu", "Yuntao", ""], ["Nie", "Ziwei", ""], ["Yang", "Xiaoping", ""]]}, {"id": "2004.12591", "submitter": "Peide Cai", "authors": "Peide Cai, Yuxiang Sun, Hengli Wang, Ming Liu", "title": "VTGNet: A Vision-based Trajectory Generation Network for Autonomous\n  Vehicles in Urban Environments", "comments": "11 pages, 14 figures, and 4 tables. The paper is accepted by IEEE\n  Transactions on Intelligent Vehicles (T-IV), 2020", "journal-ref": null, "doi": "10.1109/TIV.2020.3033878", "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods for autonomous driving are implemented with many building\nblocks from perception, planning and control, making them difficult to\ngeneralize to varied scenarios due to complex assumptions and\ninterdependencies. Recently, the end-to-end driving method has emerged, which\nperforms well and generalizes to new environments by directly learning from\nexport-provided data. However, many existing methods on this topic neglect to\ncheck the confidence of the driving actions and the ability to recover from\ndriving mistakes. In this paper, we develop an uncertainty-aware end-to-end\ntrajectory generation method based on imitation learning. It can extract\nspatiotemporal features from the front-view camera images for scene\nunderstanding, and then generate collision-free trajectories several seconds\ninto the future. The experimental results suggest that under various weather\nand lighting conditions, our network can reliably generate trajectories in\ndifferent urban environments, such as turning at intersections and slowing down\nfor collision avoidance. Furthermore, closed-loop driving tests suggest that\nthe proposed method achieves better cross-scene/platform driving results than\nthe state-of-the-art (SOTA) end-to-end control method, where our model can\nrecover from off-center and off-orientation errors and capture 80% of dangerous\ncases with high uncertainty estimations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 06:17:55 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 08:57:14 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 08:46:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Cai", "Peide", ""], ["Sun", "Yuxiang", ""], ["Wang", "Hengli", ""], ["Liu", "Ming", ""]]}, {"id": "2004.12592", "submitter": "Zhongyi Han", "authors": "Tianyang Li, Zhongyi Han, Benzheng Wei, Yuanjie Zheng, Yanfei Hong,\n  Jinyu Cong", "title": "Robust Screening of COVID-19 from Chest X-ray via Discriminative\n  Cost-Sensitive Learning", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the new problem of automated screening of coronavirus\ndisease 2019 (COVID-19) based on chest X-rays, which is urgently demanded\ntoward fast stopping the pandemic. However, robust and accurate screening of\nCOVID-19 from chest X-rays is still a globally recognized challenge because of\ntwo bottlenecks: 1) imaging features of COVID-19 share some similarities with\nother pneumonia on chest X-rays, and 2) the misdiagnosis rate of COVID-19 is\nvery high, and the misdiagnosis cost is expensive. While a few pioneering works\nhave made much progress, they underestimate both crucial bottlenecks. In this\npaper, we report our solution, discriminative cost-sensitive learning (DCSL),\nwhich should be the choice if the clinical needs the assisted screening of\nCOVID-19 from chest X-rays. DCSL combines both advantages from fine-grained\nclassification and cost-sensitive learning. Firstly, DCSL develops a\nconditional center loss that learns deep discriminative representation.\nSecondly, DCSL establishes score-level cost-sensitive learning that can\nadaptively enlarge the cost of misclassifying COVID-19 examples into other\nclasses. DCSL is so flexible that it can apply in any deep neural network. We\ncollected a large-scale multi-class dataset comprised of 2,239 chest X-ray\nexamples: 239 examples from confirmed COVID-19 cases, 1,000 examples with\nconfirmed bacterial or viral pneumonia cases, and 1,000 examples of healthy\npeople. Extensive experiments on the three-class classification show that our\nalgorithm remarkably outperforms state-of-the-art algorithms. It achieves an\naccuracy of 97.01%, a precision of 97%, a sensitivity of 97.09%, and an\nF1-score of 96.98%. These results endow our algorithm as an efficient tool for\nthe fast large-scale screening of COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 06:17:56 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 14:37:04 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Li", "Tianyang", ""], ["Han", "Zhongyi", ""], ["Wei", "Benzheng", ""], ["Zheng", "Yuanjie", ""], ["Hong", "Yanfei", ""], ["Cong", "Jinyu", ""]]}, {"id": "2004.12599", "submitter": "Cheng-Ming Chiang", "authors": "Cheng-Ming Chiang, Yu Tseng, Yu-Syuan Xu, Hsien-Kai Kuo, Yi-Min Tsai,\n  Guan-Yu Chen, Koan-Sin Tan, Wei-Ting Wang, Yu-Chieh Lin, Shou-Yao Roy Tseng,\n  Wei-Shiang Lin, Chia-Lin Yu, BY Shen, Kloze Kao, Chia-Ming Cheng, Hung-Jen\n  Chen", "title": "Deploying Image Deblurring across Mobile Devices: A Perspective of\n  Quality and Latency", "comments": "CVPR 2020 Workshop on New Trends in Image Restoration and Enhancement\n  (NTIRE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image enhancement and restoration have become important\napplications on mobile devices, such as super-resolution and image deblurring.\nHowever, most state-of-the-art networks present extremely high computational\ncomplexity. This makes them difficult to be deployed on mobile devices with\nacceptable latency. Moreover, when deploying to different mobile devices, there\nis a large latency variation due to the difference and limitation of deep\nlearning accelerators on mobile devices. In this paper, we conduct a search of\nportable network architectures for better quality-latency trade-off across\nmobile devices. We further present the effectiveness of widely used network\noptimizations for image deblurring task. This paper provides comprehensive\nexperiments and comparisons to uncover the in-depth analysis for both latency\nand image quality. Through all the above works, we demonstrate the successful\ndeployment of image deblurring application on mobile devices with the\nacceleration of deep learning accelerators. To the best of our knowledge, this\nis the first paper that addresses all the deployment issues of image deblurring\ntask across mobile devices. This paper provides practical\ndeployment-guidelines, and is adopted by the championship-winning team in NTIRE\n2020 Image Deblurring Challenge on Smartphone Track.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 06:32:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chiang", "Cheng-Ming", ""], ["Tseng", "Yu", ""], ["Xu", "Yu-Syuan", ""], ["Kuo", "Hsien-Kai", ""], ["Tsai", "Yi-Min", ""], ["Chen", "Guan-Yu", ""], ["Tan", "Koan-Sin", ""], ["Wang", "Wei-Ting", ""], ["Lin", "Yu-Chieh", ""], ["Tseng", "Shou-Yao Roy", ""], ["Lin", "Wei-Shiang", ""], ["Yu", "Chia-Lin", ""], ["Shen", "BY", ""], ["Kao", "Kloze", ""], ["Cheng", "Chia-Ming", ""], ["Chen", "Hung-Jen", ""]]}, {"id": "2004.12604", "submitter": "Michael Gadermayr", "authors": "Georg Wimmer, Michael Gadermayr, Andreas V\\'ecsei, Andreas Uhl", "title": "Improving Endoscopic Decision Support Systems by Translating Between\n  Imaging Modalities", "comments": "Submitted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel imaging technologies raise many questions concerning the adaptation of\ncomputer-aided decision support systems. Classification models either need to\nbe adapted or even newly trained from scratch to exploit the full potential of\nenhanced techniques. Both options typically require the acquisition of new\nlabeled training data. In this work we investigate the applicability of\nimage-to-image translation to endoscopic images showing different imaging\nmodalities, namely conventional white-light and narrow-band imaging. In a study\non computer-aided celiac disease diagnosis, we explore whether image-to-image\ntranslation is capable of effectively performing the translation between the\ndomains. We investigate if models can be trained on virtual (or a mixture of\nvirtual and real) samples to improve overall accuracy in a setting with limited\nlabeled training data. Finally, we also ask whether a translation of testing\nimages to another domain is capable of improving accuracy by exploiting the\nenhanced imaging characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 06:55:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wimmer", "Georg", ""], ["Gadermayr", "Michael", ""], ["V\u00e9csei", "Andreas", ""], ["Uhl", "Andreas", ""]]}, {"id": "2004.12611", "submitter": "Volker Krueger", "authors": "Bjarne Grossmann, Volker Krueger", "title": "Continuous hand-eye calibration using 3D points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of calibration algorithms has been driven into two\nmajor directions: (1) an increasing accuracy of mathematical approaches and (2)\nan increasing flexibility in usage by reducing the dependency on calibration\nobjects. These two trends, however, seem to be contradictory since the overall\naccuracy is directly related to the accuracy of the pose estimation of the\ncalibration object and therefore demanding large objects, while an increased\nflexibility leads to smaller objects or noisier estimation methods.\n  The method presented in this paper aims to resolves this problem in two\nsteps: First, we derive a simple closed-form solution with a shifted focus\ntowards the equation of translation that only solves for the necessary hand-eye\ntransformation. We show that it is superior in accuracy and robustness compared\nto traditional approaches. Second, we decrease the dependency on the\ncalibration object to a single 3D-point by using a similar formulation based on\nthe equation of translation which is much less affected by the estimation error\nof the calibration object's orientation. Moreover, it makes the estimation of\nthe orientation obsolete while taking advantage of the higher accuracy and\nrobustness from the first solution, resulting in a versatile method for\ncontinuous hand-eye calibration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 07:13:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Grossmann", "Bjarne", ""], ["Krueger", "Volker", ""]]}, {"id": "2004.12615", "submitter": "Jingjing Li", "authors": "Li Jingjing, Chen Erpeng, Ding Zhengming, Zhu Lei, Lu Ke, Shen Heng\n  Tao", "title": "Maximum Density Divergence for Domain Adaptation", "comments": "Published on IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2991050", "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation addresses the problem of transferring\nknowledge from a well-labeled source domain to an unlabeled target domain where\nthe two domains have distinctive data distributions. Thus, the essence of\ndomain adaptation is to mitigate the distribution divergence between the two\ndomains. The state-of-the-art methods practice this very idea by either\nconducting adversarial training or minimizing a metric which defines the\ndistribution gaps. In this paper, we propose a new domain adaptation method\nnamed Adversarial Tight Match (ATM) which enjoys the benefits of both\nadversarial training and metric learning. Specifically, at first, we propose a\nnovel distance loss, named Maximum Density Divergence (MDD), to quantify the\ndistribution divergence. MDD minimizes the inter-domain divergence (\"match\" in\nATM) and maximizes the intra-class density (\"tight\" in ATM). Then, to address\nthe equilibrium challenge issue in adversarial domain adaptation, we consider\nleveraging the proposed MDD into adversarial domain adaptation framework. At\nlast, we tailor the proposed MDD as a practical learning loss and report our\nATM. Both empirical evaluation and theoretical analysis are reported to verify\nthe effectiveness of the proposed method. The experimental results on four\nbenchmarks, both classical and large-scale, show that our method is able to\nachieve new state-of-the-art performance on most evaluations. Codes and\ndatasets used in this paper are available at {\\it github.com/lijin118/ATM}.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 07:35:06 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Jingjing", "Li", ""], ["Erpeng", "Chen", ""], ["Zhengming", "Ding", ""], ["Lei", "Zhu", ""], ["Ke", "Lu", ""], ["Tao", "Shen Heng", ""]]}, {"id": "2004.12623", "submitter": "Amelie Royer", "authors": "Amelie Royer, Christoph H. Lampert", "title": "Localizing Grouped Instances for Efficient Detection in Low-Resource\n  Scenarios", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art detection systems are generally evaluated on their ability\nto exhaustively retrieve objects densely distributed in the image, across a\nwide variety of appearances and semantic categories. Orthogonal to this, many\nreal-life object detection applications, for example in remote sensing, instead\nrequire dealing with large images that contain only a few small objects of a\nsingle class, scattered heterogeneously across the space. In addition, they are\noften subject to strict computational constraints, such as limited battery\ncapacity and computing power. To tackle these more practical scenarios, we\npropose a novel flexible detection scheme that efficiently adapts to variable\nobject sizes and densities: We rely on a sequence of detection stages, each of\nwhich has the ability to predict groups of objects as well as individuals.\nSimilar to a detection cascade, this multi-stage architecture spares\ncomputational effort by discarding large irrelevant regions of the image early\nduring the detection process. The ability to group objects provides further\ncomputational and memory savings, as it allows working with lower image\nresolutions in early stages, where groups are more easily detected than\nindividuals, as they are more salient. We report experimental results on two\naerial image datasets, and show that the proposed method is as accurate yet\ncomputationally more efficient than standard single-shot detectors,\nconsistently across three different backbone architectures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 07:56:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Royer", "Amelie", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2004.12626", "submitter": "Luca Guarnera", "authors": "Luca Guarnera (1 and 2), Oliver Giudice (1), Cristina Nastasi (1),\n  Sebastiano Battiato (1 and 2) ((1) University of Catania, (2) iCTLab s.r.l. -\n  Spin-off of University of Catania)", "title": "Preliminary Forensics Analysis of DeepFake Images", "comments": "Accepted at IEEE AEIT International Annual Conference 2020", "journal-ref": "2020 AEIT International Annual Conference (AEIT)", "doi": "10.23919/AEIT50178.2020.9241108", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most terrifying phenomenon nowadays is the DeepFake: the\npossibility to automatically replace a person's face in images and videos by\nexploiting algorithms based on deep learning. This paper will present a brief\noverview of technologies able to produce DeepFake images of faces. A forensics\nanalysis of those images with standard methods will be presented: not\nsurprisingly state of the art techniques are not completely able to detect the\nfakeness. To solve this, a preliminary idea on how to fight DeepFake images of\nfaces will be presented by analysing anomalies in the frequency domain.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:09:06 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:05:39 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 08:55:15 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2020 12:37:28 GMT"}, {"version": "v5", "created": "Tue, 4 Aug 2020 09:27:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Guarnera", "Luca", "", "1 and 2"], ["Giudice", "Oliver", "", "1 and 2"], ["Nastasi", "Cristina", "", "1 and 2"], ["Battiato", "Sebastiano", "", "1 and 2"]]}, {"id": "2004.12629", "submitter": "Devashish Prasad", "authors": "Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave and\n  Kavita Sultanpure", "title": "CascadeTabNet: An approach for end to end table detection and structure\n  recognition from image-based documents", "comments": "Paper has been accepted at CVPR Workshop 2020 (CVPR2020 Workshop on\n  Text and Documents in the Deep Learning Era)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic table recognition method for interpretation of tabular data in\ndocument images majorly involves solving two problems of table detection and\ntable structure recognition. The prior work involved solving both problems\nindependently using two separate approaches. More recent works signify the use\nof deep learning-based solutions while also attempting to design an end to end\nsolution. In this paper, we present an improved deep learning-based end to end\napproach for solving both problems of table detection and structure recognition\nusing a single Convolution Neural Network (CNN) model. We propose\nCascadeTabNet: a Cascade mask Region-based CNN High-Resolution Network (Cascade\nmask R-CNN HRNet) based model that detects the regions of tables and recognizes\nthe structural body cells from the detected tables at the same time. We\nevaluate our results on ICDAR 2013, ICDAR 2019 and TableBank public datasets.\nWe achieved 3rd rank in ICDAR 2019 post-competition results for table detection\nwhile attaining the best accuracy results for the ICDAR 2013 and TableBank\ndataset. We also attain the highest accuracy results on the ICDAR 2019 table\nstructure recognition dataset. Additionally, we demonstrate effective transfer\nlearning and image augmentation techniques that enable CNNs to achieve very\naccurate table detection results. Code and dataset has been made available at:\nhttps://github.com/DevashishPrasad/CascadeTabNet\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:12:48 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 08:02:43 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Prasad", "Devashish", ""], ["Gadpal", "Ayan", ""], ["Kapadni", "Kshitij", ""], ["Visave", "Manish", ""], ["Sultanpure", "Kavita", ""]]}, {"id": "2004.12636", "submitter": "Jin Hyeok Yoo", "authors": "Jin Hyeok Yoo and Yecheol Kim and Jisong Kim and Jun Won Choi", "title": "3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View\n  Spatial Feature Fusion for 3D Object Detection", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58583-9_43", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep architecture for fusing camera and LiDAR\nsensors for 3D object detection. Because the camera and LiDAR sensor signals\nhave different characteristics and distributions, fusing these two modalities\nis expected to improve both the accuracy and robustness of 3D object detection.\nOne of the challenges presented by the fusion of cameras and LiDAR is that the\nspatial feature maps obtained from each modality are represented by\nsignificantly different views in the camera and world coordinates; hence, it is\nnot an easy task to combine two heterogeneous feature maps without loss of\ninformation. To address this problem, we propose a method called 3D-CVF that\ncombines the camera and LiDAR features using the cross-view spatial feature\nfusion strategy. First, the method employs auto-calibrated projection, to\ntransform the 2D camera features to a smooth spatial feature map with the\nhighest correspondence to the LiDAR features in the bird's eye view (BEV)\ndomain. Then, a gated feature fusion network is applied to use the spatial\nattention maps to mix the camera and LiDAR features appropriately according to\nthe region. Next, camera-LiDAR feature fusion is also achieved in the\nsubsequent proposal refinement stage. The camera feature is used from the 2D\ncamera-view domain via 3D RoI grid pooling and fused with the BEV feature for\nproposal refinement. Our evaluations, conducted on the KITTI and nuScenes 3D\nobject detection datasets demonstrate that the camera-LiDAR fusion offers\nsignificant performance gain over single modality and that the proposed 3D-CVF\nachieves state-of-the-art performance in the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:34:46 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 03:00:03 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Yoo", "Jin Hyeok", ""], ["Kim", "Yecheol", ""], ["Kim", "Jisong", ""], ["Choi", "Jun Won", ""]]}, {"id": "2004.12652", "submitter": "Andreas Doering", "authors": "Umer Rafi, Andreas Doering, Bastian Leibe, Juergen Gall", "title": "Self-supervised Keypoint Correspondences for Multi-Person Pose\n  Estimation and Tracking in Videos", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58565-5_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video annotation is expensive and time consuming. Consequently, datasets for\nmulti-person pose estimation and tracking are less diverse and have more sparse\nannotations compared to large scale image datasets for human pose estimation.\nThis makes it challenging to learn deep learning based models for associating\nkeypoints across frames that are robust to nuisance factors such as motion blur\nand occlusions for the task of multi-person pose tracking. To address this\nissue, we propose an approach that relies on keypoint correspondences for\nassociating persons in videos. Instead of training the network for estimating\nkeypoint correspondences on video data, it is trained on a large scale image\ndatasets for human pose estimation using self-supervision. Combined with a\ntop-down framework for human pose estimation, we use keypoints correspondences\nto (i) recover missed pose detections (ii) associate pose detections across\nvideo frames. Our approach achieves state-of-the-art results for multi-frame\npose estimation and multi-person pose tracking on the PosTrack $2017$ and\nPoseTrack $2018$ data sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:02:24 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 09:30:30 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 11:48:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Rafi", "Umer", ""], ["Doering", "Andreas", ""], ["Leibe", "Bastian", ""], ["Gall", "Juergen", ""]]}, {"id": "2004.12668", "submitter": "Fabian Isensee", "authors": "Fabian Isensee and Klaus H. Maier-Hein", "title": "OR-UNet: an Optimized Robust Residual U-Net for Instrument Segmentation\n  in Endoscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of endoscopic images is an essential processing step for\ncomputer and robotics-assisted interventions. The Robust-MIS challenge provides\nthe largest dataset of annotated endoscopic images to date, with 5983 manually\nannotated images. Here we describe OR-UNet, our optimized robust residual 2D\nU-Net for endoscopic image segmentation. As the name implies, the network makes\nuse of residual connections in the encoder. It is trained with the sum of Dice\nand cross-entropy loss and deep supervision. During training, extensive data\naugmentation is used to increase the robustness. In an 8-fold cross-validation\non the training images, our model achieved a mean (median) Dice score of 87.41\n(94.35). We use the eight models from the cross-validation as an ensemble on\nthe test set.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:34:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Isensee", "Fabian", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "2004.12679", "submitter": "Lanyun Zhu", "authors": "Lanyun Zhu, Shiping Zhu, Xuanyi Liu, Li Luo", "title": "Distance Guided Channel Weighting for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have achieved great success in improving the performance of\nmultiple computer vision tasks by capturing features with a high channel number\nutilizing deep neural networks. However, many channels of extracted features\nare not discriminative and contain a lot of redundant information. In this\npaper, we address above issue by introducing the Distance Guided Channel\nWeighting (DGCW) Module. The DGCW module is constructed in a pixel-wise context\nextraction manner, which enhances the discriminativeness of features by\nweighting different channels of each pixel's feature vector when modeling its\nrelationship with other pixels. It can make full use of the high-discriminative\ninformation while ignore the low-discriminative information containing in\nfeature maps, as well as capture the long-range dependencies. Furthermore, by\nincorporating the DGCW module with a baseline segmentation network, we propose\nthe Distance Guided Channel Weighting Network (DGCWNet). We conduct extensive\nexperiments to demonstrate the effectiveness of DGCWNet. In particular, it\nachieves 81.6% mIoU on Cityscapes with only fine annotated data for training,\nand also gains satisfactory performance on another two semantic segmentation\ndatasets, i.e. Pascal Context and ADE20K. Code will be available soon at\nhttps://github.com/LanyunZhu/DGCWNet.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:57:12 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 11:59:41 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 13:46:04 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhu", "Lanyun", ""], ["Zhu", "Shiping", ""], ["Liu", "Xuanyi", ""], ["Luo", "Li", ""]]}, {"id": "2004.12695", "submitter": "Mikhail Kopeliovich", "authors": "Yuriy Mironenko, Konstantin Kalinin, Mikhail Kopeliovich, Mikhail\n  Petrushan", "title": "Remote Photoplethysmography: Rarely Considered Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote Photoplethysmography (rPPG) is a fast-growing technique of vital sign\nestimation by analyzing video of a person. Several major phenomena affecting\nrPPG signals have been studied (e.g. video compression, distance from person to\ncamera, skin tone, head motions). However, to develop a highly accurate rPPG\nmethod, new, minor, factors should be investigated. First considered factor is\nirregular frame rate of video recordings. Despite of PPG signal transformation\nby frame rate irregularity, no significant distortion of PPG signal spectra was\nfound in the experiments. Second factor is rolling shutter effect which\ngenerates tiny phase shift of the same PPG signal in different parts of the\nframe caused by progressive scanning. In particular conditions effect of this\nartifact could be of the same order of magnitude as physiologically caused\nphase shifts. Third factor is a size of temporal windows, which could\nsignificantly influence the estimated error of vital sign evaluation. It\nfollows that one should account difference in size of processing windows when\ncomparing rPPG methods. Short series of experiments were conducted to estimate\nimportance of these phenomena and to determine necessity of their further\ncomprehensive study.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:38:45 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mironenko", "Yuriy", ""], ["Kalinin", "Konstantin", ""], ["Kopeliovich", "Mikhail", ""], ["Petrushan", "Mikhail", ""]]}, {"id": "2004.12697", "submitter": "Jie Zhang Dr.", "authors": "Junhua Sun and Zhou Zhang and Jie Zhang", "title": "Reconstructing normal section profiles of 3D revolving structures via\n  pose-unconstrained multi-line structured-light vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The wheel of the train is a 3D revolving geometrical structure.\nReconstructing the normal section profile is an effective approach to determine\nthe critical geometric parameter and wear of the wheel in the community of\nrailway safety. The existing reconstruction methods typically require a sensor\nworking in a constrained position and pose, suffering poor flexibility and\nlimited viewangle. This paper proposes a pose-unconstrained normal section\nprofile reconstruction framework for 3D revolving structures via multiple 3D\ngeneral section profiles acquired by a multi-line structured light vision\nsensor. First, we establish a model to estimate the axis of 3D revolving\ngeometrical structure and the normal section profile using corresponding\npoints. Then, we embed the model into an iterative algorithm to optimize the\ncorresponding points and finally reconstruct the accurate normal section\nprofile. We conducted real experiment on reconstructing the normal section\nprofile of a 3D wheel. The results demonstrate that our algorithm reaches the\nmean precision of 0.068mm and good repeatability with the STD of 0.007mm. It is\nalso robust to varying pose variations of the sensor. Our proposed framework\nand models are generalized to any 3D wheeltype revolving components.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:39:38 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sun", "Junhua", ""], ["Zhang", "Zhou", ""], ["Zhang", "Jie", ""]]}, {"id": "2004.12700", "submitter": "Richard Jiang", "authors": "Ranjith Dinakaran, Li Zhang and Richard Jiang", "title": "In-Vehicle Object Detection in the Wild for Driverless Vehicles", "comments": "the 14th International FLINS Conference on Robotics and Artificial\n  Intelligence", "journal-ref": "FLINS 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-vehicle human object identification plays an important role in\nvision-based automated vehicle driving systems while objects such as\npedestrians and vehicles on roads or streets are the primary targets to protect\nfrom driverless vehicles. A challenge is the difficulty to detect objects in\nmoving under the wild conditions, while illumination and image quality could\ndrastically vary. In this work, to address this challenge, we exploit Deep\nConvolutional Generative Adversarial Networks (DCGANs) with Single Shot\nDetector (SSD) to handle with the wild conditions. In our work, a GAN was\ntrained with low-quality images to handle with the challenges arising from the\nwild conditions in smart cities, while a cascaded SSD is employed as the object\ndetector to perform with the GAN. We used tested our approach under wild\nconditions using taxi driver videos on London street in both daylight and night\ntimes, and the tests from in-vehicle videos demonstrate that this strategy can\ndrastically achieve a better detection rate under the wild conditions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:43:00 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dinakaran", "Ranjith", ""], ["Zhang", "Li", ""], ["Jiang", "Richard", ""]]}, {"id": "2004.12703", "submitter": "Mikhail Kopeliovich", "authors": "Mikhail Kopeliovich, Konstantin Kalinin, Yuriy Mironenko, Mikhail\n  Petrushan", "title": "On indirect assessment of heart rate in video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem of indirect assessment of heart rate in video is addressed. Several\nmethods of indirect evaluations (adaptive baselines) were examined on Remote\nPhysiological Signal Sensing challenge. Particularly, regression models of\ndependency of heart rate on estimated age and motion intensity were obtained on\nchallenge's train set. Accounting both motion and age in regression model led\nto top-quarter position in the leaderboard. Practical value of such adaptive\nbaseline approaches is discussed. Although such approaches are considered as\nnon-applicable in medicine, they are valuable as baseline for the\nphotoplethysmography problem.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:51:11 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kopeliovich", "Mikhail", ""], ["Kalinin", "Konstantin", ""], ["Mironenko", "Yuriy", ""], ["Petrushan", "Mikhail", ""]]}, {"id": "2004.12709", "submitter": "Hui Huang", "authors": "Chunhua Jia, Lei Zhang, Hui Huang, Weiwei Cai, Hao Hu, Rohan\n  Adivarekar", "title": "GraftNet: An Engineering Implementation of CNN for Fine-grained\n  Multi-label Task", "comments": "8 Pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label networks with branches are proved to perform well in both\naccuracy and speed, but lacks flexibility in providing dynamic extension onto\nnew labels due to the low efficiency of re-work on annotating and training. For\nmulti-label classification task, to cover new labels we need to annotate not\nonly newly collected images, but also the previous whole dataset to check\npresence of these new labels. Also training on whole re-annotated dataset costs\nmuch time. In order to recognize new labels more effectively and accurately, we\npropose GraftNet, which is a customizable tree-like network with its trunk\npretrained with a dynamic graph for generic feature extraction, and branches\nseparately trained on sub-datasets with single label to improve accuracy.\nGraftNet could reduce cost, increase flexibility, and incrementally handle new\nlabels. Experimental results show that it has good performance on our human\nattributes recognition task, which is fine-grained multi-label classification.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:08:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Jia", "Chunhua", ""], ["Zhang", "Lei", ""], ["Huang", "Hui", ""], ["Cai", "Weiwei", ""], ["Hu", "Hao", ""], ["Adivarekar", "Rohan", ""]]}, {"id": "2004.12724", "submitter": "Marco Toldo", "authors": "Teo Spadotto, Marco Toldo, Umberto Michieli and Pietro Zanuttigh", "title": "Unsupervised Domain Adaptation with Multiple Domain Discriminators and\n  Adaptive Self-Training", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) aims at improving the generalization\ncapability of a model trained on a source domain to perform well on a target\ndomain for which no labeled data is available. In this paper, we consider the\nsemantic segmentation of urban scenes and we propose an approach to adapt a\ndeep neural network trained on synthetic data to real scenes addressing the\ndomain shift between the two different data distributions. We introduce a novel\nUDA framework where a standard supervised loss on labeled synthetic data is\nsupported by an adversarial module and a self-training strategy aiming at\naligning the two domain distributions. The adversarial module is driven by a\ncouple of fully convolutional discriminators dealing with different domains:\nthe first discriminates between ground truth and generated maps, while the\nsecond between segmentation maps coming from synthetic or real world data. The\nself-training module exploits the confidence estimated by the discriminators on\nunlabeled data to select the regions used to reinforce the learning process.\nFurthermore, the confidence is thresholded with an adaptive mechanism based on\nthe per-class overall confidence. Experimental results prove the effectiveness\nof the proposed strategy in adapting a segmentation network trained on\nsynthetic datasets like GTA5 and SYNTHIA, to real world datasets like\nCityscapes and Mapillary.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:48:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Spadotto", "Teo", ""], ["Toldo", "Marco", ""], ["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2004.12725", "submitter": "Xi Li", "authors": "Yongjian Fu, Xintian Wu, Xi Li, Zhijie Pan, Daxin Luo", "title": "Semantic Neighborhood-Aware Deep Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2991510", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from many other attributes, facial expression can change in a\ncontinuous way, and therefore, a slight semantic change of input should also\nlead to the output fluctuation limited in a small scale. This consistency is\nimportant. However, current Facial Expression Recognition (FER) datasets may\nhave the extreme imbalance problem, as well as the lack of data and the\nexcessive amounts of noise, hindering this consistency and leading to a\nperformance decreasing when testing. In this paper, we not only consider the\nprediction accuracy on sample points, but also take the neighborhood smoothness\nof them into consideration, focusing on the stability of the output with\nrespect to slight semantic perturbations of the input. A novel method is\nproposed to formulate semantic perturbation and select unreliable samples\nduring training, reducing the bad effect of them. Experiments show the\neffectiveness of the proposed method and state-of-the-art results are reported,\ngetting closer to an upper limit than the state-of-the-art methods by a factor\nof 30\\% in AffectNet, the largest in-the-wild FER database by now.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:48:17 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fu", "Yongjian", ""], ["Wu", "Xintian", ""], ["Li", "Xi", ""], ["Pan", "Zhijie", ""], ["Luo", "Daxin", ""]]}, {"id": "2004.12729", "submitter": "Kilian Kleeberger", "authors": "Kilian Kleeberger and Marco F. Huber", "title": "Single Shot 6D Object Pose Estimation", "comments": "Accepted at 2020 IEEE International Conference on Robotics and\n  Automation (ICRA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel single shot approach for 6D object pose\nestimation of rigid objects based on depth images. For this purpose, a fully\nconvolutional neural network is employed, where the 3D input data is spatially\ndiscretized and pose estimation is considered as a regression task that is\nsolved locally on the resulting volume elements. With 65 fps on a GPU, our\nObject Pose Network (OP-Net) is extremely fast, is optimized end-to-end, and\nestimates the 6D pose of multiple objects in the image simultaneously. Our\napproach does not require manually 6D pose-annotated real-world datasets and\ntransfers to the real world, although being entirely trained on synthetic data.\nThe proposed method is evaluated on public benchmark datasets, where we can\ndemonstrate that state-of-the-art methods are significantly outperformed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:59:11 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kleeberger", "Kilian", ""], ["Huber", "Marco F.", ""]]}, {"id": "2004.12730", "submitter": "Yanmin Wu", "authors": "Yanmin Wu, Yunzhou Zhang, Delong Zhu, Yonghui Feng, Sonya Coleman and\n  Dermot Kerr", "title": "EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data\n  Association", "comments": "Accepted to IROS 2020. Project Page:\n  https://yanmin-wu.github.io/project/eaoslam/; Code:\n  https://github.com/yanmin-wu/EAO-SLAM", "journal-ref": "2020 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Las Vegas, NV, USA, 2020, pp. 4966-4973", "doi": "10.1109/IROS45743.2020.9341757", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-level data association and pose estimation play a fundamental role in\nsemantic SLAM, which remain unsolved due to the lack of robust and accurate\nalgorithms. In this work, we propose an ensemble data associate strategy for\nintegrating the parametric and nonparametric statistic tests. By exploiting the\nnature of different statistics, our method can effectively aggregate the\ninformation of different measurements, and thus significantly improve the\nrobustness and accuracy of data association. We then present an accurate object\npose estimation framework, in which an outliers-robust centroid and scale\nestimation algorithm and an object pose initialization algorithm are developed\nto help improve the optimality of pose estimation results. Furthermore, we\nbuild a SLAM system that can generate semi-dense or lightweight object-oriented\nmaps with a monocular camera. Extensive experiments are conducted on three\npublicly available datasets and a real scenario. The results show that our\napproach significantly outperforms state-of-the-art techniques in accuracy and\nrobustness. The source code is available on:\nhttps://github.com/yanmin-wu/EAO-SLAM.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:59:28 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 07:36:13 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wu", "Yanmin", ""], ["Zhang", "Yunzhou", ""], ["Zhu", "Delong", ""], ["Feng", "Yonghui", ""], ["Coleman", "Sonya", ""], ["Kerr", "Dermot", ""]]}, {"id": "2004.12769", "submitter": "Animesh Singh", "authors": "Animesh Singh, Ritesh Sarkhel, Nibaran Das, Mahantapas Kundu, Mita\n  Nasipuri", "title": "A Skip-connected Multi-column Network for Isolated Handwritten Bangla\n  Character and Digit recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding local invariant patterns in handwrit-ten characters and/or digits for\noptical character recognition is a difficult task. Variations in writing styles\nfrom one person to another make this task challenging. We have proposed a\nnon-explicit feature extraction method using a multi-scale multi-column skip\nconvolutional neural network in this work. Local and global features extracted\nfrom different layers of the proposed architecture are combined to derive the\nfinal feature descriptor encoding a character or digit image. Our method is\nevaluated on four publicly available datasets of isolated handwritten Bangla\ncharacters and digits. Exhaustive comparative analysis against contemporary\nmethods establishes the efficacy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:18:58 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Singh", "Animesh", ""], ["Sarkhel", "Ritesh", ""], ["Das", "Nibaran", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2004.12771", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Vaisakh Shaj and R. Venkatesh Babu", "title": "Adversarial Fooling Beyond \"Flipping the Label\"", "comments": "CVPR-AMLCV-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in CNNs have shown remarkable achievements in various\nCV/AI applications. Though CNNs show near human or better than human\nperformance in many critical tasks, they are quite vulnerable to adversarial\nattacks. These attacks are potentially dangerous in real-life deployments.\nThough there have been many adversarial attacks proposed in recent years, there\nis no proper way of quantifying the effectiveness of these attacks. As of\ntoday, mere fooling rate is used for measuring the susceptibility of the\nmodels, or the effectiveness of adversarial attacks. Fooling rate just\nconsiders label flipping and does not consider the cost of such flipping, for\ninstance, in some deployments, flipping between two species of dogs may not be\nas severe as confusing a dog category with that of a vehicle. Therefore, the\nmetric to quantify the vulnerability of the models should capture the severity\nof the flipping as well. In this work we first bring out the drawbacks of the\nexisting evaluation and propose novel metrics to capture various aspects of the\nfooling. Further, for the first time, we present a comprehensive analysis of\nseveral important adversarial attacks over a set of distinct CNN architectures.\nWe believe that the presented analysis brings valuable insights about the\ncurrent adversarial attacks and the CNN models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:21:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Shaj", "Vaisakh", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2004.12776", "submitter": "Rui Xu", "authors": "Rui Xu and Tiantian Liu and Xinchen Ye and Yen-Wei Chen", "title": "Boosting Connectivity in Retinal Vessel Segmentation via a Recursive\n  Semantics-Guided Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep learning based methods have been proposed for retinal vessel\nsegmentation, however few of them focus on the connectivity of segmented\nvessels, which is quite important for a practical computer-aided diagnosis\nsystem on retinal images. In this paper, we propose an efficient network to\naddress this problem. A U-shape network is enhanced by introducing a\nsemantics-guided module, which integrates the enriched semantics information to\nshallow layers for guiding the network to explore more powerful features.\nBesides, a recursive refinement iteratively applies the same network over the\nprevious segmentation results for progressively boosting the performance while\nincreasing no extra network parameters. The carefully designed recursive\nsemantics-guided network has been extensively evaluated on several public\ndatasets. Experimental results have shown the efficiency of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:18:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Xu", "Rui", ""], ["Liu", "Tiantian", ""], ["Ye", "Xinchen", ""], ["Chen", "Yen-Wei", ""]]}, {"id": "2004.12786", "submitter": "Tyng-Luh Liu", "authors": "Chun-Fu Yeh, Hsien-Tzu Cheng, Andy Wei, Hsin-Ming Chen, Po-Chen Kuo,\n  Keng-Chi Liu, Mong-Chi Ko, Ray-Jade Chen, Po-Chang Lee, Jen-Hsiang Chuang,\n  Chi-Mai Chen, Yi-Chang Chen, Wen-Jeng Lee, Ning Chien, Jo-Yu Chen, Yu-Sen\n  Huang, Yu-Chien Chang, Yu-Cheng Huang, Nai-Kuan Chou, Kuan-Hua Chao, Yi-Chin\n  Tu, Yeun-Chung Chang, Tyng-Luh Liu", "title": "A Cascaded Learning Strategy for Robust COVID-19 Pneumonia Chest X-Ray\n  Screening", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a comprehensive screening platform for the COVID-19 (a.k.a.,\nSARS-CoV-2) pneumonia. The proposed AI-based system works on chest x-ray (CXR)\nimages to predict whether a patient is infected with the COVID-19 disease.\nAlthough the recent international joint effort on making the availability of\nall sorts of open data, the public collection of CXR images is still relatively\nsmall for reliably training a deep neural network (DNN) to carry out COVID-19\nprediction. To better address such inefficiency, we design a cascaded learning\nstrategy to improve both the sensitivity and the specificity of the resulting\nDNN classification model. Our approach leverages a large CXR image dataset of\nnon-COVID-19 pneumonia to generalize the original well-trained classification\nmodel via a cascaded learning scheme. The resulting screening system is shown\nto achieve good classification performance on the expanded dataset, including\nthose newly added COVID-19 CXR images.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:44:51 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 09:46:13 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Yeh", "Chun-Fu", ""], ["Cheng", "Hsien-Tzu", ""], ["Wei", "Andy", ""], ["Chen", "Hsin-Ming", ""], ["Kuo", "Po-Chen", ""], ["Liu", "Keng-Chi", ""], ["Ko", "Mong-Chi", ""], ["Chen", "Ray-Jade", ""], ["Lee", "Po-Chang", ""], ["Chuang", "Jen-Hsiang", ""], ["Chen", "Chi-Mai", ""], ["Chen", "Yi-Chang", ""], ["Lee", "Wen-Jeng", ""], ["Chien", "Ning", ""], ["Chen", "Jo-Yu", ""], ["Huang", "Yu-Sen", ""], ["Chang", "Yu-Chien", ""], ["Huang", "Yu-Cheng", ""], ["Chou", "Nai-Kuan", ""], ["Chao", "Kuan-Hua", ""], ["Tu", "Yi-Chin", ""], ["Chang", "Yeun-Chung", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "2004.12805", "submitter": "Takato Yasuno", "authors": "Takato Yasuno, Nakajima Michihiro, and Noda Kazuhiro", "title": "Per-pixel Classification Rebar Exposures in Bridge Eye-inspection", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient inspection and accurate diagnosis are required for civil\ninfrastructures with 50 years since completion. Especially in municipalities,\nthe shortage of technical staff and budget constraints on repair expenses have\nbecome a critical problem. If we can detect damaged photos automatically\nper-pixels from the record of the inspection record in addition to the 5-step\njudgment and countermeasure classification of eye-inspection vision, then it is\npossible that countermeasure information can be provided more flexibly, whether\nwe need to repair and how large the expose of damage interest. A piece of\ndamage photo is often sparse as long as it is not zoomed around damage, exactly\nthe range where the detection target is photographed, is at most only 1%.\nGenerally speaking, rebar exposure is frequently occurred, and there are many\nopportunities to judge repair measure. In this paper, we propose three damage\ndetection methods of transfer learning which enables semantic segmentation in\nan image with low pixels using damaged photos of human eye-inspection. Also, we\ntried to create a deep convolutional network from scratch with the\npreprocessing that random crops with rotations are generated. In fact, we show\nthe results applied this method using the 208 rebar exposed images on the 106\nreal-world bridges. Finally, future tasks of damage detection modeling are\nmentioned.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:28:42 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Yasuno", "Takato", ""], ["Michihiro", "Nakajima", ""], ["Kazuhiro", "Noda", ""]]}, {"id": "2004.12807", "submitter": "Friso Heslinga", "authors": "Friso G. Heslinga, Mark Alberti, Josien P.W. Pluim, Javier Cabrerizo,\n  Mitko Veta", "title": "Quantifying Graft Detachment after Descemet's Membrane Endothelial\n  Keratoplasty with Deep Convolutional Neural Networks", "comments": "To be published in Translational Vision Science & Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We developed a method to automatically locate and quantify graft\ndetachment after Descemet's Membrane Endothelial Keratoplasty (DMEK) in\nAnterior Segment Optical Coherence Tomography (AS-OCT) scans. Methods: 1280\nAS-OCT B-scans were annotated by a DMEK expert. Using the annotations, a deep\nlearning pipeline was developed to localize scleral spur, center the AS-OCT\nB-scans and segment the detached graft sections. Detachment segmentation model\nperformance was evaluated per B-scan by comparing (1) length of detachment and\n(2) horizontal projection of the detached sections with the expert annotations.\nHorizontal projections were used to construct graft detachment maps. All final\nevaluations were done on a test set that was set apart during training of the\nmodels. A second DMEK expert annotated the test set to determine inter-rater\nperformance. Results: Mean scleral spur localization error was 0.155 mm,\nwhereas the inter-rater difference was 0.090 mm. The estimated graft detachment\nlengths were in 69% of the cases within a 10-pixel (~150{\\mu}m) difference from\nthe ground truth (77% for the second DMEK expert). Dice scores for the\nhorizontal projections of all B-scans with detachments were 0.896 and 0.880 for\nour model and the second DMEK expert respectively. Conclusion: Our deep\nlearning model can be used to automatically and instantly localize graft\ndetachment in AS-OCT B-scans. Horizontal detachment projections can be\ndetermined with the same accuracy as a human DMEK expert, allowing for the\nconstruction of accurate graft detachment maps. Translational Relevance:\nAutomated localization and quantification of graft detachment can support DMEK\nresearch and standardize clinical decision making.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 08:01:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Heslinga", "Friso G.", ""], ["Alberti", "Mark", ""], ["Pluim", "Josien P. W.", ""], ["Cabrerizo", "Javier", ""], ["Veta", "Mitko", ""]]}, {"id": "2004.12811", "submitter": "Zhi-Song Liu", "authors": "Zhi-Song Liu, Wan-Chi Siu, Li-Wen Wang, Chu-Tak Li, Marie-Paule Cani,\n  Yui-Lam Chan", "title": "Unsupervised Real Image Super-Resolution via Generative Variational\n  AutoEncoder", "comments": "9 pages, 7 figures, CVPR2020 NTIRE2020 Real Image Super-Resolution\n  Challenge", "journal-ref": "2020 IEEE conference on Computer Vision and Pattern Recognition\n  Workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefited from the deep learning, image Super-Resolution has been one of the\nmost developing research fields in computer vision. Depending upon whether\nusing a discriminator or not, a deep convolutional neural network can provide\nan image with high fidelity or better perceptual quality. Due to the lack of\nground truth images in real life, people prefer a photo-realistic image with\nlow fidelity to a blurry image with high fidelity. In this paper, we revisit\nthe classic example based image super-resolution approaches and come up with a\nnovel generative model for perceptual image super-resolution. Given that real\nimages contain various noise and artifacts, we propose a joint image denoising\nand super-resolution model via Variational AutoEncoder. We come up with a\nconditional variational autoencoder to encode the reference for dense feature\nvector which can then be transferred to the decoder for target image denoising.\nWith the aid of the discriminator, an additional overhead of super-resolution\nsubnetwork is attached to super-resolve the denoised image with photo-realistic\nvisual quality. We participated the NTIRE2020 Real Image Super-Resolution\nChallenge. Experimental results show that by using the proposed approach, we\ncan obtain enlarged images with clean and pleasant features compared to other\nsupervised methods. We also compared our approach with state-of-the-art methods\non various datasets to demonstrate the efficiency of our proposed unsupervised\nsuper-resolution model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:49:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liu", "Zhi-Song", ""], ["Siu", "Wan-Chi", ""], ["Wang", "Li-Wen", ""], ["Li", "Chu-Tak", ""], ["Cani", "Marie-Paule", ""], ["Chan", "Yui-Lam", ""]]}, {"id": "2004.12819", "submitter": "Zezhou Cheng", "authors": "Zezhou Cheng, Saadia Gabriel, Pankaj Bhambhani, Daniel Sheldon,\n  Subhransu Maji, Andrew Laughlin, David Winkler", "title": "Detecting and Tracking Communal Bird Roosts in Weather Radar Data", "comments": "9 pages, 6 figures, AAAI 2020 (AI for Social Impact Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The US weather radar archive holds detailed information about biological\nphenomena in the atmosphere over the last 20 years. Communally roosting birds\ncongregate in large numbers at nighttime roosting locations, and their morning\nexodus from the roost is often visible as a distinctive pattern in radar\nimages. This paper describes a machine learning system to detect and track\nroost signatures in weather radar data. A significant challenge is that labels\nwere collected opportunistically from previous research studies and there are\nsystematic differences in labeling style. We contribute a latent variable model\nand EM algorithm to learn a detection model together with models of labeling\nstyles for individual annotators. By properly accounting for these variations\nwe learn a significantly more accurate detector. The resulting system detects\npreviously unknown roosting locations and provides comprehensive\nspatio-temporal data about roosts across the US. This data will provide\nbiologists important information about the poorly understood phenomena of\nbroad-scale habitat use and movements of communally roosting birds during the\nnon-breeding season.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 02:40:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Cheng", "Zezhou", ""], ["Gabriel", "Saadia", ""], ["Bhambhani", "Pankaj", ""], ["Sheldon", "Daniel", ""], ["Maji", "Subhransu", ""], ["Laughlin", "Andrew", ""], ["Winkler", "David", ""]]}, {"id": "2004.12823", "submitter": "Gianluca Maguolo", "authors": "Gianluca Maguolo, Loris Nanni", "title": "A Critic Evaluation of Methods for COVID-19 Automatic Detection from\n  X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare and evaluate different testing protocols used for\nautomatic COVID-19 diagnosis from X-Ray images in the recent literature. We\nshow that similar results can be obtained using X-Ray images that do not\ncontain most of the lungs. We are able to remove the lungs from the images by\nturning to black the center of the X-Ray scan and training our classifiers only\non the outer part of the images. Hence, we deduce that several testing\nprotocols for the recognition are not fair and that the neural networks are\nlearning patterns in the dataset that are not correlated to the presence of\nCOVID-19. Finally, we show that creating a fair testing protocol is a\nchallenging task, and we provide a method to measure how fair a specific\ntesting protocol is. In the future research we suggest to check the fairness of\na testing protocol using our tools and we encourage researchers to look for\nbetter techniques than the ones that we propose.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:05:36 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 02:17:02 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 01:49:41 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 00:36:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Maguolo", "Gianluca", ""], ["Nanni", "Loris", ""]]}, {"id": "2004.12837", "submitter": "Matteo Polsinelli", "authors": "Matteo Polsinelli, Luigi Cinque, Giuseppe Placidi", "title": "A Light CNN for detecting COVID-19 from CT scans of the chest", "comments": null, "journal-ref": "Pattern Recognition Letters. 140 (2020) 95-100", "doi": "10.1016/j.patrec.2020.10.001", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OVID-19 is a world-wide disease that has been declared as a pandemic by the\nWorld Health Organization. Computer Tomography (CT) imaging of the chest seems\nto be a valid diagnosis tool to detect COVID-19 promptly and to control the\nspread of the disease. Deep Learning has been extensively used in medical\nimaging and convolutional neural networks (CNNs) have been also used for\nclassification of CT images. We propose a light CNN design based on the model\nof the SqueezeNet, for the efficient discrimination of COVID-19 CT images with\nother CT images (community-acquired pneumonia and/or healthy images). On the\ntested datasets, the proposed modified SqueezeNet CNN achieved 83.00\\% of\naccuracy, 85.00\\% of sensitivity, 81.00\\% of specificity, 81.73\\% of precision\nand 0.8333 of F1Score in a very efficient way (7.81 seconds medium-end laptot\nwithout GPU acceleration). Besides performance, the average classification time\nis very competitive with respect to more complex CNN designs, thus allowing its\nusability also on medium power computers. In the next future we aim at\nimproving the performances of the method along two directions: 1) by increasing\nthe training dataset (as soon as other CT images will be available); 2) by\nintroducing an efficient pre-processing strategy.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:58:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Polsinelli", "Matteo", ""], ["Cinque", "Luigi", ""], ["Placidi", "Giuseppe", ""]]}, {"id": "2004.12847", "submitter": "Haoran Dou", "authors": "Haoran Dou, Davood Karimi, Caitlin K. Rollins, Cynthia M. Ortinau,\n  Lana Vasung, Clemente Velasco-Annis, Abdelhakim Ouaalam, Xin Yang, Dong Ni,\n  and Ali Gholipour", "title": "A Deep Attentive Convolutional Neural Network for Automatic Cortical\n  Plate Segmentation in Fetal MRI", "comments": "Accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fetal cortical plate segmentation is essential in quantitative analysis of\nfetal brain maturation and cortical folding. Manual segmentation of the\ncortical plate, or manual refinement of automatic segmentations is tedious and\ntime-consuming. Automatic segmentation of the cortical plate, on the other\nhand, is challenged by the relatively low resolution of the reconstructed fetal\nbrain MRI scans compared to the thin structure of the cortical plate, partial\nvoluming, and the wide range of variations in the morphology of the cortical\nplate as the brain matures during gestation. To reduce the burden of manual\nrefinement of segmentations, we have developed a new and powerful deep learning\nsegmentation method. Our method exploits new deep attentive modules with mixed\nkernel convolutions within a fully convolutional neural network architecture\nthat utilizes deep supervision and residual connections. We evaluated our\nmethod quantitatively based on several performance measures and expert\nevaluations. Results show that our method outperforms several state-of-the-art\ndeep models for segmentation, as well as a state-of-the-art multi-atlas\nsegmentation technique. We achieved average Dice similarity coefficient of\n0.87, average Hausdorff distance of 0.96 mm, and average symmetric surface\ndifference of 0.28 mm on reconstructed fetal brain MRI scans of fetuses scanned\nin the gestational age range of 16 to 39 weeks. With a computation time of less\nthan 1 minute per fetal brain, our method can facilitate and accelerate\nlarge-scale studies on normal and altered fetal brain cortical maturation and\nfolding.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:55:22 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 21:19:37 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 13:28:34 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Dou", "Haoran", ""], ["Karimi", "Davood", ""], ["Rollins", "Caitlin K.", ""], ["Ortinau", "Cynthia M.", ""], ["Vasung", "Lana", ""], ["Velasco-Annis", "Clemente", ""], ["Ouaalam", "Abdelhakim", ""], ["Yang", "Xin", ""], ["Ni", "Dong", ""], ["Gholipour", "Ali", ""]]}, {"id": "2004.12852", "submitter": "Maria Vakalopoulou", "authors": "Guillaume Chassagnon, Maria Vakalopoulou, Enzo Battistella, Stergios\n  Christodoulidis, Trieu-Nghi Hoang-Thi, Severine Dangeard, Eric Deutsch,\n  Fabrice Andre, Enora Guillo, Nara Halm, Stefany El Hajj, Florian Bompard,\n  Sophie Neveu, Chahinez Hani, Ines Saab, Alienor Campredon, Hasmik Koulakian,\n  Souhail Bennani, Gael Freche, Aurelien Lombard, Laure Fournier, Hippolyte\n  Monnier, Teodor Grand, Jules Gregory, Antoine Khalil, Elyas Mahdjoub,\n  Pierre-Yves Brillet, Stephane Tran Ba, Valerie Bousson, Marie-Pierre Revel,\n  Nikos Paragios", "title": "AI-Driven CT-based quantification, staging and short-term outcome\n  prediction of COVID-19 pneumonia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest computed tomography (CT) is widely used for the management of\nCoronavirus disease 2019 (COVID-19) pneumonia because of its availability and\nrapidity. The standard of reference for confirming COVID-19 relies on\nmicrobiological tests but these tests might not be available in an emergency\nsetting and their results are not immediately available, contrary to CT. In\naddition to its role for early diagnosis, CT has a prognostic role by allowing\nvisually evaluating the extent of COVID-19 lung abnormalities. The objective of\nthis study is to address prediction of short-term outcomes, especially need for\nmechanical ventilation. In this multi-centric study, we propose an end-to-end\nartificial intelligence solution for automatic quantification and prognosis\nassessment by combining automatic CT delineation of lung disease meeting\nperformance of experts and data-driven identification of biomarkers for its\nprognosis. AI-driven combination of variables with CT-based biomarkers offers\nperspectives for optimal patient management given the shortage of intensive\ncare beds and ventilators.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 12:24:08 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chassagnon", "Guillaume", ""], ["Vakalopoulou", "Maria", ""], ["Battistella", "Enzo", ""], ["Christodoulidis", "Stergios", ""], ["Hoang-Thi", "Trieu-Nghi", ""], ["Dangeard", "Severine", ""], ["Deutsch", "Eric", ""], ["Andre", "Fabrice", ""], ["Guillo", "Enora", ""], ["Halm", "Nara", ""], ["Hajj", "Stefany El", ""], ["Bompard", "Florian", ""], ["Neveu", "Sophie", ""], ["Hani", "Chahinez", ""], ["Saab", "Ines", ""], ["Campredon", "Alienor", ""], ["Koulakian", "Hasmik", ""], ["Bennani", "Souhail", ""], ["Freche", "Gael", ""], ["Lombard", "Aurelien", ""], ["Fournier", "Laure", ""], ["Monnier", "Hippolyte", ""], ["Grand", "Teodor", ""], ["Gregory", "Jules", ""], ["Khalil", "Antoine", ""], ["Mahdjoub", "Elyas", ""], ["Brillet", "Pierre-Yves", ""], ["Ba", "Stephane Tran", ""], ["Bousson", "Valerie", ""], ["Revel", "Marie-Pierre", ""], ["Paragios", "Nikos", ""]]}, {"id": "2004.12880", "submitter": "Vittorio Mazzia", "authors": "Vittorio Mazzia, Aleem Khaliq, Marcello Chiaberge", "title": "Improvement in Land Cover and Crop Classification based on Temporal\n  Features Learning from Sentinel-2 Data Using Recurrent-Convolutional Neural\n  Network (R-CNN)", "comments": null, "journal-ref": "Appl. Sci. 2020, 10(1), 238", "doi": "10.3390/app10010238", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing spatial and temporal resolution of globally available\nsatellite images, such as provided by Sentinel-2, creates new possibilities for\nresearchers to use freely available multi-spectral optical images, with\ndecametric spatial resolution and more frequent revisits for remote sensing\napplications such as land cover and crop classification (LC&CC), agricultural\nmonitoring and management, environment monitoring. Existing solutions dedicated\nto cropland mapping can be categorized based on per-pixel based and\nobject-based. However, it is still challenging when more classes of\nagricultural crops are considered at a massive scale. In this paper, a novel\nand optimal deep learning model for pixel-based LC&CC is developed and\nimplemented based on Recurrent Neural Networks (RNN) in combination with\nConvolutional Neural Networks (CNN) using multi-temporal sentinel-2 imagery of\ncentral north part of Italy, which has diverse agricultural system dominated by\neconomic crop types. The proposed methodology is capable of automated feature\nextraction by learning time correlation of multiple images, which reduces\nmanual feature engineering and modeling crop phenological stages. Fifteen\nclasses, including major agricultural crops, were considered in this study. We\nalso tested other widely used traditional machine learning algorithms for\ncomparison such as support vector machine SVM, random forest (RF), Kernal SVM,\nand gradient boosting machine, also called XGBoost. The overall accuracy\nachieved by our proposed Pixel R-CNN was 96.5%, which showed considerable\nimprovements in comparison with existing mainstream methods. This study showed\nthat Pixel R-CNN based model offers a highly accurate way to assess and employ\ntime-series data for multi-temporal classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:39:50 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 10:28:07 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Mazzia", "Vittorio", ""], ["Khaliq", "Aleem", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2004.12886", "submitter": "Miad Zandavi Dr", "authors": "Seid Miad Zandavi, Vera Chung, Ali Anaissi", "title": "Control Design of Autonomous Drone Using Deep Learning Based Image\n  Understanding Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new framework to use images as the inputs for the\ncontroller to have autonomous flight, considering the noisy indoor environment\nand uncertainties. A new Proportional-Integral-Derivative-Accelerated (PIDA)\ncontrol with a derivative filter is proposed to improves drone/quadcopter\nflight stability within a noisy environment and enables autonomous flight using\nobject and depth detection techniques. The mathematical model is derived from\nan accurate model with a high level of fidelity by addressing the problems of\nnon-linearity, uncertainties, and coupling. The proposed PIDA controller is\ntuned by Stochastic Dual Simplex Algorithm (SDSA) to support autonomous flight.\nThe simulation results show that adapting the deep learning-based image\nunderstanding techniques (RetinaNet ant colony detection and PSMNet) to the\nproposed controller can enable the generation and tracking of the desired point\nin the presence of environmental disturbances.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:50:04 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 00:52:47 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 01:23:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zandavi", "Seid Miad", ""], ["Chung", "Vera", ""], ["Anaissi", "Ali", ""]]}, {"id": "2004.12906", "submitter": "Julius von K\\\"ugelgen", "authors": "Julius von K\\\"ugelgen, Ivan Ustyuzhaninov, Peter Gehler, Matthias\n  Bethge, Bernhard Sch\\\"olkopf", "title": "Towards causal generative scene models via competition of experts", "comments": "Presented at the ICLR 2020 workshop \"Causal learning for decision\n  making\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning how to model complex scenes in a modular way with recombinable\ncomponents is a pre-requisite for higher-order reasoning and acting in the\nphysical world. However, current generative models lack the ability to capture\nthe inherently compositional and layered nature of visual scenes. While recent\nwork has made progress towards unsupervised learning of object-based scene\nrepresentations, most models still maintain a global representation space\n(i.e., objects are not explicitly separated), and cannot generate scenes with\nnovel object arrangement and depth ordering. Here, we present an alternative\napproach which uses an inductive bias encouraging modularity by training an\nensemble of generative models (experts). During training, experts compete for\nexplaining parts of a scene, and thus specialise on different object classes,\nwith objects being identified as parts that re-occur across multiple scenes.\nOur model allows for controllable sampling of individual objects and\nrecombination of experts in physically plausible ways. In contrast to other\nmethods, depth layering and occlusion are handled correctly, moving this\napproach closer to a causal generative scene model. Experiments on simple toy\ndata qualitatively demonstrate the conceptual advantages of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:10:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["von K\u00fcgelgen", "Julius", ""], ["Ustyuzhaninov", "Ivan", ""], ["Gehler", "Peter", ""], ["Bethge", "Matthias", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2004.12943", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Nuno Vasconcelos, Ishan Misra", "title": "Audio-Visual Instance Discrimination with Cross-Modal Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised learning approach to learn audio-visual\nrepresentations from video and audio. Our method uses contrastive learning for\ncross-modal discrimination of video from audio and vice-versa. We show that\noptimizing for cross-modal discrimination, rather than within-modal\ndiscrimination, is important to learn good representations from video and\naudio. With this simple but powerful insight, our method achieves highly\ncompetitive performance when finetuned on action recognition tasks.\nFurthermore, while recent work in contrastive learning defines positive and\nnegative samples as individual instances, we generalize this definition by\nexploring cross-modal agreement. We group together multiple instances as\npositives by measuring their similarity in both the video and audio feature\nspaces. Cross-modal agreement creates better positive and negative sets, which\nallows us to calibrate visual similarities by seeking within-modal\ndiscrimination of positive instances, and achieve significant gains on\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:59:49 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 20:04:40 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 20:14:23 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Morgado", "Pedro", ""], ["Vasconcelos", "Nuno", ""], ["Misra", "Ishan", ""]]}, {"id": "2004.12989", "submitter": "Stefan Popov", "authors": "Stefan Popov and Pablo Bauszat and Vittorio Ferrari", "title": "CoReNet: Coherent 3D scene reconstruction from a single RGB image", "comments": "ECCV 2020, camera ready, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning techniques have allowed recent work to reconstruct\nthe shape of a single object given only one RBG image as input. Building on\ncommon encoder-decoder architectures for this task, we propose three\nextensions: (1) ray-traced skip connections that propagate local 2D information\nto the output 3D volume in a physically correct manner; (2) a hybrid 3D volume\nrepresentation that enables building translation equivariant models, while at\nthe same time encoding fine object details without an excessive memory\nfootprint; (3) a reconstruction loss tailored to capture overall object\ngeometry. Furthermore, we adapt our model to address the harder task of\nreconstructing multiple objects from a single image. We reconstruct all objects\njointly in one pass, producing a coherent reconstruction, where all objects\nlive in a single consistent 3D coordinate frame relative to the camera and they\ndo not intersect in 3D space. We also handle occlusions and resolve them by\nhallucinating the missing object parts in the 3D volume. We validate the impact\nof our contributions experimentally both on synthetic data from ShapeNet as\nwell as real images from Pix3D. Our method improves over the state-of-the-art\nsingle-object methods on both datasets. Finally, we evaluate performance\nquantitatively on multiple object reconstruction with synthetic scenes\nassembled from ShapeNet objects.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:53:07 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 15:59:48 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Popov", "Stefan", ""], ["Bauszat", "Pablo", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2004.12992", "submitter": "Yang Zhou", "authors": "Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos\n  Kalogerakis, Dingzeyu Li", "title": "MakeItTalk: Speaker-Aware Talking-Head Animation", "comments": "SIGGRAPH Asia 2020, 15 pages, 13 figures", "journal-ref": null, "doi": "10.1145/3414685.3417774", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that generates expressive talking heads from a single\nfacial image with audio as the only input. In contrast to previous approaches\nthat attempt to learn direct mappings from audio to raw pixels or points for\ncreating talking faces, our method first disentangles the content and speaker\ninformation in the input audio signal. The audio content robustly controls the\nmotion of lips and nearby facial regions, while the speaker information\ndetermines the specifics of facial expressions and the rest of the talking head\ndynamics. Another key component of our method is the prediction of facial\nlandmarks reflecting speaker-aware dynamics. Based on this intermediate\nrepresentation, our method is able to synthesize photorealistic videos of\nentire talking heads with full range of motion and also animate artistic\npaintings, sketches, 2D cartoon characters, Japanese mangas, stylized\ncaricatures in a single unified framework. We present extensive quantitative\nand qualitative evaluation of our method, in addition to user studies,\ndemonstrating generated talking heads of significantly higher quality compared\nto prior state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:56:15 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 16:31:29 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 17:57:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhou", "Yang", ""], ["Han", "Xintong", ""], ["Shechtman", "Eli", ""], ["Echevarria", "Jose", ""], ["Kalogerakis", "Evangelos", ""], ["Li", "Dingzeyu", ""]]}, {"id": "2004.13004", "submitter": "Saurabh Jha", "authors": "Saurabh Jha, Shengkun Cui, Subho S. Banerjee, Timothy Tsai, Zbigniew\n  Kalbarczyk, Ravi Iyer", "title": "ML-driven Malware that Targets AV Safety", "comments": "Accepted for DSN 2020", "journal-ref": "2020 50th Annual IEEE/IFIP International Conference on Dependable\n  Systems and Networks", "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the safety of autonomous vehicles (AVs) is critical for their mass\ndeployment and public adoption. However, security attacks that violate safety\nconstraints and cause accidents are a significant deterrent to achieving public\ntrust in AVs, and that hinders a vendor's ability to deploy AVs. Creating a\nsecurity hazard that results in a severe safety compromise (for example, an\naccident) is compelling from an attacker's perspective. In this paper, we\nintroduce an attack model, a method to deploy the attack in the form of smart\nmalware, and an experimental evaluation of its impact on production-grade\nautonomous driving software. We find that determining the time interval during\nwhich to launch the attack is{ critically} important for causing safety hazards\n(such as collisions) with a high degree of success. For example, the smart\nmalware caused 33X more forced emergency braking than random attacks did, and\naccidents in 52.6% of the driving simulations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:29:59 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 01:42:56 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jha", "Saurabh", ""], ["Cui", "Shengkun", ""], ["Banerjee", "Subho S.", ""], ["Tsai", "Timothy", ""], ["Kalbarczyk", "Zbigniew", ""], ["Iyer", "Ravi", ""]]}, {"id": "2004.13013", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Harnessing adversarial examples with a surprisingly simple defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a very simple method to defend against adversarial examples. The\nbasic idea is to raise the slope of the ReLU function at the test time.\nExperiments over MNIST and CIFAR-10 datasets demonstrate the effectiveness of\nthe proposed defense against a number of strong attacks in both untargeted and\ntargeted settings. While perhaps not as effective as the state of the art\nadversarial defenses, this approach can provide insights to understand and\nmitigate adversarial attacks. It can also be used in conjunction with other\ndefenses.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 03:09:42 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 22:49:12 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 02:52:54 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2004.13060", "submitter": "Kritik Soman", "authors": "Kritik Soman", "title": "GIMP-ML: Python Plugins for using Computer Vision Models in GIMP", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces GIMP-ML v1.1, a set of Python plugins for the widely\npopular GNU Image Manipulation Program (GIMP). It enables the use of recent\nadvances in computer vision to the conventional image editing pipeline.\nApplications from deep learning such as monocular depth estimation, semantic\nsegmentation, mask generative adversarial networks, image super-resolution,\nde-noising, de-hazing, matting, enlightening and coloring have been\nincorporated with GIMP through Python-based plugins. Additionally, operations\non images such as k-means based color clustering have also been added. GIMP-ML\nrelies on standard Python packages such as numpy, pytorch, open-cv, scipy.\nApart from these, several image manipulation techniques using these plugins\nhave been compiled and demonstrated in the YouTube channel\n(https://youtube.com/user/kritiksoman) with the objective of demonstrating the\nuse-cases for machine learning based image modification. In addition, GIMP-ML\nalso aims to bring the benefits of using deep learning networks used for\ncomputer vision tasks to routine image processing workflows. The code and\ninstallation procedure for configuring these plugins is available at\nhttps://github.com/kritiksoman/GIMP-ML.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:00:37 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 02:51:33 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 16:19:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Soman", "Kritik", ""]]}, {"id": "2004.13073", "submitter": "Matteo Stefanini", "authors": "Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "A Novel Attention-based Aggregation Function to Combine Vision and\n  Language", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint understanding of vision and language has been recently gaining a\nlot of attention in both the Computer Vision and Natural Language Processing\ncommunities, with the emergence of tasks such as image captioning, image-text\nmatching, and visual question answering. As both images and text can be encoded\nas sets or sequences of elements -- like regions and words -- proper reduction\nfunctions are needed to transform a set of encoded elements into a single\nresponse, like a classification or similarity score. In this paper, we propose\na novel fully-attentive reduction method for vision and language. Specifically,\nour approach computes a set of scores for each element of each modality\nemploying a novel variant of cross-attention, and performs a learnable and\ncross-modal reduction, which can be used for both classification and ranking.\nWe test our approach on image-text matching and visual question answering,\nbuilding fair comparisons with other reduction choices, on both COCO and VQA\n2.0 datasets. Experimentally, we demonstrate that our approach leads to a\nperformance increase on both tasks. Further, we conduct ablation studies to\nvalidate the role of each component of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:09:46 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 12:22:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Stefanini", "Matteo", ""], ["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2004.13075", "submitter": "Kim Bjerge", "authors": "Kim Bjerge, Jonathan Horsted Schougaard and Daniel Ejnar Larsen", "title": "A scalable and efficient convolutional neural network accelerator using\n  HLS for a System on Chip design", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a configurable Convolutional Neural Network Accelerator\n(CNNA) for a System on Chip design (SoC). The goal was to accelerate inference\nof different deep learning networks on an embedded SoC platform. The presented\nCNNA has a scalable architecture which uses High Level Synthesis (HLS) and\nSystemC for the hardware accelerator. It is able to accelerate any\nConvolutional Neural Network (CNN) exported from Python and supports a\ncombination of convolutional, max-pooling, and fully connected layers. A\ntraining method with fixed-point quantized weights is proposed and presented in\nthe paper. The CNNA is template-based, enabling it to scale for different\ntargets of the Xilinx Zynq platform. This approach enables design space\nexploration, which makes it possible to explore several configurations of the\nCNNA during C- and RTL-simulation, fitting it to the desired platform and\nmodel. The CNN VGG16 was used to test the solution on a Xilinx Ultra96 board\nusing PYNQ. The result gave a high level of accuracy in training with an\nauto-scaled fixed-point Q2.14 format compared to a similar floating-point\nmodel. It was able to perform inference in 2.0 seconds, while having an average\npower consumption of 2.63 W, which corresponds to a power efficiency of 6.0\nGOPS/W.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:12:22 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 06:58:55 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Bjerge", "Kim", ""], ["Schougaard", "Jonathan Horsted", ""], ["Larsen", "Daniel Ejnar", ""]]}, {"id": "2004.13076", "submitter": "Roman Pflugfelder", "authors": "Julian Pegoraro and Roman Pflugfelder", "title": "The Problem of Fragmented Occlusion in Object Detection", "comments": "accepted by the Austrian Joint Computer Vision and Robotics Workshop\n  2020 (https://acvrw20.ist.tugraz.at)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection in natural environments is still a very challenging task,\neven though deep learning has brought a tremendous improvement in performance\nover the last years. A fundamental problem of object detection based on deep\nlearning is that neither the training data nor the suggested models are\nintended for the challenge of fragmented occlusion. Fragmented occlusion is\nmuch more challenging than ordinary partial occlusion and occurs frequently in\nnatural environments such as forests. A motivating example of fragmented\nocclusion is object detection through foliage which is an essential requirement\nin green border surveillance. This paper presents an analysis of\nstate-of-the-art detectors with imagery of green borders and proposes to train\nMask R-CNN on new training data which captures explicitly the problem of\nfragmented occlusion. The results show clear improvements of Mask R-CNN with\nthis new training strategy (also against other detectors) for data showing\nslight fragmented occlusion.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:16:01 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Pegoraro", "Julian", ""], ["Pflugfelder", "Roman", ""]]}, {"id": "2004.13077", "submitter": "Boris Chidlovskii", "authors": "Assem Sadek and Boris Chidlovskii", "title": "Self-Supervised Attention Learning for Depth and Ego-motion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of depth and ego-motion estimation from image\nsequences. Recent advances in the domain propose to train a deep learning model\nfor both tasks using image reconstruction in a self-supervised manner. We\nrevise the assumptions and the limitations of the current approaches and\npropose two improvements to boost the performance of the depth and ego-motion\nestimation. We first use Lie group properties to enforce the geometric\nconsistency between images in the sequence and their reconstructions. We then\npropose a mechanism to pay an attention to image regions where the image\nreconstruction get corrupted. We show how to integrate the attention mechanism\nin the form of attention gates in the pipeline and use attention coefficients\nas a mask. We evaluate the new architecture on the KITTI datasets and compare\nit to the previous techniques. We show that our approach improves the\nstate-of-the-art results for ego-motion estimation and achieve comparable\nresults for depth estimation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:19:22 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Sadek", "Assem", ""], ["Chidlovskii", "Boris", ""]]}, {"id": "2004.13094", "submitter": "Pratyush Kumar", "authors": "Pratyush Kumar, Muktabh Mayank Srivastava", "title": "Compact retail shelf segmentation for mobile deployment", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of automation in the retail industries has rapidly increased\ndemand for applying deep learning models on mobile devices. To make the deep\nlearning models real-time on-device, a compact efficient network becomes\ninevitable. In this paper, we work on one such common problem in the retail\nindustries - Shelf segmentation. Shelf segmentation can be interpreted as a\npixel-wise classification problem, i.e., each pixel is classified as to whether\nthey belong to visible shelf edges or not. The aim is not just to segment shelf\nedges, but also to deploy the model on mobile devices. As there is no standard\nsolution for such dense classification problem on mobile devices, we look at\nsemantic segmentation architectures which can be deployed on edge. We modify\nlow-footprint semantic segmentation architectures to perform shelf\nsegmentation. In addressing this issue, we modified the famous U-net\narchitecture in certain aspects to make it fit for on-devices without impacting\nsignificant drop in accuracy and also with 15X fewer parameters. In this paper,\nwe proposed Light Weight Segmentation Network (LWSNet), a small compact model\nable to run fast on devices with limited memory and can train with less amount\n(~ 100 images) of labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:54:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Kumar", "Pratyush", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "2004.13160", "submitter": "Jie Yang", "authors": "Jie Yang and Chin-Teng Lin", "title": "Clustering via torque balance with mass and distance", "comments": "28 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouping similar objects is a fundamental tool of scientific analysis,\nubiquitous in disciplines from biology and chemistry to astronomy and pattern\nrecognition. Inspired by the torque balance that exists in gravitational\ninteractions when galaxies merge, we propose a novel clustering method based on\ntwo natural properties of the universe: mass and distance. The concept of\ntorque describing the interactions of mass and distance forms the basis of the\nproposed parameter-free clustering algorithm, which harnesses torque balance to\nrecognize any cluster, regardless of shape, size, or density. The gravitational\ninteractions govern the merger process, while the concept of torque balance\nreveals partitions that do not conform to the natural order for removal.\nExperiments on benchmark data sets show the enormous versatility of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:34:06 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yang", "Jie", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "2004.13166", "submitter": "Patrick Esser", "authors": "Patrick Esser, Robin Rombach, Bj\\\"orn Ommer", "title": "A Disentangling Invertible Interpretation Network for Explaining Latent\n  Representations", "comments": "CVPR 2020. Project Page at https://compvis.github.io/iin/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have greatly boosted performance in computer vision by\nlearning powerful representations of input data. The drawback of end-to-end\ntraining for maximal overall performance are black-box models whose hidden\nrepresentations are lacking interpretability: Since distributed coding is\noptimal for latent layers to improve their robustness, attributing meaning to\nparts of a hidden feature vector or to individual neurons is hindered. We\nformulate interpretation as a translation of hidden representations onto\nsemantic concepts that are comprehensible to the user. The mapping between both\ndomains has to be bijective so that semantic modifications in the target domain\ncorrectly alter the original representation. The proposed invertible\ninterpretation network can be transparently applied on top of existing\narchitectures with no need to modify or retrain them. Consequently, we\ntranslate an original representation to an equivalent yet interpretable one and\nbackwards without affecting the expressiveness and performance of the original.\nThe invertible interpretation network disentangles the hidden representation\ninto separate, semantically meaningful concepts. Moreover, we present an\nefficient approach to define semantic concepts by only sketching two images and\nalso an unsupervised strategy. Experimental evaluation demonstrates the wide\napplicability to interpretation of existing classification and image generation\nnetworks as well as to semantically guided image manipulation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:43:20 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Esser", "Patrick", ""], ["Rombach", "Robin", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2004.13173", "submitter": "Fangliang Bai", "authors": "Fangliang Bai, Jinchao Liu, Xiaojuan Liu, Margarita Osadchy, Chao\n  Wang, Stuart J. Gibson", "title": "LSHR-Net: a hardware-friendly solution for high-resolution computational\n  imaging using a mixed-weights neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work showed neural-network-based approaches to reconstructing images\nfrom compressively sensed measurements offer significant improvements in\naccuracy and signal compression. Such methods can dramatically boost the\ncapability of computational imaging hardware. However, to date, there have been\ntwo major drawbacks: (1) the high-precision real-valued sensing patterns\nproposed in the majority of existing works can prove problematic when used with\ncomputational imaging hardware such as a digital micromirror sampling device\nand (2) the network structures for image reconstruction involve intensive\ncomputation, which is also not suitable for hardware deployment. To address\nthese problems, we propose a novel hardware-friendly solution based on\nmixed-weights neural networks for computational imaging. In particular, learned\nbinary-weight sensing patterns are tailored to the sampling device. Moreover,\nwe proposed a recursive network structure for low-resolution image sampling and\nhigh-resolution reconstruction scheme. It reduces both the required number of\nmeasurements and reconstruction computation by operating convolution on small\nintermediate feature maps. The recursive structure further reduced the model\nsize, making the network more computationally efficient when deployed with the\nhardware. Our method has been validated on benchmark datasets and achieved the\nstate of the art reconstruction accuracy. We tested our proposed network in\nconjunction with a proof-of-concept hardware setup.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:59:51 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bai", "Fangliang", ""], ["Liu", "Jinchao", ""], ["Liu", "Xiaojuan", ""], ["Osadchy", "Margarita", ""], ["Wang", "Chao", ""], ["Gibson", "Stuart J.", ""]]}, {"id": "2004.13175", "submitter": "Mohammad Morid", "authors": "Mohammad Amin Morid, Alireza Borjali, Guilherme Del Fiol", "title": "A scoping review of transfer learning research on medical image analysis\n  using ImageNet", "comments": null, "journal-ref": "Computers in Biology and Medicine, 128 (2021)", "doi": "10.1016/j.compbiomed.2020.104115", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Employing transfer learning (TL) with convolutional neural\nnetworks (CNNs), well-trained on non-medical ImageNet dataset, has shown\npromising results for medical image analysis in recent years. We aimed to\nconduct a scoping review to identify these studies and summarize their\ncharacteristics in terms of the problem description, input, methodology, and\noutcome. Materials and Methods: To identify relevant studies, MEDLINE, IEEE,\nand ACM digital library were searched. Two investigators independently reviewed\narticles to determine eligibility and to extract data according to a study\nprotocol defined a priori. Results: After screening of 8,421 articles, 102 met\nthe inclusion criteria. Of 22 anatomical areas, eye (18%), breast (14%), and\nbrain (12%) were the most commonly studied. Data augmentation was performed in\n72% of fine-tuning TL studies versus 15% of the feature-extracting TL studies.\nInception models were the most commonly used in breast related studies (50%),\nwhile VGGNet was the common in eye (44%), skin (50%) and tooth (57%) studies.\nAlexNet for brain (42%) and DenseNet for lung studies (38%) were the most\nfrequently used models. Inception models were the most frequently used for\nstudies that analyzed ultrasound (55%), endoscopy (57%), and skeletal system\nX-rays (57%). VGGNet was the most common for fundus (42%) and optical coherence\ntomography images (50%). AlexNet was the most frequent model for brain MRIs\n(36%) and breast X-Rays (50%). 35% of the studies compared their model with\nother well-trained CNN models and 33% of them provided visualization for\ninterpretation. Discussion: This study identified the most prevalent tracks of\nimplementation in the literature for data preparation, methodology selection\nand output evaluation for medical image analysis. Also, we identified several\ncritical research gaps existing in the TL studies on medical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:01:45 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 23:22:20 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 22:25:16 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 18:21:39 GMT"}, {"version": "v5", "created": "Fri, 13 Nov 2020 18:25:06 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Morid", "Mohammad Amin", ""], ["Borjali", "Alireza", ""], ["Del Fiol", "Guilherme", ""]]}, {"id": "2004.13188", "submitter": "Jiangpeng He", "authors": "Jiangpeng He, Zeman Shao, Janine Wright, Deborah Kerr, Carol Boushey\n  and Fengqing Zhu", "title": "Multi-Task Image-Based Dietary Assessment for Food Recognition and\n  Portion Size Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have achieved impressive results in many\napplications for image-based diet assessment such as food classification and\nfood portion size estimation. However, existing methods only focus on one task\nat a time, making it difficult to apply in real life when multiple tasks need\nto be processed together. In this work, we propose an end-to-end multi-task\nframework that can achieve both food classification and food portion size\nestimation. We introduce a food image dataset collected from a nutrition study\nwhere the groundtruth food portion is provided by registered dietitians. The\nmulti-task learning uses L2-norm based soft parameter sharing to train the\nclassification and regression tasks simultaneously. We also propose the use of\ncross-domain feature adaptation together with normalization to further improve\nthe performance of food portion size estimation. Our results outperforms the\nbaseline methods for both classification accuracy and mean absolute error for\nportion estimation, which shows great potential for advancing the field of\nimage-based dietary assessment.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 21:35:07 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["He", "Jiangpeng", ""], ["Shao", "Zeman", ""], ["Wright", "Janine", ""], ["Kerr", "Deborah", ""], ["Boushey", "Carol", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2004.13204", "submitter": "Ruizhen Hu", "authors": "Ruizhen Hu, Zeyu Huang, Yuhan Tang, Oliver van Kaick, Hao Zhang, Hui\n  Huang", "title": "Graph2Plan: Learning Floorplan Generation from Layout Graphs", "comments": null, "journal-ref": "ACM Transactions on Graphics 2020", "doi": "10.1145/3386569.3392391", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning framework for automated floorplan generation which\ncombines generative modeling using deep neural networks and user-in-the-loop\ndesigns to enable human users to provide sparse design constraints. Such\nconstraints are represented by a layout graph. The core component of our\nlearning framework is a deep neural network, Graph2Plan, which converts a\nlayout graph, along with a building boundary, into a floorplan that fulfills\nboth the layout and boundary constraints. Given an input building boundary, we\nallow a user to specify room counts and other layout constraints, which are\nused to retrieve a set of floorplans, with their associated layout graphs, from\na database. For each retrieved layout graph, along with the input boundary,\nGraph2Plan first generates a corresponding raster floorplan image, and then a\nrefined set of boxes representing the rooms. Graph2Plan is trained on RPLAN, a\nlarge-scale dataset consisting of 80K annotated floorplans. The network is\nmainly based on convolutional processing over both the layout graph, via a\ngraph neural network (GNN), and the input building boundary, as well as the\nraster floorplan images, via conventional image convolution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 23:17:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Hu", "Ruizhen", ""], ["Huang", "Zeyu", ""], ["Tang", "Yuhan", ""], ["van Kaick", "Oliver", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2004.13217", "submitter": "Rodrigo Santa Cruz", "authors": "Rodrigo Santa Cruz, Anoop Cherian, Basura Fernando, Dylan Campbell,\n  and Stephen Gould", "title": "Inferring Temporal Compositions of Actions Using Probabilistic Automata", "comments": "Accepted in Workshop on Compositionality in Computer Vision at CVPR,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework to recognize temporal compositions of atomic\nactions in videos. Specifically, we propose to express temporal compositions of\nactions as semantic regular expressions and derive an inference framework using\nprobabilistic automata to recognize complex actions as satisfying these\nexpressions on the input video features. Our approach is different from\nexisting works that either predict long-range complex activities as unordered\nsets of atomic actions, or retrieve videos using natural language sentences.\nInstead, the proposed approach allows recognizing complex fine-grained\nactivities using only pretrained action classifiers, without requiring any\nadditional data, annotations or neural network training. To evaluate the\npotential of our approach, we provide experiments on synthetic datasets and\nchallenging real action recognition datasets, such as MultiTHUMOS and Charades.\nWe conclude that the proposed approach can extend state-of-the-art primitive\naction classifiers to vastly more complex activities without large performance\ndegradation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:15:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Cruz", "Rodrigo Santa", ""], ["Cherian", "Anoop", ""], ["Fernando", "Basura", ""], ["Campbell", "Dylan", ""], ["Gould", "Stephen", ""]]}, {"id": "2004.13236", "submitter": "Dung Nguyen", "authors": "Dung Nguyen, Duc Thanh Nguyen, Rui Zeng, Thanh Thi Nguyen, Son N.\n  Tran, Thin Nguyen, Sridha Sridharan, and Clinton Fookes", "title": "Deep Auto-Encoders with Sequential Learning for Multimodal Dimensional\n  Emotion Recognition", "comments": "Under Review on Transaction on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal dimensional emotion recognition has drawn a great attention from\nthe affective computing community and numerous schemes have been extensively\ninvestigated, making a significant progress in this area. However, several\nquestions still remain unanswered for most of existing approaches including:\n(i) how to simultaneously learn compact yet representative features from\nmultimodal data, (ii) how to effectively capture complementary features from\nmultimodal streams, and (iii) how to perform all the tasks in an end-to-end\nmanner. To address these challenges, in this paper, we propose a novel deep\nneural network architecture consisting of a two-stream auto-encoder and a long\nshort term memory for effectively integrating visual and audio signal streams\nfor emotion recognition. To validate the robustness of our proposed\narchitecture, we carry out extensive experiments on the multimodal emotion in\nthe wild dataset: RECOLA. Experimental results show that the proposed method\nachieves state-of-the-art recognition performance and surpasses existing\nschemes by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 01:25:00 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Nguyen", "Dung", ""], ["Nguyen", "Duc Thanh", ""], ["Zeng", "Rui", ""], ["Nguyen", "Thanh Thi", ""], ["Tran", "Son N.", ""], ["Nguyen", "Thin", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2004.13263", "submitter": "Alex Chang", "authors": "Alex Habeen Chang, Benjamin M. Case", "title": "Attacks on Image Encryption Schemes for Privacy-Preserving Deep Neural\n  Networks", "comments": "For associated code, see\n  https://github.com/ahchang98/image-encryption-scheme-attacks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preserving machine learning is an active area of research usually\nrelying on techniques such as homomorphic encryption or secure multiparty\ncomputation. Recent novel encryption techniques for performing machine learning\nusing deep neural nets on images have recently been proposed by Tanaka and\nSirichotedumrong, Kinoshita, and Kiya. We present new chosen-plaintext and\nciphertext-only attacks against both of these proposed image encryption schemes\nand demonstrate the attacks' effectiveness on several examples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 03:34:01 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 06:22:13 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Chang", "Alex Habeen", ""], ["Case", "Benjamin M.", ""]]}, {"id": "2004.13271", "submitter": "Zhaohe Liao", "authors": "Zhaohe Liao", "title": "Trainable Activation Function in Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current research of neural networks, the activation function is\nmanually specified by human and not able to change themselves during training.\nThis paper focus on how to make the activation function trainable for deep\nneural networks. We use series and linear combination of different activation\nfunctions make activation functions continuously variable. Also, we test the\nperformance of CNNs with Fourier series simulated activation(Fourier-CNN) and\nCNNs with linear combined activation function (LC-CNN) on Cifar-10 dataset. The\nresult shows our trainable activation function reveals better performance than\nthe most used ReLU activation function. Finally, we improves the performance of\nFourier-CNN with Autoencoder, and test the performance of PSO algorithm in\noptimizing the parameters of networks\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 03:50:53 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 09:05:35 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Liao", "Zhaohe", ""]]}, {"id": "2004.13278", "submitter": "Yue Wang", "authors": "Yue Wang, Shafiq Joty, Michael R. Lyu, Irwin King, Caiming Xiong,\n  Steven C.H. Hoi", "title": "VD-BERT: A Unified Vision and Dialog Transformer with BERT", "comments": "EMNLP 2020 (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual dialog is a challenging vision-language task, where a dialog agent\nneeds to answer a series of questions through reasoning on the image content\nand dialog history. Prior work has mostly focused on various attention\nmechanisms to model such intricate interactions. By contrast, in this work, we\npropose VD-BERT, a simple yet effective framework of unified vision-dialog\nTransformer that leverages the pretrained BERT language models for Visual\nDialog tasks. The model is unified in that (1) it captures all the interactions\nbetween the image and the multi-turn dialog using a single-stream Transformer\nencoder, and (2) it supports both answer ranking and answer generation\nseamlessly through the same architecture. More crucially, we adapt BERT for the\neffective fusion of vision and dialog contents via visually grounded training.\nWithout the need of pretraining on external vision-language data, our model\nyields new state of the art, achieving the top position in both single-model\nand ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog\nleaderboard. Our code and pretrained models are released at\nhttps://github.com/salesforce/VD-BERT.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:08:46 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:41:22 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 09:07:41 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Yue", ""], ["Joty", "Shafiq", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""], ["Xiong", "Caiming", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2004.13297", "submitter": "Jian Ren", "authors": "Menglei Chai, Jian Ren, Sergey Tulyakov", "title": "Neural Hair Rendering", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic neural-based hair rendering pipeline that\ncan synthesize photo-realistic images from virtual 3D hair models. Unlike\nexisting supervised translation methods that require model-level similarity to\npreserve consistent structure representation for both real images and fake\nrenderings, our method adopts an unsupervised solution to work on arbitrary\nhair models. The key component of our method is a shared latent space to encode\nappearance-invariant structure information of both domains, which generates\nrealistic renderings conditioned by extra appearance inputs. This is achieved\nby domain-specific pre-disentangled structure representation, partially shared\ndomain encoder layers and a structure discriminator. We also propose a simple\nyet effective temporal conditioning method to enforce consistency for video\nsequence generation. We demonstrate the superiority of our method by testing it\non a large number of portraits and comparing it with alternative baselines and\nstate-of-the-art unsupervised image translation methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:36:49 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 19:29:30 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Chai", "Menglei", ""], ["Ren", "Jian", ""], ["Tulyakov", "Sergey", ""]]}, {"id": "2004.13316", "submitter": "Xue Yang", "authors": "Xue Yang, Junchi Yan, Xiaokang Yang, Jin Tang, Wenlong Liao, Tao He", "title": "SCRDet++: Detecting Small, Cluttered and Rotated Objects via\n  Instance-Level Feature Denoising and Rotation Loss Smoothing", "comments": "15 pages, 12 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small and cluttered objects are common in real-world which are challenging\nfor detection. The difficulty is further pronounced when the objects are\nrotated, as traditional detectors often routinely locate the objects in\nhorizontal bounding box such that the region of interest is contaminated with\nbackground or nearby interleaved objects. In this paper, we first innovatively\nintroduce the idea of denoising to object detection. Instance-level denoising\non the feature map is performed to enhance the detection to small and cluttered\nobjects. To handle the rotation variation, we also add a novel IoU constant\nfactor to the smooth L1 loss to address the long standing boundary problem,\nwhich to our analysis, is mainly caused by the periodicity of angular (PoA) and\nexchangeability of edges (EoE). By combing these two features, our proposed\ndetector is termed as SCRDet++. Extensive experiments are performed on large\naerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image\ndataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD\nand our newly released S$^2$TLD by this paper. The results show the\neffectiveness of our approach. Project page at\nhttps://yangxue0827.github.io/SCRDet++.html.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 06:03:54 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yang", "Xue", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""], ["Tang", "Jin", ""], ["Liao", "Wenlong", ""], ["He", "Tao", ""]]}, {"id": "2004.13324", "submitter": "Qianqian Wang", "authors": "Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, Noah Snavely", "title": "Learning Feature Descriptors using Camera Pose Supervision", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on learned visual descriptors has shown promising\nimprovements in correspondence estimation, a key component of many 3D vision\ntasks. However, existing descriptor learning frameworks typically require\nground-truth correspondences between feature points for training, which are\nchallenging to acquire at scale. In this paper we propose a novel\nweakly-supervised framework that can learn feature descriptors solely from\nrelative camera poses between images. To do so, we devise both a new loss\nfunction that exploits the epipolar constraint given by camera poses, and a new\nmodel architecture that makes the whole pipeline differentiable and efficient.\nBecause we no longer need pixel-level ground-truth correspondences, our\nframework opens up the possibility of training on much larger and more diverse\ndatasets for better and unbiased descriptors. We call the resulting descriptors\nCAmera Pose Supervised, or CAPS, descriptors. Though trained with weak\nsupervision, CAPS descriptors outperform even prior fully-supervised\ndescriptors and achieve state-of-the-art performance on a variety of geometric\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 06:35:27 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 07:23:23 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Qianqian", ""], ["Zhou", "Xiaowei", ""], ["Hariharan", "Bharath", ""], ["Snavely", "Noah", ""]]}, {"id": "2004.13358", "submitter": "Xiangyu Chen", "authors": "Xiangyu Chen, Zelin Ye, Jiankai Sun, Yuda Fan, Fang Hu, Chenxi Wang,\n  Cewu Lu", "title": "Transferable Active Grasping and Real Embodied Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping in cluttered scenes is challenging for robot vision systems, as\ndetection accuracy can be hindered by partial occlusion of objects. We adopt a\nreinforcement learning (RL) framework and 3D vision architectures to search for\nfeasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To\novercome the disadvantages of photo-realistic environment simulation, we\npropose a large-scale dataset called Real Embodied Dataset (RED), which\nincludes full-viewpoint real samples on the upper hemisphere with amodal\nannotation and enables a simulator that has real visual feedback. Based on this\ndataset, a practical 3-stage transferable active grasping pipeline is\ndeveloped, that is adaptive to unseen clutter scenes. In our pipeline, we\npropose a novel mask-guided reward to overcome the sparse reward issue in\ngrasping and ensure category-irrelevant behavior. The grasping pipeline and its\npossible variants are evaluated with extensive experiments both in simulation\nand on a real-world UR-5 robotic arm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 08:15:35 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Chen", "Xiangyu", ""], ["Ye", "Zelin", ""], ["Sun", "Jiankai", ""], ["Fan", "Yuda", ""], ["Hu", "Fang", ""], ["Wang", "Chenxi", ""], ["Lu", "Cewu", ""]]}, {"id": "2004.13364", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Wenda Jin, Jun Xu, Ming-Ming Cheng", "title": "Gradient-Induced Co-Saliency Detection", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-saliency detection (Co-SOD) aims to segment the common salient foreground\nin a group of relevant images. In this paper, inspired by human behavior, we\npropose a gradient-induced co-saliency detection (GICD) method. We first\nabstract a consensus representation for the grouped images in the embedding\nspace; then, by comparing the single image with consensus representation, we\nutilize the feedback gradient information to induce more attention to the\ndiscriminative co-salient features. In addition, due to the lack of Co-SOD\ntraining data, we design a jigsaw training strategy, with which Co-SOD networks\ncan be trained on general saliency datasets without extra pixel-level\nannotations. To evaluate the performance of Co-SOD methods on discovering the\nco-salient object among multiple foregrounds, we construct a challenging CoCA\ndataset, where each image contains at least one extraneous foreground along\nwith the co-salient object. Experiments demonstrate that our GICD achieves\nstate-of-the-art performance. Our codes and dataset are available at\nhttps://mmcheng.net/gicd/.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 08:40:55 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 10:14:51 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 08:03:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Zhao", ""], ["Jin", "Wenda", ""], ["Xu", "Jun", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2004.13369", "submitter": "Xuanqin Mou", "authors": "Yang Li and Xuanqin Mou", "title": "SSIM-Based CTU-Level Joint Optimal Bit Allocation and Rate Distortion\n  Optimization", "comments": "An improved version of this manuscript has been accepted by IEEE\n  Transactions on Broadcasting DOI 10.1109/TBC.2021.3068871. The project page\n  is located at http://gr.xjtu.edu.cn/web/xqmou/sosr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural similarity (SSIM)-based distortion $D_\\text{SSIM}$ is more\nconsistent with human perception than the traditional mean squared error\n$D_\\text{MSE}$. To achieve better video quality, many studies on optimal bit\nallocation (OBA) and rate-distortion optimization (RDO) used $D_\\text{SSIM}$ as\nthe distortion metric. However, many of them failed to optimize OBA and RDO\njointly based on SSIM, thus causing a non-optimal R-$D_\\text{SSIM}$\nperformance. This problem is due to the lack of an accurate R-$D_\\text{SSIM}$\nmodel that can be used uniformly in both OBA and RDO. To solve this problem, we\npropose a $D_\\text{SSIM}$-$D_\\text{MSE}$ model first. Based on this model, the\ncomplex R-$D_\\text{SSIM}$ cost in RDO can be calculated as simpler\nR-$D_\\text{MSE}$ cost with a new SSIM-related Lagrange multiplier. This not\nonly reduces the computation burden of SSIM-based RDO, but also enables the\nR-$D_\\text{SSIM}$ model to be uniformly used in OBA and RDO. Moreover, with the\nnew SSIM-related Lagrange multiplier in hand, the joint relationship of\nR-$D_\\text{SSIM}$-$\\lambda_\\text{SSIM}$ (the negative derivative of\nR-$D_\\text{SSIM}$) can be built, based on which the R-$D_\\text{SSIM}$ model\nparameters can be calculated accurately. With accurate and unified\nR-$D_\\text{SSIM}$ model, SSIM-based OBA and SSIM-based RDO are unified together\nin our scheme, called SOSR. Compared with the HEVC reference encoder HM16.20,\nSOSR saves 4%, 10%, and 14% bitrate under the same SSIM in all-intra,\nhierarchical and non-hierarchical low-delay-B configurations, which is superior\nto other state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 08:55:21 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 05:46:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Yang", ""], ["Mou", "Xuanqin", ""]]}, {"id": "2004.13371", "submitter": "Valentin Oreiller", "authors": "Valentin Oreiller, Vincent Andrearczyk, Julien Fageot, John O. Prior,\n  Adrien Depeursinge", "title": "3D Solid Spherical Bispectrum CNNs for Biomedical Texture Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Rotation Invariant (LRI) operators have shown great potential in\nbiomedical texture analysis where patterns appear at random positions and\norientations. LRI operators can be obtained by computing the responses to the\ndiscrete rotation of local descriptors, such as Local Binary Patterns (LBP) or\nthe Scale Invariant Feature Transform (SIFT). Other strategies achieve this\ninvariance using Laplacian of Gaussian or steerable wavelets for instance,\npreventing the introduction of sampling errors during the discretization of the\nrotations. In this work, we obtain LRI operators via the local projection of\nthe image on the spherical harmonics basis, followed by the computation of the\nbispectrum, which shares and extends the invariance properties of the spectrum.\nWe investigate the benefits of using the bispectrum over the spectrum in the\ndesign of a LRI layer embedded in a shallow Convolutional Neural Network (CNN)\nfor 3D image analysis. The performance of each design is evaluated on two\ndatasets and compared against a standard 3D CNN. The first dataset is made of\n3D volumes composed of synthetically generated rotated patterns, while the\nsecond contains malignant and benign pulmonary nodules in Computed Tomography\n(CT) images. The results indicate that bispectrum CNNs allows for a\nsignificantly better characterization of 3D textures than both the spectral and\nstandard CNN. In addition, it can efficiently learn with fewer training\nexamples and trainable parameters when compared to a standard convolutional\nlayer.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:01:13 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 11:21:48 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Oreiller", "Valentin", ""], ["Andrearczyk", "Vincent", ""], ["Fageot", "Julien", ""], ["Prior", "John O.", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "2004.13379", "submitter": "Simon Vandenhende", "authors": "Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc\n  Proesmans, Dengxin Dai and Luc Van Gool", "title": "Multi-Task Learning for Dense Prediction Tasks: A Survey", "comments": "Accepted to T-PAMI. Code + Suppl. Mat. can be found here:\n  https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch IEEE\n  Copyright Notice", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3054719", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of deep learning, many dense prediction tasks, i.e. tasks\nthat produce pixel-level predictions, have seen significant performance\nimprovements. The typical approach is to learn these tasks in isolation, that\nis, a separate neural network is trained for each individual task. Yet, recent\nmulti-task learning (MTL) techniques have shown promising results w.r.t.\nperformance, computations and/or memory footprint, by jointly tackling multiple\ntasks through a learned shared representation. In this survey, we provide a\nwell-rounded view on state-of-the-art deep learning approaches for MTL in\ncomputer vision, explicitly emphasizing on dense prediction tasks. Our\ncontributions concern the following. First, we consider MTL from a network\narchitecture point-of-view. We include an extensive overview and discuss the\nadvantages/disadvantages of recent popular MTL models. Second, we examine\nvarious optimization methods to tackle the joint learning of multiple tasks. We\nsummarize the qualitative elements of these works and explore their\ncommonalities and differences. Finally, we provide an extensive experimental\nevaluation across a variety of dense prediction benchmarks to examine the pros\nand cons of the different methods, including both architectural and\noptimization based strategies.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:15:50 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 20:22:21 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 18:56:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Vandenhende", "Simon", ""], ["Georgoulis", "Stamatios", ""], ["Van Gansbeke", "Wouter", ""], ["Proesmans", "Marc", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2004.13384", "submitter": "Bogdan Boc\\c{s}e", "authors": "Bogdan Bocse and Ioan Radu Jinga", "title": "The Immersion of Directed Multi-graphs in Embedding Fields.\n  Generalisations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The purpose of this paper is to outline a generalised model for representing\nhybrids of relational-categorical, symbolic, perceptual-sensory and\nperceptual-latent data, so as to embody, in the same architectural data layer,\nrepresentations for the input, output and latent tensors. This variety of\nrepresentation is currently used by various machine-learning models in computer\nvision, NLP/NLU, reinforcement learning which allows for direct application of\ncross-domain queries and functions. This is achieved by endowing a directed\nTensor-Typed Multi-Graph with at least some edge attributes which represent the\nembeddings from various latent spaces, so as to define, construct and compute\nnew similarity and distance relationships between and across tensorial forms,\nincluding visual, linguistic, auditory latent representations, thus stitching\nthe logical-categorical view of the observed universe to the\nBayesian/statistical view.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:28:08 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bocse", "Bogdan", ""], ["Jinga", "Ioan Radu", ""]]}, {"id": "2004.13388", "submitter": "Hang Dong", "authors": "Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang,\n  Ming-Hsuan Yang", "title": "Multi-Scale Boosted Dehazing Network with Dense Feature Fusion", "comments": "Accepted by CVPR 2020. The code are available at\n  https://github.com/BookerDeWitt/MSBDN-DFF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Multi-Scale Boosted Dehazing Network with Dense\nFeature Fusion based on the U-Net architecture. The proposed method is designed\nbased on two principles, boosting and error feedback, and we show that they are\nsuitable for the dehazing problem. By incorporating the\nStrengthen-Operate-Subtract boosting strategy in the decoder of the proposed\nmodel, we develop a simple yet effective boosted decoder to progressively\nrestore the haze-free image. To address the issue of preserving spatial\ninformation in the U-Net architecture, we design a dense feature fusion module\nusing the back-projection feedback scheme. We show that the dense feature\nfusion module can simultaneously remedy the missing spatial information from\nhigh-resolution features and exploit the non-adjacent features. Extensive\nevaluations demonstrate that the proposed model performs favorably against the\nstate-of-the-art approaches on the benchmark datasets as well as real-world\nhazy images.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 09:34:47 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Dong", "Hang", ""], ["Pan", "Jinshan", ""], ["Xiang", "Lei", ""], ["Hu", "Zhe", ""], ["Zhang", "Xinyi", ""], ["Wang", "Fei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2004.13406", "submitter": "Abhilash Nandy", "authors": "Abhilash Nandy, Rachana Sathish, Debdoot Sheet", "title": "Identification of Cervical Pathology using Adversarial Neural Networks", "comments": "9 pages, 10 images, 5th MedImage Workshop of 11th Indian Conference\n  on Computer Vision, Graphics and Image Processing, Hyderabad, India, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various screening and diagnostic methods have led to a large reduction of\ncervical cancer death rates in developed countries. However, cervical cancer is\nthe leading cause of cancer related deaths in women in India and other low and\nmiddle income countries (LMICs) especially among the urban poor and slum\ndwellers. Several sophisticated techniques such as cytology tests, HPV tests\netc. have been widely used for screening of cervical cancer. These tests are\ninherently time consuming. In this paper, we propose a convolutional\nautoencoder based framework, having an architecture similar to SegNet which is\ntrained in an adversarial fashion for classifying images of the cervix acquired\nusing a colposcope. We validate performance on the Intel-Mobile ODT cervical\nimage classification dataset. The proposed method outperforms the standard\ntechnique of fine-tuning convolutional neural networks pre-trained on ImageNet\ndatabase with an average accuracy of 73.75%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 10:22:16 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Nandy", "Abhilash", ""], ["Sathish", "Rachana", ""], ["Sheet", "Debdoot", ""]]}, {"id": "2004.13410", "submitter": "Vittorio Mazzia", "authors": "Vittorio Mazzia, Francesco Salvetti, Aleem Khaliq, Marcello Chiaberge", "title": "Real-Time Apple Detection System Using Embedded Systems With Hardware\n  Accelerators: An Edge AI Application", "comments": null, "journal-ref": "IEEE Access, vol. 8, pp. 9102-9114, 2020", "doi": "10.1109/ACCESS.2020.2964608", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-time apple detection in orchards is one of the most effective ways of\nestimating apple yields, which helps in managing apple supplies more\neffectively. Traditional detection methods used highly computational machine\nlearning algorithms with intensive hardware set up, which are not suitable for\ninfield real-time apple detection due to their weight and power constraints. In\nthis study, a real-time embedded solution inspired from \"Edge AI\" is proposed\nfor apple detection with the implementation of YOLOv3-tiny algorithm on various\nembedded platforms such as Raspberry Pi 3 B+ in combination with Intel Movidius\nNeural Computing Stick (NCS), Nvidia's Jetson Nano and Jetson AGX Xavier. Data\nset for training were compiled using acquired images during field survey of\napple orchard situated in the north region of Italy, and images used for\ntesting were taken from widely used google data set by filtering out the images\ncontaining apples in different scenes to ensure the robustness of the\nalgorithm. The proposed study adapts YOLOv3-tiny architecture to detect small\nobjects. It shows the feasibility of deployment of the customized model on\ncheap and power-efficient embedded hardware without compromising mean average\ndetection accuracy (83.64%) and achieved frame rate up to 30 fps even for the\ndifficult scenarios such as overlapping apples, complex background, less\nexposure of apple due to leaves and branches. Furthermore, the proposed\nembedded solution can be deployed on the unmanned ground vehicles to detect,\ncount, and measure the size of the apples in real-time to help the farmers and\nagronomists in their decision making and management skills.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 10:40:01 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Mazzia", "Vittorio", ""], ["Salvetti", "Francesco", ""], ["Khaliq", "Aleem", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2004.13431", "submitter": "Yiming Hu", "authors": "Yiming Hu, Yuding Liang, Zichao Guo, Ruosi Wan, Xiangyu Zhang, Yichen\n  Wei, Qingyi Gu, Jian Sun", "title": "Angle-based Search Space Shrinking for Neural Architecture Search", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a simple and general search space shrinking method,\ncalled Angle-Based search space Shrinking (ABS), for Neural Architecture Search\n(NAS). Our approach progressively simplifies the original search space by\ndropping unpromising candidates, thus can reduce difficulties for existing NAS\nmethods to find superior architectures. In particular, we propose an\nangle-based metric to guide the shrinking process. We provide comprehensive\nevidences showing that, in weight-sharing supernet, the proposed metric is more\nstable and accurate than accuracy-based and magnitude-based metrics to predict\nthe capability of child models. We also show that the angle-based metric can\nconverge fast while training supernet, enabling us to get promising shrunk\nsearch spaces efficiently. ABS can easily apply to most of NAS approaches (e.g.\nSPOS, FairNAS, ProxylessNAS, DARTS and PDARTS). Comprehensive experiments show\nthat ABS can dramatically enhance existing NAS approaches by providing a\npromising shrunk search space.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 11:26:46 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 13:04:04 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 14:45:22 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hu", "Yiming", ""], ["Liang", "Yuding", ""], ["Guo", "Zichao", ""], ["Wan", "Ruosi", ""], ["Zhang", "Xiangyu", ""], ["Wei", "Yichen", ""], ["Gu", "Qingyi", ""], ["Sun", "Jian", ""]]}, {"id": "2004.13449", "submitter": "Yana Hasson", "authors": "Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys,\n  Cordelia Schmid", "title": "Leveraging Photometric Consistency over Time for Sparsely Supervised\n  Hand-Object Reconstruction", "comments": "CVPR 2020. See the project webpage at\n  https://hassony2.github.io/handobjectconsist.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling hand-object manipulations is essential for understanding how humans\ninteract with their environment. While of practical importance, estimating the\npose of hands and objects during interactions is challenging due to the large\nmutual occlusions that occur during manipulation. Recent efforts have been\ndirected towards fully-supervised methods that require large amounts of labeled\ntraining samples. Collecting 3D ground-truth data for hand-object interactions,\nhowever, is costly, tedious, and error-prone. To overcome this challenge we\npresent a method to leverage photometric consistency across time when\nannotations are only available for a sparse subset of frames in a video. Our\nmodel is trained end-to-end on color images to jointly reconstruct hands and\nobjects in 3D by inferring their poses. Given our estimated reconstructions, we\ndifferentiably render the optical flow between pairs of adjacent images and use\nit within the network to warp one frame to another. We then apply a\nself-supervised photometric loss that relies on the visual consistency between\nnearby images. We achieve state-of-the-art results on 3D hand-object\nreconstruction benchmarks and demonstrate that our approach allows us to\nimprove the pose estimation accuracy by leveraging information from neighboring\nframes in low-data regimes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:03:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Hasson", "Yana", ""], ["Tekin", "Bugra", ""], ["Bogo", "Federica", ""], ["Laptev", "Ivan", ""], ["Pollefeys", "Marc", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2004.13453", "submitter": "Mina Jafari", "authors": "Mina Jafari, Dorothee Auer, Susan Francis, Jonathan Garibaldi, Xin\n  Chen", "title": "DRU-net: An Efficient Deep Convolutional Neural Network for Medical\n  Image Segmentation", "comments": "Accepted for publication at IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2020, 5 pages, 3 figures", "journal-ref": "2020 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2020)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual network (ResNet) and densely connected network (DenseNet) have\nsignificantly improved the training efficiency and performance of deep\nconvolutional neural networks (DCNNs) mainly for object classification tasks.\nIn this paper, we propose an efficient network architecture by considering\nadvantages of both networks. The proposed method is integrated into an\nencoder-decoder DCNN model for medical image segmentation. Our method adds\nadditional skip connections compared to ResNet but uses significantly fewer\nmodel parameters than DenseNet. We evaluate the proposed method on a public\ndataset (ISIC 2018 grand-challenge) for skin lesion segmentation and a local\nbrain MRI dataset. In comparison with ResNet-based, DenseNet-based and\nattention network (AttnNet) based methods within the same encoder-decoder\nnetwork structure, our method achieves significantly higher segmentation\naccuracy with fewer number of model parameters than DenseNet and AttnNet. The\ncode is available on GitHub (GitHub link: https://github.com/MinaJf/DRU-net).\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:16:24 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jafari", "Mina", ""], ["Auer", "Dorothee", ""], ["Francis", "Susan", ""], ["Garibaldi", "Jonathan", ""], ["Chen", "Xin", ""]]}, {"id": "2004.13458", "submitter": "Timo Milbich", "authors": "Timo Milbich, Karsten Roth, Homanga Bharadhwaj, Samarth Sinha, Yoshua\n  Bengio, Bj\\\"orn Ommer, and Joseph Paul Cohen", "title": "DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning", "comments": "published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Similarity plays an important role in many computer vision\napplications. Deep metric learning (DML) is a powerful framework for learning\nsuch similarities which not only generalize from training data to identically\ndistributed test distributions, but in particular also translate to unknown\ntest classes. However, its prevailing learning paradigm is class-discriminative\nsupervised training, which typically results in representations specialized in\nseparating training classes. For effective generalization, however, such an\nimage representation needs to capture a diverse range of data characteristics.\nTo this end, we propose and study multiple complementary learning tasks,\ntargeting conceptually different data relationships by only resorting to the\navailable training samples and labels of a standard DML setting. Through\nsimultaneous optimization of our tasks we learn a single model to aggregate\ntheir training signals, resulting in strong generalization and state-of-the-art\nperformance on multiple established DML benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 12:26:50 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:01:55 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 20:53:42 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2020 16:19:05 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Milbich", "Timo", ""], ["Roth", "Karsten", ""], ["Bharadhwaj", "Homanga", ""], ["Sinha", "Samarth", ""], ["Bengio", "Yoshua", ""], ["Ommer", "Bj\u00f6rn", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "2004.13470", "submitter": "Mina Jafari", "authors": "Mina Jafari, Ruizhe Li, Yue Xing, Dorothee Auer, Susan Francis,\n  Jonathan Garibaldi, and Xin Chen", "title": "FU-net: Multi-class Image Segmentation Using Feedback Weighted U-net", "comments": "Accepted for publication at International Conference on Image and\n  Graphics (ICIG 2019)", "journal-ref": "The 10th International Conference on Image and Graphics (ICIG\n  2019)", "doi": "10.1007/978-3-030-34110-7_44", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a generic deep convolutional neural network (DCNN)\nfor multi-class image segmentation. It is based on a well-established\nsupervised end-to-end DCNN model, known as U-net. U-net is firstly modified by\nadding widely used batch normalization and residual block (named as BRU-net) to\nimprove the efficiency of model training. Based on BRU-net, we further\nintroduce a dynamically weighted cross-entropy loss function. The weighting\nscheme is calculated based on the pixel-wise prediction accuracy during the\ntraining process. Assigning higher weights to pixels with lower segmentation\naccuracies enables the network to learn more from poorly predicted image\nregions. Our method is named as feedback weighted U-net (FU-net). We have\nevaluated our method based on T1- weighted brain MRI for the segmentation of\nmidbrain and substantia nigra, where the number of pixels in each class is\nextremely unbalanced to each other. Based on the dice coefficient measurement,\nour proposed FU-net has outperformed BRU-net and U-net with statistical\nsignificance, especially when only a small number of training examples are\navailable. The code is publicly available in GitHub (GitHub link:\nhttps://github.com/MinaJf/FU-net).\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:08:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jafari", "Mina", ""], ["Li", "Ruizhe", ""], ["Xing", "Yue", ""], ["Auer", "Dorothee", ""], ["Francis", "Susan", ""], ["Garibaldi", "Jonathan", ""], ["Chen", "Xin", ""]]}, {"id": "2004.13513", "submitter": "Arthur Douillard", "authors": "Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert,\n  Eduardo Valle", "title": "PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lifelong learning has attracted much attention, but existing works still\nstruggle to fight catastrophic forgetting and accumulate knowledge over long\nstretches of incremental learning. In this work, we propose PODNet, a model\ninspired by representation learning. By carefully balancing the compromise\nbetween remembering the old classes and learning new ones, PODNet fights\ncatastrophic forgetting, even over very long runs of small incremental tasks\n--a setting so far unexplored by current works. PODNet innovates on existing\nart with an efficient spatial-based distillation-loss applied throughout the\nmodel and a representation comprising multiple proxy vectors for each class. We\nvalidate those innovations thoroughly, comparing PODNet with three\nstate-of-the-art models on three datasets: CIFAR100, ImageNet100, and\nImageNet1000. Our results showcase a significant advantage of PODNet over\nexisting art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points,\nrespectively. Code is available at\nhttps://github.com/arthurdouillard/incremental_learning.pytorch\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:45:23 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 13:20:18 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 16:10:33 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Douillard", "Arthur", ""], ["Cord", "Matthieu", ""], ["Ollion", "Charles", ""], ["Robert", "Thomas", ""], ["Valle", "Eduardo", ""]]}, {"id": "2004.13515", "submitter": "Philippe Burlina", "authors": "Philippe Burlina, Neil Joshi, William Paul, Katia D. Pacheco, Neil M.\n  Bressler", "title": "Addressing Artificial Intelligence Bias in Retinal Disease Diagnostics", "comments": "Accepted for publication at journal of Translational Vision Science\n  and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study evaluated generative methods to potentially mitigate AI bias when\ndiagnosing diabetic retinopathy (DR) resulting from training data imbalance, or\ndomain generalization which occurs when deep learning systems (DLS) face\nconcepts at test/inference time they were not initially trained on. The public\ndomain Kaggle-EyePACS dataset (88,692 fundi and 44,346 individuals, originally\ndiverse for ethnicity) was modified by adding clinician-annotated labels and\nconstructing an artificial scenario of data imbalance and domain generalization\nby disallowing training (but not testing) exemplars for images of retinas with\nDR warranting referral (DR-referable) and from darker-skin individuals, who\npresumably have greater concentration of melanin within uveal melanocytes, on\naverage, contributing to retinal image pigmentation. A traditional/baseline\ndiagnostic DLS was compared against new DLSs that would use training data\naugmented via generative models for debiasing. Accuracy (95% confidence\nintervals [CI]) of the baseline diagnostics DLS for fundus images of\nlighter-skin individuals was 73.0% (66.9%, 79.2%) vs. darker-skin of 60.5%\n(53.5%, 67.3%), demonstrating bias/disparity (delta=12.5%) (Welch t-test\nt=2.670, P=.008) in AI performance across protected subpopulations. Using novel\ngenerative methods for addressing missing subpopulation training data\n(DR-referable darker-skin) achieved instead accuracy, for lighter-skin, of\n72.0% (65.8%, 78.2%), and for darker-skin, of 71.5% (65.2%,77.8%),\ndemonstrating closer parity (delta=0.5%) in accuracy across subpopulations\n(Welch t-test t=0.111, P=.912). Findings illustrate how data imbalance and\ndomain generalization can lead to disparity of accuracy across subpopulations,\nand show that novel generative methods of synthetic fundus images may play a\nrole for debiasing AI.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:46:54 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 21:08:04 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 13:29:33 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 15:11:18 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Burlina", "Philippe", ""], ["Joshi", "Neil", ""], ["Paul", "William", ""], ["Pacheco", "Katia D.", ""], ["Bressler", "Neil M.", ""]]}, {"id": "2004.13523", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Cong Phuoc Huynh, and Fatih Porikli", "title": "Identity Enhanced Residual Image Denoising", "comments": "Accepted in CVPRW. arXiv admin note: substantial text overlap with\n  arXiv:1712.02933", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to learn a fully-convolutional network model that consists of a\nChain of Identity Mapping Modules and residual on the residual architecture for\nimage denoising. Our network structure possesses three distinctive features\nthat are important for the noise removal task. Firstly, each unit employs\nidentity mappings as the skip connections and receives pre-activated input to\npreserve the gradient magnitude propagated in both the forward and backward\ndirections. Secondly, by utilizing dilated kernels for the convolution layers\nin the residual branch, each neuron in the last convolution layer of each\nmodule can observe the full receptive field of the first layer. Lastly, we\nemploy the residual on the residual architecture to ease the propagation of the\nhigh-level information. Contrary to current state-of-the-art real denoising\nnetworks, we also present a straightforward and single-stage network for real\nimage denoising. The proposed network produces remarkably higher numerical\naccuracy and better visual image quality than the classical state-of-the-art\nand CNN algorithms when being evaluated on the three conventional benchmark and\nthree real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 04:52:22 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Anwar", "Saeed", ""], ["Huynh", "Cong Phuoc", ""], ["Porikli", "Fatih", ""]]}, {"id": "2004.13524", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Nick Barnes, and Lars Petersson", "title": "Attention Based Real Image Restoration", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.07396", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks perform better on images containing\nspatially invariant degradations, also known as synthetic degradations;\nhowever, their performance is limited on real-degraded photographs and requires\nmultiple-stage network modeling. To advance the practicability of restoration\nalgorithms, this paper proposes a novel single-stage blind real image\nrestoration network (R$^2$Net) by employing a modular architecture. We use a\nresidual on the residual structure to ease the flow of low-frequency\ninformation and apply feature attention to exploit the channel dependencies.\nFurthermore, the evaluation in terms of quantitative metrics and visual quality\nfor four restoration tasks i.e. Denoising, Super-resolution, Raindrop Removal,\nand JPEG Compression on 11 real degraded datasets against more than 30\nstate-of-the-art algorithms demonstrate the superiority of our R$^2$Net. We\nalso present the comparison on three synthetically generated degraded datasets\nfor denoising to showcase the capability of our method on synthetics denoising.\nThe codes, trained models, and results are available on\nhttps://github.com/saeed-anwar/R2Net.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 04:21:49 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 06:52:45 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Anwar", "Saeed", ""], ["Barnes", "Nick", ""], ["Petersson", "Lars", ""]]}, {"id": "2004.13546", "submitter": "Fabian K\\\"uppers", "authors": "Fabian K\\\"uppers, Jan Kronenberger, Amirhossein Shantia, Anselm\n  Haselhoff", "title": "Multivariate Confidence Calibration for Object Detection", "comments": "Accepted on CVPR 2020 Workshop: \"2nd Workshop on Safe Artificial\n  Intelligence for Automated Driving (SAIAD)\"", "journal-ref": null, "doi": "10.1109/CVPRW50498.2020.00171", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unbiased confidence estimates of neural networks are crucial especially for\nsafety-critical applications. Many methods have been developed to calibrate\nbiased confidence estimates. Though there is a variety of methods for\nclassification, the field of object detection has not been addressed yet.\nTherefore, we present a novel framework to measure and calibrate biased (or\nmiscalibrated) confidence estimates of object detection methods. The main\ndifference to related work in the field of classifier calibration is that we\nalso use additional information of the regression output of an object detector\nfor calibration. Our approach allows, for the first time, to obtain calibrated\nconfidence estimates with respect to image location and box scale. In addition,\nwe propose a new measure to evaluate miscalibration of object detectors.\nFinally, we show that our developed methods outperform state-of-the-art\ncalibration models for the task of object detection and provides reliable\nconfidence estimates across different locations and scales.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:17:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["K\u00fcppers", "Fabian", ""], ["Kronenberger", "Jan", ""], ["Shantia", "Amirhossein", ""], ["Haselhoff", "Anselm", ""]]}, {"id": "2004.13567", "submitter": "Xin Yang", "authors": "Xin Yang, Xu Wang, Yi Wang, Haoran Dou, Shengli Li, Huaxuan Wen, Yi\n  Lin, Pheng-Ann Heng, Dong Ni", "title": "Hybrid Attention for Automatic Segmentation of Whole Fetal Head in\n  Prenatal Ultrasound Volumes", "comments": "Accepted by Computer Methods and Programs in Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Biometric measurements of fetal head are important\nindicators for maternal and fetal health monitoring during pregnancy. 3D\nultrasound (US) has unique advantages over 2D scan in covering the whole fetal\nhead and may promote the diagnoses. However, automatically segmenting the whole\nfetal head in US volumes still pends as an emerging and unsolved problem. The\nchallenges that automated solutions need to tackle include the poor image\nquality, boundary ambiguity, long-span occlusion, and the appearance\nvariability across different fetal poses and gestational ages. In this paper,\nwe propose the first fully-automated solution to segment the whole fetal head\nin US volumes.\n  Methods: The segmentation task is firstly formulated as an end-to-end\nvolumetric mapping under an encoder-decoder deep architecture. We then combine\nthe segmentor with a proposed hybrid attention scheme (HAS) to select\ndiscriminative features and suppress the non-informative volumetric features in\na composite and hierarchical way. With little computation overhead, HAS proves\nto be effective in addressing boundary ambiguity and deficiency. To enhance the\nspatial consistency in segmentation, we further organize multiple segmentors in\na cascaded fashion to refine the results by revisiting context in the\nprediction of predecessors.\n  Results: Validated on a large dataset collected from 100 healthy volunteers,\nour method presents superior segmentation performance (DSC (Dice Similarity\nCoefficient), 96.05%), remarkable agreements with experts. With another 156\nvolumes collected from 52 volunteers, we ahieve high reproducibilities (mean\nstandard deviation 11.524 mL) against scan variations.\n  Conclusion: This is the first investigation about whole fetal head\nsegmentation in 3D US. Our method is promising to be a feasible solution in\nassisting the volumetric US-based prenatal studies.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:43:05 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yang", "Xin", ""], ["Wang", "Xu", ""], ["Wang", "Yi", ""], ["Dou", "Haoran", ""], ["Li", "Shengli", ""], ["Wen", "Huaxuan", ""], ["Lin", "Yi", ""], ["Heng", "Pheng-Ann", ""], ["Ni", "Dong", ""]]}, {"id": "2004.13577", "submitter": "Zhongyi Han", "authors": "Zhongyi Han, Benzheng Wei, Yilong Yin, Shuo Li", "title": "Unifying Neural Learning and Symbolic Reasoning for Spinal Medical\n  Report Generation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical report generation in spine radiology, i.e., given spinal\nmedical images and directly create radiologist-level diagnosis reports to\nsupport clinical decision making, is a novel yet fundamental study in the\ndomain of artificial intelligence in healthcare. However, it is incredibly\nchallenging because it is an extremely complicated task that involves visual\nperception and high-level reasoning processes. In this paper, we propose the\nneural-symbolic learning (NSL) framework that performs human-like learning by\nunifying deep neural learning and symbolic logical reasoning for the spinal\nmedical report generation. Generally speaking, the NSL framework firstly\nemploys deep neural learning to imitate human visual perception for detecting\nabnormalities of target spinal structures. Concretely, we design an adversarial\ngraph network that interpolates a symbolic graph reasoning module into a\ngenerative adversarial network through embedding prior domain knowledge,\nachieving semantic segmentation of spinal structures with high complexity and\nvariability. NSL secondly conducts human-like symbolic logical reasoning that\nrealizes unsupervised causal effect analysis of detected entities of\nabnormalities through meta-interpretive learning. NSL finally fills these\ndiscoveries of target diseases into a unified template, successfully achieving\na comprehensive medical report generation. When it employed in a real-world\nclinical dataset, a series of empirical studies demonstrate its capacity on\nspinal medical report generation as well as show that our algorithm remarkably\nexceeds existing methods in the detection of spinal structures. These indicate\nits potential as a clinical tool that contributes to computer-aided diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:06:24 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Han", "Zhongyi", ""], ["Wei", "Benzheng", ""], ["Yin", "Yilong", ""], ["Li", "Shuo", ""]]}, {"id": "2004.13587", "submitter": "Zhongchao Qian", "authors": "Zhongchao Qian, Tyler L. Hayes, Kushal Kafle, Christopher Kanan", "title": "Do We Need Fully Connected Output Layers in Convolutional Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, deep convolutional neural networks consist of a series of\nconvolutional and pooling layers followed by one or more fully connected (FC)\nlayers to perform the final classification. While this design has been\nsuccessful, for datasets with a large number of categories, the fully connected\nlayers often account for a large percentage of the network's parameters. For\napplications with memory constraints, such as mobile devices and embedded\nplatforms, this is not ideal. Recently, a family of architectures that involve\nreplacing the learned fully connected output layer with a fixed layer has been\nproposed as a way to achieve better efficiency. In this paper we examine this\nidea further and demonstrate that fixed classifiers offer no additional benefit\ncompared to simply removing the output layer along with its parameters. We\nfurther demonstrate that the typical approach of having a fully connected final\noutput layer is inefficient in terms of parameter count. We are able to achieve\ncomparable performance to a traditionally learned fully connected\nclassification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196,\nand Oxford Flowers-102 datasets, while not having a fully connected output\nlayer at all.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:21:44 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:20:47 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Qian", "Zhongchao", ""], ["Hayes", "Tyler L.", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "2004.13621", "submitter": "Hengshuang Zhao", "authors": "Hengshuang Zhao, Jiaya Jia, Vladlen Koltun", "title": "Exploring Self-attention for Image Recognition", "comments": "CVPR 2020, Code available at https://github.com/hszhao/SAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that self-attention can serve as a basic building block\nfor image recognition models. We explore variations of self-attention and\nassess their effectiveness for image recognition. We consider two forms of\nself-attention. One is pairwise self-attention, which generalizes standard\ndot-product attention and is fundamentally a set operator. The other is\npatchwise self-attention, which is strictly more powerful than convolution. Our\npairwise self-attention networks match or outperform their convolutional\ncounterparts, and the patchwise models substantially outperform the\nconvolutional baselines. We also conduct experiments that probe the robustness\nof learned representations and conclude that self-attention networks may have\nsignificant benefits in terms of robustness and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:01:48 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhao", "Hengshuang", ""], ["Jia", "Jiaya", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2004.13628", "submitter": "Xiaying Bai", "authors": "Yang Hu, Xiaying Bai, Pan Zhou, Fanhua Shang, Shengmei Shen", "title": "Data Augmentation Imbalance For Imbalanced Attribute Classification", "comments": "This paper needs further revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute recognition is an important multi-label classification\nproblem. Although the convolutional neural networks are prominent in learning\ndiscriminative features from images, the data imbalance in multi-label setting\nfor fine-grained tasks remains an open problem. In this paper, we propose a new\nre-sampling algorithm called: data augmentation imbalance (DAI) to explicitly\nenhance the ability to discriminate the fewer attributes via increasing the\nproportion of labels accounting for a small part. Fundamentally, by applying\nover-sampling and under-sampling on the multi-label dataset at the same time,\nthe thought of robbing the rich attributes and helping the poor makes a\nsignificant contribution to DAI. Extensive empirical evidence shows that our\nDAI algorithm achieves state-of-the-art results, based on pedestrian attribute\ndatasets, i.e. standard PA-100K and PETA datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 20:43:29 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:02:58 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 07:36:49 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hu", "Yang", ""], ["Bai", "Xiaying", ""], ["Zhou", "Pan", ""], ["Shang", "Fanhua", ""], ["Shen", "Shengmei", ""]]}, {"id": "2004.13629", "submitter": "Masahiro Oda Dr.", "authors": "Masahiro Oda, Holger R. Roth, Takayuki Kitasaka, Kazuhiro Furukawa,\n  Ryoji Miyahara, Yoshiki Hirooka, Hidemi Goto, Nassir Navab, Kensaku Mori", "title": "Colon Shape Estimation Method for Colonoscope Tracking using Recurrent\n  Neural Networks", "comments": "Accepted paper as a poster presentation at MICCAI 2018 (International\n  Conference on Medical Image Computing and Computer-Assisted Intervention),\n  Granada, Spain", "journal-ref": "Published in Proceedings of MICCAI 2018, LNCS 11073, pp 176-184", "doi": "10.1007/978-3-030-00937-3_21", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimation method using a recurrent neural network (RNN) of the\ncolon's shape where deformation was occurred by a colonoscope insertion.\nColonoscope tracking or a navigation system that navigates physician to polyp\npositions is needed to reduce such complications as colon perforation. Previous\ntracking methods caused large tracking errors at the transverse and sigmoid\ncolons because these areas largely deform during colonoscope insertion. Colon\ndeformation should be taken into account in tracking processes. We propose a\ncolon deformation estimation method using RNN and obtain the colonoscope shape\nfrom electromagnetic sensors during its insertion into the colon. This method\nobtains positional, directional, and an insertion length from the colonoscope\nshape. From its shape, we also calculate the relative features that represent\nthe positional and directional relationships between two points on a\ncolonoscope. Long short-term memory is used to estimate the current colon shape\nfrom the past transition of the features of the colonoscope shape. We performed\ncolon shape estimation in a phantom study and correctly estimated the colon\nshapes during colonoscope insertion with 12.39 (mm) estimation error.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:43:58 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Oda", "Masahiro", ""], ["Roth", "Holger R.", ""], ["Kitasaka", "Takayuki", ""], ["Furukawa", "Kazuhiro", ""], ["Miyahara", "Ryoji", ""], ["Hirooka", "Yoshiki", ""], ["Goto", "Hidemi", ""], ["Navab", "Nassir", ""], ["Mori", "Kensaku", ""]]}, {"id": "2004.13630", "submitter": "Daniel Haehn", "authors": "Daniel Haehn, Loraine Franke, Fan Zhang, Suheyla Cetin Karayumak,\n  Steve Pieper, Lauren O'Donnell, Yogesh Rathi", "title": "TRAKO: Efficient Transmission of Tractography Data for Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fiber tracking produces large tractography datasets that are tens of\ngigabytes in size consisting of millions of streamlines. Such vast amounts of\ndata require formats that allow for efficient storage, transfer, and\nvisualization. We present TRAKO, a new data format based on the Graphics Layer\nTransmission Format (glTF) that enables immediate graphical and\nhardware-accelerated processing. We integrate a state-of-the-art compression\ntechnique for vertices, streamlines, and attached scalar and property data. We\nthen compare TRAKO to existing tractography storage methods and provide a\ndetailed evaluation on eight datasets. TRAKO can achieve data reductions of\nover 28x without loss of statistical significance when used to replicate\nanalysis from previously published studies.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 01:19:50 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Haehn", "Daniel", ""], ["Franke", "Loraine", ""], ["Zhang", "Fan", ""], ["Karayumak", "Suheyla Cetin", ""], ["Pieper", "Steve", ""], ["O'Donnell", "Lauren", ""], ["Rathi", "Yogesh", ""]]}, {"id": "2004.13649", "submitter": "Rob Fergus", "authors": "Ilya Kostrikov, Denis Yarats, Rob Fergus", "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement\n  Learning from Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple data augmentation technique that can be applied to\nstandard model-free reinforcement learning algorithms, enabling robust learning\ndirectly from pixels without the need for auxiliary losses or pre-training. The\napproach leverages input perturbations commonly used in computer vision tasks\nto regularize the value function. Existing model-free approaches, such as Soft\nActor-Critic (SAC), are not able to train deep networks effectively from image\npixels. However, the addition of our augmentation method dramatically improves\nSAC's performance, enabling it to reach state-of-the-art performance on the\nDeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC)\nmethods and recently proposed contrastive learning (CURL). Our approach can be\ncombined with any model-free reinforcement learning algorithm, requiring only\nminor modifications. An implementation can be found at\nhttps://sites.google.com/view/data-regularized-q.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:48:16 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 14:51:10 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 13:47:47 GMT"}, {"version": "v4", "created": "Sun, 7 Mar 2021 16:37:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Yarats", "Denis", ""], ["Fergus", "Rob", ""]]}, {"id": "2004.13652", "submitter": "Hu Cao", "authors": "Bin Li, Hu Cao, Zhongnan Qu, Yingbai Hu, Zhenke Wang, and Zichen Liang", "title": "Event-based Robotic Grasping Detection with Neuromorphic Vision Sensor\n  and Event-Stream Dataset", "comments": "submit to the Frontiers Neurorobotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic grasping plays an important role in the field of robotics. The\ncurrent state-of-the-art robotic grasping detection systems are usually built\non the conventional vision, such as RGB-D camera. Compared to traditional\nframe-based computer vision, neuromorphic vision is a small and young community\nof research. Currently, there are limited event-based datasets due to the\ntroublesome annotation of the asynchronous event stream. Annotating large scale\nvision dataset often takes lots of computation resources, especially the\ntroublesome data for video-level annotation. In this work, we consider the\nproblem of detecting robotic grasps in a moving camera view of a scene\ncontaining objects. To obtain more agile robotic perception, a neuromorphic\nvision sensor (DAVIS) attaching to the robot gripper is introduced to explore\nthe potential usage in grasping detection. We construct a robotic grasping\ndataset named Event-Stream Dataset with 91 objects. A spatio-temporal mixed\nparticle filter (SMP Filter) is proposed to track the led-based grasp\nrectangles which enables video-level annotation of a single grasp rectangle per\nobject. As leds blink at high frequency, the Event-Stream dataset is annotated\nin a high frequency of 1 kHz. Based on the Event-Stream dataset, we develop a\ndeep neural network for grasping detection which consider the angle learning\nproblem as classification instead of regression. The method performs high\ndetection accuracy on our Event-Stream dataset with 93% precision at\nobject-wise level. This work provides a large-scale and well-annotated dataset,\nand promotes the neuromorphic vision applications in agile robot.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 16:55:19 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:59:14 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Li", "Bin", ""], ["Cao", "Hu", ""], ["Qu", "Zhongnan", ""], ["Hu", "Yingbai", ""], ["Wang", "Zhenke", ""], ["Liang", "Zichen", ""]]}, {"id": "2004.13664", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Toru Lin, Kexin Yi, Daniel M. Bear, Daniel L. K. Yamins,\n  Jiajun Wu, Joshua B. Tenenbaum, Antonio Torralba", "title": "Visual Grounding of Learned Physical Models", "comments": "The second and the third authors contributed equally to this paper,\n  and are listed in alphabetical order. Project Page:\n  http://visual-physics-grounding.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans intuitively recognize objects' physical properties and predict their\nmotion, even when the objects are engaged in complicated interactions. The\nabilities to perform physical reasoning and to adapt to new environments, while\nintrinsic to humans, remain challenging to state-of-the-art computational\nmodels. In this work, we present a neural model that simultaneously reasons\nabout physics and makes future predictions based on visual and dynamics priors.\nThe visual prior predicts a particle-based representation of the system from\nvisual observations. An inference module operates on those particles,\npredicting and refining estimates of particle locations, object states, and\nphysical parameters, subject to the constraints imposed by the dynamics prior,\nwhich we refer to as visual grounding. We demonstrate the effectiveness of our\nmethod in environments involving rigid objects, deformable materials, and\nfluids. Experiments show that our model can infer the physical properties\nwithin a few observations, which allows the model to quickly adapt to unseen\nscenarios and make accurate predictions into the future.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:06:38 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 15:13:21 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Yunzhu", ""], ["Lin", "Toru", ""], ["Yi", "Kexin", ""], ["Bear", "Daniel M.", ""], ["Yamins", "Daniel L. K.", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""]]}, {"id": "2004.13665", "submitter": "Leonardo Rossi", "authors": "Leonardo Rossi, Akbar Karimi, Andrea Prati", "title": "A novel Region of Interest Extraction Layer for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the wide diffusion of deep neural network architectures for computer\nvision tasks, several new applications are nowadays more and more feasible.\nAmong them, a particular attention has been recently given to instance\nsegmentation, by exploiting the results achievable by two-stage networks (such\nas Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex\narchitectures, a crucial role is played by the Region of Interest (RoI)\nextraction layer, devoted to extracting a coherent subset of features from a\nsingle Feature Pyramid Network (FPN) layer attached on top of a backbone.\n  This paper is motivated by the need to overcome the limitations of existing\nRoI extractors which select only one (the best) layer from FPN. Our intuition\nis that all the layers of FPN retain useful information. Therefore, the\nproposed layer (called Generic RoI Extractor - GRoIE) introduces non-local\nbuilding blocks and attention mechanisms to boost the performance.\n  A comprehensive ablation study at component level is conducted to find the\nbest set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can\nbe integrated seamlessly with every two-stage architecture for both object\ndetection and instance segmentation tasks. Therefore, the improvements brought\nabout by the use of GRoIE in different state-of-the-art architectures are also\nevaluated. The proposed layer leads up to gain a 1.1% AP improvement on\nbounding box detection and 1.7% AP improvement on instance segmentation.\n  The code is publicly available on GitHub repository at\nhttps://github.com/IMPLabUniPr/mmdetection/tree/groie_dev\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:07:32 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 14:12:03 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Rossi", "Leonardo", ""], ["Karimi", "Akbar", ""], ["Prati", "Andrea", ""]]}, {"id": "2004.13674", "submitter": "Jie Cai", "authors": "Jie Cai, Zibo Meng, Chiu Man Ho", "title": "Residual Channel Attention Generative Adversarial Network for Image\n  Super-Resolution and Noise Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution is one of the important computer vision techniques\naiming to reconstruct high-resolution images from corresponding low-resolution\nones. Most recently, deep learning-based approaches have been demonstrated for\nimage super-resolution. However, as the deep networks go deeper, they become\nmore difficult to train and more difficult to restore the finer texture\ndetails, especially under real-world settings. In this paper, we propose a\nResidual Channel Attention-Generative Adversarial Network(RCA-GAN) to solve\nthese problems. Specifically, a novel residual channel attention block is\nproposed to form RCA-GAN, which consists of a set of residual blocks with\nshortcut connections, and a channel attention mechanism to model the\ninterdependence and interaction of the feature representations among different\nchannels. Besides, a generative adversarial network (GAN) is employed to\nfurther produce realistic and highly detailed results. Benefiting from these\nimprovements, the proposed RCA-GAN yields consistently better visual quality\nwith more detailed and natural textures than baseline models; and achieves\ncomparable or better performance compared with the state-of-the-art methods for\nreal-world image super-resolution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:23:46 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Cai", "Jie", ""], ["Meng", "Zibo", ""], ["Ho", "Chiu Man", ""]]}, {"id": "2004.13681", "submitter": "Pavel Rojtberg", "authors": "Pavel Rojtberg, Thomas P\\\"ollabauer and Arjan Kuijper", "title": "Style-transfer GANs for bridging the domain gap in synthetic pose\n  estimator training", "comments": null, "journal-ref": null, "doi": "10.1109/AIVR50618.2020.00039", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given the dependency of current CNN architectures on a large training set,\nthe possibility of using synthetic data is alluring as it allows generating a\nvirtually infinite amount of labeled training data. However, producing such\ndata is a non-trivial task as current CNN architectures are sensitive to the\ndomain gap between real and synthetic data. We propose to adopt general-purpose\nGAN models for pixel-level image translation, allowing to formulate the domain\ngap itself as a learning problem. The obtained models are then used either\nduring training or inference to bridge the domain gap. Here, we focus on\ntraining the single-stage YOLO6D object pose estimator on synthetic CAD\ngeometry only, where not even approximate surface information is available.\nWhen employing paired GAN models, we use an edge-based intermediate domain and\nintroduce different mappings to represent the unknown surface properties. Our\nevaluation shows a considerable improvement in model performance when compared\nto a model trained with the same degree of domain randomization, while\nrequiring only very little additional effort.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:35:03 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 19:06:13 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Rojtberg", "Pavel", ""], ["P\u00f6llabauer", "Thomas", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2004.13780", "submitter": "Shah Nawaz", "authors": "Muhammad Saad Saeed, Shah Nawaz, Pietro Morerio, Arif Mahmood, Ignazio\n  Gallo, Muhammad Haroon Yousaf, and Alessio Del Bue", "title": "Cross-modal Speaker Verification and Recognition: A Multilingual\n  Perspective", "comments": "Accepted: CVPRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen a surge in finding association between faces and\nvoices within a cross-modal biometric application along with speaker\nrecognition. Inspired from this, we introduce a challenging task in\nestablishing association between faces and voices across multiple languages\nspoken by the same set of persons. The aim of this paper is to answer two\nclosely related questions: \"Is face-voice association language independent?\"\nand \"Can a speaker be recognised irrespective of the spoken language?\". These\ntwo questions are very important to understand effectiveness and to boost\ndevelopment of multilingual biometric systems. To answer them, we collected a\nMultilingual Audio-Visual dataset, containing human speech clips of $154$\nidentities with $3$ language annotations extracted from various videos uploaded\nonline. Extensive experiments on the three splits of the proposed dataset have\nbeen performed to investigate and answer these novel research questions that\nclearly point out the relevance of the multilingual problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:15:23 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 15:10:21 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Saeed", "Muhammad Saad", ""], ["Nawaz", "Shah", ""], ["Morerio", "Pietro", ""], ["Mahmood", "Arif", ""], ["Gallo", "Ignazio", ""], ["Yousaf", "Muhammad Haroon", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2004.13795", "submitter": "Daniele Cattaneo", "authors": "Daniele Cattaneo, Domenico Giorgio Sorrenti, Abhinav Valada", "title": "CMRNet++: Map and Camera Agnostic Monocular Visual Localization in LiDAR\n  Maps", "comments": "Spotlight talk at IEEE ICRA 2020 Workshop on Emerging Learning and\n  Algorithmic Methods for Data Association in Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization is a critically essential and crucial enabler of autonomous\nrobots. While deep learning has made significant strides in many computer\nvision tasks, it is still yet to make a sizeable impact on improving\ncapabilities of metric visual localization. One of the major hindrances has\nbeen the inability of existing Convolutional Neural Network (CNN)-based pose\nregression methods to generalize to previously unseen places. Our recently\nintroduced CMRNet effectively addresses this limitation by enabling map\nindependent monocular localization in LiDAR-maps. In this paper, we now take it\na step further by introducing CMRNet++, which is a significantly more robust\nmodel that not only generalizes to new places effectively, but is also\nindependent of the camera parameters. We enable this capability by combining\ndeep learning with geometric techniques, and by moving the metric reasoning\noutside the learning process. In this way, the weights of the network are not\ntied to a specific camera. Extensive evaluations of CMRNet++ on three\nchallenging autonomous driving datasets, i.e., KITTI, Argoverse, and Lyft5,\nshow that CMRNet++ outperforms CMRNet as well as other baselines by a large\nmargin. More importantly, for the first-time, we demonstrate the ability of a\ndeep learning approach to accurately localize without any retraining or\nfine-tuning in a completely new environment and independent of the camera\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:10:14 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 09:00:25 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Cattaneo", "Daniele", ""], ["Sorrenti", "Domenico Giorgio", ""], ["Valada", "Abhinav", ""]]}, {"id": "2004.13799", "submitter": "Michael McCoyd", "authors": "Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper,\n  Minjune Hwang, Jason Xinyu Liu and David Wagner", "title": "Minority Reports Defense: Defending Against Adversarial Patches", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning image classification is vulnerable to adversarial attack, even\nif the attacker changes just a small patch of the image. We propose a defense\nagainst patch attacks based on partially occluding the image around each\ncandidate patch location, so that a few occlusions each completely hide the\npatch. We demonstrate on CIFAR-10, Fashion MNIST, and MNIST that our defense\nprovides certified security against patch attacks of a certain size.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 20:11:18 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["McCoyd", "Michael", ""], ["Park", "Won", ""], ["Chen", "Steven", ""], ["Shah", "Neil", ""], ["Roggenkemper", "Ryan", ""], ["Hwang", "Minjune", ""], ["Liu", "Jason Xinyu", ""], ["Wagner", "David", ""]]}, {"id": "2004.13824", "submitter": "Yiqun Mei", "authors": "Yiqun Mei, Yuchen Fan, Yulun Zhang, Jiahui Yu, Yuqian Zhou, Ding Liu,\n  Yun Fu, Thomas S. Huang and Humphrey Shi", "title": "Pyramid Attention Networks for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-similarity refers to the image prior widely used in image restoration\nalgorithms that small but similar patterns tend to occur at different locations\nand scales. However, recent advanced deep convolutional neural network based\nmethods for image restoration do not take full advantage of self-similarities\nby relying on self-attention neural modules that only process information at\nthe same scale. To solve this problem, we present a novel Pyramid Attention\nmodule for image restoration, which captures long-range feature correspondences\nfrom a multi-scale feature pyramid. Inspired by the fact that corruptions, such\nas noise or compression artifacts, drop drastically at coarser image scales,\nour attention module is designed to be able to borrow clean signals from their\n\"clean\" correspondences at the coarser levels. The proposed pyramid attention\nmodule is a generic building block that can be flexibly integrated into various\nneural architectures. Its effectiveness is validated through extensive\nexperiments on multiple image restoration tasks: image denoising, demosaicing,\ncompression artifact reduction, and super resolution. Without any bells and\nwhistles, our PANet (pyramid attention module with simple network backbones)\ncan produce state-of-the-art results with superior accuracy and visual quality.\nOur code will be available at\nhttps://github.com/SHI-Labs/Pyramid-Attention-Networks\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:12:36 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 00:58:54 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 14:11:21 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 18:47:11 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Mei", "Yiqun", ""], ["Fan", "Yuchen", ""], ["Zhang", "Yulun", ""], ["Yu", "Jiahui", ""], ["Zhou", "Yuqian", ""], ["Liu", "Ding", ""], ["Fu", "Yun", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "2004.13846", "submitter": "Kenya Sakka", "authors": "Kenya Sakka, Kotaro Nakayama, Nisei Kimura, Taiki Inoue, Yusuke\n  Iwasawa, Ryohei Yamaguchi, Yosimasa Kawazoe, Kazuhiko Ohe, Yutaka Matsuo", "title": "Character-level Japanese Text Generation with Attention Mechanism for\n  Chest Radiography Diagnosis", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiography is a general method for diagnosing a patient's condition\nand identifying important information; therefore, radiography is used\nextensively in routine medical practice in various situations, such as\nemergency medical care and medical checkup. However, a high level of expertise\nis required to interpret chest radiographs. Thus, medical specialists spend\nconsiderable time in diagnosing such huge numbers of radiographs. In order to\nsolve these problems, methods for generating findings have been proposed.\nHowever, the study of generating chest radiograph findings has primarily\nfocused on the English language, and to the best of our knowledge, no studies\nhave studied Japanese data on this subject. There are two challenges involved\nin generating findings in the Japanese language. The first challenge is that\nword splitting is difficult because the boundaries of Japanese word are not\nclear. The second challenge is that there are numerous orthographic variants.\nFor deal with these two challenges, we proposed an end-to-end model that\ngenerates Japanese findings at the character-level from chest radiographs. In\naddition, we introduced the attention mechanism to improve not only the\naccuracy, but also the interpretation ability of the results. We evaluated the\nproposed method using a public dataset with Japanese findings. The\neffectiveness of the proposed method was confirmed using the Bilingual\nEvaluation Understudy score. And, we were confirmed from the generated findings\nthat the proposed method was able to consider the orthographic variants.\nFurthermore, we confirmed via visual inspection that the attention mechanism\ncaptures the features and positional information of radiographs.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 18:19:27 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 05:37:51 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sakka", "Kenya", ""], ["Nakayama", "Kotaro", ""], ["Kimura", "Nisei", ""], ["Inoue", "Taiki", ""], ["Iwasawa", "Yusuke", ""], ["Yamaguchi", "Ryohei", ""], ["Kawazoe", "Yosimasa", ""], ["Ohe", "Kazuhiko", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2004.13849", "submitter": "Dario Fontanel", "authors": "Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Samuel Rota\n  Bul\\`o, Elisa Ricci, Barbara Caputo", "title": "Boosting Deep Open World Recognition by Clustering", "comments": "IROS/RAL 2020", "journal-ref": "IEEE Robotics and Automation Letters 2020", "doi": "10.1109/LRA.2020.3010753", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks have brought significant advances in\nrobot vision, their ability is often limited to closed world scenarios, where\nthe number of semantic concepts to be recognized is determined by the available\ntraining set. Since it is practically impossible to capture all possible\nsemantic concepts present in the real world in a single training set, we need\nto break the closed world assumption, equipping our robot with the capability\nto act in an open world. To provide such ability, a robot vision system should\nbe able to (i) identify whether an instance does not belong to the set of known\ncategories (i.e. open set recognition), and (ii) extend its knowledge to learn\nnew classes over time (i.e. incremental learning). In this work, we show how we\ncan boost the performance of deep open world recognition algorithms by means of\na new loss formulation enforcing a global to local clustering of class-specific\nfeatures. In particular, a first loss term, i.e. global clustering, forces the\nnetwork to map samples closer to the class centroid they belong to while the\nsecond one, local clustering, shapes the representation space in such a way\nthat samples of the same class get closer in the representation space while\npushing away neighbours belonging to other classes. Moreover, we propose a\nstrategy to learn class-specific rejection thresholds, instead of heuristically\nestimating a single global threshold, as in previous works. Experiments on\nRGB-D Object and Core50 datasets show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 12:07:39 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 09:35:37 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Fontanel", "Dario", ""], ["Cermelli", "Fabio", ""], ["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""]]}, {"id": "2004.13856", "submitter": "Vinicius Ribeiro", "authors": "Vinicius Ribeiro, Sandra Avila, Eduardo Valle", "title": "Less is More: Sample Selection and Label Conditioning Improve Skin\n  Lesion Segmentation", "comments": "Accepted to the ISIC Skin Image Analysis Workshop @ CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting skin lesions images is relevant both for itself and for assisting\nin lesion classification, but suffers from the challenge in obtaining annotated\ndata. In this work, we show that segmentation may improve with less data, by\nselecting the training samples with best inter-annotator agreement, and\nconditioning the ground-truth masks to remove excessive detail. We perform an\nexhaustive experimental design considering several sources of variation,\nincluding three different test sets, two different deep-learning architectures,\nand several replications, for a total of 540 experimental runs. We found that\nsample selection and detail removal may have impacts corresponding,\nrespectively, to 12% and 16% of the one obtained by picking a better\ndeep-learning model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:24:51 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Ribeiro", "Vinicius", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "2004.13866", "submitter": "Nikita Jaipuria", "authors": "Nikita Jaipuria, Xianling Zhang, Rohan Bhasin, Mayar Arafa, Punarjay\n  Chakravarty, Shubham Shrivastava, Sagar Manglani, Vidya N. Murali", "title": "Deflating Dataset Bias Using Synthetic Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has seen an unprecedented increase in vision applications since\nthe publication of large-scale object recognition datasets and introduction of\nscalable compute hardware. State-of-the-art methods for most vision tasks for\nAutonomous Vehicles (AVs) rely on supervised learning and often fail to\ngeneralize to domain shifts and/or outliers. Dataset diversity is thus key to\nsuccessful real-world deployment. No matter how big the size of the dataset,\ncapturing long tails of the distribution pertaining to task-specific\nenvironmental factors is impractical. The goal of this paper is to investigate\nthe use of targeted synthetic data augmentation - combining the benefits of\ngaming engine simulations and sim2real style transfer techniques - for filling\ngaps in real datasets for vision tasks. Empirical studies on three different\ncomputer vision tasks of practical use to AVs - parking slot detection, lane\ndetection and monocular depth estimation - consistently show that having\nsynthetic data in the training mix provides a significant boost in\ncross-dataset generalization performance as compared to training on real data\nonly, for the same size of the training set.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:56:10 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jaipuria", "Nikita", ""], ["Zhang", "Xianling", ""], ["Bhasin", "Rohan", ""], ["Arafa", "Mayar", ""], ["Chakravarty", "Punarjay", ""], ["Shrivastava", "Shubham", ""], ["Manglani", "Sagar", ""], ["Murali", "Vidya N.", ""]]}, {"id": "2004.13874", "submitter": "Ronald Wilson", "authors": "Ronald Wilson, Navid Asadizanjani, Domenic Forte and Damon L. Woodard", "title": "Histogram-based Auto Segmentation: A Novel Approach to Segmenting\n  Integrated Circuit Structures from SEM Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Reverse Engineering and Hardware Assurance domain, a majority of the\ndata acquisition is done through electron microscopy techniques such as\nScanning Electron Microscopy (SEM). However, unlike its counterparts in optical\nimaging, only a limited number of techniques are available to enhance and\nextract information from the raw SEM images. In this paper, we introduce an\nalgorithm to segment out Integrated Circuit (IC) structures from the SEM image.\nUnlike existing algorithms discussed in this paper, this algorithm is\nunsupervised, parameter-free and does not require prior information on the\nnoise model or features in the target image making it effective in low quality\nimage acquisition scenarios as well. Furthermore, the results from the\napplication of the algorithm on various structures and layers in the IC are\nreported and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:24:08 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wilson", "Ronald", ""], ["Asadizanjani", "Navid", ""], ["Forte", "Domenic", ""], ["Woodard", "Damon L.", ""]]}, {"id": "2004.13877", "submitter": "Jaime Forero-Romero", "authors": "Catalina G\\'omez, Mauricio Neira, Marcela Hern\\'andez Hoyos, Pablo\n  Arbel\\'aez, Jaime E. Forero-Romero", "title": "Classifying Image Sequences of Astronomical Transients with Deep Neural\n  Networks", "comments": "11 pages, 7 figures. MNRAS accepted", "journal-ref": null, "doi": "10.1093/mnras/staa2973", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised classification of temporal sequences of astronomical images into\nmeaningful transient astrophysical phenomena has been considered a hard problem\nbecause it requires the intervention of human experts. The classifier uses the\nexpert's knowledge to find heuristic features to process the images, for\ninstance, by performing image subtraction or by extracting sparse information\nsuch as flux time series, also known as light curves. We present a successful\ndeep learning approach that learns directly from imaging data. Our method\nmodels explicitly the spatio-temporal patterns with Deep Convolutional Neural\nNetworks and Gated Recurrent Units. We train these deep neural networks using\n1.3 million real astronomical images from the Catalina Real-Time Transient\nSurvey to classify the sequences into five different types of astronomical\ntransient classes. The TAO-Net (for Transient Astronomical Objects Network)\narchitecture outperforms the results from random forest classification on light\ncurves by 10 percentage points as measured by the F1 score for each class; the\naverage F1 over classes goes from $45\\%$ with random forest classification to\n$55\\%$ with TAO-Net. This achievement with TAO-Net opens the possibility to\ndevelop new deep learning architectures for early transient detection. We make\navailable the training dataset and trained models of TAO-Net to allow for\nfuture extensions of this work.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:29:01 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:18:00 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["G\u00f3mez", "Catalina", ""], ["Neira", "Mauricio", ""], ["Hoyos", "Marcela Hern\u00e1ndez", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Forero-Romero", "Jaime E.", ""]]}, {"id": "2004.13883", "submitter": "Moulay Akhloufi", "authors": "Moulay A. Akhloufi, Nicolas A. Castro, Andy Couturier", "title": "Unmanned Aerial Systems for Wildland and Forest Fires", "comments": "A recent published version of this paper is available at:\n  https://doi.org/10.3390/drones5010015", "journal-ref": "Drones. 2021; 5(1):15; pp 1-25", "doi": "10.3390/drones5010015", "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wildfires represent an important natural risk causing economic losses, human\ndeath and important environmental damage. In recent years, we witness an\nincrease in fire intensity and frequency. Research has been conducted towards\nthe development of dedicated solutions for wildland and forest fire assistance\nand fighting. Systems were proposed for the remote detection and tracking of\nfires. These systems have shown improvements in the area of efficient data\ncollection and fire characterization within small scale environments. However,\nwildfires cover large areas making some of the proposed ground-based systems\nunsuitable for optimal coverage. To tackle this limitation, Unmanned Aerial\nSystems (UAS) were proposed. UAS have proven to be useful due to their\nmaneuverability, allowing for the implementation of remote sensing, allocation\nstrategies and task planning. They can provide a low-cost alternative for the\nprevention, detection and real-time support of firefighting. In this paper we\nreview previous work related to the use of UAS in wildfires. Onboard sensor\ninstruments, fire perception algorithms and coordination strategies are\nconsidered. In addition, we present some of the recent frameworks proposing the\nuse of both aerial vehicles and Unmanned Ground Vehicles (UV) for a more\nefficient wildland firefighting strategy at a larger scale.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:01:12 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:33:41 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Akhloufi", "Moulay A.", ""], ["Castro", "Nicolas A.", ""], ["Couturier", "Andy", ""]]}, {"id": "2004.13903", "submitter": "Evan Yu", "authors": "Evan M. Yu, Juan Eugenio Iglesias, Adrian V. Dalca, Mert R. Sabuncu", "title": "An Auto-Encoder Strategy for Adaptive Image Segmentation", "comments": "MIDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are powerful tools for biomedical image segmentation.\nThese models are often trained with heavy supervision, relying on pairs of\nimages and corresponding voxel-level labels. However, obtaining segmentations\nof anatomical regions on a large number of cases can be prohibitively\nexpensive. Thus there is a strong need for deep learning-based segmentation\ntools that do not require heavy supervision and can continuously adapt. In this\npaper, we propose a novel perspective of segmentation as a discrete\nrepresentation learning problem, and present a variational autoencoder\nsegmentation strategy that is flexible and adaptive. Our method, called\nSegmentation Auto-Encoder (SAE), leverages all available unlabeled scans and\nmerely requires a segmentation prior, which can be a single unpaired\nsegmentation image. In experiments, we apply SAE to brain MRI scans. Our\nresults show that SAE can produce good quality segmentations, particularly when\nthe prior is good. We demonstrate that a Markov Random Field prior can yield\nsignificantly better results than a spatially independent prior. Our code is\nfreely available at https://github.com/evanmy/sae.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 00:53:24 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Yu", "Evan M.", ""], ["Iglesias", "Juan Eugenio", ""], ["Dalca", "Adrian V.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2004.13931", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou", "title": "Span-based Localizing Network for Natural Language Video Localization", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an untrimmed video and a text query, natural language video\nlocalization (NLVL) is to locate a matching span from the video that\nsemantically corresponds to the query. Existing solutions formulate NLVL either\nas a ranking task and apply multimodal matching architecture, or as a\nregression task to directly regress the target video span. In this work, we\naddress NLVL task with a span-based QA approach by treating the input video as\ntext passage. We propose a video span localizing network (VSLNet), on top of\nthe standard span-based QA framework, to address NLVL. The proposed VSLNet\ntackles the differences between NLVL and span-based QA through a simple yet\neffective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to\nsearch for matching video span within a highlighted region. Through extensive\nexperiments on three benchmark datasets, we show that the proposed VSLNet\noutperforms the state-of-the-art methods; and adopting span-based QA framework\nis a promising direction to solve NLVL.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:47:04 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 08:49:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Zhou", "Joey Tianyi", ""]]}, {"id": "2004.13959", "submitter": "Mohammadhossein Toutiaee", "authors": "Mohammadhossein Toutiaee, Abbas Keshavarzi, Abolfazl Farahani, John A.\n  Miller", "title": "Video Contents Understanding using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel application of Transfer Learning to classify video-frame\nsequences over multiple classes. This is a pre-weighted model that does not\nrequire to train a fresh CNN. This representation is achieved with the advent\nof \"deep neural network\" (DNN), which is being studied these days by many\nresearchers. We utilize the classical approaches for video classification task\nusing object detection techniques for comparison, such as \"Google Video\nIntelligence API\" and this study will run experiments as to how those\narchitectures would perform in foggy or rainy weather conditions. Experimental\nevaluation on video collections shows that the new proposed classifier achieves\nsuperior performance over existing solutions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 05:18:40 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Toutiaee", "Mohammadhossein", ""], ["Keshavarzi", "Abbas", ""], ["Farahani", "Abolfazl", ""], ["Miller", "John A.", ""]]}, {"id": "2004.13973", "submitter": "Sriram Baireddy", "authors": "Enyu Cai, Sriram Baireddy, Changye Yang, Melba Crawford, Edward J.\n  Delp", "title": "Deep Transfer Learning For Plant Center Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant phenotyping focuses on the measurement of plant characteristics\nthroughout the growing season, typically with the goal of evaluating genotypes\nfor plant breeding. Estimating plant location is important for identifying\ngenotypes which have low emergence, which is also related to the environment\nand management practices such as fertilizer applications. The goal of this\npaper is to investigate methods that estimate plant locations for a field-based\ncrop using RGB aerial images captured using Unmanned Aerial Vehicles (UAVs).\nDeep learning approaches provide promising capability for locating plants\nobserved in RGB images, but they require large quantities of labeled data\n(ground truth) for training. Using a deep learning architecture fine-tuned on a\nsingle field or a single type of crop on fields in other geographic areas or\nwith other crops may not have good results. The problem of generating ground\ntruth for each new field is labor-intensive and tedious. In this paper, we\npropose a method for estimating plant centers by transferring an existing model\nto a new scenario using limited ground truth data. We describe the use of\ntransfer learning using a model fine-tuned for a single field or a single type\nof plant on a varied set of similar crops and fields. We show that transfer\nlearning provides promising results for detecting plant locations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:29:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Cai", "Enyu", ""], ["Baireddy", "Sriram", ""], ["Yang", "Changye", ""], ["Crawford", "Melba", ""], ["Delp", "Edward J.", ""]]}, {"id": "2004.13977", "submitter": "Xinbo Yu", "authors": "Bruce X. B. Yu, Yan Liu, Keith C. C. Chan", "title": "Effective Human Activity Recognition Based on Small Datasets", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent work on vision-based human activity recognition (HAR) focuses on\ndesigning complex deep learning models for the task. In so doing, there is a\nrequirement for large datasets to be collected. As acquiring and processing\nlarge training datasets are usually very expensive, the problem of how dataset\nsize can be reduced without affecting recognition accuracy has to be tackled.\nTo do so, we propose a HAR method that consists of three steps: (i) data\ntransformation involving the generation of new features based on transforming\nof raw data, (ii) feature extraction involving the learning of a classifier\nbased on the AdaBoost algorithm and the use of training data consisting of the\ntransformed features, and (iii) parameter determination and pattern recognition\ninvolving the determination of parameters based on the features generated in\n(ii) and the use of the parameters as training data for deep learning\nalgorithms to be used to recognize human activities. Compared to existing\napproaches, this proposed approach has the advantageous characteristics that it\nis simple and robust. The proposed approach has been tested with a number of\nexperiments performed on a relatively small real dataset. The experimental\nresults indicate that using the proposed method, human activities can be more\naccurately recognized even with smaller training data size.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:38:23 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Yu", "Bruce X. B.", ""], ["Liu", "Yan", ""], ["Chan", "Keith C. C.", ""]]}, {"id": "2004.13979", "submitter": "Xinbo Yu", "authors": "Bruce X. B. Yu, Yan Liu, Keith C. C. Chan", "title": "Skeleton Focused Human Activity Recognition in RGB Video", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data-driven approach that learns an optimal representation of vision\nfeatures like skeleton frames or RGB videos is currently a dominant paradigm\nfor activity recognition. While great improvements have been achieved from\nexisting single modal approaches with increasingly larger datasets, the fusion\nof various data modalities at the feature level has seldom been attempted. In\nthis paper, we propose a multimodal feature fusion model that utilizes both\nskeleton and RGB modalities to infer human activity. The objective is to\nimprove the activity recognition accuracy by effectively utilizing the mutual\ncomplemental information among different data modalities. For the skeleton\nmodality, we propose to use a graph convolutional subnetwork to learn the\nskeleton representation. Whereas for the RGB modality, we will use the\nspatial-temporal region of interest from RGB videos and take the attention\nfeatures from the skeleton modality to guide the learning process. The model\ncould be either individually or uniformly trained by the back-propagation\nalgorithm in an end-to-end manner. The experimental results for the NTU-RGB+D\nand Northwestern-UCLA Multiview datasets achieved state-of-the-art performance,\nwhich indicates that the proposed skeleton-driven attention mechanism for the\nRGB modality increases the mutual communication between different data\nmodalities and brings more discriminative features for inferring human\nactivities.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:40:42 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Yu", "Bruce X. B.", ""], ["Liu", "Yan", ""], ["Chan", "Keith C. C.", ""]]}, {"id": "2004.13985", "submitter": "Jingbo Wang", "authors": "Jingbo Wang, Sijie Yan, Yuanjun Xiong, Dahua Lin", "title": "Motion Guided 3D Pose Estimation from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new loss function, called motion loss, for the problem of\nmonocular 3D Human pose estimation from 2D pose. In computing motion loss, a\nsimple yet effective representation for keypoint motion, called pairwise motion\nencoding, is introduced. We design a new graph convolutional network\narchitecture, U-shaped GCN (UGCN). It captures both short-term and long-term\nmotion information to fully leverage the additional supervision from the motion\nloss. We experiment training UGCN with the motion loss on two large scale\nbenchmarks: Human3.6M and MPI-INF-3DHP. Our model surpasses other\nstate-of-the-art models by a large margin. It also demonstrates strong capacity\nin producing smooth 3D sequences and recovering keypoint motion.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:59:30 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wang", "Jingbo", ""], ["Yan", "Sijie", ""], ["Xiong", "Yuanjun", ""], ["Lin", "Dahua", ""]]}, {"id": "2004.13992", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI), Christine Vartin (HCL), Peter\n  Boyle (IPRI, SIGPH@iPRI), Laurent Kodjikian (MATEIS, HCL)", "title": "Retinal vessel segmentation by probing adaptive to lighting variations", "comments": "Proceedings of 2020 IEEE 17th International Symposium on Biomedical\n  Imaging (ISBI).To appear in https://ieeexplore.ieee.org", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging\n  (ISBI), IEEE, Apr 2020, Iowa City, United States. pp.1246-1249", "doi": "10.1109/ISBI45749.2020.9098332", "report-no": null, "categories": "cs.CV cs.NA eess.SP math.NA physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method to extract the vessels in eye fun-dus images\nwhich is adaptive to lighting variations. In the Logarithmic Image Processing\nframework, a 3-segment probe detects the vessels by probing the topographic\nsurface of an image from below. A map of contrasts between the probe and the\nimage allows to detect the vessels by a threshold. In a lowly contrasted image,\nresults show that our method better extract the vessels than another state-of\nthe-art method. In a highly contrasted image database (DRIVE) with a reference\n, ours has an accuracy of 0.9454 which is similar or better than three\nstate-of-the-art methods and below three others. The three best methods have a\nhigher accuracy than a manual segmentation by another expert. Importantly, our\nmethod automatically adapts to the lighting conditions of the image\nacquisition.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 07:10:17 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"], ["Vartin", "Christine", "", "HCL"], ["Boyle", "Peter", "", "IPRI, SIGPH@iPRI"], ["Kodjikian", "Laurent", "", "MATEIS, HCL"]]}, {"id": "2004.14003", "submitter": "Arjun Desai", "authors": "Arjun D. Desai, Francesco Caliva, Claudia Iriondo, Naji Khosravan,\n  Aliasghar Mortazi, Sachin Jambawalikar, Drew Torigian, Jutta Ellermann,\n  Mehmet Akcakaya, Ulas Bagci, Radhika Tibrewala, Io Flament, Matthew O`Brien,\n  Sharmila Majumdar, Mathias Perslev, Akshay Pai, Christian Igel, Erik B. Dam,\n  Sibaji Gaj, Mingrui Yang, Kunio Nakamura, Xiaojuan Li, Cem M. Deniz, Vladimir\n  Juras, Ravinder Regatte, Garry E. Gold, Brian A. Hargreaves, Valentina\n  Pedoia, Akshay S. Chaudhari", "title": "The International Workshop on Osteoarthritis Imaging Knee MRI\n  Segmentation Challenge: A Multi-Institute Evaluation and Analysis Framework\n  on a Standardized Dataset", "comments": "Submitted to Radiology: Artificial Intelligence; Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To organize a knee MRI segmentation challenge for characterizing the\nsemantic and clinical efficacy of automatic segmentation methods relevant for\nmonitoring osteoarthritis progression.\n  Methods: A dataset partition consisting of 3D knee MRI from 88 subjects at\ntwo timepoints with ground-truth articular (femoral, tibial, patellar)\ncartilage and meniscus segmentations was standardized. Challenge submissions\nand a majority-vote ensemble were evaluated using Dice score, average symmetric\nsurface distance, volumetric overlap error, and coefficient of variation on a\nhold-out test set. Similarities in network segmentations were evaluated using\npairwise Dice correlations. Articular cartilage thickness was computed per-scan\nand longitudinally. Correlation between thickness error and segmentation\nmetrics was measured using Pearson's coefficient. Two empirical upper bounds\nfor ensemble performance were computed using combinations of model outputs that\nconsolidated true positives and true negatives.\n  Results: Six teams (T1-T6) submitted entries for the challenge. No\nsignificant differences were observed across all segmentation metrics for all\ntissues (p=1.0) among the four top-performing networks (T2, T3, T4, T6). Dice\ncorrelations between network pairs were high (>0.85). Per-scan thickness errors\nwere negligible among T1-T4 (p=0.99) and longitudinal changes showed minimal\nbias (<0.03mm). Low correlations (<0.41) were observed between segmentation\nmetrics and thickness error. The majority-vote ensemble was comparable to top\nperforming networks (p=1.0). Empirical upper bound performances were similar\nfor both combinations (p=1.0).\n  Conclusion: Diverse networks learned to segment the knee similarly where high\nsegmentation accuracy did not correlate to cartilage thickness accuracy. Voting\nensembles did not outperform individual networks but may help regularize\nindividual models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 07:54:26 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 19:24:14 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Desai", "Arjun D.", ""], ["Caliva", "Francesco", ""], ["Iriondo", "Claudia", ""], ["Khosravan", "Naji", ""], ["Mortazi", "Aliasghar", ""], ["Jambawalikar", "Sachin", ""], ["Torigian", "Drew", ""], ["Ellermann", "Jutta", ""], ["Akcakaya", "Mehmet", ""], ["Bagci", "Ulas", ""], ["Tibrewala", "Radhika", ""], ["Flament", "Io", ""], ["O`Brien", "Matthew", ""], ["Majumdar", "Sharmila", ""], ["Perslev", "Mathias", ""], ["Pai", "Akshay", ""], ["Igel", "Christian", ""], ["Dam", "Erik B.", ""], ["Gaj", "Sibaji", ""], ["Yang", "Mingrui", ""], ["Nakamura", "Kunio", ""], ["Li", "Xiaojuan", ""], ["Deniz", "Cem M.", ""], ["Juras", "Vladimir", ""], ["Regatte", "Ravinder", ""], ["Gold", "Garry E.", ""], ["Hargreaves", "Brian A.", ""], ["Pedoia", "Valentina", ""], ["Chaudhari", "Akshay S.", ""]]}, {"id": "2004.14010", "submitter": "Laura Zabawa", "authors": "Laura Zabawa, Anna Kicherer, Lasse Klingbeil, Reinhard T\\\"opfer,\n  Heiner Kuhlmann, Ribana Roscher", "title": "Counting of Grapevine Berries in Images via Semantic Segmentation using\n  Convolutional Neural Networks", "comments": null, "journal-ref": "Journal of Photogrammetry and Remote Sensing, vol. 164,\n  pp.73-83,2020", "doi": "10.1016/j.isprsjprs.2020.04.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of phenotypic traits is often very time and labour intensive.\nEspecially the investigation in viticulture is restricted to an on-site\nanalysis due to the perennial nature of grapevine. Traditionally skilled\nexperts examine small samples and extrapolate the results to a whole plot.\nThereby different grapevine varieties and training systems, e.g. vertical shoot\npositioning (VSP) and semi minimal pruned hedges (SMPH) pose different\nchallenges. In this paper we present an objective framework based on automatic\nimage analysis which works on two different training systems. The images are\ncollected semi automatic by a camera system which is installed in a modified\ngrape harvester. The system produces overlapping images from the sides of the\nplants. Our framework uses a convolutional neural network to detect single\nberries in images by performing a semantic segmentation. Each berry is then\ncounted with a connected component algorithm. We compare our results with the\nMask-RCNN, a state-of-the-art network for instance segmentation and with a\nregression approach for counting. The experiments presented in this paper show\nthat we are able to detect green berries in images despite of different\ntraining systems. We achieve an accuracy for the berry detection of 94.0% in\nthe VSP and 85.6% in the SMPH.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:10:19 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zabawa", "Laura", ""], ["Kicherer", "Anna", ""], ["Klingbeil", "Lasse", ""], ["T\u00f6pfer", "Reinhard", ""], ["Kuhlmann", "Heiner", ""], ["Roscher", "Ribana", ""]]}, {"id": "2004.14043", "submitter": "Yunpei Jia", "authors": "Yunpei Jia, Jie Zhang, Shiguang Shan, Xilin Chen", "title": "Single-Side Domain Generalization for Face Anti-Spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain generalization methods for face anti-spoofing endeavor to\nextract common differentiation features to improve the generalization. However,\ndue to large distribution discrepancies among fake faces of different domains,\nit is difficult to seek a compact and generalized feature space for the fake\nfaces. In this work, we propose an end-to-end single-side domain generalization\nframework (SSDG) to improve the generalization ability of face anti-spoofing.\nThe main idea is to learn a generalized feature space, where the feature\ndistribution of the real faces is compact while that of the fake ones is\ndispersed among domains but compact within each domain. Specifically, a feature\ngenerator is trained to make only the real faces from different domains\nundistinguishable, but not for the fake ones, thus forming a single-side\nadversarial learning. Moreover, an asymmetric triplet loss is designed to\nconstrain the fake faces of different domains separated while the real ones\naggregated. The above two points are integrated into a unified framework in an\nend-to-end training manner, resulting in a more generalized class boundary,\nespecially good for samples from novel domains. Feature and weight\nnormalization is incorporated to further improve the generalization ability.\nExtensive experiments show that our proposed approach is effective and\noutperforms the state-of-the-art methods on four public databases.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 09:32:54 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jia", "Yunpei", ""], ["Zhang", "Jie", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2004.14052", "submitter": "Cenek Albl", "authors": "Zuzana Kukelova, Cenek Albl, Akihiro Sugimoto, Konrad Schindler, Tomas\n  Pajdla", "title": "Minimal Rolling Shutter Absolute Pose with Unknown Focal Length and\n  Radial Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal geometry of most modern consumer cameras is not adequately\ndescribed by the perspective projection. Almost all cameras exhibit some radial\nlens distortion and are equipped with an electronic rolling shutter that\ninduces distortions when the camera moves during the image capture. When focal\nlength has not been calibrated offline, the parameters that describe the radial\nand rolling shutter distortions are usually unknown. While for global shutter\ncameras, minimal solvers for the absolute camera pose and unknown focal length\nand radial distortion are available, solvers for the rolling shutter were\nmissing. We present the first minimal solutions for the absolute pose of a\nrolling shutter camera with unknown rolling shutter parameters, focal length,\nand radial distortion. Our new minimal solvers combine iterative schemes\ndesigned for calibrated rolling shutter cameras with fast generalized\neigenvalue and Groebner basis solvers. In a series of experiments, with both\nsynthetic and real data, we show that our new solvers provide accurate\nestimates of the camera pose, rolling shutter parameters, focal length, and\nradial distortion parameters.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:03:03 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kukelova", "Zuzana", ""], ["Albl", "Cenek", ""], ["Sugimoto", "Akihiro", ""], ["Schindler", "Konrad", ""], ["Pajdla", "Tomas", ""]]}, {"id": "2004.14071", "submitter": "Noa Fish", "authors": "Noa Fish, Richard Zhang, Lilach Perry, Daniel Cohen-Or, Eli Shechtman,\n  Connelly Barnes", "title": "Image Morphing with Perceptual Constraints and STN Alignment", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.14027", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image morphing, a sequence of plausible frames are synthesized and\ncomposited together to form a smooth transformation between given instances.\nIntermediates must remain faithful to the input, stand on their own as members\nof the set, and maintain a well-paced visual transition from one to the next.\nIn this paper, we propose a conditional GAN morphing framework operating on a\npair of input images. The network is trained to synthesize frames corresponding\nto temporal samples along the transformation, and learns a proper shape prior\nthat enhances the plausibility of intermediate frames. While individual frame\nplausibility is boosted by the adversarial setup, a special training protocol\nproducing sequences of frames, combined with a perceptual similarity loss,\npromote smooth transformation over time. Explicit stating of correspondences is\nreplaced with a grid-based freeform deformation spatial transformer that\npredicts the geometric warp between the inputs, instituting the smooth\ngeometric effect by bringing the shapes into an initial alignment. We provide\ncomparisons to classic as well as latent space morphing techniques, and\ndemonstrate that, given a set of images for self-supervision, our network\nlearns to generate visually pleasing morphing effects featuring believable\nin-betweens, with robustness to changes in shape and texture, requiring no\ncorrespondence annotation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:49:10 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Fish", "Noa", ""], ["Zhang", "Richard", ""], ["Perry", "Lilach", ""], ["Cohen-Or", "Daniel", ""], ["Shechtman", "Eli", ""], ["Barnes", "Connelly", ""]]}, {"id": "2004.14079", "submitter": "Dan Jia", "authors": "Dan Jia, Alexander Hermans, and Bastian Leibe", "title": "DR-SPAAM: A Spatial-Attention and Auto-regressive Model for Person\n  Detection in 2D Range Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting persons using a 2D LiDAR is a challenging task due to the low\ninformation content of 2D range data. To alleviate the problem caused by the\nsparsity of the LiDAR points, current state-of-the-art methods fuse multiple\nprevious scans and perform detection using the combined scans. The downside of\nsuch a backward looking fusion is that all the scans need to be aligned\nexplicitly, and the necessary alignment operation makes the whole pipeline more\nexpensive -- often too expensive for real-world applications. In this paper, we\npropose a person detection network which uses an alternative strategy to\ncombine scans obtained at different times. Our method, Distance Robust SPatial\nAttention and Auto-regressive Model (DR-SPAAM), follows a forward looking\nparadigm. It keeps the intermediate features from the backbone network as a\ntemplate and recurrently updates the template when a new scan becomes\navailable. The updated feature template is in turn used for detecting persons\ncurrently in the scene. On the DROW dataset, our method outperforms the\nexisting state-of-the-art, while being approximately four times faster, running\nat 87.2 FPS on a laptop with a dedicated GPU and at 22.6 FPS on an NVIDIA\nJetson AGX embedded GPU. We release our code in PyTorch and a ROS node\nincluding pre-trained models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:01:44 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 16:43:53 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Jia", "Dan", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "2004.14107", "submitter": "He Wang", "authors": "Feixiang He, Yuanhang Xiang, Xi Zhao, He Wang", "title": "Informative Scene Decomposition for Crowd Analysis, Comparison and\n  Simulation Guidance", "comments": "accepted in SIGGRAPH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd simulation is a central topic in several fields including graphics. To\nachieve high-fidelity simulations, data has been increasingly relied upon for\nanalysis and simulation guidance. However, the information in real-world data\nis often noisy, mixed and unstructured, making it difficult for effective\nanalysis, therefore has not been fully utilized. With the fast-growing volume\nof crowd data, such a bottleneck needs to be addressed. In this paper, we\npropose a new framework which comprehensively tackles this problem. It centers\nat an unsupervised method for analysis. The method takes as input raw and noisy\ndata with highly mixed multi-dimensional (space, time and dynamics)\ninformation, and automatically structure it by learning the correlations among\nthese dimensions. The dimensions together with their correlations fully\ndescribe the scene semantics which consists of recurring activity patterns in a\nscene, manifested as space flows with temporal and dynamics profiles. The\neffectiveness and robustness of the analysis have been tested on datasets with\ngreat variations in volume, duration, environment and crowd dynamics. Based on\nthe analysis, new methods for data visualization, simulation evaluation and\nsimulation guidance are also proposed. Together, our framework establishes a\nhighly automated pipeline from raw data to crowd analysis, comparison and\nsimulation guidance. Extensive experiments and evaluations have been conducted\nto show the flexibility, versatility and intuitiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:03:32 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["He", "Feixiang", ""], ["Xiang", "Yuanhang", ""], ["Zhao", "Xi", ""], ["Wang", "He", ""]]}, {"id": "2004.14133", "submitter": "Huazhu Fu", "authors": "Deng-Ping Fan, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen, Huazhu Fu,\n  Jianbing Shen, Ling Shao", "title": "Inf-Net: Automatic COVID-19 Lung Infection Segmentation from CT Images", "comments": "To appear in IEEE TMI. The code is released in:\n  https://github.com/DengPingFan/Inf-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) spread globally in early 2020, causing\nthe world to face an existential health crisis. Automated detection of lung\ninfections from computed tomography (CT) images offers a great potential to\naugment the traditional healthcare strategy for tackling COVID-19. However,\nsegmenting infected regions from CT slices faces several challenges, including\nhigh variation in infection characteristics, and low intensity contrast between\ninfections and normal tissues. Further, collecting a large amount of data is\nimpractical within a short time period, inhibiting the training of a deep\nmodel. To address these challenges, a novel COVID-19 Lung Infection\nSegmentation Deep Network (Inf-Net) is proposed to automatically identify\ninfected regions from chest CT slices. In our Inf-Net, a parallel partial\ndecoder is used to aggregate the high-level features and generate a global map.\nThen, the implicit reverse attention and explicit edge-attention are utilized\nto model the boundaries and enhance the representations. Moreover, to alleviate\nthe shortage of labeled data, we present a semi-supervised segmentation\nframework based on a randomly selected propagation strategy, which only\nrequires a few labeled images and leverages primarily unlabeled data. Our\nsemi-supervised framework can improve the learning ability and achieve a higher\nperformance. Extensive experiments on our COVID-SemiSeg and real CT volumes\ndemonstrate that the proposed Inf-Net outperforms most cutting-edge\nsegmentation models and advances the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 07:30:56 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 13:01:30 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 19:40:13 GMT"}, {"version": "v4", "created": "Thu, 21 May 2020 18:23:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Zhou", "Tao", ""], ["Ji", "Ge-Peng", ""], ["Zhou", "Yi", ""], ["Chen", "Geng", ""], ["Fu", "Huazhu", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2004.14143", "submitter": "Mahdi Rezaei", "authors": "Mahdi Rezaei and Mahsa Shahidi", "title": "Zero-Shot Learning and its Applications from Autonomous Vehicles to\n  COVID-19 Diagnosis: A Review", "comments": "Accepted in Journal of Intelligence-Based Medicine (Elsevier)", "journal-ref": "Journal of Intelligence-Based Medicine, Volumes 4, 2020", "doi": "10.1016/j.ibmed.2020.100005", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The challenge of learning a new concept, object, or a new medical disease\nrecognition without receiving any examples beforehand is called Zero-Shot\nLearning (ZSL). One of the major issues in deep learning based methodologies\nsuch as in Medical Imaging and other real-world applications is the requirement\nof large annotated datasets prepared by clinicians or experts to train the\nmodel. ZSL is known for having minimal human intervention by relying only on\npreviously known or trained concepts plus currently existing auxiliary\ninformation. This makes the ZSL applicable in many real-world scenarios, from\nunknown object detection in autonomous vehicles to medical imaging and\nunforeseen diseases such as COVID-19 Chest X-Ray (CXR) based diagnosis. We\nintroduce a novel and broaden solution called Few/one-shot learning, and\npresent the definition of the ZSL problem as an extreme case of the few-shot\nlearning. We review over fundamentals and the challenging steps of Zero-Shot\nLearning, including state-of-the-art categories of solutions, as well as our\nrecommended solution, motivations behind each approach, their advantages over\neach category to guide both clinicians and AI researchers to proceed with the\nbest techniques and practices based on their applications. We then review\nthrough different datasets inducing medical and non-medical images, the variety\nof splits, and the evaluation protocols proposed so far. Finally, we discuss\nthe recent applications and future directions of ZSL. We aim to convey a useful\nintuition through this paper towards the goal of handling complex learning\ntasks more similar to the way humans learn. We mainly focus on two applications\nin the current modern yet challenging era: coping with an early and fast\ndiagnosis of COVID-19 cases, and also encouraging the readers to develop other\nsimilar AI-based automated detection/recognition systems using ZSL.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:45:35 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 12:04:17 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 03:27:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rezaei", "Mahdi", ""], ["Shahidi", "Mahsa", ""]]}, {"id": "2004.14152", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad", "title": "A Fast 3D CNN for Hyperspectral Image Classification", "comments": "5 pages, 8 figures, (IEEE GRSL)", "journal-ref": null, "doi": "10.1109/LGRS.2020.3043710", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging (HSI) has been extensively utilized for a number of\nreal-world applications. HSI classification (HSIC) is a challenging task due to\nhigh inter-class similarity, high intra-class variability, overlapping, and\nnested regions. A 2D Convolutional Neural Network (CNN) is a viable approach\nwhereby HSIC highly depends on both Spectral-Spatial information, therefore, 3D\nCNN can be an alternative but highly computational complex due to the volume\nand spectral dimensions. Furthermore, these models do not extract quality\nfeature maps and may underperform over the regions having similar textures.\nTherefore, this work proposed a 3D CNN model that utilizes both\nspatial-spectral feature maps to attain good performance. In order to achieve\nthe said performance, the HSI cube is first divided into small overlapping 3D\npatches. Later these patches are processed to generate 3D feature maps using a\n3D kernel function over multiple contiguous bands that persevere the spectral\ninformation as well. Benchmark HSI datasets (Pavia University, Salinas and\nIndian Pines) are considered to validate the performance of our proposed\nmethod. The results are further compared with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:57:36 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ahmad", "Muhammad", ""]]}, {"id": "2004.14173", "submitter": "Sarath Pathari", "authors": "Sarath P, Soorya M, Shaik Abdul Rahman A, S Suresh Kumar, K Devaki", "title": "Assessing Car Damage using Mask R-CNN", "comments": null, "journal-ref": null, "doi": "10.35940/ijeat.C5302.029320", "report-no": "C5302029320", "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Picture based vehicle protection handling is a significant region with\nenormous degree for mechanization. In this paper we consider the issue of\nvehicle harm characterization, where a portion of the classifications can be\nfine-granular. We investigate profound learning based procedures for this\nreason. At first, we attempt legitimately preparing a CNN. In any case, because\nof little arrangement of marked information, it doesn't function admirably. At\nthat point, we investigate the impact of space explicit pre-preparing followed\nby tweaking. At last, we explore different avenues regarding move learning and\noutfit learning. Trial results show that move learning works superior to space\nexplicit tweaking. We accomplish precision of 89.5% with blend of move and\ngathering learning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:19:25 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 04:13:34 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 03:29:43 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["P", "Sarath", ""], ["M", "Soorya", ""], ["A", "Shaik Abdul Rahman", ""], ["Kumar", "S Suresh", ""], ["Devaki", "K", ""]]}, {"id": "2004.14178", "submitter": "Rahul U", "authors": "Rahul U, Ragul M, Raja Vignesh K, Tejeswinee K", "title": "Deepfake Video Forensics based on Transfer Learning", "comments": "This submission has been removed by arXiv administrators due to\n  copyright infringement", "journal-ref": null, "doi": "10.35940/ijrte.F9747.038620", "report-no": "F9747038620", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deeplearning has been used to solve complex problems in various domains. As\nit advances, it also creates applications which become a major threat to our\nprivacy, security and even to our Democracy. Such an application which is being\ndeveloped recently is the \"Deepfake\". Deepfake models can create fake images\nand videos that humans cannot differentiate them from the genuine ones.\nTherefore, the counter application to automatically detect and analyze the\ndigital visual media is necessary in today world. This paper details retraining\nthe image classification models to apprehend the features from each deepfake\nvideo frames. After feeding different sets of deepfake clips of video fringes\nthrough a pretrained layer of bottleneck in the neural network is made for\nevery video frame, already stated layer contains condense data for all images\nand exposes artificial manipulations in Deepfake videos. When checking Deepfake\nvideos, this technique received more than 87 per cent accuracy. This technique\nhas been tested on the Face Forensics dataset and obtained good accuracy in\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:21:28 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["U", "Rahul", ""], ["M", "Ragul", ""], ["K", "Raja Vignesh", ""], ["K", "Tejeswinee", ""]]}, {"id": "2004.14231", "submitter": "Sen He", "authors": "Sen He, Wentong Liao, Hamed R. Tavakoli, Michael Yang, Bodo Rosenhahn,\n  Nicolas Pugeault", "title": "Image Captioning through Image Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic captioning of images is a task that combines the challenges of\nimage analysis and text generation. One important aspect in captioning is the\nnotion of attention: How to decide what to describe and in which order.\nInspired by the successes in text analysis and translation, previous work have\nproposed the \\textit{transformer} architecture for image captioning. However,\nthe structure between the \\textit{semantic units} in images (usually the\ndetected regions from object detection model) and sentences (each single word)\nis different. Limited work has been done to adapt the transformer's internal\narchitecture to images. In this work, we introduce the \\textbf{\\textit{image\ntransformer}}, which consists of a modified encoding transformer and an\nimplicit decoding transformer, motivated by the relative spatial relationship\nbetween image regions. Our design widen the original transformer layer's inner\narchitecture to adapt to the structure of images. With only regions feature as\ninputs, our model achieves new state-of-the-art performance on both MSCOCO\noffline and online testing benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:30:57 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:26:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["He", "Sen", ""], ["Liao", "Wentong", ""], ["Tavakoli", "Hamed R.", ""], ["Yang", "Michael", ""], ["Rosenhahn", "Bodo", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "2004.14251", "submitter": "Jan-Nico Zaech", "authors": "Jan-Nico Zaech, Dengxin Dai, Alexander Liniger, Luc Van Gool", "title": "Action Sequence Predictions of Vehicles in Urban Environments using Map\n  and Social Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of predicting the sequence of future actions\nfor surround vehicles in real-world driving scenarios. To this aim, we make\nthree main contributions. The first contribution is an automatic method to\nconvert the trajectories recorded in real-world driving scenarios to action\nsequences with the help of HD maps. The method enables automatic dataset\ncreation for this task from large-scale driving data. Our second contribution\nlies in applying the method to the well-known traffic agent tracking and\nprediction dataset Argoverse, resulting in 228,000 action sequences.\nAdditionally, 2,245 action sequences were manually annotated for testing. The\nthird contribution is to propose a novel action sequence prediction method by\nintegrating past positions and velocities of the traffic agents, map\ninformation and social context into a single end-to-end trainable neural\nnetwork. Our experiments prove the merit of the data creation method and the\nvalue of the created dataset - prediction performance improves consistently\nwith the size of the dataset and shows that our action prediction method\noutperforms comparing models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:59:58 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zaech", "Jan-Nico", ""], ["Dai", "Dengxin", ""], ["Liniger", "Alexander", ""], ["Van Gool", "Luc", ""]]}, {"id": "2004.14273", "submitter": "Xi-Le Zhao", "authors": "Meng Ding, Ting-Zhu Huang, Xi-Le Zhao, Michael K. Ng, Tian-Hui Ma", "title": "Tensor train rank minimization with nonlocal self-similarity for tensor\n  completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tensor train (TT) rank has received increasing attention in tensor\ncompletion due to its ability to capture the global correlation of high-order\ntensors ($\\textrm{order} >3$). For third order visual data, direct TT rank\nminimization has not exploited the potential of TT rank for high-order tensors.\nThe TT rank minimization accompany with \\emph{ket augmentation}, which\ntransforms a lower-order tensor (e.g., visual data) into a higher-order tensor,\nsuffers from serious block-artifacts. To tackle this issue, we suggest the TT\nrank minimization with nonlocal self-similarity for tensor completion by\nsimultaneously exploring the spatial, temporal/spectral, and nonlocal\nredundancy in visual data. More precisely, the TT rank minimization is\nperformed on a formed higher-order tensor called group by stacking similar\ncubes, which naturally and fully takes advantage of the ability of TT rank for\nhigh-order tensors. Moreover, the perturbation analysis for the TT low-rankness\nof each group is established. We develop the alternating direction method of\nmultipliers tailored for the specific structure to solve the proposed model.\nExtensive experiments demonstrate that the proposed method is superior to\nseveral existing state-of-the-art methods in terms of both qualitative and\nquantitative measures.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:39:39 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Ding", "Meng", ""], ["Huang", "Ting-Zhu", ""], ["Zhao", "Xi-Le", ""], ["Ng", "Michael K.", ""], ["Ma", "Tian-Hui", ""]]}, {"id": "2004.14281", "submitter": "Titas De", "authors": "Nick Haber, Catalin Voss, Jena Daniels, Peter Washington, Azar Fazel,\n  Aaron Kline, Titas De, Terry Winograd, Carl Feinstein, Dennis P. Wall", "title": "A Wearable Social Interaction Aid for Children with Autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With most recent estimates giving an incidence rate of 1 in 68 children in\nthe United States, the autism spectrum disorder (ASD) is a growing public\nhealth crisis. Many of these children struggle to make eye contact, recognize\nfacial expressions, and engage in social interactions. Today the standard for\ntreatment of the core autism-related deficits focuses on a form of behavior\ntraining known as Applied Behavioral Analysis. To address perceived deficits in\nexpression recognition, ABA approaches routinely involve the use of prompts\nsuch as flash cards for repetitive emotion recognition training via\nmemorization. These techniques must be administered by trained practitioners\nand often at clinical centers that are far outnumbered by and out of reach from\nthe many children and families in need of attention. Waitlists for access are\nup to 18 months long, and this wait may lead to children regressing down a path\nof isolation that worsens their long-term prognosis. There is an urgent need to\ninnovate new methods of care delivery that can appropriately empower caregivers\nof children at risk or with a diagnosis of autism, and that capitalize on\nmobile tools and wearable devices for use outside of clinical settings.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 13:14:32 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Haber", "Nick", ""], ["Voss", "Catalin", ""], ["Daniels", "Jena", ""], ["Washington", "Peter", ""], ["Fazel", "Azar", ""], ["Kline", "Aaron", ""], ["De", "Titas", ""], ["Winograd", "Terry", ""], ["Feinstein", "Carl", ""], ["Wall", "Dennis P.", ""]]}, {"id": "2004.14289", "submitter": "Shailesh Arya", "authors": "Shailesh Arya, Hrithik Mesariya, Vishal Parekh", "title": "Smart Attendance System Usign CNN", "comments": "4 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on the attendance system has been going for a very long time,\nnumerous arrangements have been proposed in the last decade to make this system\nefficient and less time consuming, but all those systems have several flaws. In\nthis paper, we are introducing a smart and efficient system for attendance\nusing face detection and face recognition. This system can be used to take\nattendance in colleges or offices using real-time face recognition with the\nhelp of the Convolution Neural Network(CNN). The conventional methods like\nEigenfaces and Fisher faces are sensitive to lighting, noise, posture,\nobstruction, illumination etc. Hence, we have used CNN to recognize the face\nand overcome such difficulties. The attendance records will be updated\nautomatically and stored in an excel sheet as well as in a database. We have\nused MongoDB as a backend database for attendance records.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 09:04:33 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Arya", "Shailesh", ""], ["Mesariya", "Hrithik", ""], ["Parekh", "Vishal", ""]]}, {"id": "2004.14326", "submitter": "Joon Son Chung", "authors": "Soo-Whan Chung, Hong Goo Kang, Joon Son Chung", "title": "Seeing voices and hearing voices: learning discriminative embeddings\n  using cross-modal self-supervision", "comments": "Under submission as a conference paper", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1113", "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to train discriminative cross-modal embeddings\nwithout access to manually annotated data. Recent advances in self-supervised\nlearning have shown that effective representations can be learnt from natural\ncross-modal synchrony. We build on earlier work to train embeddings that are\nmore discriminative for uni-modal downstream tasks. To this end, we propose a\nnovel training strategy that not only optimises metrics across modalities, but\nalso enforces intra-class feature separation within each of the modalities. The\neffectiveness of the method is demonstrated on two downstream tasks: lip\nreading using the features trained on audio-visual synchronisation, and speaker\nrecognition using the features trained for cross-modal biometric matching. The\nproposed method outperforms state-of-the-art self-supervised baselines by a\nsignficant margin.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:51:50 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:56:36 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Soo-Whan", ""], ["Kang", "Hong Goo", ""], ["Chung", "Joon Son", ""]]}, {"id": "2004.14338", "submitter": "Jack Hessel", "authors": "Jack Hessel, Zhenhai Zhu, Bo Pang, Radu Soricut", "title": "Beyond Instructional Videos: Probing for More Diverse Visual-Textual\n  Grounding on YouTube", "comments": "11 pages including supplementary materials", "journal-ref": "Published in EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining from unlabelled web videos has quickly become the de-facto means\nof achieving high performance on many video understanding tasks. Features are\nlearned via prediction of grounded relationships between visual content and\nautomatic speech recognition (ASR) tokens. However, prior pretraining work has\nbeen limited to only instructional videos; a priori, we expect this domain to\nbe relatively \"easy:\" speakers in instructional videos will often reference the\nliteral objects/actions being depicted. We ask: can similar models be trained\non more diverse video corpora? And, if so, what types of videos are \"grounded\"\nand what types are not? We fit a representative pretraining model to the\ndiverse YouTube8M dataset, and study its success and failure cases. We find\nthat visual-textual grounding is indeed possible across previously unexplored\nvideo categories, and that pretraining on a more diverse set results in\nrepresentations that generalize to both non-instructional and instructional\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:10:10 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 17:30:51 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hessel", "Jack", ""], ["Zhu", "Zhenhai", ""], ["Pang", "Bo", ""], ["Soricut", "Radu", ""]]}, {"id": "2004.14367", "submitter": "Edo Collins", "authors": "Edo Collins, Raja Bala, Bob Price, Sabine S\\\"usstrunk", "title": "Editing in Style: Uncovering the Local Semantics of GANs", "comments": "IEEE Conference on Computer Vision and Patten Recognition (CVPR),\n  2020. Code: https://github.com/IVRL/GANLocalEditing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the quality of GAN image synthesis has improved tremendously in recent\nyears, our ability to control and condition the output is still limited.\nFocusing on StyleGAN, we introduce a simple and effective method for making\nlocal, semantically-aware edits to a target output image. This is accomplished\nby borrowing elements from a source image, also a GAN output, via a novel\nmanipulation of style vectors. Our method requires neither supervision from an\nexternal model, nor involves complex spatial morphing operations. Instead, it\nrelies on the emergent disentanglement of semantic objects that is learned by\nStyleGAN during its training. Semantic editing is demonstrated on GANs\nproducing human faces, indoor scenes, cats, and cars. We measure the locality\nand photorealism of the edits produced by our method, and find that it\naccomplishes both.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:45:56 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 13:01:07 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Collins", "Edo", ""], ["Bala", "Raja", ""], ["Price", "Bob", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2004.14368", "submitter": "Honglie Chen", "authors": "Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman", "title": "VGGSound: A Large-scale Audio-Visual Dataset", "comments": "ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our goal is to collect a large-scale audio-visual dataset with low label\nnoise from videos in the wild using computer vision techniques. The resulting\ndataset can be used for training and evaluating audio recognition models. We\nmake three contributions. First, we propose a scalable pipeline based on\ncomputer vision techniques to create an audio dataset from open-source media.\nOur pipeline involves obtaining videos from YouTube; using image classification\nalgorithms to localize audio-visual correspondence; and filtering out ambient\nnoise using audio verification. Second, we use this pipeline to curate the\nVGGSound dataset consisting of more than 210k videos for 310 audio classes.\nThird, we investigate various Convolutional Neural Network~(CNN) architectures\nand aggregation approaches to establish audio recognition baselines for our new\ndataset. Compared to existing audio datasets, VGGSound ensures audio-visual\ncorrespondence and is collected under unconstrained conditions. Code and the\ndataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:46:54 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 00:26:52 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chen", "Honglie", ""], ["Xie", "Weidi", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2004.14421", "submitter": "Vittorio Mazzia", "authors": "Vittorio Mazzia, Lorenzo Comba, Aleem Khaliq, Marcello Chiaberge,\n  Paolo Gay", "title": "UAV and Machine Learning Based Refinement of a Satellite-Driven\n  Vegetation Index for Precision Agriculture", "comments": null, "journal-ref": "Sensors 2020, 20(9), 2530", "doi": "10.3390/s20092530", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precision agriculture is considered to be a fundamental approach in pursuing\na low-input, high-efficiency, and sustainable kind of agriculture when\nperforming site-specific management practices. To achieve this objective, a\nreliable and updated description of the local status of crops is required.\nRemote sensing, and in particular satellite-based imagery, proved to be a\nvaluable tool in crop mapping, monitoring, and diseases assessment. However,\nfreely available satellite imagery with low or moderate resolutions showed some\nlimits in specific agricultural applications, e.g., where crops are grown by\nrows. Indeed, in this framework, the satellite's output could be biased by\nintra-row covering, giving inaccurate information about crop status. This paper\npresents a novel satellite imagery refinement framework, based on a deep\nlearning technique which exploits information properly derived from high\nresolution images acquired by unmanned aerial vehicle (UAV) airborne\nmultispectral sensors. To train the convolutional neural network, only a single\nUAV-driven dataset is required, making the proposed approach simple and\ncost-effective. A vineyard in Serralunga d'Alba (Northern Italy) was chosen as\na case study for validation purposes. Refined satellite-driven normalized\ndifference vegetation index (NDVI) maps, acquired in four different periods\nduring the vine growing season, were shown to better describe crop status with\nrespect to raw datasets by correlation analysis and ANOVA. In addition, using a\nK-means based classifier, 3-class vineyard vigor maps were profitably derived\nfrom the NDVI maps, which are a valuable tool for growers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 18:34:48 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mazzia", "Vittorio", ""], ["Comba", "Lorenzo", ""], ["Khaliq", "Aleem", ""], ["Chiaberge", "Marcello", ""], ["Gay", "Paolo", ""]]}, {"id": "2004.14451", "submitter": "Allen Nie", "authors": "Allen Nie, Reuben Cohn-Gordon, and Christopher Potts", "title": "Pragmatic Issue-Sensitive Image Captioning", "comments": "15 pages, 7 figures. EMNLP 2020 Findings Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image captioning systems have recently improved dramatically, but they still\ntend to produce captions that are insensitive to the communicative goals that\ncaptions should meet. To address this, we propose Issue-Sensitive Image\nCaptioning (ISIC). In ISIC, a captioning system is given a target image and an\nissue, which is a set of images partitioned in a way that specifies what\ninformation is relevant. The goal of the captioner is to produce a caption that\nresolves this issue. To model this task, we use an extension of the Rational\nSpeech Acts model of pragmatic language use. Our extension is built on top of\nstate-of-the-art pretrained neural image captioners and explicitly reasons\nabout issues in our sense. We establish experimentally that these models\ngenerate captions that are both highly descriptive and issue-sensitive, and we\nshow how ISIC can complement and enrich the related task of Visual Question\nAnswering.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:00:53 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 23:24:41 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nie", "Allen", ""], ["Cohn-Gordon", "Reuben", ""], ["Potts", "Christopher", ""]]}, {"id": "2004.14487", "submitter": "Matthew Purri", "authors": "Matthew Purri and Kristin Dana", "title": "Teaching Cameras to Feel: Estimating Tactile Physical Properties of\n  Surfaces From Images", "comments": "19 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection between visual input and tactile sensing is critical for\nobject manipulation tasks such as grasping and pushing. In this work, we\nintroduce the challenging task of estimating a set of tactile physical\nproperties from visual information. We aim to build a model that learns the\ncomplex mapping between visual information and tactile physical properties. We\nconstruct a first of its kind image-tactile dataset with over 400 multiview\nimage sequences and the corresponding tactile properties. A total of fifteen\ntactile physical properties across categories including friction, compliance,\nadhesion, texture, and thermal conductance are measured and then estimated by\nour models. We develop a cross-modal framework comprised of an adversarial\nobjective and a novel visuo-tactile joint classification loss. Additionally, we\ndevelop a neural architecture search framework capable of selecting optimal\ncombinations of viewing angles for estimating a given physical property.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:27:26 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 16:43:34 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Purri", "Matthew", ""], ["Dana", "Kristin", ""]]}, {"id": "2004.14489", "submitter": "Daniel S\\'ykora", "authors": "Ond\\v{r}ej Texler, David Futschik, Michal Ku\\v{c}era, Ond\\v{r}ej\n  Jamri\\v{s}ka, \\v{S}\\'arka Sochorov\\'a, Menglei Chai, Sergey Tulyakov, and\n  Daniel S\\'ykora", "title": "Interactive Video Stylization Using Few-Shot Patch-Based Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a learning-based method to the keyframe-based video\nstylization that allows an artist to propagate the style from a few selected\nkeyframes to the rest of the sequence. Its key advantage is that the resulting\nstylization is semantically meaningful, i.e., specific parts of moving objects\nare stylized according to the artist's intention. In contrast to previous style\ntransfer techniques, our approach does not require any lengthy pre-training\nprocess nor a large training dataset. We demonstrate how to train an appearance\ntranslation network from scratch using only a few stylized exemplars while\nimplicitly preserving temporal consistency. This leads to a video stylization\nframework that supports real-time inference, parallel processing, and random\naccess to an arbitrary output frame. It can also merge the content from\nmultiple keyframes without the need to perform an explicit blending operation.\nWe demonstrate its practical utility in various interactive scenarios, where\nthe user paints over a selected keyframe and sees her style transferred to an\nexisting recorded sequence or a live video stream.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:33:28 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Texler", "Ond\u0159ej", ""], ["Futschik", "David", ""], ["Ku\u010dera", "Michal", ""], ["Jamri\u0161ka", "Ond\u0159ej", ""], ["Sochorov\u00e1", "\u0160\u00e1rka", ""], ["Chai", "Menglei", ""], ["Tulyakov", "Sergey", ""], ["S\u00fdkora", "Daniel", ""]]}, {"id": "2004.14491", "submitter": "Shruti Agarwal", "authors": "Shruti Agarwal (1), Tarek El-Gaaly (2), Hany Farid (1), Ser-Nam Lim\n  (2) ((1) Univeristy of California, Berkeley, Berkeley, CA, USA, (2) Facebook\n  Research, New York, NY, USA)", "title": "Detecting Deep-Fake Videos from Appearance and Behavior", "comments": null, "journal-ref": "IEEE Workshop on Image Forensics and Security, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetically-generated audios and videos -- so-called deep fakes -- continue\nto capture the imagination of the computer-graphics and computer-vision\ncommunities. At the same time, the democratization of access to technology that\ncan create sophisticated manipulated video of anybody saying anything continues\nto be of concern because of its power to disrupt democratic elections, commit\nsmall to large-scale fraud, fuel dis-information campaigns, and create\nnon-consensual pornography. We describe a biometric-based forensic technique\nfor detecting face-swap deep fakes. This technique combines a static biometric\nbased on facial recognition with a temporal, behavioral biometric based on\nfacial expressions and head movements, where the behavioral embedding is\nlearned using a CNN with a metric-learning objective function. We show the\nefficacy of this approach across several large-scale video datasets, as well as\nin-the-wild deep fakes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:38:22 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Agarwal", "Shruti", ""], ["El-Gaaly", "Tarek", ""], ["Farid", "Hany", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2004.14492", "submitter": "Yuchen Liu", "authors": "Yuchen Liu, David Wentzlaff, and S.Y. Kung", "title": "Rethinking Class-Discrimination Based CNN Channel Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning has received ever-increasing focus on network compression. In\nparticular, class-discrimination based channel pruning has made major headway,\nas it fits seamlessly with the classification objective of CNNs and provides\ngood explainability. Prior works singly propose and evaluate their discriminant\nfunctions, while further study on the effectiveness of the adopted metrics is\nabsent. To this end, we initiate the first study on the effectiveness of a\nbroad range of discriminant functions on channel pruning. Conventional\nsingle-variate binary-class statistics like Student's T-Test are also included\nin our study via an intuitive generalization. The winning metric of our study\nhas a greater ability to select informative channels over other\nstate-of-the-art methods, which is substantiated by our qualitative and\nquantitative analysis. Moreover, we develop a FLOP-normalized sensitivity\nanalysis scheme to automate the structural pruning procedure. On CIFAR-10,\nCIFAR-100, and ILSVRC-2012 datasets, our pruned models achieve higher accuracy\nwith less inference cost compared to state-of-the-art results. For example, on\nILSVRC-2012, our 44.3% FLOPs-pruned ResNet-50 has only a 0.3% top-1 accuracy\ndrop, which significantly outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:40:23 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Liu", "Yuchen", ""], ["Wentzlaff", "David", ""], ["Kung", "S. Y.", ""]]}, {"id": "2004.14525", "submitter": "Yunyang Xiong", "authors": "Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender,\n  Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan, Vikas Singh, Bo Chen", "title": "MobileDets: Searching for Object Detection Architectures for Mobile\n  Accelerators", "comments": "Accepted at CVPR 2021; Code and models are available in the\n  TensorFlow Object Detection API:\n  https://github.com/tensorflow/models/tree/master/research/object_detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverted bottleneck layers, which are built upon depthwise convolutions, have\nbeen the predominant building blocks in state-of-the-art object detection\nmodels on mobile devices. In this work, we investigate the optimality of this\ndesign pattern over a broad range of mobile accelerators by revisiting the\nusefulness of regular convolutions. We discover that regular convolutions are a\npotent component to boost the latency-accuracy trade-off for object detection\non accelerators, provided that they are placed strategically in the network via\nneural architecture search. By incorporating regular convolutions in the search\nspace and directly optimizing the network architectures for object detection,\nwe obtain a family of object detection models, MobileDets, that achieve\nstate-of-the-art results across mobile accelerators. On the COCO object\ndetection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at\ncomparable mobile CPU inference latencies. MobileDets also outperform\nMobileNetV2+SSDLite by 1.9 mAP on mobile CPUs, 3.7 mAP on Google EdgeTPU, 3.4\nmAP on Qualcomm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing\nlatency. Moreover, MobileDets are comparable with the state-of-the-art MnasFPN\non mobile CPUs even without using the feature pyramid, and achieve better mAP\nscores on both EdgeTPUs and DSPs with up to 2x speedup. Code and models are\navailable in the TensorFlow Object Detection API:\nhttps://github.com/tensorflow/models/tree/master/research/object_detection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:21:30 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 02:15:49 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 01:21:42 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xiong", "Yunyang", ""], ["Liu", "Hanxiao", ""], ["Gupta", "Suyog", ""], ["Akin", "Berkin", ""], ["Bender", "Gabriel", ""], ["Wang", "Yongzhe", ""], ["Kindermans", "Pieter-Jan", ""], ["Tan", "Mingxing", ""], ["Singh", "Vikas", ""], ["Chen", "Bo", ""]]}, {"id": "2004.14528", "submitter": "Jugurta Montalv\\~ao", "authors": "Jugurta Montalv\\~ao, J\\^anio Canuto, Luiz Miranda", "title": "Bias-corrected estimator for intrinsic dimension and differential\n  entropy--a visual multiscale approach", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic dimension and differential entropy estimators are studied in this\npaper, including their systematic bias. A pragmatic approach for joint\nestimation and bias correction of these two fundamental measures is proposed.\nShared steps on both estimators are highlighted, along with their useful\nconsequences to data analysis. It is shown that both estimators can be\ncomplementary parts of a single approach, and that the simultaneous estimation\nof differential entropy and intrinsic dimension give meaning to each other,\nwhere estimates at different observation scales convey different perspectives\nof underlying manifolds. Experiments with synthetic and real datasets are\npresented to illustrate how to extract meaning from visual inspections, and how\nto compensate for biases.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:29:28 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Montalv\u00e3o", "Jugurta", ""], ["Canuto", "J\u00e2nio", ""], ["Miranda", "Luiz", ""]]}, {"id": "2004.14539", "submitter": "Zihang Meng", "authors": "Zihang Meng, Sathya N. Ravi, Vikas Singh", "title": "Physarum Powered Differentiable Linear Programming Layers and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a learning algorithm, which involves an internal call to an\noptimization routine such as a generalized eigenvalue problem, a cone\nprogramming problem or even sorting. Integrating such a method as a layer(s)\nwithin a trainable deep neural network (DNN) in an efficient and numerically\nstable way is not straightforward -- for instance, only recently, strategies\nhave emerged for eigendecomposition and differentiable sorting. We propose an\nefficient and differentiable solver for general linear programming problems\nwhich can be used in a plug and play manner within DNNs as a layer. Our\ndevelopment is inspired by a fascinating but not widely used link between\ndynamics of slime mold (physarum) and optimization schemes such as steepest\ndescent. We describe our development and show the use of our solver in a video\nsegmentation task and meta-learning for few-shot learning. We review the\nexisting results and provide a technical analysis describing its applicability\nfor our use cases. Our solver performs comparably with a customized projected\ngradient descent method on the first task and outperforms the differentiable\nCVXPY-SCS solver on the second task. Experiments show that our solver converges\nquickly without the need for a feasible initial point. Our proposal is easy to\nimplement and can easily serve as layers whenever a learning procedure needs a\nfast approximate solution to a LP, within a larger network.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 01:50:37 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 17:21:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Meng", "Zihang", ""], ["Ravi", "Sathya N.", ""], ["Singh", "Vikas", ""]]}, {"id": "2004.14552", "submitter": "Guangyu Ren", "authors": "Guangyu Ren, Tianhong Dai, Panagiotis Barmpoutis, Tania Stathaki", "title": "Salient Object Detection Combining a Self-attention Module and a Feature\n  Pyramid Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection has achieved great improvement by using the Fully\nConvolution Network (FCN). However, the FCN-based U-shape architecture may\ncause the dilution problem in the high-level semantic information during the\nup-sample operations in the top-down pathway. Thus, it can weaken the ability\nof salient object localization and produce degraded boundaries. To this end, in\norder to overcome this limitation, we propose a novel pyramid self-attention\nmodule (PSAM) and the adoption of an independent feature-complementing\nstrategy. In PSAM, self-attention layers are equipped after multi-scale pyramid\nfeatures to capture richer high-level features and bring larger receptive\nfields to the model. In addition, a channel-wise attention module is also\nemployed to reduce the redundant features of the FPN and provide refined\nresults. Experimental analysis shows that the proposed PSAM effectively\ncontributes to the whole model so that it outperforms state-of-the-art results\nover five challenging datasets. Finally, quantitative results show that PSAM\ngenerates clear and integral salient maps which can provide further help to\nother computer vision tasks, such as object detection and semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:08:34 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ren", "Guangyu", ""], ["Dai", "Tianhong", ""], ["Barmpoutis", "Panagiotis", ""], ["Stathaki", "Tania", ""]]}, {"id": "2004.14557", "submitter": "Zi Li", "authors": "Risheng Liu, Zi Li, Xin Fan, Chenying Zhao, Hao Huang and Zhongxuan\n  Luo", "title": "Learning Deformable Image Registration from Optimization: Perspective,\n  Modules, Bilevel Training and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional deformable registration methods aim at solving an optimization\nmodel carefully designed on image pairs and their computational costs are\nexceptionally high. In contrast, recent deep learning based approaches can\nprovide fast deformation estimation. These heuristic network architectures are\nfully data-driven and thus lack explicit geometric constraints, e.g.,\ntopology-preserving, which are indispensable to generate plausible\ndeformations. We design a new deep learning based framework to optimize a\ndiffeomorphic model via multi-scale propagation in order to integrate\nadvantages and avoid limitations of these two categories of approaches.\nSpecifically, we introduce a generic optimization model to formulate\ndiffeomorphic registration and develop a series of learnable architectures to\nobtain propagative updating in the coarse-to-fine feature space. Moreover, we\npropose a novel bilevel self-tuned training strategy, allowing efficient search\nof task-specific hyper-parameters. This training strategy increases the\nflexibility to various types of data while reduces computational and human\nburdens. We conduct two groups of image registration experiments on 3D volume\ndatasets including image-to-atlas registration on brain MRI data and\nimage-to-image registration on liver CT data. Extensive results demonstrate the\nstate-of-the-art performance of the proposed method with diffeomorphic\nguarantee and extreme efficiency. We also apply our framework to challenging\nmulti-modal image registration, and investigate how our registration to support\nthe down-streaming tasks for medical image analysis including multi-modal\nfusion and image segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:23:45 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 05:25:18 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 12:41:40 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Liu", "Risheng", ""], ["Li", "Zi", ""], ["Fan", "Xin", ""], ["Zhao", "Chenying", ""], ["Huang", "Hao", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "2004.14566", "submitter": "Yuhui Xu", "authors": "Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi,\n  Yiran Chen, Weiyao Lin, Hongkai Xiong", "title": "TRP: Trained Rank Pruning for Efficient Deep Neural Networks", "comments": "Accepted by IJCAI2020, An extension version of arXiv:1812.02402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable DNNs on edge devices like mobile phones, low-rank approximation has\nbeen widely adopted because of its solid theoretical rationale and efficient\nimplementations. Several previous works attempted to directly approximate a\npretrained model by low-rank decomposition; however, small approximation errors\nin parameters can ripple over a large prediction loss. As a result, performance\nusually drops significantly and a sophisticated effort on fine-tuning is\nrequired to recover accuracy. Apparently, it is not optimal to separate\nlow-rank approximation from training. Unlike previous works, this paper\nintegrates low rank approximation and regularization into the training process.\nWe propose Trained Rank Pruning (TRP), which alternates between low rank\napproximation and training. TRP maintains the capacity of the original network\nwhile imposing low-rank constraints during training. A nuclear regularization\noptimized by stochastic sub-gradient descent is utilized to further promote low\nrank in TRP. The TRP trained network inherently has a low-rank structure, and\nis approximated with negligible performance loss, thus eliminating the\nfine-tuning process after low rank decomposition. The proposed method is\ncomprehensively evaluated on CIFAR-10 and ImageNet, outperforming previous\ncompression methods using low rank approximation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:37:36 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Xu", "Yuhui", ""], ["Li", "Yuxi", ""], ["Zhang", "Shuai", ""], ["Wen", "Wei", ""], ["Wang", "Botao", ""], ["Qi", "Yingyong", ""], ["Chen", "Yiran", ""], ["Lin", "Weiyao", ""], ["Xiong", "Hongkai", ""]]}, {"id": "2004.14569", "submitter": "Jiangning Zhang", "authors": "Jiangning Zhang, Liang Liu, Zhucun Xue, Yong Liu", "title": "APB2Face: Audio-guided face reenactment with auxiliary pose and blink\n  signals", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-guided face reenactment aims at generating photorealistic faces using\naudio information while maintaining the same facial movement as when speaking\nto a real person. However, existing methods can not generate vivid face images\nor only reenact low-resolution faces, which limits the application value. To\nsolve those problems, we propose a novel deep neural network named APB2Face,\nwhich consists of GeometryPredictor and FaceReenactor modules.\nGeometryPredictor uses extra head pose and blink state signals as well as audio\nto predict the latent landmark geometry information, while FaceReenactor inputs\nthe face landmark image to reenact the photorealistic face. A new dataset AnnVI\ncollected from YouTube is presented to support the approach, and experimental\nresults indicate the superiority of our method than state-of-the-arts, whether\nin authenticity or controllability.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:44:35 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zhang", "Jiangning", ""], ["Liu", "Liang", ""], ["Xue", "Zhucun", ""], ["Liu", "Yong", ""]]}, {"id": "2004.14581", "submitter": "Eisuke Shibuya", "authors": "Eisuke Shibuya, Kazuhiro Hotta", "title": "Feedback U-net for Cell Image Segmentation", "comments": "Accepted by CVPR2020 Workshop \"Computer Vision for Microscopy Image\n  Analysis (CVMI)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human brain is a layered structure, and performs not only a feedforward\nprocess from a lower layer to an upper layer but also a feedback process from\nan upper layer to a lower layer. The layer is a collection of neurons, and\nneural network is a mathematical model of the function of neurons. Although\nneural network imitates the human brain, everyone uses only feedforward process\nfrom the lower layer to the upper layer, and feedback process from the upper\nlayer to the lower layer is not used. Therefore, in this paper, we propose\nFeedback U-Net using Convolutional LSTM which is the segmentation method using\nConvolutional LSTM and feedback process. The output of U-net gave feedback to\nthe input, and the second round is performed. By using Convolutional LSTM, the\nfeatures in the second round are extracted based on the features acquired in\nthe first round. On both of the Drosophila cell image and Mouse cell image\ndatasets, our method outperformed conventional U-Net which uses only\nfeedforward process.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 04:16:26 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Shibuya", "Eisuke", ""], ["Hotta", "Kazuhiro", ""]]}, {"id": "2004.14582", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Zheng Lin, Jun Xu, Wenda Jin, Shao-Ping Lu, Deng-Ping Fan", "title": "Bilateral Attention Network for RGB-D Salient Object Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3049959", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing RGB-D salient object detection (SOD) methods focus on the\nforeground region when utilizing the depth images. However, the background also\nprovides important information in traditional SOD methods for promising\nperformance. To better explore salient information in both foreground and\nbackground regions, this paper proposes a Bilateral Attention Network (BiANet)\nfor the RGB-D SOD task. Specifically, we introduce a Bilateral Attention Module\n(BAM) with a complementary attention mechanism: foreground-first (FF) attention\nand background-first (BF) attention. The FF attention focuses on the foreground\nregion with a gradual refinement style, while the BF one recovers potentially\nuseful salient information in the background region. Benefitted from the\nproposed BAM module, our BiANet can capture more meaningful foreground and\nbackground cues, and shift more attention to refining the uncertain details\nbetween foreground and background regions. Additionally, we extend our BAM by\nleveraging the multi-scale techniques for better SOD performance. Extensive\nexperiments on six benchmark datasets demonstrate that our BiANet outperforms\nother state-of-the-art RGB-D SOD methods in terms of objective metrics and\nsubjective visual comparison. Our BiANet can run up to 80fps on $224\\times224$\nRGB-D images, with an NVIDIA GeForce RTX 2080Ti GPU. Comprehensive ablation\nstudies also validate our contributions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 04:23:32 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhang", "Zhao", ""], ["Lin", "Zheng", ""], ["Xu", "Jun", ""], ["Jin", "Wenda", ""], ["Lu", "Shao-Ping", ""], ["Fan", "Deng-Ping", ""]]}, {"id": "2004.14584", "submitter": "Ragav Venkatesan", "authors": "Ragav Venkatesan, Gurumurthy Swaminathan, Xiong Zhou, Anna Luo", "title": "Out-of-the-box channel pruned networks", "comments": "Under review at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade convolutional neural networks have become gargantuan.\nPre-trained models, when used as initializers are able to fine-tune ever larger\nnetworks on small datasets. Consequently, not all the convolutional features\nthat these fine-tuned models detect are requisite for the end-task. Several\nworks of channel pruning have been proposed to prune away compute and memory\nfrom models that were trained already. Typically, these involve policies that\ndecide which and how many channels to remove from each layer leading to\nchannel-wise and/or layer-wise pruning profiles, respectively. In this paper,\nwe conduct several baseline experiments and establish that profiles from random\nchannel-wise pruning policies are as good as metric-based ones. We also\nestablish that there may exist profiles from some layer-wise pruning policies\nthat are measurably better than common baselines. We then demonstrate that the\ntop layer-wise pruning profiles found using an exhaustive random search from\none datatset are also among the top profiles for other datasets. This implies\nthat we could identify out-of-the-box layer-wise pruning profiles using\nbenchmark datasets and use these directly for new datasets. Furthermore, we\ndevelop a Reinforcement Learning (RL) policy-based search algorithm with a\ndirect objective of finding transferable layer-wise pruning profiles using many\nmodels for the same architecture. We use a novel reward formulation that drives\nthis RL search towards an expected compression while maximizing accuracy. Our\nresults show that our transferred RL-based profiles are as good or better than\nbest profiles found on the original dataset via exhaustive search. We then\ndemonstrate that if we found the profiles using a mid-sized dataset such as\nCifar10/100, we are able to transfer them to even a large dataset such as\nImagenet.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 04:40:47 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Swaminathan", "Gurumurthy", ""], ["Zhou", "Xiong", ""], ["Luo", "Anna", ""]]}, {"id": "2004.14595", "submitter": "Christian Marzahl", "authors": "Christian Marzahl, Marc Aubreville, Christof A. Bertram, Jennifer\n  Maier, Christian Bergler, Christine Kr\\\"oger, J\\\"orn Voigt, Katharina\n  Breininger, Robert Klopfleisch, and Andreas Maier", "title": "EXACT: A collaboration toolset for algorithm-aided annotation of images\n  with annotation version control", "comments": null, "journal-ref": "Scientific Reports 2021", "doi": "10.1038/s41598-021-83827-4", "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many research areas, scientific progress is accelerated by\nmultidisciplinary access to image data and their interdisciplinary annotation.\nHowever, keeping track of these annotations to ensure a high-quality\nmulti-purpose data set is a challenging and labour intensive task. We developed\nthe open-source online platform EXACT (EXpert Algorithm Collaboration Tool)\nthat enables the collaborative interdisciplinary analysis of images from\ndifferent domains online and offline. EXACT supports multi-gigapixel medical\nwhole slide images as well as image series with thousands of images. The\nsoftware utilises a flexible plugin system that can be adapted to diverse\napplications such as counting mitotic figures with a screening mode, finding\nfalse annotations on a novel validation view, or using the latest deep learning\nimage analysis technologies. This is combined with a version control system\nwhich makes it possible to keep track of changes in the data sets and, for\nexample, to link the results of deep learning experiments to specific data set\nversions. EXACT is freely available and has already been successfully applied\nto a broad range of annotation tasks, including highly diverse applications\nlike deep learning supported cytology scoring, interdisciplinary multi-centre\nwhole slide image tumour annotation, and highly specialised whale sound\nspectroscopy clustering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 06:07:21 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 09:00:42 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 12:29:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Marzahl", "Christian", ""], ["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Maier", "Jennifer", ""], ["Bergler", "Christian", ""], ["Kr\u00f6ger", "Christine", ""], ["Voigt", "J\u00f6rn", ""], ["Breininger", "Katharina", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "2004.14603", "submitter": "Thao Minh Le", "authors": "Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran", "title": "Dynamic Language Binding in Relational Visual Reasoning", "comments": "Early version accepted by IJCAI20, Code available at\n  https://github.com/thaolmk54/LOGNet-VQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Language-binding Object Graph Network, the first neural reasoning\nmethod with dynamic relational structures across both visual and textual\ndomains with applications in visual question answering. Relaxing the common\nassumption made by current models that the object predicates pre-exist and stay\nstatic, passive to the reasoning process, we propose that these dynamic\npredicates expand across the domain borders to include pair-wise\nvisual-linguistic object binding. In our method, these contextualized object\nlinks are actively found within each recurrent reasoning step without relying\non external predicative priors. These dynamic structures reflect the\nconditional dual-domain object dependency given the evolving context of the\nreasoning through co-attention. Such discovered dynamic graphs facilitate\nmulti-step knowledge combination and refinements that iteratively deduce the\ncompact representation of the final answer. The effectiveness of this model is\ndemonstrated on image question answering demonstrating favorable performance on\nmajor VQA datasets. Our method outperforms other methods in sophisticated\nquestion-answering tasks wherein multiple object relations are involved. The\ngraph structure effectively assists the progress of training, and therefore the\nnetwork learns efficiently compared to other reasoning models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 06:26:20 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 01:24:55 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 03:35:24 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Le", "Thao Minh", ""], ["Le", "Vuong", ""], ["Venkatesh", "Svetha", ""], ["Tran", "Truyen", ""]]}, {"id": "2004.14619", "submitter": "Zheng Tang", "authors": "Milind Naphade, Shuo Wang, David Anastasiu, Zheng Tang, Ming-Ching\n  Chang, Xiaodong Yang, Liang Zheng, Anuj Sharma, Rama Chellappa, Pranamesh\n  Chakraborty", "title": "The 4th AI City Challenge", "comments": "Organization summary of the 4th AI City Challenge Workshop @ CVPR\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AI City Challenge was created to accelerate intelligent video analysis\nthat helps make cities smarter and safer. Transportation is one of the largest\nsegments that can benefit from actionable insights derived from data captured\nby sensors, where computer vision and deep learning have shown promise in\nachieving large-scale practical deployment. The 4th annual edition of the AI\nCity Challenge has attracted 315 participating teams across 37 countries, who\nleveraged city-scale real traffic data and high-quality synthetic data to\ncompete in four challenge tracks. Track 1 addressed video-based automatic\nvehicle counting, where the evaluation is conducted on both algorithmic\neffectiveness and computational efficiency. Track 2 addressed city-scale\nvehicle re-identification with augmented synthetic data to substantially\nincrease the training set for the task. Track 3 addressed city-scale\nmulti-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly\ndetection. The evaluation system shows two leader boards, in which a general\nleader board shows all submitted results, and a public leader board shows\nresults limited to our contest participation rules, that teams are not allowed\nto use external data in their work. The public leader board shows results more\nclose to real-world situations where annotated data are limited. Our results\nshow promise that AI technology can enable smarter and safer transportation\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:47:14 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Naphade", "Milind", ""], ["Wang", "Shuo", ""], ["Anastasiu", "David", ""], ["Tang", "Zheng", ""], ["Chang", "Ming-Ching", ""], ["Yang", "Xiaodong", ""], ["Zheng", "Liang", ""], ["Sharma", "Anuj", ""], ["Chellappa", "Rama", ""], ["Chakraborty", "Pranamesh", ""]]}, {"id": "2004.14638", "submitter": "Sinan Tan", "authors": "Sinan Tan, Huaping Liu, Di Guo, Xinyu Zhang, Fuchun Sun", "title": "Towards Embodied Scene Description", "comments": "Accepted in Robotics: Science and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodiment is an important characteristic for all intelligent agents\n(creatures and robots), while existing scene description tasks mainly focus on\nanalyzing images passively and the semantic understanding of the scenario is\nseparated from the interaction between the agent and the environment. In this\nwork, we propose the Embodied Scene Description, which exploits the embodiment\nability of the agent to find an optimal viewpoint in its environment for scene\ndescription tasks. A learning framework with the paradigms of imitation\nlearning and reinforcement learning is established to teach the intelligent\nagent to generate corresponding sensorimotor activities. The proposed framework\nis tested on both the AI2Thor dataset and a real world robotic platform\ndemonstrating the effectiveness and extendability of the developed method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 08:50:25 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 09:14:37 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Tan", "Sinan", ""], ["Liu", "Huaping", ""], ["Guo", "Di", ""], ["Zhang", "Xinyu", ""], ["Sun", "Fuchun", ""]]}, {"id": "2004.14644", "submitter": "Pierre Jacob", "authors": "Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein", "title": "DIABLO: Dictionary-based Attention Block for Deep Metric Learning", "comments": "Pre-print. Accepted for publication at Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2020.03.020", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent breakthroughs in representation learning of unseen classes and\nexamples have been made in deep metric learning by training at the same time\nthe image representations and a corresponding metric with deep networks. Recent\ncontributions mostly address the training part (loss functions, sampling\nstrategies, etc.), while a few works focus on improving the discriminative\npower of the image representation. In this paper, we propose DIABLO, a\ndictionary-based attention method for image embedding. DIABLO produces richer\nrepresentations by aggregating only visually-related features together while\nbeing easier to train than other attention-based methods in deep metric\nlearning. This is experimentally confirmed on four deep metric learning\ndatasets (Cub-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes\nRetrieval) for which DIABLO shows state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:05:56 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Jacob", "Pierre", ""], ["Picard", "David", ""], ["Histace", "Aymeric", ""], ["Klein", "Edouard", ""]]}, {"id": "2004.14672", "submitter": "Dino Ienco", "authors": "Dino Ienco, Yawogan Jean Eudes Gbodjo, Roberto Interdonato, and\n  Raffaele Gaetano", "title": "Attentive Weakly Supervised land cover mapping for object-based\n  satellite image time series data with spatial interpretation", "comments": "Under submission to Elsevier journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, modern Earth Observation systems continuously collect massive\namounts of satellite information. The unprecedented possibility to acquire high\nresolution Satellite Image Time Series (SITS) data (series of images with high\nrevisit time period on the same geographical area) is opening new opportunities\nto monitor the different aspects of the Earth Surface but, at the same time, it\nis raising up new challenges in term of suitable methods to analyze and exploit\nsuch huge amount of rich and complex image data. One of the main task\nassociated to SITS data analysis is related to land cover mapping where\nsatellite data are exploited via learning methods to recover the Earth Surface\nstatus aka the corresponding land cover classes. Due to operational\nconstraints, the collected label information, on which machine learning\nstrategies are trained, is often limited in volume and obtained at coarse\ngranularity carrying out inexact and weak knowledge that can affect the whole\nprocess. To cope with such issues, in the context of object-based SITS land\ncover mapping, we propose a new deep learning framework, named TASSEL\n(aTtentive weAkly Supervised Satellite image time sEries cLassifier), that is\nable to intelligently exploit the weak supervision provided by the coarse\ngranularity labels. Furthermore, our framework also produces an additional\nside-information that supports the model interpretability with the aim to make\nthe black box gray. Such side-information allows to associate spatial\ninterpretation to the model decision via visual inspection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:23:12 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Ienco", "Dino", ""], ["Gbodjo", "Yawogan Jean Eudes", ""], ["Interdonato", "Roberto", ""], ["Gaetano", "Raffaele", ""]]}, {"id": "2004.14674", "submitter": "Manu Mathew", "authors": "Aniket Limaye, Manu Mathew, Soyeb Nagori, Pramod Kumar Swami,\n  Debapriya Maji, Kumar Desappan", "title": "SS3D: Single Shot 3D Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single stage deep learning algorithm for 2D object detection was made popular\nby Single Shot MultiBox Detector (SSD) and it was heavily adopted in several\nembedded applications. PointPillars is a state of the art 3D object detection\nalgorithm that uses a Single Shot Detector adapted for 3D object detection. The\nmain downside of PointPillars is that it has a two stage approach with learned\ninput representation based on fully connected layers followed by the Single\nShot Detector for 3D detection. In this paper we present Single Shot 3D Object\nDetection (SS3D) - a single stage 3D object detection algorithm which combines\nstraight forward, statistically computed input representation and a Single Shot\nDetector (based on PointPillars). Computing the input representation is\nstraight forward, does not involve learning and does not have much\ncomputational cost. We also extend our method to stereo input and show that,\naided by additional semantic segmentation input; our method produces similar\naccuracy as state of the art stereo based detectors. Achieving the accuracy of\ntwo stage detectors using a single stage approach is important as single stage\napproaches are simpler to implement in embedded, real-time applications. With\nLiDAR as well as stereo input, our method outperforms PointPillars. When using\nLiDAR input, our input representation is able to improve the AP3D of Cars\nobjects in the moderate category from 74.99 to 76.84. When using stereo input,\nour input representation is able to improve the AP3D of Cars objects in the\nmoderate category from 38.13 to 45.13. Our results are also better than other\npopular 3D object detectors such as AVOD and F-PointNet.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:28:08 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 12:33:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Limaye", "Aniket", ""], ["Mathew", "Manu", ""], ["Nagori", "Soyeb", ""], ["Swami", "Pramod Kumar", ""], ["Maji", "Debapriya", ""], ["Desappan", "Kumar", ""]]}, {"id": "2004.14705", "submitter": "Yuheng Jia", "authors": "Yuheng Jia, Hui Liu, Junhui Hou, Sam Kwong, Qingfu Zhang", "title": "Multi-View Spectral Clustering Tailored Tensor Low-Rank Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of multi-view spectral clustering (MVSC)\nbased on tensor low-rank modeling. Unlike the existing methods that all adopt\nan off-the-shelf tensor low-rank norm without considering the special\ncharacteristics of the tensor in MVSC, we design a novel structured tensor\nlow-rank norm tailored to MVSC. Specifically, we explicitly impose a symmetric\nlow-rank constraint and a structured sparse low-rank constraint on the frontal\nand horizontal slices of the tensor to characterize the intra-view and\ninter-view relationships, respectively. Moreover, the two constraints could be\njointly optimized to achieve mutual refinement. On the basis of the novel\ntensor low-rank norm, we formulate MVSC as a convex low-rank tensor recovery\nproblem, which is then efficiently solved with an augmented Lagrange multiplier\nbased method iteratively. Extensive experimental results on five benchmark\ndatasets show that the proposed method outperforms state-of-the-art methods to\na significant extent. Impressively, our method is able to produce perfect\nclustering. In addition, the parameters of our method can be easily tuned, and\nthe proposed model is robust to different datasets, demonstrating its potential\nin practice.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:52:12 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 13:35:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Jia", "Yuheng", ""], ["Liu", "Hui", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""], ["Zhang", "Qingfu", ""]]}, {"id": "2004.14716", "submitter": "Ylva Jansson", "authors": "Ylva Jansson, Maksim Maydanskiy, Lukas Finnveden and Tony Lindeberg", "title": "Inability of spatial transformations of CNN feature maps to support\n  invariant recognition", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of deep learning architectures use spatial transformations of\nCNN feature maps or filters to better deal with variability in object\nappearance caused by natural image transformations. In this paper, we prove\nthat spatial transformations of CNN feature maps cannot align the feature maps\nof a transformed image to match those of its original, for general affine\ntransformations, unless the extracted features are themselves invariant. Our\nproof is based on elementary analysis for both the single- and multi-layer\nnetwork case. The results imply that methods based on spatial transformations\nof CNN feature maps or filters cannot replace image alignment of the input and\ncannot enable invariant recognition for general affine transformations,\nspecifically not for scaling transformations or shear transformations. For\nrotations and reflections, spatially transforming feature maps or filters can\nenable invariance but only for networks with learnt or hardcoded rotation- or\nreflection-invariant features\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:12:58 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Jansson", "Ylva", ""], ["Maydanskiy", "Maksim", ""], ["Finnveden", "Lukas", ""], ["Lindeberg", "Tony", ""]]}, {"id": "2004.14745", "submitter": "Ralf Raumanns", "authors": "Ralf Raumanns, Elif K Contar, Gerard Schouten, Veronika Cheplygina", "title": "Multi-task Ensembles with Crowdsourced Features Improve Skin Lesion\n  Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has a recognised need for large amounts of annotated data.\nDue to the high cost of expert annotations, crowdsourcing, where non-experts\nare asked to label or outline images, has been proposed as an alternative.\nAlthough many promising results are reported, the quality of diagnostic\ncrowdsourced labels is still unclear. We propose to address this by instead\nasking the crowd about visual features of the images, which can be provided\nmore intuitively, and by using these features in a multi-task learning\nframework through ensemble strategies. We compare our proposed approach to a\nbaseline model with a set of 2000 skin lesions from the ISIC 2017 challenge\ndataset. The baseline model only predicts a binary label from the skin lesion\nimage, while our multi-task model also predicts one of the following features:\nasymmetry of the lesion, border irregularity and color. We show that multi-task\nmodels with individual crowdsourced features have limited effect on the model,\nbut when combined in an ensembles, leads to improved generalisation. The area\nunder the receiver operating characteristic curve is 0.794 for the baseline\nmodel and 0.811 and 0.808 for multi-task ensembles respectively. Finally, we\ndiscuss the findings, identify some limitations and recommend directions for\nfurther research. The code of the models is available at\nhttps://github.com/raumannsr/hints_crowd.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:48:40 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:12:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Raumanns", "Ralf", ""], ["Contar", "Elif K", ""], ["Schouten", "Gerard", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2004.14747", "submitter": "Miguel Angel Sotelo", "authors": "Raul Quintero, Ignacio Parra, David Fernandez Llorca, Miguel Angel\n  Sotelo", "title": "Pedestrian Path, Pose and Intention Prediction through Gaussian Process\n  Dynamical Models and Pedestrian Activity Recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to several reports published by worldwide organisations, thousands\nof pedestrians die in road accidents every year. Due to this fact, vehicular\ntechnologies have been evolving with the intent of reducing these fatalities.\nThis evolution has not finished yet since, for instance, the predictions of\npedestrian paths could improve the current Automatic Emergency Braking Systems\n(AEBS). For this reason, this paper proposes a method to predict future\npedestrian paths, poses and intentions up to 1s in advance. This method is\nbased on Balanced Gaussian Process Dynamical Models (B-GPDMs), which reduce the\n3D time-related information extracted from keypoints or joints placed along\npedestrian bodies into low-dimensional spaces. The B-GPDM is also capable of\ninferring future latent positions and reconstruct their associated\nobservations. However, learning a generic model for all kind of pedestrian\nactivities normally provides less ccurate predictions. For this reason, the\nproposed method obtains multiple models of four types of activity, i.e.\nwalking, stopping, starting and standing, and selects the most similar model to\nestimate future pedestrian states. This method detects starting activities\n125ms after the gait initiation with an accuracy of 80% and recognises stopping\nintentions 58.33ms before the event with an accuracy of 70%. Concerning the\npath prediction, the mean error for stopping activities at a Time-To-Event\n(TTE) of 1s is 238.01mm and, for starting actions, the mean error at a TTE of\n0s is 331.93mm.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:08:46 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Quintero", "Raul", ""], ["Parra", "Ignacio", ""], ["Llorca", "David Fernandez", ""], ["Sotelo", "Miguel Angel", ""]]}, {"id": "2004.14774", "submitter": "Qi She", "authors": "Qi She, Fan Feng, Qi Liu, Rosa H. M. Chan, Xinyue Hao, Chuanlin Lan,\n  Qihan Yang, Vincenzo Lomonaco, German I. Parisi, Heechul Bae, Eoin Brophy,\n  Baoquan Chen, Gabriele Graffieti, Vidit Goel, Hyonyoung Han, Sathursan\n  Kanagarajah, Somesh Kumar, Siew-Kei Lam, Tin Lun Lam, Liang Ma, Davide\n  Maltoni, Lorenzo Pellegrini, Duvindu Piyasena, Shiliang Pu, Debdoot Sheet,\n  Soonyong Song, Youngsung Son, Zhengwei Wang, Tomas E. Ward, Jianwen Wu,\n  Meiqing Wu, Di Xie, Yangsheng Xu, Lin Yang, Qiaoyong Zhong, Liguang Zhou", "title": "IROS 2019 Lifelong Robotic Vision Challenge -- Lifelong Object\n  Recognition Report", "comments": "9 pages, 11 figures, 3 tables, accepted into IEEE Robotics and\n  Automation Magazine. arXiv admin note: text overlap with arXiv:1911.06487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report summarizes IROS 2019-Lifelong Robotic Vision Competition\n(Lifelong Object Recognition Challenge) with methods and results from the top\n$8$ finalists (out of over~$150$ teams). The competition dataset (L)ifel(O)ng\n(R)obotic V(IS)ion (OpenLORIS) - Object Recognition (OpenLORIS-object) is\ndesigned for driving lifelong/continual learning research and application in\nrobotic vision domain, with everyday objects in home, office, campus, and mall\nscenarios. The dataset explicitly quantifies the variants of illumination,\nobject occlusion, object size, camera-object distance/angles, and clutter\ninformation. Rules are designed to quantify the learning capability of the\nrobotic vision system when faced with the objects appearing in the dynamic\nenvironments in the contest. Individual reports, dataset information, rules,\nand released source code can be found at the project homepage:\n\"https://lifelong-robotic-vision.github.io/competition/\".\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:33:55 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["She", "Qi", ""], ["Feng", "Fan", ""], ["Liu", "Qi", ""], ["Chan", "Rosa H. M.", ""], ["Hao", "Xinyue", ""], ["Lan", "Chuanlin", ""], ["Yang", "Qihan", ""], ["Lomonaco", "Vincenzo", ""], ["Parisi", "German I.", ""], ["Bae", "Heechul", ""], ["Brophy", "Eoin", ""], ["Chen", "Baoquan", ""], ["Graffieti", "Gabriele", ""], ["Goel", "Vidit", ""], ["Han", "Hyonyoung", ""], ["Kanagarajah", "Sathursan", ""], ["Kumar", "Somesh", ""], ["Lam", "Siew-Kei", ""], ["Lam", "Tin Lun", ""], ["Ma", "Liang", ""], ["Maltoni", "Davide", ""], ["Pellegrini", "Lorenzo", ""], ["Piyasena", "Duvindu", ""], ["Pu", "Shiliang", ""], ["Sheet", "Debdoot", ""], ["Song", "Soonyong", ""], ["Son", "Youngsung", ""], ["Wang", "Zhengwei", ""], ["Ward", "Tomas E.", ""], ["Wu", "Jianwen", ""], ["Wu", "Meiqing", ""], ["Xie", "Di", ""], ["Xu", "Yangsheng", ""], ["Yang", "Lin", ""], ["Zhong", "Qiaoyong", ""], ["Zhou", "Liguang", ""]]}, {"id": "2004.14795", "submitter": "Jingcai Guo", "authors": "Jingcai Guo, Song Guo", "title": "A Novel Perspective to Zero-shot Learning: Towards an Alignment of\n  Manifold Structures via Semantic Feature Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning aims at recognizing unseen classes (no training example)\nwith knowledge transferred from seen classes. This is typically achieved by\nexploiting a semantic feature space shared by both seen and unseen classes,\ni.e., attribute or word vector, as the bridge. One common practice in zero-shot\nlearning is to train a projection between the visual and semantic feature\nspaces with labeled seen classes examples. When inferring, this learned\nprojection is applied to unseen classes and recognizes the class labels by some\nmetrics. However, the visual and semantic feature spaces are mutually\nindependent and have quite different manifold structures. Under such a\nparadigm, most existing methods easily suffer from the domain shift problem and\nweaken the performance of zero-shot recognition. To address this issue, we\npropose a novel model called AMS-SFE. It considers the alignment of manifold\nstructures by semantic feature expansion. Specifically, we build upon an\nautoencoder-based model to expand the semantic features from the visual inputs.\nAdditionally, the expansion is jointly guided by an embedded manifold extracted\nfrom the visual feature space of the data. Our model is the first attempt to\nalign both feature spaces by expanding semantic features and derives two\nbenefits: first, we expand some auxiliary features that enhance the semantic\nfeature space; second and more importantly, we implicitly align the manifold\nstructures between the visual and semantic feature spaces; thus, the projection\ncan be better trained and mitigate the domain shift problem. Extensive\nexperiments show significant performance improvement, which verifies the\neffectiveness of our model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:08:10 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Guo", "Jingcai", ""], ["Guo", "Song", ""]]}, {"id": "2004.14798", "submitter": "Jiawei Du", "authors": "Jiawei Du, Hanshu Yan, Vincent Y. F. Tan, Joey Tianyi Zhou, Rick Siow\n  Mong Goh, Jiashi Feng", "title": "RAIN: A Simple Approach for Robust and Accurate Image Classification\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that the majority of existing adversarial defense methods\nachieve robustness at the cost of sacrificing prediction accuracy. The\nundesirable severe drop in accuracy adversely affects the reliability of\nmachine learning algorithms and prohibits their deployment in realistic\napplications. This paper aims to address this dilemma by proposing a novel\npreprocessing framework, which we term Robust and Accurate Image\nclassificatioN(RAIN), to improve the robustness of given CNN classifiers and,\nat the same time, preserve their high prediction accuracies. RAIN introduces a\nnew randomization-enhancement scheme. It applies randomization over inputs to\nbreak the ties between the model forward prediction path and the backward\ngradient path, thus improving the model robustness. However, similar to\nexisting preprocessing-based methods, the randomized process will degrade the\nprediction accuracy. To understand why this is the case, we compare the\ndifference between original and processed images, and find it is the loss of\nhigh-frequency components in the input image that leads to accuracy drop of the\nclassifier. Based on this finding, RAIN enhances the input's high-frequency\ndetails to retain the CNN's high prediction accuracy. Concretely, RAIN consists\nof two novel randomization modules: randomized small circular shift (RdmSCS)\nand randomized down-upsampling (RdmDU). The RdmDU module randomly downsamples\nthe input image, and then the RdmSCS module circularly shifts the input image\nalong a randomly chosen direction by a small but random number of pixels.\nFinally, the RdmDU module performs upsampling with a detail-enhancement model,\nsuch as deep super-resolution networks. We conduct extensive experiments on the\nSTL10 and ImageNet datasets to verify the effectiveness of RAIN against various\ntypes of adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 02:03:56 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 11:34:11 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 16:56:42 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 13:24:52 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Du", "Jiawei", ""], ["Yan", "Hanshu", ""], ["Tan", "Vincent Y. F.", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""], ["Feng", "Jiashi", ""]]}, {"id": "2004.14840", "submitter": "Georgios Paraskevopoulos", "authors": "Georgios Paraskevopoulos, Srinivas Parthasarathy, Aparna Khare, and\n  Shiva Sundaram", "title": "Multiresolution and Multimodal Speech Recognition with Transformers", "comments": "Accepted for ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an audio visual automatic speech recognition (AV-ASR)\nsystem using a Transformer-based architecture. We particularly focus on the\nscene context provided by the visual information, to ground the ASR. We extract\nrepresentations for audio features in the encoder layers of the transformer and\nfuse video features using an additional crossmodal multihead attention layer.\nAdditionally, we incorporate a multitask training criterion for multiresolution\nASR, where we train the model to generate both character and subword level\ntranscriptions.\n  Experimental results on the How2 dataset, indicate that multiresolution\ntraining can speed up convergence by around 50% and relatively improves word\nerror rate (WER) performance by upto 18% over subword prediction models.\nFurther, incorporating visual information improves performance with relative\ngains upto 3.76% over audio only models.\n  Our results are comparable to state-of-the-art Listen, Attend and Spell-based\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 09:32:11 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Paraskevopoulos", "Georgios", ""], ["Parthasarathy", "Srinivas", ""], ["Khare", "Aparna", ""], ["Sundaram", "Shiva", ""]]}, {"id": "2004.14858", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis,\n  Xinchen Du, Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Bj\\\"orn W.\n  Schuller, Iulia Lefter, Erik Cambria, Ioannis Kompatsiaris", "title": "MuSe 2020 -- The First International Multimodal Sentiment Analysis in\n  Real-life Media Challenge and Workshop", "comments": "Baseline Paper MuSe 2020, MuSe Workshop Challenge, ACM Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal Sentiment Analysis in Real-life Media (MuSe) 2020 is a\nChallenge-based Workshop focusing on the tasks of sentiment recognition, as\nwell as emotion-target engagement and trustworthiness detection by means of\nmore comprehensively integrating the audio-visual and language modalities. The\npurpose of MuSe 2020 is to bring together communities from different\ndisciplines; mainly, the audio-visual emotion recognition community\n(signal-based), and the sentiment analysis community (symbol-based). We present\nthree distinct sub-challenges: MuSe-Wild, which focuses on continuous emotion\n(arousal and valence) prediction; MuSe-Topic, in which participants recognise\ndomain-specific topics as the target of 3-class (low, medium, high) emotions;\nand MuSe-Trust, in which the novel aspect of trustworthiness is to be\npredicted. In this paper, we provide detailed information on MuSe-CaR, the\nfirst of its kind in-the-wild database, which is utilised for the challenge, as\nwell as the state-of-the-art features and modelling approaches applied. For\neach sub-challenge, a competitive baseline for participants is set; namely, on\ntest we report for MuSe-Wild a combined (valence and arousal) CCC of .2568, for\nMuSe-Topic a score (computed as 0.34$\\cdot$ UAR + 0.66$\\cdot$F1) of 76.78 % on\nthe 10-class topic and 40.64 % on the 3-class emotion prediction, and for\nMuSe-Trust a CCC of .4359.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:54:22 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 16:05:49 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 08:37:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Rizos", "Georgios", ""], ["Tzirakis", "Panagiotis", ""], ["Du", "Xinchen", ""], ["Hafner", "Felix", ""], ["Schumann", "Lea", ""], ["Mallol-Ragolta", "Adria", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Lefter", "Iulia", ""], ["Cambria", "Erik", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2004.14874", "submitter": "Ben Saunders", "authors": "Ben Saunders and Necati Cihan Camgoz and Richard Bowden", "title": "Progressive Transformers for End-to-End Sign Language Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of automatic Sign Language Production (SLP) is to translate spoken\nlanguage to a continuous stream of sign language video at a level comparable to\na human translator. If this was achievable, then it would revolutionise Deaf\nhearing communications. Previous work on predominantly isolated SLP has shown\nthe need for architectures that are better suited to the continuous domain of\nfull sign sequences.\n  In this paper, we propose Progressive Transformers, a novel architecture that\ncan translate from discrete spoken language sentences to continuous 3D skeleton\npose outputs representing sign language. We present two model configurations,\nan end-to-end network that produces sign direct from text and a stacked network\nthat utilises a gloss intermediary.\n  Our transformer network architecture introduces a counter that enables\ncontinuous sequence generation at training and inference. We also provide\nseveral data augmentation processes to overcome the problem of drift and\nimprove the performance of SLP models. We propose a back translation evaluation\nmechanism for SLP, presenting benchmark quantitative results on the challenging\nRWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:20:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 10:20:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2004.14875", "submitter": "Nicolas Girard", "authors": "Nicolas Girard, Dmitriy Smirnov, Justin Solomon, Yuliya Tarabalka", "title": "Polygonal Building Segmentation by Frame Field Learning", "comments": "CVPR 2021 - IEEE Conference on Computer Vision and Pattern\n  Recognition, Jun 2021, Pittsburg / Virtual, United States", "journal-ref": null, "doi": null, "report-no": "hal-02548545, v2", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state of the art image segmentation models typically output\nsegmentations in raster format, applications in geographic information systems\noften require vector polygons. To help bridge the gap between deep network\noutput and the format used in downstream tasks, we add a frame field output to\na deep segmentation model for extracting buildings from remote sensing images.\nWe train a deep neural network that aligns a predicted frame field to ground\ntruth contours. This additional objective improves segmentation quality by\nleveraging multi-task learning and provides structural information that later\nfacilitates polygonization; we also introduce a polygonization algorithm that\nutilizes the frame field along with the raster segmentation. Our code is\navailable at https://github.com/Lydorn/Polygonization-by-Frame-Field-Learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:21:56 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 13:30:18 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Girard", "Nicolas", ""], ["Smirnov", "Dmitriy", ""], ["Solomon", "Justin", ""], ["Tarabalka", "Yuliya", ""]]}, {"id": "2004.14878", "submitter": "Zdenek Straka", "authors": "Zdenek Straka, Tomas Svoboda, Matej Hoffmann", "title": "PreCNet: Next Frame Video Prediction Based on Predictive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding, currently a highly influential theory in neuroscience, has\nnot been widely adopted in machine learning yet. In this work, we transform the\nseminal model of Rao and Ballard (1999) into a modern deep learning framework\nwhile remaining maximally faithful to the original schema. The resulting\nnetwork we propose (PreCNet) is tested on a widely used next frame video\nprediction benchmark, which consists of images from an urban environment\nrecorded from a car-mounted camera. On this benchmark (training: 41k images\nfrom KITTI dataset; testing: Caltech Pedestrian dataset), we achieve to our\nknowledge the best performance to date when measured with the Structural\nSimilarity Index (SSIM). Performance on all measures was further improved when\na larger training set (2M images from BDD100k), pointing to the limitations of\nthe KITTI training set. This work demonstrates that an architecture carefully\nbased in a neuroscience model, without being explicitly tailored to the task at\nhand, can exhibit unprecedented performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:31:24 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 13:58:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Straka", "Zdenek", ""], ["Svoboda", "Tomas", ""], ["Hoffmann", "Matej", ""]]}, {"id": "2004.14899", "submitter": "Shihao Zou", "authors": "Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chuan Guo, Chi Xu,\n  Minglun Gong, and Li Cheng", "title": "Polarization Human Shape and Pose Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarization images are known to be able to capture polarized reflected\nlights that preserve rich geometric cues of an object, which has motivated its\nrecent applications in reconstructing detailed surface normal of the objects of\ninterest. Meanwhile, inspired by the recent breakthroughs in human shape\nestimation from a single color image, we attempt to investigate the new\nquestion of whether the geometric cues from polarization camera could be\nleveraged in estimating detailed human body shapes. This has led to the\ncuration of Polarization Human Shape and Pose Dataset (PHSPD), our home-grown\npolarization image dataset of various human shapes and poses.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 15:58:21 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 23:48:40 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Zou", "Shihao", ""], ["Zuo", "Xinxin", ""], ["Qian", "Yiming", ""], ["Wang", "Sen", ""], ["Guo", "Chuan", ""], ["Xu", "Chi", ""], ["Gong", "Minglun", ""], ["Cheng", "Li", ""]]}, {"id": "2004.14936", "submitter": "Michael Gadermayr", "authors": "Maximilian Ernst Tschuchnig, Gertie Janneke Oostingh, Michael\n  Gadermayr", "title": "Generative Adversarial Networks in Digital Pathology: A Survey on Trends\n  and Future Potential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image analysis in the field of digital pathology has recently gained\nincreased popularity. The use of high-quality whole slide scanners enables the\nfast acquisition of large amounts of image data, showing extensive context and\nmicroscopic detail at the same time. Simultaneously, novel machine learning\nalgorithms have boosted the performance of image analysis approaches. In this\npaper, we focus on a particularly powerful class of architectures, called\nGenerative Adversarial Networks (GANs), applied to histological image data.\nBesides improving performance, GANs also enable application scenarios in this\nfield, which were previously intractable. However, GANs could exhibit a\npotential for introducing bias. Hereby, we summarize the recent\nstate-of-the-art developments in a generalizing notation, present the main\napplications of GANs and give an outlook of some chosen promising approaches\nand their possible future applications. In addition, we identify currently\nunavailable methods with potential for future applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:38:06 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 06:05:07 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Tschuchnig", "Maximilian Ernst", ""], ["Oostingh", "Gertie Janneke", ""], ["Gadermayr", "Michael", ""]]}, {"id": "2004.14960", "submitter": "Yi Zhu", "authors": "Yi Zhu, Zhongyue Zhang, Chongruo Wu, Zhi Zhang, Tong He, Hang Zhang,\n  R. Manmatha, Mu Li, Alexander Smola", "title": "Improving Semantic Segmentation via Self-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning usually achieves the best results with complete supervision. In\nthe case of semantic segmentation, this means that large amounts of pixelwise\nannotations are required to learn accurate models. In this paper, we show that\nwe can obtain state-of-the-art results using a semi-supervised approach,\nspecifically a self-training paradigm. We first train a teacher model on\nlabeled data, and then generate pseudo labels on a large set of unlabeled data.\nOur robust training framework can digest human-annotated and pseudo labels\njointly and achieve top performances on Cityscapes, CamVid and KITTI datasets\nwhile requiring significantly less supervision. We also demonstrate the\neffectiveness of self-training on a challenging cross-domain generalization\ntask, outperforming conventional finetuning method by a large margin. Lastly,\nto alleviate the computational burden caused by the large amount of pseudo\nlabels, we propose a fast training schedule to accelerate the training of\nsegmentation models by up to 2x without performance degradation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:09:17 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 16:57:02 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Zhu", "Yi", ""], ["Zhang", "Zhongyue", ""], ["Wu", "Chongruo", ""], ["Zhang", "Zhi", ""], ["He", "Tong", ""], ["Zhang", "Hang", ""], ["Manmatha", "R.", ""], ["Li", "Mu", ""], ["Smola", "Alexander", ""]]}, {"id": "2004.14973", "submitter": "Arjun Majumdar", "authors": "Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi\n  Parikh, Dhruv Batra", "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the\n  Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a navigation instruction such as 'Walk down the stairs and stop at\nthe brown sofa' requires embodied AI agents to ground scene elements referenced\nvia language (e.g. 'stairs') to visual content in the environment (pixels\ncorresponding to 'stairs').\n  We ask the following question -- can we leverage abundant 'disembodied'\nweb-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn\nvisual groundings (what do 'stairs' look like?) that improve performance on a\nrelatively data-starved embodied perception task (Vision-and-Language\nNavigation)? Specifically, we develop VLN-BERT, a visiolinguistic\ntransformer-based model for scoring the compatibility between an instruction\n('...stop at the brown sofa') and a sequence of panoramic RGB images captured\nby the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from\nthe web before fine-tuning on embodied path-instruction data significantly\nimproves performance on VLN -- outperforming the prior state-of-the-art in the\nfully-observed setting by 4 absolute percentage points on success rate.\nAblations of our pretraining curriculum show each stage to be impactful -- with\ntheir combination resulting in further positive synergistic effects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:22:40 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 17:16:50 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Majumdar", "Arjun", ""], ["Shrivastava", "Ayush", ""], ["Lee", "Stefan", ""], ["Anderson", "Peter", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "2004.15004", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Robert Turko, Omar Shaikh, Haekyu Park, Nilaksh Das,\n  Fred Hohman, Minsuk Kahng, Duen Horng Chau", "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive\n  Visualization", "comments": "11 pages, 14 figures, to be presented at IEEE VIS 2020. For a demo\n  video, see https://youtu.be/HnWIHWFbuUQ . For a live demo, visit\n  https://poloclub.github.io/cnn-explainer/", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030418", "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning's great success motivates many practitioners and students to\nlearn about this exciting technology. However, it is often challenging for\nbeginners to take their first step due to the complexity of understanding and\napplying deep learning. We present CNN Explainer, an interactive visualization\ntool designed for non-experts to learn and examine convolutional neural\nnetworks (CNNs), a foundational deep learning model architecture. Our tool\naddresses key challenges that novices face while learning about CNNs, which we\nidentify from interviews with instructors and a survey with past students. CNN\nExplainer tightly integrates a model overview that summarizes a CNN's\nstructure, and on-demand, dynamic visual explanation views that help users\nunderstand the underlying components of CNNs. Through smooth transitions across\nlevels of abstraction, our tool enables users to inspect the interplay between\nlow-level mathematical operations and high-level model structures. A\nqualitative user study shows that CNN Explainer helps users more easily\nunderstand the inner workings of CNNs, and is engaging and enjoyable to use. We\nalso derive design lessons from our study. Developed using modern web\ntechnologies, CNN Explainer runs locally in users' web browsers without the\nneed for installation or specialized hardware, broadening the public's\neducation access to modern deep learning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:49:44 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 01:37:29 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 18:42:23 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wang", "Zijie J.", ""], ["Turko", "Robert", ""], ["Shaikh", "Omar", ""], ["Park", "Haekyu", ""], ["Das", "Nilaksh", ""], ["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2004.15014", "submitter": "Siddhartha Gairola", "authors": "Siddhartha Gairola, Mayur Hemani, Ayush Chopra and Balaji\n  Krishnamurthy", "title": "SimPropNet: Improved Similarity Propagation for Few-shot Image\n  Segmentation", "comments": "An updated version of this work was accepted at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot segmentation (FSS) methods perform image segmentation for a\nparticular object class in a target (query) image, using a small set of\n(support) image-mask pairs. Recent deep neural network based FSS methods\nleverage high-dimensional feature similarity between the foreground features of\nthe support images and the query image features. In this work, we demonstrate\ngaps in the utilization of this similarity information in existing methods, and\npresent a framework - SimPropNet, to bridge those gaps. We propose to jointly\npredict the support and query masks to force the support features to share\ncharacteristics with the query features. We also propose to utilize\nsimilarities in the background regions of the query and support images using a\nnovel foreground-background attentive fusion mechanism. Our method achieves\nstate-of-the-art results for one-shot and five-shot segmentation on the\nPASCAL-5i dataset. The paper includes detailed analysis and ablation studies\nfor the proposed improvements and quantitative comparisons with contemporary\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:56:48 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 08:00:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Gairola", "Siddhartha", ""], ["Hemani", "Mayur", ""], ["Chopra", "Ayush", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2004.15021", "submitter": "Jia-Bin Huang", "authors": "Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf", "title": "Consistent Video Depth Estimation", "comments": "SIGGRAPH 2020. Video: https://www.youtube.com/watch?v=5Tia2oblJAg\n  Project page: https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\n  Code: https://github.com/facebookresearch/consistent_depth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for reconstructing dense, geometrically consistent\ndepth for all pixels in a monocular video. We leverage a conventional\nstructure-from-motion reconstruction to establish geometric constraints on\npixels in the video. Unlike the ad-hoc priors in classical reconstruction, we\nuse a learning-based prior, i.e., a convolutional neural network trained for\nsingle-image depth estimation. At test time, we fine-tune this network to\nsatisfy the geometric constraints of a particular input video, while retaining\nits ability to synthesize plausible depth details in parts of the video that\nare less constrained. We show through quantitative validation that our method\nachieves higher accuracy and a higher degree of geometric consistency than\nprevious monocular reconstruction methods. Visually, our results appear more\nstable. Our algorithm is able to handle challenging hand-held captured input\nvideos with a moderate degree of dynamic motion. The improved quality of the\nreconstruction enables several applications, such as scene reconstruction and\nadvanced video-based visual effects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:59:26 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 20:11:33 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Luo", "Xuan", ""], ["Huang", "Jia-Bin", ""], ["Szeliski", "Richard", ""], ["Matzen", "Kevin", ""], ["Kopf", "Johannes", ""]]}]