[{"id": "2107.00005", "submitter": "Pradipta Sasmal", "authors": "Pradipta Sasmal, Avinash Paul, M.K. Bhuyan, and Yuji Iwahori", "title": "Extraction of Key-frames of Endoscopic Videos by using Depth Information", "comments": null, "journal-ref": "The paper is under consideration at Pattern Recognition Letters,\n  Elsevier, 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deep learning-based monocular depth estimation (MDE) technique is proposed\nfor selection of most informative frames (key frames) of an endoscopic video.\nIn most of the cases, ground truth depth maps of polyps are not readily\navailable and that is why the transfer learning approach is adopted in our\nmethod. An endoscopic modalities generally capture thousands of frames. In this\nscenario, it is quite important to discard low-quality and clinically\nirrelevant frames of an endoscopic video while the most informative frames\nshould be retained for clinical diagnosis. In this view, a key-frame selection\nstrategy is proposed by utilizing the depth information of polyps. In our\nmethod, image moment, edge magnitude, and key-points are considered for\nadaptively selecting the key frames. One important application of our proposed\nmethod could be the 3D reconstruction of polyps with the help of extracted key\nframes. Also, polyps are localized with the help of extracted depth maps.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:48:23 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sasmal", "Pradipta", ""], ["Paul", "Avinash", ""], ["Bhuyan", "M. K.", ""], ["Iwahori", "Yuji", ""]]}, {"id": "2107.00057", "submitter": "Xianzhi Du", "authors": "Xianzhi Du, Barret Zoph, Wei-Chih Hung, Tsung-Yi Lin", "title": "Simple Training Strategies and Model Scaling for Object Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The speed-accuracy Pareto curve of object detection systems have advanced\nthrough a combination of better model architectures, training and inference\nmethods. In this paper, we methodically evaluate a variety of these techniques\nto understand where most of the improvements in modern detection systems come\nfrom. We benchmark these improvements on the vanilla ResNet-FPN backbone with\nRetinaNet and RCNN detectors. The vanilla detectors are improved by 7.7% in\naccuracy while being 30% faster in speed. We further provide simple scaling\nstrategies to generate family of models that form two Pareto curves, named\nRetinaNet-RS and Cascade RCNN-RS. These simple rescaled detectors explore the\nspeed-accuracy trade-off between the one-stage RetinaNet detectors and\ntwo-stage RCNN detectors. Our largest Cascade RCNN-RS models achieve 52.9% AP\nwith a ResNet152-FPN backbone and 53.6% with a SpineNet143L backbone. Finally,\nwe show the ResNet architecture, with three minor architectural changes,\noutperforms EfficientNet as the backbone for object detection and instance\nsegmentation systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 18:41:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Du", "Xianzhi", ""], ["Zoph", "Barret", ""], ["Hung", "Wei-Chih", ""], ["Lin", "Tsung-Yi", ""]]}, {"id": "2107.00067", "submitter": "Pratik Mazumder", "authors": "Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri", "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision\n  and Self-Distillation", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models generally learn the biases present in the training data.\nResearchers have proposed several approaches to mitigate such biases and make\nthe model fair. Bias mitigation techniques assume that a sufficiently large\nnumber of training examples are present. However, we observe that if the\ntraining data is limited, then the effectiveness of bias mitigation methods is\nseverely degraded. In this paper, we propose a novel approach to address this\nproblem. Specifically, we adapt self-supervision and self-distillation to\nreduce the impact of biases on the model in this setting. Self-supervision and\nself-distillation are not used for bias mitigation. However, through this work,\nwe demonstrate for the first time that these techniques are very effective in\nbias mitigation. We empirically show that our approach can significantly reduce\nthe biases learned by the model. Further, we experimentally demonstrate that\nour approach is complementary to other bias mitigation strategies. Our approach\nsignificantly improves their performance and further reduces the model biases\nin the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset,\nour approach significantly reduces the bias score of the baseline model by\n78.22% and outperforms it in terms of accuracy by a significant absolute margin\nof 8.89%. It also significantly reduces the bias score for the state-of-the-art\ndomain independent bias mitigation method by 59.26% and improves its\nperformance by a significant absolute margin of 7.08%.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:22:46 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Mazumder", "Pratik", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2107.00070", "submitter": "Yang Li", "authors": "Yang Li and Shihao Ji", "title": "Dep-$L_0$: Improving $L_0$-based Network Sparsification via Dependency\n  Modeling", "comments": "Published as a conference paper at ECML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks with an $L_0$ regularization is one of the\nprominent approaches for network pruning or sparsification. The method prunes\nthe network during training by encouraging weights to become exactly zero.\nHowever, recent work of Gale et al. reveals that although this method yields\nhigh compression rates on smaller datasets, it performs inconsistently on\nlarge-scale learning tasks, such as ResNet50 on ImageNet. We analyze this\nphenomenon through the lens of variational inference and find that it is likely\ndue to the independent modeling of binary gates, the mean-field approximation,\nwhich is known in Bayesian statistics for its poor performance due to the crude\napproximation. To mitigate this deficiency, we propose a dependency modeling of\nbinary gates, which can be modeled effectively as a multi-layer perceptron\n(MLP). We term our algorithm Dep-$L_0$ as it prunes networks via a\ndependency-enabled $L_0$ regularization. Extensive experiments on CIFAR10,\nCIFAR100 and ImageNet with VGG16, ResNet50, ResNet56 show that our Dep-$L_0$\noutperforms the original $L_0$-HC algorithm of Louizos et al. by a significant\nmargin, especially on ImageNet. Compared with the state-of-the-arts network\nsparsification algorithms, our dependency modeling makes the $L_0$-based\nsparsification once again very competitive on large-scale learning tasks. Our\nsource code is available at https://github.com/leo-yangli/dep-l0.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:33:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Yang", ""], ["Ji", "Shihao", ""]]}, {"id": "2107.00085", "submitter": "Ankit Singh", "authors": "Ankit Singh", "title": "CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) aims to align the labeled source\ndistribution with the unlabeled target distribution to obtain domain invariant\npredictive models. However, the application of well-known UDA approaches does\nnot generalize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where\nfew labeled samples from the target domain are available. In this paper, we\npropose a simple Contrastive Learning framework for semi-supervised Domain\nAdaptation (CLDA) that attempts to bridge the intra-domain gap between the\nlabeled and unlabeled target distributions and inter-domain gap between source\nand unlabeled target distribution in SSDA. We suggest employing class-wise\ncontrastive learning to reduce the inter-domain gap and instance-level\ncontrastive alignment between the original (input image) and strongly augmented\nunlabeled target images to minimize the intra-domain discrepancy. We have shown\nempirically that both of these modules complement each other to achieve\nsuperior performance. Experiments on three well-known domain adaptation\nbenchmark datasets namely DomainNet, Office-Home, and Office31 demonstrate the\neffectiveness of our approach. CLDA achieves state-of-the-art results on all\nthe above datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 20:23:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Singh", "Ankit", ""]]}, {"id": "2107.00115", "submitter": "Vasudevan Lakshminarayanan", "authors": "Vasudevan Lakshminarayanan, Hoda Kherdfallah, Arya Sarkar, J. Jothi\n  Balaji", "title": "Automated Detection and Diagnosis of Diabetic Retinopathy: A\n  Comprehensive Survey", "comments": "Submitted to MDPI Journal of Imaging special issue \"Frontiers In\n  Retinal Image Processing\"2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetic Retinopathy (DR) is a leading cause of vision loss in the world,. In\nthe past few Diabetic Retinopathy (DR) is a leading cause of vision loss in the\nworld. In the past few years, Artificial Intelligence (AI) based approaches\nhave been used to detect and grade DR. Early detection enables appropriate\ntreatment and thus prevents vision loss, Both fundus and optical coherence\ntomography (OCT) images are used to image the retina. With deep\nlearning/machine learning apprroaches it is possible to extract features from\nthe images and detect the presence of DR. Multiple strategies are implemented\nto detect and grade the presence of DR using classification, segmentation, and\nhybrid techniques. This review covers the literature dealing with AI approaches\nto DR that have been published in the open literature over a five year span\n(2016-2021). In addition a comprehensive list of available DR datasets is\nreported. Both the PICO (P-patient, I-intervention, C-control O-outcome) and\nPreferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA)2009\nsearch strategies were employed. We summarize a total of 114 published articles\nwhich conformed to the scope of the review. In addition a list of 43 major\ndatasets is presented.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 21:45:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lakshminarayanan", "Vasudevan", ""], ["Kherdfallah", "Hoda", ""], ["Sarkar", "Arya", ""], ["Balaji", "J. Jothi", ""]]}, {"id": "2107.00135", "submitter": "Arsha Nagrani", "authors": "Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid\n  and Chen Sun", "title": "Attention Bottlenecks for Multimodal Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perceive the world by concurrently processing and fusing\nhigh-dimensional inputs from multiple modalities such as vision and audio.\nMachine perception models, in stark contrast, are typically modality-specific\nand optimised for unimodal benchmarks, and hence late-stage fusion of final\nrepresentations or predictions from each modality (`late-fusion') is still a\ndominant paradigm for multimodal video classification. Instead, we introduce a\nnovel transformer based architecture that uses `fusion bottlenecks' for\nmodality fusion at multiple layers. Compared to traditional pairwise\nself-attention, our model forces information between different modalities to\npass through a small number of bottleneck latents, requiring the model to\ncollate and condense the most relevant information in each modality and only\nshare what is necessary. We find that such a strategy improves fusion\nperformance, at the same time reducing computational cost. We conduct thorough\nablation studies, and achieve state-of-the-art results on multiple audio-visual\nclassification benchmarks including Audioset, Epic-Kitchens and VGGSound. All\ncode and models will be released.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:44:12 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nagrani", "Arsha", ""], ["Yang", "Shan", ""], ["Arnab", "Anurag", ""], ["Jansen", "Aren", ""], ["Schmid", "Cordelia", ""], ["Sun", "Chen", ""]]}, {"id": "2107.00143", "submitter": "Takato Yasuno", "authors": "Takato Yasuno, Junichiro Fujii, Sakura Fukami", "title": "One-class Steel Detector Using Patch GAN Discriminator for Visualising\n  Anomalous Feature Map", "comments": "14 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For steel product manufacturing in indoor factories, steel defect detection\nis important for quality control. For example, a steel sheet is extremely\ndelicate, and must be accurately inspected. However, to maintain the painted\nsteel parts of the infrastructure around a severe outdoor environment,\ncorrosion detection is critical for predictive maintenance. In this paper, we\npropose a general-purpose application for steel anomaly detection that consists\nof the following four components. The first, a learner, is a unit image\nclassification network to determine whether the region of interest or\nbackground has been recognised, after dividing the original large sized image\ninto 256 square unit images. The second, an extractor, is a discriminator\nfeature encoder based on a pre-trained steel generator with a patch generative\nadversarial network discriminator(GAN). The third, an anomaly detector, is a\none-class support vector machine(SVM) to predict the anomaly score using the\ndiscriminator feature. The fourth, an indicator, is an anomalous probability\nmap used to visually explain the anomalous features. Furthermore, we\ndemonstrated our method through the inspection of steel sheet defects with\n13,774 unit images using high-speed cameras, and painted steel corrosion with\n19,766 unit images based on an eye inspection of the photographs. Finally, we\nvisualise anomalous feature maps of steel using a strip and painted steel\ninspection dataset\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 23:01:09 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yasuno", "Takato", ""], ["Fujii", "Junichiro", ""], ["Fukami", "Sakura", ""]]}, {"id": "2107.00166", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan\n  Chen, Ning Liu, Minghai Qin, Sijia Liu, Zhangyang Wang, Yanzhi Wang", "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win\n  the Jackpot?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 01:27:07 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ma", "Xiaolong", ""], ["Yuan", "Geng", ""], ["Shen", "Xuan", ""], ["Chen", "Tianlong", ""], ["Chen", "Xuxi", ""], ["Chen", "Xiaohan", ""], ["Liu", "Ning", ""], ["Qin", "Minghai", ""], ["Liu", "Sijia", ""], ["Wang", "Zhangyang", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2107.00181", "submitter": "Zhen Huang", "authors": "Zhen Huang, Xu Shen, Jun Xing, Tongliang Liu, Xinmei Tian, Houqiang\n  Li, Bing Deng, Jianqiang Huang and Xian-Sheng Hua", "title": "Revisiting Knowledge Distillation: An Inheritance and Exploration\n  Framework", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge Distillation (KD) is a popular technique to transfer knowledge from\na teacher model or ensemble to a student model. Its success is generally\nattributed to the privileged information on similarities/consistency between\nthe class distributions or intermediate feature representations of the teacher\nmodel and the student model. However, directly pushing the student model to\nmimic the probabilities/features of the teacher model to a large extent limits\nthe student model in learning undiscovered knowledge/features. In this paper,\nwe propose a novel inheritance and exploration knowledge distillation framework\n(IE-KD), in which a student model is split into two parts - inheritance and\nexploration. The inheritance part is learned with a similarity loss to transfer\nthe existing learned knowledge from the teacher model to the student model,\nwhile the exploration part is encouraged to learn representations different\nfrom the inherited ones with a dis-similarity loss. Our IE-KD framework is\ngeneric and can be easily combined with existing distillation or mutual\nlearning methods for training deep neural networks. Extensive experiments\ndemonstrate that these two parts can jointly push the student model to learn\nmore diversified and effective representations, and our IE-KD can be a general\ntechnique to improve the student network to achieve SOTA performance.\nFurthermore, by applying our IE-KD to the training of two networks, the\nperformance of both can be improved w.r.t. deep mutual learning. The code and\nmodels of IE-KD will be make publicly available at\nhttps://github.com/yellowtownhz/IE-KD.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:20:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Huang", "Zhen", ""], ["Shen", "Xu", ""], ["Xing", "Jun", ""], ["Liu", "Tongliang", ""], ["Tian", "Xinmei", ""], ["Li", "Houqiang", ""], ["Deng", "Bing", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2107.00191", "submitter": "Wonju Lee", "authors": "Wonju Lee, Seok-Yong Byun, Jooeun Kim, Minje Park, Kirill Chechil", "title": "Unsupervised Model Drift Estimation with Batch Normalization Statistics\n  for Dataset Shift Detection and Model Selection", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While many real-world data streams imply that they change frequently in a\nnonstationary way, most of deep learning methods optimize neural networks on\ntraining data, and this leads to severe performance degradation when dataset\nshift happens. However, it is less possible to annotate or inspect newly\nstreamed data by humans, and thus it is desired to measure model drift at\ninference time in an unsupervised manner. In this paper, we propose a novel\nmethod of model drift estimation by exploiting statistics of batch\nnormalization layer on unlabeled test data. To remedy possible sampling error\nof streamed input data, we adopt low-rank approximation to each\nrepresentational layer. We show the effectiveness of our method not only on\ndataset shift detection but also on model selection when there are multiple\ncandidate models among model zoo or training trajectories in an unsupervised\nway. We further demonstrate the consistency of our method by comparing model\ndrift scores between different network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 03:04:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lee", "Wonju", ""], ["Byun", "Seok-Yong", ""], ["Kim", "Jooeun", ""], ["Park", "Minje", ""], ["Chechil", "Kirill", ""]]}, {"id": "2107.00197", "submitter": "Wei-Lun Chao", "authors": "Han-Jia Ye, Lu Ming, De-Chuan Zhan, Wei-Lun Chao", "title": "Few-Shot Learning with a Strong Teacher", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot learning (FSL) aims to train a strong classifier using limited\nlabeled examples. Many existing works take the meta-learning approach, sampling\nfew-shot tasks in turn and optimizing the few-shot learner's performance on\nclassifying the query examples. In this paper, we point out two potential\nweaknesses of this approach. First, the sampled query examples may not provide\nsufficient supervision for the few-shot learner. Second, the effectiveness of\nmeta-learning diminishes sharply with increasing shots (i.e., the number of\ntraining examples per class). To resolve these issues, we propose a novel\nobjective to directly train the few-shot learner to perform like a strong\nclassifier. Concretely, we associate each sampled few-shot task with a strong\nclassifier, which is learned with ample labeled examples. The strong classifier\nhas a better generalization ability and we use it to supervise the few-shot\nlearner. We present an efficient way to construct the strong classifier, making\nour proposed objective an easily plug-and-play term to existing meta-learning\nbased FSL methods. We validate our approach in combinations with many\nrepresentative meta-learning methods. On several benchmark datasets including\nminiImageNet and tiredImageNet, our approach leads to a notable improvement\nacross a variety of tasks. More importantly, with our approach, meta-learning\nbased FSL methods can consistently outperform non-meta-learning based ones,\neven in a many-shot setting, greatly strengthening their applicability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 03:20:46 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ye", "Han-Jia", ""], ["Ming", "Lu", ""], ["Zhan", "De-Chuan", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2107.00206", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Zhenyu Guo, Yang Liu, Yao Zhao", "title": "Multi-modal Graph Learning for Disease Prediction", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Benefiting from the powerful expressive capability of graphs, graph-based\napproaches have achieved impressive performance in various biomedical\napplications. Most existing methods tend to define the adjacency matrix among\nsamples manually based on meta-features, and then obtain the node embeddings\nfor downstream tasks by Graph Representation Learning (GRL). However, it is not\neasy for these approaches to generalize to unseen samples. Meanwhile, the\ncomplex correlation between modalities is also ignored. As a result, these\nfactors inevitably yield the inadequacy of providing valid information about\nthe patient's condition for a reliable diagnosis. In this paper, we propose an\nend-to-end Multimodal Graph Learning framework (MMGL) for disease prediction.\nTo effectively exploit the rich information across multi-modality associated\nwith diseases, amodal-attentional multi-modal fusion is proposed to integrate\nthe features of each modality by leveraging the correlation and complementarity\nbetween the modalities. Furthermore, instead of defining the adjacency matrix\nmanually as existing methods, the latent graph structure can be captured\nthrough a novel way of adaptive graph learning. It could be jointly optimized\nwith the prediction model, thus revealing the intrinsic connections among\nsamples. Unlike the previous transductive methods, our model is also applicable\nto the scenario of inductive learning for those unseen data. An extensive group\nof experiments on two disease prediction problems is then carefully designed\nand presented, demonstrating that MMGL obtains more favorable performances. In\naddition, we also visualize and analyze the learned graph structure to provide\nmore reliable decision support for doctors in real medical applications and\ninspiration for disease research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 03:59:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zheng", "Shuai", ""], ["Zhu", "Zhenfeng", ""], ["Liu", "Zhizhe", ""], ["Guo", "Zhenyu", ""], ["Liu", "Yang", ""], ["Zhao", "Yao", ""]]}, {"id": "2107.00222", "submitter": "Mi Tian", "authors": "Mi Tian, Qiong Nie, Hao Shen, Xiahua Xia", "title": "Deep auxiliary learning for visual localization using colorization task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is one of the most important components for robotics and\nautonomous driving. Recently, inspiring results have been shown with CNN-based\nmethods which provide a direct formulation to end-to-end regress 6-DoF absolute\npose. Additional information like geometric or semantic constraints is\ngenerally introduced to improve performance. Especially, the latter can\naggregate high-level semantic information into localization task, but it\nusually requires enormous manual annotations. To this end, we propose a novel\nauxiliary learning strategy for camera localization by introducing\nscene-specific high-level semantics from self-supervised representation\nlearning task. Viewed as a powerful proxy task, image colorization task is\nchosen as complementary task that outputs pixel-wise color version of grayscale\nphotograph without extra annotations. In our work, feature representations from\ncolorization network are embedded into localization network by design to\nproduce discriminative features for pose regression. Meanwhile an attention\nmechanism is introduced for the benefit of localization performance. Extensive\nexperiments show that our model significantly improve localization accuracy\nover state-of-the-arts on both indoor and outdoor datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:25:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tian", "Mi", ""], ["Nie", "Qiong", ""], ["Shen", "Hao", ""], ["Xia", "Xiahua", ""]]}, {"id": "2107.00223", "submitter": "Kei Uchizawa Dr.", "authors": "Kei Uchizawa and Haruki Abe", "title": "Circuit Complexity of Visual Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study computational hardness of feature and conjunction search through the\nlens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,\ny_n)$) be Boolean variables each of which takes the value one if and only if a\nneuron at place $i$ detects a feature (resp., another feature). We then simply\nformulate the feature and conjunction search as Boolean functions ${\\rm\nFTR}_n(x) = \\bigvee_{i=1}^n x_i$ and ${\\rm CONJ}_n(x, y) = \\bigvee_{i=1}^n x_i\n\\wedge y_i$, respectively. We employ a threshold circuit or a discretized\ncircuit (such as a sigmoid circuit or a ReLU circuit with discretization) as\nour models of neural networks, and consider the following four computational\nresources: [i] the number of neurons (size), [ii] the number of levels (depth),\n[iii] the number of active neurons outputting non-zero values (energy), and\n[iv] synaptic weight resolution (weight).\n  We first prove that any threshold circuit $C$ of size $s$, depth $d$, energy\n$e$ and weight $w$ satisfies $\\log rk(M_C) \\le ed (\\log s + \\log w + \\log n)$,\nwhere $rk(M_C)$ is the rank of the communication matrix $M_C$ of a\n$2n$-variable Boolean function that $C$ computes. Since ${\\rm CONJ}_n$ has rank\n$2^n$, we have $n \\le ed (\\log s + \\log w + \\log n)$. Thus, an exponential\nlower bound on the size of even sublinear-depth threshold circuits exists if\nthe energy and weight are sufficiently small. Since ${\\rm FTR}_n$ is computable\nindependently of $n$, our result suggests that computational capacity for the\nfeature and conjunction search are different. We also show that the inequality\nis tight up to a constant factor if $ed = o(n/ \\log n)$. We next show that a\nsimilar inequality holds for any discretized circuit. Thus, if we regard the\nnumber of gates outputting non-zero values as a measure for sparse activity,\nour results suggest that larger depth helps neural networks to acquire sparse\nactivity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:37:53 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Uchizawa", "Kei", ""], ["Abe", "Haruki", ""]]}, {"id": "2107.00228", "submitter": "Marc Fischer", "authors": "Marc Fischer, Maximilian Baader, Martin Vechev", "title": "Scalable Certified Segmentation via Randomized Smoothing", "comments": "ICML'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new certification method for image and point cloud segmentation\nbased on randomized smoothing. The method leverages a novel scalable algorithm\nfor prediction and certification that correctly accounts for multiple testing,\nnecessary for ensuring statistical guarantees. The key to our approach is\nreliance on established multiple-testing correction mechanisms as well as the\nability to abstain from classifying single pixels or points while still\nrobustly segmenting the overall input. Our experimental evaluation on synthetic\ndata and challenging datasets, such as Pascal Context, Cityscapes, and\nShapeNet, shows that our algorithm can achieve, for the first time, competitive\naccuracy and certification guarantees on real-world segmentation tasks. We\nprovide an implementation at https://github.com/eth-sri/segmentation-smoothing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:52:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fischer", "Marc", ""], ["Baader", "Maximilian", ""], ["Vechev", "Martin", ""]]}, {"id": "2107.00229", "submitter": "Yonghao Long", "authors": "Yonghao Long, Zhaoshuo Li, Chi Hang Yee, Chi Fai Ng, Russell H.\n  Taylor, Mathias Unberath, Qi Dou", "title": "E-DSSR: Efficient Dynamic Surgical Scene Reconstruction with\n  Transformer-based Stereoscopic Depth Perception", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the scene of robotic surgery from the stereo endoscopic video\nis an important and promising topic in surgical data science, which potentially\nsupports many applications such as surgical visual perception, robotic surgery\neducation and intra-operative context awareness. However, current methods are\nmostly restricted to reconstructing static anatomy assuming no tissue\ndeformation, tool occlusion and de-occlusion, and camera movement. However,\nthese assumptions are not always satisfied in minimal invasive robotic\nsurgeries. In this work, we present an efficient reconstruction pipeline for\nhighly dynamic surgical scenes that runs at 28 fps. Specifically, we design a\ntransformer-based stereoscopic depth perception for efficient depth estimation\nand a light-weight tool segmentor to handle tool occlusion. After that, a\ndynamic reconstruction algorithm which can estimate the tissue deformation and\ncamera movement, and aggregate the information over time is proposed for\nsurgical scene reconstruction. We evaluate the proposed pipeline on two\ndatasets, the public Hamlyn Centre Endoscopic Video Dataset and our in-house\nDaVinci robotic surgery dataset. The results demonstrate that our method can\nrecover the scene obstructed by the surgical tool and handle the movement of\ncamera in realistic surgical scenarios effectively at real-time speed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:57:41 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Long", "Yonghao", ""], ["Li", "Zhaoshuo", ""], ["Yee", "Chi Hang", ""], ["Ng", "Chi Fai", ""], ["Taylor", "Russell H.", ""], ["Unberath", "Mathias", ""], ["Dou", "Qi", ""]]}, {"id": "2107.00233", "submitter": "Tehrim Yoon", "authors": "Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang", "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:14:51 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yoon", "Tehrim", ""], ["Shin", "Sumin", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Eunho", ""]]}, {"id": "2107.00235", "submitter": "Stoyan Pavlov", "authors": "Stoyan Pavlov, Galina Momcheva, Pavlina Burlakova, Simeon Atanasov,\n  Dimo Stoyanov, Martin Ivanov, Anton Tonchev", "title": "Feasibility of Haralick's Texture Features for the Classification of\n  Chromogenic In-situ Hybridization Images", "comments": "4 pages, 1 figure", "journal-ref": "2020 International Conference on Biomedical Innovations and\n  Applications (BIA), 2020, pp. 65-68", "doi": "10.1109/BIA50171.2020.9244282", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a proof of concept for the usefulness of second-order\ntexture features for the qualitative analysis and classification of chromogenic\nin-situ hybridization whole slide images in high-throughput imaging\nexperiments. The challenge is that currently, the gold standard for gene\nexpression grading in such images is expert assessment. The idea of the\nresearch team is to use different approaches in the analysis of these images\nthat will be used for structural segmentation and functional analysis in gene\nexpression. The article presents such perspective idea to select a number of\ntextural features that are going to be used for classification. In our\nexperiment, natural grouping of image samples (tiles) depending on their local\ntexture properties was explored in an unsupervised classification procedure.\nThe features are reduced to two dimensions with fuzzy c-means clustering. The\noverall conclusion of this experiment is that Haralick features are a viable\nchoice for classification and analysis of chromogenic in-situ hybridization\nimage data. The principal component analysis approach produced slightly more\n\"understandable\" from an annotator's point of view classes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:18:40 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Pavlov", "Stoyan", ""], ["Momcheva", "Galina", ""], ["Burlakova", "Pavlina", ""], ["Atanasov", "Simeon", ""], ["Stoyanov", "Dimo", ""], ["Ivanov", "Martin", ""], ["Tonchev", "Anton", ""]]}, {"id": "2107.00239", "submitter": "Congcong Li", "authors": "Dexiang Hong, Congcong Li, Longyin Wen, Xinyao Wang, Libo Zhang", "title": "Generic Event Boundary Detection Challenge at CVPR 2021 Technical\n  Report: Cascaded Temporal Attention Network (CASTANET)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report presents the approach used in the submission of Generic Event\nBoundary Detection (GEBD) Challenge at CVPR21. In this work, we design a\nCascaded Temporal Attention Network (CASTANET) for GEBD, which is formed by\nthree parts, the backbone network, the temporal attention module, and the\nclassification module. Specifically, the Channel-Separated Convolutional\nNetwork (CSN) is used as the backbone network to extract features, and the\ntemporal attention module is designed to enforce the network to focus on the\ndiscriminative features. After that, the cascaded architecture is used in the\nclassification module to generate more accurate boundaries. In addition, the\nensemble strategy is used to further improve the performance of the proposed\nmethod. The proposed method achieves 83.30% F1 score on Kinetics-GEBD test set,\nwhich improves 20.5% F1 score compared to the baseline method. Code is\navailable at https://github.com/DexiangHong/Cascade-PC.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:37:01 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hong", "Dexiang", ""], ["Li", "Congcong", ""], ["Wen", "Longyin", ""], ["Wang", "Xinyao", ""], ["Zhang", "Libo", ""]]}, {"id": "2107.00249", "submitter": "Xinxin Zhu", "authors": "Jing Liu, Xinxin Zhu, Fei Liu, Longteng Guo, Zijia Zhao, Mingzhen Sun,\n  Weining Wang, Hanqing Lu, Shiyu Zhou, Jiajun Zhang, Jinqiao Wang", "title": "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an Omni-perception Pre-Trainer (OPT) for\ncross-modal understanding and generation, by jointly modeling visual, text and\naudio resources. OPT is constructed in an encoder-decoder framework, including\nthree single-modal encoders to generate token-based embeddings for each\nmodality, a cross-modal encoder to encode the correlations among the three\nmodalities, and two cross-modal decoders to generate text and image\nrespectively. For the OPT's pre-training, we design a multi-task pretext\nlearning scheme to model multi-modal resources from three different data\ngranularities, \\ie, token-, modality-, and sample-level modeling, through which\nOPT learns to align and translate among different modalities. The pre-training\ntask is carried out on a large amount of image-text-audio triplets from Open\nImages. Experimental results show that OPT can learn strong image-text-audio\nmulti-modal representations and achieve promising results on a variety of\ncross-modal understanding and generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:59:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 03:18:27 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["Liu", "Fei", ""], ["Guo", "Longteng", ""], ["Zhao", "Zijia", ""], ["Sun", "Mingzhen", ""], ["Wang", "Weining", ""], ["Lu", "Hanqing", ""], ["Zhou", "Shiyu", ""], ["Zhang", "Jiajun", ""], ["Wang", "Jinqiao", ""]]}, {"id": "2107.00254", "submitter": "Mingkui Tan", "authors": "Shuaicheng Niu, Jiaxiang Wu, Guanghui Xu, Yifan Zhang, Yong Guo,\n  Peilin Zhao, Peng Wang, Mingkui Tan", "title": "AdaXpert: Adapting Neural Architecture for Growing Data", "comments": "accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications, data often come in a growing manner, where the\ndata volume and the number of classes may increase dynamically. This will bring\na critical challenge for learning: given the increasing data volume or the\nnumber of classes, one has to instantaneously adjust the neural model capacity\nto obtain promising performance. Existing methods either ignore the growing\nnature of data or seek to independently search an optimal architecture for a\ngiven dataset, and thus are incapable of promptly adjusting the architectures\nfor the changed data. To address this, we present a neural architecture\nadaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust\nprevious architectures on the growing data. Specifically, we introduce an\narchitecture adjuster to generate a suitable architecture for each data\nsnapshot, based on the previous architecture and the different extent between\ncurrent and previous data distributions. Furthermore, we propose an adaptation\ncondition to determine the necessity of adjustment, thereby avoiding\nunnecessary and time-consuming adjustments. Extensive experiments on two growth\nscenarios (increasing data volume and number of classes) demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 07:22:05 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Niu", "Shuaicheng", ""], ["Wu", "Jiaxiang", ""], ["Xu", "Guanghui", ""], ["Zhang", "Yifan", ""], ["Guo", "Yong", ""], ["Zhao", "Peilin", ""], ["Wang", "Peng", ""], ["Tan", "Mingkui", ""]]}, {"id": "2107.00272", "submitter": "David Ahmedt-Aristizabal", "authors": "David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton\n  Fookes, Lars Petersson", "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the remarkable success of representation learning for prediction\nproblems, we have witnessed a rapid expansion of the use of machine learning\nand deep learning for the analysis of digital pathology and biopsy image\npatches. However, traditional learning over patch-wise features using\nconvolutional neural networks limits the model when attempting to capture\nglobal contextual information. The phenotypical and topological distribution of\nconstituent histological entities play a critical role in tissue diagnosis. As\nsuch, graph data representations and deep learning have attracted significant\nattention for encoding tissue representations, and capturing intra- and inter-\nentity level interactions. In this review, we provide a conceptual grounding of\ngraph-based deep learning and discuss its current success for tumor\nlocalization and classification, tumor invasion and staging, image retrieval,\nand survival prediction. We provide an overview of these methods in a\nsystematic manner organized by the graph representation of the input image\nincluding whole slide images and tissue microarrays. We also outline the\nlimitations of existing techniques, and suggest potential future advances in\nthis domain.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 07:50:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ahmedt-Aristizabal", "David", ""], ["Armin", "Mohammad Ali", ""], ["Denman", "Simon", ""], ["Fookes", "Clinton", ""], ["Petersson", "Lars", ""]]}, {"id": "2107.00283", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Steven A. Hicks, P{\\aa}l Halvorsen, Michael A.\n  Riegler", "title": "DivergentNets: Medical Image Segmentation by Network Ensemble", "comments": "the winning model of the segmentation generalization challenge at\n  EndoCV 2021", "journal-ref": "Proceedings of the 3rd International Workshop and Challenge on\n  Computer Vision in Endoscopy (EndoCV 2021) colocated with with the 17th IEEE\n  International Symposium on Biomedical Imaging (ISBI 2021)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection of colon polyps has become a trending topic in the intersecting\nfields of machine learning and gastrointestinal endoscopy. The focus has mainly\nbeen on per-frame classification. More recently, polyp segmentation has gained\nattention in the medical community. Segmentation has the advantage of being\nmore accurate than per-frame classification or object detection as it can show\nthe affected area in greater detail. For our contribution to the EndoCV 2021\nsegmentation challenge, we propose two separate approaches. First, a\nsegmentation model named TriUNet composed of three separate UNet models.\nSecond, we combine TriUNet with an ensemble of well-known segmentation models,\nnamely UNet++, FPN, DeepLabv3, and DeepLabv3+, into a model called\nDivergentNets to produce more generalizable medical image segmentation masks.\nIn addition, we propose a modified Dice loss that calculates loss only for a\nsingle class when performing multiclass segmentation, forcing the model to\nfocus on what is most important. Overall, the proposed methods achieved the\nbest average scores for each respective round in the challenge, with TriUNet\nbeing the winning model in Round I and DivergentNets being the winning model in\nRound II of the segmentation generalization challenge at EndoCV 2021. The\nimplementation of our approach is made publicly available on GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:15:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Thambawita", "Vajira", ""], ["Hicks", "Steven A.", ""], ["Halvorsen", "P\u00e5l", ""], ["Riegler", "Michael A.", ""]]}, {"id": "2107.00285", "submitter": "Xin Liu", "authors": "Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, Guoying\n  Zhaoz?", "title": "iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding\n  and Emotion Analysis", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new dataset for the emotional artificial intelligence\nresearch: identity-free video dataset for Micro-Gesture Understanding and\nEmotion analysis (iMiGUE). Different from existing public datasets, iMiGUE\nfocuses on nonverbal body gestures without using any identity information,\nwhile the predominant researches of emotion analysis concern sensitive\nbiometric data, like face and speech. Most importantly, iMiGUE focuses on\nmicro-gestures, i.e., unintentional behaviors driven by inner feelings, which\nare different from ordinary scope of gestures from other gesture datasets which\nare mostly intentionally performed for illustrative purposes. Furthermore,\niMiGUE is designed to evaluate the ability of models to analyze the emotional\nstates by integrating information of recognized micro-gesture, rather than just\nrecognizing prototypes in the sequences separately (or isolatedly). This is\nbecause the real need for emotion AI is to understand the emotional states\nbehind gestures in a holistic way. Moreover, to counter for the challenge of\nimbalanced sample distribution of this dataset, an unsupervised learning method\nis proposed to capture latent representations from the micro-gesture sequences\nthemselves. We systematically investigate representative methods on this\ndataset, and comprehensive experimental results reveal several interesting\ninsights from the iMiGUE, e.g., micro-gesture-based analysis can promote\nemotion understanding. We confirm that the new iMiGUE dataset could advance\nstudies of micro-gesture and emotion AI.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:15:14 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "Xin", ""], ["Shi", "Henglin", ""], ["Chen", "Haoyu", ""], ["Yu", "Zitong", ""], ["Li", "Xiaobai", ""], ["Zhaoz?", "Guoying", ""]]}, {"id": "2107.00296", "submitter": "Yuhao Niu", "authors": "Yuhao Niu, Lin Gu, Yitian Zhao, Feng Lu", "title": "Explainable Diabetic Retinopathy Detection and Retinal Image Generation", "comments": "Code is available at https://github.com/zzdyyy/Patho-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain diseases, most of them give few\nexplanations on how to make predictions. Inspired by Koch's Postulates, the\nfoundation in evidence-based medicine (EBM) to identify the pathogen, we\npropose to exploit the interpretability of deep learning application in medical\ndiagnosis. By determining and isolating the neuron activation patterns on which\ndiabetic retinopathy (DR) detector relies to make decisions, we demonstrate the\ndirect relation between the isolated neuron activation and lesions for a\npathological explanation. To be specific, we first define novel pathological\ndescriptors using activated neurons of the DR detector to encode both spatial\nand appearance information of lesions. Then, to visualize the symptom encoded\nin the descriptor, we propose Patho-GAN, a new network to synthesize medically\nplausible retinal images. By manipulating these descriptors, we could even\narbitrarily control the position, quantity, and categories of generated\nlesions. We also show that our synthesized images carry the symptoms directly\nrelated to diabetic retinopathy diagnosis. Our generated images are both\nqualitatively and quantitatively superior to the ones by previous methods.\nBesides, compared to existing methods that take hours to generate an image, our\nsecond level speed endows the potential to be an effective solution for data\naugmentation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 08:30:04 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Niu", "Yuhao", ""], ["Gu", "Lin", ""], ["Zhao", "Yitian", ""], ["Lu", "Feng", ""]]}, {"id": "2107.00315", "submitter": "Neeraj Varshney", "authors": "Neeraj Varshney, Swaroop Mishra, Chitta Baral", "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP\n  Systems", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:08:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Varshney", "Neeraj", ""], ["Mishra", "Swaroop", ""], ["Baral", "Chitta", ""]]}, {"id": "2107.00327", "submitter": "Ming Zhang", "authors": "Ming Zhang, Xuefei Zhe, Hong Yan", "title": "Orthonormal Product Quantization Network for Scalable Face Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, deep hashing with Hamming distance metric has drawn increasing\nattention for face image retrieval tasks. However, its counterpart deep\nquantization methods, which learn binary code representations with\ndictionary-related distance metrics, have seldom been explored for the task.\nThis paper makes the first attempt to integrate product quantization into an\nend-to-end deep learning framework for face image retrieval. Unlike prior deep\nquantization methods where the codewords for quantization are learned from\ndata, we propose a novel scheme using predefined orthonormal vectors as\ncodewords, which aims to enhance the quantization informativeness and reduce\nthe codewords' redundancy. To make the most of the discriminative information,\nwe design a tailored loss function that maximizes the identity discriminability\nin each quantization subspace for both the quantized and the original features.\nFurthermore, an entropy-based regularization term is imposed to reduce the\nquantization error. We conduct experiments on three commonly-used datasets\nunder the settings of both single-domain and cross-domain retrieval. It shows\nthat the proposed method outperforms all the compared deep hashing/quantization\nmethods under both settings with significant superiority. The proposed\ncodewords scheme consistently improves both regular model performance and model\ngeneralization ability, verifying the importance of codewords' distribution for\nthe quantization quality. Besides, our model's better generalization ability\nthan deep hashing models indicates that it is more suitable for scalable face\nimage retrieval tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:30:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zhang", "Ming", ""], ["Zhe", "Xuefei", ""], ["Yan", "Hong", ""]]}, {"id": "2107.00328", "submitter": "Shurun Wang", "authors": "Shurun Wang, Zhao Wang, Shiqi Wang, Yan Ye", "title": "End-to-end Compression Towards Machine Vision: Network Architecture\n  Design and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The research of visual signal compression has a long history. Fueled by deep\nlearning, exciting progress has been made recently. Despite achieving better\ncompression performance, existing end-to-end compression algorithms are still\ndesigned towards better signal quality in terms of rate-distortion\noptimization. In this paper, we show that the design and optimization of\nnetwork architecture could be further improved for compression towards machine\nvision. We propose an inverted bottleneck structure for end-to-end compression\ntowards machine vision, which specifically accounts for efficient\nrepresentation of the semantic information. Moreover, we quest the capability\nof optimization by incorporating the analytics accuracy into the optimization\nprocess, and the optimality is further explored with generalized rate-accuracy\noptimization in an iterative manner. We use object detection as a showcase for\nend-to-end compression towards machine vision, and extensive experiments show\nthat the proposed scheme achieves significant BD-rate savings in terms of\nanalysis performance. Moreover, the promise of the scheme is also demonstrated\nwith strong generalization capability towards other machine vision tasks, due\nto the enabling of signal-level reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:36:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Shurun", ""], ["Wang", "Zhao", ""], ["Wang", "Shiqi", ""], ["Ye", "Yan", ""]]}, {"id": "2107.00337", "submitter": "Chiara Plizzari", "authors": "Chiara Plizzari, Mirco Planamente, Emanuele Alberti, Barbara Caputo", "title": "PoliTO-IIT Submission to the EPIC-KITCHENS-100 Unsupervised Domain\n  Adaptation Challenge for Action Recognition", "comments": "3rd place in the 2021 EPIC-KITCHENS-100 Unsupervised Domain\n  Adaptation Challenge for Action Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we describe the technical details of our submission to the\nEPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action\nRecognition. To tackle the domain-shift which exists under the UDA setting, we\nfirst exploited a recent Domain Generalization (DG) technique, called Relative\nNorm Alignment (RNA). It consists in designing a model able to generalize well\nto any unseen domain, regardless of the possibility to access target data at\ntraining time. Then, in a second phase, we extended the approach to work on\nunlabelled target data, allowing the model to adapt to the target distribution\nin an unsupervised fashion. For this purpose, we included in our framework\nexisting UDA algorithms, such as Temporal Attentive Adversarial Adaptation\nNetwork (TA3N), jointly with new multi-stream consistency losses, namely\nTemporal Hard Norm Alignment (T-HNA) and Min-Entropy Consistency (MEC). Our\nsubmission (entry 'plnet') is visible on the leaderboard and it achieved the\n1st position for 'verb', and the 3rd position for both 'noun' and 'action'.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 10:02:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Plizzari", "Chiara", ""], ["Planamente", "Mirco", ""], ["Alberti", "Emanuele", ""], ["Caputo", "Barbara", ""]]}, {"id": "2107.00346", "submitter": "Kailun Yang", "authors": "Kunyu Peng, Juncong Fei, Kailun Yang, Alina Roitberg, Jiaming Zhang,\n  Frank Bieder, Philipp Heidenreich, Christoph Stiller, Rainer Stiefelhagen", "title": "MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense\n  Top-View Understanding", "comments": "14 pages, 7 figures, 4 tables. Code will be made publicly available\n  at https://github.com/KPeng9510/MASS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of all automated driving systems is the ability to sense the\nsurroundings, e.g., through semantic segmentation of LiDAR sequences, which\nexperienced a remarkable progress due to the release of large datasets such as\nSemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse\nsegmentation of the LiDAR input, dense output masks provide self-driving cars\nwith almost complete environment information. In this paper, we introduce MASS\n- a Multi-Attentional Semantic Segmentation model specifically built for dense\ntop-view understanding of the driving scenes. Our framework operates on pillar-\nand occupancy features and comprises three attention-based building blocks: (1)\na keypoint-driven graph attention, (2) an LSTM-based attention computed from a\nvector embedding of the spatial input, and (3) a pillar-based attention,\nresulting in a dense 360-degree segmentation mask. With extensive experiments\non both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the\neffectiveness of our model, outperforming the state of the art by 19.0% on\nSemanticKITTI and reaching 32.7% in mIoU on nuScenes-LidarSeg, where MASS is\nthe first work addressing the dense segmentation task. Furthermore, our\nmulti-attention model is shown to be very effective for 3D object detection\nvalidated on the KITTI-3D dataset, showcasing its high generalizability to\nother tasks related to 3D vision.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 10:19:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Peng", "Kunyu", ""], ["Fei", "Juncong", ""], ["Yang", "Kailun", ""], ["Roitberg", "Alina", ""], ["Zhang", "Jiaming", ""], ["Bieder", "Frank", ""], ["Heidenreich", "Philipp", ""], ["Stiller", "Christoph", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.00358", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li, Xialei Liu, Hakan Bilen", "title": "Improving Task Adaptation for Cross-domain Few-shot Learning", "comments": "Code will be available at https://github.com/VICO-UoE/URL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we look at the problem of cross-domain few-shot classification\nthat aims to learn a classifier from previously unseen classes and domains with\nfew labeled samples. We study several strategies including various adapter\ntopologies and operations in terms of their performance and efficiency that can\nbe easily attached to existing methods with different meta-training strategies\nand adapt them for a given task during meta-test phase. We show that parametric\nadapters attached to convolutional layers with residual connections performs\nthe best, and significantly improves the performance of the state-of-the-art\nmodels in the Meta-Dataset benchmark with minor additional cost. Our code will\nbe available at https://github.com/VICO-UoE/URL.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 10:47:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Wei-Hong", ""], ["Liu", "Xialei", ""], ["Bilen", "Hakan", ""]]}, {"id": "2107.00360", "submitter": "Marco Huber", "authors": "Nina Schaaf, Omar de Mitri, Hang Beom Kim, Alexander Windberger, Marco\n  F. Huber", "title": "Towards Measuring Bias in Image Classification", "comments": "Accepted for publication at the 30th International Conference on\n  Artificial Neural Networks (ICANN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have become de fact state-of-the-art for\nthe main computer vision tasks. However, due to the complex underlying\nstructure their decisions are hard to understand which limits their use in some\ncontext of the industrial world. A common and hard to detect challenge in\nmachine learning (ML) tasks is data bias. In this work, we present a systematic\napproach to uncover data bias by means of attribution maps. For this purpose,\nfirst an artificial dataset with a known bias is created and used to train\nintentionally biased CNNs. The networks' decisions are then inspected using\nattribution maps. Finally, meaningful metrics are used to measure the\nattribution maps' representativeness with respect to the known bias. The\nproposed study shows that some attribution map techniques highlight the\npresence of bias in the data better than others and metrics can support the\nidentification of bias.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 10:50:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Schaaf", "Nina", ""], ["de Mitri", "Omar", ""], ["Kim", "Hang Beom", ""], ["Windberger", "Alexander", ""], ["Huber", "Marco F.", ""]]}, {"id": "2107.00362", "submitter": "Claudio Piciarelli", "authors": "Claudio Piciarelli and Gian Luca Foresti", "title": "Drone swarm patrolling with uneven coverage requirements", "comments": "This paper has been published on IET Computer Vision. Please cite it\n  accordingly (see journal reference below)", "journal-ref": "IET Computer Vision, 14: 452-461 (2020)", "doi": "10.1049/iet-cvi.2019.0963", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Swarms of drones are being more and more used in many practical scenarios,\nsuch as surveillance, environmental monitoring, search and rescue in\nhardly-accessible areas, etc.. While a single drone can be guided by a human\noperator, the deployment of a swarm of multiple drones requires proper\nalgorithms for automatic task-oriented control. In this paper, we focus on\nvisual coverage optimization with drone-mounted camera sensors. In particular,\nwe consider the specific case in which the coverage requirements are uneven,\nmeaning that different parts of the environment have different coverage\npriorities. We model these coverage requirements with relevance maps and\npropose a deep reinforcement learning algorithm to guide the swarm. The paper\nfirst defines a proper learning model for a single drone, and then extends it\nto the case of multiple drones both with greedy and cooperative strategies.\nExperimental results show the performance of the proposed method, also compared\nwith a standard patrolling algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 10:58:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Piciarelli", "Claudio", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "2107.00372", "submitter": "Jianing Qiu", "authors": "Jianing Qiu, Frank P.-W. Lo, Xiao Gu, Modou L. Jobarteh, Wenyan Jia,\n  Tom Baranowski, Matilda Steiner-Asiedu, Alex K. Anderson, Megan A McCrory,\n  Edward Sazonov, Mingui Sun, Gary Frost, Benny Lo", "title": "Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake\n  Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Camera-based passive dietary intake monitoring is able to continuously\ncapture the eating episodes of a subject, recording rich visual information,\nsuch as the type and volume of food being consumed, as well as the eating\nbehaviours of the subject. However, there currently is no method that is able\nto incorporate these visual clues and provide a comprehensive context of\ndietary intake from passive recording (e.g., is the subject sharing food with\nothers, what food the subject is eating, and how much food is left in the\nbowl). On the other hand, privacy is a major concern while egocentric wearable\ncameras are used for capturing. In this paper, we propose a privacy-preserved\nsecure solution (i.e., egocentric image captioning) for dietary assessment with\npassive monitoring, which unifies food recognition, volume estimation, and\nscene understanding. By converting images into rich text descriptions,\nnutritionists can assess individual dietary intake based on the captions\ninstead of the original images, reducing the risk of privacy leakage from\nimages. To this end, an egocentric dietary image captioning dataset has been\nbuilt, which consists of in-the-wild images captured by head-worn and\nchest-worn cameras in field studies in Ghana. A novel transformer-based\narchitecture is designed to caption egocentric dietary images. Comprehensive\nexperiments have been conducted to evaluate the effectiveness and to justify\nthe design of the proposed architecture for egocentric dietary image\ncaptioning. To the best of our knowledge, this is the first work that applies\nimage captioning to dietary intake assessment in real life settings.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:16:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Qiu", "Jianing", ""], ["Lo", "Frank P. -W.", ""], ["Gu", "Xiao", ""], ["Jobarteh", "Modou L.", ""], ["Jia", "Wenyan", ""], ["Baranowski", "Tom", ""], ["Steiner-Asiedu", "Matilda", ""], ["Anderson", "Alex K.", ""], ["McCrory", "Megan A", ""], ["Sazonov", "Edward", ""], ["Sun", "Mingui", ""], ["Frost", "Gary", ""], ["Lo", "Benny", ""]]}, {"id": "2107.00382", "submitter": "Lin Li", "authors": "Lin Li, Xin Kong, Xiangrui Zhao, Tianxin Huang and Yong Liu", "title": "SSC: Semantic Scan Context for Large-Scale Place Recognition", "comments": "8 pages, Accepted by IROS-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Place recognition gives a SLAM system the ability to correct cumulative\nerrors. Unlike images that contain rich texture features, point clouds are\nalmost pure geometric information which makes place recognition based on point\nclouds challenging. Existing works usually encode low-level features such as\ncoordinate, normal, reflection intensity, etc., as local or global descriptors\nto represent scenes. Besides, they often ignore the translation between point\nclouds when matching descriptors. Different from most existing methods, we\nexplore the use of high-level features, namely semantics, to improve the\ndescriptor's representation ability. Also, when matching descriptors, we try to\ncorrect the translation between point clouds to improve accuracy. Concretely,\nwe propose a novel global descriptor, Semantic Scan Context, which explores\nsemantic information to represent scenes more effectively. We also present a\ntwo-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align\nthe point cloud to improve matching performance. Our experiments on the KITTI\ndataset show that our approach outperforms the state-of-the-art methods with a\nlarge margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:51:19 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 07:47:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Lin", ""], ["Kong", "Xin", ""], ["Zhao", "Xiangrui", ""], ["Huang", "Tianxin", ""], ["Liu", "Yong", ""]]}, {"id": "2107.00395", "submitter": "Baotian Hu", "authors": "Yunxin Li, Yu Zhao, Baotian Hu, Qingcai Chen, Yang Xiang, Xiaolong\n  Wang, Yuxin Ding, Lin Ma", "title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character\n  with its Glyph", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:14:05 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Yunxin", ""], ["Zhao", "Yu", ""], ["Hu", "Baotian", ""], ["Chen", "Qingcai", ""], ["Xiang", "Yang", ""], ["Wang", "Xiaolong", ""], ["Ding", "Yuxin", ""], ["Ma", "Lin", ""]]}, {"id": "2107.00396", "submitter": "Konstantin Bulatov", "authors": "Konstantin Bulatov, Ekaterina Emelianova, Daniil Tropin, Natalya\n  Skoryukina, Yulia Chernyshova, Alexander Sheshkus, Sergey Usilin, Zuheng\n  Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman, Vladimir V. Arlazarov", "title": "MIDV-2020: A Comprehensive Benchmark Dataset for Identity Document\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity documents recognition is an important sub-field of document\nanalysis, which deals with tasks of robust document detection, type\nidentification, text fields recognition, as well as identity fraud prevention\nand document authenticity validation given photos, scans, or video frames of an\nidentity document capture. Significant amount of research has been published on\nthis topic in recent years, however a chief difficulty for such research is\nscarcity of datasets, due to the subject matter being protected by security\nrequirements. A few datasets of identity documents which are available lack\ndiversity of document types, capturing conditions, or variability of document\nfield values. In addition, the published datasets were typically designed only\nfor a subset of document recognition problems, not for a complex identity\ndocument analysis. In this paper, we present a dataset MIDV-2020 which consists\nof 1000 video clips, 2000 scanned images, and 1000 photos of 1000 unique mock\nidentity documents, each with unique text field values and unique artificially\ngenerated faces, with rich annotation. For the presented benchmark dataset\nbaselines are provided for such tasks as document location and identification,\ntext fields recognition, and face detection. With 72409 annotated images in\ntotal, to the date of publication the proposed dataset is the largest publicly\navailable identity documents dataset with variable artificially generated data,\nand we believe that it will prove invaluable for advancement of the field of\ndocument analysis and recognition. The dataset is available for download at\nftp://smartengines.com/midv-2020 and http://l3i-share.univ-lr.fr .\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:14:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bulatov", "Konstantin", ""], ["Emelianova", "Ekaterina", ""], ["Tropin", "Daniil", ""], ["Skoryukina", "Natalya", ""], ["Chernyshova", "Yulia", ""], ["Sheshkus", "Alexander", ""], ["Usilin", "Sergey", ""], ["Ming", "Zuheng", ""], ["Burie", "Jean-Christophe", ""], ["Luqman", "Muhammad Muzzamil", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "2107.00400", "submitter": "Dat Nguyen Thanh", "authors": "Dat Thanh Nguyen, Maurice Quach, Giuseppe Valenzise, Pierre Duhamel", "title": "Lossless Coding of Point Cloud Geometry using a Deep Generative Model", "comments": "This paper has been submitted to the IEEE Transactions on Circuits\n  and Systems for Video Technology (TCSVT). arXiv admin note: text overlap with\n  arXiv:2011.14700", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a lossless point cloud (PC) geometry compression method\nthat uses neural networks to estimate the probability distribution of voxel\noccupancy. First, to take into account the PC sparsity, our method adaptively\npartitions a point cloud into multiple voxel block sizes. This partitioning is\nsignalled via an octree. Second, we employ a deep auto-regressive generative\nmodel to estimate the occupancy probability of each voxel given the previously\nencoded ones. We then employ the estimated probabilities to code efficiently a\nblock using a context-based arithmetic coder. Our context has variable size and\ncan expand beyond the current block to learn more accurate probabilities. We\nalso consider using data augmentation techniques to increase the generalization\ncapability of the learned probability models, in particular in the presence of\nnoise and lower-density point clouds. Experimental evaluation, performed on a\nvariety of point clouds from four different datasets and with diverse\ncharacteristics, demonstrates that our method reduces significantly (by up to\n30%) the rate for lossless coding compared to the state-of-the-art MPEG codec.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:20:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nguyen", "Dat Thanh", ""], ["Quach", "Maurice", ""], ["Valenzise", "Giuseppe", ""], ["Duhamel", "Pierre", ""]]}, {"id": "2107.00415", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio and Giacomo Pira and Maurizio Martina and Guido\n  Masera and Muhammad Shafique", "title": "DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking\n  Neural Networks", "comments": "Accepted for publication at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs), despite being energy-efficient when\nimplemented on neuromorphic hardware and coupled with event-based Dynamic\nVision Sensors (DVS), are vulnerable to security threats, such as adversarial\nattacks, i.e., small perturbations added to the input for inducing a\nmisclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet\nefficient adversarial attack methodologies targeted to perturb the event\nsequences that compose the input of the SNNs. First, we show that noise filters\nfor DVS can be used as defense mechanisms against adversarial attacks.\nAfterwards, we implement several attacks and test them in the presence of two\ntypes of noise filters for DVS cameras. The experimental results show that the\nfilters can only partially defend the SNNs against our proposed DVS-Attacks.\nUsing the best settings for the noise filters, our proposed Mask Filter-Aware\nDash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset\nand by more than 65% on the MNIST dataset, compared to the original clean\nframes. The source code of all the proposed DVS-Attacks and noise filters is\nreleased at https://github.com/albertomarchisio/DVS-Attacks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:56:36 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Marchisio", "Alberto", ""], ["Pira", "Giacomo", ""], ["Martina", "Maurizio", ""], ["Masera", "Guido", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2107.00418", "submitter": "Yong Oh Lee Dr", "authors": "Sungho Suh, Sojeong Cheon, Wonseo Choi, Yeon Woong Chung, Won-Kyung\n  Cho, Ji-Sun Paik, Sung Eun Kim, Dong-Jin Chang, Yong Oh Lee", "title": "Supervised Segmentation with Domain Adaptation for Small Sampled Orbital\n  CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been widely used for medical image analysis.\nHowever, the lack of access a to large-scale annotated dataset poses a great\nchallenge, especially in the case of rare diseases, or new domains for the\nresearch society. Transfer of pre-trained features, from the relatively large\ndataset is a considerable solution. In this paper, we have explored supervised\nsegmentation using domain adaptation for optic nerve and orbital tumor, when\nonly small sampled CT images are given. Even the lung image database consortium\nimage collection (LIDC-IDRI) is a cross-domain to orbital CT, but the proposed\ndomain adaptation method improved the performance of attention U-Net for the\nsegmentation in public optic nerve dataset and our clinical orbital tumor\ndataset. The code and dataset are available at https://github.com/cmcbigdata.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:00:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Suh", "Sungho", ""], ["Cheon", "Sojeong", ""], ["Choi", "Wonseo", ""], ["Chung", "Yeon Woong", ""], ["Cho", "Won-Kyung", ""], ["Paik", "Ji-Sun", ""], ["Kim", "Sung Eun", ""], ["Chang", "Dong-Jin", ""], ["Lee", "Yong Oh", ""]]}, {"id": "2107.00420", "submitter": "Tingting Liang", "authors": "Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei\n  Chu, Jingdong Chen, Haibin Ling", "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. In this paper, we propose a novel and flexible\nbackbone framework, namely CBNetV2, to construct high-performance detectors\nusing existing open-sourced pre-trained backbones under the pre-training\nfine-tuning paradigm. In particular, CBNetV2 architecture groups multiple\nidentical backbones, which are connected through composite connections.\nSpecifically, it integrates the high- and low-level features of multiple\nbackbone networks and gradually expands the receptive field to more efficiently\nperform object detection. We also propose a better training strategy with\nassistant supervision for CBNet-based detectors. Without additional\npre-training of the composite backbone, CBNetV2 can be adapted to various\nbackbones (CNN-based vs. Transformer-based) and head designs of most mainstream\ndetectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based).\nExperiments provide strong evidence that, compared with simply increasing the\ndepth and width of the network, CBNetV2 introduces a more efficient, effective,\nand resource-friendly way to build high-performance backbone networks.\nParticularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO\ntest-dev under the single-model and single-scale testing protocol, which is\nsignificantly better than the state-of-the-art result (57.7% box AP and 50.2%\nmask AP) achieved by Swin-L, while the training schedule is reduced by\n6$\\times$. With multi-scale testing, we push the current best single model\nresult to a new record of 60.1% box AP and 52.3% mask AP without using extra\ntraining data. Code is available at https://github.com/VDIGPKU/CBNetV2.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:05:11 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 06:44:58 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 16:42:55 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 09:12:05 GMT"}, {"version": "v5", "created": "Sat, 24 Jul 2021 16:50:16 GMT"}, {"version": "v6", "created": "Thu, 29 Jul 2021 03:28:29 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liang", "Tingting", ""], ["Chu", "Xiaojie", ""], ["Liu", "Yudong", ""], ["Wang", "Yongtao", ""], ["Tang", "Zhi", ""], ["Chu", "Wei", ""], ["Chen", "Jingdong", ""], ["Ling", "Haibin", ""]]}, {"id": "2107.00422", "submitter": "Stefan Becker", "authors": "Stefan Becker and Ronny Hug and Wolfgang H\\\"ubner and Michael Arens\n  and Brendan T. Morris", "title": "Generating Synthetic Training Data for Deep Learning-Based UAV\n  Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based models, such as recurrent neural networks (RNNs), have\nbeen applied to various sequence learning tasks with great success. Following\nthis, these models are increasingly replacing classic approaches in object\ntracking applications for motion prediction. On the one hand, these models can\ncapture complex object dynamics with less modeling required, but on the other\nhand, they depend on a large amount of training data for parameter tuning.\nTowards this end, we present an approach for generating synthetic trajectory\ndata of unmanned-aerial-vehicles (UAVs) in image space. Since UAVs, or rather\nquadrotors are dynamical systems, they can not follow arbitrary trajectories.\nWith the prerequisite that UAV trajectories fulfill a smoothness criterion\ncorresponding to a minimal change of higher-order motion, methods for planning\naggressive quadrotors flights can be utilized to generate optimal trajectories\nthrough a sequence of 3D waypoints. By projecting these maneuver trajectories,\nwhich are suitable for controlling quadrotors, to image space, a versatile\ntrajectory data set is realized. To demonstrate the applicability of the\nsynthetic trajectory data, we show that an RNN-based prediction model solely\ntrained on the generated data can outperform classic reference models on a\nreal-world UAV tracking dataset. The evaluation is done on the publicly\navailable ANTI-UAV dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:08:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Becker", "Stefan", ""], ["Hug", "Ronny", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""], ["Morris", "Brendan T.", ""]]}, {"id": "2107.00430", "submitter": "Bo Liu", "authors": "Bo Liu, Shuang Deng, Qiulei Dong, Zhanyi Hu", "title": "Segmenting 3D Hybrid Scenes via Zero-Shot Learning", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work is to tackle the problem of point cloud semantic segmentation for\n3D hybrid scenes under the framework of zero-shot learning. Here by hybrid, we\nmean the scene consists of both seen-class and unseen-class 3D objects, a more\ngeneral and realistic setting in application. To our knowledge, this problem\nhas not been explored in the literature. To this end, we propose a network to\nsynthesize point features for various classes of objects by leveraging the\nsemantic features of both seen and unseen object classes, called PFNet. The\nproposed PFNet employs a GAN architecture to synthesize point features, where\nthe semantic relationship between seen-class and unseen-class features is\nconsolidated by adapting a new semantic regularizer, and the synthesized\nfeatures are used to train a classifier for predicting the labels of the\ntesting 3D scene points. Besides we also introduce two benchmarks for\nalgorithmic evaluation by re-organizing the public S3DIS and ScanNet datasets\nunder six different data splits. Experimental results on the two benchmarks\nvalidate our proposed method, and we hope our introduced two benchmarks and\nmethodology could be of help for more research on this new direction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:21:49 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 09:07:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Liu", "Bo", ""], ["Deng", "Shuang", ""], ["Dong", "Qiulei", ""], ["Hu", "Zhanyi", ""]]}, {"id": "2107.00434", "submitter": "Zicong Fan", "authors": "Zicong Fan, Adrian Spurr, Muhammed Kocabas, Siyu Tang, Michael J.\n  Black, Otmar Hilliges", "title": "Learning to Disambiguate Strongly Interacting Hands via Probabilistic\n  Per-pixel Part Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In natural conversation and interaction, our hands often overlap or are in\ncontact with each other. Due to the homogeneous appearance of hands, this makes\nestimating the 3D pose of interacting hands from images difficult. In this\npaper we demonstrate that self-similarity, and the resulting ambiguities in\nassigning pixel observations to the respective hands and their parts, is a\nmajor cause of the final 3D pose error. Motivated by this insight, we propose\nDIGIT, a novel method for estimating the 3D poses of two interacting hands from\na single monocular image. The method consists of two interwoven branches that\nprocess the input imagery into a per-pixel semantic part segmentation mask and\na visual feature volume. In contrast to prior work, we do not decouple the\nsegmentation from the pose estimation stage, but rather leverage the per-pixel\nprobabilities directly in the downstream pose estimation task. To do so, the\npart probabilities are merged with the visual features and processed via\nfully-convolutional layers. We experimentally show that the proposed approach\nachieves new state-of-the-art performance on the InterHand2.6M dataset for both\nsingle and interacting hands across all metrics. We provide detailed ablation\nstudies to demonstrate the efficacy of our method and to provide insights into\nhow the modelling of pixel ownership affects single and interacting hand pose\nestimation. Our code will be released for research purposes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:28:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fan", "Zicong", ""], ["Spurr", "Adrian", ""], ["Kocabas", "Muhammed", ""], ["Tang", "Siyu", ""], ["Black", "Michael J.", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2107.00436", "submitter": "Erik Larsen", "authors": "Erik Larsen, David Noever, Korey MacVittie and John Lilly", "title": "Overhead-MNIST: Machine Learning Baselines for Image Classification", "comments": "6 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Twenty-three machine learning algorithms were trained then scored to\nestablish baseline comparison metrics and to select an image classification\nalgorithm worthy of embedding into mission-critical satellite imaging systems.\nThe Overhead-MNIST dataset is a collection of satellite images similar in style\nto the ubiquitous MNIST hand-written digits found in the machine learning\nliterature. The CatBoost classifier, Light Gradient Boosting Machine, and\nExtreme Gradient Boosting models produced the highest accuracies, Areas Under\nthe Curve (AUC), and F1 scores in a PyCaret general comparison. Separate\nevaluations showed that a deep convolutional architecture was the most\npromising. We present results for the overall best performing algorithm as a\nbaseline for edge deployability and future performance improvement: a\nconvolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen\ntest data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:30:39 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Larsen", "Erik", ""], ["Noever", "David", ""], ["MacVittie", "Korey", ""], ["Lilly", "John", ""]]}, {"id": "2107.00451", "submitter": "Raivo Koot", "authors": "Raivo Koot, Haiping Lu", "title": "VideoLightFormer: Lightweight Action Recognition using Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient video action recognition remains a challenging problem. One large\nmodel after another takes the place of the state-of-the-art on the Kinetics\ndataset, but real-world efficiency evaluations are often lacking. In this work,\nwe fill this gap and investigate the use of transformers for efficient action\nrecognition. We propose a novel, lightweight action recognition architecture,\nVideoLightFormer. In a factorized fashion, we carefully extend the 2D\nconvolutional Temporal Segment Network with transformers, while maintaining\nspatial and temporal video structure throughout the entire model. Existing\nmethods often resort to one of the two extremes, where they either apply huge\ntransformers to video features, or minimal transformers on highly pooled video\nfeatures. Our method differs from them by keeping the transformer models small,\nbut leveraging full spatiotemporal feature structure. We evaluate\nVideoLightFormer in a high-efficiency setting on the temporally-demanding\nEPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it\nachieves a better mix of efficiency and accuracy than existing state-of-the-art\nmodels, apart from the Temporal Shift Module on SSV2.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:55:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Koot", "Raivo", ""], ["Lu", "Haiping", ""]]}, {"id": "2107.00456", "submitter": "Xiaotian Lu", "authors": "Xiaotian Lu, Arseny Tolmachev, Tatsuya Yamamoto, Koh Takeuchi, Seiji\n  Okajima, Tomoyoshi Takebayashi, Koji Maruhashi, Hisashi Kashima", "title": "Crowdsourcing Evaluation of Saliency-based XAI Methods", "comments": "16 pages, 7 figures, 2 tables, Accepted for ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, \"Peek-a-boom\",\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 17:37:53 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lu", "Xiaotian", ""], ["Tolmachev", "Arseny", ""], ["Yamamoto", "Tatsuya", ""], ["Takeuchi", "Koh", ""], ["Okajima", "Seiji", ""], ["Takebayashi", "Tomoyoshi", ""], ["Maruhashi", "Koji", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2107.00462", "submitter": "Skylar Wurster", "authors": "Skylar W. Wurster, Han-Wei Shen, Hanqi Guo, Thomas Peterka, Mukund\n  Raj, and Jiayi Xu", "title": "Deep Hierarchical Super-Resolution for Scientific Data Reduction and\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for hierarchical super resolution (SR) using neural\nnetworks on an octree data representation. We train a hierarchy of neural\nnetworks, each capable of 2x upscaling in each spatial dimension between two\nlevels of detail, and use these networks in tandem to facilitate large scale\nfactor super resolution, scaling with the number of trained networks. We\nutilize these networks in a hierarchical super resolution algorithm that\nupscales multiresolution data to a uniform high resolution without introducing\nseam artifacts on octree node boundaries. We evaluate application of this\nalgorithm in a data reduction framework by dynamically downscaling input data\nto an octree-based data structure to represent the multiresolution data before\ncompressing for additional storage reduction. We demonstrate that our approach\navoids seam artifacts common to multiresolution data formats, and show how\nneural network super resolution assisted data reduction can preserve global\nfeatures better than compressors alone at the same compression ratios.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 18:32:11 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wurster", "Skylar W.", ""], ["Shen", "Han-Wei", ""], ["Guo", "Hanqi", ""], ["Peterka", "Thomas", ""], ["Raj", "Mukund", ""], ["Xu", "Jiayi", ""]]}, {"id": "2107.00471", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Pegah Salehi, Sajad Amouei Sheshkal, Steven A.\n  Hicks, Hugo L.Hammer, Sravanthi Parasa, Thomas de Lange, P{\\aa}l Halvorsen,\n  Michael A. Riegler", "title": "SinGAN-Seg: Synthetic Training Data Generation for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing medical data to find abnormalities is a time-consuming and costly\ntask, requiring tremendous efforts from medical experts. Therefore, Ai has\nbecome a popular tool for the automatic processing of medical data, acting as a\nsupportive tool for doctors. AI tools highly depend on data for training the\nmodels. However, there are several constraints to access to large amounts of\nmedical data to train machine learning algorithms in the medical domain, e.g.,\ndue to privacy concerns and the costly, time-consuming medical data annotation\nprocess. To address this, in this paper we present a novel synthetic data\ngeneration pipeline called SinGAN-Seg to produce synthetic medical data with\nthe corresponding annotated ground truth masks. We show that these synthetic\ndata generation pipelines can be used as an alternative to bypass privacy\nconcerns and as an alternative way to produce artificial segmentation datasets\nwith corresponding ground truth masks to avoid the tedious medical data\nannotation process. As a proof of concept, we used an open polyp segmentation\ndataset. By training UNet++ using both the real polyp segmentation dataset and\nthe corresponding synthetic dataset generated from the SinGAN-Seg pipeline, we\nshow that the synthetic data can achieve a very close performance to the real\ndata when the real segmentation datasets are large enough. In addition, we show\nthat synthetic data generated from the SinGAN-Seg pipeline improving the\nperformance of segmentation algorithms when the training dataset is very small.\nSince our SinGAN-Seg pipeline is applicable for any medical dataset, this\npipeline can be used with any other segmentation datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:34:34 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Thambawita", "Vajira", ""], ["Salehi", "Pegah", ""], ["Sheshkal", "Sajad Amouei", ""], ["Hicks", "Steven A.", ""], ["Hammer", "Hugo L.", ""], ["Parasa", "Sravanthi", ""], ["de Lange", "Thomas", ""], ["Halvorsen", "P\u00e5l", ""], ["Riegler", "Michael A.", ""]]}, {"id": "2107.00500", "submitter": "Xufeng Lin", "authors": "Xufeng Lin, Chang-Tsun Li, Victor Sanchez, Carsten Maple", "title": "On the detection-to-track association for online multi-object tracking", "comments": null, "journal-ref": "Pattern Recognition Letters 146 (2021) 200-207", "doi": "10.1016/j.patrec.2021.03.022", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:44:12 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lin", "Xufeng", ""], ["Li", "Chang-Tsun", ""], ["Sanchez", "Victor", ""], ["Maple", "Carsten", ""]]}, {"id": "2107.00544", "submitter": "Tariq Iqbal", "authors": "Mohammad Samin Yasar and Tariq Iqbal", "title": "Improving Human Motion Prediction Through Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is an essential component for enabling closer\nhuman-robot collaboration. The task of accurately predicting human motion is\nnon-trivial. It is compounded by the variability of human motion, both at a\nskeletal level due to the varying size of humans and at a motion level due to\nindividual movement's idiosyncrasies. These variables make it challenging for\nlearning algorithms to obtain a general representation that is robust to the\ndiverse spatio-temporal patterns of human motion. In this work, we propose a\nmodular sequence learning approach that allows end-to-end training while also\nhaving the flexibility of being fine-tuned. Our approach relies on the\ndiversity of training samples to first learn a robust representation, which can\nthen be fine-tuned in a continual learning setup to predict the motion of new\nsubjects. We evaluated the proposed approach by comparing its performance\nagainst state-of-the-art baselines. The results suggest that our approach\noutperforms other methods over all the evaluated temporal horizons, using a\nsmall amount of data for fine-tuning. The improved performance of our approach\nopens up the possibility of using continual learning for personalized and\nreliable motion prediction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:34:41 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yasar", "Mohammad Samin", ""], ["Iqbal", "Tariq", ""]]}, {"id": "2107.00559", "submitter": "Mohamed Amine Kerkouri", "authors": "Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Rachid\n  Harba", "title": "SALYPATH: A Deep-Based Architecture for visual attention prediction", "comments": "Accepted at ICIP, 5 pages, 2 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human vision is naturally more attracted by some regions within their field\nof view than others. This intrinsic selectivity mechanism, so-called visual\nattention, is influenced by both high- and low-level factors; such as the\nglobal environment (illumination, background texture, etc.), stimulus\ncharacteristics (color, intensity, orientation, etc.), and some prior visual\ninformation. Visual attention is useful for many computer vision applications\nsuch as image compression, recognition, and captioning. In this paper, we\npropose an end-to-end deep-based method, so-called SALYPATH (SALiencY and\nscanPATH), that efficiently predicts the scanpath of an image through features\nof a saliency model. The idea is predict the scanpath by exploiting the\ncapacity of a deep-based model to predict the saliency. The proposed method was\nevaluated through 2 well-known datasets. The results obtained showed the\nrelevance of the proposed framework comparing to state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:53:51 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kerkouri", "Mohamed Amine", ""], ["Tliba", "Marouane", ""], ["Chetouani", "Aladine", ""], ["Harba", "Rachid", ""]]}, {"id": "2107.00583", "submitter": "Reuben Dorent", "authors": "Reuben Dorent, Samuel Joutard, Jonathan Shapey, Aaron Kujawa, Marc\n  Modat, Sebastien Ourselin, Tom Vercauteren", "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation", "comments": "Early accept at MICCAI 2021 - code available at:\n  https://github.com/ReubenDo/InExtremIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce $\\textit{InExtremIS}$, a weakly supervised 3D approach to train\na deep image segmentation network using particularly weak train-time\nannotations: only 6 extreme clicks at the boundary of the objects of interest.\nOur fully-automatic method is trained end-to-end and does not require any\ntest-time annotations. From the extreme points, 3D bounding boxes are extracted\naround objects of interest. Then, deep geodesics connecting extreme points are\ngenerated to increase the amount of \"annotated\" voxels within the bounding\nboxes. Finally, a weakly supervised regularised loss derived from a Conditional\nRandom Field formulation is used to encourage prediction consistency over\nhomogeneous regions. Extensive experiments are performed on a large open\ndataset for Vestibular Schwannoma segmentation. $\\textit{InExtremIS}$ obtained\ncompetitive performance, approaching full supervision and outperforming\nsignificantly other weakly supervised techniques based on bounding boxes.\nMoreover, given a fixed annotation time budget, $\\textit{InExtremIS}$\noutperforms full supervision. Our code and data are available online.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:16:50 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dorent", "Reuben", ""], ["Joutard", "Samuel", ""], ["Shapey", "Jonathan", ""], ["Kujawa", "Aaron", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2107.00590", "submitter": "Rongjun Qin", "authors": "Hessah Albanwan, Rongjun Qin, Xiaohu Lu, Mao Li, Desheng Liu,\n  Jean-Michel Guldmann", "title": "3D Iterative Spatiotemporal Filtering for Classification of\n  Multitemporal Satellite Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current practice in land cover/land use change analysis relies heavily on\nthe individually classified maps of the multitemporal data set. Due to varying\nacquisition conditions (e.g., illumination, sensors, seasonal differences), the\nclassification maps yielded are often inconsistent through time for robust\nstatistical analysis. 3D geometric features have been shown to be stable for\nassessing differences across the temporal data set. Therefore, in this article\nwe investigate he use of a multitemporal orthophoto and digital surface model\nderived from satellite data for spatiotemporal classification. Our approach\nconsists of two major steps: generating per-class probability distribution maps\nusing the random-forest classifier with limited training samples, and making\nspatiotemporal inferences using an iterative 3D spatiotemporal filter operating\non per-class probability maps. Our experimental results demonstrate that the\nproposed methods can consistently improve the individual classification results\nby 2%-6% and thus can be an important postclassification refinement approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:26:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Albanwan", "Hessah", ""], ["Qin", "Rongjun", ""], ["Lu", "Xiaohu", ""], ["Li", "Mao", ""], ["Liu", "Desheng", ""], ["Guldmann", "Jean-Michel", ""]]}, {"id": "2107.00592", "submitter": "Rongjun Qin", "authors": "Changlin Xiao, Rongjun Qin, Xiao Xie, Xu Huang", "title": "Individual Tree Detection and Crown Delineation with 3D Information from\n  Multi-view Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual tree detection and crown delineation (ITDD) are critical in forest\ninventory management and remote sensing based forest surveys are largely\ncarried out through satellite images. However, most of these surveys only use\n2D spectral information which normally has not enough clues for ITDD. To fully\nexplore the satellite images, we propose a ITDD method using the orthophoto and\ndigital surface model (DSM) derived from the multi-view satellite data. Our\nalgorithm utilizes the top-hat morphological operation to efficiently extract\nthe local maxima from DSM as treetops, and then feed them to a modi-fied\nsuperpixel segmentation that combines both 2D and 3D information for tree crown\ndelineation. In subsequent steps, our method incorporates the biological\ncharacteristics of the crowns through plant allometric equation to falsify\npotential outliers. Experiments against manually marked tree plots on three\nrepresentative regions have demonstrated promising results - the best overall\ndetection accuracy can be 89%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:28:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Xiao", "Changlin", ""], ["Qin", "Rongjun", ""], ["Xie", "Xiao", ""], ["Huang", "Xu", ""]]}, {"id": "2107.00598", "submitter": "Rongjun Qin", "authors": "Xiao Ling, Xu Huang, Rongjun Qin", "title": "A Unified Framework of Bundle Adjustment and Feature Matching for\n  High-Resolution Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundle adjustment (BA) is a technique for refining sensor orientations of\nsatellite images, while adjustment accuracy is correlated with feature matching\nresults. Feature match-ing often contains high uncertainties in weak/repeat\ntextures, while BA results are helpful in reducing these uncertainties. To\ncompute more accurate orientations, this article incorpo-rates BA and feature\nmatching in a unified framework and formulates the union as the optimization of\na global energy function so that the solutions of the BA and feature matching\nare constrained with each other. To avoid a degeneracy in the optimization, we\npropose a comprised solution by breaking the optimization of the global energy\nfunction into two-step suboptimizations and compute the local minimums of each\nsuboptimization in an incremental manner. Experiments on multi-view\nhigh-resolution satellite images show that our proposed method outperforms\nstate-of-the-art orientation techniques with or without accurate least-squares\nmatching.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:40:25 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ling", "Xiao", ""], ["Huang", "Xu", ""], ["Qin", "Rongjun", ""]]}, {"id": "2107.00606", "submitter": "Francesco Salvetti", "authors": "Vittorio Mazzia, Simone Angarano, Francesco Salvetti, Federico\n  Angelini and Marcello Chiaberge", "title": "Action Transformer: A Self-Attention Model for Short-Time Human Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent, and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time\nshort-time human action recognition. Extensive experimentation on MPOSE2021\nwith our proposed methodology and several previous architectural solutions\nproves the effectiveness of the AcT model and poses the base for future work on\nHAR.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:53:16 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:33:48 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 09:11:17 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mazzia", "Vittorio", ""], ["Angarano", "Simone", ""], ["Salvetti", "Francesco", ""], ["Angelini", "Federico", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2107.00627", "submitter": "Junqing Huang", "authors": "Junqing Huang, Haihui Wang, Xuechao Wang, Michael Ruzhansky", "title": "Semi-Sparsity for Smoothing Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose an interesting semi-sparsity smoothing algorithm\nbased on a novel sparsity-inducing optimization framework. This method is\nderived from the multiple observations, that is, semi-sparsity prior knowledge\nis more universally applicable, especially in areas where sparsity is not fully\nadmitted, such as polynomial-smoothing surfaces. We illustrate that this\nsemi-sparsity can be identified into a generalized $L_0$-norm minimization in\nhigher-order gradient domains, thereby giving rise to a new \"feature-aware\"\nfiltering method with a powerful simultaneous-fitting ability in both sparse\nfeatures (singularities and sharpening edges) and non-sparse regions\n(polynomial-smoothing surfaces). Notice that a direct solver is always\nunavailable due to the non-convexity and combinatorial nature of $L_0$-norm\nminimization. Instead, we solve the model based on an efficient half-quadratic\nsplitting minimization with fast Fourier transforms (FFTs) for acceleration. We\nfinally demonstrate its versatility and many benefits to a series of\nsignal/image processing and computer vision applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:31:42 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 07:46:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Huang", "Junqing", ""], ["Wang", "Haihui", ""], ["Wang", "Xuechao", ""], ["Ruzhansky", "Michael", ""]]}, {"id": "2107.00637", "submitter": "Andrea Dittadi", "authors": "Andrea Dittadi, Samuele Papa, Michele De Vita, Bernhard Sch\\\"olkopf,\n  Ole Winther, Francesco Locatello", "title": "Generalization and Robustness Implications in Object-Centric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and learning\nefficiency of downstream tasks in scenes with multiple objects. In this paper,\nwe train state-of-the-art unsupervised models on five common multi-object\ndatasets and evaluate segmentation accuracy and downstream object property\nprediction. In addition, we study systematic generalization and robustness by\ninvestigating the settings where either single objects are out-of-distribution\n-- e.g., having unseen colors, textures, and shapes -- or global properties of\nthe scene are altered -- e.g., by occlusions, cropping, or increasing the\nnumber of objects. From our experimental study, we find object-centric\nrepresentations to be generally useful for downstream tasks and robust to\nshifts in the data distribution, especially if shifts affect single objects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:51:11 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dittadi", "Andrea", ""], ["Papa", "Samuele", ""], ["De Vita", "Michele", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Winther", "Ole", ""], ["Locatello", "Francesco", ""]]}, {"id": "2107.00641", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu\n  Yuan, Jianfeng Gao", "title": "Focal Self-attention for Local-Global Interactions in Vision\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Vision Transformer and its variants have shown great promise on\nvarious computer vision tasks. The ability of capturing short- and long-range\nvisual dependencies through self-attention is arguably the main source for the\nsuccess. But it also brings challenges due to quadratic computational overhead,\nespecially for the high-resolution vision tasks (e.g., object detection). In\nthis paper, we present focal self-attention, a new mechanism that incorporates\nboth fine-grained local and coarse-grained global interactions. Using this new\nmechanism, each token attends the closest surrounding tokens at fine\ngranularity but the tokens far away at coarse granularity, and thus can capture\nboth short- and long-range visual dependencies efficiently and effectively.\nWith focal self-attention, we propose a new variant of Vision Transformer\nmodels, called Focal Transformer, which achieves superior performance over the\nstate-of-the-art vision Transformers on a range of public image classification\nand object detection benchmarks. In particular, our Focal Transformer models\nwith a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8\nTop-1 accuracy, respectively, on ImageNet classification at 224x224 resolution.\nUsing Focal Transformers as the backbones, we obtain consistent and substantial\nimprovements over the current state-of-the-art Swin Transformers for 6\ndifferent object detection methods trained with standard 1x and 3x schedules.\nOur largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs\non COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation,\ncreating new SoTA on three of the most challenging computer vision tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:56:09 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yang", "Jianwei", ""], ["Li", "Chunyuan", ""], ["Zhang", "Pengchuan", ""], ["Dai", "Xiyang", ""], ["Xiao", "Bin", ""], ["Yuan", "Lu", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2107.00644", "submitter": "Nicklas Hansen", "authors": "Nicklas Hansen, Hao Su, Xiaolong Wang", "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under\n  Data Augmentation", "comments": "Code and videos are available at https://nicklashansen.github.io/SVEA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While agents trained by Reinforcement Learning (RL) can solve increasingly\nchallenging tasks directly from visual observations, generalizing learned\nskills to novel environments remains very challenging. Extensive use of data\naugmentation is a promising technique for improving generalization in RL, but\nit is often found to decrease sample efficiency and can even lead to\ndivergence. In this paper, we investigate causes of instability when using data\naugmentation in common off-policy RL algorithms. We identify two problems, both\nrooted in high-variance Q-targets. Based on our findings, we propose a simple\nyet effective technique for stabilizing this class of algorithms under\naugmentation. We perform extensive empirical evaluation of image-based RL using\nboth ConvNets and Vision Transformers (ViT) on a family of benchmarks based on\nDeepMind Control Suite, as well as in robotic manipulation tasks. Our method\ngreatly improves stability and sample efficiency of ConvNets under\naugmentation, and achieves generalization results competitive with\nstate-of-the-art methods for image-based RL. We further show that our method\nscales to RL with ViT-based architectures, and that data augmentation may be\nespecially important in this setting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:58:05 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hansen", "Nicklas", ""], ["Su", "Hao", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2107.00645", "submitter": "Yongming Rao", "authors": "Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, Jie Zhou", "title": "Global Filter Networks for Image Classification", "comments": "Project page: https://gfnet.ivg-research.xyz/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:58:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Rao", "Yongming", ""], ["Zhao", "Wenliang", ""], ["Zhu", "Zheng", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.00646", "submitter": "Yen-Chen Lin", "authors": "Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, Tsung-Yi Lin", "title": "Learning to See before Learning to Act: Visual Pre-training for\n  Manipulation", "comments": "Accepted to ICRA 2020. Porject page:\n  http://yenchenlin.me/vision2action/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does having visual priors (e.g. the ability to detect objects) facilitate\nlearning to perform vision-based manipulation (e.g. picking up objects)? We\nstudy this problem under the framework of transfer learning, where the model is\nfirst trained on a passive vision task, and adapted to perform an active\nmanipulation task. We find that pre-training on vision tasks significantly\nimproves generalization and sample efficiency for learning to manipulate\nobjects. However, realizing these gains requires careful selection of which\nparts of the model to transfer. Our key insight is that outputs of standard\nvision models highly correlate with affordance maps commonly used in\nmanipulation. Therefore, we explore directly transferring model parameters from\nvision networks to affordance prediction networks, and show that this can\nresult in successful zero-shot adaptation, where a robot can pick up certain\nobjects with zero robotic experience. With just a small amount of robotic\nexperience, we can further fine-tune the affordance model to achieve better\nresults. With just 10 minutes of suction experience or 1 hour of grasping\nexperience, our method achieves ~80% success rate at picking up novel objects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:58:37 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yen-Chen", "Lin", ""], ["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Isola", "Phillip", ""], ["Lin", "Tsung-Yi", ""]]}, {"id": "2107.00648", "submitter": "Nathaniel Braman", "authors": "Nathaniel Braman, Jacob W. H. Gordon, Emery T. Goossens, Caleb Willis,\n  Martin C. Stumpe, Jagadish Venkataraman", "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery\n  Integrating Radiology, Pathology, Genomic, and Clinical Data", "comments": "Accepted for presentation at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM q-bio.GN q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:01 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Braman", "Nathaniel", ""], ["Gordon", "Jacob W. H.", ""], ["Goossens", "Emery T.", ""], ["Willis", "Caleb", ""], ["Stumpe", "Martin C.", ""], ["Venkataraman", "Jagadish", ""]]}, {"id": "2107.00649", "submitter": "Janis Postels", "authors": "Janis Postels, Mattia Segu, Tao Sun, Luc Van Gool, Fisher Yu, Federico\n  Tombari", "title": "On the Practicality of Deterministic Epistemic Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of novel approaches for estimating epistemic uncertainty in deep neural\nnetworks with a single forward pass has recently emerged as a valid alternative\nto Bayesian Neural Networks. On the premise of informative representations,\nthese deterministic uncertainty methods (DUMs) achieve strong performance on\ndetecting out-of-distribution (OOD) data while adding negligible computational\ncosts at inference time. However, it remains unclear whether DUMs are well\ncalibrated and can seamlessly scale to real-world applications - both\nprerequisites for their practical deployment. To this end, we first provide a\ntaxonomy of DUMs, evaluate their calibration under continuous distributional\nshifts and their performance on OOD detection for image classification tasks.\nThen, we extend the most promising approaches to semantic segmentation. We find\nthat, while DUMs scale to realistic vision tasks and perform well on OOD\ndetection, the practicality of current methods is undermined by poor\ncalibration under realistic distributional shifts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:07 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 19:49:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Postels", "Janis", ""], ["Segu", "Mattia", ""], ["Sun", "Tao", ""], ["Van Gool", "Luc", ""], ["Yu", "Fisher", ""], ["Tombari", "Federico", ""]]}, {"id": "2107.00650", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Anna Rohrbach, Trevor Darrell", "title": "CLIP-It! Language-Guided Video Summarization", "comments": "Website at https://medhini.github.io/clip_it/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method's strong generalization\ncapabilities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:27 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""]]}, {"id": "2107.00651", "submitter": "Houwen Peng", "authors": "Minghao Chen, Houwen Peng, Jianlong Fu, Haibin Ling", "title": "AutoFormer: Searching Transformers for Visual Recognition", "comments": "Github: https://github.com/microsoft/AutoML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pure transformer-based models have shown great potentials for\nvision tasks such as image classification and detection. However, the design of\ntransformer networks is challenging. It has been observed that the depth,\nembedding dimension, and number of heads can largely affect the performance of\nvision transformers. Previous models configure these dimensions based upon\nmanual crafting. In this work, we propose a new one-shot architecture search\nframework, namely AutoFormer, dedicated to vision transformer search.\nAutoFormer entangles the weights of different blocks in the same layers during\nsupernet training. Benefiting from the strategy, the trained supernet allows\nthousands of subnets to be very well-trained. Specifically, the performance of\nthese subnets with weights inherited from the supernet is comparable to those\nretrained from scratch. Besides, the searched models, which we refer to\nAutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In\nparticular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy\non ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify\nthe transferability of AutoFormer by providing the performance on downstream\nbenchmarks and distillation experiments. Code and models are available at\nhttps://github.com/microsoft/AutoML.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chen", "Minghao", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Ling", "Haibin", ""]]}, {"id": "2107.00652", "submitter": "Dongdong Chen", "authors": "Xiaoyi Dong and Jianmin Bao and Dongdong Chen and Weiming Zhang and\n  Nenghai Yu and Lu Yuan and Dong Chen and Baining Guo", "title": "CSWin Transformer: A General Vision Transformer Backbone with\n  Cross-Shaped Windows", "comments": "The code repo is available at\n  https://github.com/microsoft/CSWin-Transformer, SOTA performance on ADE20k\n  Segmentation benchmark is updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:59:56 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 17:59:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Dong", "Xiaoyi", ""], ["Bao", "Jianmin", ""], ["Chen", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""], ["Yuan", "Lu", ""], ["Chen", "Dong", ""], ["Guo", "Baining", ""]]}, {"id": "2107.00689", "submitter": "Youngjoo Kim", "authors": "Youngjoo Kim", "title": "Aerial Map-Based Navigation Using Semantic Segmentation and Pattern\n  Matching", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching between aerial images and a map database. By using\nsemantic segmentation, the ground objects are labelled and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm which does not require\naltitude information or a camera model to estimate the absolute horizontal\nposition. The feasibility analysis with simulated images shows the proposed\nmap-based navigation can be realized with the proposed pattern matching\nalgorithm and it is able to provide positions given the labelled objects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:31:42 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kim", "Youngjoo", ""]]}, {"id": "2107.00691", "submitter": "Ehsan Mirsadeghi", "authors": "S. Ehsan Mirsadeghi, Ali Royat, Hamid Rezatofighi", "title": "Unsupervised Image Segmentation by Mutual Information Maximization and\n  Adversarial Regularization", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters (RA-L 2021) & IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic segmentation is one of the basic, yet essential scene understanding\ntasks for an autonomous agent. The recent developments in supervised machine\nlearning and neural networks have enjoyed great success in enhancing the\nperformance of the state-of-the-art techniques for this task. However, their\nsuperior performance is highly reliant on the availability of a large-scale\nannotated dataset. In this paper, we propose a novel fully unsupervised\nsemantic segmentation method, the so-called Information Maximization and\nAdversarial Regularization Segmentation (InMARS). Inspired by human perception\nwhich parses a scene into perceptual groups, rather than analyzing each pixel\nindividually, our proposed approach first partitions an input image into\nmeaningful regions (also known as superpixels). Next, it utilizes\nMutual-Information-Maximization followed by an adversarial training strategy to\ncluster these regions into semantically meaningful classes. To customize an\nadversarial training scheme for the problem, we incorporate adversarial pixel\nnoise along with spatial perturbations to impose photometrical and geometrical\ninvariance on the deep neural network. Our experiments demonstrate that our\nmethod achieves the state-of-the-art performance on two commonly used\nunsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:36:27 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mirsadeghi", "S. Ehsan", ""], ["Royat", "Ali", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "2107.00704", "submitter": "Junqing Huang", "authors": "Junqing Huang, Michael Ruzhansky, Qianying Zhang, Haihui Wang", "title": "Intrinsic Image Transfer for Illumination Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel intrinsic image transfer (IIT) algorithm for\nillumination manipulation, which creates a local image translation between two\nillumination surfaces. This model is built on an optimization-based framework\nconsisting of three photo-realistic losses defined on the sub-layers factorized\nby an intrinsic image decomposition. We illustrate that all losses can be\nreduced without the necessity of taking an intrinsic image decomposition under\nthe well-known spatial-varying illumination illumination-invariant reflectance\nprior knowledge. Moreover, with a series of relaxations, all of them can be\ndirectly defined on images, giving a closed-form solution for image\nillumination manipulation. This new paradigm differs from the prevailing\nRetinex-based algorithms, as it provides an implicit way to deal with the\nper-pixel image illumination. We finally demonstrate its versatility and\nbenefits to the illumination-related tasks such as illumination compensation,\nimage enhancement, and high dynamic range (HDR) image compression, and show the\nhigh-quality results on natural image datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 19:12:24 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Huang", "Junqing", ""], ["Ruzhansky", "Michael", ""], ["Zhang", "Qianying", ""], ["Wang", "Haihui", ""]]}, {"id": "2107.00708", "submitter": "Jiahui Zhang", "authors": "Jiahui Zhang, Shijian Lu, Fangneng Zhan, Yingchen Yu", "title": "Blind Image Super-Resolution via Contrastive Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution (SR) research has witnessed impressive progress thanks\nto the advance of convolutional neural networks (CNNs) in recent years.\nHowever, most existing SR methods are non-blind and assume that degradation has\na single fixed and known distribution (e.g., bicubic) which struggle while\nhandling degradation in real-world data that usually follows a multi-modal,\nspatially variant, and unknown distribution. The recent blind SR studies\naddress this issue via degradation estimation, but they do not generalize well\nto multi-source degradation and cannot handle spatially variant degradation. We\ndesign CRL-SR, a contrastive representation learning network that focuses on\nblind SR of images with multi-modal and spatially variant distributions. CRL-SR\naddresses the blind SR challenges from two perspectives. The first is\ncontrastive decoupling encoding which introduces contrastive learning to\nextract resolution-invariant embedding and discard resolution-variant embedding\nunder the guidance of a bidirectional contrastive loss. The second is\ncontrastive feature refinement which generates lost or corrupted high-frequency\ndetails under the guidance of a conditional contrastive loss. Extensive\nexperiments on synthetic datasets and real images show that the proposed CRL-SR\ncan handle multi-modal and spatially variant degradation effectively under\nblind settings and it also outperforms state-of-the-art SR methods\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 19:34:23 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zhang", "Jiahui", ""], ["Lu", "Shijian", ""], ["Zhan", "Fangneng", ""], ["Yu", "Yingchen", ""]]}, {"id": "2107.00710", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Ulysse C\\^ot\\'e-Allard, Petter Jakobsen, Andrea Stautland, Tine\n  Nordgreen, Ole Bernt Fasmer, Ketil Joachim Oedegaard, Jim Torresen", "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition\n  Based on Wrist-worn Sensors", "comments": "Submitted for peer-review. 11 pages + 3. 2 Figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 19:35:54 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Jakobsen", "Petter", ""], ["Stautland", "Andrea", ""], ["Nordgreen", "Tine", ""], ["Fasmer", "Ole Bernt", ""], ["Oedegaard", "Ketil Joachim", ""], ["Torresen", "Jim", ""]]}, {"id": "2107.00712", "submitter": "Manuel Rebol", "authors": "Manuel Rebol and Christian G\\\"utl and Krzysztof Pietroszek", "title": "Passing a Non-verbal Turing Test: Evaluating Gesture Animations\n  Generated from Speech", "comments": null, "journal-ref": "2021 IEEE Virtual Reality and 3D User Interfaces (VR)", "doi": "10.1109/VR50410.2021.00082", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real life, people communicate using both speech and non-verbal signals\nsuch as gestures, face expression or body pose. Non-verbal signals impact the\nmeaning of the spoken utterance in an abundance of ways. An absence of\nnon-verbal signals impoverishes the process of communication. Yet, when users\nare represented as avatars, it is difficult to translate non-verbal signals\nalong with the speech into the virtual world without specialized motion-capture\nhardware. In this paper, we propose a novel, data-driven technique for\ngenerating gestures directly from speech. Our approach is based on the\napplication of Generative Adversarial Neural Networks (GANs) to model the\ncorrelation rather than causation between speech and gestures. This approach\napproximates neuroscience findings on how non-verbal communication and speech\nare correlated. We create a large dataset which consists of speech and\ncorresponding gestures in a 3D human pose format from which our model learns\nthe speaker-specific correlation. We evaluate the proposed technique in a user\nstudy that is inspired by the Turing test. For the study, we animate the\ngenerated gestures on a virtual character. We find that users are not able to\ndistinguish between the generated and the recorded gestures. Moreover, users\nare able to identify our synthesized gestures as related or not related to a\ngiven utterance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 19:38:43 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Rebol", "Manuel", ""], ["G\u00fctl", "Christian", ""], ["Pietroszek", "Krzysztof", ""]]}, {"id": "2107.00717", "submitter": "Suraj Kothawade", "authors": "Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, Rishabh Iyer", "title": "SIMILAR: Submodular Information Measures Based Active Learning In\n  Realistic Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 19:49:44 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kothawade", "Suraj", ""], ["Beck", "Nathan", ""], ["Killamsetty", "Krishnateja", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2107.00727", "submitter": "Shanu Kumar", "authors": "Shanu Kumar, Vinod Kumar Kurmi, Praphul Singh, Vinay P Namboodiri", "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 20:08:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kumar", "Shanu", ""], ["Kurmi", "Vinod Kumar", ""], ["Singh", "Praphul", ""], ["Namboodiri", "Vinay P", ""]]}, {"id": "2107.00769", "submitter": "Nathaniel Glaser", "authors": "Nathaniel Glaser, Yen-Cheng Liu, Junjiao Tian, Zsolt Kira", "title": "Enhancing Multi-Robot Perception via Learned Data Association", "comments": "Accepted to ICRA 2020 Workshop on \"Emerging Learning and Algorithmic\n  Methods for Data Association in Robotics\"; associated spotlight talk\n  available at https://www.youtube.com/watch?v=-lEVvtsfz0I&t=16743s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the multi-robot collaborative perception problem,\nspecifically in the context of multi-view infilling for distributed semantic\nsegmentation. This setting entails several real-world challenges, especially\nthose relating to unregistered multi-agent image data. Solutions must\neffectively leverage multiple, non-static, and intermittently-overlapping RGB\nperspectives. To this end, we propose the Multi-Agent Infilling Network: an\nextensible neural architecture that can be deployed (in a distributed manner)\nto each agent in a robotic swarm. Specifically, each robot is in charge of\nlocally encoding and decoding visual information, and an extensible neural\nmechanism allows for an uncertainty-aware and context-based exchange of\nintermediate features. We demonstrate improved performance on a realistic\nmulti-robot AirSim dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 22:45:26 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Glaser", "Nathaniel", ""], ["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Kira", "Zsolt", ""]]}, {"id": "2107.00771", "submitter": "Nathaniel Glaser", "authors": "Nathaniel Glaser, Yen-Cheng Liu, Junjiao Tian, Zsolt Kira", "title": "Overcoming Obstructions via Bandwidth-Limited Multi-Agent Spatial\n  Handshaking", "comments": "Accepted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address bandwidth-limited and obstruction-prone\ncollaborative perception, specifically in the context of multi-agent semantic\nsegmentation. This setting presents several key challenges, including\nprocessing and exchanging unregistered robotic swarm imagery. To be successful,\nsolutions must effectively leverage multiple non-static and\nintermittently-overlapping RGB perspectives, while heeding bandwidth\nconstraints and overcoming unwanted foreground obstructions. As such, we\npropose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH)\nto process, compress, and propagate visual information across a robotic swarm.\nOur distributed communication module operates directly (and exclusively) on raw\nimage data, without additional input requirements such as pose, depth, or\nwarping data. We demonstrate superior performance of our model compared against\nseveral baselines in a photo-realistic multi-robot AirSim environment,\nespecially in the presence of image occlusions. Our method achieves an absolute\n11% IoU improvement over strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 22:56:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Glaser", "Nathaniel", ""], ["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Kira", "Zsolt", ""]]}, {"id": "2107.00781", "submitter": "Yunhe Gao", "authors": "Yunhe Gao, Mu Zhou, Dimitris Metaxas", "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformer architecture has emerged to be successful in a number of natural\nlanguage processing tasks. However, its applications to medical vision remain\nlargely unexplored. In this study, we present UTNet, a simple yet powerful\nhybrid Transformer architecture that integrates self-attention into a\nconvolutional neural network for enhancing medical image segmentation. UTNet\napplies self-attention modules in both encoder and decoder for capturing\nlong-range dependency at different scales with minimal overhead. To this end,\nwe propose an efficient self-attention mechanism along with relative position\nencoding that reduces the complexity of self-attention operation significantly\nfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also\nproposed to recover fine-grained details from the skipped connections in the\nencoder. Our approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer design allows\nthe initialization of Transformer into convolutional networks without a need of\npre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging cohort. UTNet demonstrates superior segmentation\nperformance and robustness against the state-of-the-art approaches, holding the\npromise to generalize well on other medical image segmentations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 00:56:27 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gao", "Yunhe", ""], ["Zhou", "Mu", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2107.00782", "submitter": "Dong Huang", "authors": "Huajun Liu, Fuqiang Liu, Xinyi Fan, Dong Huang", "title": "Polarized Self-Attention: Towards High-quality Pixel-wise Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pixel-wise regression is probably the most common problem in fine-grained\ncomputer vision tasks, such as estimating keypoint heatmaps and segmentation\nmasks. These regression problems are very challenging particularly because they\nrequire, at low computation overheads, modeling long-range dependencies on\nhigh-resolution inputs/outputs to estimate the highly nonlinear pixel-wise\nsemantics. While attention mechanisms in Deep Convolutional Neural\nNetworks(DCNNs) has become popular for boosting long-range dependencies,\nelement-specific attention, such as Nonlocal blocks, is highly complex and\nnoise-sensitive to learn, and most of simplified attention hybrids try to reach\nthe best compromise among multiple types of tasks. In this paper, we present\nthe Polarized Self-Attention(PSA) block that incorporates two critical designs\ntowards high-quality pixel-wise regression: (1) Polarized filtering: keeping\nhigh internal resolution in both channel and spatial attention computation\nwhile completely collapsing input tensors along their counterpart dimensions.\n(2) Enhancement: composing non-linearity that directly fits the output\ndistribution of typical fine-grained regression, such as the 2D Gaussian\ndistribution (keypoint heatmaps), or the 2D Binormial distribution (binary\nsegmentation masks). PSA appears to have exhausted the representation capacity\nwithin its channel-only and spatial-only branches, such that there is only\nmarginal metric differences between its sequential and parallel layouts.\nExperimental results show that PSA boosts standard baselines by $2-4$ points,\nand boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic\nsegmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 01:03:11 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 15:33:31 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Liu", "Huajun", ""], ["Liu", "Fuqiang", ""], ["Fan", "Xinyi", ""], ["Huang", "Dong", ""]]}, {"id": "2107.00789", "submitter": "Komei Sugiura", "authors": "Motonari Kambara and Komei Sugiura", "title": "Case Relation Transformer: A Crossmodal Language Generation Model for\n  Fetching Instructions", "comments": "Accepted for presentation at IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 01:40:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kambara", "Motonari", ""], ["Sugiura", "Komei", ""]]}, {"id": "2107.00808", "submitter": "Xiaoni Li", "authors": "Xiaoni Li, Yucan Zhou, Yu Zhou, Weiping Wang", "title": "MMF: Multi-Task Multi-Structure Fusion for Hierarchical Image\n  Classification", "comments": "Accpeted by ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical classification is significant for complex tasks by providing\nmulti-granular predictions and encouraging better mistakes. As the label\nstructure decides its performance, many existing approaches attempt to\nconstruct an excellent label structure for promoting the classification\nresults. In this paper, we consider that different label structures provide a\nvariety of prior knowledge for category recognition, thus fusing them is\nhelpful to achieve better hierarchical classification results. Furthermore, we\npropose a multi-task multi-structure fusion model to integrate different label\nstructures. It contains two kinds of branches: one is the traditional\nclassification branch to classify the common subclasses, the other is\nresponsible for identifying the heterogeneous superclasses defined by different\nlabel structures. Besides the effect of multiple label structures, we also\nexplore the architecture of the deep model for better hierachical\nclassification and adjust the hierarchical evaluation metrics for multiple\nlabel structures. Experimental results on CIFAR100 and Car196 show that our\nmethod obtains significantly better results than using a flat classifier or a\nhierarchical classifier with any single label structure.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 02:53:35 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Li", "Xiaoni", ""], ["Zhou", "Yucan", ""], ["Zhou", "Yu", ""], ["Wang", "Weiping", ""]]}, {"id": "2107.00811", "submitter": "Komei Sugiura", "authors": "Shintaro Ishikawa and Komei Sugiura", "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language\n  Comprehension Model for Domestic Service Robots", "comments": "Accepted for presentation at IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 03:11:02 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ishikawa", "Shintaro", ""], ["Sugiura", "Komei", ""]]}, {"id": "2107.00818", "submitter": "Pengcheng Wang", "authors": "Pengcheng Wang, Lingqiao Ji, Zhilong Ji, Yuan Gao, Xiao Liu", "title": "1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face\n  detection in the low light condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this technical report, we briefly introduce the solution of our team\n\"TAL-ai\" for (Semi-) supervised Face detection in the low light condition in\nUG2+ Challenge in CVPR 2021. By conducting several experiments with popular\nimage enhancement methods and image transfer methods, we pulled the low light\nimage and the normal image to a more closer domain. And it is observed that\nusing these data to training can achieve better performance. We also adapt\nseveral popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and\nlarge backbone like Swin-transformer. Finally, we ensemble several models which\nachieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 04:12:23 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wang", "Pengcheng", ""], ["Ji", "Lingqiao", ""], ["Ji", "Zhilong", ""], ["Gao", "Yuan", ""], ["Liu", "Xiao", ""]]}, {"id": "2107.00842", "submitter": "Yingying Zhu", "authors": "Hongji Yang, Xiufan Lu and Yingying Zhu", "title": "Cross-view Geo-localization with Evolving Transformer", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we address the problem of cross-view geo-localization, which\nestimates the geospatial location of a street view image by matching it with a\ndatabase of geo-tagged aerial images. The cross-view matching task is extremely\nchallenging due to drastic appearance and geometry differences across views.\nUnlike existing methods that predominantly fall back on CNN, here we devise a\nnovel evolving geo-localization Transformer (EgoTR) that utilizes the\nproperties of self-attention in Transformer to model global dependencies, thus\nsignificantly decreasing visual ambiguities in cross-view geo-localization. We\nalso exploit the positional encoding of Transformer to help the EgoTR\nunderstand and correspond geometric configurations between ground and aerial\nimages. Compared to state-of-the-art methods that impose strong assumption on\ngeometry knowledge, the EgoTR flexibly learns the positional embeddings through\nthe training objective and hence becomes more practical in many real-world\nscenarios. Although Transformer is well suited to our task, its vanilla\nself-attention mechanism independently interacts within image patches in each\nlayer, which overlooks correlations between layers. Instead, this paper propose\na simple yet effective self-cross attention mechanism to improve the quality of\nlearned representations. The self-cross attention models global dependencies\nbetween adjacent layers, which relates between image patches while modeling how\nfeatures evolve in the previous layer. As a result, the proposed self-cross\nattention leads to more stable training, improves the generalization ability\nand encourages representations to keep evolving as the network goes deeper.\nExtensive experiments demonstrate that our EgoTR performs favorably against\nstate-of-the-art methods on standard, fine-grained and cross-dataset cross-view\ngeo-localization tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 05:33:14 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 02:23:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yang", "Hongji", ""], ["Lu", "Xiufan", ""], ["Zhu", "Yingying", ""]]}, {"id": "2107.00860", "submitter": "Hayeon Lee", "authors": "Hayeon Lee, Eunyoung Hyung, Sung Ju Hwang", "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from\n  Datasets", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 06:33:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Lee", "Hayeon", ""], ["Hyung", "Eunyoung", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2107.00875", "submitter": "Negin Ghamsarian", "authors": "Negin Ghamsarian, Mario Taschwer, Doris Putzgruber-Adamitsch,\n  Stephanie Sarny, Yosuf El-Shabrawi, Klaus Schoeffmann", "title": "LensID: A CNN-RNN-Based Framework Towards Lens Irregularity Detection in\n  Cataract Surgery Videos", "comments": "13 pages, 5 figures, accepted at 24th international conference on\n  Medical Image Computing & Computer Assisted Intervention (MICCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A critical complication after cataract surgery is the dislocation of the lens\nimplant leading to vision deterioration and eye trauma. In order to reduce the\nrisk of this complication, it is vital to discover the risk factors during the\nsurgery. However, studying the relationship between lens dislocation and its\nsuspicious risk factors using numerous videos is a time-extensive procedure.\nHence, the surgeons demand an automatic approach to enable a larger-scale and,\naccordingly, more reliable study. In this paper, we propose a novel framework\nas the major step towards lens irregularity detection. In particular, we\npropose (I) an end-to-end recurrent neural network to recognize the\nlens-implantation phase and (II) a novel semantic segmentation network to\nsegment the lens and pupil after the implantation phase. The phase recognition\nresults reveal the effectiveness of the proposed surgical phase recognition\napproach. Moreover, the segmentation results confirm the proposed segmentation\nnetwork's effectiveness compared to state-of-the-art rival approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 07:27:29 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ghamsarian", "Negin", ""], ["Taschwer", "Mario", ""], ["Putzgruber-Adamitsch", "Doris", ""], ["Sarny", "Stephanie", ""], ["El-Shabrawi", "Yosuf", ""], ["Schoeffmann", "Klaus", ""]]}, {"id": "2107.00887", "submitter": "Shreyas Hampali", "authors": "Shreyas Hampali, Sayan Deb Sarkar, Vincent Lepetit", "title": "HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HO-3D is a dataset providing image sequences of various hand-object\ninteraction scenarios annotated with the 3D pose of the hand and the object and\nwas originally introduced as HO-3D_v2. The annotations were obtained\nautomatically using an optimization method, 'HOnnotate', introduced in the\noriginal paper. HO-3D_v3 provides more accurate annotations for both the hand\nand object poses thus resulting in better estimates of contact regions between\nthe hand and the object. In this report, we elaborate on the improvements to\nthe HOnnotate method and provide evaluations to compare the accuracy of\nHO-3D_v2 and HO-3D_v3. HO-3D_v3 results in 4mm higher accuracy compared to\nHO-3D_v2 for hand poses while exhibiting higher contact regions with the object\nsurface.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 08:06:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Hampali", "Shreyas", ""], ["Sarkar", "Sayan Deb", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2107.00932", "submitter": "Conghao Wong", "authors": "Conghao Wong, Beihao Xia, Qinmu Peng, Xinge You", "title": "MSN: Multi-Style Network for Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential but challenging to predict future trajectories of various\nagents in complex scenes. Whether it is internal personality factors of agents,\ninteractive behavior of the neighborhood, or the influence of surroundings, it\nwill have an impact on their future behavior styles. It means that even for the\nsame physical type of agents, there are huge differences in their behavior\npreferences. Although recent works have made significant progress in studying\nagents' multi-modal plannings, most of them still apply the same prediction\nstrategy to all agents, which makes them difficult to fully show the multiple\nstyles of vast agents. In this paper, we propose the Multi-Style Network (MSN)\nto focus on this problem by divide agents' preference styles into several\nhidden behavior categories adaptively and train each category's prediction\nnetwork separately, therefore giving agents all styles of predictions\nsimultaneously. Experiments demonstrate that our deterministic MSN-D and\ngenerative MSN-G outperform many recent state-of-the-art methods and show\nbetter multi-style characteristics in the visualized results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:43:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wong", "Conghao", ""], ["Xia", "Beihao", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""]]}, {"id": "2107.00934", "submitter": "Jiahui Li", "authors": "Jiahui Li, Wen Chen, Xiaodi Huang, Zhiqiang Hu, Qi Duan, Hongsheng Li,\n  Dimitris N. Metaxas, Shaoting Zhang", "title": "Mixed Supervision Learning for Whole Slide Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weak supervision learning on classification labels has demonstrated high\nperformance in various tasks. When a few pixel-level fine annotations are also\naffordable, it is natural to leverage both of the pixel-level (e.g.,\nsegmentation) and image level (e.g., classification) annotation to further\nimprove the performance. In computational pathology, however, such weak or\nmixed supervision learning is still a challenging task, since the high\nresolution of whole slide images makes it unattainable to perform end-to-end\ntraining of classification models. An alternative approach is to analyze such\ndata by patch-base model training, i.e., using self-supervised learning to\ngenerate pixel-level pseudo labels for patches. However, such methods usually\nhave model drifting issues, i.e., hard to converge, because the noise\naccumulates during the self-training process. To handle those problems, we\npropose a mixed supervision learning framework for super high-resolution images\nto effectively utilize their various labels (e.g., sufficient image-level\ncoarse annotations and a few pixel-level fine labels). During the patch\ntraining stage, this framework can make use of coarse image-level labels to\nrefine self-supervised learning and generate high-quality pixel-level pseudo\nlabels. A comprehensive strategy is proposed to suppress pixel-level false\npositives and false negatives. Three real-world datasets with very large number\nof images (i.e., more than 10,000 whole slide images) and various types of\nlabels are used to evaluate the effectiveness of mixed supervision learning. We\nreduced the false positive rate by around one third compared to state of the\nart while retaining 100% sensitivity, in the task of image-level\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:46:06 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 03:09:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Li", "Jiahui", ""], ["Chen", "Wen", ""], ["Huang", "Xiaodi", ""], ["Hu", "Zhiqiang", ""], ["Duan", "Qi", ""], ["Li", "Hongsheng", ""], ["Metaxas", "Dimitris N.", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2107.00961", "submitter": "Anastasios Kyrillidis", "authors": "Chen Dun, Cameron R. Wolfe, Christopher M. Jermaine, Anastasios\n  Kyrillidis", "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:48:50 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dun", "Chen", ""], ["Wolfe", "Cameron R.", ""], ["Jermaine", "Christopher M.", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "2107.00964", "submitter": "Ioannis Tzortzis", "authors": "Charalampos Zafeiropoulos, Ioannis N. Tzortzis, Ioannis Rallis,\n  Eftychios Protopapadakis, Nikolaos Doulamis and Anastasios Doulamis", "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural\n  Heritage Monuments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:51:28 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zafeiropoulos", "Charalampos", ""], ["Tzortzis", "Ioannis N.", ""], ["Rallis", "Ioannis", ""], ["Protopapadakis", "Eftychios", ""], ["Doulamis", "Nikolaos", ""], ["Doulamis", "Anastasios", ""]]}, {"id": "2107.00968", "submitter": "Thanaphon Suwannaphong", "authors": "Thanaphon Suwannaphong, Sawaphob Chavana, Sahapol Tongsom, Duangdao\n  Palasuwan, Thanarat H. Chalidabhongse and Nantheera Anantrasirichai", "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic\n  Images using Transfer Learning", "comments": "7 pages, 9 figures, Preprint submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:05:45 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Suwannaphong", "Thanaphon", ""], ["Chavana", "Sawaphob", ""], ["Tongsom", "Sahapol", ""], ["Palasuwan", "Duangdao", ""], ["Chalidabhongse", "Thanarat H.", ""], ["Anantrasirichai", "Nantheera", ""]]}, {"id": "2107.00977", "submitter": "Hadrien Reynaud", "authors": "Hadrien Reynaud, Athanasios Vlontzos, Benjamin Hou, Arian Beqiri, Paul\n  Leeson, Bernhard Kainz", "title": "Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation", "comments": "Accepted for MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:23:09 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Reynaud", "Hadrien", ""], ["Vlontzos", "Athanasios", ""], ["Hou", "Benjamin", ""], ["Beqiri", "Arian", ""], ["Leeson", "Paul", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2107.00986", "submitter": "Zongsheng Yue", "authors": "Zongsheng Yue, Qian Zhao, Jianwen Xie, Lei Zhang and Deyu Meng", "title": "Unsupervised Single Image Super-resolution Under Complex Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While the researches on single image super-resolution (SISR), especially\nequipped with deep neural networks (DNNs), have achieved tremendous successes\nrecently, they still suffer from two major limitations. Firstly, the real image\ndegradation is usually unknown and highly variant from one to another, making\nit extremely hard to train a single model to handle the general SISR task.\nSecondly, most of current methods mainly focus on the downsampling process of\nthe degradation, but ignore or underestimate the inevitable noise\ncontamination. For example, the commonly-used independent and identically\ndistributed (i.i.d.) Gaussian noise distribution always largely deviates from\nthe real image noise (e.g., camera sensor noise), which limits their\nperformance in real scenarios. To address these issues, this paper proposes a\nmodel-based unsupervised SISR method to deal with the general SISR task with\nunknown degradations. Instead of the traditional i.i.d. Gaussian noise\nassumption, a novel patch-based non-i.i.d. noise modeling method is proposed to\nfit the complex real noise. Besides, a deep generator parameterized by a DNN is\nused to map the latent variable to the high-resolution image, and the\nconventional hyper-Laplacian prior is also elaborately embedded into such\ngenerator to further constrain the image gradients. Finally, a Monte Carlo EM\nalgorithm is designed to solve our model, which provides a general inference\nframework to update the image generator both w.r.t. the latent variable and the\nnetwork parameters. Comprehensive experiments demonstrate that the proposed\nmethod can evidently surpass the current state of the art (SotA) method (about\n1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster\nspeed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:55:40 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yue", "Zongsheng", ""], ["Zhao", "Qian", ""], ["Xie", "Jianwen", ""], ["Zhang", "Lei", ""], ["Meng", "Deyu", ""]]}, {"id": "2107.00987", "submitter": "Anastasiia Kornilova", "authors": "Azat Akhmetyanov, Anastasiia Kornilova, Marsel Faizullin, David Pozo,\n  Gonzalo Ferrer", "title": "Sub-millisecond Video Synchronization of Multiple Android Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper addresses the problem of building an affordable easy-to-setup\nsynchronized multi-view camera system, which is in demand for many Computer\nVision and Robotics applications in high-dynamic environments. In our work, we\npropose a solution for this problem - a publicly-available Android application\nfor synchronized video recording on multiple smartphones with sub-millisecond\naccuracy. We present a generalized mathematical model of timestamping for\nAndroid smartphones and prove its applicability on 47 different physical\ndevices. Also, we estimate the time drift parameter for those smartphones,\nwhich is less than 1.2 millisecond per minute for most of the considered\ndevices, that makes smartphones' camera system a worthy analog for professional\nmulti-view systems. Finally, we demonstrate Android-app performance on the\ncamera system built from Android smartphones quantitatively, showing less than\n300 microseconds synchronization error, and qualitatively - on panorama\nstitching task.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:56:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Akhmetyanov", "Azat", ""], ["Kornilova", "Anastasiia", ""], ["Faizullin", "Marsel", ""], ["Pozo", "David", ""], ["Ferrer", "Gonzalo", ""]]}, {"id": "2107.00993", "submitter": "Zeba Khanam", "authors": "Zeba Khanam and Atiya Usmani", "title": "Optical Braille Recognition using Circular Hough Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Braille has empowered visually challenged community to read and write. But at\nthe same time, it has created a gap due to widespread inability of non-Braille\nusers to understand Braille scripts. This gap has fuelled researchers to\npropose Optical Braille Recognition techniques to convert Braille documents to\nnatural language. The main motivation of this work is to cement the\ncommunication gap at academic institutions by translating personal documents of\nblind students. This has been accomplished by proposing an economical and\neffective technique which digitizes Braille documents using a smartphone\ncamera. For any given Braille image, a dot detection mechanism based on Hough\ntransform is proposed which is invariant to skewness, noise and other\ndeterrents. The detected dots are then clustered into Braille cells using\ndistance-based clustering algorithm. In succession, the standard physical\nparameters of each Braille cells are estimated for feature extraction and\nclassification as natural language characters. The comprehensive evaluation of\nthis technique on the proposed dataset of 54 Braille scripts has yielded into\naccuracy of 98.71%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 12:15:24 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Khanam", "Zeba", ""], ["Usmani", "Atiya", ""]]}, {"id": "2107.01002", "submitter": "Ilia Karmanov", "authors": "Ilia Karmanov, Farhad G. Zanjani, Simone Merlin, Ishaque Kadampot,\n  Daniel Dijkman", "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:09:46 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Karmanov", "Ilia", ""], ["Zanjani", "Farhad G.", ""], ["Merlin", "Simone", ""], ["Kadampot", "Ishaque", ""], ["Dijkman", "Daniel", ""]]}, {"id": "2107.01063", "submitter": "Yibao Sun", "authors": "Yibao Sun, Xingru Huang, Yaqi Wang, Huiyu Zhou, Qianni Zhang", "title": "Magnification-independent Histopathological Image Classification with\n  Similarity-based Multi-scale Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classification of histopathological images is of great value in both\ncancer diagnosis and pathological studies. However, multiple reasons, such as\nvariations caused by magnification factors and class imbalance, make it a\nchallenging task where conventional methods that learn from image-label\ndatasets perform unsatisfactorily in many cases. We observe that tumours of the\nsame class often share common morphological patterns. To exploit this fact, we\npropose an approach that learns similarity-based multi-scale embeddings (SMSE)\nfor magnification-independent histopathological image classification. In\nparticular, a pair loss and a triplet loss are leveraged to learn\nsimilarity-based embeddings from image pairs or image triplets. The learned\nembeddings provide accurate measurements of similarities between images, which\nare regarded as a more effective form of representation for histopathological\nmorphology than normal image features. Furthermore, in order to ensure the\ngenerated models are magnification-independent, images acquired at different\nmagnification factors are simultaneously fed to networks during training for\nlearning multi-scale embeddings. In addition to the SMSE, to eliminate the\nimpact of class imbalance, instead of using the hard sample mining strategy\nthat intuitively discards some easy samples, we introduce a new reinforced\nfocal loss to simultaneously punish hard misclassified samples while\nsuppressing easy well-classified samples. Experimental results show that the\nSMSE improves the performance for histopathological image classification tasks\nfor both breast and liver cancers by a large margin compared to previous\nmethods. In particular, the SMSE achieves the best performance on the BreakHis\nbenchmark with an improvement ranging from 5% to 18% compared to previous\nmethods using traditional features.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:18:45 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Sun", "Yibao", ""], ["Huang", "Xingru", ""], ["Wang", "Yaqi", ""], ["Zhou", "Huiyu", ""], ["Zhang", "Qianni", ""]]}, {"id": "2107.01079", "submitter": "Chen Chen", "authors": "Chen Chen, Kerstin Hammernik, Cheng Ouyang, Chen Qin, Wenjia Bai,\n  Daniel Rueckert", "title": "Cooperative Training and Latent Space Data Augmentation for Robust\n  Medical Image Segmentation", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:39:13 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Chen", ""], ["Hammernik", "Kerstin", ""], ["Ouyang", "Cheng", ""], ["Qin", "Chen", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2107.01086", "submitter": "Manu Airaksinen", "authors": "Manu Airaksinen, Sampsa Vanhatalo, Okko R\\\"as\\\"anen", "title": "Comparison of end-to-end neural network architectures and data\n  augmentation methods for automatic infant motility assessment using wearable\n  sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infant motility assessment using intelligent wearables is a promising new\napproach for assessment of infant neurophysiological development, and where\nefficient signal analysis plays a central role. This study investigates the use\nof different end-to-end neural network architectures for processing infant\nmotility data from wearable sensors. We focus on the performance and\ncomputational burden of alternative sensor encoder and time-series modelling\nmodules and their combinations. In addition, we explore the benefits of data\naugmentation methods in ideal and non-ideal recording conditions. The\nexperiments are conducted using a data-set of multi-sensor movement recordings\nfrom 7-month-old infants, as captured by a recently proposed smart jumpsuit for\ninfant motility assessment. Our results indicate that the choice of the encoder\nmodule has a major impact on classifier performance. For sensor encoders, the\nbest performance was obtained with parallel 2-dimensional convolutions for\nintra-sensor channel fusion with shared weights for all sensors. The results\nalso indicate that a relatively compact feature representation is obtainable\nfor within-sensor feature extraction without a drastic loss to classifier\nperformance. Comparison of time-series models revealed that feed-forward\ndilated convolutions with residual and skip connections outperformed all\nRNN-based models in performance, training time, and training stability. The\nexperiments also indicate that data augmentation improves model robustness in\nsimulated packet loss or sensor dropout scenarios. In particular, signal- and\nsensor-dropout-based augmentation strategies provided considerable boosts to\nperformance without negatively affecting the baseline performance. Overall the\nresults provide tangible suggestions on how to optimize end-to-end neural\nnetwork training for multi-channel movement sensor data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 14:02:05 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Airaksinen", "Manu", ""], ["Vanhatalo", "Sampsa", ""], ["R\u00e4s\u00e4nen", "Okko", ""]]}, {"id": "2107.01125", "submitter": "Zenglin Shi", "authors": "Zenglin Shi, Pascal Mettes, Subhransu Maji, and Cees G. M. Snoek", "title": "On Measuring and Controlling the Spectral Bias of the Deep Image Prior", "comments": "Spectral bias; Deep image prior; 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The deep image prior has demonstrated the remarkable ability that untrained\nnetworks can address inverse imaging problems, such as denoising, inpainting\nand super-resolution, by optimizing on just a single degraded image. Despite\nits promise, it suffers from two limitations. First, it remains unclear how one\ncan control the prior beyond the choice of the network architecture. Second, it\nrequires an oracle to determine when to stop the optimization as the\nperformance degrades after reaching a peak. In this paper, we study the deep\nimage prior from a spectral bias perspective to address these problems. By\nintroducing a frequency-band correspondence measure, we observe that deep image\npriors for inverse imaging exhibit a spectral bias during optimization, where\nlow-frequency image signals are learned faster and better than high-frequency\nnoise signals. This pinpoints why degraded images can be denoised or inpainted\nwhen the optimization is stopped at the right time. Based on our observations,\nwe propose to control the spectral bias in the deep image prior to prevent\nperformance degradation and to speed up optimization convergence. We do so in\nthe two core layer types of inverse imaging networks: the convolution layer and\nthe upsampling layer. We present a Lipschitz-controlled approach for the\nconvolution and a Gaussian-controlled approach for the upsampling layer. We\nfurther introduce a stopping criterion to avoid superfluous computation. The\nexperiments on denoising, inpainting and super-resolution show that our method\nno longer suffers from performance degradation during optimization, relieving\nus from the need for an oracle criterion to stop early. We further outline a\nstopping criterion to avoid superfluous computation. Finally, we show that our\napproach obtains favorable restoration results compared to current approaches,\nacross all tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:10:42 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Shi", "Zenglin", ""], ["Mettes", "Pascal", ""], ["Maji", "Subhransu", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2107.01130", "submitter": "Davood Zabihzadeh", "authors": "Davood Zabihzadeh", "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric\n  Learning methods", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:19:46 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zabihzadeh", "Davood", ""]]}, {"id": "2107.01151", "submitter": "Haiyang Wang", "authors": "Haiyang Wang, Wenguan Wang, Xizhou Zhu, Jifeng Dai, Liwei Wang", "title": "Collaborative Visual Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a fundamental problem for Artificial Intelligence, multi-agent system\n(MAS) is making rapid progress, mainly driven by multi-agent reinforcement\nlearning (MARL) techniques. However, previous MARL methods largely focused on\ngrid-world like or game environments; MAS in visually rich environments has\nremained less explored. To narrow this gap and emphasize the crucial role of\nperception in MAS, we propose a large-scale 3D dataset, CollaVN, for\nmulti-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed\nto cooperatively navigate across photo-realistic environments to reach target\nlocations. Diverse MAVN variants are explored to make our problem more general.\nMoreover, a memory-augmented communication framework is proposed. Each agent is\nequipped with a private, external memory to persistently store communication\ninformation. This allows agents to make better use of their past communication\ninformation, enabling more efficient collaboration and robust long-term\nplanning. In our experiments, several baselines and evaluation metrics are\ndesigned. We also empirically verify the efficacy of our proposed MARL approach\nacross different MAVN task settings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:48:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 12:19:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Haiyang", ""], ["Wang", "Wenguan", ""], ["Zhu", "Xizhou", ""], ["Dai", "Jifeng", ""], ["Wang", "Liwei", ""]]}, {"id": "2107.01152", "submitter": "Junya Chen", "authors": "Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao,\n  Tagyoung Chung, Yi Xu, Belinda Zeng, Wenlian Lu, Fan Li, Lawrence Carin,\n  Chenyang Tao", "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive\n  Learners With FlatNCE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:50:43 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Junya", ""], ["Gan", "Zhe", ""], ["Li", "Xuan", ""], ["Guo", "Qing", ""], ["Chen", "Liqun", ""], ["Gao", "Shuyang", ""], ["Chung", "Tagyoung", ""], ["Xu", "Yi", ""], ["Zeng", "Belinda", ""], ["Lu", "Wenlian", ""], ["Li", "Fan", ""], ["Carin", "Lawrence", ""], ["Tao", "Chenyang", ""]]}, {"id": "2107.01153", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Tianfei Zhou, Fatih Porikli, David Crandall, Luc Van\n  Gool", "title": "A Survey on Deep Learning Technique for Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video segmentation, i.e., partitioning video frames into multiple segments or\nobjects, plays a critical role in a broad range of practical applications,\ne.g., visual effect assistance in movie, scene understanding in autonomous\ndriving, and virtual background creation in video conferencing, to name a few.\nRecently, due to the renaissance of connectionism in computer vision, there has\nbeen an influx of numerous deep learning based approaches that have been\ndedicated to video segmentation and delivered compelling performance. In this\nsurvey, we comprehensively review two basic lines of research in this area,\ni.e., generic object segmentation (of unknown categories) in videos and video\nsemantic segmentation, by introducing their respective task settings,\nbackground concepts, perceived need, development history, and main challenges.\nWe also provide a detailed overview of representative literature on both\nmethods and datasets. Additionally, we present quantitative performance\ncomparisons of the reviewed methods on benchmark datasets. At last, we point\nout a set of unsolved open issues in this field, and suggest possible\nopportunities for further research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:51:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wang", "Wenguan", ""], ["Zhou", "Tianfei", ""], ["Porikli", "Fatih", ""], ["Crandall", "David", ""], ["Van Gool", "Luc", ""]]}, {"id": "2107.01175", "submitter": "Su Zhang", "authors": "Su Zhang, Yi Ding, Ziquan Wei, Cuntai Guan", "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.469 for valence and 0.649 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:28:55 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 09:07:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Su", ""], ["Ding", "Yi", ""], ["Wei", "Ziquan", ""], ["Guan", "Cuntai", ""]]}, {"id": "2107.01181", "submitter": "Li Mi", "authors": "Li Mi, Yangjun Ou, Zhenzhong Chen", "title": "Visual Relationship Forecasting in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world scenarios often require the anticipation of object interactions in\nunknown future, which would assist the decision-making process of both humans\nand agents. To meet this challenge, we present a new task named Visual\nRelationship Forecasting (VRF) in videos to explore the prediction of visual\nrelationships in a reasoning manner. Specifically, given a subject-object pair\nwith H existing frames, VRF aims to predict their future interactions for the\nnext T frames without visual evidence. To evaluate the VRF task, we introduce\ntwo video datasets named VRF-AG and VRF-VidOR, with a series of\nspatio-temporally localized visual relation annotations in a video. These two\ndatasets densely annotate 13 and 35 visual relationships in 1923 and 13447\nvideo clips, respectively. In addition, we present a novel Graph Convolutional\nTransformer (GCT) framework, which captures both object-level and frame-level\ndependencies by spatio-temporal Graph Convolution Network and Transformer.\nExperimental results on both VRF-AG and VRF-VidOR datasets demonstrate that GCT\noutperforms the state-of-the-art sequence modelling methods on visual\nrelationship forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:43:19 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Mi", "Li", ""], ["Ou", "Yangjun", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2107.01189", "submitter": "Jerrick Liu", "authors": "Jerrick Liu, Nathan Inkawhich, Oliver Nina, Radu Timofte, Sahil Jain,\n  Bob Lee, Yuru Duan, Wei Wei, Lei Zhang, Songzheng Xu, Yuxuan Sun, Jiaqi Tang,\n  Xueli Geng, Mengru Ma, Gongzhe Li, Xueli Geng, Huanqia Cai, Chengxue Cai, Sol\n  Cummings, Casian Miron, Alexandru Pasarica, Cheng-Yen Yang, Hung-Min Hsu,\n  Jiarui Cai, Jie Mei, Chia-Ying Yeh, Jenq-Neng Hwang, Michael Xin, Zhongkai\n  Shangguan, Zihe Zheng, Xu Yifei, Lehan Yang, Kele Xu, Min Feng", "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge", "comments": "10 pages, 1 figure. Conference on Computer Vision and Pattern\n  Recognition", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2021, 588-595", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:55:08 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Jerrick", ""], ["Inkawhich", "Nathan", ""], ["Nina", "Oliver", ""], ["Timofte", "Radu", ""], ["Jain", "Sahil", ""], ["Lee", "Bob", ""], ["Duan", "Yuru", ""], ["Wei", "Wei", ""], ["Zhang", "Lei", ""], ["Xu", "Songzheng", ""], ["Sun", "Yuxuan", ""], ["Tang", "Jiaqi", ""], ["Geng", "Xueli", ""], ["Ma", "Mengru", ""], ["Li", "Gongzhe", ""], ["Geng", "Xueli", ""], ["Cai", "Huanqia", ""], ["Cai", "Chengxue", ""], ["Cummings", "Sol", ""], ["Miron", "Casian", ""], ["Pasarica", "Alexandru", ""], ["Yang", "Cheng-Yen", ""], ["Hsu", "Hung-Min", ""], ["Cai", "Jiarui", ""], ["Mei", "Jie", ""], ["Yeh", "Chia-Ying", ""], ["Hwang", "Jenq-Neng", ""], ["Xin", "Michael", ""], ["Shangguan", "Zhongkai", ""], ["Zheng", "Zihe", ""], ["Yifei", "Xu", ""], ["Yang", "Lehan", ""], ["Xu", "Kele", ""], ["Feng", "Min", ""]]}, {"id": "2107.01194", "submitter": "Lin Zhang", "authors": "Lin Zhang, Qi She, Zhengyang Shen, Changhu Wang", "title": "How Incomplete is Contrastive Learning? An Inter-intra Variant Dual\n  Representation Method for Self-supervised Video Recognition", "comments": "10 pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive learning applied to self-supervised representation learning has\nseen a resurgence in deep models. In this paper, we find that existing\ncontrastive learning based solutions for self-supervised video recognition\nfocus on inter-variance encoding but ignore the intra-variance existing in\nclips within the same video. We thus propose to learn dual representations for\neach clip which (\\romannumeral 1) encode intra-variance through a shuffle-rank\npretext task; (\\romannumeral 2) encode inter-variance through a temporal\ncoherent contrastive loss. Experiment results show that our method plays an\nessential role in balancing inter and intra variances and brings consistent\nperformance gains on multiple backbones and contrastive learning frameworks.\nIntegrated with SimCLR and pretrained on Kinetics-400, our method achieves\n$\\textbf{82.0\\%}$ and $\\textbf{51.2\\%}$ downstream classification accuracy on\nUCF101 and HMDB51 test sets respectively and $\\textbf{46.1\\%}$ video retrieval\naccuracy on UCF101, outperforming both pretext-task based and contrastive\nlearning based counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 17:03:04 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 02:07:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Lin", ""], ["She", "Qi", ""], ["Shen", "Zhengyang", ""], ["Wang", "Changhu", ""]]}, {"id": "2107.01205", "submitter": "Vladislav Golyanik", "authors": "Jameel Malik and Soshi Shimada and Ahmed Elhayek and Sk Aziz Ali and\n  Christian Theobalt and Vladislav Golyanik and Didier Stricker", "title": "HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural\n  Networks", "comments": "13 pages, 6 tables, 7 figures; project webpage:\n  https://gvv.mpi-inf.mpg.de/projects/HandVoxNet++/. arXiv admin note: text\n  overlap with arXiv:2004.01588", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand shape and pose estimation from a single depth map is a new and\nchallenging computer vision problem with many applications. Existing methods\naddressing it directly regress hand meshes via 2D convolutional neural\nnetworks, which leads to artifacts due to perspective distortions in the\nimages. To address the limitations of the existing methods, we develop\nHandVoxNet++, i.e., a voxel-based deep network with 3D and graph convolutions\ntrained in a fully supervised manner. The input to our network is a 3D\nvoxelized-depth-map-based on the truncated signed distance function (TSDF).\nHandVoxNet++ relies on two hand shape representations. The first one is the 3D\nvoxelized grid of hand shape, which does not preserve the mesh topology and\nwhich is the most accurate representation. The second representation is the\nhand surface that preserves the mesh topology. We combine the advantages of\nboth representations by aligning the hand surface to the voxelized hand shape\neither with a new neural Graph-Convolutions-based Mesh Registration\n(GCN-MeshReg) or classical segment-wise Non-Rigid Gravitational Approach\n(NRGA++) which does not rely on training data. In extensive evaluations on\nthree public benchmarks, i.e., SynHand5M, depth-based HANDS19 challenge and\nHO-3D, the proposed HandVoxNet++ achieves the state-of-the-art performance. In\nthis journal extension of our previous approach presented at CVPR 2020, we gain\n41.09% and 13.7% higher shape alignment accuracy on SynHand5M and HANDS19\ndatasets, respectively. Our method is ranked first on the HANDS19 challenge\ndataset (Task 1: Depth-Based 3D Hand Pose Estimation) at the moment of the\nsubmission of our results to the portal in August 2020.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 17:59:54 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Malik", "Jameel", ""], ["Shimada", "Soshi", ""], ["Elhayek", "Ahmed", ""], ["Ali", "Sk Aziz", ""], ["Theobalt", "Christian", ""], ["Golyanik", "Vladislav", ""], ["Stricker", "Didier", ""]]}, {"id": "2107.01248", "submitter": "Vinod Kumar Kurmi", "authors": "Indu Joshi and Ayush Utkarsh and Riya Kothari and Vinod K Kurmi and\n  Antitza Dantcheva and Sumantra Dutta Roy and Prem Kumar Kalra", "title": "Data Uncertainty Guided Noise-aware Preprocessing Of Fingerprints", "comments": "IJCNN 2021 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The effectiveness of fingerprint-based authentication systems on good quality\nfingerprints is established long back. However, the performance of standard\nfingerprint matching systems on noisy and poor quality fingerprints is far from\nsatisfactory. Towards this, we propose a data uncertainty-based framework which\nenables the state-of-the-art fingerprint preprocessing models to quantify noise\npresent in the input image and identify fingerprint regions with background\nnoise and poor ridge clarity. Quantification of noise helps the model two\nfolds: firstly, it makes the objective function adaptive to the noise in a\nparticular input fingerprint and consequently, helps to achieve robust\nperformance on noisy and distorted fingerprint regions. Secondly, it provides a\nnoise variance map which indicates noisy pixels in the input fingerprint image.\nThe predicted noise variance map enables the end-users to understand erroneous\npredictions due to noise present in the input image. Extensive experimental\nevaluation on 13 publicly available fingerprint databases, across different\narchitectural choices and two fingerprint processing tasks demonstrate\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 19:47:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Joshi", "Indu", ""], ["Utkarsh", "Ayush", ""], ["Kothari", "Riya", ""], ["Kurmi", "Vinod K", ""], ["Dantcheva", "Antitza", ""], ["Roy", "Sumantra Dutta", ""], ["Kalra", "Prem Kumar", ""]]}, {"id": "2107.01273", "submitter": "Naftali Cohen", "authors": "Naftali Cohen, Srijan Sood, Zhen Zeng, Tucker Balch, Manuela Veloso", "title": "Visual Time Series Forecasting: An Image-driven Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-fin.ST q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 20:59:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Cohen", "Naftali", ""], ["Sood", "Srijan", ""], ["Zeng", "Zhen", ""], ["Balch", "Tucker", ""], ["Veloso", "Manuela", ""]]}, {"id": "2107.01284", "submitter": "Fahim Faisal Niloy", "authors": "Fahim Faisal Niloy, Arif, Abu Bakar Siddik Nayem, Anis Sarker, Ovi\n  Paul, M. Ashraful Amin, Amin Ahsan Ali, Moinul Islam Zaber, AKM Mahbubur\n  Rahman", "title": "A Novel Disaster Image Dataset and Characteristics Analysis using\n  Attention Model", "comments": "ICPR 2020", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9412504", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advancement of deep learning technology has enabled us to develop systems\nthat outperform any other classification technique. However, success of any\nempirical system depends on the quality and diversity of the data available to\ntrain the proposed system. In this research, we have carefully accumulated a\nrelatively challenging dataset that contains images collected from various\nsources for three different disasters: fire, water and land. Besides this, we\nhave also collected images for various damaged infrastructure due to natural or\nman made calamities and damaged human due to war or accidents. We have also\naccumulated image data for a class named non-damage that contains images with\nno such disaster or sign of damage in them. There are 13,720 manually annotated\nimages in this dataset, each image is annotated by three individuals. We are\nalso providing discriminating image class information annotated manually with\nbounding box for a set of 200 test images. Images are collected from different\nnews portals, social media, and standard datasets made available by other\nresearchers. A three layer attention model (TLAM) is trained and average five\nfold validation accuracy of 95.88% is achieved. Moreover, on the 200 unseen\ntest images this accuracy is 96.48%. We also generate and compare attention\nmaps for these test images to determine the characteristics of the trained\nattention model. Our dataset is available at\nhttps://niloy193.github.io/Disaster-Dataset\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 21:18:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Niloy", "Fahim Faisal", ""], ["Arif", "", ""], ["Nayem", "Abu Bakar Siddik", ""], ["Sarker", "Anis", ""], ["Paul", "Ovi", ""], ["Amin", "M. Ashraful", ""], ["Ali", "Amin Ahsan", ""], ["Zaber", "Moinul Islam", ""], ["Rahman", "AKM Mahbubur", ""]]}, {"id": "2107.01318", "submitter": "Daniel Mario Lima", "authors": "Marcelo Toledo, Daniel Lima, Jos\\'e Krieger, Marco Gutierrez", "title": "A study of CNN capacity applied to Left Venticle Segmentation in Cardiac\n  MRI", "comments": "Submitted to IJBRA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  CNN (Convolutional Neural Network) models have been successfully used for\nsegmentation of the left ventricle (LV) in cardiac MRI (Magnetic Resonance\nImaging), providing clinical measurements.In practice, two questions arise with\ndeployment of CNNs: 1) when is it better to use a shallow model instead of a\ndeeper one? 2) how the size of a dataset might change the network performance?\nWe propose a framework to answer them, by experimenting with deep and shallow\nversions of three U-Net families, trained from scratch in six subsets varying\nfrom 100 to 10,000 images, different network sizes, learning rates and\nregularization values. 1620 models were evaluated using 5-foldcross-validation\nby loss and DICE. The results indicate that: sample size affects performance\nmore than architecture or hyper-parameters; in small samples the performance is\nmore sensitive to hyper-parameters than architecture; the performance\ndifference between shallow and deeper networks is not the same across families.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 00:56:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Toledo", "Marcelo", ""], ["Lima", "Daniel", ""], ["Krieger", "Jos\u00e9", ""], ["Gutierrez", "Marco", ""]]}, {"id": "2107.01319", "submitter": "Yifan Xing", "authors": "Yifan Xing, Tong He, Tianjun Xiao, Yongxin Wang, Yuanjun Xiong, Wei\n  Xia, David Wipf, Zheng Zhang, Stefano Soatto", "title": "Learning Hierarchical Graph Neural Networks for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a hierarchical graph neural network (GNN) model that learns how to\ncluster a set of images into an unknown number of identities using a training\nset of images annotated with labels belonging to a disjoint set of identities.\nOur hierarchical GNN uses a novel approach to merge connected components\npredicted at each level of the hierarchy to form a new graph at the next level.\nUnlike fully unsupervised hierarchical clustering, the choice of grouping and\ncomplexity criteria stems naturally from supervision in the training set. The\nresulting method, Hi-LANDER, achieves an average of 54% improvement in F-score\nand 8% increase in Normalized Mutual Information (NMI) relative to current\nGNN-based clustering algorithms. Additionally, state-of-the-art GNN-based\nmethods rely on separate models to predict linkage probabilities and node\ndensities as intermediate steps of the clustering process. In contrast, our\nunified framework achieves a seven-fold decrease in computational cost. We\nrelease our training and inference code at\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 01:28:42 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 14:50:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Xing", "Yifan", ""], ["He", "Tong", ""], ["Xiao", "Tianjun", ""], ["Wang", "Yongxin", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Wipf", "David", ""], ["Zhang", "Zheng", ""], ["Soatto", "Stefano", ""]]}, {"id": "2107.01327", "submitter": "Huy Hieu Pham", "authors": "Hoang C. Nguyen and Tung T. Le and Hieu H. Pham and Ha Q. Nguyen", "title": "VinDr-RibCXR: A Benchmark Dataset for Automatic Segmentation and\n  Labeling of Individual Ribs on Chest X-rays", "comments": "This is a preprint of our paper, which was accepted for publication\n  by Medical Imaging with Deep Learning (MIDL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new benchmark dataset, namely VinDr-RibCXR, for automatic\nsegmentation and labeling of individual ribs from chest X-ray (CXR) scans. The\nVinDr-RibCXR contains 245 CXRs with corresponding ground truth annotations\nprovided by human experts. A set of state-of-the-art segmentation models are\ntrained on 196 images from the VinDr-RibCXR to segment and label 20 individual\nribs. Our best performing model obtains a Dice score of 0.834 (95% CI,\n0.810--0.853) on an independent test set of 49 images. Our study, therefore,\nserves as a proof of concept and baseline performance for future research.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 02:36:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Nguyen", "Hoang C.", ""], ["Le", "Tung T.", ""], ["Pham", "Hieu H.", ""], ["Nguyen", "Ha Q.", ""]]}, {"id": "2107.01330", "submitter": "Md Nazmul Karim", "authors": "Nazmul Karim and Nazanin Rahnavard", "title": "SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Single-pixel imaging is a novel imaging scheme that has gained popularity due\nto its huge computational gain and potential for a low-cost alternative to\nimaging beyond the visible spectrum. The traditional reconstruction methods\nstruggle to produce a clear recovery when one limits the number of illumination\npatterns from a spatial light modulator. As a remedy, several\ndeep-learning-based solutions have been proposed which lack good generalization\nability due to the architectural setup and loss functions. In this paper, we\npropose a generative adversarial network-based reconstruction framework for\nsingle-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images\nwith 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This\nfacilitates much faster reconstruction making our method suitable for\nsingle-pixel video. Furthermore, our ResNet-like architecture for the generator\nleads to useful representation learning that allows us to reconstruct\ncompletely unseen objects. The experimental results demonstrate that SPI-GAN\nachieves significant performance gain, e.g. near 3dB PSNR gain, over the\ncurrent state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 03:06:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Karim", "Nazmul", ""], ["Rahnavard", "Nazanin", ""]]}, {"id": "2107.01337", "submitter": "Md Selim", "authors": "Md Selim, Jie Zhang, Baowei Fei, Guo-Qiang Zhang, Jin Chen", "title": "CT Image Harmonization for Enhancing Radiomics Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While remarkable advances have been made in Computed Tomography (CT),\ncapturing CT images with non-standardized protocols causes low reproducibility\nregarding radiomic features, forming a barrier on CT image analysis in a large\nscale. RadiomicGAN is developed to effectively mitigate the discrepancy caused\nby using non-standard reconstruction kernels. RadiomicGAN consists of hybrid\nneural blocks including both pre-trained and trainable layers adopted to learn\nradiomic feature distributions efficiently. A novel training approach, called\nDynamic Window-based Training, has been developed to smoothly transform the\npre-trained model to the medical imaging domain. Model performance evaluated\nusing 1401 radiomic features show that RadiomicGAN clearly outperforms the\nstate-of-art image standardization models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 04:03:42 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Selim", "Md", ""], ["Zhang", "Jie", ""], ["Fei", "Baowei", ""], ["Zhang", "Guo-Qiang", ""], ["Chen", "Jin", ""]]}, {"id": "2107.01349", "submitter": "Dong-Wan Choi", "authors": "Jong-Yeong Kim and Dong-Wan Choi", "title": "Split-and-Bridge: Adaptable Class Incremental Learning within a Single\n  Neural Network", "comments": "In AAAI-2021", "journal-ref": "In Proceedings of the AAAI Conference on Artificial Intelligence\n  (Vol. 35, No. 9, pp. 8137-8145) 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning has been a major problem in the deep learning community,\nwhere the main challenge is how to effectively learn a series of newly arriving\ntasks without forgetting the knowledge of previous tasks. Initiated by Learning\nwithout Forgetting (LwF), many of the existing works report that knowledge\ndistillation is effective to preserve the previous knowledge, and hence they\ncommonly use a soft label for the old task, namely a knowledge distillation\n(KD) loss, together with a class label for the new task, namely a cross entropy\n(CE) loss, to form a composite loss for a single neural network. However, this\napproach suffers from learning the knowledge by a CE loss as a KD loss often\nmore strongly influences the objective function when they are in a competitive\nsituation within a single network. This could be a critical problem\nparticularly in a class incremental scenario, where the knowledge across tasks\nas well as within the new task, both of which can only be acquired by a CE\nloss, is essentially learned due to the existence of a unified classifier. In\nthis paper, we propose a novel continual learning method, called\nSplit-and-Bridge, which can successfully address the above problem by partially\nsplitting a neural network into two partitions for training the new task\nseparated from the old task and re-connecting them for learning the knowledge\nacross tasks. In our thorough experimental analysis, our Split-and-Bridge\nmethod outperforms the state-of-the-art competitors in KD-based continual\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 05:51:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kim", "Jong-Yeong", ""], ["Choi", "Dong-Wan", ""]]}, {"id": "2107.01351", "submitter": "Jun Wang", "authors": "Jun Wang, Xiaohan Yu and Yongsheng Gao", "title": "EAR-NET: Error Attention Refining Network For Retinal Vessel\n  Segmentation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise detection of blood vessels in retinal images is crucial to the\nearly diagnosis of the retinal vascular diseases, e.g., diabetic, hypertensive\nand solar retinopathies. Existing works often fail in predicting the abnormal\nareas, e.g, sudden brighter and darker areas and are inclined to predict a\npixel to background due to the significant class imbalance, leading to high\naccuracy and specificity while low sensitivity. To that end, we propose a novel\nerror attention refining network (ERA-Net) that is capable of learning and\npredicting the potential false predictions in a two-stage manner for effective\nretinal vessel segmentation. The proposed ERA-Net in the refine stage drives\nthe model to focus on and refine the segmentation errors produced in the\ninitial training stage. To achieve this, unlike most previous attention\napproaches that run in an unsupervised manner, we introduce a novel error\nattention mechanism which considers the differences between the ground truth\nand the initial segmentation masks as the ground truth to supervise the\nattention map learning. Experimental results demonstrate that our method\nachieves state-of-the-art performance on two common retinal blood vessel\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 06:03:46 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Xiaohan", ""], ["Gao", "Yongsheng", ""]]}, {"id": "2107.01358", "submitter": "Girish Varma", "authors": "Sandeep Nagar, Marius Dufraisse, Girish Varma", "title": "CInC Flow: Characterizable Invertible 3x3 Convolution", "comments": "Accepted for the 4th Workshop on Tractable Probabilistic\n  Modeling,(UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 06:55:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Nagar", "Sandeep", ""], ["Dufraisse", "Marius", ""], ["Varma", "Girish", ""]]}, {"id": "2107.01361", "submitter": "Vinod Kumar Kurmi", "authors": "Indu Joshi and Ayush Utkarsh and Riya Kothari and Vinod K Kurmi and\n  Antitza Dantcheva and Sumantra Dutta Roy and Prem Kumar Kalra", "title": "Sensor-invariant Fingerprint ROI Segmentation Using Recurrent\n  Adversarial Learning", "comments": "IJCNN 2021 (Accepted)", "journal-ref": "IJCNN 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A fingerprint region of interest (roi) segmentation algorithm is designed to\nseparate the foreground fingerprint from the background noise. All the learning\nbased state-of-the-art fingerprint roi segmentation algorithms proposed in the\nliterature are benchmarked on scenarios when both training and testing\ndatabases consist of fingerprint images acquired from the same sensors.\nHowever, when testing is conducted on a different sensor, the segmentation\nperformance obtained is often unsatisfactory. As a result, every time a new\nfingerprint sensor is used for testing, the fingerprint roi segmentation model\nneeds to be re-trained with the fingerprint image acquired from the new sensor\nand its corresponding manually marked ROI. Manually marking fingerprint ROI is\nexpensive because firstly, it is time consuming and more importantly, requires\ndomain expertise. In order to save the human effort in generating annotations\nrequired by state-of-the-art, we propose a fingerprint roi segmentation model\nwhich aligns the features of fingerprint images derived from the unseen sensor\nsuch that they are similar to the ones obtained from the fingerprints whose\nground truth roi masks are available for training. Specifically, we propose a\nrecurrent adversarial learning based feature alignment network that helps the\nfingerprint roi segmentation model to learn sensor-invariant features.\nConsequently, sensor-invariant features learnt by the proposed roi segmentation\nmodel help it to achieve improved segmentation performance on fingerprints\nacquired from the new sensor. Experiments on publicly available FVC databases\ndemonstrate the efficacy of the proposed work.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 07:16:39 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Joshi", "Indu", ""], ["Utkarsh", "Ayush", ""], ["Kothari", "Riya", ""], ["Kurmi", "Vinod K", ""], ["Dantcheva", "Antitza", ""], ["Roy", "Sumantra Dutta", ""], ["Kalra", "Prem Kumar", ""]]}, {"id": "2107.01378", "submitter": "Ding Jia", "authors": "Ding Jia, Kai Han, Yunhe Wang, Yehui Tang, Jianyuan Guo, Chao Zhang,\n  Dacheng Tao", "title": "Efficient Vision Transformers via Fine-Grained Manifold Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the model compression problem of vision transformers.\nBenefit from the self-attention module, transformer architectures have shown\nextraordinary performance on many computer vision tasks. Although the network\nperformance is boosted, transformers are often required more computational\nresources including memory usage and the inference complexity. Compared with\nthe existing knowledge distillation approaches, we propose to excavate useful\ninformation from the teacher transformer through the relationship between\nimages and the divided patches. We then explore an efficient fine-grained\nmanifold distillation approach that simultaneously calculates cross-images,\ncross-patch, and random-selected manifolds in teacher and student models.\nExperimental results conducted on several benchmarks demonstrate the\nsuperiority of the proposed algorithm for distilling portable transformer\nmodels with higher performance. For example, our approach achieves 75.06% Top-1\naccuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which\noutperforms other ViT distillation methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 08:28:34 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 03:48:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jia", "Ding", ""], ["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Tang", "Yehui", ""], ["Guo", "Jianyuan", ""], ["Zhang", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "2107.01392", "submitter": "Ayushe Gangal", "authors": "Peeyush Kumar, Ayushe Gangal and Sunita Kumari", "title": "WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative\n  Cases and Vaticinating the Probability of Maturation to ARDS using\n  Posteroanterior Chest X-Rays", "comments": "10 pages, 4 figures, 1 table", "journal-ref": "J Pure Appl Microbiol. 2020;14(suppl 1):869-878, Article Number:\n  6236", "doi": "10.22207/JPAM.14.SPL1.24", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Coronavirus is a large virus family consisting of diverse viruses, some of\nwhich disseminate among mammals and others cause sickness among humans.\nCOVID-19 is highly contagious and is rapidly spreading, rendering its early\ndiagnosis of preeminent status. Researchers, medical specialists and\norganizations all over the globe have been working tirelessly to combat this\nvirus and help in its containment. In this paper, a novel neural network called\nWisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays.\nThe WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is\na two-layered convolutional Neural Network (CNN), which takes chest x-ray\nimages as input. Both layers of the proposed neural network consist of a number\nof neural networks each. The dataset used for this study consists of chest\nx-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on\nGitHub, and the chest x-ray images of healthy lungs and lungs affected by viral\nand bacterial pneumonia were obtained from Kaggle. The network not only\npinpoints the presence of COVID-19, but also gives the probability of the\ndisease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus,\npredicting the progression of the disease in the COVID-19 positive patients.\nThe network also slender the occurrences of false negative cases by employing a\nhigh threshold value, thus aids in curbing the spread of the disease and gives\nan accuracy of 100% for successfully predicting COVID-19 among the chest x-rays\nof patients affected with COVID-19, bacterial and viral pneumonia.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 09:55:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kumar", "Peeyush", ""], ["Gangal", "Ayushe", ""], ["Kumari", "Sunita", ""]]}, {"id": "2107.01396", "submitter": "Yajie Wang", "authors": "Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an Tan and\n  Quanxin Zhang", "title": "Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations\n  with Perceptual Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been found to be vulnerable to adversarial\nexamples. Adversarial examples are malicious images with visually imperceptible\nperturbations. While these carefully crafted perturbations restricted with\ntight $\\Lp$ norm bounds are small, they are still easily perceivable by humans.\nThese perturbations also have limited success rates when attacking black-box\nmodels or models with defenses like noise reduction filters. To solve these\nproblems, we propose Demiguise Attack, crafting ``unrestricted'' perturbations\nwith Perceptual Similarity. Specifically, we can create powerful and\nphotorealistic adversarial examples by manipulating semantic information based\non Perceptual Similarity. Adversarial examples we generate are friendly to the\nhuman visual system (HVS), although the perturbations are of large magnitudes.\nWe extend widely-used attacks with our approach, enhancing adversarial\neffectiveness impressively while contributing to imperceptibility. Extensive\nexperiments show that the proposed method not only outperforms various\nstate-of-the-art attacks in terms of fooling rate, transferability, and\nrobustness against defenses but can also improve attacks effectively. In\naddition, we also notice that our implementation can simulate illumination and\ncontrast changes that occur in real-world scenarios, which will contribute to\nexposing the blind spots of DNNs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 10:14:01 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Yajie", ""], ["Wu", "Shangbo", ""], ["Jiang", "Wenyi", ""], ["Hao", "Shengang", ""], ["Tan", "Yu-an", ""], ["Zhang", "Quanxin", ""]]}, {"id": "2107.01401", "submitter": "Ivan Y. Tyukin", "authors": "Santos J. N\\'u\\~nez Jare\\~no, Dani\\\"el P. van Helden, Evgeny M.\n  Mirkes, Ivan Y. Tyukin, Penelope M. Allison", "title": "Learning from scarce information: using synthetic data to classify Roman\n  fine ware pottery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we consider a version of the challenging problem of learning\nfrom datasets whose size is too limited to allow generalisation beyond the\ntraining set. To address the challenge we propose to use a transfer learning\napproach whereby the model is first trained on a synthetic dataset replicating\nfeatures of the original objects. In this study the objects were smartphone\nphotographs of near-complete Roman terra sigillata pottery vessels from the\ncollection of the Museum of London. Taking the replicated features from\npublished profile drawings of pottery forms allowed the integration of expert\nknowledge into the process through our synthetic data generator. After this\nfirst initial training the model was fine-tuned with data from photographs of\nreal vessels. We show, through exhaustive experiments across several popular\ndeep learning architectures, different test priors, and considering the impact\nof the photograph viewpoint and excessive damage to the vessels, that the\nproposed hybrid approach enables the creation of classifiers with appropriate\ngeneralisation performance. This performance is significantly better than that\nof classifiers trained exclusively on the original data which shows the promise\nof the approach to alleviate the fundamental issue of learning from small\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 10:30:46 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Jare\u00f1o", "Santos J. N\u00fa\u00f1ez", ""], ["van Helden", "Dani\u00ebl P.", ""], ["Mirkes", "Evgeny M.", ""], ["Tyukin", "Ivan Y.", ""], ["Allison", "Penelope M.", ""]]}, {"id": "2107.01422", "submitter": "Shiqi Xu", "authors": "Shiqi Xu, Xi Yang, Wenhui Liu, Joakim Jonsson, Ruobing Qian, Pavan\n  Chandra Konda, Kevin C. Zhou, Qionghai Dai, Haoqian Wang, Edouard Berrocal,\n  Roarke Horstmeyer", "title": "Imaging dynamics beneath turbid media via parallelized single-photon\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV eess.IV q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard methods aim to form images based upon optical absorption or\nfluorescent emission, it is also well-established that the temporal correlation\nof scattered coherent light diffuses through tissue much like optical\nintensity. Few works to date, however, have aimed to experimentally measure and\nprocess such data to demonstrate deep-tissue imaging of decorrelation dynamics.\nIn this work, we take advantage of a single-photon avalanche diode (SPAD) array\ncamera, with over one thousand detectors, to simultaneously detect speckle\nfluctuations at the single-photon level from 12 different phantom tissue\nsurface locations delivered via a customized fiber bundle array. We then apply\na deep neural network to convert the acquired single-photon measurements into\nvideo of scattering dynamics beneath rapidly decorrelating liquid tissue\nphantoms. We demonstrate the ability to record video of dynamic events\noccurring 5-8 mm beneath a decorrelating tissue phantom with mm-scale\nresolution and at a 2.5-10 Hz frame rate.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 12:32:21 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 17:37:21 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Xu", "Shiqi", ""], ["Yang", "Xi", ""], ["Liu", "Wenhui", ""], ["Jonsson", "Joakim", ""], ["Qian", "Ruobing", ""], ["Konda", "Pavan Chandra", ""], ["Zhou", "Kevin C.", ""], ["Dai", "Qionghai", ""], ["Wang", "Haoqian", ""], ["Berrocal", "Edouard", ""], ["Horstmeyer", "Roarke", ""]]}, {"id": "2107.01435", "submitter": "Roozbeh Rajabi", "authors": "Fatemeh Mahdavi, Roozbeh Rajabi", "title": "Drone Detection Using Convolutional Neural Networks", "comments": "5 pages, conference", "journal-ref": null, "doi": "10.1109/ICSPIS51611.2020.9349620", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In image processing, it is essential to detect and track air targets,\nespecially UAVs. In this paper, we detect the flying drone using a fisheye\ncamera. In the field of diagnosis and classification of objects, there are\nalways many problems that prevent the development of rapid and significant\nprogress in this area. During the previous decades, a couple of advanced\nclassification methods such as convolutional neural networks and support vector\nmachines have been developed. In this study, the drone was detected using three\nmethods of classification of convolutional neural network (CNN), support vector\nmachine (SVM), and nearest neighbor. The outcomes show that CNN, SVM, and\nnearest neighbor have total accuracy of 95%, 88%, and 80%, respectively.\nCompared with other classifiers with the same experimental conditions, the\naccuracy of the convolutional neural network classifier is satisfactory.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 13:26:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mahdavi", "Fatemeh", ""], ["Rajabi", "Roozbeh", ""]]}, {"id": "2107.01456", "submitter": "Huy Trinh Quoc", "authors": "Quoc Huy Trinh, Minh Van Nguyen", "title": "Custom Deep Neural Network for 3D Covid Chest CT-scan Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D CT-scan base on chest is one of the controversial topisc of the researcher\nnowadays. There are many tasks to diagnose the disease through CT-scan images,\ninclude Covid19. In this paper, we propose a method that custom and combine\nDeep Neural Network to classify the series of 3D CT-scans chest images. In our\nmethods, we experiment with 2 backbones is DenseNet 121 and ResNet 101. In this\nproposal, we separate the experiment into 2 tasks, one is for 2 backbones\ncombination of ResNet and DenseNet, one is for DenseNet backbones combination.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 15:54:38 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Trinh", "Quoc Huy", ""], ["Van Nguyen", "Minh", ""]]}, {"id": "2107.01469", "submitter": "Zangwei Zheng", "authors": "Zangwei Zheng, Xiangyu Yue, Kurt Keutzer, Alberto Sangiovanni\n  Vincentelli", "title": "Scene-aware Learning Network for Radar Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is essential to safe autonomous or assisted driving.\nPrevious works usually utilize RGB images or LiDAR point clouds to identify and\nlocalize multiple objects in self-driving. However, cameras tend to fail in bad\ndriving conditions, e.g. bad weather or weak lighting, while LiDAR scanners are\ntoo expensive to get widely deployed in commercial applications. Radar has been\ndrawing more and more attention due to its robustness and low cost. In this\npaper, we propose a scene-aware radar learning framework for accurate and\nrobust object detection. First, the learning framework contains branches\nconditioning on the scene category of the radar sequence; with each branch\noptimized for a specific type of scene. Second, three different 3D\nautoencoder-based architectures are proposed for radar object detection and\nensemble learning is performed over the different architectures to further\nboost the final performance. Third, we propose novel scene-aware sequence mix\naugmentation (SceneMix) and scene-specific post-processing to generate more\nrobust detection results. In the ROD2021 Challenge, we achieved a final result\nof average precision of 75.0% and an average recall of 81.0%. Moreover, in the\nparking lot scene, our framework ranks first with an average precision of 97.8%\nand an average recall of 98.6%, which demonstrates the effectiveness of our\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 17:19:56 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zheng", "Zangwei", ""], ["Yue", "Xiangyu", ""], ["Keutzer", "Kurt", ""], ["Vincentelli", "Alberto Sangiovanni", ""]]}, {"id": "2107.01502", "submitter": "Hejie Cui", "authors": "Hejie Cui, Xinglong Liu, Ning Huang", "title": "Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest\n  CT Images", "comments": "Published in Medical Image Computing and Computer Assisted\n  Intervention (MICCAI 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_33", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary vessel segmentation is important for clinical diagnosis of\npulmonary diseases, while is also challenging due to the complicated structure.\nIn this work, we present an effective framework and refinement process of\npulmonary vessel segmentation from chest computed tomographic (CT) images. The\nkey to our approach is a 2.5D segmentation network applied from three\northogonal axes, which presents a robust and fully automated pulmonary vessel\nsegmentation result with lower network complexity and memory usage compared to\n3D networks. The slice radius is introduced to convolve the adjacent\ninformation of the center slice and the multi-planar fusion optimizes the\npresentation of intra- and inter- slice features. Besides, the tree-like\nstructure of the pulmonary vessel is extracted in the post-processing process,\nwhich is used for segmentation refining and pruning. In the evaluation\nexperiments, three fusion methods are tested and the most promising one is\ncompared with the state-of-the-art 2D and 3D structures on 300 cases of lung\nimages randomly selected from LIDC dataset. Our method outperforms other\nnetwork structures by a large margin and achieves by far the highest average\nDICE score of 0.9272 and precision of 0.9310, as per our knowledge from the\npulmonary vessel segmentation models available in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 21:46:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Cui", "Hejie", ""], ["Liu", "Xinglong", ""], ["Huang", "Ning", ""]]}, {"id": "2107.01527", "submitter": "Arash Mohammadi", "authors": "Nastaran Enshaei, Anastasia Oikonomou, Moezedin Javad Rafiee, Parnian\n  Afshar, Shahin Heidarian, Arash Mohammadi, Konstantinos N. Plataniotis, and\n  Farnoosh Naderkhani", "title": "COVID-Rate: An Automated Framework for Segmentation of COVID-19 Lesions\n  from Chest CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novel Coronavirus disease (COVID-19) is a highly contagious respiratory\ninfection that has had devastating effects on the world. Recently, new COVID-19\nvariants are emerging making the situation more challenging and threatening.\nEvaluation and quantification of COVID-19 lung abnormalities based on chest\nComputed Tomography (CT) scans can help determining the disease stage,\nefficiently allocating limited healthcare resources, and making informed\ntreatment decisions. During pandemic era, however, visual assessment and\nquantification of COVID-19 lung lesions by expert radiologists become expensive\nand prone to error, which raises an urgent quest to develop practical\nautonomous solutions. In this context, first, the paper introduces an open\naccess COVID-19 CT segmentation dataset containing 433 CT images from 82\npatients that have been annotated by an expert radiologist. Second, a Deep\nNeural Network (DNN)-based framework is proposed, referred to as the\nCOVID-Rate, that autonomously segments lung abnormalities associated with\nCOVID-19 from chest CT scans. Performance of the proposed COVID-Rate framework\nis evaluated through several experiments based on the introduced and external\ndatasets. The results show a dice score of 0:802 and specificity and\nsensitivity of 0:997 and 0:832, respectively. Furthermore, the results indicate\nthat the COVID-Rate model can efficiently segment COVID-19 lesions in both 2D\nCT images and whole lung volumes. Results on the external dataset illustrate\ngeneralization capabilities of the COVID-Rate model to CT images obtained from\na different scanner.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 03:19:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Enshaei", "Nastaran", ""], ["Oikonomou", "Anastasia", ""], ["Rafiee", "Moezedin Javad", ""], ["Afshar", "Parnian", ""], ["Heidarian", "Shahin", ""], ["Mohammadi", "Arash", ""], ["Plataniotis", "Konstantinos N.", ""], ["Naderkhani", "Farnoosh", ""]]}, {"id": "2107.01547", "submitter": "Zhihao Wang", "authors": "Zhihao Wang, Yanwei Yu, Yibo Wang, Haixu Long, and Fazheng Wang", "title": "Robust End-to-End Offline Chinese Handwriting Text Page Spotter with\n  Text Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offline Chinese handwriting text recognition is a long-standing research\ntopic in the field of pattern recognition. In previous studies, text detection\nand recognition are separated, which leads to the fact that text recognition is\nhighly dependent on the detection results. In this paper, we propose a robust\nend-to-end Chinese text page spotter framework. It unifies text detection and\ntext recognition with text kernel that integrates global text feature\ninformation to optimize the recognition from multiple scales, which reduces the\ndependence of detection and improves the robustness of the system. Our method\nachieves state-of-the-art results on the CASIA-HWDB2.0-2.2 dataset and\nICDAR-2013 competition dataset. Without any language model, the correct rates\nare 99.12% and 94.27% for line-level recognition, and 99.03% and 94.20% for\npage-level recognition, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:42:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Zhihao", ""], ["Yu", "Yanwei", ""], ["Wang", "Yibo", ""], ["Long", "Haixu", ""], ["Wang", "Fazheng", ""]]}, {"id": "2107.01548", "submitter": "Mingbo Hong", "authors": "Mingbo Hong, Shuiwang Li, Yuchao Yang, Feiyu Zhu, Qijun Zhao and Li Lu", "title": "SSPNet: Scale Selection Pyramid Network for Tiny Person Detection from\n  UAV Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing demand for search and rescue, it is highly demanded to\ndetect objects of interest in large-scale images captured by Unmanned Aerial\nVehicles (UAVs), which is quite challenging due to extremely small scales of\nobjects. Most existing methods employed Feature Pyramid Network (FPN) to enrich\nshallow layers' features by combing deep layers' contextual features. However,\nunder the limitation of the inconsistency in gradient computation across\ndifferent layers, the shallow layers in FPN are not fully exploited to detect\ntiny objects. In this paper, we propose a Scale Selection Pyramid network\n(SSPNet) for tiny person detection, which consists of three components: Context\nAttention Module (CAM), Scale Enhancement Module (SEM), and Scale Selection\nModule (SSM). CAM takes account of context information to produce hierarchical\nattention heatmaps. SEM highlights features of specific scales at different\nlayers, leading the detector to focus on objects of specific scales instead of\nvast backgrounds. SSM exploits adjacent layers' relationships to fulfill\nsuitable feature sharing between deep layers and shallow layers, thereby\navoiding the inconsistency in gradient computation across different layers.\nBesides, we propose a Weighted Negative Sampling (WNS) strategy to guide the\ndetector to select more representative samples. Experiments on the TinyPerson\nbenchmark show that our method outperforms other state-of-the-art (SOTA)\ndetectors.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:46:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hong", "Mingbo", ""], ["Li", "Shuiwang", ""], ["Yang", "Yuchao", ""], ["Zhu", "Feiyu", ""], ["Zhao", "Qijun", ""], ["Lu", "Li", ""]]}, {"id": "2107.01558", "submitter": "Xiaopeng Hong", "authors": "Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yunfeng Qiu, Yaowei\n  Wang, Yihong Gong", "title": "Direct Measure Matching for Crowd Counting", "comments": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional crowd counting approaches usually use Gaussian assumption to\ngenerate pseudo density ground truth, which suffers from problems like\ninaccurate estimation of the Gaussian kernel sizes. In this paper, we propose a\nnew measure-based counting approach to regress the predicted density maps to\nthe scattered point-annotated ground truth directly. First, crowd counting is\nformulated as a measure matching problem. Second, we derive a semi-balanced\nform of Sinkhorn divergence, based on which a Sinkhorn counting loss is\ndesigned for measure matching. Third, we propose a self-supervised mechanism by\ndevising a Sinkhorn scale consistency loss to resist scale changes. Finally, an\nefficient optimization method is provided to minimize the overall loss\nfunction. Extensive experiments on four challenging crowd counting datasets\nnamely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 06:37:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Lin", "Hui", ""], ["Hong", "Xiaopeng", ""], ["Ma", "Zhiheng", ""], ["Wei", "Xing", ""], ["Qiu", "Yunfeng", ""], ["Wang", "Yaowei", ""], ["Gong", "Yihong", ""]]}, {"id": "2107.01579", "submitter": "Linqing Zhao", "authors": "Linqing Zhao, Jiwen Lu and Jie Zhou", "title": "Similarity-Aware Fusion Network for 3D Semantic Segmentation", "comments": "Accepted by 2021 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 09:28:18 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 14:34:02 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 05:25:11 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhao", "Linqing", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.01619", "submitter": "Eungyeup Kim", "authors": "Eungyeup Kim, Sanghyeon Lee, Jeonghoon Park, Somi Choi, Choonghyun\n  Seo, Jaegul Choo", "title": "Deep Edge-Aware Interactive Colorization against Color-Bleeding Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep image colorization networks often suffer from the color-bleeding\nartifact, a problematic color spreading near the boundaries between adjacent\nobjects. The color-bleeding artifacts debase the reality of generated outputs,\nlimiting the applicability of colorization models on a practical application.\nAlthough previous approaches have tackled this problem in an automatic manner,\nthey often generate imperfect outputs because their enhancements are available\nonly in limited cases, such as having a high contrast of gray-scale value in an\ninput image. Instead, leveraging user interactions would be a promising\napproach, since it can help the edge correction in the desired regions. In this\npaper, we propose a novel edge-enhancing framework for the regions of interest,\nby utilizing user scribbles that indicate where to enhance. Our method requires\nminimal user effort to obtain satisfactory enhancements. Experimental results\non various datasets demonstrate that our interactive approach has outstanding\nperformance in improving color-bleeding artifacts against the existing\nbaselines.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 13:14:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kim", "Eungyeup", ""], ["Lee", "Sanghyeon", ""], ["Park", "Jeonghoon", ""], ["Choi", "Somi", ""], ["Seo", "Choonghyun", ""], ["Choo", "Jaegul", ""]]}, {"id": "2107.01671", "submitter": "Xuejiao Tang", "authors": "Xuejiao Tang, Xin Huang, Wenbin Zhang, Travers B. Child, Qiong Hu,\n  Zhen Liu and Ji Zhang", "title": "Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory", "comments": "DaWaK 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Visual Commonsense Reasoning (VCR) predicts an answer with corresponding\nrationale, given a question-image input. VCR is a recently introduced visual\nscene understanding task with a wide range of applications, including visual\nquestion answering, automated vehicle systems, and clinical decision support.\nPrevious approaches to solving the VCR task generally rely on pre-training or\nexploiting memory with long dependency relationship encoded models. However,\nthese approaches suffer from a lack of generalizability and prior knowledge. In\nthis paper we propose a dynamic working memory based cognitive VCR network,\nwhich stores accumulated commonsense between sentences to provide prior\nknowledge for inference. Extensive experiments show that the proposed model\nyields significant improvements over existing methods on the benchmark VCR\ndataset. Moreover, the proposed model provides intuitive interpretation into\nvisual commonsense reasoning. A Python implementation of our mechanism is\npublicly available at https://github.com/tanjatang/DMVCR\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 15:58:31 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 15:52:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tang", "Xuejiao", ""], ["Huang", "Xin", ""], ["Zhang", "Wenbin", ""], ["Child", "Travers B.", ""], ["Hu", "Qiong", ""], ["Liu", "Zhen", ""], ["Zhang", "Ji", ""]]}, {"id": "2107.01682", "submitter": "Xiaohong Gao Prof.", "authors": "Xiaohong Gao, Yu Qian, Alice Gao", "title": "COVID-VIT: Classification of COVID-19 from CT chest images based on\n  vision transformer models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is responding to the MIA-COV19 challenge to classify COVID from\nnon-COVID based on CT lung images. The COVID-19 virus has devastated the world\nin the last eighteen months by infecting more than 182 million people and\ncausing over 3.9 million deaths. The overarching aim is to predict the\ndiagnosis of the COVID-19 virus from chest radiographs, through the development\nof explainable vision transformer deep learning techniques, leading to\npopulation screening in a more rapid, accurate and transparent way. In this\ncompetition, there are 5381 three-dimensional (3D) datasets in total, including\n1552 for training, 374 for evaluation and 3455 for testing. While most of the\ndata volumes are in axial view, there are a number of subjects' data are in\ncoronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D\ndata based classification is investigated, in this competition, 2D images\nremains the main focus. Two deep learning methods are studied, which are vision\ntransformer (ViT) based on attention models and DenseNet that is built upon\nconventional convolutional neural network (CNN). Initial evaluation results\nbased on validation datasets whereby the ground truth is known indicate that\nViT performs better than DenseNet with F1 scores being 0.76 and 0.72\nrespectively. Codes are available at GitHub at\n<https://github/xiaohong1/COVID-ViT>.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 16:55:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Gao", "Xiaohong", ""], ["Qian", "Yu", ""], ["Gao", "Alice", ""]]}, {"id": "2107.01691", "submitter": "Haohang Xu", "authors": "Haohang Xu and Jiemin Fang and Xiaopeng Zhang and Lingxi Xie and\n  Xinggang Wang and Wenrui Dai and Hongkai Xiong and Qi Tian", "title": "Bag of Instances Aggregation Boosts Self-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-supervised learning have experienced remarkable\nprogress, especially for contrastive learning based methods, which regard each\nimage as well as its augmentations as an individual class and try to\ndistinguish them from all other images. However, due to the large quantity of\nexemplars, this kind of pretext task intrinsically suffers from slow\nconvergence and is hard for optimization. This is especially true for small\nscale models, which we find the performance drops dramatically comparing with\nits supervised counterpart. In this paper, we propose a simple but effective\ndistillation strategy for unsupervised learning. The highlight is that the\nrelationship among similar samples counts and can be seamlessly transferred to\nthe student to boost the performance. Our method, termed as BINGO, which is\nshort for \\textbf{B}ag of \\textbf{I}nsta\\textbf{N}ces\na\\textbf{G}gregati\\textbf{O}n, targets at transferring the relationship learned\nby the teacher to the student. Here bag of instances indicates a set of similar\nsamples constructed by the teacher and are grouped within a bag, and the goal\nof distillation is to aggregate compact representations over the student with\nrespect to instances in a bag. Notably, BINGO achieves new state-of-the-art\nperformance on small scale models, \\emph{i.e.}, 65.5% and 68.9% top-1\naccuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as\nbackbone, respectively, surpassing baselines (52.5% and 57.4% top-1 accuracies)\nby a significant margin. The code will be available at\n\\url{https://github.com/haohang96/bingo}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 17:33:59 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Xu", "Haohang", ""], ["Fang", "Jiemin", ""], ["Zhang", "Xiaopeng", ""], ["Xie", "Lingxi", ""], ["Wang", "Xinggang", ""], ["Dai", "Wenrui", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "2107.01748", "submitter": "Spyridon Thermos", "authors": "Spyridon Thermos, Xiao Liu, Alison O'Neil, Sotirios A. Tsaftaris", "title": "Controllable cardiac synthesis via disentangled anatomy arithmetic", "comments": "Accepted for publication in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Acquiring annotated data at scale with rare diseases or conditions remains a\nchallenge. It would be extremely useful to have a method that controllably\nsynthesizes images that can correct such underrepresentation. Assuming a proper\nlatent representation, the idea of a \"latent vector arithmetic\" could offer the\nmeans of achieving such synthesis. A proper representation must encode the\nfidelity of the input data, preserve invariance and equivariance, and permit\narithmetic operations. Motivated by the ability to disentangle images into\nspatial anatomy (tensor) factors and accompanying imaging (vector)\nrepresentations, we propose a framework termed \"disentangled anatomy\narithmetic\", in which a generative model learns to combine anatomical factors\nof different input images such that when they are re-entangled with the desired\nimaging modality (e.g. MRI), plausible new cardiac images are created with the\ntarget characteristics. To encourage a realistic combination of anatomy factors\nafter the arithmetic step, we propose a localized noise injection network that\nprecedes the generator. Our model is used to generate realistic images,\npathology labels, and segmentation masks that are used to augment the existing\ndatasets and subsequently improve post-hoc classification and segmentation\ntasks. Code is publicly available at https://github.com/vios-s/DAA-GAN.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 23:13:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Thermos", "Spyridon", ""], ["Liu", "Xiao", ""], ["O'Neil", "Alison", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2107.01776", "submitter": "Zhiwei Lin", "authors": "Zhiwei Lin, Yongtao Wang and Hongxiang Lin", "title": "Continual Contrastive Self-supervised Learning for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For artificial learning systems, continual learning over time from a stream\nof data is essential. The burgeoning studies on supervised continual learning\nhave achieved great progress, while the study of catastrophic forgetting in\nunsupervised learning is still blank. Among unsupervised learning methods,\nself-supervise learning method shows tremendous potential on visual\nrepresentation without any labeled data at scale. To improve the visual\nrepresentation of self-supervised learning, larger and more varied data is\nneeded. In the real world, unlabeled data is generated at all times. This\ncircumstance provides a huge advantage for the learning of the self-supervised\nmethod. However, in the current paradigm, packing previous data and current\ndata together and training it again is a waste of time and resources. Thus, a\ncontinual self-supervised learning method is badly needed. In this paper, we\nmake the first attempt to implement the continual contrastive self-supervised\nlearning by proposing a rehearsal method, which keeps a few exemplars from the\nprevious data. Instead of directly combining saved exemplars with the current\ndata set for training, we leverage self-supervised knowledge distillation to\ntransfer contrastive information among previous data to the current network by\nmimicking similarity score distribution inferred by the old network over a set\nof saved exemplars. Moreover, we build an extra sample queue to assist the\nnetwork to distinguish between previous and current data and prevent mutual\ninterference while learning their own feature representation. Experimental\nresults show that our method performs well on CIFAR100 and ImageNet-Sub.\nCompared with the baselines, which learning tasks without taking any technique,\nwe improve the image classification top-1 accuracy by 1.60% on CIFAR100, 2.86%\non ImageNet-Sub and 1.29% on ImageNet-Full under 10 incremental steps setting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 03:53:42 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 03:00:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lin", "Zhiwei", ""], ["Wang", "Yongtao", ""], ["Lin", "Hongxiang", ""]]}, {"id": "2107.01779", "submitter": "Wenbo Zhang", "authors": "Wenbo Zhang, Ge-Peng Ji, Zhuo Wang, Keren Fu, Qijun Zhao", "title": "Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient\n  Object Detection", "comments": "accepted in ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RGB-D salient object detection (SOD) recently has attracted increasing\nresearch interest by benefiting conventional RGB SOD with extra depth\ninformation. However, existing RGB-D SOD models often fail to perform well in\nterms of both efficiency and accuracy, which hinders their potential\napplications on mobile devices and real-world problems. An underlying challenge\nis that the model accuracy usually degrades when the model is simplified to\nhave few parameters. To tackle this dilemma and also inspired by the fact that\ndepth quality is a key factor influencing the accuracy, we propose a novel\ndepth quality-inspired feature manipulation (DQFM) process, which is efficient\nitself and can serve as a gating mechanism for filtering depth features to\ngreatly boost the accuracy. DQFM resorts to the alignment of low-level RGB and\ndepth features, as well as holistic attention of the depth stream to explicitly\ncontrol and enhance cross-modal fusion. We embed DQFM to obtain an efficient\nlight-weight model called DFM-Net, where we also design a tailored depth\nbackbone and a two-stage decoder for further efficiency consideration.\nExtensive experimental results demonstrate that our DFM-Net achieves\nstate-of-the-art accuracy when comparing to existing non-efficient models, and\nmeanwhile runs at 140ms on CPU (2.2$\\times$ faster than the prior fastest\nefficient model) with only $\\sim$8.5Mb model size (14.9% of the prior\nlightest). Our code will be available at https://github.com/zwbx/DFM-Net.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 04:03:02 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:07:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhang", "Wenbo", ""], ["Ji", "Ge-Peng", ""], ["Wang", "Zhuo", ""], ["Fu", "Keren", ""], ["Zhao", "Qijun", ""]]}, {"id": "2107.01782", "submitter": "Tidor-Vlad Pricope", "authors": "Tidor-Vlad Pricope", "title": "A contextual analysis of multi-layer perceptron models in classifying\n  hand-written digits and letters: limited resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifying hand-written digits and letters has taken a big leap with the\nintroduction of ConvNets. However, on very constrained hardware the time\nnecessary to train such models would be high. Our main contribution is twofold.\nFirst, we extensively test an end-to-end vanilla neural network (MLP) approach\nin pure numpy without any pre-processing or feature extraction done beforehand.\nSecond, we show that basic data mining operations can significantly improve the\nperformance of the models in terms of computational time, without sacrificing\nmuch accuracy. We illustrate our claims on a simpler variant of the Extended\nMNIST dataset, called Balanced EMNIST dataset. Our experiments show that,\nwithout any data mining, we get increased generalization performance when using\nmore hidden layers and regularization techniques, the best model achieving\n84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA\nwe were able to increase that figure to 85.08% with only 10% of the original\nfeature space, reducing the memory size needed by 64%. Finally, adding methods\nto remove possibly harmful training samples like deviation from the mean helped\nus to still achieve over 84% test accuracy but with only 32.8% of the original\nmemory size for the training set. This compares favorably to the majority of\nliterature results obtained through similar architectures. Although this\napproach gets outshined by state-of-the-art models, it does scale to some\n(AlexNet, VGGNet) trained on 50% of the same dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 04:30:37 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pricope", "Tidor-Vlad", ""]]}, {"id": "2107.01784", "submitter": "Robin Karlsson", "authors": "Robin Karlsson, David Robert Wong, Simon Thompson, Kazuya Takeda", "title": "Learning a Model for Inferring a Spatial Road Lane Network Graph using\n  Self-Supervision", "comments": "Accepted for IEEE ITSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interconnected road lanes are a central concept for navigating urban roads.\nCurrently, most autonomous vehicles rely on preconstructed lane maps as\ndesigning an algorithmic model is difficult. However, the generation and\nmaintenance of such maps is costly and hinders large-scale adoption of\nautonomous vehicle technology. This paper presents the first self-supervised\nlearning method to train a model to infer a spatially grounded lane-level road\nnetwork graph based on a dense segmented representation of the road scene\ngenerated from onboard sensors. A formal road lane network model is presented\nand proves that any structured road scene can be represented by a directed\nacyclic graph of at most depth three while retaining the notion of intersection\nregions, and that this is the most compressed representation. The formal model\nis implemented by a hybrid neural and search-based model, utilizing a novel\nbarrier function loss formulation for robust learning from partial labels.\nExperiments are conducted for all common road intersection layouts. Results\nshow that the model can generalize to new road layouts, unlike previous\napproaches, demonstrating its potential for real-world application as a\npractical learning-based lane-level map generator.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 04:34:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Karlsson", "Robin", ""], ["Wong", "David Robert", ""], ["Thompson", "Simon", ""], ["Takeda", "Kazuya", ""]]}, {"id": "2107.01787", "submitter": "Dongbao Yang", "authors": "Dongbao Yang, Yu Zhou and Weiping Wang", "title": "Multi-View Correlation Distillation for Incremental Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real applications, new object classes often emerge after the detection\nmodel has been trained on a prepared dataset with fixed classes. Due to the\nstorage burden and the privacy of old data, sometimes it is impractical to\ntrain the model from scratch with both old and new data. Fine-tuning the old\nmodel with only new data will lead to a well-known phenomenon of catastrophic\nforgetting, which severely degrades the performance of modern object detectors.\nIn this paper, we propose a novel \\textbf{M}ulti-\\textbf{V}iew\n\\textbf{C}orrelation \\textbf{D}istillation (MVCD) based incremental object\ndetection method, which explores the correlations in the feature space of the\ntwo-stage object detector (Faster R-CNN). To better transfer the knowledge\nlearned from the old classes and maintain the ability to learn new classes, we\ndesign correlation distillation losses from channel-wise, point-wise and\ninstance-wise views to regularize the learning of the incremental model. A new\nmetric named Stability-Plasticity-mAP is proposed to better evaluate both the\nstability for old classes and the plasticity for new classes in incremental\nobject detection. The extensive experiments conducted on VOC2007 and COCO\ndemonstrate that MVCD can effectively learn to detect objects of new classes\nand mitigate the problem of catastrophic forgetting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 04:36:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yang", "Dongbao", ""], ["Zhou", "Yu", ""], ["Wang", "Weiping", ""]]}, {"id": "2107.01814", "submitter": "Stephen Xi Chen", "authors": "Stephen Xi Chen, Saurajit Mukherjee, Unmesh Phadke, Tingting Wang,\n  Junwon Park, Ravi Theja Yada", "title": "Web-Scale Generic Object Detection at Microsoft Bing", "comments": "In Proceedings of the 27th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (KDD) 2021, Virtual Event, Singapore", "journal-ref": null, "doi": "10.1145/3447548.3467122", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present Generic Object Detection (GenOD), one of the\nlargest object detection systems deployed to a web-scale general visual search\nengine that can detect over 900 categories for all Microsoft Bing Visual Search\nqueries in near real-time. It acts as a fundamental visual query understanding\nservice that provides object-centric information and shows gains in multiple\nproduction scenarios, improving upon domain-specific models. We discuss the\nchallenges of collecting data, training, deploying and updating such a\nlarge-scale object detection model with multiple dependencies. We discuss a\ndata collection pipeline that reduces per-bounding box labeling cost by 81.5%\nand latency by 61.2% while improving on annotation quality. We show that GenOD\ncan improve weighted average precision by over 20% compared to multiple\ndomain-specific models. We also improve the model update agility by nearly 2\ntimes with the proposed disjoint detector training compared to joint\nfine-tuning. Finally we demonstrate how GenOD benefits visual search\napplications by significantly improving object-level search relevance by 54.9%\nand user engagement by 59.9%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 06:46:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chen", "Stephen Xi", ""], ["Mukherjee", "Saurajit", ""], ["Phadke", "Unmesh", ""], ["Wang", "Tingting", ""], ["Park", "Junwon", ""], ["Yada", "Ravi Theja", ""]]}, {"id": "2107.01824", "submitter": "Agathe Balayn", "authors": "Agathe Balayn, Bogdan Kulynych, Seda Guerses", "title": "Exploring Data Pipelines through the Process Lens: a Reference Model\n  forComputer Vision", "comments": "Presented at the CVPR workshop 2021 Beyond Fair Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers have identified datasets used for training computer vision (CV)\nmodels as an important source of hazardous outcomes, and continue to examine\npopular CV datasets to expose their harms. These works tend to treat datasets\nas objects, or focus on particular steps in data production pipelines. We argue\nhere that we could further systematize our analysis of harms by examining CV\ndata pipelines through a process-oriented lens that captures the creation, the\nevolution and use of these datasets. As a step towards cultivating a\nprocess-oriented lens, we embarked on an empirical study of CV data pipelines\ninformed by the field of method engineering. We present here a preliminary\nresult: a reference model of CV data pipelines. Besides exploring the questions\nthat this endeavor raises, we discuss how the process lens could support\nresearchers in discovering understudied issues, and could help practitioners in\nmaking their processes more transparent.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 07:15:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Balayn", "Agathe", ""], ["Kulynych", "Bogdan", ""], ["Guerses", "Seda", ""]]}, {"id": "2107.01836", "submitter": "Jim Mainprice", "authors": "Janik Hager, Ruben Bauer, Marc Toussaint, Jim Mainprice", "title": "GraspME -- Grasp Manifold Estimator", "comments": "Accepted to RoMan 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we introduce a Grasp Manifold Estimator (GraspME) to detect\ngrasp affordances for objects directly in 2D camera images. To perform\nmanipulation tasks autonomously it is crucial for robots to have such\ngraspability models of the surrounding objects. Grasp manifolds have the\nadvantage of providing continuously infinitely many grasps, which is not the\ncase when using other grasp representations such as predefined grasp points.\nFor instance, this property can be leveraged in motion optimization to define\ngoal sets as implicit surface constraints in the robot configuration space. In\nthis work, we restrict ourselves to the case of estimating possible\nend-effector positions directly from 2D camera images. To this extend, we\ndefine grasp manifolds via a set of key points and locate them in images using\na Mask R-CNN backbone. Using learned features allows generalizing to different\nview angles, with potentially noisy images, and objects that were not part of\nthe training set. We rely on simulation data only and perform experiments on\nsimple and complex objects, including unseen ones. Our framework achieves an\ninference speed of 11.5 fps on a GPU, an average precision for keypoint\nestimation of 94.5% and a mean pixel distance of only 1.29. This shows that we\ncan estimate the objects very well via bounding boxes and segmentation masks as\nwell as approximate the correct grasp manifold's keypoint coordinates.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 07:49:12 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hager", "Janik", ""], ["Bauer", "Ruben", ""], ["Toussaint", "Marc", ""], ["Mainprice", "Jim", ""]]}, {"id": "2107.01869", "submitter": "Rania Briq", "authors": "Rania Briq, Pratika Kochar, Juergen Gall", "title": "Towards Better Adversarial Synthesis of Human Images from Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes an approach that generates multiple 3D human meshes from\ntext. The human shapes are represented by 3D meshes based on the SMPL model.\nThe model's performance is evaluated on the COCO dataset, which contains\nchallenging human shapes and intricate interactions between individuals. The\nmodel is able to capture the dynamics of the scene and the interactions between\nindividuals based on text. We further show how using such a shape as input to\nimage synthesis frameworks helps to constrain the network to synthesize humans\nwith realistic human shapes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:47:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Briq", "Rania", ""], ["Kochar", "Pratika", ""], ["Gall", "Juergen", ""]]}, {"id": "2107.01872", "submitter": "Chuan Tang", "authors": "Chuan Tang, Xi Yang, Bojian Wu, Zhizhong Han, Yi Chang", "title": "Part2Word: Learning Joint Embedding of Point Clouds and Text by Matching\n  Parts to Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to learn joint embedding for 3D shapes and text in different\nshape understanding tasks, such as shape-text matching, retrieval, and shape\ncaptioning. Current multi-view based methods learn a mapping from multiple\nrendered views to text. However, these methods can not analyze 3D shapes well\ndue to the self-occlusion and limitation of learning manifolds. To resolve this\nissue, we propose a method to learn joint embedding of point clouds and text by\nmatching parts from shapes to words from sentences in a common space.\nSpecifically, we first learn segmentation prior to segment point clouds into\nparts. Then, we map parts and words into an optimized space, where the parts\nand words can be matched with each other. In the optimized space, we represent\na part by aggregating features of all points within the part, while\nrepresenting each word with its context information, where we train our network\nto minimize the triplet ranking loss. Moreover, we also introduce cross-modal\nattention to capture the relationship of part-word in this matching procedure,\nwhich enhances joint embedding learning. Our experimental results outperform\nthe state-of-the-art in multi-modal retrieval under the widely used benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:55:34 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tang", "Chuan", ""], ["Yang", "Xi", ""], ["Wu", "Bojian", ""], ["Han", "Zhizhong", ""], ["Chang", "Yi", ""]]}, {"id": "2107.01877", "submitter": "Lia Morra", "authors": "Francesco Manigrasso and Filomeno Davide Miro and Lia Morra and\n  Fabrizio Lamberti", "title": "Faster-LTN: a neuro-symbolic, end-to-end object detection architecture", "comments": "accepted for presentation at ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of semantic relationships between objects represented in an\nimage is one of the fundamental challenges in image interpretation.\nNeural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the\ncombination of semantic knowledge representation and reasoning with the ability\nto efficiently learn from examples typical of neural networks. We here propose\nFaster-LTN, an object detector composed of a convolutional backbone and an LTN.\nTo the best of our knowledge, this is the first attempt to combine both\nframeworks in an end-to-end training setting. This architecture is trained by\noptimizing a grounded theory which combines labelled examples with prior\nknowledge, in the form of logical axioms. Experimental comparisons show\ncompetitive performance with respect to the traditional Faster R-CNN\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:09:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Manigrasso", "Francesco", ""], ["Miro", "Filomeno Davide", ""], ["Morra", "Lia", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2107.01886", "submitter": "Bi'an Du", "authors": "Bi'an Du, Xiang Gao, Wei Hu, Xin Li", "title": "Self-Contrastive Learning with Hard Negative Sampling for\n  Self-supervised Point Cloud Learning", "comments": "Accepted to ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds have attracted increasing attention as a natural representation\nof 3D shapes. Significant progress has been made in developing methods for\npoint cloud analysis, which often requires costly human annotation as\nsupervision in practice. To address this issue, we propose a novel\nself-contrastive learning for self-supervised point cloud representation\nlearning, aiming to capture both local geometric patterns and nonlocal semantic\nprimitives based on the nonlocal self-similarity of point clouds. The\ncontributions are two-fold: on the one hand, instead of contrasting among\ndifferent point clouds as commonly employed in contrastive learning, we exploit\nself-similar point cloud patches within a single point cloud as positive\nsamples and otherwise negative ones to facilitate the task of contrastive\nlearning. Such self-contrastive learning is well aligned with the emerging\nparadigm of self-supervised learning for point cloud analysis. On the other\nhand, we actively learn hard negative samples that are close to positive\nsamples in the representation space for discriminative feature learning, which\nare sampled conditional on each anchor patch leveraging on the degree of\nself-similarity. Experimental results show that the proposed method achieves\nstate-of-the-art performance on widely used benchmark datasets for\nself-supervised point cloud segmentation and transfer learning for\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:17:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Du", "Bi'an", ""], ["Gao", "Xiang", ""], ["Hu", "Wei", ""], ["Li", "Xin", ""]]}, {"id": "2107.01889", "submitter": "Li Niu", "authors": "Liu Liu, Bo Zhang, Jiangtong Li, Li Niu, Qingyang Liu, Liqing Zhang", "title": "OPA: Object Placement Assessment Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition aims to generate realistic composite image by inserting an\nobject from one image into another background image, where the placement (e.g.,\nlocation, size, occlusion) of inserted object may be unreasonable, which would\nsignificantly degrade the quality of the composite image. Although some works\nattempted to learn object placement to create realistic composite images, they\ndid not focus on assessing the plausibility of object placement. In this paper,\nwe focus on object placement assessment task, which verifies whether a\ncomposite image is plausible in terms of the object placement. To accomplish\nthis task, we construct the first Object Placement Assessment (OPA) dataset\nconsisting of composite images and their rationality labels. Dataset is\navailable at https://github.com/bcmi/Object-Placement-Assessment-Dataset-OPA.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:23:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Liu", "Liu", ""], ["Zhang", "Bo", ""], ["Li", "Jiangtong", ""], ["Niu", "Li", ""], ["Liu", "Qingyang", ""], ["Zhang", "Liqing", ""]]}, {"id": "2107.01899", "submitter": "Wenjing Bian", "authors": "Wenjing Bian and Zirui Wang and Kejie Li and Victor Adrian Prisacariu", "title": "Ray-ONet: Efficient 3D Reconstruction From A Single RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose Ray-ONet to reconstruct detailed 3D models from monocular images\nefficiently. By predicting a series of occupancy probabilities along a ray that\nis back-projected from a pixel in the camera coordinate, our method Ray-ONet\nimproves the reconstruction accuracy in comparison with Occupancy Networks\n(ONet), while reducing the network inference complexity to O($N^2$). As a\nresult, Ray-ONet achieves state-of-the-art performance on the ShapeNet\nbenchmark with more than 20$\\times$ speed-up at $128^3$ resolution and\nmaintains a similar memory footprint during inference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:45:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bian", "Wenjing", ""], ["Wang", "Zirui", ""], ["Li", "Kejie", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2107.01900", "submitter": "Minkyo Seo", "authors": "Minkyo Seo, Yoonho Lee, Suha Kwak", "title": "On The Distribution of Penultimate Activations of Classification\n  Networks", "comments": "8 pages, UAI 2021, The first two authors equally contributed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies probability distributions of penultimate activations of\nclassification networks. We show that, when a classification network is trained\nwith the cross-entropy loss, its final classification layer forms a\nGenerative-Discriminative pair with a generative classifier based on a specific\ndistribution of penultimate activations. More importantly, the distribution is\nparameterized by the weights of the final fully-connected layer, and can be\nconsidered as a generative model that synthesizes the penultimate activations\nwithout feeding input data. We empirically demonstrate that this generative\nmodel enables stable knowledge distillation in the presence of domain shift,\nand can transfer knowledge from a classifier to variational autoencoders and\ngenerative adversarial networks for class-conditional image generation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:47:10 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 01:10:24 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Seo", "Minkyo", ""], ["Lee", "Yoonho", ""], ["Kwak", "Suha", ""]]}, {"id": "2107.01903", "submitter": "Haocong Rao", "authors": "Haocong Rao, Xiping Hu, Jun Cheng, Bin Hu", "title": "SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework\n  for Person Re-Identification", "comments": "Accepted at ACMMM 2021 Main Track. Sole copyright holder is ACMMM.\n  Codes are available at https://github.com/Kali-Hac/SM-SGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification via 3D skeletons is an emerging topic with great\npotential in security-critical applications. Existing methods typically learn\nbody and motion features from the body-joint trajectory, whereas they lack a\nsystematic way to model body structure and underlying relations of body\ncomponents beyond the scale of body joints. In this paper, we for the first\ntime propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE)\nframework that comprehensively models human body, component relations, and\nskeleton dynamics from unlabeled skeleton graphs of various scales to learn an\neffective skeleton representation for person Re-ID. Specifically, we first\ndevise multi-scale skeleton graphs with coarse-to-fine human body partitions,\nwhich enables us to model body structure and skeleton dynamics at multiple\nlevels. Second, to mine inherent correlations between body components in\nskeletal motion, we propose a multi-scale graph relation network to learn\nstructural relations between adjacent body-component nodes and collaborative\nrelations among nodes of different scales, so as to capture more discriminative\nskeleton graph features. Last, we propose a novel multi-scale skeleton\nreconstruction mechanism to enable our framework to encode skeleton dynamics\nand high-level semantics from unlabeled skeleton graphs, which encourages\nlearning a discriminative skeleton representation for person Re-ID. Extensive\nexperiments show that SM-SGE outperforms most state-of-the-art skeleton-based\nmethods. We further demonstrate its effectiveness on 3D skeleton data estimated\nfrom large-scale RGB videos. Our codes are open at\nhttps://github.com/Kali-Hac/SM-SGE.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:53:08 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rao", "Haocong", ""], ["Hu", "Xiping", ""], ["Cheng", "Jun", ""], ["Hu", "Bin", ""]]}, {"id": "2107.01980", "submitter": "Xin Cai", "authors": "Xin Cai, Boyu Chen, Jiabei Zeng, Jiajun Zhang, Yunjia Sun, Xiao Wang,\n  Zhilong Ji, Xiao Liu, Xilin Chen, Shiguang Shan", "title": "Gaze Estimation with an Ensemble of Four Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for gaze estimation according to face images. We\ntrain several gaze estimators adopting four different network architectures,\nincluding an architecture designed for gaze estimation (i.e.,iTracker-MHSA) and\nthree originally designed for general computer vision tasks(i.e., BoTNet,\nHRNet, ResNeSt). Then, we select the best six estimators and ensemble their\npredictions through a linear combination. The method ranks the first on the\nleader-board of ETH-XGaze Competition, achieving an average angular error of\n$3.11^{\\circ}$ on the ETH-XGaze test set.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:40:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Cai", "Xin", ""], ["Chen", "Boyu", ""], ["Zeng", "Jiabei", ""], ["Zhang", "Jiajun", ""], ["Sun", "Yunjia", ""], ["Wang", "Xiao", ""], ["Ji", "Zhilong", ""], ["Liu", "Xiao", ""], ["Chen", "Xilin", ""], ["Shan", "Shiguang", ""]]}, {"id": "2107.01988", "submitter": "Pietro Gori", "authors": "Robin Louiset and Pietro Gori and Benoit Dufumier and Josselin Houenou\n  and Antoine Grigis and Edouard Duchesnay", "title": "UCSL : A Machine Learning Expectation-Maximization framework for\n  Unsupervised Clustering driven by Supervised Learning", "comments": "ECML/PKDD 2021", "journal-ref": "ECML/PKDD 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subtype Discovery consists in finding interpretable and consistent sub-parts\nof a dataset, which are also relevant to a certain supervised task. From a\nmathematical point of view, this can be defined as a clustering task driven by\nsupervised learning in order to uncover subgroups in line with the supervised\nprediction. In this paper, we propose a general Expectation-Maximization\nensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised\nLearning). Our method is generic, it can integrate any clustering method and\ncan be driven by both binary classification and regression. We propose to\nconstruct a non-linear model by merging multiple linear estimators, one per\ncluster. Each hyperplane is estimated so that it correctly discriminates - or\npredict - only one cluster. We use SVC or Logistic Regression for\nclassification and SVR for regression. Furthermore, to perform cluster analysis\nwithin a more suitable space, we also propose a dimension-reduction algorithm\nthat projects the data onto an orthonormal space relevant to the supervised\ntask. We analyze the robustness and generalization capability of our algorithm\nusing synthetic and experimental datasets. In particular, we validate its\nability to identify suitable consistent sub-types by conducting a\npsychiatric-diseases cluster analysis with known ground-truth labels. The gain\nof the proposed method over previous state-of-the-art techniques is about +1.9\npoints in terms of balanced accuracy. Finally, we make codes and examples\navailable in a scikit-learn-compatible Python package at\nhttps://github.com/neurospin-projects/2021_rlouiset_ucsl\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 12:55:13 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Louiset", "Robin", ""], ["Gori", "Pietro", ""], ["Dufumier", "Benoit", ""], ["Houenou", "Josselin", ""], ["Grigis", "Antoine", ""], ["Duchesnay", "Edouard", ""]]}, {"id": "2107.01999", "submitter": "Kele Xu", "authors": "Zhishan Zhao, Sen Yang, Guohui Liu, Dawei Feng, Kele Xu", "title": "FINT: Field-aware INTeraction Neural Network For CTR Prediction", "comments": "5 pages, Submitted to CIKM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a critical component for online advertising and marking, click-through\nrate (CTR) prediction has draw lots of attentions from both industry and\nacademia field. Recently, the deep learning has become the mainstream\nmethodological choice for CTR. Despite of sustainable efforts have been made,\nexisting approaches still pose several challenges. On the one hand, high-order\ninteraction between the features is under-explored. On the other hand,\nhigh-order interactions may neglect the semantic information from the low-order\nfields. In this paper, we proposed a novel prediction method, named FINT, that\nemploys the Field-aware INTeraction layer which captures high-order feature\ninteractions while retaining the low-order field information. To empirically\ninvestigate the effectiveness and robustness of the FINT, we perform extensive\nexperiments on the three realistic databases: KDD2012, Criteo and Avazu. The\nobtained results demonstrate that the FINT can significantly improve the\nperformance compared to the existing methods, without increasing the amount of\ncomputation required. Moreover, the proposed method brought about 2.72\\%\nincrease to the advertising revenue of a big online video app through A/B\ntesting. To better promote the research in CTR field, we will release our code\nas well as reference implementation of those baseline models in the final\nversion.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 13:17:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhao", "Zhishan", ""], ["Yang", "Sen", ""], ["Liu", "Guohui", ""], ["Feng", "Dawei", ""], ["Xu", "Kele", ""]]}, {"id": "2107.02008", "submitter": "Wanda Benesova", "authors": "Frantisek Sefcik, Wanda Benesova", "title": "Improving a neural network model by explanation-guided training for\n  glioma classification based on MRI data", "comments": "8 pages, 7 figures, submitted for review to the journal \"Machine\n  vision and applications\" (in reviewing process)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial intelligence (AI) systems have come to the\nforefront. These systems, mostly based on Deep learning (DL), achieve excellent\nresults in areas such as image processing, natural language processing, or\nspeech recognition. Despite the statistically high accuracy of deep learning\nmodels, their output is often a decision of \"black box\". Thus, Interpretability\nmethods have become a popular way to gain insight into the decision-making\nprocess of deep learning models. Explanation of a deep learning model is\ndesirable in the medical domain since the experts have to justify their\njudgments to the patient. In this work, we proposed a method for\nexplanation-guided training that uses a Layer-wise relevance propagation (LRP)\ntechnique to force the model to focus only on the relevant part of the image.\nWe experimentally verified our method on a convolutional neural network (CNN)\nmodel for low-grade and high-grade glioma classification problems. Our\nexperiments show promising results in a way to use interpretation techniques in\nthe model training process.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 13:27:28 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sefcik", "Frantisek", ""], ["Benesova", "Wanda", ""]]}, {"id": "2107.02010", "submitter": "Pietro Gori", "authors": "Jean Feydy and Pierre Roussillon and Alain Trouv\\'e and Pietro Gori", "title": "Fast and Scalable Optimal Transport for Brain Tractograms", "comments": "MICCAI 2019", "journal-ref": "MICCAI 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new multiscale algorithm for solving regularized Optimal\nTransport problems on the GPU, with a linear memory footprint. Relying on\nSinkhorn divergences which are convex, smooth and positive definite loss\nfunctions, this method enables the computation of transport plans between\nmillions of points in a matter of minutes. We show the effectiveness of this\napproach on brain tractograms modeled either as bundles of fibers or as track\ndensity maps. We use the resulting smooth assignments to perform label transfer\nfor atlas-based segmentation of fiber tractograms. The parameters -- blur and\nreach -- of our method are meaningful, defining the minimum and maximum\ndistance at which two fibers are compared with each other. They can be set\naccording to anatomical knowledge. Furthermore, we also propose to estimate a\nprobabilistic atlas of a population of track density maps as a Wasserstein\nbarycenter. Our CUDA implementation is endowed with a user-friendly PyTorch\ninterface, freely available on the PyPi repository (pip install geomloss) and\nat www.kernel-operations.io/geomloss.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 13:28:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Feydy", "Jean", ""], ["Roussillon", "Pierre", ""], ["Trouv\u00e9", "Alain", ""], ["Gori", "Pietro", ""]]}, {"id": "2107.02016", "submitter": "Gaojian Wang", "authors": "Gaojian Wang, Qian Jiang, Xin Jin and Xiaohui Cui", "title": "FFR_FD: Effective and Fast Detection of DeepFakes Based on Feature Point\n  Defects", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet is filled with fake face images and videos synthesized by deep\ngenerative models. These realistic DeepFakes pose a challenge to determine the\nauthenticity of multimedia content. As countermeasures, artifact-based\ndetection methods suffer from insufficiently fine-grained features that lead to\nlimited detection performance. DNN-based detection methods are not efficient\nenough, given that a DeepFake can be created easily by mobile apps and\nDNN-based models require high computational resources. We show that DeepFake\nfaces have fewer feature points than real ones, especially in certain facial\nregions. Inspired by feature point detector-descriptors to extract\ndiscriminative features at the pixel level, we propose the Fused Facial\nRegion_Feature Descriptor (FFR_FD) for effective and fast DeepFake detection.\nFFR_FD is only a vector extracted from the face, and it can be constructed from\nany feature point detector-descriptors. We train a random forest classifier\nwith FFR_FD and conduct extensive experiments on six large-scale DeepFake\ndatasets, whose results demonstrate that our method is superior to most state\nof the art DNN-based models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 13:35:39 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Gaojian", ""], ["Jiang", "Qian", ""], ["Jin", "Xin", ""], ["Cui", "Xiaohui", ""]]}, {"id": "2107.02033", "submitter": "Felix Biessmann", "authors": "Felix Biessmann and Dionysius Refiano", "title": "Quality Metrics for Transparent Machine Learning With and Without Humans\n  In the Loop Are Not Correlated", "comments": "Proceedings of the ICML Workshop on Theoretical Foundations,\n  Criticism, and Application Trends of Explainable AI held in conjunction with\n  the 38th International Conference on Machine Learning (ICML), a\n  non-peer-reviewed longer version was previously published as preprint here\n  arXiv:1912.05011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The field explainable artificial intelligence (XAI) has brought about an\narsenal of methods to render Machine Learning (ML) predictions more\ninterpretable. But how useful explanations provided by transparent ML methods\nare for humans remains difficult to assess. Here we investigate the quality of\ninterpretable computer vision algorithms using techniques from psychophysics.\nIn crowdsourced annotation tasks we study the impact of different\ninterpretability approaches on annotation accuracy and task time. We compare\nthese quality metrics with classical XAI, automated quality metrics. Our\nresults demonstrate that psychophysical experiments allow for robust quality\nassessment of transparency in machine learning. Interestingly the quality\nmetrics computed without humans in the loop did not provide a consistent\nranking of interpretability methods nor were they representative for how useful\nan explanation was for humans. These findings highlight the potential of\nmethods from classical psychophysics for modern machine learning applications.\nWe hope that our results provide convincing arguments for evaluating\ninterpretability in its natural habitat, human-ML interaction, if the goal is\nto obtain an authentic assessment of interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:30:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Biessmann", "Felix", ""], ["Refiano", "Dionysius", ""]]}, {"id": "2107.02036", "submitter": "Doris Tsao", "authors": "Thomas Tsao and Doris Y. Tsao", "title": "A topological solution to object segmentation and tracking", "comments": "21 pages, 6 main figures, 3 supplemental figures, and supplementary\n  material containing mathematical proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is composed of objects, the ground, and the sky. Visual perception\nof objects requires solving two fundamental challenges: segmenting visual input\ninto discrete units, and tracking identities of these units despite appearance\nchanges due to object deformation, changing perspective, and dynamic occlusion.\nCurrent computer vision approaches to segmentation and tracking that approach\nhuman performance all require learning, raising the question: can objects be\nsegmented and tracked without learning? Here, we show that the mathematical\nstructure of light rays reflected from environment surfaces yields a natural\nrepresentation of persistent surfaces, and this surface representation provides\na solution to both the segmentation and tracking problems. We describe how to\ngenerate this surface representation from continuous visual input, and\ndemonstrate that our approach can segment and invariantly track objects in\ncluttered synthetic video despite severe appearance changes, without requiring\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 13:52:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tsao", "Thomas", ""], ["Tsao", "Doris Y.", ""]]}, {"id": "2107.02041", "submitter": "Zicheng Zhang", "authors": "Zicheng Zhang, Wei Sun, Xiongkuo Min, Tao Wang, Wei Lu, and Guangtao\n  Zhai", "title": "No-Reference Quality Assessment for Colored Point Cloud and Mesh Based\n  on Natural Scene Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the viewer's quality of experience and optimize processing systems\nin computer graphics applications, the 3D quality assessment (3D-QA) has become\nan important task in the multimedia area. Point cloud and mesh are the two most\nwidely used electronic representation formats of 3D models, the quality of\nwhich is quite sensitive to operations like simplification and compression.\nTherefore, many studies concerning point cloud quality assessment (PCQA) and\nmesh quality assessment (MQA) have been carried out to measure the visual\nquality degradations caused by lossy operations. However, a large part of\nprevious studies utilizes full-reference (FR) metrics, which means they may\nfail to predict the accurate quality level of 3D models when the reference 3D\nmodel is not available. Furthermore, limited numbers of 3D-QA metrics are\ncarried out to take color features into consideration, which significantly\nrestricts the effectiveness and scope of application. In many quality\nassessment studies, natural scene statistics (NSS) have shown a good ability to\nquantify the distortion of natural scenes to statistical parameters. Therefore,\nwe propose an NSS-based no-reference quality assessment metric for colored 3D\nmodels. In this paper, quality-aware features are extracted from the aspects of\ncolor and geometry directly from the 3D models. Then the statistic parameters\nare estimated using different distribution models to describe the\ncharacteristic of the 3D models. Our method is mainly validated on the colored\npoint cloud quality assessment database (SJTU-PCQA) and the colored mesh\nquality assessment database (CMDM). The experimental results show that the\nproposed method outperforms all the state-of-art NR 3D-QA metrics and obtains\nan acceptable gap with the state-of-art FR 3D-QA metrics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:03:15 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 05:21:16 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 14:44:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Zicheng", ""], ["Sun", "Wei", ""], ["Min", "Xiongkuo", ""], ["Wang", "Tao", ""], ["Lu", "Wei", ""], ["Zhai", "Guangtao", ""]]}, {"id": "2107.02045", "submitter": "Xiaoyu Cao", "authors": "Xiaoyu Cao and Neil Zhenqiang Gong", "title": "Understanding the Security of Deepfake Detection", "comments": "To appear in SecureComm 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfakes pose growing challenges to the trust of information on the\nInternet. Thus, detecting deepfakes has attracted increasing attentions from\nboth academia and industry. State-of-the-art deepfake detection methods consist\nof two key components, i.e., face extractor and face classifier, which extract\nthe face region in an image and classify it to be real/fake, respectively.\nExisting studies mainly focused on improving the detection performance in\nnon-adversarial settings, leaving security of deepfake detection in adversarial\nsettings largely unexplored. In this work, we aim to bridge the gap. In\nparticular, we perform a systematic measurement study to understand the\nsecurity of the state-of-the-art deepfake detection methods in adversarial\nsettings. We use two large-scale public deepfakes data sources including\nFaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes\nare fake face images; and we train state-of-the-art deepfake detection methods.\nThese detection methods can achieve 0.94--0.99 accuracies in non-adversarial\nsettings on these datasets. However, our measurement results uncover multiple\nsecurity limitations of the deepfake detection methods in adversarial settings.\nFirst, we find that an attacker can evade a face extractor, i.e., the face\nextractor fails to extract the correct face regions, via adding small Gaussian\nnoise to its deepfake images. Second, we find that a face classifier trained\nusing deepfakes generated by one method cannot detect deepfakes generated by\nanother method, i.e., an attacker can evade detection via generating deepfakes\nusing a new method. Third, we find that an attacker can leverage backdoor\nattacks developed by the adversarial machine learning community to evade a face\nclassifier. Our results highlight that deepfake detection should consider the\nadversarial nature of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:18:21 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 13:04:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cao", "Xiaoyu", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "2107.02053", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang", "title": "MixStyle Neural Networks for Domain Generalization and Adaptation", "comments": "Extension of https://openreview.net/forum?id=6xHJ37MVxxp. Code\n  available at https://github.com/KaiyangZhou/mixstyle-release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) often have poor generalization\nperformance under domain shift. One way to improve domain generalization is to\ncollect diverse source data from multiple relevant domains so that a CNN model\nis allowed to learn more domain-invariant, and hence generalizable\nrepresentations. In this work, we address domain generalization with MixStyle,\na plug-and-play, parameter-free module that is simply inserted to shallow CNN\nlayers and requires no modification to training objectives. Specifically,\nMixStyle probabilistically mixes feature statistics between instances. This\nidea is inspired by the observation that visual domains can often be\ncharacterized by image styles which are in turn encapsulated within\ninstance-level feature statistics in shallow CNN layers. Therefore, inserting\nMixStyle modules in effect synthesizes novel domains albeit in an implicit way.\nMixStyle is not only simple and flexible, but also versatile -- it can be used\nfor problems whereby unlabeled images are available, such as semi-supervised\ndomain generalization and unsupervised domain adaptation, with a simple\nextension to mix feature statistics between labeled and pseudo-labeled\ninstances. We demonstrate through extensive experiments that MixStyle can\nsignificantly boost the out-of-distribution generalization performance across a\nwide range of tasks including object recognition, instance retrieval, and\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:29:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Qiao", "Yu", ""], ["Xiang", "Tao", ""]]}, {"id": "2107.02057", "submitter": "Simon Bultmann", "authors": "Moritz Zappel, Simon Bultmann and Sven Behnke", "title": "6D Object Pose Estimation using Keypoints and Part Affinity Fields", "comments": "In: Proceedings of 24th RoboCup International Symposium, June 2021,\n  12 pages, 6 figures", "journal-ref": "Proceedings of 24th RoboCup International Symposium, June 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of 6D object pose estimation from RGB images is an important\nrequirement for autonomous service robots to be able to interact with the real\nworld. In this work, we present a two-step pipeline for estimating the 6 DoF\ntranslation and orientation of known objects. Keypoints and Part Affinity\nFields (PAFs) are predicted from the input image adopting the OpenPose CNN\narchitecture from human pose estimation. Object poses are then calculated from\n2D-3D correspondences between detected and model keypoints via the PnP-RANSAC\nalgorithm. The proposed approach is evaluated on the YCB-Video dataset and\nachieves accuracy on par with recent methods from the literature. Using PAFs to\nassemble detected keypoints into object instances proves advantageous over only\nusing heatmaps. Models trained to predict keypoints of a single object class\nperform significantly better than models trained for several classes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:41:19 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 13:22:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zappel", "Moritz", ""], ["Bultmann", "Simon", ""], ["Behnke", "Sven", ""]]}, {"id": "2107.02067", "submitter": "Silvia Bucci", "authors": "Silvia Bucci, Francesco Cappio Borlino, Barbara Caputo, Tatiana\n  Tommasi", "title": "Distance-based Hyperspherical Classification for Multi-source Open-Set\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision systems trained in closed-world scenarios will inevitably fail when\npresented with new environmental conditions, new data distributions and novel\nclasses at deployment time. How to move towards open-world learning is a long\nstanding research question, but the existing solutions mainly focus on specific\naspects of the problem (single domain Open-Set, multi-domain Closed-Set), or\npropose complex strategies which combine multiple losses and manually tuned\nhyperparameters. In this work we tackle multi-source Open-Set domain adaptation\nby introducing HyMOS: a straightforward supervised model that exploits the\npower of contrastive learning and the properties of its hyperspherical feature\nspace to correctly predict known labels on the target, while rejecting samples\nbelonging to any unknown class. HyMOS includes a tailored data balancing to\nenforce cross-source alignment and introduces style transfer among the instance\ntransformations of contrastive learning for source-target adaptation, avoiding\nthe risk of negative transfer. Finally a self-training strategy refines the\nmodel without the need for handcrafted thresholds. We validate our method over\nthree challenging datasets and provide an extensive quantitative and\nqualitative experimental analysis. The obtained results show that HyMOS\noutperforms several Open-Set and universal domain adaptation approaches,\ndefining the new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:56:57 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 10:12:42 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bucci", "Silvia", ""], ["Borlino", "Francesco Cappio", ""], ["Caputo", "Barbara", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2107.02086", "submitter": "Nathan Hubens", "authors": "Nathan Hubens and Matei Mancas and Bernard Gosselin and Marius Preda\n  and Titus Zaharia", "title": "One-Cycle Pruning: Pruning ConvNets Under a Tight Training Budget", "comments": "Accepted at Sparsity in Neural Networks (SNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introducing sparsity in a neural network has been an efficient way to reduce\nits complexity while keeping its performance almost intact. Most of the time,\nsparsity is introduced using a three-stage pipeline: 1) train the model to\nconvergence, 2) prune the model according to some criterion, 3) fine-tune the\npruned model to recover performance. The last two steps are often performed\niteratively, leading to reasonable results but also to a time-consuming and\ncomplex process. In our work, we propose to get rid of the first step of the\npipeline and to combine the two other steps in a single pruning-training cycle,\nallowing the model to jointly learn for the optimal weights while being pruned.\nWe do this by introducing a novel pruning schedule, named One-Cycle Pruning,\nwhich starts pruning from the beginning of the training, and until its very\nend. Adopting such a schedule not only leads to better performing pruned models\nbut also drastically reduces the training budget required to prune a model.\nExperiments are conducted on a variety of architectures (VGG-16 and ResNet-18)\nand datasets (CIFAR-10, CIFAR-100 and Caltech-101), and for relatively high\nsparsity values (80%, 90%, 95% of weights removed). Our results show that\nOne-Cycle Pruning consistently outperforms commonly used pruning schedules such\nas One-Shot Pruning, Iterative Pruning and Automated Gradual Pruning, on a\nfixed training budget.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:27:07 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hubens", "Nathan", ""], ["Mancas", "Matei", ""], ["Gosselin", "Bernard", ""], ["Preda", "Marius", ""], ["Zaharia", "Titus", ""]]}, {"id": "2107.02095", "submitter": "Hugo Caselles-Dupr\\'e", "authors": "Hugo Caselles-Dupr\\'e, Michael Garcia-Ortiz, David Filliat", "title": "Are standard Object Segmentation models sufficient for Learning\n  Affordance Segmentation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affordances are the possibilities of actions the environment offers to the\nindividual. Ordinary objects (hammer, knife) usually have many affordances\n(grasping, pounding, cutting), and detecting these allow artificial agents to\nunderstand what are their possibilities in the environment, with obvious\napplication in Robotics. Proposed benchmarks and state-of-the-art prediction\nmodels for supervised affordance segmentation are usually modifications of\npopular object segmentation models such as Mask R-CNN. We observe that\ntheoretically, these popular object segmentation methods should be sufficient\nfor detecting affordances masks. So we ask the question: is it necessary to\ntailor new architectures to the problem of learning affordances? We show that\napplying the out-of-the-box Mask R-CNN to the problem of affordances\nsegmentation outperforms the current state-of-the-art. We conclude that the\nproblem of supervised affordance segmentation is included in the problem of\nobject segmentation and argue that better benchmarks for affordance learning\nshould include action capacities.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:34:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Caselles-Dupr\u00e9", "Hugo", ""], ["Garcia-Ortiz", "Michael", ""], ["Filliat", "David", ""]]}, {"id": "2107.02104", "submitter": "Benjamin Hou", "authors": "Benjamin Hou, Georgios Kaissis, Ronald Summers, Bernhard Kainz", "title": "RATCHET: Medical Transformer for Chest X-ray Diagnosis and Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest radiographs are one of the most common diagnostic modalities in\nclinical routine. It can be done cheaply, requires minimal equipment, and the\nimage can be diagnosed by every radiologists. However, the number of chest\nradiographs obtained on a daily basis can easily overwhelm the available\nclinical capacities. We propose RATCHET: RAdiological Text Captioning for Human\nExamined Thoraces. RATCHET is a CNN-RNN-based medical transformer that is\ntrained end-to-end. It is capable of extracting image features from chest\nradiographs, and generates medically accurate text reports that fit seamlessly\ninto clinical work flows. The model is evaluated for its natural language\ngeneration ability using common metrics from NLP literature, as well as its\nmedically accuracy through a surrogate report classification task. The model is\navailable for download at: http://www.github.com/farrell236/RATCHET.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:58:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hou", "Benjamin", ""], ["Kaissis", "Georgios", ""], ["Summers", "Ronald", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2107.02108", "submitter": "Peter Hardy", "authors": "Peter Hardy, Srinandan Dasmahapatra, Hansung Kim", "title": "Super Resolution in Human Pose Estimation: Pixelated Poses to a\n  Resolution Result?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results obtained from state of the art human pose estimation (HPE) models\ndegrade rapidly when evaluating people of a low resolution, but can super\nresolution (SR) be used to help mitigate this effect? By using various SR\napproaches we enhanced two low resolution datasets and evaluated the change in\nperformance of both an object and keypoint detector as well as end-to-end HPE\nresults. We remark the following observations. First we find that for low\nresolution people their keypoint detection performance improved once SR was\napplied. Second, the keypoint detection performance gained is dependent on the\npersons initial resolution (segmentation area in pixels) in the original image;\nkeypoint detection performance was improved when SR was applied to people with\na small initial segmentation area, but degrades as this becomes larger. To\naddress this we introduced a novel Mask-RCNN approach, utilising a segmentation\narea threshold to decide when to use SR during the keypoint detection step.\nThis approach achieved the best results for each of our HPE performance\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:06:55 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hardy", "Peter", ""], ["Dasmahapatra", "Srinandan", ""], ["Kim", "Hansung", ""]]}, {"id": "2107.02112", "submitter": "Meng-Jiun Chiou", "authors": "Meng-Jiun Chiou, Henghui Ding, Hanshu Yan, Changhu Wang, Roger\n  Zimmermann, Jiashi Feng", "title": "Recovering the Unbiased Scene Graphs from the Biased Ones", "comments": "Accepted by ACMMM 2021. Source code will be available at\n  https://github.com/coldmanck/recovering-unbiased-scene-graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given input images, scene graph generation (SGG) aims to produce\ncomprehensive, graphical representations describing visual relationships among\nsalient objects. Recently, more efforts have been paid to the long tail problem\nin SGG; however, the imbalance in the fraction of missing labels of different\nclasses, or reporting bias, exacerbating the long tail is rarely considered and\ncannot be solved by the existing debiasing methods. In this paper we show that,\ndue to the missing labels, SGG can be viewed as a \"Learning from Positive and\nUnlabeled data\" (PU learning) problem, where the reporting bias can be removed\nby recovering the unbiased probabilities from the biased ones by utilizing\nlabel frequencies, i.e., the per-class fraction of labeled, positive examples\nin all the positive examples. To obtain accurate label frequency estimates, we\npropose Dynamic Label Frequency Estimation (DLFE) to take advantage of\ntraining-time data augmentation and average over multiple training iterations\nto introduce more valid examples. Extensive experiments show that DLFE is more\neffective in estimating label frequencies than a naive variant of the\ntraditional estimate, and DLFE significantly alleviates the long tail and\nachieves state-of-the-art debiasing performance on the VG dataset. We also show\nqualitatively that SGG models with DLFE produce prominently more balanced and\nunbiased scene graphs.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:10:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chiou", "Meng-Jiun", ""], ["Ding", "Henghui", ""], ["Yan", "Hanshu", ""], ["Wang", "Changhu", ""], ["Zimmermann", "Roger", ""], ["Feng", "Jiashi", ""]]}, {"id": "2107.02114", "submitter": "Srikrishna Varadarajan", "authors": "Jaydeep Chauhan, Srikrishna Varadarajan, Muktabh Mayank Srivastava", "title": "Semi-supervised Learning for Dense Object Detection in Retail Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retail scenes usually contain densely packed high number of objects in each\nimage. Standard object detection techniques use fully supervised training\nmethodology. This is highly costly as annotating a large dense retail object\ndetection dataset involves an order of magnitude more effort compared to\nstandard datasets. Hence, we propose semi-supervised learning to effectively\nuse the large amount of unlabeled data available in the retail domain. We adapt\na popular self supervised method called noisy student initially proposed for\nobject classification to the task of dense object detection. We show that using\nunlabeled data with the noisy student training methodology, we can improve the\nstate of the art on precise detection of objects in densely packed retail\nscenes. We also show that performance of the model increases as you increase\nthe amount of unlabeled data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:12:38 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chauhan", "Jaydeep", ""], ["Varadarajan", "Srikrishna", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "2107.02133", "submitter": "Yizhuo Li", "authors": "Miao Hao, Yizhuo Li, Zonglin Di, Nitesh B. Gundavarapu, Xiaolong Wang", "title": "Test-Time Personalization with a Transformer for Human Pose Estimation", "comments": "Project page: http://liyz15.github.io/TTP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to personalize a human pose estimator given a set of test images\nof a person without using any manual annotations. While there is a significant\nadvancement in human pose estimation, it is still very challenging for a model\nto generalize to different unknown environments and unseen persons. Instead of\nusing a fixed model for every test case, we adapt our pose estimator during\ntest time to exploit person-specific information. We first train our model on\ndiverse data with both a supervised and a self-supervised pose estimation\nobjectives jointly. We use a Transformer model to build a transformation\nbetween the self-supervised keypoints and the supervised keypoints. During test\ntime, we personalize and adapt our model by fine-tuning with the\nself-supervised objective. The pose is then improved by transforming the\nupdated self-supervised keypoints. We experiment with multiple datasets and\nshow significant improvements on pose estimations with our self-supervised\npersonalization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:48:34 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hao", "Miao", ""], ["Li", "Yizhuo", ""], ["Di", "Zonglin", ""], ["Gundavarapu", "Nitesh B.", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2107.02156", "submitter": "Zhongdao Wang", "authors": "Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip H.S.\n  Torr, Luca Bertinetto", "title": "Do Different Tracking Tasks Require Different Appearance Models?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tracking objects of interest in a video is one of the most popular and widely\napplicable problems in computer vision. However, with the years, a Cambrian\nexplosion of use cases and benchmarks has fragmented the problem in a multitude\nof different experimental setups. As a consequence, the literature has\nfragmented too, and now the novel approaches proposed by the community are\nusually specialised to fit only one specific setup. To understand to what\nextent this specialisation is actually necessary, in this work we present\nUniTrack, a unified tracking solution to address five different tasks within\nthe same framework. UniTrack consists of a single and task-agnostic appearance\nmodel, which can be learned in a supervised or self-supervised fashion, and\nmultiple \"heads\" to address individual tasks and that do not require training.\nWe show how most tracking tasks can be solved within this framework, and that\nthe same appearance model can be used to obtain performance that is competitive\nagainst specialised methods for all the five tasks considered. The framework\nalso allows us to analyse appearance models obtained with the most recent\nself-supervised methods, thus significantly extending their evaluation and\ncomparison to a larger variety of important problems. Code available at\nhttps://github.com/Zhongdao/UniTrack.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:40:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Zhongdao", ""], ["Zhao", "Hengshuang", ""], ["Li", "Ya-Li", ""], ["Wang", "Shengjin", ""], ["Torr", "Philip H. S.", ""], ["Bertinetto", "Luca", ""]]}, {"id": "2107.02162", "submitter": "Sudipta Banerjee", "authors": "Sudipta Banerjee and Arun Ross", "title": "Conditional Identity Disentanglement for Differential Face Morph\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the task of differential face morph attack detection using a\nconditional generative network (cGAN). To determine whether a face image in an\nidentification document, such as a passport, is morphed or not, we propose an\nalgorithm that learns to implicitly disentangle identities from the morphed\nimage conditioned on the trusted reference image using the cGAN. Furthermore,\nthe proposed method can also recover some underlying information about the\nsecond subject used in generating the morph. We performed experiments on AMSL\nface morph, MorGAN, and EMorGAN datasets to demonstrate the effectiveness of\nthe proposed method. We also conducted cross-dataset and cross-attack detection\nexperiments. We obtained promising results of 3% BPCER @ 10% APCER on\nintra-dataset evaluation, which is comparable to existing methods; and 4.6%\nBPCER @ 10% APCER on cross-dataset evaluation, which outperforms\nstate-of-the-art methods by at least 13.9%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:44:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Banerjee", "Sudipta", ""], ["Ross", "Arun", ""]]}, {"id": "2107.02170", "submitter": "Wei-Lun Chao", "authors": "Tai-Yu Pan, Cheng Zhang, Yandong Li, Hexiang Hu, Dong Xuan, Soravit\n  Changpinyo, Boqing Gong, Wei-Lun Chao", "title": "On Model Calibration for Long-Tailed Object Detection and Instance\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vanilla models for object detection and instance segmentation suffer from the\nheavy bias toward detecting frequent objects in the long-tailed setting.\nExisting methods address this issue mostly during training, e.g., by\nre-sampling or re-weighting. In this paper, we investigate a largely overlooked\napproach -- post-processing calibration of confidence scores. We propose\nNorCal, Normalized Calibration for long-tailed object detection and instance\nsegmentation, a simple and straightforward recipe that reweighs the predicted\nscores of each class by its training sample size. We show that separately\nhandling the background class and normalizing the scores over classes for each\nproposal are keys to achieving superior performance. On the LVIS dataset,\nNorCal can effectively improve nearly all the baseline models not only on rare\nclasses but also on common and frequent classes. Finally, we conduct extensive\nanalysis and ablation studies to offer insights into various modeling choices\nand mechanisms of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:57:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pan", "Tai-Yu", ""], ["Zhang", "Cheng", ""], ["Li", "Yandong", ""], ["Hu", "Hexiang", ""], ["Xuan", "Dong", ""], ["Changpinyo", "Soravit", ""], ["Gong", "Boqing", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2107.02174", "submitter": "Yuxin Fang", "authors": "Yuxin Fang, Xinggang Wang, Rui Wu, Jianwei Niu, Wenyu Liu", "title": "What Makes for Hierarchical Vision Transformer?", "comments": "Preprint. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that hierarchical Vision Transformer with interleaved\nnon-overlapped intra window self-attention \\& shifted window self-attention is\nable to achieve state-of-the-art performance in various visual recognition\ntasks and challenges CNN's dense sliding window paradigm. Most follow-up works\ntry to replace shifted window operation with other kinds of cross window\ncommunication while treating self-attention as the de-facto standard for intra\nwindow information aggregation. In this short preprint, we question whether\nself-attention is the only choice for hierarchical Vision Transformer to attain\nstrong performance, and what makes for hierarchical Vision Transformer? We\nreplace self-attention layers in Swin Transformer and Shuffle Transformer with\nsimple linear mapping and keep other components unchanged. The resulting\narchitecture with 25.4M parameters and 4.2G FLOPs achieves 80.5\\% Top-1\naccuracy, compared to 81.3\\% for Swin Transformer with 28.3M parameters and\n4.5G FLOPs. We also experiment with other alternatives to self-attention for\ncontext aggregation inside each non-overlapped window, which all give similar\ncompetitive results under the same architecture. Our study reveals that the\n\\textbf{macro architecture} of Swin model families (i.e., interleaved intra\nwindow \\& cross window communications), other than specific aggregation layers\nor specific means of cross window communication, may be more responsible for\nits strong performance and is the real challenger to CNN's dense sliding window\nparadigm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:59:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Fang", "Yuxin", ""], ["Wang", "Xinggang", ""], ["Wu", "Rui", ""], ["Niu", "Jianwei", ""], ["Liu", "Wenyu", ""]]}, {"id": "2107.02189", "submitter": "Eugene Vorontsov", "authors": "Eugene Vorontsov, Samuel Kadoury", "title": "Label noise in segmentation networks : mitigation must deal with bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imperfect labels limit the quality of predictions learned by deep neural\nnetworks. This is particularly relevant in medical image segmentation, where\nreference annotations are difficult to collect and vary significantly even\nacross expert annotators. Prior work on mitigating label noise focused on\nsimple models of mostly uniform noise. In this work, we explore biased and\nunbiased errors artificially introduced to brain tumour annotations on MRI\ndata. We found that supervised and semi-supervised segmentation methods are\nrobust or fairly robust to unbiased errors but sensitive to biased errors. It\nis therefore important to identify the sorts of errors expected in medical\nimage labels and especially mitigate the biased errors.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:00:07 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Vorontsov", "Eugene", ""], ["Kadoury", "Samuel", ""]]}, {"id": "2107.02191", "submitter": "Aljaz Bozic", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Pablo Palafox, Justus Thies, Angela Dai,\n  Matthias Nie{\\ss}ner", "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers", "comments": "Video: https://youtu.be/LIpTKYfKSqw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TransformerFusion, a transformer-based 3D scene reconstruction\napproach. From an input monocular RGB video, the video frames are processed by\na transformer network that fuses the observations into a volumetric feature\ngrid representing the scene; this feature grid is then decoded into an implicit\n3D scene representation. Key to our approach is the transformer architecture\nthat enables the network to learn to attend to the most relevant image frames\nfor each 3D location in the scene, supervised only by the scene reconstruction\ntask. Features are fused in a coarse-to-fine fashion, storing fine-level\nfeatures only where needed, requiring lower memory storage and enabling fusion\nat interactive rates. The feature grid is then decoded to a higher-resolution\nscene reconstruction, using an MLP-based surface occupancy prediction from\ninterpolated coarse-to-fine 3D features. Our approach results in an accurate\nsurface reconstruction, outperforming state-of-the-art multi-view stereo depth\nestimation methods, fully-convolutional 3D reconstruction approaches, and\napproaches using LSTM- or GRU-based recurrent networks for video sequence\nfusion.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:00:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Palafox", "Pablo", ""], ["Thies", "Justus", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2107.02192", "submitter": "Wei Ping", "authors": "Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein,\n  Anima Anandkumar, Bryan Catanzaro", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:00:14 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:34:30 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhu", "Chen", ""], ["Ping", "Wei", ""], ["Xiao", "Chaowei", ""], ["Shoeybi", "Mohammad", ""], ["Goldstein", "Tom", ""], ["Anandkumar", "Anima", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2107.02211", "submitter": "Mantas Luko\\v{s}evi\\v{c}ius", "authors": "Rokas Pe\\v{c}iulis and Mantas Luko\\v{s}evi\\v{c}ius and Algimantas\n  Kri\\v{s}\\v{c}iukaitis and Robertas Petrolis and Dovil\\.e Buteikien\\.e", "title": "Automated age-related macular degeneration area estimation -- first\n  results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to research an automatic method for detecting Age-related\nMacular Degeneration (AMD) lesions in RGB eye fundus images. For this, we align\ninvasively obtained eye fundus contrast images (the \"golden standard\"\ndiagnostic) to the RGB ones and use them to hand-annotate the lesions. This is\ndone using our custom-made tool. Using the data, we train and test five\ndifferent convolutional neural networks: a custom one to classify healthy and\nAMD-affected eye fundi, and four well-known networks: ResNet50, ResNet101,\nMobileNetV3, and UNet to segment (localize) the AMD lesions in the affected eye\nfundus images. We achieve 93.55% accuracy or 69.71% Dice index as the\npreliminary best results in segmentation with MobileNetV3.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:29:56 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Pe\u010diulis", "Rokas", ""], ["Luko\u0161evi\u010dius", "Mantas", ""], ["Kri\u0161\u010diukaitis", "Algimantas", ""], ["Petrolis", "Robertas", ""], ["Buteikien\u0117", "Dovil\u0117", ""]]}, {"id": "2107.02220", "submitter": "Zhang Yuqi", "authors": "Yuqi Zhang, Qian Qi, Chong Liu, Weihua Chen, Fan Wang, Hao Li, Rong\n  Jin", "title": "Graph Convolution for Re-ranking in Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, deep learning is widely applied to extract features for similarity\ncomputation in person re-identification (re-ID) and have achieved great\nsuccess. However, due to the non-overlapping between training and testing IDs,\nthe difference between the data used for model training and the testing data\nmakes the performance of learned feature degraded during testing. Hence,\nre-ranking is proposed to mitigate this issue and various algorithms have been\ndeveloped. However, most of existing re-ranking methods focus on replacing the\nEuclidean distance with sophisticated distance metrics, which are not friendly\nto downstream tasks and hard to be used for fast retrieval of massive data in\nreal applications. In this work, we propose a graph-based re-ranking method to\nimprove learned features while still keeping Euclidean distance as the\nsimilarity metric. Inspired by graph convolution networks, we develop an\noperator to propagate features over an appropriate graph. Since graph is the\nessential key for the propagation, two important criteria are considered for\ndesigning the graph, and three different graphs are explored accordingly.\nFurthermore, a simple yet effective method is proposed to generate a profile\nvector for each tracklet in videos, which helps extend our method to video\nre-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501,\nDuke, and MARS, demonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:40:43 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhang", "Yuqi", ""], ["Qi", "Qian", ""], ["Liu", "Chong", ""], ["Chen", "Weihua", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2107.02239", "submitter": "Pranav Jeevan P", "authors": "Pranav Jeevan, Amit Sethi (Indian Institute of Technology Bombay)", "title": "Vision Xformers: Efficient Attention for Image Classification", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear attention mechanisms provide hope for overcoming the bottleneck of\nquadratic complexity which restricts application of transformer models in\nvision tasks. We modify the ViT architecture to work on longer sequence data by\nreplacing the quadratic attention with efficient transformers like Performer,\nLinformer and Nystr\\\"omformer of linear complexity creating Vision X-formers\n(ViX). We show that ViX performs better than ViT in image classification\nconsuming lesser computing resources. We further show that replacing the\nembedding linear layer by convolutional layers in ViX further increases their\nperformance. Our test on recent visions transformer models like LeViT and\nCompact Convolutional Transformer (CCT) show that replacing the attention with\nNystr\\\"omformer or Performer saves GPU usage and memory without deteriorating\nperformance. Incorporating these changes can democratize transformers by making\nthem accessible to those with limited data and computing resources.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 19:24:23 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jeevan", "Pranav", "", "Indian Institute of Technology Bombay"], ["Sethi", "Amit", "", "Indian Institute of Technology Bombay"]]}, {"id": "2107.02259", "submitter": "Fabian Leinen Leinen", "authors": "Fabian Leinen, Vittorio Cozzolino, Torsten Sch\\\"on", "title": "VolNet: Estimating Human Body Part Volumes from a Single RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human body volume estimation from a single RGB image is a challenging problem\ndespite minimal attention from the research community. However VolNet, an\narchitecture leveraging 2D and 3D pose estimation, body part segmentation and\nvolume regression extracted from a single 2D RGB image combined with the\nsubject's body height can be used to estimate the total body volume. VolNet is\ndesigned to predict the 2D and 3D pose as well as the body part segmentation in\nintermediate tasks. We generated a synthetic, large-scale dataset of\nphoto-realistic images of human bodies with a wide range of body shapes and\nrealistic poses called SURREALvols. By using Volnet and combining multiple\nstacked hourglass networks together with ResNeXt, our model correctly predicted\nthe volume in ~82% of cases with a 10% tolerance threshold. This is a\nconsiderable improvement compared to state-of-the-art solutions such as BodyNet\nwith only a ~38% success rate.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 20:38:44 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Leinen", "Fabian", ""], ["Cozzolino", "Vittorio", ""], ["Sch\u00f6n", "Torsten", ""]]}, {"id": "2107.02287", "submitter": "Natanael Magalh\\~aes Cardoso", "authors": "N. M. Cardoso, G. B. O. Schwarz, L. O. Dias, C. R. Bom, L. Sodr\\'e\n  Jr., C. Mendes de Oliveira", "title": "Morphological Classification of Galaxies in S-PLUS using an Ensemble of\n  Convolutional Networks", "comments": "18 pages, 13 figures, codes and data available at\n  https://natanael.net, text in portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The universe is composed of galaxies that have diverse shapes. Once the\nstructure of a galaxy is determined, it is possible to obtain important\ninformation about its formation and evolution. Morphologically classifying\ngalaxies means cataloging them according to their visual appearance and the\nclassification is linked to the physical properties of the galaxy. A\nmorphological classification made through visual inspection is subject to\nbiases introduced by subjective observations made by human volunteers. For this\nreason, systematic, objective and easily reproducible classification of\ngalaxies has been gaining importance since the astronomer Edwin Hubble created\nhis famous classification method. In this work, we combine accurate visual\nclassifications of the Galaxy Zoo project with \\emph {Deep Learning} methods.\nThe goal is to find an efficient technique at human performance level\nclassification, but in a systematic and automatic way, for classification of\nelliptical and spiral galaxies. For this, a neural network model was created\nthrough an Ensemble of four other convolutional models, allowing a greater\naccuracy in the classification than what would be obtained with any one\nindividual. Details of the individual models and improvements made are also\ndescribed. The present work is entirely based on the analysis of images (not\nparameter tables) from DR1 (www.datalab.noao.edu) of the Southern Photometric\nLocal Universe Survey (S-PLUS). In terms of classification, we achieved, with\nthe Ensemble, an accuracy of $\\approx 99 \\%$ in the test sample (using\npre-trained networks).\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:51:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Cardoso", "N. M.", ""], ["Schwarz", "G. B. O.", ""], ["Dias", "L. O.", ""], ["Bom", "C. R.", ""], ["Sodr\u00e9", "L.", "Jr."], ["de Oliveira", "C. Mendes", ""]]}, {"id": "2107.02293", "submitter": "Rohollah Moosavi Tayebi", "authors": "Rohollah Moosavi Tayebi, Youqing Mu, Taher Dehkharghanian, Catherine\n  Ross, Monalisa Sur, Ronan Foley, Hamid R. Tizhoosh, and Clinton JV Campbell", "title": "Histogram of Cell Types: Deep Learning for Automated Bone Marrow\n  Cytology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bone marrow cytology is required to make a hematological diagnosis,\ninfluencing critical clinical decision points in hematology. However, bone\nmarrow cytology is tedious, limited to experienced reference centers and\nassociated with high inter-observer variability. This may lead to a delayed or\nincorrect diagnosis, leaving an unmet need for innovative supporting\ntechnologies. We have developed the first ever end-to-end deep learning-based\ntechnology for automated bone marrow cytology. Starting with a bone marrow\naspirate digital whole slide image, our technology rapidly and automatically\ndetects suitable regions for cytology, and subsequently identifies and\nclassifies all bone marrow cells in each region. This collective\ncytomorphological information is captured in a novel representation called\nHistogram of Cell Types (HCT) quantifying bone marrow cell class probability\ndistribution and acting as a cytological \"patient fingerprint\". The approach\nachieves high accuracy in region detection (0.97 accuracy and 0.99 ROC AUC),\nand cell detection and cell classification (0.75 mAP, 0.78 F1-score,\nLog-average miss rate of 0.31). HCT has potential to revolutionize\nhematopathology diagnostic workflows, leading to more cost-effective, accurate\ndiagnosis and opening the door to precision medicine.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:55:00 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 16:11:28 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Tayebi", "Rohollah Moosavi", ""], ["Mu", "Youqing", ""], ["Dehkharghanian", "Taher", ""], ["Ross", "Catherine", ""], ["Sur", "Monalisa", ""], ["Foley", "Ronan", ""], ["Tizhoosh", "Hamid R.", ""], ["Campbell", "Clinton JV", ""]]}, {"id": "2107.02299", "submitter": "Ziyi Liu", "authors": "Ziyi Liu, Jie Yang, Orly Yadid-Pecht", "title": "LightFuse: Lightweight CNN based Dual-exposure Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) aided high dynamic range (HDR)\nimaging recently received a lot of attention. The quality of DCNN generated HDR\nimages have overperformed the traditional counterparts. However, DCNN is prone\nto be computationally intensive and power-hungry. To address the challenge, we\npropose LightFuse, a light-weight CNN-based algorithm for extreme dual-exposure\nimage fusion, which can be implemented on various embedded computing platforms\nwith limited power and hardware resources. Two sub-networks are utilized: a\nGlobalNet (G) and a DetailNet (D). The goal of G is to learn the global\nillumination information on the spatial dimension, whereas D aims to enhance\nlocal details on the channel dimension. Both G and D are based solely on\ndepthwise convolution (D Conv) and pointwise convolution (P Conv) to reduce\nrequired parameters and computations. Experimental results display that the\nproposed technique could generate HDR images with plausible details in\nextremely exposed regions. Our PSNR score exceeds the other state-of-the-art\napproaches by 1.2 to 1.6 times and achieves 1.4 to 20 times FLOP and parameter\nreduction compared with others.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 22:10:29 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Ziyi", ""], ["Yang", "Jie", ""], ["Yadid-Pecht", "Orly", ""]]}, {"id": "2107.02306", "submitter": "Artem Vysogorets", "authors": "Artem Vysogorets, Julia Kempe", "title": "Connectivity Matters: Neural Network Pruning Through the Lens of\n  Effective Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning is a fruitful area of research with surging interest\nin high sparsity regimes. Benchmarking in this domain heavily relies on\nfaithful representation of the sparsity of subnetworks, which has been\ntraditionally computed as the fraction of removed connections (direct\nsparsity). This definition, however, fails to recognize unpruned parameters\nthat detached from input or output layers of underlying subnetworks,\npotentially underestimating actual effective sparsity: the fraction of\ninactivated connections. While this effect might be negligible for moderately\npruned networks (up to 10-100 compression rates), we find that it plays an\nincreasing role for thinner subnetworks, greatly distorting comparison between\ndifferent pruning algorithms. For example, we show that effective compression\nof a randomly pruned LeNet-300-100 can be orders of magnitude larger than its\ndirect counterpart, while no discrepancy is ever observed when using SynFlow\nfor pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective\nsparsity to reevaluate several recent pruning algorithms on common benchmark\narchitectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their\nabsolute and relative performance changes dramatically in this new and more\nappropriate framework. To aim for effective, rather than direct, sparsity, we\ndevelop a low-cost extension to most pruning algorithms. Further, equipped with\neffective sparsity as a reference frame, we partially reconfirm that random\npruning with appropriate sparsity allocation across layers performs as well or\nbetter than more sophisticated algorithms for pruning at initialization [Su et\nal., 2020]. In response to this observation, using a simple analogy of pressure\ndistribution in coupled cylinders from physics, we design novel layerwise\nsparsity quotas that outperform all existing baselines in the context of random\npruning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 22:36:57 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Vysogorets", "Artem", ""], ["Kempe", "Julia", ""]]}, {"id": "2107.02308", "submitter": "Joseph Ortiz", "authors": "Joseph Ortiz, Talfan Evans, Andrew J. Davison", "title": "A visual introduction to Gaussian Belief Propagation", "comments": "See online version of this article: https://gaussianbp.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present a visual introduction to Gaussian Belief\nPropagation (GBP), an approximate probabilistic inference algorithm that\noperates by passing messages between the nodes of arbitrarily structured factor\ngraphs. A special case of loopy belief propagation, GBP updates rely only on\nlocal information and will converge independently of the message schedule. Our\nkey argument is that, given recent trends in computing hardware, GBP has the\nright computational properties to act as a scalable distributed probabilistic\ninference framework for future machine learning systems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 22:43:27 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ortiz", "Joseph", ""], ["Evans", "Talfan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2107.02314", "submitter": "Ujjwal Baid", "authors": "Ujjwal Baid, Satyam Ghodasara, Michel Bilello, Suyash Mohan, Evan\n  Calabrese, Errol Colak, Keyvan Farahani, Jayashree Kalpathy-Cramer, Felipe C.\n  Kitamura, Sarthak Pati, Luciano M. Prevedello, Jeffrey D. Rudie, Chiharu\n  Sako, Russell T. Shinohara, Timothy Bergquist, Rong Chai, James Eddy, Julia\n  Elliott, Walter Reade, Thomas Schaffter, Thomas Yu, Jiaxin Zheng, BraTS\n  Annotators, Christos Davatzikos, John Mongan, Christopher Hess, Soonmee Cha,\n  Javier Villanueva-Meyer, John B. Freymann, Justin S. Kirby, Benedikt\n  Wiestler, Priscila Crivellaro, Rivka R.Colen, Aikaterini Kotrotsou, Daniel\n  Marcus, Mikhail Milchenko, Arash Nazeri, Hassan Fathallah-Shaykh, Roland\n  Wiest, Andras Jakab, Marc-Andre Weber, Abhishek Mahajan, Bjoern Menze, Adam\n  E. Flanders, Spyridon Bakas", "title": "The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation\n  and Radiogenomic Classification", "comments": "17 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The BraTS 2021 challenge celebrates its 10th anniversary and is jointly\norganized by the Radiological Society of North America (RSNA), the American\nSociety of Neuroradiology (ASNR), and the Medical Image Computing and Computer\nAssisted Interventions (MICCAI) society. Since its inception, BraTS has been\nfocusing on being a common benchmarking venue for brain glioma segmentation\nalgorithms, with well-curated multi-institutional multi-parametric magnetic\nresonance imaging (mpMRI) data. Gliomas are the most common primary\nmalignancies of the central nervous system, with varying degrees of\naggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets\nthe evaluation of computational algorithms assessing the same tumor\ncompartmentalization, as well as the underlying tumor's molecular\ncharacterization, in pre-operative baseline mpMRI data from 2,000 patients.\nSpecifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation\nof the histologically distinct brain tumor sub-regions, and b) the\nclassification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT)\npromoter methylation status. The performance evaluation of all participating\nalgorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse\nplatform (Task 1) and Kaggle (Task 2), concluding in distributing to the top\nranked participants monetary awards of $60,000 collectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 23:12:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Baid", "Ujjwal", ""], ["Ghodasara", "Satyam", ""], ["Bilello", "Michel", ""], ["Mohan", "Suyash", ""], ["Calabrese", "Evan", ""], ["Colak", "Errol", ""], ["Farahani", "Keyvan", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Kitamura", "Felipe C.", ""], ["Pati", "Sarthak", ""], ["Prevedello", "Luciano M.", ""], ["Rudie", "Jeffrey D.", ""], ["Sako", "Chiharu", ""], ["Shinohara", "Russell T.", ""], ["Bergquist", "Timothy", ""], ["Chai", "Rong", ""], ["Eddy", "James", ""], ["Elliott", "Julia", ""], ["Reade", "Walter", ""], ["Schaffter", "Thomas", ""], ["Yu", "Thomas", ""], ["Zheng", "Jiaxin", ""], ["Annotators", "BraTS", ""], ["Davatzikos", "Christos", ""], ["Mongan", "John", ""], ["Hess", "Christopher", ""], ["Cha", "Soonmee", ""], ["Villanueva-Meyer", "Javier", ""], ["Freymann", "John B.", ""], ["Kirby", "Justin S.", ""], ["Wiestler", "Benedikt", ""], ["Crivellaro", "Priscila", ""], ["Colen", "Rivka R.", ""], ["Kotrotsou", "Aikaterini", ""], ["Marcus", "Daniel", ""], ["Milchenko", "Mikhail", ""], ["Nazeri", "Arash", ""], ["Fathallah-Shaykh", "Hassan", ""], ["Wiest", "Roland", ""], ["Jakab", "Andras", ""], ["Weber", "Marc-Andre", ""], ["Mahajan", "Abhishek", ""], ["Menze", "Bjoern", ""], ["Flanders", "Adam E.", ""], ["Bakas", "Spyridon", ""]]}, {"id": "2107.02319", "submitter": "Debesh Jha", "authors": "Debesh Jha, Sharib Ali, Michael A. Riegler, Dag Johansen, H{\\aa}vard\n  D. Johansen, P{\\aa}l Halvorsen", "title": "Exploring Deep Learning Methods for Real-Time Surgical Instrument\n  Segmentation in Laparoscopy", "comments": null, "journal-ref": "BHI 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimally invasive surgery is a surgical intervention used to examine the\norgans inside the abdomen and has been widely used due to its effectiveness\nover open surgery. Due to the hardware improvements such as high definition\ncameras, this procedure has significantly improved and new software methods\nhave demonstrated potential for computer-assisted procedures. However, there\nexists challenges and requirements to improve detection and tracking of the\nposition of the instruments during these surgical procedures. To this end, we\nevaluate and compare some popular deep learning methods that can be explored\nfor the automated segmentation of surgical instruments in laparoscopy, an\nimportant step towards tool tracking. Our experimental results exhibit that the\nDual decoder attention network (DDANet) produces a superior result compared to\nother recent deep learning methods. DDANet yields a Dice coefficient of 0.8739\nand mean intersection-over-union of 0.8183 for the Robust Medical Instrument\nSegmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of\n101.36 frames-per-second that is critical for such procedures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 23:32:05 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jha", "Debesh", ""], ["Ali", "Sharib", ""], ["Riegler", "Michael A.", ""], ["Johansen", "Dag", ""], ["Johansen", "H\u00e5vard D.", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2107.02328", "submitter": "Huaju Liang", "authors": "Huaju Liang, Hongyang Bai, Ke Hu and Xinbo Lv", "title": "Polarized skylight orientation determination artificial neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an artificial neural network to determine orientation\nusing polarized skylight. This neural network has specific dilated convolution,\nwhich can extract light intensity information of different polarization\ndirections. Then, the degree of polarization (DOP) and angle of polarization\n(AOP) are directly extracted in the network. In addition, the exponential\nfunction encoding of orientation is designed as the network output, which can\nbetter reflect the insect's encoding of polarization information, and improve\nthe accuracy of orientation determination. Finally, training and testing were\nconducted on a public polarized skylight navigation dataset, and the\nexperimental results proved the stability and effectiveness of the network.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 00:19:22 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liang", "Huaju", ""], ["Bai", "Hongyang", ""], ["Hu", "Ke", ""], ["Lv", "Xinbo", ""]]}, {"id": "2107.02331", "submitter": "Siddharth Karamcheti", "authors": "Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, Christopher D.\n  Manning", "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on\n  Active Learning for Visual Question Answering", "comments": "Accepted at ACL-IJCNLP 2021. 17 pages, 16 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning promises to alleviate the massive data needs of supervised\nmachine learning: it has successfully improved sample efficiency by an order of\nmagnitude on traditional tasks like topic classification and object\nrecognition. However, we uncover a striking contrast to this promise: across 5\nmodels and 4 datasets on the task of visual question answering, a wide variety\nof active learning approaches fail to outperform random selection. To\nunderstand this discrepancy, we profile 8 active learning methods on a\nper-example basis, and identify the problem as collective outliers -- groups of\nexamples that active learning methods prefer to acquire but models fail to\nlearn (e.g., questions that ask about text in images or require external\nknowledge). Through systematic ablation experiments and qualitative\nvisualizations, we verify that collective outliers are a general phenomenon\nresponsible for degrading pool-based active learning. Notably, we show that\nactive learning sample efficiency increases significantly as the number of\ncollective outliers in the active learning pool decreases. We conclude with a\ndiscussion and prescriptive recommendations for mitigating the effects of these\noutliers in future work.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 00:52:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Karamcheti", "Siddharth", ""], ["Krishna", "Ranjay", ""], ["Fei-Fei", "Li", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2107.02338", "submitter": "Varun Kelkar", "authors": "Xiaohui Zhang, Varun A. Kelkar, Jason Granstedt, Hua Li, Mark A.\n  Anastasio", "title": "Impact of deep learning-based image super-resolution on binary signal\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based image super-resolution (DL-SR) has shown great promise in\nmedical imaging applications. To date, most of the proposed methods for DL-SR\nhave only been assessed by use of traditional measures of image quality (IQ)\nthat are commonly employed in the field of computer vision. However, the impact\nof these methods on objective measures of image quality that are relevant to\nmedical imaging tasks remains largely unexplored. In this study, we investigate\nthe impact of DL-SR methods on binary signal detection performance. Two popular\nDL-SR methods, the super-resolution convolutional neural network (SRCNN) and\nthe super-resolution generative adversarial network (SRGAN), were trained by\nuse of simulated medical image data. Binary signal-known-exactly with\nbackground-known-statistically (SKE/BKS) and signal-known-statistically with\nbackground-known-statistically (SKS/BKS) detection tasks were formulated.\nNumerical observers, which included a neural network-approximated ideal\nobserver and common linear numerical observers, were employed to assess the\nimpact of DL-SR on task performance. The impact of the complexity of the DL-SR\nnetwork architectures on task-performance was quantified. In addition, the\nutility of DL-SR for improving the task-performance of sub-optimal observers\nwas investigated. Our numerical experiments confirmed that, as expected, DL-SR\ncould improve traditional measures of IQ. However, for many of the study\ndesigns considered, the DL-SR methods provided little or no improvement in task\nperformance and could even degrade it. It was observed that DL-SR could improve\nthe task-performance of sub-optimal observers under certain conditions. The\npresented study highlights the urgent need for the objective assessment of\nDL-SR methods and suggests avenues for improving their efficacy in medical\nimaging applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 01:27:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhang", "Xiaohui", ""], ["Kelkar", "Varun A.", ""], ["Granstedt", "Jason", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2107.02341", "submitter": "Jun Wang", "authors": "Jun Wang, Xiaohan Yu and Yongsheng Gao", "title": "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization", "comments": "9 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core for tackling the fine-grained visual categorization (FGVC) is to\nlearn subtle yet discriminative features. Most previous works achieve this by\nexplicitly selecting the discriminative parts or integrating the attention\nmechanism via CNN-based approaches.However, these methods enhance the\ncomputational complexity and make the modeldominated by the regions containing\nthe most of the objects. Recently, vision trans-former (ViT) has achieved SOTA\nperformance on general image recognition tasks. Theself-attention mechanism\naggregates and weights the information from all patches to the classification\ntoken, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation\ntoken in the deep layer pays more attention to the global information, lacking\nthe local and low-level features that are essential for FGVC. In this work, we\nproposea novel pure transformer-based framework Feature Fusion Vision\nTransformer (FFVT)where we aggregate the important tokens from each transformer\nlayer to compensate thelocal, low-level and middle-level information. We design\na novel token selection mod-ule called mutual attention weight selection (MAWS)\nto guide the network effectively and efficiently towards selecting\ndiscriminative tokens without introducing extra param-eters. We verify the\neffectiveness of FFVT on three benchmarks where FFVT achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 01:48:43 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 08:39:42 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Xiaohan", ""], ["Gao", "Yongsheng", ""]]}, {"id": "2107.02345", "submitter": "Timothy Yu", "authors": "Ricky Chen, Timothy T. Yu, Gavin Xu, Da Ma, Marinko V. Sarunic, Mirza\n  Faisal Beg", "title": "Domain Adaptation via CycleGAN for Retina Segmentation in Optical\n  Coherence Tomography", "comments": "10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the FDA approval of Artificial Intelligence (AI) for point-of-care\nclinical diagnoses, model generalizability is of the utmost importance as\nclinical decision-making must be domain-agnostic. A method of tackling the\nproblem is to increase the dataset to include images from a multitude of\ndomains; while this technique is ideal, the security requirements of medical\ndata is a major limitation. Additionally, researchers with developed tools\nbenefit from the addition of open-sourced data, but are limited by the\ndifference in domains. Herewith, we investigated the implementation of a\nCycle-Consistent Generative Adversarial Networks (CycleGAN) for the domain\nadaptation of Optical Coherence Tomography (OCT) volumes. This study was done\nin collaboration with the Biomedical Optics Research Group and Functional &\nAnatomical Imaging & Shape Analysis Lab at Simon Fraser University. In this\nstudy, we investigated a learning-based approach of adapting the domain of a\npublicly available dataset, UK Biobank dataset (UKB). To evaluate the\nperformance of domain adaptation, we utilized pre-existing retinal layer\nsegmentation tools developed on a different set of RETOUCH OCT data. This study\nprovides insight on state-of-the-art tools for domain adaptation compared to\ntraditional processing techniques as well as a pipeline for adapting publicly\navailable retinal data to the domains previously used by our collaborators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:07:53 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Ricky", ""], ["Yu", "Timothy T.", ""], ["Xu", "Gavin", ""], ["Ma", "Da", ""], ["Sarunic", "Marinko V.", ""], ["Beg", "Mirza Faisal", ""]]}, {"id": "2107.02347", "submitter": "Marcus Kalander", "authors": "Yong Wen, Marcus Kalander, Chanfei Su, Lujia Pan", "title": "An Ensemble Noise-Robust K-fold Cross-Validation Selection Method for\n  Noisy Labels", "comments": "Accepted by the IJCAI2021 Weakly Supervised Representation Learning\n  (WSRL) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training robust and accurate deep neural networks\n(DNNs) when subject to various proportions of noisy labels. Large-scale\ndatasets tend to contain mislabeled samples that can be memorized by DNNs,\nimpeding the performance. With appropriate handling, this degradation can be\nalleviated. There are two problems to consider: how to distinguish clean\nsamples and how to deal with noisy samples. In this paper, we present Ensemble\nNoise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select\nclean samples from noisy data, solving the first problem. For the second\nproblem, we create a new pseudo label for any sample determined to have an\nuncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels\nfor each sample and the entropy of these labels is used to tune the weight\ngiven to the pseudo label and the given label. Theoretical analysis and\nextensive verification of the algorithms in the noisy label setting are\nprovided. We evaluate our approach on various image and text classification\ntasks where the labels have been manually corrupted with different noise\nratios. Additionally, two large real-world noisy datasets are also used,\nClothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant\nto considerable proportions of label noise and has a consistent improvement\nover state-of-the-art methods. Especially on more difficult datasets with\nhigher noise ratios, we can achieve a significant improvement over the\nsecond-best model. Moreover, our proposed approach can easily be integrated\ninto existing DNN methods to improve their robustness against label noise.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:14:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wen", "Yong", ""], ["Kalander", "Marcus", ""], ["Su", "Chanfei", ""], ["Pan", "Lujia", ""]]}, {"id": "2107.02368", "submitter": "Taehun Kim", "authors": "Taehun Kim, Hyemin Lee, Daijin Kim", "title": "UACANet: Uncertainty Augmented Context Attention for Polyp Segmentation", "comments": "9 pages, 6 figures, 4 tables. To appear in the Proceedings of the\n  29th ACM International Conference on Multimedia (ACM MM '21), October 20-24,\n  2021, Chengdu, China. DOI will be added soon", "journal-ref": null, "doi": "10.1145/3474085.3475375", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Uncertainty Augmented Context Attention network (UACANet) for\npolyp segmentation which consider a uncertain area of the saliency map. We\nconstruct a modified version of U-Net shape network with additional encoder and\ndecoder and compute a saliency map in each bottom-up stream prediction module\nand propagate to the next prediction module. In each prediction module,\npreviously predicted saliency map is utilized to compute foreground, background\nand uncertain area map and we aggregate the feature map with three area maps\nfor each representation. Then we compute the relation between each\nrepresentation and each pixel in the feature map. We conduct experiments on\nfive popular polyp segmentation benchmarks, Kvasir, CVC-ClinicDB, ETIS,\nCVC-ColonDB and CVC-300, and achieve state-of-the-art performance. Especially,\nwe achieve 76.6% mean Dice on ETIS dataset which is 13.8% improvement compared\nto the previous state-of-the-art method. Source code is publicly available at\nhttps://github.com/plemeri/UACANet\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 03:11:12 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:26:27 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 00:20:25 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kim", "Taehun", ""], ["Lee", "Hyemin", ""], ["Kim", "Daijin", ""]]}, {"id": "2107.02380", "submitter": "Mengxi Jia", "authors": "Mengxi Jia, Xinhua Cheng, Shijian Lu and Jian Zhang", "title": "Learning Disentangled Representation Implicitly via Transformer for\n  Occluded Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person re-identification (re-ID) under various occlusions has been a\nlong-standing challenge as person images with different types of occlusions\noften suffer from misalignment in image matching and ranking. Most existing\nmethods tackle this challenge by aligning spatial features of body parts\naccording to external semantic cues or feature similarities but this alignment\napproach is complicated and sensitive to noises. We design DRL-Net, a\ndisentangled representation learning network that handles occluded re-ID\nwithout requiring strict person image alignment or any additional supervision.\nLeveraging transformer architectures, DRL-Net achieves alignment-free re-ID via\nglobal reasoning of local features of occluded person images. It measures image\nsimilarity by automatically disentangling the representation of undefined\nsemantic components, e.g., human body parts or obstacles, under the guidance of\nsemantic preference object queries in the transformer. In addition, we design a\ndecorrelation constraint in the transformer decoder and impose it over object\nqueries for better focus on different semantic components. To better eliminate\ninterference from occlusions, we design a contrast feature learning technique\n(CFL) for better separation of occlusion features and discriminative ID\nfeatures. Extensive experiments over occluded and holistic re-ID benchmarks\n(Occluded-DukeMTMC, Market1501 and DukeMTMC) show that the DRL-Net achieves\nsuperior re-ID performance consistently and outperforms the state-of-the-art by\nlarge margins for Occluded-DukeMTMC.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 04:24:10 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jia", "Mengxi", ""], ["Cheng", "Xinhua", ""], ["Lu", "Shijian", ""], ["Zhang", "Jian", ""]]}, {"id": "2107.02389", "submitter": "Qingyong Hu", "authors": "Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua\n  Wang, Niki Trigoni and Andrew Markham", "title": "Learning Semantic Segmentation of Large-Scale Point Clouds with Random\n  Sampling", "comments": "IEEE TPAMI 2021. arXiv admin note: substantial text overlap with\n  arXiv:1911.11236", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3083288", "report-no": null, "categories": "cs.CV cs.AI cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of efficient semantic segmentation of large-scale 3D\npoint clouds. By relying on expensive sampling techniques or computationally\nheavy pre/post-processing steps, most existing approaches are only able to be\ntrained and operate over small-scale point clouds. In this paper, we introduce\nRandLA-Net, an efficient and lightweight neural architecture to directly infer\nper-point semantics for large-scale point clouds. The key to our approach is to\nuse random point sampling instead of more complex point selection approaches.\nAlthough remarkably computation and memory efficient, random sampling can\ndiscard key features by chance. To overcome this, we introduce a novel local\nfeature aggregation module to progressively increase the receptive field for\neach 3D point, thereby effectively preserving geometric details. Comparative\nexperiments show that our RandLA-Net can process 1 million points in a single\npass up to 200x faster than existing approaches. Moreover, extensive\nexperiments on five large-scale point cloud datasets, including Semantic3D,\nSemanticKITTI, Toronto3D, NPM3D and S3DIS, demonstrate the state-of-the-art\nsemantic segmentation performance of our RandLA-Net.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 05:08:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Xie", "Linhai", ""], ["Rosa", "Stefano", ""], ["Guo", "Yulan", ""], ["Wang", "Zhihua", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2107.02393", "submitter": "Sota Kato", "authors": "Sota Kato, Kazuhiro Hotta", "title": "MSE Loss with Outlying Label for Imbalanced Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose mean squared error (MSE) loss with outlying label\nfor class imbalanced classification. Cross entropy (CE) loss, which is widely\nused for image recognition, is learned so that the probability value of true\nclass is closer to one by back propagation. However, for imbalanced datasets,\nthe learning is insufficient for the classes with a small number of samples.\nTherefore, we propose a novel classification method using the MSE loss that can\nbe learned the relationships of all classes no matter which image is input.\nUnlike CE loss, MSE loss is possible to equalize the number of back propagation\nfor all classes and to learn the feature space considering the relationships\nbetween classes as metric learning. Furthermore, instead of the usual one-hot\nteacher label, we use a novel teacher label that takes the number of class\nsamples into account. This induces the outlying label which depends on the\nnumber of samples in each class, and the class with a small number of samples\nhas outlying margin in a feature space. It is possible to create the feature\nspace for separating high-difficulty classes and low-difficulty classes. By the\nexperiments on imbalanced classification and semantic segmentation, we\nconfirmed that the proposed method was much improved in comparison with\nstandard CE loss and conventional methods, even though only the loss and\nteacher labels were changed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 05:17:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kato", "Sota", ""], ["Hotta", "Kazuhiro", ""]]}, {"id": "2107.02396", "submitter": "Wei Li", "authors": "Wei Li, Yuanjun Xiong, Shuo Yang, Mingze Xu, Yongxin Wang, Wei Xia", "title": "Semi-TCL: Semi-Supervised Track Contrastive Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online tracking of multiple objects in videos requires strong capacity of\nmodeling and matching object appearances. Previous methods for learning\nappearance embedding mostly rely on instance-level matching without considering\nthe temporal continuity provided by videos. We design a new instance-to-track\nmatching objective to learn appearance embedding that compares a candidate\ndetection to the embedding of the tracks persisted in the tracker. It enables\nus to learn not only from videos labeled with complete tracks, but also\nunlabeled or partially labeled videos. We implement this learning objective in\na unified form following the spirit of constrastive loss. Experiments on\nmultiple object tracking datasets demonstrate that our method can effectively\nlearning discriminative appearance embeddings in a semi-supervised fashion and\noutperform state of the art methods on representative benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 05:23:30 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Li", "Wei", ""], ["Xiong", "Yuanjun", ""], ["Yang", "Shuo", ""], ["Xu", "Mingze", ""], ["Wang", "Yongxin", ""], ["Xia", "Wei", ""]]}, {"id": "2107.02398", "submitter": "Shang Li", "authors": "Shang Li, Guixuan Zhang, Zhengxiong Luo, Jie Liu, Zhi Zeng, Shuwu\n  Zhang", "title": "From General to Specific: Online Updating for Blind Super-Resolution", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most deep learning-based super-resolution (SR) methods are not\nimage-specific: 1) They are exhaustively trained on datasets synthesized by\npredefined blur kernels (\\eg bicubic), regardless of the domain gap with test\nimages. 2) Their model weights are fixed during testing, which means that test\nimages with various degradations are super-resolved by the same set of weights.\nHowever, degradations of real images are various and unknown (\\ie blind SR). It\nis hard for a single model to perform well in all cases. To address these\nissues, we propose an online super-resolution (ONSR) method. It does not rely\non predefined blur kernels and allows the model weights to be updated according\nto the degradation of the test image. Specifically, ONSR consists of two\nbranches, namely internal branch (IB) and external branch (EB). IB could learn\nthe specific degradation of the given test LR image, and EB could learn to\nsuper resolve images degraded by the learned degradation. In this way, ONSR\ncould customize a specific model for each test image, and thus could be more\ntolerant with various degradations in real applications. Extensive experiments\non both synthesized and real-world images show that ONSR can generate more\nvisually favorable SR results and achieve state-of-the-art performance in blind\nSR.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 05:25:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Li", "Shang", ""], ["Zhang", "Guixuan", ""], ["Luo", "Zhengxiong", ""], ["Liu", "Jie", ""], ["Zeng", "Zhi", ""], ["Zhang", "Shuwu", ""]]}, {"id": "2107.02407", "submitter": "Marc Habermann", "authors": "Marc Habermann, Weipeng Xu, Helge Rhodin, Michael Zollhoefer, Gerard\n  Pons-Moll, Christian Theobalt", "title": "NRST: Non-rigid Surface Tracking from Monocular Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an efficient method for non-rigid surface tracking from monocular\nRGB videos. Given a video and a template mesh, our algorithm sequentially\nregisters the template non-rigidly to each frame. We formulate the per-frame\nregistration as an optimization problem that includes a novel texture term\nspecifically tailored towards tracking objects with uniform texture but\nfine-scale structure, such as the regular micro-structural patterns of fabric.\nOur texture term exploits the orientation information in the micro-structures\nof the objects, e.g., the yarn patterns of fabrics. This enables us to\naccurately track uniformly colored materials that have these high frequency\nmicro-structures, for which traditional photometric terms are usually less\neffective. The results demonstrate the effectiveness of our method on both\ngeneral textured non-rigid objects and monochromatic fabrics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:06:45 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 08:55:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Habermann", "Marc", ""], ["Xu", "Weipeng", ""], ["Rhodin", "Helge", ""], ["Zollhoefer", "Michael", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "2107.02408", "submitter": "Shahroz Tariq", "authors": "Minha Kim and Shahroz Tariq and Simon S. Woo", "title": "CoReD: Generalizing Fake Media Detection with Continual Representation\n  using Distillation", "comments": "10 pages, 2 Figures, 10 Tables, Accepted for publication in the 29th\n  ACM International Conference on Multimedia (ACMMM '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CoL), Representation Learning (ReL), and\nKnowledge Distillation (KD). We design CoReD to perform sequential domain\nadaptation tasks on new deepfake and GAN-generated synthetic face datasets,\nwhile effectively minimizing the catastrophic forgetting in a teacher-student\nmodel setting. Our extensive experimental results demonstrate that our method\nis efficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:07:17 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 05:27:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kim", "Minha", ""], ["Tariq", "Shahroz", ""], ["Woo", "Simon S.", ""]]}, {"id": "2107.02411", "submitter": "Yohei Koga", "authors": "Yohei Koga, Hiroyuki Miyazaki, Ryosuke Shibasaki", "title": "Adapting Vehicle Detector to Target Domain by Adversarial Prediction\n  Alignment", "comments": "The accepted version of the article in IEEE International Geoscience\n  and Remote Sensing Symposium (IGARSS) 2021. Copyright 2021 IEEE. Code\n  available: https://github.com/monotaro3/vd_pred_align", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent advancement of domain adaptation techniques is significant, most\nof methods only align a feature extractor and do not adapt a classifier to\ntarget domain, which would be a cause of performance degradation. We propose\nnovel domain adaptation technique for object detection that aligns prediction\noutput space. In addition to feature alignment, we aligned predictions of\nlocations and class confidences of our vehicle detector for satellite images by\nadversarial training. The proposed method significantly improved AP score by\nover 5%, which shows effectivity of our method for object detection tasks in\nsatellite images.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:21:39 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Koga", "Yohei", ""], ["Miyazaki", "Hiroyuki", ""], ["Shibasaki", "Ryosuke", ""]]}, {"id": "2107.02433", "submitter": "Zhe Xu", "authors": "Zhe Xu, Jie Luo, Donghuan Lu, Jiangpeng Yan, Jayender Jagadeesan,\n  William Wells III, Sarah Frisken, Kai Ma, Yefeng Zheng, Raymond Kai-yu Tong", "title": "Double-Uncertainty Assisted Spatial and Temporal Regularization\n  Weighting for Learning-based Registration", "comments": "11 pages, version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to tackle the difficulty associated with the ill-posed nature of the\nimage registration problem, researchers use regularization to constrain the\nsolution space. For most learning-based registration approaches, the\nregularization usually has a fixed weight and only constrains the spatial\ntransformation. Such convention has two limitations: (1) The regularization\nstrength of a specific image pair should be associated with the content of the\nimages, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially\nregularizing the transformation (but overlooking the temporal consistency of\ndifferent estimations) may not be the best strategy to cope with the\nill-posedness. In this study, we propose a mean-teacher based registration\nframework. This framework incorporates an additional \\textit{temporal\nregularization} term by encouraging the teacher model's temporal ensemble\nprediction to be consistent with that of the student model. At each training\nstep, it also automatically adjusts the weights of the \\textit{spatial\nregularization} and the \\textit{temporal regularization} by taking account of\nthe transformation uncertainty and appearance uncertainty derived from the\nperturbed teacher model. We perform experiments on multi- and uni-modal\nregistration tasks, and the results show that our strategy outperforms the\ntraditional and learning-based benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:19:49 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 03:17:06 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Zhe", ""], ["Luo", "Jie", ""], ["Lu", "Donghuan", ""], ["Yan", "Jiangpeng", ""], ["Jagadeesan", "Jayender", ""], ["Wells", "William", "III"], ["Frisken", "Sarah", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""], ["Tong", "Raymond Kai-yu", ""]]}, {"id": "2107.02434", "submitter": "Shunquan Tan", "authors": "Long Zhuo and Shunquan Tan and Bin Li and Jiwu Huang", "title": "Self-Adversarial Training incorporating Forgery Attention for Image\n  Forgery Localization", "comments": "submitted to TIFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image editing techniques enable people to modify the content of an image\nwithout leaving visual traces and thus may cause serious security risks. Hence\nthe detection and localization of these forgeries become quite necessary and\nchallenging. Furthermore, unlike other tasks with extensive data, there is\nusually a lack of annotated forged images for training due to annotation\ndifficulties. In this paper, we propose a self-adversarial training strategy\nand a reliable coarse-to-fine network that utilizes a self-attention mechanism\nto localize forged regions in forgery images. The self-attention module is\nbased on a Channel-Wise High Pass Filter block (CW-HPF). CW-HPF leverages\ninter-channel relationships of features and extracts noise features by high\npass filters. Based on the CW-HPF, a self-attention mechanism, called forgery\nattention, is proposed to capture rich contextual dependencies of intrinsic\ninconsistency extracted from tampered regions. Specifically, we append two\ntypes of attention modules on top of CW-HPF respectively to model internal\ninterdependencies in spatial dimension and external dependencies among\nchannels. We exploit a coarse-to-fine network to enhance the noise\ninconsistency between original and tampered regions. More importantly, to\naddress the issue of insufficient training data, we design a self-adversarial\ntraining strategy that expands training data dynamically to achieve more robust\nperformance. Specifically, in each training iteration, we perform adversarial\nattacks against our network to generate adversarial examples and train our\nmodel on them. Extensive experimental results demonstrate that our proposed\nalgorithm steadily outperforms state-of-the-art methods by a clear margin in\ndifferent benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:20:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhuo", "Long", ""], ["Tan", "Shunquan", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "2107.02440", "submitter": "Nikhil Varma Keetha", "authors": "Nikhil Varma Keetha, Michael Milford and Sourav Garg", "title": "A Hierarchical Dual Model of Environment- and Place-Specific Utility for\n  Visual Place Recognition", "comments": "Accepted to IEEE Robotics and Automation Letters (RA-L) and IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) approaches have typically attempted to match\nplaces by identifying visual cues, image regions or landmarks that have high\n``utility'' in identifying a specific place. But this concept of utility is not\nsingular - rather it can take a range of forms. In this paper, we present a\nnovel approach to deduce two key types of utility for VPR: the utility of\nvisual cues `specific' to an environment, and to a particular place. We employ\ncontrastive learning principles to estimate both the environment- and\nplace-specific utility of Vector of Locally Aggregated Descriptors (VLAD)\nclusters in an unsupervised manner, which is then used to guide local feature\nmatching through keypoint selection. By combining these two utility measures,\nour approach achieves state-of-the-art performance on three challenging\nbenchmark datasets, while simultaneously reducing the required storage and\ncompute time. We provide further analysis demonstrating that unsupervised\ncluster selection results in semantically meaningful results, that finer\ngrained categorization often has higher utility for VPR than high level\nsemantic categorization (e.g. building, road), and characterise how these two\nutility measures vary across different places and environments. Source code is\nmade publicly available at https://github.com/Nik-V9/HEAPUtil.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:38:47 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Keetha", "Nikhil Varma", ""], ["Milford", "Michael", ""], ["Garg", "Sourav", ""]]}, {"id": "2107.02450", "submitter": "Dumindu Tissera", "authors": "Dumindu Tissera, Kasun Vithanage, Rukshan Wijessinghe, Subha Fernando,\n  Ranga Rodrigo", "title": "End-To-End Data-Dependent Routing in Multi-Path Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to give better performance with increased depth due\nto their ability to learn more abstract features. Although the deepening of\nnetworks has been well established, there is still room for efficient feature\nextraction within a layer which would reduce the need for mere parameter\nincrement. The conventional widening of networks by having more filters in each\nlayer introduces a quadratic increment of parameters. Having multiple parallel\nconvolutional/dense operations in each layer solves this problem, but without\nany context-dependent allocation of resources among these operations: the\nparallel computations tend to learn similar features making the widening\nprocess less effective. Therefore, we propose the use of multi-path neural\nnetworks with data-dependent resource allocation among parallel computations\nwithin layers, which also lets an input to be routed end-to-end through these\nparallel paths. To do this, we first introduce a cross-prediction based\nalgorithm between parallel tensors of subsequent layers. Second, we further\nreduce the routing overhead by introducing feature-dependent cross-connections\nbetween parallel tensors of successive layers. Our multi-path networks show\nsuperior performance to existing widening and adaptive feature extraction, and\neven ensembles, and deeper networks at similar complexity in the image\nrecognition task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:58:07 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tissera", "Dumindu", ""], ["Vithanage", "Kasun", ""], ["Wijessinghe", "Rukshan", ""], ["Fernando", "Subha", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "2107.02451", "submitter": "Kun He Prof.", "authors": "Kun He, Chao Li, Yixiao Yang, Gao Huang, John E. Hopcroft", "title": "Integrating Circle Kernels into Convolutional Neural Networks", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The square kernel is a standard unit for contemporary Convolutional Neural\nNetworks (CNNs), as it fits well on the tensor computation for the convolution\noperation. However, the receptive field in the human visual system is actually\nisotropic like a circle. Motivated by this observation, we propose using circle\nkernels with isotropic receptive fields for the convolution, and our training\ntakes approximately equivalent amount of calculation when compared with the\ncorresponding CNN with square kernels. Our preliminary experiments demonstrate\nthe rationality of circle kernels. We then propose a kernel boosting strategy\nthat integrates the circle kernels with square kernels for the training and\ninference, and we further let the kernel size/radius be learnable during the\ntraining. Note that we reparameterize the circle kernels or integrated kernels\nbefore the inference, thus taking no extra computation as well as the number of\nparameter overhead for the testing. Extensive experiments on several standard\ndatasets, ImageNet, CIFAR-10 and CIFAR-100, using the circle kernels or\nintegrated kernels on typical existing CNNs, show that our approach exhibits\nhighly competitive performance. Specifically, on ImageNet with standard data\naugmentation, our approach dramatically boosts the performance of\nMobileNetV3-Small by 5.20% top-1 accuracy and 3.39% top-5 accuracy, and boosts\nthe performance of MobileNetV3-Large by 2.16% top-1 accuracy and 1.18% top-5\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:59:36 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 09:10:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["He", "Kun", ""], ["Li", "Chao", ""], ["Yang", "Yixiao", ""], ["Huang", "Gao", ""], ["Hopcroft", "John E.", ""]]}, {"id": "2107.02453", "submitter": "Dumindu Tissera", "authors": "Dumindu Tissera, Kasun Vithanage, Rukshan Wijesinghe, Alex Xavier,\n  Sanath Jayasena, Subha Fernando, Ranga Rodrigo", "title": "Neural Mixture Models with Expectation-Maximization for End-to-end Deep\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any clustering algorithm must synchronously learn to model the clusters and\nallocate data to those clusters in the absence of labels. Mixture model-based\nmethods model clusters with pre-defined statistical distributions and allocate\ndata to those clusters based on the cluster likelihoods. They iteratively\nrefine those distribution parameters and member assignments following the\nExpectation-Maximization (EM) algorithm. However, the cluster representability\nof such hand-designed distributions that employ a limited amount of parameters\nis not adequate for most real-world clustering tasks. In this paper, we realize\nmixture model-based clustering with a neural network where the final layer\nneurons, with the aid of an additional transformation, approximate cluster\ndistribution outputs. The network parameters pose as the parameters of those\ndistributions. The result is an elegant, much-generalized representation of\nclusters than a restricted mixture of hand-designed distributions. We train the\nnetwork end-to-end via batch-wise EM iterations where the forward pass acts as\nthe E-step and the backward pass acts as the M-step. In image clustering, the\nmixture-based EM objective can be used as the clustering objective along with\nexisting representation learning methods. In particular, we show that when\nmixture-EM optimization is fused with consistency optimization, it improves the\nsole consistency optimization performance in clustering. Our trained networks\noutperform single-stage deep clustering methods that still depend on k-means,\nwith unsupervised classification accuracy of 63.8% in STL10, 58% in CIFAR10,\n25.9% in CIFAR100, and 98.9% in MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:00:58 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tissera", "Dumindu", ""], ["Vithanage", "Kasun", ""], ["Wijesinghe", "Rukshan", ""], ["Xavier", "Alex", ""], ["Jayasena", "Sanath", ""], ["Fernando", "Subha", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "2107.02462", "submitter": "Mengyang Wu", "authors": "Mengyang Wu, Wei Zeng, Chi-Wing Fu", "title": "FloorLevel-Net: Recognizing Floor-Level Lines with\n  Height-Attention-Guided Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to recognize the position and order of the floor-level lines that\ndivide adjacent building floors can benefit many applications, for example,\nurban augmented reality (AR). This work tackles the problem of locating\nfloor-level lines in street-view images, using a supervised deep learning\napproach. Unfortunately, very little data is available for training such a\nnetwork $-$ current street-view datasets contain either semantic annotations\nthat lack geometric attributes, or rectified facades without perspective\npriors. To address this issue, we first compile a new dataset and develop a new\ndata augmentation scheme to synthesize training samples by harassing (i) the\nrich semantics of existing rectified facades and (ii) perspective priors of\nbuildings in diverse street views. Next, we design FloorLevel-Net, a multi-task\nlearning network that associates explicit features of building facades and\nimplicit floor-level lines, along with a height-attention mechanism to help\nenforce a vertical ordering of floor-level lines. The generated segmentations\nare then passed to a second-stage geometry post-processing to exploit\nself-constrained geometric priors for plausible and consistent reconstruction\nof floor-level lines. Quantitative and qualitative evaluations conducted on\nassorted facades in existing datasets and street views from Google demonstrate\nthe effectiveness of our approach. Also, we present context-aware image overlay\nresults and show the potentials of our approach in enriching AR-related\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:17:59 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wu", "Mengyang", ""], ["Zeng", "Wei", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2107.02476", "submitter": "Dimitrios I. Fotiadis", "authors": "Dimitrios G. Zaridis, Eugenia Mylona, Nikolaos S. Tachos, Kostas\n  Marias, Nikolaos Papanikolaou, Manolis Tsiknakis, Dimitrios I. Fotiadis", "title": "A new smart-cropping pipeline for prostate segmentation using deep\n  learning networks", "comments": "8 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prostate segmentation from magnetic resonance imaging (MRI) is a challenging\ntask. In recent years, several network architectures have been proposed to\nautomate this process and alleviate the burden of manual annotation. Although\nthe performance of these models has achieved promising results, there is still\nroom for improvement before these models can be used safely and effectively in\nclinical practice. One of the major challenges in prostate MR image\nsegmentation is the presence of class imbalance in the image labels where the\nbackground pixels dominate over the prostate. In the present work we propose a\nDL-based pipeline for cropping the region around the prostate from MRI images\nto produce a more balanced distribution of the foreground pixels (prostate) and\nthe background pixels and improve segmentation accuracy. The effect of\nDL-cropping for improving the segmentation performance compared to standard\ncenter-cropping is assessed using five popular DL networks for prostate\nsegmentation, namely U-net, U-net+, Res Unet++, Bridge U-net and Dense U-net.\nThe proposed smart-cropping outperformed the standard center cropping in terms\nof segmentation accuracy for all the evaluated prostate segmentation networks.\nIn terms of Dice score, the highest improvement was achieved for the U-net+ and\nResU-net++ architectures corresponding to 8.9% and 8%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:42:48 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:03:09 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zaridis", "Dimitrios G.", ""], ["Mylona", "Eugenia", ""], ["Tachos", "Nikolaos S.", ""], ["Marias", "Kostas", ""], ["Papanikolaou", "Nikolaos", ""], ["Tsiknakis", "Manolis", ""], ["Fotiadis", "Dimitrios I.", ""]]}, {"id": "2107.02477", "submitter": "Huafeng Yang", "authors": "Huafeng Yang, Xingjian Chen, Fangyi Zhang, Guangyue Hei, Yunjie Wang\n  and Rong Du", "title": "GCN-Based Linkage Prediction for Face Clustering on Imbalanced Datasets:\n  An Empirical Study", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, benefiting from the expressive power of Graph Convolutional\nNetworks (GCNs), significant breakthroughs have been made in face clustering.\nHowever, rare attention has been paid to GCN-based clustering on imbalanced\ndata. Although imbalance problem has been extensively studied, the impact of\nimbalanced data on GCN-based linkage prediction task is quite different, which\nwould cause problems in two aspects: imbalanced linkage labels and biased graph\nrepresentations. The problem of imbalanced linkage labels is similar to that in\nimage classification task, but the latter is a particular problem in GCN-based\nclustering via linkage prediction. Significantly biased graph representations\nin training can cause catastrophic overfitting of a GCN model. To tackle these\nproblems, we evaluate the feasibility of those existing methods for imbalanced\nimage classification problem on graphs with extensive experiments, and present\na new method to alleviate the imbalanced labels and also augment graph\nrepresentations using a Reverse-Imbalance Weighted Sampling (RIWS) strategy,\nfollowed with insightful analyses and discussions. The code and a series of\nimbalanced benchmark datasets synthesized from MS-Celeb-1M and DeepFashion are\navailable on https://github.com/espectre/GCNs_on_imbalanced_datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:45:26 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 08:08:15 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yang", "Huafeng", ""], ["Chen", "Xingjian", ""], ["Zhang", "Fangyi", ""], ["Hei", "Guangyue", ""], ["Wang", "Yunjie", ""], ["Du", "Rong", ""]]}, {"id": "2107.02488", "submitter": "Takami Sato", "authors": "Takami Sato and Qi Alfred Chen", "title": "On Robustness of Lane Detection Models to Physical-World Adversarial\n  Attacks in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the 2017 TuSimple Lane Detection Challenge, its evaluation based on\naccuracy and F1 score has become the de facto standard to measure the\nperformance of lane detection methods. In this work, we conduct the first\nlarge-scale empirical study to evaluate the robustness of state-of-the-art lane\ndetection methods under physical-world adversarial attacks in autonomous\ndriving. We evaluate 4 major types of lane detection approaches with the\nconventional evaluation and end-to-end evaluation in autonomous driving\nscenarios and then discuss the security proprieties of each lane detection\nmodel. We demonstrate that the conventional evaluation fails to reflect the\nrobustness in end-to-end autonomous driving scenarios. Our results show that\nthe most robust model on the conventional metrics is the least robust in the\nend-to-end evaluation. Although the competition dataset and its metrics have\nplayed a substantial role in developing performant lane detection methods along\nwith the rapid development of deep neural networks, the conventional evaluation\nis becoming obsolete and the gap between the metrics and practicality is\ncritical. We hope that our study will help the community make further progress\nin building a more comprehensive framework to evaluate lane detection models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:04:47 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Sato", "Takami", ""], ["Chen", "Qi Alfred", ""]]}, {"id": "2107.02493", "submitter": "Jiajun Deng", "authors": "Xiaomeng Chu, Jiajun Deng, Yao Li, Zhenxun Yuan, Yanyong Zhang,\n  Jianmin Ji and Yu Zhang", "title": "Neighbor-Vote: Improving Monocular 3D Object Detection through Neighbor\n  Distance Voting", "comments": "Accepted by ACM Multimedia 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cameras are increasingly deployed in new application domains such as\nautonomous driving, performing 3D object detection on monocular images becomes\nan important task for visual scene understanding. Recent advances on monocular\n3D object detection mainly rely on the ``pseudo-LiDAR'' generation, which\nperforms monocular depth estimation and lifts the 2D pixels to pseudo 3D\npoints. However, depth estimation from monocular images, due to its poor\naccuracy, leads to inevitable position shift of pseudo-LiDAR points within the\nobject. Therefore, the predicted bounding boxes may suffer from inaccurate\nlocation and deformed shape. In this paper, we present a novel neighbor-voting\nmethod that incorporates neighbor predictions to ameliorate object detection\nfrom severely deformed pseudo-LiDAR point clouds. Specifically, each feature\npoint around the object forms their own predictions, and then the ``consensus''\nis achieved through voting. In this way, we can effectively combine the\nneighbors' predictions with local prediction and achieve more accurate 3D\ndetection. To further enlarge the difference between the foreground region of\ninterest (ROI) pseudo-LiDAR points and the background points, we also encode\nthe ROI prediction scores of 2D foreground pixels into the corresponding\npseudo-LiDAR points. We conduct extensive experiments on the KITTI benchmark to\nvalidate the merits of our proposed method. Our results on the bird's eye view\ndetection outperform the state-of-the-art performance by a large margin,\nespecially for the ``hard'' level detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:18:33 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chu", "Xiaomeng", ""], ["Deng", "Jiajun", ""], ["Li", "Yao", ""], ["Yuan", "Zhenxun", ""], ["Zhang", "Yanyong", ""], ["Ji", "Jianmin", ""], ["Zhang", "Yu", ""]]}, {"id": "2107.02494", "submitter": "Kai Ye", "authors": "Kai Ye, Yinru Ye, Minqiang Yang, Bin Hu", "title": "Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main challenges of image-to-image (I2I) translation are to make the\ntranslated image realistic and retain as much information from the source\ndomain as possible. To address this issue, we propose a novel architecture,\ntermed as IEGAN, which removes the encoder of each network and introduces an\nencoder that is independent of other networks. Compared with previous models,\nit embodies three advantages of our model: Firstly, it is more directly and\ncomprehensively to grasp image information since the encoder no longer receives\nloss from generator and discriminator. Secondly, the independent encoder allows\neach network to focus more on its own goal which makes the translated image\nmore realistic. Thirdly, the reduction in the number of encoders performs more\nunified image representation. However, when the independent encoder applies two\ndown-sampling blocks, it's hard to extract semantic information. To tackle this\nproblem, we propose deep and shallow information space containing\ncharacteristic and semantic information, which can guide the model to translate\nhigh-quality images under the task with significant shape or texture change. We\ncompare IEGAN with other previous models, and conduct researches on semantic\ninformation consistency and component ablation at the same time. These\nexperiments show the superiority and effectiveness of our architecture. Our\ncode is published on: https://github.com/Elvinky/IEGAN.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:18:59 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ye", "Kai", ""], ["Ye", "Yinru", ""], ["Yang", "Minqiang", ""], ["Hu", "Bin", ""]]}, {"id": "2107.02500", "submitter": "Jiatong Cai", "authors": "Jiatong Cai, Chenglu Zhu, Can Cui, Honglin Li, Tong Wu, Shichuan\n  Zhang, Lin Yang", "title": "Generalizing Nucleus Recognition Model in Multi-source Images via\n  Pruning", "comments": "Accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ki67 is a significant biomarker in the diagnosis and prognosis of cancer,\nwhose index can be evaluated by quantifying its expression in Ki67\nimmunohistochemistry (IHC) stained images. However, quantitative analysis on\nmulti-source Ki67 images is yet a challenging task in practice due to\ncross-domain distribution differences, which result from imaging variation,\nstaining styles, and lesion types. Many recent studies have made some efforts\non domain generalization (DG), whereas there are still some noteworthy\nlimitations. Specifically in the case of Ki67 images, learning invariant\nrepresentation is at the mercy of the insufficient number of domains and the\ncell categories mismatching in different domains. In this paper, we propose a\nnovel method to improve DG by searching the domain-agnostic subnetwork in a\ndomain merging scenario. Partial model parameters are iteratively pruned\naccording to the domain gap, which is caused by the data converting from a\nsingle domain into merged domains during training. In addition, the model is\noptimized by fine-tuning on merged domains to eliminate the interference of\nclass mismatching among various domains. Furthermore, an appropriate\nimplementation is attained by applying the pruning method to different parts of\nthe framework. Compared with known DG methods, our method yields excellent\nperformance in multiclass nucleus recognition of Ki67 IHC images, especially in\nthe lost category cases. Moreover, our competitive results are also evaluated\non the public dataset over the state-of-the-art DG methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:36:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Cai", "Jiatong", ""], ["Zhu", "Chenglu", ""], ["Cui", "Can", ""], ["Li", "Honglin", ""], ["Wu", "Tong", ""], ["Zhang", "Shichuan", ""], ["Yang", "Lin", ""]]}, {"id": "2107.02504", "submitter": "Amelia Jim\\'enez-S\\'anchez", "authors": "Amelia Jim\\'enez-S\\'anchez, Mickael Tardy, Miguel A. Gonz\\'alez\n  Ballester, Diana Mateus, Gemma Piella", "title": "Memory-aware curriculum federated learning for breast cancer\n  classification", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For early breast cancer detection, regular screening with mammography imaging\nis recommended. Routinary examinations result in datasets with a predominant\namount of negative samples. A potential solution to such class-imbalance is\njoining forces across multiple institutions. Developing a collaborative\ncomputer-aided diagnosis system is challenging in different ways. Patient\nprivacy and regulations need to be carefully respected. Data across\ninstitutions may be acquired from different devices or imaging protocols,\nleading to heterogeneous non-IID data. Also, for learning-based methods, new\noptimization strategies working on distributed data are required. Recently,\nfederated learning has emerged as an effective tool for collaborative learning.\nIn this setting, local models perform computation on their private data to\nupdate the global model. The order and the frequency of local updates influence\nthe final global model. Hence, the order in which samples are locally presented\nto the optimizers plays an important role. In this work, we define a\nmemory-aware curriculum learning method for the federated setting. Our\ncurriculum controls the order of the training samples paying special attention\nto those that are forgotten after the deployment of the global model. Our\napproach is combined with unsupervised domain adaptation to deal with domain\nshift while preserving data privacy. We evaluate our method with three clinical\ndatasets from different vendors. Our results verify the effectiveness of\nfederated adversarial learning for the multi-site breast cancer classification.\nMoreover, we show that our proposed memory-aware curriculum method is\nbeneficial to further improve classification performance. Our code is publicly\navailable at: https://github.com/ameliajimenez/curriculum-federated-learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:50:20 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Amelia", ""], ["Tardy", "Mickael", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""], ["Mateus", "Diana", ""], ["Piella", "Gemma", ""]]}, {"id": "2107.02524", "submitter": "Lang Nie", "authors": "Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao", "title": "Depth-Aware Multi-Grid Deep Homography Estimation with Contextual\n  Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homography estimation is an important task in computer vision, such as image\nstitching, video stabilization, and camera calibration. Traditional homography\nestimation methods heavily depend on the quantity and distribution of feature\npoints, leading to poor robustness in textureless scenes. The learning\nsolutions, on the contrary, try to learn robust deep features but demonstrate\nunsatisfying performance in the scenes of low overlap rates. In this paper, we\naddress the two problems simultaneously, by designing a contextual correlation\nlayer, which can capture the long-range correlation on feature maps and\nflexibly be bridged in a learning framework. In addition, considering that a\nsingle homography can not represent the complex spatial transformation in\ndepth-varying images with parallax, we propose to predict multi-grid homography\nfrom global to local. Moreover, we equip our network with depth perception\ncapability, by introducing a novel depth-aware shape-preserved loss. Extensive\nexperiments demonstrate the superiority of our method over other\nstate-of-the-art solutions in the synthetic benchmark dataset and real-world\ndataset. The codes and models will be available at\nhttps://github.com/nie-lang/Multi-Grid-Deep-Homogarphy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:33:12 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nie", "Lang", ""], ["Lin", "Chunyu", ""], ["Liao", "Kang", ""], ["Liu", "Shuaicheng", ""], ["Zhao", "Yao", ""]]}, {"id": "2107.02525", "submitter": "Ana-Cristina Rogoz", "authors": "Ana-Cristina Rogoz, Radu Muntean, Stefan Cobeli", "title": "Semantic Segmentation Alternative Technique: Segmentation Domain\n  Generation", "comments": "Accepted contribution at EEML2021 with poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting objects of interest in images was always a compelling task to\nautomate. In recent years this task was more and more explored using deep\nlearning techniques, mostly using region-based convolutional networks. In this\nproject we propose an alternative semantic segmentation technique making use of\nGenerative Adversarial Networks. We consider semantic segmentation to be a\ndomain transfer problem. Thus, we train a feed forward network (FFNN) to\nreceive as input a seed real image and generate as output its segmentation\nmask.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:34:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Rogoz", "Ana-Cristina", ""], ["Muntean", "Radu", ""], ["Cobeli", "Stefan", ""]]}, {"id": "2107.02543", "submitter": "Hasan Mahmud", "authors": "Hasan Mahmud, Mashrur Mahmud Morshed, Md. Kamrul Hasan", "title": "A deep-learning--based multimodal depth-aware dynamic hand gesture\n  recognition system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Any spatio-temporal movement or reorientation of the hand, done with the\nintention of conveying a specific meaning, can be considered as a hand gesture.\nInputs to hand gesture recognition systems can be in several forms, such as\ndepth images, monocular RGB, or skeleton joint points. We observe that raw\ndepth images possess low contrasts in the hand regions of interest (ROI). They\ndo not highlight important details to learn, such as finger bending information\n(whether a finger is overlapping the palm, or another finger). Recently, in\ndeep-learning--based dynamic hand gesture recognition, researchers are tying to\nfuse different input modalities (e.g. RGB or depth images and hand skeleton\njoint points) to improve the recognition accuracy. In this paper, we focus on\ndynamic hand gesture (DHG) recognition using depth quantized image features and\nhand skeleton joint points. In particular, we explore the effect of using\ndepth-quantized features in Convolutional Neural Network (CNN) and Recurrent\nNeural Network (RNN) based multi-modal fusion networks. We find that our method\nimproves existing results on the SHREC-DHG-14 dataset. Furthermore, using our\nmethod, we show that it is possible to reduce the resolution of the input\nimages by more than four times and still obtain comparable or better accuracy\nto that of the resolutions used in previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:18:53 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mahmud", "Hasan", ""], ["Morshed", "Mashrur Mahmud", ""], ["Hasan", "Md. Kamrul", ""]]}, {"id": "2107.02555", "submitter": "Dror Freirich", "authors": "Dror Freirich, Tomer Michaeli, Ron Meir", "title": "A Theory of the Distortion-Perception Tradeoff in Wasserstein Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lower the distortion of an estimator, the more the distribution of its\noutputs generally deviates from the distribution of the signals it attempts to\nestimate. This phenomenon, known as the perception-distortion tradeoff, has\ncaptured significant attention in image restoration, where it implies that\nfidelity to ground truth images comes at the expense of perceptual quality\n(deviation from statistics of natural images). However, despite the increasing\npopularity of performing comparisons on the perception-distortion plane, there\nremains an important open question: what is the minimal distortion that can be\nachieved under a given perception constraint? In this paper, we derive a closed\nform expression for this distortion-perception (DP) function for the mean\nsquared-error (MSE) distortion and the Wasserstein-2 perception index. We prove\nthat the DP function is always quadratic, regardless of the underlying\ndistribution. This stems from the fact that estimators on the DP curve form a\ngeodesic in Wasserstein space. In the Gaussian setting, we further provide a\nclosed form expression for such estimators. For general distributions, we show\nhow these estimators can be constructed from the estimators at the two extremes\nof the tradeoff: The global MSE minimizer, and a minimizer of the MSE under a\nperfect perceptual quality constraint. The latter can be obtained as a\nstochastic transformation of the former.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:53:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Freirich", "Dror", ""], ["Michaeli", "Tomer", ""], ["Meir", "Ron", ""]]}, {"id": "2107.02557", "submitter": "Pengpeng Liang", "authors": "Chengcheng Guo, Minjie Lin, Heyang Guo, Pengpeng Liang and Erkang\n  Cheng", "title": "Coarse-to-fine Semantic Localization with HD Map for Autonomous Driving\n  in Structural Scenes", "comments": "The IEEE/RSJ International Conference on Intelligent Robots and\n  Systems, IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Robust and accurate localization is an essential component for robotic\nnavigation and autonomous driving. The use of cameras for localization with\nhigh definition map (HD Map) provides an affordable localization sensor set.\nExisting methods suffer from pose estimation failure due to error prone data\nassociation or initialization with accurate initial pose requirement. In this\npaper, we propose a cost-effective vehicle localization system with HD map for\nautonomous driving that uses cameras as primary sensors. To this end, we\nformulate vision-based localization as a data association problem that maps\nvisual semantics to landmarks in HD map. Specifically, system initialization is\nfinished in a coarse to fine manner by combining coarse GPS (Global Positioning\nSystem) measurement and fine pose searching. In tracking stage, vehicle pose is\nrefined by implicitly aligning the semantic segmentation result between image\nand landmarks in HD maps with photometric consistency. Finally, vehicle pose is\ncomputed by pose graph optimization in a sliding window fashion. We evaluate\nour method on two datasets and demonstrate that the proposed approach yields\npromising localization results in different driving scenarios. Additionally,\nour approach is suitable for both monocular camera and multi-cameras that\nprovides flexibility and improves robustness for the localization system.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:58:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Guo", "Chengcheng", ""], ["Lin", "Minjie", ""], ["Guo", "Heyang", ""], ["Liang", "Pengpeng", ""], ["Cheng", "Erkang", ""]]}, {"id": "2107.02561", "submitter": "Sameera Ramasinghe Mr.", "authors": "Jianqiao Zheng, Sameera Ramasinghe, Simon Lucey", "title": "Rethinking Positional Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well noted that coordinate based MLPs benefit greatly -- in terms of\npreserving high-frequency information -- through the encoding of coordinate\npositions as an array of Fourier features. Hitherto, the rationale for the\neffectiveness of these positional encodings has been solely studied through a\nFourier lens. In this paper, we strive to broaden this understanding by showing\nthat alternative non-Fourier embedding functions can indeed be used for\npositional encoding. Moreover, we show that their performance is entirely\ndetermined by a trade-off between the stable rank of the embedded matrix and\nthe distance preservation between embedded coordinates. We further establish\nthat the now ubiquitous Fourier feature mapping of position is a special case\nthat fulfills these conditions. Consequently, we present a more general theory\nto analyze positional encoding in terms of shifted basis functions. To this\nend, we develop the necessary theoretical formulae and empirically verify that\nour theoretical claims hold in practice. Codes available at\nhttps://github.com/osiriszjq/Rethinking-positional-encoding.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:04:04 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:38:30 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zheng", "Jianqiao", ""], ["Ramasinghe", "Sameera", ""], ["Lucey", "Simon", ""]]}, {"id": "2107.02568", "submitter": "Christoph Berger", "authors": "Christoph Berger, Magdalini Paschali, Ben Glocker, Konstantinos\n  Kamnitsas", "title": "Confidence-based Out-of-Distribution Detection: A Comparative Study and\n  Analysis", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification models deployed in the real world may receive inputs\noutside the intended data distribution. For critical applications such as\nclinical decision making, it is important that a model can detect such\nout-of-distribution (OOD) inputs and express its uncertainty. In this work, we\nassess the capability of various state-of-the-art approaches for\nconfidence-based OOD detection through a comparative study and in-depth\nanalysis. First, we leverage a computer vision benchmark to reproduce and\ncompare multiple OOD detection methods. We then evaluate their capabilities on\nthe challenging task of disease classification using chest X-rays. Our study\nshows that high performance in a computer vision task does not directly\ntranslate to accuracy in a medical imaging task. We analyse factors that affect\nperformance of the methods between the two tasks. Our results provide useful\ninsights for developing the next generation of OOD detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:10:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Berger", "Christoph", ""], ["Paschali", "Magdalini", ""], ["Glocker", "Ben", ""], ["Kamnitsas", "Konstantinos", ""]]}, {"id": "2107.02572", "submitter": "Riccardo Barbano", "authors": "Riccardo Barbano, Zeljko Kereta, Andreas Hauptmann, Simon R. Arridge,\n  Bangti Jin", "title": "Unsupervised Knowledge-Transfer for Learned Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based image reconstruction approaches have demonstrated\nimpressive empirical performance in many imaging modalities. These approaches\ngenerally require a large amount of high-quality training data, which is often\nnot available. To circumvent this issue, we develop a novel unsupervised\nknowledge-transfer paradigm for learned iterative reconstruction within a\nBayesian framework. The proposed approach learns an iterative reconstruction\nnetwork in two phases. The first phase trains a reconstruction network with a\nset of ordered pairs comprising of ground truth images and measurement data.\nThe second phase fine-tunes the pretrained network to the measurement data\nwithout supervision. Furthermore, the framework delivers uncertainty\ninformation over the reconstructed image. We present extensive experimental\nresults on low-dose and sparse-view computed tomography, showing that the\nproposed framework significantly improves reconstruction quality not only\nvisually, but also quantitatively in terms of PSNR and SSIM, and is competitive\nwith several state-of-the-art supervised and unsupervised reconstruction\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:19:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Barbano", "Riccardo", ""], ["Kereta", "Zeljko", ""], ["Hauptmann", "Andreas", ""], ["Arridge", "Simon R.", ""], ["Jin", "Bangti", ""]]}, {"id": "2107.02575", "submitter": "Yunze Liu", "authors": "Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas Funkhouser,\n  Li Yi", "title": "Contrastive Multimodal Fusion with TupleInfoNCE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for representation learning of multimodal data\nusing contrastive losses. A traditional approach is to contrast different\nmodalities to learn the information shared between them. However, that approach\ncould fail to learn the complementary synergies between modalities that might\nbe useful for downstream tasks. Another approach is to concatenate all the\nmodalities into a tuple and then contrast positive and negative tuple\ncorrespondences. However, that approach could consider only the stronger\nmodalities while ignoring the weaker ones. To address these issues, we propose\na novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based\nnot only on positive and negative correspondences but also by composing new\nnegative tuples using modalities describing different scenes. Training with\nthese additional negatives encourages the learning model to examine the\ncorrespondences among modalities in the same tuple, ensuring that weak\nmodalities are not ignored. We provide a theoretical justification based on\nmutual information for why this approach works, and we propose a sample\noptimization algorithm to generate positive and negative samples to maximize\ntraining efficacy. We find that TupleInfoNCE significantly outperforms the\nprevious state of the arts on three different downstream tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:26:58 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Yunze", ""], ["Fan", "Qingnan", ""], ["Zhang", "Shanghang", ""], ["Dong", "Hao", ""], ["Funkhouser", "Thomas", ""], ["Yi", "Li", ""]]}, {"id": "2107.02583", "submitter": "Lifa Zhu", "authors": "Lifa Zhu, Dongrui Liu, Changwei Lin, Rui Yan, Francisco\n  G\\'omez-Fern\\'andez, Ninghua Yang, Ziyong Feng", "title": "Point Cloud Registration using Representative Overlapping Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D point cloud registration is a fundamental task in robotics and computer\nvision. Recently, many learning-based point cloud registration methods based on\ncorrespondences have emerged. However, these methods heavily rely on such\ncorrespondences and meet great challenges with partial overlap. In this paper,\nwe propose ROPNet, a new deep learning model using Representative Overlapping\nPoints with discriminative features for registration that transforms\npartial-to-partial registration into partial-to-complete registration.\nSpecifically, we propose a context-guided module which uses an encoder to\nextract global features for predicting point overlap score. To better find\nrepresentative overlapping points, we use the extracted global features for\ncoarse alignment. Then, we introduce a Transformer to enrich point features and\nremove non-representative points based on point overlap score and feature\nmatching. A similarity matrix is built in a partial-to-complete mode, and\nfinally, weighted SVD is adopted to estimate a transformation matrix. Extensive\nexperiments over ModelNet40 using noisy and partially overlapping point clouds\nshow that the proposed method outperforms traditional and learning-based\nmethods, achieving state-of-the-art performance. The code is available at\nhttps://github.com/zhulf0804/ROPNet.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:52:22 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhu", "Lifa", ""], ["Liu", "Dongrui", ""], ["Lin", "Changwei", ""], ["Yan", "Rui", ""], ["G\u00f3mez-Fern\u00e1ndez", "Francisco", ""], ["Yang", "Ninghua", ""], ["Feng", "Ziyong", ""]]}, {"id": "2107.02586", "submitter": "Georgios Kaissis", "authors": "Alexander Ziller, Dmitrii Usynin, Nicolas Remerscheid, Moritz Knolle,\n  Marcus Makowski, Rickmer Braren, Daniel Rueckert, Georgios Kaissis", "title": "Differentially private federated deep learning for multi-site medical\n  image segmentation", "comments": "Submitted to the Journal of Machine Learning in Biomedical Imaging\n  (MELBA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative machine learning techniques such as federated learning (FL)\nenable the training of models on effectively larger datasets without data\ntransfer. Recent initiatives have demonstrated that segmentation models trained\nwith FL can achieve performance similar to locally trained models. However, FL\nis not a fully privacy-preserving technique and privacy-centred attacks can\ndisclose confidential patient data. Thus, supplementing FL with\nprivacy-enhancing technologies (PTs) such as differential privacy (DP) is a\nrequirement for clinical applications in a multi-institutional setting. The\napplication of PTs to FL in medical imaging and the trade-offs between privacy\nguarantees and model utility, the ramifications on training performance and the\nsusceptibility of the final models to attacks have not yet been conclusively\ninvestigated. Here we demonstrate the first application of differentially\nprivate gradient descent-based FL on the task of semantic segmentation in\ncomputed tomography. We find that high segmentation performance is possible\nunder strong privacy guarantees with an acceptable training time penalty. We\nfurthermore demonstrate the first successful gradient-based model inversion\nattack on a semantic segmentation model and show that the application of DP\nprevents it from divulging sensitive image features.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:57:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ziller", "Alexander", ""], ["Usynin", "Dmitrii", ""], ["Remerscheid", "Nicolas", ""], ["Knolle", "Moritz", ""], ["Makowski", "Marcus", ""], ["Braren", "Rickmer", ""], ["Rueckert", "Daniel", ""], ["Kaissis", "Georgios", ""]]}, {"id": "2107.02600", "submitter": "Edgar Kaziakhmedov", "authors": "Paul Hilt, Edgar Kaziakhmedov, Sourabh Bhide, Maria Leptin, Constantin\n  Pape, Anna Kreshuk", "title": "Stateless actor-critic for instance segmentation with high-level priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instance segmentation is an important computer vision problem which remains\nchallenging despite impressive recent advances due to deep learning-based\nmethods. Given sufficient training data, fully supervised methods can yield\nexcellent performance, but annotation of ground-truth data remains a major\nbottleneck, especially for biomedical applications where it has to be performed\nby domain experts. The amount of labels required can be drastically reduced by\nusing rules derived from prior knowledge to guide the segmentation. However,\nthese rules are in general not differentiable and thus cannot be used with\nexisting methods. Here, we relax this requirement by using stateless actor\ncritic reinforcement learning, which enables non-differentiable rewards. We\nformulate the instance segmentation problem as graph partitioning and the actor\ncritic predicts the edge weights driven by the rewards, which are based on the\nconformity of segmented instances to high-level priors on object shape,\nposition or size. The experiments on toy and real datasets demonstrate that we\ncan achieve excellent performance without any direct supervision based only on\na rich set of priors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:20:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hilt", "Paul", ""], ["Kaziakhmedov", "Edgar", ""], ["Bhide", "Sourabh", ""], ["Leptin", "Maria", ""], ["Pape", "Constantin", ""], ["Kreshuk", "Anna", ""]]}, {"id": "2107.02612", "submitter": "Davide Coccomini", "authors": "Davide Coccomini, Nicola Messina, Claudio Gennaro and Fabrizio Falchi", "title": "Combining EfficientNet and Vision Transformers for Video Deepfake\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfakes are the result of digital manipulation to obtain credible videos in\norder to deceive the viewer. This is done through deep learning techniques\nbased on autoencoders or GANs that become more accessible and accurate year\nafter year, resulting in fake videos that are very difficult to distinguish\nfrom real ones. Traditionally, CNN networks have been used to perform deepfake\ndetection, with the best results obtained using methods based on EfficientNet\nB7. In this study, we combine various types of Vision Transformers with a\nconvolutional EfficientNet B0 used as a feature extractor, obtaining comparable\nresults with some very recent methods that use Vision Transformers. Differently\nfrom the state-of-the-art approaches, we use neither distillation nor ensemble\nmethods. The best model achieved an AUC of 0.951 and an F1 score of 88.0%, very\nclose to the state-of-the-art on the DeepFake Detection Challenge (DFDC).\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:35:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Coccomini", "Davide", ""], ["Messina", "Nicola", ""], ["Gennaro", "Claudio", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "2107.02622", "submitter": "Jeremy Tan", "authors": "Jeremy Tan, Benjamin Hou, Thomas Day, John Simpson, Daniel Rueckert,\n  Bernhard Kainz", "title": "Detecting Outliers with Poisson Image Interpolation", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning of every possible pathology is unrealistic for many\nprimary care applications like health screening. Image anomaly detection\nmethods that learn normal appearance from only healthy data have shown\npromising results recently. We propose an alternative to image\nreconstruction-based and image embedding-based methods and propose a new\nself-supervised method to tackle pathological anomaly detection. Our approach\noriginates in the foreign patch interpolation (FPI) strategy that has shown\nsuperior performance on brain MRI and abdominal CT data. We propose to use a\nbetter patch interpolation strategy, Poisson image interpolation (PII), which\nmakes our method suitable for applications in challenging data regimes. PII\noutperforms state-of-the-art methods by a good margin when tested on surrogate\ntasks like identifying common lung anomalies in chest X-rays or hypo-plastic\nleft heart syndrome in prenatal, fetal cardiac ultrasound images. Code\navailable at https://github.com/jemtan/PII.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:53:17 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tan", "Jeremy", ""], ["Hou", "Benjamin", ""], ["Day", "Thomas", ""], ["Simpson", "John", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2107.02629", "submitter": "Yufei Wang", "authors": "Yufei Wang, Haoliang Li, Lap-pui Chau, Alex C. Kot", "title": "Embracing the Dark Knowledge: Domain Generalization Using Regularized\n  Knowledge Distillation", "comments": "Accepted by ACM MM, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though convolutional neural networks are widely used in different tasks, lack\nof generalization capability in the absence of sufficient and representative\ndata is one of the challenges that hinder their practical application. In this\npaper, we propose a simple, effective, and plug-and-play training strategy\nnamed Knowledge Distillation for Domain Generalization (KDDG) which is built\nupon a knowledge distillation framework with the gradient filter as a novel\nregularization term. We find that both the ``richer dark knowledge\" from the\nteacher network, as well as the gradient filter we proposed, can reduce the\ndifficulty of learning the mapping which further improves the generalization\nability of the model. We also conduct experiments extensively to show that our\nframework can significantly improve the generalization capability of deep\nneural networks in different tasks including image classification,\nsegmentation, reinforcement learning by comparing our method with existing\nstate-of-the-art domain generalization techniques. Last but not the least, we\npropose to adopt two metrics to analyze our proposed method in order to better\nunderstand how our proposed method benefits the generalization capability of\ndeep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:08:54 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Yufei", ""], ["Li", "Haoliang", ""], ["Chau", "Lap-pui", ""], ["Kot", "Alex C.", ""]]}, {"id": "2107.02630", "submitter": "Wele Gedara Chaminda Bandara", "authors": "Wele Gedara Chaminda Bandara, Jeya Maria Jose Valanarasu, Vishal M.\n  Patel", "title": "Hyperspectral Pansharpening Based on Improved Deep Image Prior and\n  Residual Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral pansharpening aims to synthesize a low-resolution hyperspectral\nimage (LR-HSI) with a registered panchromatic image (PAN) to generate an\nenhanced HSI with high spectral and spatial resolution. Recently proposed HS\npansharpening methods have obtained remarkable results using deep convolutional\nnetworks (ConvNets), which typically consist of three steps: (1) up-sampling\nthe LR-HSI, (2) predicting the residual image via a ConvNet, and (3) obtaining\nthe final fused HSI by adding the outputs from first and second steps. Recent\nmethods have leveraged Deep Image Prior (DIP) to up-sample the LR-HSI due to\nits excellent ability to preserve both spatial and spectral information,\nwithout learning from large data sets. However, we observed that the quality of\nup-sampled HSIs can be further improved by introducing an additional\nspatial-domain constraint to the conventional spectral-domain energy function.\nWe define our spatial-domain constraint as the $L_1$ distance between the\npredicted PAN image and the actual PAN image. To estimate the PAN image of the\nup-sampled HSI, we also propose a learnable spectral response function (SRF).\nMoreover, we noticed that the residual image between the up-sampled HSI and the\nreference HSI mainly consists of edge information and very fine structures. In\norder to accurately estimate fine information, we propose a novel over-complete\nnetwork, called HyperKite, which focuses on learning high-level features by\nconstraining the receptive from increasing in the deep layers. We perform\nexperiments on three HSI datasets to demonstrate the superiority of our\nDIP-HyperKite over the state-of-the-art pansharpening methods. The deployment\ncodes, pre-trained models, and final fusion outputs of our DIP-HyperKite and\nthe methods used for the comparisons will be publicly made available at\nhttps://github.com/wgcban/DIP-HyperKite.git.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:11:03 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bandara", "Wele Gedara Chaminda", ""], ["Valanarasu", "Jeya Maria Jose", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2107.02638", "submitter": "Sanket Biswas", "authors": "Sanket Biswas, Pau Riba, Josep Llad\\'os and Umapada Pal", "title": "DocSynth: A Layout Guided Approach for Controllable Document Image\n  Synthesis", "comments": "Accepted by ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite significant progress on current state-of-the-art image generation\nmodels, synthesis of document images containing multiple and complex object\nlayouts is a challenging task. This paper presents a novel approach, called\nDocSynth, to automatically synthesize document images based on a given layout.\nIn this work, given a spatial layout (bounding boxes with object categories) as\na reference by the user, our proposed DocSynth model learns to generate a set\nof realistic document images consistent with the defined layout. Also, this\nframework has been adapted to this work as a superior baseline model for\ncreating synthetic document image datasets for augmenting real data during\ntraining for document layout analysis tasks. Different sets of learning\nobjectives have been also used to improve the model performance.\nQuantitatively, we also compare the generated results of our model with real\ndata using standard evaluation metrics. The results highlight that our model\ncan successfully generate realistic and diverse document images with multiple\nobjects. We also present a comprehensive qualitative analysis summary of the\ndifferent scopes of synthetic image generation tasks. Lastly, to our knowledge\nthis is the first work of its kind.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:24:30 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Biswas", "Sanket", ""], ["Riba", "Pau", ""], ["Llad\u00f3s", "Josep", ""], ["Pal", "Umapada", ""]]}, {"id": "2107.02643", "submitter": "Samuel Budd", "authors": "Samuel Budd, Matthew Sinclair, Thomas Day, Athanasios Vlontzos, Jeremy\n  Tan, Tianrui Liu, Jaqueline Matthew, Emily Skelton, John Simpson, Reza\n  Razavi, Ben Glocker, Daniel Rueckert, Emma C. Robinson, Bernhard Kainz", "title": "Detecting Hypo-plastic Left Heart Syndrome in Fetal Ultrasound via\n  Disease-specific Atlas Maps", "comments": "MICCAI'21 Main Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fetal ultrasound screening during pregnancy plays a vital role in the early\ndetection of fetal malformations which have potential long-term health impacts.\nThe level of skill required to diagnose such malformations from live ultrasound\nduring examination is high and resources for screening are often limited. We\npresent an interpretable, atlas-learning segmentation method for automatic\ndiagnosis of Hypo-plastic Left Heart Syndrome (HLHS) from a single `4 Chamber\nHeart' view image. We propose to extend the recently introduced\nImage-and-Spatial Transformer Networks (Atlas-ISTN) into a framework that\nenables sensitising atlas generation to disease. In this framework we can\njointly learn image segmentation, registration, atlas construction and disease\nprediction while providing a maximum level of clinical interpretability\ncompared to direct image classification methods. As a result our segmentation\nallows diagnoses competitive with expert-derived manual diagnosis and yields an\nAUC-ROC of 0.978 (1043 cases for training, 260 for validation and 325 for\ntesting).\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:31:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Budd", "Samuel", ""], ["Sinclair", "Matthew", ""], ["Day", "Thomas", ""], ["Vlontzos", "Athanasios", ""], ["Tan", "Jeremy", ""], ["Liu", "Tianrui", ""], ["Matthew", "Jaqueline", ""], ["Skelton", "Emily", ""], ["Simpson", "John", ""], ["Razavi", "Reza", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""], ["Robinson", "Emma C.", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2107.02655", "submitter": "Pietro Gori", "authors": "Giammarco La Barbera and Pietro Gori and Haithem Boussaid and Bruno\n  Belucci and Alessandro Delmonte and Jeanne Goulin and Sabine Sarnacki and\n  Laurence Rouet and Isabelle Bloch", "title": "Automatic size and pose homogenization with spatial transformer network\n  to improve and accelerate pediatric segmentation", "comments": "ISBI 2021", "journal-ref": "ISBI 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a high heterogeneity in pose and size and to a limited number of\navailable data, segmentation of pediatric images is challenging for deep\nlearning methods. In this work, we propose a new CNN architecture that is pose\nand scale invariant thanks to the use of Spatial Transformer Network (STN). Our\narchitecture is composed of three sequential modules that are estimated\ntogether during training: (i) a regression module to estimate a similarity\nmatrix to normalize the input image to a reference one; (ii) a differentiable\nmodule to find the region of interest to segment; (iii) a segmentation module,\nbased on the popular UNet architecture, to delineate the object. Unlike the\noriginal UNet, which strives to learn a complex mapping, including pose and\nscale variations, from a finite training dataset, our segmentation module\nlearns a simpler mapping focusing on images with normalized pose and size.\nFurthermore, the use of an automatic bounding box detection through STN allows\nsaving time and especially memory, while keeping similar performance. We test\nthe proposed method in kidney and renal tumor segmentation on abdominal\npediatric CT scanners. Results indicate that the estimated STN homogenization\nof size and pose accelerates the segmentation (25h), compared to standard\ndata-augmentation (33h), while obtaining a similar quality for the kidney\n(88.01\\% of Dice score) and improving the renal tumor delineation (from 85.52\\%\nto 87.12\\%).\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:50:03 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["La Barbera", "Giammarco", ""], ["Gori", "Pietro", ""], ["Boussaid", "Haithem", ""], ["Belucci", "Bruno", ""], ["Delmonte", "Alessandro", ""], ["Goulin", "Jeanne", ""], ["Sarnacki", "Sabine", ""], ["Rouet", "Laurence", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2107.02660", "submitter": "Shuaizheng Yan", "authors": "Shuaizheng Yan, Xingyu Chen, Zhengxing Wu, Jian Wang, Yue Lu, Min Tan,\n  and Junzhi Yu", "title": "HybrUR: A Hybrid Physical-Neural Solution for Unsupervised Underwater\n  Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Robust vision restoration for an underwater image remains a challenging\nproblem. For the lack of aligned underwater-terrestrial image pairs, the\nunsupervised method is more suited to this task. However, the pure data-driven\nunsupervised method usually has difficulty in achieving realistic color\ncorrection for lack of optical constraint. In this paper, we propose a data-\nand physics-driven unsupervised architecture that learns underwater vision\nrestoration from unpaired underwater-terrestrial images. For sufficient domain\ntransformation and detail preservation, the underwater degeneration needs to be\nexplicitly constructed based on the optically unambiguous physics law. Thus, we\nemploy the Jaffe-McGlamery degradation theory to design the generation models,\nand use neural networks to describe the process of underwater degradation.\nFurthermore, to overcome the problem of invalid gradient when optimizing the\nhybrid physical-neural model, we fully investigate the intrinsic correlation\nbetween the scene depth and the degradation factors for the backscattering\nestimation, to improve the restoration performance through physical\nconstraints. Our experimental results show that the proposed method is able to\nperform high-quality restoration for unconstrained underwater images without\nany supervision. On multiple benchmarks, we outperform several state-of-the-art\nsupervised and unsupervised approaches. We also demonstrate that our methods\nyield encouraging results on real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:00:30 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yan", "Shuaizheng", ""], ["Chen", "Xingyu", ""], ["Wu", "Zhengxing", ""], ["Wang", "Jian", ""], ["Lu", "Yue", ""], ["Tan", "Min", ""], ["Yu", "Junzhi", ""]]}, {"id": "2107.02672", "submitter": "Nam Nguyen", "authors": "Nam Nguyen, J. Morris Chang", "title": "COVID-19 Pneumonia Severity Prediction using Hybrid\n  Convolution-Attention Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study proposed a novel framework for COVID-19 severity prediction, which\nis a combination of data-centric and model-centric approaches. First, we\npropose a data-centric pre-training for extremely scare data scenarios of the\ninvestigating dataset. Second, we propose two hybrid convolution-attention\nneural architectures that leverage the self-attention from the Transformer and\nthe Dense Associative Memory (Modern Hopfield networks). Our proposed approach\nachieves significant improvement from the conventional baseline approach. The\nbest model from our proposed approach achieves $R^2 = 0.85 \\pm 0.05$ and\nPearson correlation coefficient $\\rho = 0.92 \\pm 0.02$ in geographic extend and\n$R^2 = 0.72 \\pm 0.09, \\rho = 0.85\\pm 0.06$ in opacity prediction.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:26:07 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 17:59:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nguyen", "Nam", ""], ["Chang", "J. Morris", ""]]}, {"id": "2107.02673", "submitter": "Artem Savkin", "authors": "Kevin Strauss, Artem Savkin, Federico Tombari", "title": "Attention-based Adversarial Appearance Learning of Augmented Pedestrians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data became already an essential component of machine\nlearning-based perception in the field of autonomous driving. Yet it still\ncannot replace real data completely due to the sim2real domain shift. In this\nwork, we propose a method that leverages the advantages of the augmentation\nprocess and adversarial training to synthesize realistic data for the\npedestrian recognition task. Our approach utilizes an attention mechanism\ndriven by an adversarial loss to learn domain discrepancies and improve\nsim2real adaptation. Our experiments confirm that the proposed adaptation\nmethod is robust to such discrepancies and reveals both visual realism and\nsemantic consistency. Furthermore, we evaluate our data generation pipeline on\nthe task of pedestrian recognition and demonstrate that generated data resemble\nproperties of the real domain.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:27:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Strauss", "Kevin", ""], ["Savkin", "Artem", ""], ["Tombari", "Federico", ""]]}, {"id": "2107.02675", "submitter": "Hafez Farazi", "authors": "Arash Amini, Hafez Farazi, Sven Behnke", "title": "Real-time Pose Estimation from Images for Multiple Humanoid Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation commonly refers to computer vision methods that recognize\npeople's body postures in images or videos. With recent advancements in deep\nlearning, we now have compelling models to tackle the problem in real-time.\nSince these models are usually designed for human images, one needs to adapt\nexisting models to work on other creatures, including robots. This paper\nexamines different state-of-the-art pose estimation models and proposes a\nlightweight model that can work in real-time on humanoid robots in the RoboCup\nHumanoid League environment. Additionally, we present a novel dataset called\nthe HumanoidRobotPose dataset. The results of this work have the potential to\nenable many advanced behaviors for soccer-playing robots.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:33:57 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Amini", "Arash", ""], ["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "2107.02681", "submitter": "Jaemin Cho", "authors": "Zineng Tang, Jaemin Cho, Hao Tan, Mohit Bansal", "title": "VidLanKD: Improving Language Understanding via Video-Distilled Knowledge\n  Transfer", "comments": "18 pages (5 figures, 10 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization has attracted\nattention by using the predictions of a text-to-image retrieval model as labels\nfor language model supervision. Despite its success, the method suffers from\napproximation error of using finite image labels and the lack of vocabulary\ndiversity of a small image-text dataset. To overcome these limitations, we\npresent VidLanKD, a video-language knowledge distillation method for improving\nlanguage understanding. We train a multi-modal teacher model on a video-text\ndataset, and then transfer its knowledge to a student language model with a\ntext dataset. To avoid approximation error, we propose to use different\nknowledge distillation objectives. In addition, the use of a large-scale\nvideo-text dataset helps learn diverse and richer vocabularies. In our\nexperiments, VidLanKD achieves consistent improvements over text-only language\nmodels and vokenization models, on several downstream language understanding\ntasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world\nknowledge, physical reasoning, and temporal reasoning capabilities of our model\nby evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we\npresent comprehensive ablation studies as well as visualizations of the learned\ntext-to-video grounding results of our teacher and student language models. Our\ncode and models are available at: https://github.com/zinengtang/VidLanKD\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:41:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tang", "Zineng", ""], ["Cho", "Jaemin", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2107.02701", "submitter": "Rongjun Qin", "authors": "Hessah Albanwan, Rongjun Qin", "title": "Spatiotemporal Fusion in Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing images and techniques are powerful tools to investigate earth\nsurface. Data quality is the key to enhance remote sensing applications and\nobtaining a clear and noise-free set of data is very difficult in most\nsituations due to the varying acquisition (e.g., atmosphere and season),\nsensor, and platform (e.g., satellite angles and sensor characteristics)\nconditions. With the increasing development of satellites, nowadays Terabytes\nof remote sensing images can be acquired every day. Therefore, information and\ndata fusion can be particularly important in the remote sensing community. The\nfusion integrates data from various sources acquired asynchronously for\ninformation extraction, analysis, and quality improvement. In this chapter, we\naim to discuss the theory of spatiotemporal fusion by investigating previous\nworks, in addition to describing the basic concepts and some of its\napplications by summarizing our prior and ongoing works.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:04:04 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Albanwan", "Hessah", ""], ["Qin", "Rongjun", ""]]}, {"id": "2107.02704", "submitter": "Divya Varadarajan", "authors": "Divya Varadarajan, Katherine L. Bouman, Andre van der Kouwe, Bruce\n  Fischl, Adrian V. Dalca", "title": "Unsupervised learning of MRI tissue properties using MRI physics models", "comments": "11 Pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In neuroimaging, MRI tissue properties characterize underlying neurobiology,\nprovide quantitative biomarkers for neurological disease detection and\nanalysis, and can be used to synthesize arbitrary MRI contrasts. Estimating\ntissue properties from a single scan session using a protocol available on all\nclinical scanners promises to reduce scan time and cost, enable quantitative\nanalysis in routine clinical scans and provide scan-independent biomarkers of\ndisease. However, existing tissue properties estimation methods - most often\n$\\mathbf{T_1}$ relaxation, $\\mathbf{T_2^*}$ relaxation, and proton density\n($\\mathbf{PD}$) - require data from multiple scan sessions and cannot estimate\nall properties from a single clinically available MRI protocol such as the\nmultiecho MRI scan. In addition, the widespread use of non-standard acquisition\nparameters across clinical imaging sites require estimation methods that can\ngeneralize across varying scanner parameters. However, existing learning\nmethods are acquisition protocol specific and cannot estimate from heterogenous\nclinical data from different imaging sites. In this work we propose an\nunsupervised deep-learning strategy that employs MRI physics to estimate all\nthree tissue properties from a single multiecho MRI scan session, and\ngeneralizes across varying acquisition parameters. The proposed strategy\noptimizes accurate synthesis of new MRI contrasts from estimated latent tissue\nproperties, enabling unsupervised training, we also employ random acquisition\nparameters during training to achieve acquisition generalization. We provide\nthe first demonstration of estimating all tissue properties from a single\nmultiecho scan session. We demonstrate improved accuracy and generalizability\nfor tissue property estimation and MRI synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:07:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Varadarajan", "Divya", ""], ["Bouman", "Katherine L.", ""], ["van der Kouwe", "Andre", ""], ["Fischl", "Bruce", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2107.02713", "submitter": "Zhenzhong Chen", "authors": "Leitian Tao, Li Mi, Nannan Li, Xianhang Cheng, Yaosi Hu, and Zhenzhong\n  Chen", "title": "Predicate correlation learning for scene graph generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a typical Scene Graph Generation (SGG) method, there is often a large gap\nin the performance of the predicates' head classes and tail classes. This\nphenomenon is mainly caused by the semantic overlap between different\npredicates as well as the long-tailed data distribution. In this paper, a\nPredicate Correlation Learning (PCL) method for SGG is proposed to address the\nabove two problems by taking the correlation between predicates into\nconsideration. To describe the semantic overlap between strong-correlated\npredicate classes, a Predicate Correlation Matrix (PCM) is defined to quantify\nthe relationship between predicate pairs, which is dynamically updated to\nremove the matrix's long-tailed bias. In addition, PCM is integrated into a\nPredicate Correlation Loss function ($L_{PC}$) to reduce discouraging gradients\nof unannotated classes. The proposed method is evaluated on Visual Genome\nbenchmark, where the performance of the tail classes is significantly improved\nwhen built on the existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:24:33 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tao", "Leitian", ""], ["Mi", "Li", ""], ["Li", "Nannan", ""], ["Cheng", "Xianhang", ""], ["Hu", "Yaosi", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2107.02718", "submitter": "Takehiko Ohkawa", "authors": "Takehiko Ohkawa, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku,\n  Yoichi Sato", "title": "Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain\n  Adaptation of First-Person Hand Segmentation", "comments": "Accepted to IEEE Access 2021", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3094052", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand segmentation is a crucial task in first-person vision. Since\nfirst-person images exhibit strong bias in appearance among different\nenvironments, adapting a pre-trained segmentation model to a new domain is\nrequired in hand segmentation. Here, we focus on appearance gaps for hand\nregions and backgrounds separately. We propose (i) foreground-aware image\nstylization and (ii) consensus pseudo-labeling for domain adaptation of hand\nsegmentation. We stylize source images independently for the foreground and\nbackground using target images as style. To resolve the domain shift that the\nstylization has not addressed, we apply careful pseudo-labeling by taking a\nconsensus between the models trained on the source and stylized source images.\nWe validated our method on domain adaptation of hand segmentation from real and\nsimulation images. Our method achieved state-of-the-art performance in both\nsettings. We also demonstrated promising results in challenging multi-target\ndomain adaptation and domain generalization settings. Code is available at\nhttps://github.com/ut-vision/FgSty-CPL.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:39:06 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 01:16:07 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 01:52:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ohkawa", "Takehiko", ""], ["Yagi", "Takuma", ""], ["Hashimoto", "Atsushi", ""], ["Ushiku", "Yoshitaka", ""], ["Sato", "Yoichi", ""]]}, {"id": "2107.02739", "submitter": "Eric Schulman", "authors": "Sukjin Han, Eric H. Schulman, Kristen Grauman, and Santhosh\n  Ramakrishnan", "title": "Shapes as Product Differentiation: Neural Network Embedding in the\n  Analysis of Markets for Fonts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many differentiated products have key attributes that are unstructured and\nthus high-dimensional (e.g., design, text). Instead of treating unstructured\nattributes as unobservables in economic models, quantifying them can be\nimportant to answer interesting economic questions. To propose an analytical\nframework for this type of products, this paper considers one of the simplest\ndesign products -- fonts -- and investigates merger and product differentiation\nusing an original dataset from the world's largest online marketplace for\nfonts. We quantify font shapes by constructing embeddings from a deep\nconvolutional neural network. Each embedding maps a font's shape onto a\nlow-dimensional vector. In the resulting product space, designers are assumed\nto engage in Hotelling-type spatial competition. From the image embeddings, we\nconstruct two alternative measures that capture the degree of design\ndifferentiation. We then study the causal effects of a merger on the merging\nfirm's creative decisions using the constructed measures in a synthetic control\nmethod. We find that the merger causes the merging firm to increase the visual\nvariety of font design. Notably, such effects are not captured when using\ntraditional measures for product offerings (e.g., specifications and the number\nof products) constructed from structured data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:12:27 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Han", "Sukjin", ""], ["Schulman", "Eric H.", ""], ["Grauman", "Kristen", ""], ["Ramakrishnan", "Santhosh", ""]]}, {"id": "2107.02778", "submitter": "Mayur Parate", "authors": "Devashree R. Patrikar, Mayur Rajram Parate", "title": "Anomaly Detection using Edge Computing in Video Surveillance System:\n  Review", "comments": "26 pages, 6 figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current concept of Smart Cities influences urban planners and researchers\nto provide modern, secured and sustainable infrastructure and give a decent\nquality of life to its residents. To fulfill this need video surveillance\ncameras have been deployed to enhance the safety and well-being of the\ncitizens. Despite technical developments in modern science, abnormal event\ndetection in surveillance video systems is challenging and requires exhaustive\nhuman efforts. In this paper, we surveyed various methodologies developed to\ndetect anomalies in intelligent video surveillance. Firstly, we revisit the\nsurveys on anomaly detection in the last decade. We then present a systematic\ncategorization of methodologies developed for ease of understanding.\nConsidering the notion of anomaly depends on context, we identify different\nobjects-of-interest and publicly available datasets in anomaly detection. Since\nanomaly detection is considered a time-critical application of computer vision,\nour emphasis is on anomaly detection using edge devices and approaches\nexplicitly designed for them. Further, we discuss the challenges and\nopportunities involved in anomaly detection at the edge.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:41:56 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Patrikar", "Devashree R.", ""], ["Parate", "Mayur Rajram", ""]]}, {"id": "2107.02790", "submitter": "Andreas Blattmann", "authors": "Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Bj\\\"orn Ommer", "title": "iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis", "comments": "Project page is available at https://bit.ly/3dJN4Lf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How would a static scene react to a local poke? What are the effects on other\nparts of an object if you could locally push it? There will be distinctive\nmovement, despite evident variations caused by the stochastic nature of our\nworld. These outcomes are governed by the characteristic kinematics of objects\nthat dictate their overall motion caused by a local interaction. Conversely,\nthe movement of an object provides crucial information about its underlying\ndistinctive kinematics and the interdependencies between its parts. This\ntwo-way relation motivates learning a bijective mapping between object\nkinematics and plausible future image sequences. Therefore, we propose iPOKE -\ninvertible Prediction of Object Kinematics - that, conditioned on an initial\nframe and a local poke, allows to sample object kinematics and establishes a\none-to-one correspondence to the corresponding plausible videos, thereby\nproviding a controlled stochastic video synthesis. In contrast to previous\nworks, we do not generate arbitrary realistic videos, but provide efficient\ncontrol of movements, while still capturing the stochastic nature of our\nenvironment and the diversity of plausible outcomes it entails. Moreover, our\napproach can transfer kinematics onto novel object instances and is not\nconfined to particular object classes. Project page is available at\nhttps://bit.ly/3dJN4Lf\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:57:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Blattmann", "Andreas", ""], ["Milbich", "Timo", ""], ["Dorkenwald", "Michael", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2107.02791", "submitter": "Kangle Deng", "authors": "Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan", "title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free", "comments": "Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:\n  https://github.com/dunbar12138/DSNeRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One common failure mode of Neural Radiance Field (NeRF) models is fitting\nincorrect geometries when given an insufficient number of input views. We\npropose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning\nneural radiance fields that takes advantage of readily-available depth\nsupervision. Our key insight is that sparse depth supervision can be used to\nregularize the learned geometry, a crucial component for effectively rendering\nnovel views using NeRF. We exploit the fact that current NeRF pipelines require\nimages with known camera poses that are typically estimated by running\nstructure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that\ncan be used as ``free\" depth supervision during training: we simply add a loss\nto ensure that depth rendered along rays that intersect these 3D points is\nclose to the observed depth. We find that DS-NeRF can render more accurate\nimages given fewer training views while training 2-6x faster. With only two\ntraining views on real-world images, DS-NeRF significantly outperforms NeRF as\nwell as other sparse-view variants. We show that our loss is compatible with\nthese NeRF models, demonstrating that depth is a cheap and easily digestible\nsupervisory signal. Finally, we show that DS-NeRF supports other types of depth\nsupervision such as scanned depth sensors and RGBD reconstruction outputs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:58:35 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Deng", "Kangle", ""], ["Liu", "Andrew", ""], ["Zhu", "Jun-Yan", ""], ["Ramanan", "Deva", ""]]}, {"id": "2107.02792", "submitter": "Arun Narenthiran Sivakumar", "authors": "Arun Narenthiran Sivakumar and Sahil Modi and Mateus Valverde\n  Gasparino and Che Ellis and Andres Eduardo Baquero Velasquez and Girish\n  Chowdhary and Saurabh Gupta", "title": "Learned Visual Navigation for Under-Canopy Agricultural Robots", "comments": "RSS 2021. Project website with data and videos:\n  https://ansivakumar.github.io/learned-visual-navigation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a system for visually guided autonomous navigation of\nunder-canopy farm robots. Low-cost under-canopy robots can drive between crop\nrows under the plant canopy and accomplish tasks that are infeasible for\nover-the-canopy drones or larger agricultural equipment. However, autonomously\nnavigating them under the canopy presents a number of challenges: unreliable\nGPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to\nleaves and weeds, and large variability in appearance over the season and\nacross crop types. We address these challenges by building a modular system\nthat leverages machine learning for robust and generalizable perception from\nmonocular RGB images from low-cost cameras, and model predictive control for\naccurate control in challenging terrain. Our system, CropFollow, is able to\nautonomously drive 485 meters per intervention on average, outperforming a\nstate-of-the-art LiDAR based system (286 meters per intervention) in extensive\nfield testing spanning over 25 km.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:59:02 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Sivakumar", "Arun Narenthiran", ""], ["Modi", "Sahil", ""], ["Gasparino", "Mateus Valverde", ""], ["Ellis", "Che", ""], ["Velasquez", "Andres Eduardo Baquero", ""], ["Chowdhary", "Girish", ""], ["Gupta", "Saurabh", ""]]}, {"id": "2107.02823", "submitter": "Yante Li", "authors": "Yante Li, Jinsheng Wei, Seyednavid Mohammadifoumani, Yang Liu, Guoying\n  Zhao", "title": "Deep Learning based Micro-expression Recognition: A Survey", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Micro-expressions (MEs) are involuntary facial movements revealing people's\nhidden feelings in high-stake situations and have practical importance in\nmedical treatment, national security, interrogations and many human-computer\ninteraction systems. Early methods for MER mainly based on traditional\nappearance and geometry features. Recently, with the success of deep learning\n(DL) in various fields, neural networks have received increasing interests in\nMER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid\nfacial movements, leading to difficult data collection, thus have small-scale\ndatasets. DL based MER becomes challenging due to above ME characters. To data,\nvarious DL approaches have been proposed to solve the ME issues and improve MER\nperformance. In this survey, we provide a comprehensive review of deep\nmicro-expression recognition (MER), including datasets, deep MER pipeline, and\nthe bench-marking of most influential methods. This survey defines a new\ntaxonomy for the field, encompassing all aspects of MER based on DL. For each\naspect, the basic approaches and advanced developments are summarized and\ndiscussed. In addition, we conclude the remaining challenges and and potential\ndirections for the design of robust deep MER systems. To the best of our\nknowledge, this is the first survey of deep MER methods, and this survey can\nserve as a reference point for future MER research.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 18:05:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Li", "Yante", ""], ["Wei", "Jinsheng", ""], ["Mohammadifoumani", "Seyednavid", ""], ["Liu", "Yang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2107.02827", "submitter": "Weixin Jiang", "authors": "Weixin Jiang, Eric Schwenker, Trevor Spreadbury, Kai Li, Maria K.Y.\n  Chan, Oliver Cossairt", "title": "Plot2Spectra: an Automatic Spectra Extraction Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Different types of spectroscopies, such as X-ray absorption near edge\nstructure (XANES) and Raman spectroscopy, play a very important role in\nanalyzing the characteristics of different materials. In scientific literature,\nXANES/Raman data are usually plotted in line graphs which is a visually\nappropriate way to represent the information when the end-user is a human\nreader. However, such graphs are not conducive to direct programmatic analysis\ndue to the lack of automatic tools. In this paper, we develop a plot digitizer,\nnamed Plot2Spectra, to extract data points from spectroscopy graph images in an\nautomatic fashion, which makes it possible for large scale data acquisition and\nanalysis. Specifically, the plot digitizer is a two-stage framework. In the\nfirst axis alignment stage, we adopt an anchor-free detector to detect the plot\nregion and then refine the detected bounding boxes with an edge-based\nconstraint to locate the position of two axes. We also apply scene text\ndetector to extract and interpret all tick information below the x-axis. In the\nsecond plot data extraction stage, we first employ semantic segmentation to\nseparate pixels belonging to plot lines from the background, and from there,\nincorporate optical flow constraints to the plot line pixels to assign them to\nthe appropriate line (data instance) they encode. Extensive experiments are\nconducted to validate the effectiveness of the proposed plot digitizer, which\nshows that such a tool could help accelerate the discovery and machine learning\nof materials properties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 18:17:28 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Jiang", "Weixin", ""], ["Schwenker", "Eric", ""], ["Spreadbury", "Trevor", ""], ["Li", "Kai", ""], ["Chan", "Maria K. Y.", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2107.02859", "submitter": "Ioannis Marras", "authors": "Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng,\n  Grigorios Chrysos, Stefanos Zafeiriou", "title": "Poly-NL: Linear Complexity Non-local Layers with Polynomials", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial self-attention layers, in the form of Non-Local blocks, introduce\nlong-range dependencies in Convolutional Neural Networks by computing pairwise\nsimilarities among all possible positions. Such pairwise functions underpin the\neffectiveness of non-local layers, but also determine a complexity that scales\nquadratically with respect to the input size both in space and time. This is a\nseverely limiting factor that practically hinders the applicability of\nnon-local blocks to even moderately sized inputs. Previous works focused on\nreducing the complexity by modifying the underlying matrix operations, however\nin this work we aim to retain full expressiveness of non-local layers while\nkeeping complexity linear. We overcome the efficiency limitation of non-local\nblocks by framing them as special cases of 3rd order polynomial functions. This\nfact enables us to formulate novel fast Non-Local blocks, capable of reducing\nthe complexity from quadratic to linear with no loss in performance, by\nreplacing any direct computation of pairwise similarities with element-wise\nmultiplications. The proposed method, which we dub as \"Poly-NL\", is competitive\nwith state-of-the-art performance across image recognition, instance\nsegmentation, and face detection tasks, while having considerably less\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 19:51:37 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Babiloni", "Francesca", ""], ["Marras", "Ioannis", ""], ["Kokkinos", "Filippos", ""], ["Deng", "Jiankang", ""], ["Chrysos", "Grigorios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2107.02909", "submitter": "Shota Hattori", "authors": "Shota Hattori, Tatsuya Yatagawa, Yutaka Ohtake, Hiromasa Suzuki", "title": "Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional\n  Networks", "comments": "10 pages, 9 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses mesh restoration problems, i.e., denoising and\ncompletion, by learning self-similarity in an unsupervised manner. For this\npurpose, the proposed method, which we refer to as Deep Mesh Prior, uses a\ngraph convolutional network on meshes to learn the self-similarity. The network\ntakes a single incomplete mesh as input data and directly outputs the\nreconstructed mesh without being trained using large-scale datasets. Our method\ndoes not use any intermediate representations such as an implicit field because\nthe whole process works on a mesh. We demonstrate that our unsupervised method\nperforms equally well or even better than the state-of-the-art methods using\nlarge-scale datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 07:21:10 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hattori", "Shota", ""], ["Yatagawa", "Tatsuya", ""], ["Ohtake", "Yutaka", ""], ["Suzuki", "Hiromasa", ""]]}, {"id": "2107.02927", "submitter": "Suraj Mishra", "authors": "Suraj Mishra, Danny Z. Chen, X. Sharon Hu", "title": "Image Complexity Guided Network Compression for Biomedical Image\n  Segmentation", "comments": "ACM JETC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compression is a standard procedure for making convolutional neural networks\n(CNNs) adhere to some specific computing resource constraints. However,\nsearching for a compressed architecture typically involves a series of\ntime-consuming training/validation experiments to determine a good compromise\nbetween network size and performance accuracy. To address this, we propose an\nimage complexity-guided network compression technique for biomedical image\nsegmentation. Given any resource constraints, our framework utilizes data\ncomplexity and network architecture to quickly estimate a compressed model\nwhich does not require network training. Specifically, we map the dataset\ncomplexity to the target network accuracy degradation caused by compression.\nSuch mapping enables us to predict the final accuracy for different network\nsizes, based on the computed dataset complexity. Thus, one may choose a\nsolution that meets both the network size and segmentation accuracy\nrequirements. Finally, the mapping is used to determine the convolutional\nlayer-wise multiplicative factor for generating a compressed network. We\nconduct experiments using 5 datasets, employing 3 commonly-used CNN\narchitectures for biomedical image segmentation as representative networks. Our\nproposed framework is shown to be effective for generating compressed\nsegmentation networks, retaining up to $\\approx 95\\%$ of the full-sized network\nsegmentation accuracy, and at the same time, utilizing $\\approx 32x$ fewer\nnetwork trainable weights (average reduction) of the full-sized networks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 22:28:10 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Mishra", "Suraj", ""], ["Chen", "Danny Z.", ""], ["Hu", "X. Sharon", ""]]}, {"id": "2107.02958", "submitter": "Frederic Poitevin", "authors": "Youssef S. G. Nashed, Frederic Poitevin, Harshit Gupta, Geoffrey\n  Woollard, Michael Kagan, Chuck Yoon, Daniel Ratner", "title": "End-to-End Simultaneous Learning of Single-particle Orientation and 3D\n  Map Reconstruction from Cryo-electron Microscopy Data", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryogenic electron microscopy (cryo-EM) provides images from different copies\nof the same biomolecule in arbitrary orientations. Here, we present an\nend-to-end unsupervised approach that learns individual particle orientations\nfrom cryo-EM data while reconstructing the average 3D map of the biomolecule,\nstarting from a random initialization. The approach relies on an auto-encoder\narchitecture where the latent space is explicitly interpreted as orientations\nused by the decoder to form an image according to the linear projection model.\nWe evaluate our method on simulated data and show that it is able to\nreconstruct 3D particle maps from noisy- and CTF-corrupted 2D projection images\nof unknown particle orientations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 00:39:58 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nashed", "Youssef S. G.", ""], ["Poitevin", "Frederic", ""], ["Gupta", "Harshit", ""], ["Woollard", "Geoffrey", ""], ["Kagan", "Michael", ""], ["Yoon", "Chuck", ""], ["Ratner", "Daniel", ""]]}, {"id": "2107.02960", "submitter": "Peixia Li", "authors": "Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming\n  Sun, Junjie yan, Wanli Ouyang", "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the first Neural Architecture Search (NAS) method to find a\nbetter transformer architecture for image recognition. Recently, transformers\nwithout CNN-based backbones are found to achieve impressive performance for\nimage recognition. However, the transformer is designed for NLP tasks and thus\ncould be sub-optimal when directly used for image recognition. In order to\nimprove the visual representation ability for transformers, we propose a new\nsearch space and searching algorithm. Specifically, we introduce a locality\nmodule that models the local correlations in images explicitly with fewer\ncomputational cost. With the locality module, our search space is defined to\nlet the search algorithm freely trade off between global and local information\nas well as optimizing the low-level design choice in each module. To tackle the\nproblem caused by huge search space, a hierarchical neural architecture search\nmethod is proposed to search the optimal vision transformer from two levels\nseparately with the evolutionary algorithm. Extensive experiments on the\nImageNet dataset demonstrate that our method can find more discriminative and\nefficient transformer variants than the ResNet family (e.g., ResNet101) and the\nbaseline ViT for image classification.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 00:48:09 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chen", "Boyu", ""], ["Li", "Peixia", ""], ["Li", "Chuming", ""], ["Li", "Baopu", ""], ["Bai", "Lei", ""], ["Lin", "Chen", ""], ["Sun", "Ming", ""], ["yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2107.02963", "submitter": "Zehui Chen", "authors": "Zehui Chen, Chenhongyi Yang, Qiaofei Li, Feng Zhao, Zheng-Jun Zha,\n  Feng Wu", "title": "Disentangle Your Dense Object Detector", "comments": "ACM MM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based dense object detectors have achieved great success in the\npast few years and have been applied to numerous multimedia applications such\nas video understanding. However, the current training pipeline for dense\ndetectors is compromised to lots of conjunctions that may not hold. In this\npaper, we investigate three such important conjunctions: 1) only samples\nassigned as positive in classification head are used to train the regression\nhead; 2) classification and regression share the same input feature and\ncomputational fields defined by the parallel head architecture; and 3) samples\ndistributed in different feature pyramid layers are treated equally when\ncomputing the loss. We first carry out a series of pilot experiments to show\ndisentangling such conjunctions can lead to persistent performance improvement.\nThen, based on these findings, we propose Disentangled Dense Object Detector\n(DDOD), in which simple and effective disentanglement mechanisms are designed\nand integrated into the current state-of-the-art dense object detectors.\nExtensive experiments on MS COCO benchmark show that our approach can lead to\n2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements on RetinaNet, FCOS, and ATSS\nbaselines with negligible extra overhead. Notably, our best model reaches 55.0\nmAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE,\nachieving new state-of-the-art performance on these two competitive benchmarks.\nCode is available at https://github.com/zehuichen123/DDOD.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 00:52:16 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 11:15:13 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Chen", "Zehui", ""], ["Yang", "Chenhongyi", ""], ["Li", "Qiaofei", ""], ["Zhao", "Feng", ""], ["Zha", "Zheng-Jun", ""], ["Wu", "Feng", ""]]}, {"id": "2107.02966", "submitter": "Yijing Yang", "authors": "Yijing Yang, Vasileios Magoulianitis and C.-C. Jay Kuo", "title": "E-PixelHop: An Enhanced PixelHop Method for Object Classification", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on PixelHop and PixelHop++, which are recently developed using the\nsuccessive subspace learning (SSL) framework, we propose an enhanced solution\nfor object classification, called E-PixelHop, in this work. E-PixelHop consists\nof the following steps. First, to decouple the color channels for a color\nimage, we apply principle component analysis and project RGB three color\nchannels onto two principle subspaces which are processed separately for\nclassification. Second, to address the importance of multi-scale features, we\nconduct pixel-level classification at each hop with various receptive fields.\nThird, to further improve pixel-level classification accuracy, we develop a\nsupervised label smoothing (SLS) scheme to ensure prediction consistency.\nForth, pixel-level decisions from each hop and from each color subspace are\nfused together for image-level decision. Fifth, to resolve confusing classes\nfor further performance boosting, we formulate E-PixelHop as a two-stage\npipeline. In the first stage, multi-class classification is performed to get a\nsoft decision for each class, where the top 2 classes with the highest\nprobabilities are called confusing classes. Then,we conduct a binary\nclassification in the second stage. The main contributions lie in Steps 1, 3\nand 5.We use the classification of the CIFAR-10 dataset as an example to\ndemonstrate the effectiveness of the above-mentioned key components of\nE-PixelHop.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:22:12 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yang", "Yijing", ""], ["Magoulianitis", "Vasileios", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2107.02967", "submitter": "Numair Khan", "authors": "Numair Khan, Min H. Kim and James Tompkin", "title": "Edge-aware Bidirectional Diffusion for Dense Depth Estimation from Light\n  Fields", "comments": "Project webpage: http://visual.cs.brown.edu/lightfielddepth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to estimate fast and accurate depth maps from light\nfields via a sparse set of depth edges and gradients. Our proposed approach is\nbased around the idea that true depth edges are more sensitive than texture\nedges to local constraints, and so they can be reliably disambiguated through a\nbidirectional diffusion process. First, we use epipolar-plane images to\nestimate sub-pixel disparity at a sparse set of pixels. To find sparse points\nefficiently, we propose an entropy-based refinement approach to a line estimate\nfrom a limited set of oriented filter banks. Next, to estimate the diffusion\ndirection away from sparse points, we optimize constraints at these points via\nour bidirectional diffusion method. This resolves the ambiguity of which\nsurface the edge belongs to and reliably separates depth from texture edges,\nallowing us to diffuse the sparse set in a depth-edge and occlusion-aware\nmanner to obtain accurate dense depth maps.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:26:25 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Khan", "Numair", ""], ["Kim", "Min H.", ""], ["Tompkin", "James", ""]]}, {"id": "2107.02970", "submitter": "Shobhita Sundaram", "authors": "Shobhita Sundaram and Neha Hulkund", "title": "GAN-based Data Augmentation for Chest X-ray Classification", "comments": "Spotlight Talk at KDD 2021 - Applied Data Science for Healthcare\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in computer vision -- particularly in medical applications\n-- is a lack of sufficiently diverse, large sets of training data. These\ndatasets often suffer from severe class imbalance. As a result, networks often\noverfit and are unable to generalize to novel examples. Generative Adversarial\nNetworks (GANs) offer a novel method of synthetic data augmentation. In this\nwork, we evaluate the use of GAN- based data augmentation to artificially\nexpand the CheXpert dataset of chest radiographs. We compare performance to\ntraditional augmentation and find that GAN-based augmentation leads to higher\ndownstream performance for underrepresented classes. Furthermore, we see that\nthis result is pronounced in low data regimens. This suggests that GAN-based\naugmentation a promising area of research to improve network performance when\ndata collection is prohibitively expensive.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:36:48 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Sundaram", "Shobhita", ""], ["Hulkund", "Neha", ""]]}, {"id": "2107.02972", "submitter": "Yi Fang", "authors": "Xiang Li, Lingjing Wang, Yi Fang", "title": "Learn to Learn Metric Space for Few-Shot Segmentation of 3D Shapes", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent research has seen numerous supervised learning-based methods for 3D\nshape segmentation and remarkable performance has been achieved on various\nbenchmark datasets. These supervised methods require a large amount of\nannotated data to train deep neural networks to ensure the generalization\nability on the unseen test set. In this paper, we introduce a\nmeta-learning-based method for few-shot 3D shape segmentation where only a few\nlabeled samples are provided for the unseen classes. To achieve this, we treat\nthe shape segmentation as a point labeling problem in the metric space.\nSpecifically, we first design a meta-metric learner to transform input shapes\ninto embedding space and our model learns to learn a proper metric space for\neach object class based on point embeddings. Then, for each class, we design a\nmetric learner to extract part-specific prototype representations from a few\nsupport shapes and our model performs per-point segmentation over the query\nshapes by matching each point to its nearest prototype in the learned metric\nspace. A metric-based loss function is used to dynamically modify distances\nbetween point embeddings thus maximizes in-part similarity while minimizing\ninter-part similarity. A dual segmentation branch is adopted to make full use\nof the support information and implicitly encourages consistency between the\nsupport and query prototypes. We demonstrate the superior performance of our\nproposed on the ShapeNet part dataset under the few-shot scenario, compared\nwith well-established baseline and state-of-the-art semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:47:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Lingjing", ""], ["Fang", "Yi", ""]]}, {"id": "2107.02974", "submitter": "Iury Cleveston", "authors": "Iury Cleveston, Esther L. Colombini", "title": "RAM-VO: Less is more in Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building vehicles capable of operating without human supervision requires the\ndetermination of the agent's pose. Visual Odometry (VO) algorithms estimate the\negomotion using only visual changes from the input images. The most recent VO\nmethods implement deep-learning techniques using convolutional neural networks\n(CNN) extensively, which add a substantial cost when dealing with\nhigh-resolution images. Furthermore, in VO tasks, more input data does not mean\na better prediction; on the contrary, the architecture may filter out useless\ninformation. Therefore, the implementation of computationally efficient and\nlightweight architectures is essential. In this work, we propose the RAM-VO, an\nextension of the Recurrent Attention Model (RAM) for visual odometry tasks.\nRAM-VO improves the visual and temporal representation of information and\nimplements the Proximal Policy Optimization (PPO) algorithm to learn robust\npolicies. The results indicate that RAM-VO can perform regressions with six\ndegrees of freedom from monocular input images using approximately 3 million\nparameters. In addition, experiments on the KITTI dataset demonstrate that\nRAM-VO achieves competitive results using only 5.7% of the available visual\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:48:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cleveston", "Iury", ""], ["Colombini", "Esther L.", ""]]}, {"id": "2107.02980", "submitter": "Yuanxin Zhong", "authors": "Yuanxin Zhong, Minghan Zhu, Huei Peng", "title": "VIN: Voxel-based Implicit Network for Joint 3D Object Detection and\n  Segmentation for Lidars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified neural network structure is presented for joint 3D object detection\nand point cloud segmentation in this paper. We leverage rich supervision from\nboth detection and segmentation labels rather than using just one of them. In\naddition, an extension based on single-stage object detectors is proposed based\non the implicit function widely used in 3D scene and object understanding. The\nextension branch takes the final feature map from the object detection module\nas input, and produces an implicit function that generates semantic\ndistribution for each point for its corresponding voxel center. We demonstrated\nthe performance of our structure on nuScenes-lidarseg, a large-scale outdoor\ndataset. Our solution achieves competitive results against state-of-the-art\nmethods in both 3D object detection and point cloud segmentation with little\nadditional computation load compared with object detection solutions. The\ncapability of efficient weakly supervision semantic segmentation of the\nproposed method is also validated by experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 02:16:20 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhong", "Yuanxin", ""], ["Zhu", "Minghan", ""], ["Peng", "Huei", ""]]}, {"id": "2107.02984", "submitter": "Reza Jalil Mozhdehi", "authors": "Reza Jalil Mozhdehi and Henry Medeiros", "title": "Deep Convolutional Correlation Iterative Particle Filter for Visual\n  Tracking", "comments": "28 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work proposes a novel framework for visual tracking based on the\nintegration of an iterative particle filter, a deep convolutional neural\nnetwork, and a correlation filter. The iterative particle filter enables the\nparticles to correct themselves and converge to the correct target position. We\nemploy a novel strategy to assess the likelihood of the particles after the\niterations by applying K-means clustering. Our approach ensures a consistent\nsupport for the posterior distribution. Thus, we do not need to perform\nresampling at every video frame, improving the utilization of prior\ndistribution information. Experimental results on two different benchmark\ndatasets show that our tracker performs favorably against state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 02:44:43 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Mozhdehi", "Reza Jalil", ""], ["Medeiros", "Henry", ""]]}, {"id": "2107.02988", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Zhu Han and Jing Yao and Lianru Gao and Bing Zhang\n  and Antonio Plaza and Jocelyn Chanussot", "title": "SpectralFormer: Rethinking Hyperspectral Image Classification with\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral (HS) images are characterized by approximately contiguous\nspectral information, enabling the fine identification of materials by\ncapturing subtle spectral discrepancies. Owing to their excellent locally\ncontextual modeling ability, convolutional neural networks (CNNs) have been\nproven to be a powerful feature extractor in HS image classification. However,\nCNNs fail to mine and represent the sequence attributes of spectral signatures\nwell due to the limitations of their inherent network backbone. To solve this\nissue, we rethink HS image classification from a sequential perspective with\ntransformers, and propose a novel backbone network called \\ul{SpectralFormer}.\nBeyond band-wise representations in classic transformers, SpectralFormer is\ncapable of learning spectrally local sequence information from neighboring\nbands of HS images, yielding group-wise spectral embeddings. More\nsignificantly, to reduce the possibility of losing valuable information in the\nlayer-wise propagation process, we devise a cross-layer skip connection to\nconvey memory-like components from shallow to deep layers by adaptively\nlearning to fuse \"soft\" residuals across layers. It is worth noting that the\nproposed SpectralFormer is a highly flexible backbone network, which can be\napplicable to both pixel- and patch-wise inputs. We evaluate the classification\nperformance of the proposed SpectralFormer on three HS datasets by conducting\nextensive experiments, showing the superiority over classic transformers and\nachieving a significant improvement in comparison with state-of-the-art\nbackbone networks. The codes of this work will be available at\n\\url{https://sites.google.com/view/danfeng-hong} for the sake of\nreproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 02:59:21 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hong", "Danfeng", ""], ["Han", "Zhu", ""], ["Yao", "Jing", ""], ["Gao", "Lianru", ""], ["Zhang", "Bing", ""], ["Plaza", "Antonio", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2107.03000", "submitter": "Diego Thomas", "authors": "Akihiko Sayo, Diego Thomas, Hiroshi Kawasaki, Yuta Nakashima, Katsushi\n  Ikeuchi", "title": "PoseRN: A 2D pose refinement network for bias-free multi-view 3D human\n  pose estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new 2D pose refinement network that learns to predict the human\nbias in the estimated 2D pose. There are biases in 2D pose estimations that are\ndue to differences between annotations of 2D joint locations based on\nannotators' perception and those defined by motion capture (MoCap) systems.\nThese biases are crafted into publicly available 2D pose datasets and cannot be\nremoved with existing error reduction approaches. Our proposed pose refinement\nnetwork allows us to efficiently remove the human bias in the estimated 2D\nposes and achieve highly accurate multi-view 3D human pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 03:49:36 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Sayo", "Akihiko", ""], ["Thomas", "Diego", ""], ["Kawasaki", "Hiroshi", ""], ["Nakashima", "Yuta", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "2107.03006", "submitter": "Jacob Austin", "authors": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow and Rianne\n  van den Berg", "title": "Structured Denoising Diffusion Models in Discrete State-Spaces", "comments": "10 pages plus references and appendices. First two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown\nimpressive results on image and waveform generation in continuous state spaces.\nHere, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),\ndiffusion-like generative models for discrete data that generalize the\nmultinomial diffusion model of Hoogeboom et al. 2021, by going beyond\ncorruption processes with uniform transition probabilities. This includes\ncorruption with transition matrices that mimic Gaussian kernels in continuous\nspace, matrices based on nearest neighbors in embedding space, and matrices\nthat introduce absorbing states. The third allows us to draw a connection\nbetween diffusion models and autoregressive and mask-based generative models.\nWe show that the choice of transition matrix is an important design decision\nthat leads to improved results in image and text domains. We also introduce a\nnew loss function that combines the variational lower bound with an auxiliary\ncross entropy loss. For text, this model class achieves strong results on\ncharacter-level text generation while scaling to large vocabularies on LM1B. On\nthe image dataset CIFAR-10, our models approach the sample quality and exceed\nthe log-likelihood of the continuous-space DDPM model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:11:00 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 17:09:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Austin", "Jacob", ""], ["Johnson", "Daniel D.", ""], ["Ho", "Jonathan", ""], ["Tarlow", "Daniel", ""], ["Berg", "Rianne van den", ""]]}, {"id": "2107.03008", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang, Junbao Zhuo, Shuhao Cui, Shuhui Wang", "title": "Learning Invariant Representation with Consistency and Diversity for\n  Semi-supervised Source Hypothesis Transfer", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised domain adaptation (SSDA) aims to solve tasks in target domain\nby utilizing transferable information learned from the available source domain\nand a few labeled target data. However, source data is not always accessible in\npractical scenarios, which restricts the application of SSDA in real world\ncircumstances. In this paper, we propose a novel task named Semi-supervised\nSource Hypothesis Transfer (SSHT), which performs domain adaptation based on\nsource trained model, to generalize well in target domain with a few\nsupervisions. In SSHT, we are facing two challenges: (1) The insufficient\nlabeled target data may result in target features near the decision boundary,\nwith the increased risk of mis-classification; (2) The data are usually\nimbalanced in source domain, so the model trained with these data is biased.\nThe biased model is prone to categorize samples of minority categories into\nmajority ones, resulting in low prediction diversity. To tackle the above\nissues, we propose Consistency and Diversity Learning (CDL), a simple but\neffective framework for SSHT by facilitating prediction consistency between two\nrandomly augmented unlabeled data and maintaining the prediction diversity when\nadapting model to target domain. Encouraging consistency regularization brings\ndifficulty to memorize the few labeled target data and thus enhances the\ngeneralization ability of the learned model. We further integrate Batch\nNuclear-norm Maximization into our method to enhance the discriminability and\ndiversity. Experimental results show that our method outperforms existing SSDA\nmethods and unsupervised model adaptation methods on DomainNet, Office-Home and\nOffice-31 datasets. The code is available at\nhttps://github.com/Wang-xd1899/SSHT.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:14:24 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 02:37:34 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Xiaodong", ""], ["Zhuo", "Junbao", ""], ["Cui", "Shuhao", ""], ["Wang", "Shuhui", ""]]}, {"id": "2107.03009", "submitter": "Sachihiro Youoku", "authors": "Sachihiro Youoku, Takahisa Yamamoto, Junya Saito, Akiyoshi Uchida,\n  Xiaoyu Mi, Ziqiang Shi, Liu Liu, Zhongling Liu, Osafumi Nakayama, Kentaro\n  Murase", "title": "Multi-modal Affect Analysis using standardized data within subjects in\n  the Wild", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human affective recognition is an important factor in human-computer\ninteraction. However, the method development with in-the-wild data is not yet\naccurate enough for practical usage. In this paper, we introduce the affective\nrecognition method focusing on facial expression (EXP) and valence-arousal\ncalculation that was submitted to the Affective Behavior Analysis in-the-wild\n(ABAW) 2021 Contest.\n  When annotating facial expressions from a video, we thought that it would be\njudged not only from the features common to all people, but also from the\nrelative changes in the time series of individuals. Therefore, after learning\nthe common features for each frame, we constructed a facial expression\nestimation model and valence-arousal model using time-series data after\ncombining the common features and the standardized features for each video.\nFurthermore, the above features were learned using multi-modal data such as\nimage features, AU, Head pose, and Gaze. In the validation set, our model\nachieved a facial expression score of 0.546. These verification results reveal\nthat our proposed framework can improve estimation accuracy and robustness\neffectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:18:28 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 09:21:14 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 05:33:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Youoku", "Sachihiro", ""], ["Yamamoto", "Takahisa", ""], ["Saito", "Junya", ""], ["Uchida", "Akiyoshi", ""], ["Mi", "Xiaoyu", ""], ["Shi", "Ziqiang", ""], ["Liu", "Liu", ""], ["Liu", "Zhongling", ""], ["Nakayama", "Osafumi", ""], ["Murase", "Kentaro", ""]]}, {"id": "2107.03011", "submitter": "Yifu Wang", "authors": "Yifu Wang, Jiaqi Yang, Xin Peng, Peng Wu, Ling Gao, Kun Huang, Jiaben\n  Chen, Laurent Kneip", "title": "Visual Odometry with an Event Camera Using Continuous Ray Warping and\n  Volumetric Contrast Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new solution to tracking and mapping with an event camera. The\nmotion of the camera contains both rotation and translation, and the\ndisplacements happen in an arbitrarily structured environment. As a result, the\nimage matching may no longer be represented by a low-dimensional homographic\nwarping, thus complicating an application of the commonly used Image of Warped\nEvents (IWE). We introduce a new solution to this problem by performing\ncontrast maximization in 3D. The 3D location of the rays cast for each event is\nsmoothly varied as a function of a continuous-time motion parametrization, and\nthe optimal parameters are found by maximizing the contrast in a volumetric ray\ndensity field. Our method thus performs joint optimization over motion and\nstructure. The practical validity of our approach is supported by an\napplication to AGV motion estimation and 3D reconstruction with a single\nvehicle-mounted event camera. The method approaches the performance obtained\nwith regular cameras, and eventually outperforms in challenging visual\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:32:57 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Yifu", ""], ["Yang", "Jiaqi", ""], ["Peng", "Xin", ""], ["Wu", "Peng", ""], ["Gao", "Ling", ""], ["Huang", "Kun", ""], ["Chen", "Jiaben", ""], ["Kneip", "Laurent", ""]]}, {"id": "2107.03021", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Yingchen Yu, Rongliang Wu, Kaiwen Cui, Aoran Xiao,\n  Shijian Lu, Ling Shao", "title": "Bi-level Feature Alignment for Versatile Image Translation and\n  Manipulation", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) have achieved great success in image\ntranslation and manipulation. However, high-fidelity image generation with\nfaithful style control remains a grand challenge in computer vision. This paper\npresents a versatile image translation and manipulation framework that achieves\naccurate semantic and style guidance in image generation by explicitly building\na correspondence. To handle the quadratic complexity incurred by building the\ndense correspondences, we introduce a bi-level feature alignment strategy that\nadopts a top-$k$ operation to rank block-wise features followed by dense\nattention between block features which reduces memory cost substantially. As\nthe top-$k$ operation involves index swapping which precludes the gradient\npropagation, we propose to approximate the non-differentiable top-$k$ operation\nwith a regularized earth mover's problem so that its gradient can be\neffectively back-propagated. In addition, we design a novel semantic position\nencoding mechanism that builds up coordinate for each individual semantic\nregion to preserve texture structures while building correspondences. Further,\nwe design a novel confidence feature injection module which mitigates mismatch\nproblem by fusing features adaptively according to the reliability of built\ncorrespondences. Extensive experiments show that our method achieves superior\nperformance qualitatively and quantitatively as compared with the\nstate-of-the-art. The code is available at\n\\href{https://github.com/fnzhan/RABIT}{https://github.com/fnzhan/RABIT}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 05:26:29 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhan", "Fangneng", ""], ["Yu", "Yingchen", ""], ["Wu", "Rongliang", ""], ["Cui", "Kaiwen", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""], ["Shao", "Ling", ""]]}, {"id": "2107.03024", "submitter": "Xumeng Han", "authors": "Xumeng Han, Xuehui Yu, Nan Jiang, Guorong Li, Jian Zhao, Qixiang Ye,\n  Zhenjun Han", "title": "Group Sampling for Unsupervised Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised person re-identification (re-ID) remains a challenging task,\nwhere the classifier and feature representation could be easily misled by the\nnoisy pseudo labels towards deteriorated over-fitting. In this paper, we\npropose a simple yet effective approach, termed Group Sampling, to alleviate\nthe negative impact of noisy pseudo labels within unsupervised person re-ID\nmodels. The idea behind Group Sampling is that it can gather a group of samples\nfrom the same class in the same mini-batch, such that the model is trained upon\ngroup normalized samples while alleviating the effect of a single sample. Group\nsampling updates the pipeline of pseudo label generation by guaranteeing the\nsamples to be better divided into the correct classes. Group Sampling\nregularizes classifier training and representation learning, leading to the\nstatistical stability of feature representation in a progressive fashion.\nQualitative and quantitative experiments on Market-1501, DukeMTMC-reID, and\nMSMT17 show that Grouping Sampling improves the state-of-the-arts by up to\n2.2%~6.1%. Code is available at https://github.com/wavinflaghxm/GroupSampling.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 05:39:58 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Han", "Xumeng", ""], ["Yu", "Xuehui", ""], ["Jiang", "Nan", ""], ["Li", "Guorong", ""], ["Zhao", "Jian", ""], ["Ye", "Qixiang", ""], ["Han", "Zhenjun", ""]]}, {"id": "2107.03030", "submitter": "Hu Chen Dr.", "authors": "Hu Chen, Hong Li, Bifu Hu, Kenan Ma, Yuchun Sun", "title": "A convolutional neural network for teeth margin detection on\n  3-dimensional dental meshes", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a convolutional neural network for vertex classification on\n3-dimensional dental meshes, and used it to detect teeth margins. An expanding\nlayer was constructed to collect statistic values of neighbor vertex features\nand compute new features for each vertex with convolutional neural networks. An\nend-to-end neural network was proposed to take vertex features, including\ncoordinates, curvatures and distance, as input and output each vertex\nclassification label. Several network structures with different parameters of\nexpanding layers and a base line network without expanding layers were designed\nand trained by 1156 dental meshes. The accuracy, recall and precision were\nvalidated on 145 dental meshes to rate the best network structures, which were\nfinally tested on another 144 dental meshes. All networks with our expanding\nlayers performed better than baseline, and the best one achieved an accuracy of\n0.877 both on validation dataset and test dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 06:16:17 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chen", "Hu", ""], ["Li", "Hong", ""], ["Hu", "Bifu", ""], ["Ma", "Kenan", ""], ["Sun", "Yuchun", ""]]}, {"id": "2107.03035", "submitter": "Xinghua Ma", "authors": "Xinghua Ma, Gongning Luo, Wei Wang and Kuanquan Wang", "title": "Transformer Network for Significant Stenosis Detection in CCTA of\n  Coronary Arteries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery disease (CAD) has posed a leading threat to the lives of\ncardiovascular disease patients worldwide for a long time. Therefore, automated\ndiagnosis of CAD has indispensable significance in clinical medicine. However,\nthe complexity of coronary artery plaques that cause CAD makes the automatic\ndetection of coronary artery stenosis in Coronary CT angiography (CCTA) a\ndifficult task. In this paper, we propose a Transformer network (TR-Net) for\nthe automatic detection of significant stenosis (i.e. luminal narrowing > 50%)\nwhile practically completing the computer-assisted diagnosis of CAD. The\nproposed TR-Net introduces a novel Transformer, and tightly combines\nconvolutional layers and Transformer encoders, allowing their advantages to be\ndemonstrated in the task. By analyzing semantic information sequences, TR-Net\ncan fully understand the relationship between image information in each\nposition of a multiplanar reformatted (MPR) image, and accurately detect\nsignificant stenosis based on both local and global information. We evaluate\nour TR-Net on a dataset of 76 patients from different patients annotated by\nexperienced radiologists. Experimental results illustrate that our TR-Net has\nachieved better results in ACC (0.92), Spec (0.96), PPV (0.84), F1 (0.79) and\nMCC (0.74) indicators compared with the state-of-the-art methods. The source\ncode is publicly available from the link (https://github.com/XinghuaMa/TR-Net).\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 06:27:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ma", "Xinghua", ""], ["Luo", "Gongning", ""], ["Wang", "Wei", ""], ["Wang", "Kuanquan", ""]]}, {"id": "2107.03038", "submitter": "Ying Siu Liang", "authors": "Ying Siu Liang, Dongkyu Choi, Kenneth Kwok", "title": "Maintaining a Reliable World Model using Action-aware Perceptual\n  Anchoring", "comments": "7 pages, 3 figures", "journal-ref": "2021 International Conference on Robotics and Automation (ICRA\n  2021)", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable perception is essential for robots that interact with the world. But\nsensors alone are often insufficient to provide this capability, and they are\nprone to errors due to various conditions in the environment. Furthermore,\nthere is a need for robots to maintain a model of its surroundings even when\nobjects go out of view and are no longer visible. This requires anchoring\nperceptual information onto symbols that represent the objects in the\nenvironment. In this paper, we present a model for action-aware perceptual\nanchoring that enables robots to track objects in a persistent manner. Our\nrule-based approach considers inductive biases to perform high-level reasoning\nover the results from low-level object detection, and it improves the robot's\nperceptual capability for complex tasks. We evaluate our model against existing\nbaseline models for object permanence and show that it outperforms these on a\nsnitch localisation task using a dataset of 1,371 videos. We also integrate our\naction-aware perceptual anchoring in the context of a cognitive architecture\nand demonstrate its benefits in a realistic gearbox assembly task on a\nUniversal Robot.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 06:35:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liang", "Ying Siu", ""], ["Choi", "Dongkyu", ""], ["Kwok", "Kenneth", ""]]}, {"id": "2107.03050", "submitter": "Nayyer Aafaq Mr.", "authors": "Nayyer Aafaq, Naveed Akhtar, Wei Liu, Mubarak Shah and Ajmal Mian", "title": "Controlled Caption Generation for Images Through Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning is found to be vulnerable to adversarial examples. However, its\nadversarial susceptibility in image caption generation is under-explored. We\nstudy adversarial examples for vision and language models, which typically\nadopt an encoder-decoder framework consisting of two major components: a\nConvolutional Neural Network (i.e., CNN) for image feature extraction and a\nRecurrent Neural Network (RNN) for caption generation. In particular, we\ninvestigate attacks on the visual encoder's hidden layer that is fed to the\nsubsequent recurrent network. The existing methods either attack the\nclassification layer of the visual encoder or they back-propagate the gradients\nfrom the language model. In contrast, we propose a GAN-based algorithm for\ncrafting adversarial examples for neural image captioning that mimics the\ninternal representation of the CNN such that the resulting deep features of the\ninput image enable a controlled incorrect caption generation through the\nrecurrent network. Our contribution provides new insights for understanding\nadversarial attacks on vision systems with language component. The proposed\nmethod employs two strategies for a comprehensive evaluation. The first\nexamines if a neural image captioning system can be misled to output targeted\nimage captions. The second analyzes the possibility of keywords into the\npredicted captions. Experiments show that our algorithm can craft effective\nadversarial images based on the CNN hidden layers to fool captioning framework.\nMoreover, we discover the proposed attack to be highly transferable. Our work\nleads to new robustness implications for neural image captioning.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 07:22:41 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Aafaq", "Nayyer", ""], ["Akhtar", "Naveed", ""], ["Liu", "Wei", ""], ["Shah", "Mubarak", ""], ["Mian", "Ajmal", ""]]}, {"id": "2107.03055", "submitter": "Anran Liu", "authors": "Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, Chao Dong", "title": "Blind Image Super-Resolution: A Survey and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image super-resolution (SR), aiming to super-resolve low-resolution\nimages with unknown degradation, has attracted increasing attention due to its\nsignificance in promoting real-world applications. Many novel and effective\nsolutions have been proposed recently, especially with the powerful deep\nlearning techniques. Despite years of efforts, it still remains as a\nchallenging research problem. This paper serves as a systematic review on\nrecent progress in blind image SR, and proposes a taxonomy to categorize\nexisting methods into three different classes according to their ways of\ndegradation modelling and the data used for solving the SR model. This taxonomy\nhelps summarize and distinguish among existing methods. We hope to provide\ninsights into current research states, as well as to reveal novel research\ndirections worth exploring. In addition, we make a summary on commonly used\ndatasets and previous competitions related to blind image SR. Last but not\nleast, a comparison among different methods is provided with detailed analysis\non their merits and demerits using both synthetic and real testing images.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 07:38:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Anran", ""], ["Liu", "Yihao", ""], ["Gu", "Jinjin", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2107.03068", "submitter": "Hajime Taira", "authors": "Hajime Taira, Koki Onbe, Naoyuki Miyashita, Masatoshi Okutomi", "title": "Video-Based Camera Localization Using Anchor View Detection and\n  Recursive 3D Reconstruction", "comments": "This paper have been accepted and will be appeared in the proceedings\n  of 17th International Conference on Machine Vision Applications (MVA2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new camera localization strategy designed for\nimage sequences captured in challenging industrial situations such as\nindustrial parts inspection. To deal with peculiar appearances that hurt\nstandard 3D reconstruction pipeline, we exploit pre-knowledge of the scene by\nselecting key frames in the sequence (called as anchors) which are roughly\nconnected to a certain location. Our method then seek the location of each\nframe in time-order, while recursively updating an augmented 3D model which can\nprovide current camera location and surrounding 3D structure. In an experiment\non a practical industrial situation, our method can localize over 99% frames in\nthe input sequence, whereas standard localization methods fail to reconstruct a\ncomplete camera trajectory.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:13:33 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Taira", "Hajime", ""], ["Onbe", "Koki", ""], ["Miyashita", "Naoyuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2107.03070", "submitter": "Monty Santarossa", "authors": "Monty Santarossa, Lukas Schneider, Claudius Zelenka, Lars Schmarje,\n  Reinhard Koch, Uwe Franke", "title": "Learning Stixel-based Instance Segmentation", "comments": "Accepted for publication in IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stixels have been successfully applied to a wide range of vision tasks in\nautonomous driving, recently including instance segmentation. However, due to\ntheir sparse occurrence in the image, until now Stixels seldomly served as\ninput for Deep Learning algorithms, restricting their utility for such\napproaches. In this work we present StixelPointNet, a novel method to perform\nfast instance segmentation directly on Stixels. By regarding the Stixel\nrepresentation as unstructured data similar to point clouds, architectures like\nPointNet are able to learn features from Stixels. We use a bounding box\ndetector to propose candidate instances, for which the relevant Stixels are\nextracted from the input image. On these Stixels, a PointNet models learns\nbinary segmentations, which we then unify throughout the whole image in a final\nselection step. StixelPointNet achieves state-of-the-art performance on\nStixel-level, is considerably faster than pixel-based segmentation methods, and\nshows that with our approach the Stixel domain can be introduced to many new 3D\nDeep Learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:16:42 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Santarossa", "Monty", ""], ["Schneider", "Lukas", ""], ["Zelenka", "Claudius", ""], ["Schmarje", "Lars", ""], ["Koch", "Reinhard", ""], ["Franke", "Uwe", ""]]}, {"id": "2107.03088", "submitter": "Peidong Liu", "authors": "Peidong Liu, Zibin He, Xiyu Yan, Yong Jiang, Shutao Xia, Feng Zheng,\n  Maowei Hu", "title": "WeClick: Weakly-Supervised Video Semantic Segmentation with Click\n  Annotations", "comments": "Accepted by ACM MM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with tedious per-pixel mask annotating, it is much easier to\nannotate data by clicks, which costs only several seconds for an image.\nHowever, applying clicks to learn video semantic segmentation model has not\nbeen explored before. In this work, we propose an effective weakly-supervised\nvideo semantic segmentation pipeline with click annotations, called WeClick,\nfor saving laborious annotating effort by segmenting an instance of the\nsemantic class with only a single click. Since detailed semantic information is\nnot captured by clicks, directly training with click labels leads to poor\nsegmentation predictions. To mitigate this problem, we design a novel memory\nflow knowledge distillation strategy to exploit temporal information (named\nmemory flow) in abundant unlabeled video frames, by distilling the neighboring\npredictions to the target frame via estimated motion. Moreover, we adopt\nvanilla knowledge distillation for model compression. In this case, WeClick\nlearns compact video semantic segmentation models with the low-cost click\nannotations during the training phase yet achieves real-time and accurate\nmodels during the inference period. Experimental results on Cityscapes and\nCamvid show that WeClick outperforms the state-of-the-art methods, increases\nperformance by 10.24% mIoU than baseline, and achieves real-time execution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:12:46 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Peidong", ""], ["He", "Zibin", ""], ["Yan", "Xiyu", ""], ["Jiang", "Yong", ""], ["Xia", "Shutao", ""], ["Zheng", "Feng", ""], ["Hu", "Maowei", ""]]}, {"id": "2107.03098", "submitter": "Jia Li", "authors": "Jia Li, Linhua Xiang, Jiwei Chen, Zengfu Wang", "title": "Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation", "comments": "Accepted by PRCV2021, the official PRCV2021 version is different", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet reliable bottom-up approach with a good trade-off\nbetween accuracy and efficiency for the problem of multi-person pose\nestimation. Given an image, we employ an Hourglass Network to infer all the\nkeypoints from different persons indiscriminately as well as the guiding\noffsets connecting the adjacent keypoints belonging to the same persons. Then,\nwe greedily group the candidate keypoints into multiple human poses (if any),\nutilizing the predicted guiding offsets. And we refer to this process as greedy\noffset-guided keypoint grouping (GOG). Moreover, we revisit the\nencoding-decoding method for the multi-person keypoint coordinates and reveal\nsome important facts affecting accuracy. Experiments have demonstrated the\nobvious performance improvements brought by the introduced components. Our\napproach is comparable to the state of the art on the challenging COCO dataset\nunder fair conditions. The source code and our pre-trained model are publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:32:01 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Li", "Jia", ""], ["Xiang", "Linhua", ""], ["Chen", "Jiwei", ""], ["Wang", "Zengfu", ""]]}, {"id": "2107.03101", "submitter": "Shuang Deng", "authors": "Shuang Deng and Qiulei Dong", "title": "GA-NET: Global Attention Network for Point Cloud Semantic Segmentation", "comments": null, "journal-ref": "IEEE Signal Processing Letter, Vol. 28, pp. 1300-1304, 2021", "doi": "10.1109/LSP.2021.3082851", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How to learn long-range dependencies from 3D point clouds is a challenging\nproblem in 3D point cloud analysis. Addressing this problem, we propose a\nglobal attention network for point cloud semantic segmentation, named as\nGA-Net, consisting of a point-independent global attention module and a\npoint-dependent global attention module for obtaining contextual information of\n3D point clouds in this paper. The point-independent global attention module\nsimply shares a global attention map for all 3D points. In the point-dependent\nglobal attention module, for each point, a novel random cross attention block\nusing only two randomly sampled subsets is exploited to learn the contextual\ninformation of all the points. Additionally, we design a novel point-adaptive\naggregation block to replace linear skip connection for aggregating more\ndiscriminate features. Extensive experimental results on three 3D public\ndatasets demonstrate that our method outperforms state-of-the-art methods in\nmost cases.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:35:59 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Deng", "Shuang", ""], ["Dong", "Qiulei", ""]]}, {"id": "2107.03105", "submitter": "Shuang Deng", "authors": "Shuang Deng, Bo Liu, Qiulei Dong, and Zhanyi Hu", "title": "Rotation Transformation Network: Learning View-Invariant Point Cloud for\n  Classification and Segmentation", "comments": null, "journal-ref": "IEEE International Conference on Multimedia and Expo (ICME), 2021", "doi": "10.1109/ICME51207.2021.9428265", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many recent works show that a spatial manipulation module could boost the\nperformances of deep neural networks (DNNs) for 3D point cloud analysis. In\nthis paper, we aim to provide an insight into spatial manipulation modules.\nFirstly, we find that the smaller the rotational degree of freedom (RDF) of\nobjects is, the more easily these objects are handled by these DNNs. Then, we\ninvestigate the effect of the popular T-Net module and find that it could not\nreduce the RDF of objects. Motivated by the above two issues, we propose a\nrotation transformation network for point cloud analysis, called RTN, which\ncould reduce the RDF of input 3D objects to 0. The RTN could be seamlessly\ninserted into many existing DNNs for point cloud analysis. Extensive\nexperimental results on 3D point cloud classification and segmentation tasks\ndemonstrate that the proposed RTN could improve the performances of several\nstate-of-the-art methods significantly.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:45:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Deng", "Shuang", ""], ["Liu", "Bo", ""], ["Dong", "Qiulei", ""], ["Hu", "Zhanyi", ""]]}, {"id": "2107.03106", "submitter": "Mohamed Elgharib", "authors": "Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian\n  Theobalt, William A. P. Smith", "title": "Self-supervised Outdoor Scene Relighting", "comments": "Published in ECCV '20,\n  http://gvv.mpi-inf.mpg.de/projects/SelfRelight/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor scene relighting is a challenging problem that requires good\nunderstanding of the scene geometry, illumination and albedo. Current\ntechniques are completely supervised, requiring high quality synthetic\nrenderings to train a solution. Such renderings are synthesized using priors\nlearned from limited data. In contrast, we propose a self-supervised approach\nfor relighting. Our approach is trained only on corpora of images collected\nfrom the internet without any user-supervision. This virtually endless source\nof training data allows training a general relighting solution. Our approach\nfirst decomposes an image into its albedo, geometry and illumination. A novel\nrelighting is then produced by modifying the illumination parameters. Our\nsolution capture shadow using a dedicated shadow prediction map, and does not\nrely on accurate geometry estimation. We evaluate our technique subjectively\nand objectively using a new dataset with ground-truth relighting. Results show\nthe ability of our technique to produce photo-realistic and physically\nplausible results, that generalizes to unseen scenes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:46:19 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yu", "Ye", ""], ["Meka", "Abhimitra", ""], ["Elgharib", "Mohamed", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""], ["Smith", "William A. P.", ""]]}, {"id": "2107.03107", "submitter": "Mouath Aouayeb", "authors": "Mouath Aouayeb, Wassim Hamidouche, Catherine Soladie, Kidiyo Kpalma,\n  Renaud Seguier", "title": "Learning Vision Transformer with Squeeze and Excitation for Facial\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As various databases of facial expressions have been made accessible over the\nlast few decades, the Facial Expression Recognition (FER) task has gotten a lot\nof interest. The multiple sources of the available databases raised several\nchallenges for facial recognition task. These challenges are usually addressed\nby Convolution Neural Network (CNN) architectures. Different from CNN models, a\nTransformer model based on attention mechanism has been presented recently to\naddress vision tasks. One of the major issue with Transformers is the need of a\nlarge data for training, while most FER databases are limited compared to other\nvision applications. Therefore, we propose in this paper to learn a vision\nTransformer jointly with a Squeeze and Excitation (SE) block for FER task. The\nproposed method is evaluated on different publicly available FER databases\nincluding CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model\noutperforms state-of-the-art methods on CK+ and SFEW and achieves competitive\nresults on JAFFE and RAF-DB.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:49:01 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 10:37:00 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 12:18:48 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 07:49:19 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Aouayeb", "Mouath", ""], ["Hamidouche", "Wassim", ""], ["Soladie", "Catherine", ""], ["Kpalma", "Kidiyo", ""], ["Seguier", "Renaud", ""]]}, {"id": "2107.03109", "submitter": "Mohamed Elgharib", "authors": "Mohamed Elgharib, Mohit Mendiratta, Justus Thies, Matthias\n  Nie{\\ss}ner, Hans-Peter Seidel, Ayush Tewari, Vladislav Golyanik, Christian\n  Theobalt", "title": "Egocentric Videoconferencing", "comments": "Mohamed Elgharib and Mohit Mendiratta contributed equally to this\n  work. http://gvv.mpi-inf.mpg.de/projects/EgoChat/", "journal-ref": "ACM Transactions on Graphics, volume = 39, number = 6, articleno =\n  268, year = 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for egocentric videoconferencing that enables\nhands-free video calls, for instance by people wearing smart glasses or other\nmixed-reality devices. Videoconferencing portrays valuable non-verbal\ncommunication and face expression cues, but usually requires a front-facing\ncamera. Using a frontal camera in a hands-free setting when a person is on the\nmove is impractical. Even holding a mobile phone camera in the front of the\nface while sitting for a long duration is not convenient. To overcome these\nissues, we propose a low-cost wearable egocentric camera setup that can be\nintegrated into smart glasses. Our goal is to mimic a classical video call, and\ntherefore, we transform the egocentric perspective of this camera into a front\nfacing video. To this end, we employ a conditional generative adversarial\nneural network that learns a transition from the highly distorted egocentric\nviews to frontal views common in videoconferencing. Our approach learns to\ntransfer expression details directly from the egocentric view without using a\ncomplex intermediate parametric expressions model, as it is used by related\nface reenactment methods. We successfully handle subtle expressions, not easily\ncaptured by parametric blendshape-based solutions, e.g., tongue movement, eye\nmovements, eye blinking, strong expressions and depth varying movements. To get\ncontrol over the rigid head movements in the target view, we condition the\ngenerator on synthetic renderings of a moving neutral face. This allows us to\nsynthesis results at different head poses. Our technique produces temporally\nsmooth video-realistic renderings in real-time using a video-to-video\ntranslation network in conjunction with a temporal discriminator. We\ndemonstrate the improved capabilities of our technique by comparing against\nrelated state-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:49:39 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Elgharib", "Mohamed", ""], ["Mendiratta", "Mohit", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["Seidel", "Hans-Peter", ""], ["Tewari", "Ayush", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "2107.03120", "submitter": "Hao Tang", "authors": "Gaowen Liu, Hao Tang, Hugo Latapie, Jason Corso, Yan Yan", "title": "Cross-View Exocentric to Egocentric Video Synthesis", "comments": "ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view video synthesis task seeks to generate video sequences of one view\nfrom another dramatically different view. In this paper, we investigate the\nexocentric (third-person) view to egocentric (first-person) view video\ngeneration task. This is challenging because egocentric view sometimes is\nremarkably different from the exocentric view. Thus, transforming the\nappearances across the two different views is a non-trivial task. Particularly,\nwe propose a novel Bi-directional Spatial Temporal Attention Fusion Generative\nAdversarial Network (STA-GAN) to learn both spatial and temporal information to\ngenerate egocentric video sequences from the exocentric view. The proposed\nSTA-GAN consists of three parts: temporal branch, spatial branch, and attention\nfusion. First, the temporal and spatial branches generate a sequence of fake\nframes and their corresponding features. The fake frames are generated in both\ndownstream and upstream directions for both temporal and spatial branches.\nNext, the generated four different fake frames and their corresponding features\n(spatial and temporal branches in two directions) are fed into a novel\nmulti-generation attention fusion module to produce the final video sequence.\nMeanwhile, we also propose a novel temporal and spatial dual-discriminator for\nmore robust network optimization. Extensive experiments on the Side2Ego and\nTop2Ego datasets show that the proposed STA-GAN significantly outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:00:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Gaowen", ""], ["Tang", "Hao", ""], ["Latapie", "Hugo", ""], ["Corso", "Jason", ""], ["Yan", "Yan", ""]]}, {"id": "2107.03143", "submitter": "Junya Saito", "authors": "Junya Saito, Xiaoyu Mi, Akiyoshi Uchida, Sachihiro Youoku, Takahisa\n  Yamamoto, Kentaro Murase, Osafumi Nakayama", "title": "Action Units Recognition Using Improved Pairwise Deep Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Action Units (AUs) represent a set of facial muscular activities and\nvarious combinations of AUs can represent a wide range of emotions. AU\nrecognition is often used in many applications, including marketing,\nhealthcare, education, and so forth. Although a lot of studies have developed\nvarious methods to improve recognition accuracy, it still remains a major\nchallenge for AU recognition. In the Affective Behavior Analysis in-the-wild\n(ABAW) 2020 competition, we proposed a new automatic Action Units (AUs)\nrecognition method using a pairwise deep architecture to derive the\nPseudo-Intensities of each AU and then convert them into predicted intensities.\nThis year, we introduced a new technique to last year's framework to further\nreduce AU recognition errors due to temporary face occlusion such as hands on\nface or large face orientation. We obtained a score of 0.65 in the validation\ndata set for this year's competition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:10:13 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 11:45:51 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Saito", "Junya", ""], ["Mi", "Xiaoyu", ""], ["Uchida", "Akiyoshi", ""], ["Youoku", "Sachihiro", ""], ["Yamamoto", "Takahisa", ""], ["Murase", "Kentaro", ""], ["Nakayama", "Osafumi", ""]]}, {"id": "2107.03145", "submitter": "Rao Muhammad Umer", "authors": "Rao Muhammad Umer, Asad Munir, Christian Micheloni", "title": "A Deep Residual Star Generative Adversarial Network for multi-domain\n  Image Super-Resolution", "comments": "5 pages, 6th International Conference on Smart and Sustainable\n  Technologies 2021. arXiv admin note: text overlap with arXiv:2009.03693,\n  arXiv:2005.00953", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, most of state-of-the-art single image super-resolution (SISR)\nmethods have attained impressive performance by using deep convolutional neural\nnetworks (DCNNs). The existing SR methods have limited performance due to a\nfixed degradation settings, i.e. usually a bicubic downscaling of\nlow-resolution (LR) image. However, in real-world settings, the LR degradation\nprocess is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,\nor real LR. Therefore, most SR methods are ineffective and inefficient in\nhandling more than one degradation settings within a single network. To handle\nthe multiple degradation, i.e. refers to multi-domain image super-resolution,\nwe propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and\nscalable approach that super-resolves the LR images for the multiple LR domains\nusing only a single model. The proposed scheme is trained in a StarGAN like\nnetwork topology with a single generator and discriminator networks. We\ndemonstrate the effectiveness of our proposed approach in quantitative and\nqualitative experiments compared to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:15:17 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Umer", "Rao Muhammad", ""], ["Munir", "Asad", ""], ["Micheloni", "Christian", ""]]}, {"id": "2107.03163", "submitter": "Zhi Chen", "authors": "Zhi Chen, Yadan Luo, Sen Wang, Ruihong Qiu, Jingjing Li, Zi Huang", "title": "Mitigating Generation Shifts for Generalized Zero-Shot Learning", "comments": "ACM Multimedia 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Zero-Shot Learning (GZSL) is the task of leveraging semantic\ninformation (e.g., attributes) to recognize the seen and unseen samples, where\nunseen classes are not observable during training. It is natural to derive\ngenerative models and hallucinate training samples for unseen classes based on\nthe knowledge learned from the seen samples. However, most of these models\nsuffer from the `generation shifts', where the synthesized samples may drift\nfrom the real distribution of unseen data. In this paper, we conduct an\nin-depth analysis on this issue and propose a novel Generation Shifts\nMitigating Flow (GSMFlow) framework, which is comprised of multiple conditional\naffine coupling layers for learning unseen data synthesis efficiently and\neffectively. In particular, we identify three potential problems that trigger\nthe generation shifts, i.e., semantic inconsistency, variance decay, and\nstructural permutation and address them respectively. First, to reinforce the\ncorrelations between the generated samples and the respective attributes, we\nexplicitly embed the semantic information into the transformations in each of\nthe coupling layers. Second, to recover the intrinsic variance of the\nsynthesized unseen features, we introduce a visual perturbation strategy to\ndiversify the intra-class variance of generated data and hereby help adjust the\ndecision boundary of the classifier. Third, to avoid structural permutation in\nthe semantic space, we propose a relative positioning strategy to manipulate\nthe attribute embeddings, guiding which to fully preserve the inter-class\ngeometric structure. Experimental results demonstrate that GSMFlow achieves\nstate-of-the-art recognition performance in both conventional and generalized\nzero-shot settings. Our code is available at:\nhttps://github.com/uqzhichen/GSMFlow\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:43:59 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chen", "Zhi", ""], ["Luo", "Yadan", ""], ["Wang", "Sen", ""], ["Qiu", "Ruihong", ""], ["Li", "Jingjing", ""], ["Huang", "Zi", ""]]}, {"id": "2107.03166", "submitter": "Kaiwen Cui", "authors": "Kaiwen Cui, Gongjie Zhang, Fangneng Zhan, Jiaxing Huang, Shijian Lu", "title": "FBC-GAN: Diverse and Flexible Image Synthesis via Foreground-Background\n  Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have become the de-facto standard in\nimage synthesis. However, without considering the foreground-background\ndecomposition, existing GANs tend to capture excessive content correlation\nbetween foreground and background, thus constraining the diversity in image\ngeneration. This paper presents a novel Foreground-Background Composition GAN\n(FBC-GAN) that performs image generation by generating foreground objects and\nbackground scenes concurrently and independently, followed by composing them\nwith style and geometrical consistency. With this explicit design, FBC-GAN can\ngenerate images with foregrounds and backgrounds that are mutually independent\nin contents, thus lifting the undesirably learned content correlation\nconstraint and achieving superior diversity. It also provides excellent\nflexibility by allowing the same foreground object with different background\nscenes, the same background scene with varying foreground objects, or the same\nforeground object and background scene with different object positions, sizes\nand poses. It can compose foreground objects and background scenes sampled from\ndifferent datasets as well. Extensive experiments over multiple datasets show\nthat FBC-GAN achieves competitive visual realism and superior diversity as\ncompared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:49:14 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cui", "Kaiwen", ""], ["Zhang", "Gongjie", ""], ["Zhan", "Fangneng", ""], ["Huang", "Jiaxing", ""], ["Lu", "Shijian", ""]]}, {"id": "2107.03172", "submitter": "Kailun Yang", "authors": "Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin\n  M\\\"uller, Rainer Stiefelhagen", "title": "Trans4Trans: Efficient Transformer for Transparent Object Segmentation\n  to Help Visually Impaired People Navigate in the Real World", "comments": "8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common fully glazed facades and transparent objects present architectural\nbarriers and impede the mobility of people with low vision or blindness, for\ninstance, a path detected behind a glass door is inaccessible unless it is\ncorrectly perceived and reacted. However, segmenting these safety-critical\nobjects is rarely covered by conventional assistive technologies. To tackle\nthis issue, we construct a wearable system with a novel dual-head Transformer\nfor Transparency (Trans4Trans) model, which is capable of segmenting general\nand transparent objects and performing real-time wayfinding to assist people\nwalking alone more safely. Especially, both decoders created by our proposed\nTransformer Parsing Module (TPM) enable effective joint learning from different\ndatasets. Besides, the efficient Trans4Trans model composed of symmetric\ntransformer-based encoder and decoder, requires little computational expenses\nand is readily deployed on portable GPUs. Our Trans4Trans model outperforms\nstate-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2\ndatasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various\npre-tests and a user study conducted in indoor and outdoor scenarios, the\nusability and reliability of our assistive system have been extensively\nverified.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:06:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhang", "Jiaming", ""], ["Yang", "Kailun", ""], ["Constantinescu", "Angela", ""], ["Peng", "Kunyu", ""], ["M\u00fcller", "Karin", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.03180", "submitter": "Kailun Yang", "authors": "Huayao Liu, Ruiping Liu, Kailun Yang, Jiaming Zhang, Kunyu Peng,\n  Rainer Stiefelhagen", "title": "HIDA: Towards Holistic Indoor Understanding for the Visually Impaired\n  via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor", "comments": "10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independently exploring unknown spaces or finding objects in an indoor\nenvironment is a daily but challenging task for visually impaired people.\nHowever, common 2D assistive systems lack depth relationships between various\nobjects, resulting in difficulty to obtain accurate spatial layout and relative\npositions of objects. To tackle these issues, we propose HIDA, a lightweight\nassistive system based on 3D point cloud instance segmentation with a\nsolid-state LiDAR sensor, for holistic indoor detection and avoidance. Our\nentire system consists of three hardware components, two interactive\nfunctions~(obstacle avoidance and object finding) and a voice user interface.\nBased on voice guidance, the point cloud from the most recent state of the\nchanging indoor environment is captured through an on-site scanning performed\nby the user. In addition, we design a point cloud segmentation model with dual\nlightweight decoders for semantic and offset predictions, which satisfies the\nefficiency of the whole system. After the 3D instance segmentation, we\npost-process the segmented point cloud by removing outliers and projecting all\npoints onto a top-view 2D map representation. The system integrates the\ninformation above and interacts with users intuitively by acoustic feedback.\nThe proposed 3D instance segmentation model has achieved state-of-the-art\nperformance on ScanNet v2 dataset. Comprehensive field tests with various tasks\nin a user study verify the usability and effectiveness of our system for\nassisting visually impaired people in holistic indoor understanding, obstacle\navoidance and object search.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:23:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Huayao", ""], ["Liu", "Ruiping", ""], ["Yang", "Kailun", ""], ["Zhang", "Jiaming", ""], ["Peng", "Kunyu", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.03182", "submitter": "Mahdi Maktabdar Oghaz", "authors": "Emily Waters, Mahdi Maktabdar Oghaz, Lakshmi Babu Saheer", "title": "Urban Tree Species Classification Using Aerial Imagery", "comments": "International Conference on Machine Learning (ICML 2021), Workshop on\n  Tackling Climate Change with Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban trees help regulate temperature, reduce energy consumption, improve\nurban air quality, reduce wind speeds, and mitigating the urban heat island\neffect. Urban trees also play a key role in climate change mitigation and\nglobal warming by capturing and storing atmospheric carbon-dioxide which is the\nlargest contributor to greenhouse gases. Automated tree detection and species\nclassification using aerial imagery can be a powerful tool for sustainable\nforest and urban tree management. Hence, This study first offers a pipeline for\ngenerating labelled dataset of urban trees using Google Map's aerial images and\nthen investigates how state of the art deep Convolutional Neural Network models\nsuch as VGG and ResNet handle the classification problem of urban tree aerial\nimages under different parameters. Experimental results show our best model\nachieves an average accuracy of 60% over 6 tree species.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:30:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Waters", "Emily", ""], ["Oghaz", "Mahdi Maktabdar", ""], ["Saheer", "Lakshmi Babu", ""]]}, {"id": "2107.03212", "submitter": "Lu Yin", "authors": "Lu Yin, Vlado Menkovski, Shiwei Liu, Mykola Pechenizkiy", "title": "Hierarchical Semantic Segmentation using Psychometric Learning", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning meaning to parts of image data is the goal of semantic image\nsegmentation. Machine learning methods, specifically supervised learning is\ncommonly used in a variety of tasks formulated as semantic segmentation. One of\nthe major challenges in the supervised learning approaches is expressing and\ncollecting the rich knowledge that experts have with respect to the meaning\npresent in the image data. Towards this, typically a fixed set of labels is\nspecified and experts are tasked with annotating the pixels, patches or\nsegments in the images with the given labels. In general, however, the set of\nclasses does not fully capture the rich semantic information present in the\nimages. For example, in medical imaging such as histology images, the different\nparts of cells could be grouped and sub-grouped based on the expertise of the\npathologist.\n  To achieve such a precise semantic representation of the concepts in the\nimage, we need access to the full depth of knowledge of the annotator. In this\nwork, we develop a novel approach to collect segmentation annotations from\nexperts based on psychometric testing. Our method consists of the psychometric\ntesting procedure, active query selection, query enhancement, and a deep metric\nlearning model to achieve a patch-level image embedding that allows for\nsemantic segmentation of images. We show the merits of our method with\nevaluation on the synthetically generated image, aerial image and histology\nimage.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:38:33 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yin", "Lu", ""], ["Menkovski", "Vlado", ""], ["Liu", "Shiwei", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2107.03215", "submitter": "Hanbin Dai", "authors": "Hanbin Dai, Hailin Shi, Wu Liu, Linfang Wang, Yinglu Liu and Tao Mei", "title": "FasterPose: A Faster Simple Baseline for Human Pose Estimation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of human pose estimation depends on the spatial accuracy of\nkeypoint localization. Most existing methods pursue the spatial accuracy\nthrough learning the high-resolution (HR) representation from input images. By\nthe experimental analysis, we find that the HR representation leads to a sharp\nincrease of computational cost, while the accuracy improvement remains marginal\ncompared with the low-resolution (LR) representation. In this paper, we propose\na design paradigm for cost-effective network with LR representation for\nefficient pose estimation, named FasterPose. Whereas the LR design largely\nshrinks the model complexity, yet how to effectively train the network with\nrespect to the spatial accuracy is a concomitant challenge. We study the\ntraining behavior of FasterPose, and formulate a novel regressive cross-entropy\n(RCE) loss function for accelerating the convergence and promoting the\naccuracy. The RCE loss generalizes the ordinary cross-entropy loss from the\nbinary supervision to a continuous range, thus the training of pose estimation\nnetwork is able to benefit from the sigmoid function. By doing so, the output\nheatmap can be inferred from the LR features without loss of spatial accuracy,\nwhile the computational cost and model size has been significantly reduced.\nCompared with the previously dominant network of pose estimation, our method\nreduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy.\nExtensive experiments show that FasterPose yields promising results on the\ncommon benchmarks, i.e., COCO and MPII, consistently validating the\neffectiveness and efficiency for practical utilization, especially the\nlow-latency and low-energy-budget applications in the non-GPU scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:39:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Dai", "Hanbin", ""], ["Shi", "Hailin", ""], ["Liu", "Wu", ""], ["Wang", "Linfang", ""], ["Liu", "Yinglu", ""], ["Mei", "Tao", ""]]}, {"id": "2107.03216", "submitter": "Shunning He", "authors": "Haiwei Pan, Shuning He, Kejia Zhang, Bo Qu, Chunling Chen, and Kun Shi", "title": "MuVAM: A Multi-View Attention-based Model for Medical Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Visual Question Answering (VQA) is a multi-modal challenging task\nwidely considered by research communities of the computer vision and natural\nlanguage processing. Since most current medical VQA models focus on visual\ncontent, ignoring the importance of text, this paper proposes a multi-view\nattention-based model(MuVAM) for medical visual question answering which\nintegrates the high-level semantics of medical images on the basis of text\ndescription. Firstly, different methods are utilized to extract the features of\nthe image and the question for the two modalities of vision and text. Secondly,\nthis paper proposes a multi-view attention mechanism that include\nImage-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view\nattention can correlate the question with image and word in order to better\nanalyze the question and get an accurate answer. Thirdly, a composite loss is\npresented to predict the answer accurately after multi-modal feature fusion and\nimprove the similarity between visual and textual cross-modal features. It\nconsists of classification loss and image-question complementary (IQC) loss.\nFinally, for data errors and missing labels in the VQA-RAD dataset, we\ncollaborate with medical experts to correct and complete this dataset and then\nconstruct an enhanced dataset, VQA-RADPh. The experiments on these two datasets\nshow that the effectiveness of MuVAM surpasses the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:40:25 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Pan", "Haiwei", ""], ["He", "Shuning", ""], ["Zhang", "Kejia", ""], ["Qu", "Bo", ""], ["Chen", "Chunling", ""], ["Shi", "Kun", ""]]}, {"id": "2107.03225", "submitter": "Xiaohan Xing", "authors": "Xiaohan Xing, Yuenan Hou, Hang Li, Yixuan Yuan, Hongsheng Li, Max\n  Q.-H. Meng", "title": "Categorical Relation-Preserving Contrastive Knowledge Distillation for\n  Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of medical images for training deep classification models is\ntypically very scarce, making these deep models prone to overfit the training\ndata. Studies showed that knowledge distillation (KD), especially the\nmean-teacher framework which is more robust to perturbations, can help mitigate\nthe over-fitting effect. However, directly transferring KD from computer vision\nto medical image classification yields inferior performance as medical images\nsuffer from higher intra-class variance and class imbalance. To address these\nissues, we propose a novel Categorical Relation-preserving Contrastive\nKnowledge Distillation (CRCKD) algorithm, which takes the commonly used\nmean-teacher model as the supervisor. Specifically, we propose a novel\nClass-guided Contrastive Distillation (CCD) module to pull closer positive\nimage pairs from the same class in the teacher and student models, while\npushing apart negative image pairs from different classes. With this\nregularization, the feature distribution of the student model shows higher\nintra-class similarity and inter-class variance. Besides, we propose a\nCategorical Relation Preserving (CRP) loss to distill the teacher's relational\nknowledge in a robust and class-balanced manner. With the contribution of the\nCCD and CRP, our CRCKD algorithm can distill the relational knowledge more\ncomprehensively. Extensive experiments on the HAM10000 and APTOS datasets\ndemonstrate the superiority of the proposed CRCKD method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:56:38 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Xing", "Xiaohan", ""], ["Hou", "Yuenan", ""], ["Li", "Hang", ""], ["Yuan", "Yixuan", ""], ["Li", "Hongsheng", ""], ["Meng", "Max Q. -H.", ""]]}, {"id": "2107.03227", "submitter": "Deep Patel", "authors": "Deep Patel, Erin Gao, Anirudh Koul, Siddha Ganju, Meher Anand Kasam", "title": "Scalable Data Balancing for Unlabeled Satellite Imagery", "comments": "Accepted to COSPAR 2021 Workshop on Machine Learning for Space\n  Sciences. 5 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data imbalance is a ubiquitous problem in machine learning. In large scale\ncollected and annotated datasets, data imbalance is either mitigated manually\nby undersampling frequent classes and oversampling rare classes, or planned for\nwith imputation and augmentation techniques. In both cases balancing data\nrequires labels. In other words, only annotated data can be balanced.\nCollecting fully annotated datasets is challenging, especially for large scale\nsatellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.\nAlthough the NASA Earth Imagery dataset is unlabeled, there are implicit\nproperties of the data source that we can rely on to hypothesize about its\nimbalance, such as distribution of land and water in the case of the Earth's\nimagery. We present a new iterative method to balance unlabeled data. Our\nmethod utilizes image embeddings as a proxy for image labels that can be used\nto balance data, and ultimately when trained increases overall accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:58:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Patel", "Deep", ""], ["Gao", "Erin", ""], ["Koul", "Anirudh", ""], ["Ganju", "Siddha", ""], ["Kasam", "Meher Anand", ""]]}, {"id": "2107.03279", "submitter": "Omar Vidal Pino", "authors": "Omar Vidal Pino, Erickson Rangel Nascimento, Mario Fernando Montenegro\n  Campos", "title": "Introducing the structural bases of typicality effects in deep learning", "comments": "14 pages (12 + 2 reference); 13 Figures and 2 Tables. arXiv admin\n  note: text overlap with arXiv:1906.03365", "journal-ref": null, "doi": "10.1016/j.imavis.2021.104249", "report-no": null, "categories": "cs.CV cs.AI math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we hypothesize that the effects of the degree of typicality in\nnatural semantic categories can be generated based on the structure of\nartificial categories learned with deep learning models. Motivated by the human\napproach to representing natural semantic categories and based on the Prototype\nTheory foundations, we propose a novel Computational Prototype Model (CPM) to\nrepresent the internal structure of semantic categories. Unlike other prototype\nlearning approaches, our mathematical framework proposes a first approach to\nprovide deep neural networks with the ability to model abstract semantic\nconcepts such as category central semantic meaning, typicality degree of an\nobject's image, and family resemblance relationship. We proposed several\nmethodologies based on the typicality's concept to evaluate our CPM-model in\nimage semantic processing tasks such as image classification, a global semantic\ndescription, and transfer learning. Our experiments on different image\ndatasets, such as ImageNet and Coco, showed that our approach might be an\nadmissible proposition in the effort to endow machines with greater power of\nabstraction for the semantic representation of objects' categories.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:15:43 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Pino", "Omar Vidal", ""], ["Nascimento", "Erickson Rangel", ""], ["Campos", "Mario Fernando Montenegro", ""]]}, {"id": "2107.03292", "submitter": "Alireza Asvadi", "authors": "Alireza Asvadi, Guillaume Dardenne, Jocelyne Troccaz, Valerie Burdin", "title": "Bone Surface Reconstruction and Clinical Features Estimation from Sparse\n  Landmarks and Statistical Shape Models: A feasibility study on the femur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study, we investigated a method allowing the determination of the\nfemur bone surface as well as its mechanical axis from some easy-to-identify\nbony landmarks. The reconstruction of the whole femur is therefore performed\nfrom these landmarks using a Statistical Shape Model (SSM). The aim of this\nresearch is therefore to assess the impact of the number, the position, and the\naccuracy of the landmarks for the reconstruction of the femur and the\ndetermination of its related mechanical axis, an important clinical parameter\nto consider for the lower limb analysis. Two statistical femur models were\ncreated from our in-house dataset and a publicly available dataset. Both were\nevaluated in terms of average point-to-point surface distance error and through\nthe mechanical axis of the femur. Furthermore, the clinical impact of using\nlandmarks on the skin in replacement of bony landmarks is investigated. The\npredicted proximal femurs from bony landmarks were more accurate compared to\non-skin landmarks while both had less than 3.5 degrees mechanical axis angle\ndeviation error. The results regarding the non-invasive determination of the\nmechanical axis are very encouraging and could open very interesting clinical\nperspectives for the analysis of the lower limb either for orthopedics or\nfunctional rehabilitation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:27:30 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Asvadi", "Alireza", ""], ["Dardenne", "Guillaume", ""], ["Troccaz", "Jocelyne", ""], ["Burdin", "Valerie", ""]]}, {"id": "2107.03315", "submitter": "Devin Guillory", "authors": "Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell,\n  Ludwig Schmidt", "title": "Predicting with Confidence on Unseen Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown that the performance of machine learning models can\nvary substantially when models are evaluated on data drawn from a distribution\nthat is close to but different from the training distribution. As a result,\npredicting model performance on unseen distributions is an important challenge.\nOur work connects techniques from domain adaptation and predictive uncertainty\nliterature, and allows us to predict model accuracy on challenging unseen\ndistributions without access to labeled data. In the context of distribution\nshift, distributional distances are often used to adapt models and improve\ntheir performance on new domains, however accuracy estimation, or other forms\nof predictive uncertainty, are often neglected in these investigations. Through\ninvestigating a wide range of established distributional distances, such as\nFrechet distance or Maximum Mean Discrepancy, we determine that they fail to\ninduce reliable estimates of performance under distribution shift. On the other\nhand, we find that the difference of confidences (DoC) of a classifier's\npredictions successfully estimates the classifier's performance change over a\nvariety of shifts. We specifically investigate the distinction between\nsynthetic and natural distribution shifts and observe that despite its\nsimplicity DoC consistently outperforms other quantifications of distributional\ndifference. $DoC$ reduces predictive error by almost half ($46\\%$) on several\nrealistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust\nand ImageNet-Rendition datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:50:18 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Guillory", "Devin", ""], ["Shankar", "Vaishaal", ""], ["Ebrahimi", "Sayna", ""], ["Darrell", "Trevor", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "2107.03323", "submitter": "Tim Cvetko", "authors": "Tim Cvetko", "title": "AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for\n  Brain Tumor Segmentation", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Brain tumor segmentation is a challenging problem in medical image analysis.\nThe endpoint is to generate the salient masks that accurately identify brain\ntumor regions in an fMRI screening. In this paper, we propose a novel attention\ngate (AG model) for brain tumor segmentation that utilizes both the edge\ndetecting unit and the attention gated network to highlight and segment the\nsalient regions from fMRI images. This feature enables us to eliminate the\nnecessity of having to explicitly point towards the damaged area(external\ntissue localization) and classify(classification) as per classical computer\nvision techniques. AGs can easily be integrated within the deep convolutional\nneural networks(CNNs). Minimal computional overhead is required while the AGs\nincrease the sensitivity scores significantly. We show that the edge detector\nalong with an attention gated mechanism provide a sufficient enough method for\nbrain segmentation reaching an IOU of 0.78\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:01:24 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cvetko", "Tim", ""]]}, {"id": "2107.03331", "submitter": "Aram Davtyan", "authors": "Aram Davtyan, Sepehr Sameni, Llukman Cerkezi, Givi Meishvilli, Adam\n  Bielski, Paolo Favaro", "title": "KaFiStO: A Kalman Filtering Framework for Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization is often cast as a deterministic problem, where the solution is\nfound through some iterative procedure such as gradient descent. However, when\ntraining neural networks the loss function changes over (iteration) time due to\nthe randomized selection of a subset of the samples. This randomization turns\nthe optimization problem into a stochastic one. We propose to consider the loss\nas a noisy observation with respect to some reference optimum. This\ninterpretation of the loss allows us to adopt Kalman filtering as an optimizer,\nas its recursive formulation is designed to estimate unknown parameters from\nnoisy measurements. Moreover, we show that the Kalman Filter dynamical model\nfor the evolution of the unknown parameters can be used to capture the gradient\ndynamics of advanced methods such as Momentum and Adam. We call this stochastic\noptimization method KaFiStO. KaFiStO is an easy to implement, scalable, and\nefficient method to train neural networks. We show that it also yields\nparameter estimates that are on par with or better than existing optimization\nalgorithms across several neural network architectures and machine learning\ntasks, such as computer vision and language modeling.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:13:57 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Davtyan", "Aram", ""], ["Sameni", "Sepehr", ""], ["Cerkezi", "Llukman", ""], ["Meishvilli", "Givi", ""], ["Bielski", "Adam", ""], ["Favaro", "Paolo", ""]]}, {"id": "2107.03332", "submitter": "Yanjie Li", "authors": "Yanjie Li, Sen Yang, Shoukui Zhang, Zhicheng Wang, Wankou Yang,\n  Shu-Tao Xia, Erjin Zhou", "title": "Is 2D Heatmap Representation Even Necessary for Human Pose Estimation?", "comments": "Code will be made publicly available at\n  https://github.com/leeyegy/SimDR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2D heatmap representation has dominated human pose estimation for years\ndue to its high performance. However, heatmap-based approaches have some\ndrawbacks: 1) The performance drops dramatically in the low-resolution images,\nwhich are frequently encountered in real-world scenarios. 2) To improve the\nlocalization precision, multiple upsample layers may be needed to recover the\nfeature map resolution from low to high, which are computationally expensive.\n3) Extra coordinate refinement is usually necessary to reduce the quantization\nerror of downscaled heatmaps. To address these issues, we propose a\n\\textbf{Sim}ple yet promising \\textbf{D}isentangled \\textbf{R}epresentation for\nkeypoint coordinate (\\emph{SimDR}), reformulating human keypoint localization\nas a task of classification. In detail, we propose to disentangle the\nrepresentation of horizontal and vertical coordinates for keypoint location,\nleading to a more efficient scheme without extra upsampling and refinement.\nComprehensive experiments conducted over COCO dataset show that the proposed\n\\emph{heatmap-free} methods outperform \\emph{heatmap-based} counterparts in all\ntested input resolutions, especially in lower resolutions by a large margin.\nCode will be made publicly available at \\url{https://github.com/leeyegy/SimDR}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:20:12 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 16:24:27 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Yanjie", ""], ["Yang", "Sen", ""], ["Zhang", "Shoukui", ""], ["Wang", "Zhicheng", ""], ["Yang", "Wankou", ""], ["Xia", "Shu-Tao", ""], ["Zhou", "Erjin", ""]]}, {"id": "2107.03337", "submitter": "Michael D. Multerer", "authors": "Helmut Harbrecht and Michael Multerer", "title": "Samplets: A new paradigm for data compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the concept of samplets by transferring the\nconstruction of Tausch-White wavelets to the realm of data. This way we obtain\na multilevel representation of discrete data which directly enables data\ncompression, detection of singularities and adaptivity. Applying samplets to\nrepresent kernel matrices, as they arise in kernel based learning or Gaussian\nprocess regression, we end up with quasi-sparse matrices. By thresholding small\nentries, these matrices are compressible to O(N log N) relevant entries, where\nN is the number of data points. This feature allows for the use of fill-in\nreducing reorderings to obtain a sparse factorization of the compressed\nmatrices. Besides the comprehensive introduction to samplets and their\nproperties, we present extensive numerical studies to benchmark the approach.\nOur results demonstrate that samplets mark a considerable step in the direction\nof making large data sets accessible for analysis.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:25:12 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 08:06:29 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Harbrecht", "Helmut", ""], ["Multerer", "Michael", ""]]}, {"id": "2107.03352", "submitter": "Morven Jiang", "authors": "Chengzhi Jiang, Yanzhou Su, Wen Wang, Haiwei Bai, Haijun Liu, Jian\n  Cheng", "title": "IntraLoss: Further Margin via Gradient-Enhancing Term for Deep Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing classification-based face recognition methods have achieved\nremarkable progress, introducing large margin into hypersphere manifold to\nlearn discriminative facial representations. However, the feature distribution\nis ignored. Poor feature distribution will wipe out the performance improvement\nbrought about by margin scheme. Recent studies focus on the unbalanced\ninter-class distribution and form a equidistributed feature representations by\npenalizing the angle between identity and its nearest neighbor. But the problem\nis more than that, we also found the anisotropy of intra-class distribution. In\nthis paper, we propose the `gradient-enhancing term' that concentrates on the\ndistribution characteristics within the class. This method, named IntraLoss,\nexplicitly performs gradient enhancement in the anisotropic region so that the\nintra-class distribution continues to shrink, resulting in isotropic and more\ncompact intra-class distribution and further margin between identities. The\nexperimental results on LFW, YTF and CFP-FP show that our outperforms\nstate-of-the-art methods by gradient enhancement, demonstrating the superiority\nof our method. In addition, our method has intuitive geometric interpretation\nand can be easily combined with existing methods to solve the previously\nignored problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:53:45 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Jiang", "Chengzhi", ""], ["Su", "Yanzhou", ""], ["Wang", "Wen", ""], ["Bai", "Haiwei", ""], ["Liu", "Haijun", ""], ["Cheng", "Jian", ""]]}, {"id": "2107.03358", "submitter": "Bingchen Zhao", "authors": "Bingchen Zhao, Kai Han", "title": "Novel Visual Category Discovery with Dual Ranking Statistics and Mutual\n  Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of novel visual category discovery,\ni.e., grouping unlabelled images from new classes into different semantic\npartitions by leveraging a labelled dataset that contains images from other\ndifferent but relevant categories. This is a more realistic and challenging\nsetting than conventional semi-supervised learning. We propose a two-branch\nlearning framework for this problem, with one branch focusing on local\npart-level information and the other branch focusing on overall\ncharacteristics. To transfer knowledge from the labelled data to the\nunlabelled, we propose using dual ranking statistics on both branches to\ngenerate pseudo labels for training on the unlabelled data. We further\nintroduce a mutual knowledge distillation method to allow information exchange\nand encourage agreement between the two branches for discovering new\ncategories, allowing our model to enjoy the benefits of global and local\nfeatures. We comprehensively evaluate our method on public benchmarks for\ngeneric object classification, as well as the more challenging datasets for\nfine-grained visual recognition, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:14:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhao", "Bingchen", ""], ["Han", "Kai", ""]]}, {"id": "2107.03368", "submitter": "Bart Iver van Blokland", "authors": "Bart Iver van Blokland and Theoharis Theoharis", "title": "Partial 3D Object Retrieval using Local Binary QUICCI Descriptors and\n  Dissimilarity Tree Indexing", "comments": "19 pages, 17 figures, to be published in Computers & Graphics", "journal-ref": null, "doi": "10.1016/j.cag.2021.07.018", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A complete pipeline is presented for accurate and efficient partial 3D object\nretrieval based on Quick Intersection Count Change Image (QUICCI) binary local\ndescriptors and a novel indexing tree. It is shown how a modification to the\nQUICCI query descriptor makes it ideal for partial retrieval. An indexing\nstructure called Dissimilarity Tree is proposed which can significantly\naccelerate searching the large space of local descriptors; this is applicable\nto QUICCI and other binary descriptors. The index exploits the distribution of\nbits within descriptors for efficient retrieval. The retrieval pipeline is\ntested on the artificial part of SHREC'16 dataset with near-ideal retrieval\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:30:47 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["van Blokland", "Bart Iver", ""], ["Theoharis", "Theoharis", ""]]}, {"id": "2107.03375", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo and Yang Gao", "title": "Differentiable Architecture Pruning for Transfer Learning", "comments": "19 pages (main + appendix), 7 figures and 1 table, Workshop @ ICML\n  2021, 24th July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new gradient-based approach for extracting sub-architectures\nfrom a given large model. Contrarily to existing pruning methods, which are\nunable to disentangle the network architecture and the corresponding weights,\nour architecture-pruning scheme produces transferable new structures that can\nbe successfully retrained to solve different tasks. We focus on a\ntransfer-learning setup where architectures can be trained on a large data set\nbut very few data points are available for fine-tuning them on new tasks. We\ndefine a new gradient-based algorithm that trains architectures of arbitrarily\nlow complexity independently from the attached weights. Given a search space\ndefined by an existing large neural model, we reformulate the architecture\nsearch task as a complexity-penalized subset-selection problem and solve it\nthrough a two-temperature relaxation scheme. We provide theoretical convergence\nguarantees and validate the proposed transfer-learning strategy on real data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:44:59 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Colombo", "Nicolo", ""], ["Gao", "Yang", ""]]}, {"id": "2107.03377", "submitter": "Mingze Xu", "authors": "Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia, Zhuowen Tu,\n  Stefano Soatto", "title": "Long Short-Term Transformer for Online Action Detection", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present Long Short-term TRansformer (LSTR), a new temporal\nmodeling algorithm for online action detection, by employing a long- and\nshort-term memories mechanism that is able to model prolonged sequence data. It\nconsists of an LSTR encoder that is capable of dynamically exploiting\ncoarse-scale historical information from an extensively long time window (e.g.,\n2048 long-range frames of up to 8 minutes), together with an LSTR decoder that\nfocuses on a short time window (e.g., 32 short-range frames of 8 seconds) to\nmodel the fine-scale characterization of the ongoing event. Compared to prior\nwork, LSTR provides an effective and efficient method to model long videos with\nless heuristic algorithm design. LSTR achieves significantly improved results\non standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS\nSegment, over the existing state-of-the-art approaches. Extensive empirical\nanalysis validates the setup of the long- and short-term memories and the\ndesign choices of LSTR.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:49:51 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Xu", "Mingze", ""], ["Xiong", "Yuanjun", ""], ["Chen", "Hao", ""], ["Li", "Xinyu", ""], ["Xia", "Wei", ""], ["Tu", "Zhuowen", ""], ["Soatto", "Stefano", ""]]}, {"id": "2107.03436", "submitter": "James Oldfield", "authors": "Yannis Panagakis, Jean Kossaifi, Grigorios G. Chrysos, James Oldfield,\n  Mihalis A. Nicolaou, Anima Anandkumar, Stefanos Zafeiriou", "title": "Tensor Methods in Computer Vision and Deep Learning", "comments": "Proceedings of the IEEE (2021)", "journal-ref": null, "doi": "10.1109/JPROC.2021.3074329", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors, or multidimensional arrays, are data structures that can naturally\nrepresent visual data of multiple dimensions. Inherently able to efficiently\ncapture structured, latent semantic spaces and high-order interactions, tensors\nhave a long history of applications in a wide span of computer vision problems.\nWith the advent of the deep learning paradigm shift in computer vision, tensors\nhave become even more fundamental. Indeed, essential ingredients in modern deep\nlearning architectures, such as convolutions and attention mechanisms, can\nreadily be considered as tensor mappings. In effect, tensor methods are\nincreasingly finding significant applications in deep learning, including the\ndesign of memory and compute efficient network architectures, improving\nrobustness to random noise and adversarial attacks, and aiding the theoretical\nunderstanding of deep networks.\n  This article provides an in-depth and practical review of tensors and tensor\nmethods in the context of representation learning and deep learning, with a\nparticular focus on visual data analysis and computer vision applications.\nConcretely, besides fundamental work in tensor-based visual data analysis\nmethods, we focus on recent developments that have brought on a gradual\nincrease of tensor methods, especially in deep learning architectures, and\ntheir implications in computer vision applications. To further enable the\nnewcomer to grasp such concepts quickly, we provide companion Python notebooks,\ncovering key aspects of the paper and implementing them, step-by-step with\nTensorLy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 18:42:45 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Panagakis", "Yannis", ""], ["Kossaifi", "Jean", ""], ["Chrysos", "Grigorios G.", ""], ["Oldfield", "James", ""], ["Nicolaou", "Mihalis A.", ""], ["Anandkumar", "Anima", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2107.03438", "submitter": "Junha Roh", "authors": "Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox", "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To realize robots that can understand human instructions and perform\nmeaningful tasks in the near future, it is important to develop learned models\nthat can understand referential language to identify common objects in\nreal-world 3D scenes. In this paper, we develop a spatial-language model for a\n3D visual grounding problem. Specifically, given a reconstructed 3D scene in\nthe form of a point cloud with 3D bounding boxes of potential object\ncandidates, and a language utterance referring to a target object in the scene,\nour model identifies the target object from a set of potential candidates. Our\nspatial-language model uses a transformer-based architecture that combines\nspatial embedding from bounding-box with a finetuned language embedding from\nDistilBert and reasons among the objects in the 3D scene to find the target\nobject. We show that our model performs competitively on visio-linguistic\ndatasets proposed by ReferIt3D. We provide additional analysis of performance\nin spatial reasoning tasks decoupled from perception noise, the effect of\nview-dependent utterances in terms of accuracy, and view-point annotations for\npotential robotics applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 18:55:03 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 04:55:50 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Roh", "Junha", ""], ["Desingh", "Karthik", ""], ["Farhadi", "Ali", ""], ["Fox", "Dieter", ""]]}, {"id": "2107.03442", "submitter": "Mohammad Hamghalam", "authors": "Mohammad Hamghalam, Alejandro F. Frangi, Baiying Lei, and Amber L.\n  Simpson", "title": "Modality Completion via Gaussian Process Prior Variational Autoencoders\n  for Multi-Modal Glioma Segmentation", "comments": "Accepted in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In large studies involving multi protocol Magnetic Resonance Imaging (MRI),\nit can occur to miss one or more sub-modalities for a given patient owing to\npoor quality (e.g. imaging artifacts), failed acquisitions, or hallway\ninterrupted imaging examinations. In some cases, certain protocols are\nunavailable due to limited scan time or to retrospectively harmonise the\nimaging protocols of two independent studies. Missing image modalities pose a\nchallenge to segmentation frameworks as complementary information contributed\nby the missing scans is then lost. In this paper, we propose a novel model,\nMulti-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute\none or more missing sub-modalities for a patient scan. MGP-VAE can leverage the\nGaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the\nsubjects/patients and sub-modalities correlations. Instead of designing one\nnetwork for each possible subset of present sub-modalities or using frameworks\nto mix feature maps, missing data can be generated from a single model based on\nall the available samples. We show the applicability of MGP-VAE on brain tumor\nsegmentation where either, two, or three of four sub-modalities may be missing.\nOur experiments against competitive segmentation baselines with missing\nsub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE\nmodel for segmentation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:06:34 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hamghalam", "Mohammad", ""], ["Frangi", "Alejandro F.", ""], ["Lei", "Baiying", ""], ["Simpson", "Amber L.", ""]]}, {"id": "2107.03453", "submitter": "Xinlin Li", "authors": "Xinlin Li, Bang Liu, Yaoliang Yu, Wulong Liu, Chunjing Xu, Vahid\n  Partovi Nia", "title": "$S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of\n  Low-bit Shift Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shift neural networks reduce computation complexity by removing expensive\nmultiplication operations and quantizing continuous weights into low-bit\ndiscrete values, which are fast and energy efficient compared to conventional\nneural networks. However, existing shift networks are sensitive to the weight\ninitialization, and also yield a degraded performance caused by vanishing\ngradient and weight sign freezing problem. To address these issues, we propose\nS low-bit re-parameterization, a novel technique for training low-bit shift\nnetworks. Our method decomposes a discrete parameter in a sign-sparse-shift\n3-fold manner. In this way, it efficiently learns a low-bit network with a\nweight dynamics similar to full-precision networks and insensitive to weight\ninitialization. Our proposed training method pushes the boundaries of shift\nneural networks and shows 3-bit shift networks out-performs their\nfull-precision counterparts in terms of top-1 accuracy on ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:33:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Li", "Xinlin", ""], ["Liu", "Bang", ""], ["Yu", "Yaoliang", ""], ["Liu", "Wulong", ""], ["Xu", "Chunjing", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "2107.03461", "submitter": "Carmina Perez-Guerrero", "authors": "Carmina P\\'erez-Guerrero, Adriana Palacios, Gilberto Ochoa-Ruiz,\n  Christian Mata, Miguel Gonzalez-Mendoza, Luis Eduardo Falc\\'on-Morales", "title": "Comparing ML based Segmentation Models on Jet Fire Radiation Zone", "comments": "This pre-print is currently under review for the Mexican Conference\n  on AI (MICAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Risk assessment is relevant in any workplace, however there is a degree of\nunpredictability when dealing with flammable or hazardous materials so that\ndetection of fire accidents by itself may not be enough. An example of this is\nthe impingement of jet fires, where the heat fluxes of the flame could reach\nnearby equipment and dramatically increase the probability of a domino effect\nwith catastrophic results. Because of this, the characterization of such fire\naccidents is important from a risk management point of view. One such\ncharacterization would be the segmentation of different radiation zones within\nthe flame, so this paper presents an exploratory research regarding several\ntraditional computer vision and Deep Learning segmentation approaches to solve\nthis specific problem. A data set of propane jet fires is used to train and\nevaluate the different approaches and given the difference in the distribution\nof the zones and background of the images, different loss functions, that seek\nto alleviate data imbalance, are also explored. Additionally, different metrics\nare correlated to a manual ranking performed by experts to make an evaluation\nthat closely resembles the expert's criteria. The Hausdorff Distance and\nAdjsted Random Index were the metrics with the highest correlation and the best\nresults were obtained from the UNet architecture with a Weighted Cross-Entropy\nLoss. These results can be used in future research to extract more geometric\ninformation from the segmentation masks or could even be implemented on other\ntypes of fire accidents.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:52:52 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["P\u00e9rez-Guerrero", "Carmina", ""], ["Palacios", "Adriana", ""], ["Ochoa-Ruiz", "Gilberto", ""], ["Mata", "Christian", ""], ["Gonzalez-Mendoza", "Miguel", ""], ["Falc\u00f3n-Morales", "Luis Eduardo", ""]]}, {"id": "2107.03463", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Javad Khaghani, Li Cheng, Hossein\n  Ghanei-Yakhdan, Shohreh Kasaei", "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural\n  Architecture Search", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strong visual object tracker nowadays relies on its well-crafted modules,\nwhich typically consist of manually-designed network architectures to deliver\nhigh-quality tracking results. Not surprisingly, the manual design process\nbecomes a particularly challenging barrier, as it demands sufficient prior\nexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,\nneural architecture search has gaining grounds in practical applications such\nas image segmentation, as a promising method in tackling the issue of automated\nsearch of feasible network structures. In this work, we propose a novel\ncell-level differentiable architecture search mechanism to automate the network\ndesign of the tracking module, aiming to adapt backbone features to the\nobjective of a tracking network during offline training. The proposed approach\nis simple, efficient, and with no need to stack a series of modules to\nconstruct a network. Our approach is easy to be incorporated into existing\ntrackers, which is empirically validated using different differentiable\narchitecture search-based methods and tracking objectives. Extensive\nexperimental evaluations demonstrate the superior performance of our approach\nover five commonly-used benchmarks. Meanwhile, our automated searching process\ntakes 41 (18) hours for the second (first) order DARTS method on the\nTrackingNet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:16:45 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Khaghani", "Javad", ""], ["Cheng", "Li", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2107.03465", "submitter": "Panagiotis Antoniadis", "authors": "Panagiotis Antoniadis, Ioannis Pikoulis, Panagiotis P. Filntisis,\n  Petros Maragos", "title": "An audiovisual and contextual approach for categorical and continuous\n  emotion recognition in-the-wild", "comments": "6 pages, 1 figure, 2 tables, submitted to the 2nd Affective Behavior\n  Analysis in-the-wild (ABAW2) Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we tackle the task of video-based audio-visual emotion\nrecognition, within the premises of the 2nd Workshop and Competition on\nAffective Behavior Analysis in-the-wild (ABAW). Poor illumination conditions,\nhead/body orientation and low image resolution constitute factors that can\npotentially hinder performance in case of methodologies that solely rely on the\nextraction and analysis of facial features. In order to alleviate this problem,\nwe leverage bodily as well as contextual features, as part of a broader emotion\nrecognition framework. We choose to use a standard CNN-RNN cascade as the\nbackbone of our proposed model for sequence-to-sequence (seq2seq) learning.\nApart from learning through the RGB input modality, we construct an aural\nstream which operates on sequences of extracted mel-spectrograms. Our extensive\nexperiments on the challenging and newly assembled Affect-in-the-wild-2\n(Aff-Wild2) dataset verify the superiority of our methods over existing\napproaches, while by properly incorporating all of the aforementioned modules\nin a network ensemble, we manage to surpass the previous best published\nrecognition scores, in the official validation set. All the code was\nimplemented using PyTorch\\footnote{\\url{https://pytorch.org/}} and is publicly\navailable\\footnote{\\url{https://github.com/PanosAntoniadis/NTUA-ABAW2021}}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 20:13:17 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 13:07:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Antoniadis", "Panagiotis", ""], ["Pikoulis", "Ioannis", ""], ["Filntisis", "Panagiotis P.", ""], ["Maragos", "Petros", ""]]}, {"id": "2107.03552", "submitter": "Jeffrey Gu", "authors": "Jeffrey Gu and Serena Yeung", "title": "Staying in Shape: Learning Invariant Shape Representations using\n  Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating representations of shapes that are invari-ant to isometric or\nalmost-isometric transforma-tions has long been an area of interest in shape\nanal-ysis, since enforcing invariance allows the learningof more effective and\nrobust shape representations.Most existing invariant shape representations\narehandcrafted, and previous work on learning shaperepresentations do not focus\non producing invariantrepresentations. To solve the problem of\nlearningunsupervised invariant shape representations, weuse contrastive\nlearning, which produces discrimi-native representations through learning\ninvarianceto user-specified data augmentations. To producerepresentations that\nare specifically isometry andalmost-isometry invariant, we propose new\ndataaugmentations that randomly sample these transfor-mations. We show\nexperimentally that our methodoutperforms previous unsupervised learning\nap-proaches in both effectiveness and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 00:53:24 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Gu", "Jeffrey", ""], ["Yeung", "Serena", ""]]}, {"id": "2107.03554", "submitter": "Byeongjoon Noh", "authors": "Byeongjoon Noh, Wonjun Noh, David Lee, Hwasoo Yeo", "title": "Automated Object Behavioral Feature Extraction for Potential Risk\n  Analysis based on Video Sensor", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrians are exposed to risk of death or serious injuries on roads,\nespecially unsignalized crosswalks, for a variety of reasons. To date, an\nextensive variety of studies have reported on vision based traffic safety\nsystem. However, many studies required manual inspection of the volumes of\ntraffic video to reliably obtain traffic related objects behavioral factors. In\nthis paper, we propose an automated and simpler system for effectively\nextracting object behavioral features from video sensors deployed on the road.\nWe conduct basic statistical analysis on these features, and show how they can\nbe useful for monitoring the traffic behavior on the road. We confirm the\nfeasibility of the proposed system by applying our prototype to two\nunsignalized crosswalks in Osan city, South Korea. To conclude, we compare\nbehaviors of vehicles and pedestrians in those two areas by simple statistical\nanalysis. This study demonstrates the potential for a network of connected\nvideo sensors to provide actionable data for smart cities to improve pedestrian\nsafety in dangerous road environments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 01:11:31 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Noh", "Byeongjoon", ""], ["Noh", "Wonjun", ""], ["Lee", "David", ""], ["Yeo", "Hwasoo", ""]]}, {"id": "2107.03575", "submitter": "Pengxiang Ding", "authors": "Pengxiang Ding and Jianqin Yin", "title": "Uncertainty-aware Human Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is essential for tasks such as human motion analysis\nand human-robot interactions. Most existing approaches have been proposed to\nrealize motion prediction. However, they ignore an important task, the\nevaluation of the quality of the predicted result. It is far more enough for\ncurrent approaches in actual scenarios because people can't know how to\ninteract with the machine without the evaluation of prediction, and unreliable\npredictions may mislead the machine to harm the human. Hence, we propose an\nuncertainty-aware framework for human motion prediction (UA-HMP). Concretely,\nwe first design an uncertainty-aware predictor through Gaussian modeling to\nachieve the value and the uncertainty of predicted motion. Then, an\nuncertainty-guided learning scheme is proposed to quantitate the uncertainty\nand reduce the negative effect of the noisy samples during optimization for\nbetter performance. Our proposed framework is easily combined with current SOTA\nbaselines to overcome their weakness in uncertainty modeling with slight\nparameters increment. Extensive experiments also show that they can achieve\nbetter performance in both short and long-term predictions in H3.6M, CMU-Mocap.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 03:09:01 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ding", "Pengxiang", ""], ["Yin", "Jianqin", ""]]}, {"id": "2107.03576", "submitter": "Jian Jia", "authors": "Jian Jia, Houjing Huang, Xiaotang Chen and Kaiqi Huang", "title": "Rethinking of Pedestrian Attribute Recognition: A Reliable Evaluation\n  under Zero-Shot Pedestrian Identity Setting", "comments": "13 pages, 6 figures, journal version of arXiv:2005.11909, submitted\n  to TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute recognition aims to assign multiple attributes to one\npedestrian image captured by a video surveillance camera. Although numerous\nmethods are proposed and make tremendous progress, we argue that it is time to\nstep back and analyze the status quo of the area. We review and rethink the\nrecent progress from three perspectives. First, given that there is no explicit\nand complete definition of pedestrian attribute recognition, we formally define\nand distinguish pedestrian attribute recognition from other similar tasks.\nSecond, based on the proposed definition, we expose the limitations of the\nexisting datasets, which violate the academic norm and are inconsistent with\nthe essential requirement of practical industry application. Thus, we propose\ntwo datasets, PETA\\textsubscript{$ZS$} and RAP\\textsubscript{$ZS$}, constructed\nfollowing the zero-shot settings on pedestrian identity. In addition, we also\nintroduce several realistic criteria for future pedestrian attribute dataset\nconstruction. Finally, we reimplement existing state-of-the-art methods and\nintroduce a strong baseline method to give reliable evaluations and fair\ncomparisons. Experiments are conducted on four existing datasets and two\nproposed datasets to measure progress on pedestrian attribute recognition.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 03:12:24 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Jia", "Jian", ""], ["Huang", "Houjing", ""], ["Chen", "Xiaotang", ""], ["Huang", "Kaiqi", ""]]}, {"id": "2107.03578", "submitter": "Wei Li", "authors": "Wei Li, Dezhao Luo, Bo Fang, Yu Zhou, Weiping Wang", "title": "Video 3D Sampling for Self-supervised Representation Learning", "comments": "9 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing video self-supervised methods mainly leverage temporal\nsignals of videos, ignoring that the semantics of moving objects and\nenvironmental information are all critical for video-related tasks. In this\npaper, we propose a novel self-supervised method for video representation\nlearning, referred to as Video 3D Sampling (V3S). In order to sufficiently\nutilize the information (spatial and temporal) provided in videos, we\npre-process a video from three dimensions (width, height, time). As a result,\nwe can leverage the spatial information (the size of objects), temporal\ninformation (the direction and magnitude of motions) as our learning target. In\nour implementation, we combine the sampling of the three dimensions and propose\nthe scale and projection transformations in space and time respectively. The\nexperimental results show that, when applied to action recognition, video\nretrieval and action similarity labeling, our approach improves the\nstate-of-the-arts with significant margins.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 03:22:06 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Li", "Wei", ""], ["Luo", "Dezhao", ""], ["Fang", "Bo", ""], ["Zhou", "Yu", ""], ["Wang", "Weiping", ""]]}, {"id": "2107.03591", "submitter": "Yonghao Dang", "authors": "Yonghao Dang and Jianqin Yin", "title": "Relation-Based Associative Joint Location for Human Pose Estimation in\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based human pose estimation (HPE) is a vital yet challenging task.\nWhile deep learning methods have made significant progress for the HPE, most\napproaches to this task detect each joint independently, damaging the pose\nstructural information. In this paper, unlike the prior methods, we propose a\nRelation-based Pose Semantics Transfer Network (RPSTN) to locate joints\nassociatively. Specifically, we design a lightweight joint relation extractor\n(JRE) to model the pose structural features and associatively generate heatmaps\nfor joints by modeling the relation between any two joints heuristically\ninstead of building each joint heatmap independently. Actually, the proposed\nJRE module models the spatial configuration of human poses through the\nrelationship between any two joints. Moreover, considering the temporal\nsemantic continuity of videos, the pose semantic information in the current\nframe is beneficial for guiding the location of joints in the next frame.\nTherefore, we use the idea of knowledge reuse to propagate the pose semantic\ninformation between consecutive frames. In this way, the proposed RPSTN\ncaptures temporal dynamics of poses. On the one hand, the JRE module can infer\ninvisible joints according to the relationship between the invisible joints and\nother visible joints in space. On the other hand, in the time, the propose\nmodel can transfer the pose semantic features from the non-occluded frame to\nthe occluded frame to locate occluded joints. Therefore, our method is robust\nto the occlusion and achieves state-of-the-art results on the two challenging\ndatasets, which demonstrates its effectiveness for video-based human pose\nestimation. We will release the code and models publicly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 04:05:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Dang", "Yonghao", ""], ["Yin", "Jianqin", ""]]}, {"id": "2107.03601", "submitter": "Shuang Deng", "authors": "Shuang Deng, Qiulei Dong, and Bo Liu", "title": "SCSS-Net: Superpoint Constrained Semi-supervised Segmentation Network\n  for 3D Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many existing deep neural networks (DNNs) for 3D point cloud semantic\nsegmentation require a large amount of fully labeled training data. However,\nmanually assigning point-level labels on the complex scenes is time-consuming.\nWhile unlabeled point clouds can be easily obtained from sensors or\nreconstruction, we propose a superpoint constrained semi-supervised\nsegmentation network for 3D point clouds, named as SCSS-Net. Specifically, we\nuse the pseudo labels predicted from unlabeled point clouds for self-training,\nand the superpoints produced by geometry-based and color-based Region Growing\nalgorithms are combined to modify and delete pseudo labels with low confidence.\nAdditionally, we propose an edge prediction module to constrain the features\nfrom edge points of geometry and color. A superpoint feature aggregation module\nand superpoint feature consistency loss functions are introduced to smooth the\npoint features in each superpoint. Extensive experimental results on two 3D\npublic indoor datasets demonstrate that our method can achieve better\nperformance than some state-of-the-art point cloud segmentation networks and\nsome popular semi-supervised segmentation methods with few labeled scenes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 04:43:21 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 07:18:51 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Deng", "Shuang", ""], ["Dong", "Qiulei", ""], ["Liu", "Bo", ""]]}, {"id": "2107.03602", "submitter": "Noriaki Hashimoto", "authors": "Noriaki Hashimoto, Yusuke Takagi, Hiroki Masuda, Hiroaki Miyoshi, Kei\n  Kohno, Miharu Nagaishi, Kensaku Sato, Mai Takeuchi, Takuya Furuta, Keisuke\n  Kawamoto, Kyohei Yamada, Mayuko Moritsubo, Kanako Inoue, Yasumasa Shimasaki,\n  Yusuke Ogura, Teppei Imamoto, Tatsuzo Mishina, Koichi Ohshima, Hidekata\n  Hontani, Ichiro Takeuchi", "title": "Case-based similar image retrieval for weakly annotated large\n  histopathological images of malignant lymphoma using deep metric learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present study, we propose a novel case-based similar image retrieval\n(SIR) method for hematoxylin and eosin (H&E)-stained histopathological images\nof malignant lymphoma. When a whole slide image (WSI) is used as an input\nquery, it is desirable to be able to retrieve similar cases by focusing on\nimage patches in pathologically important regions such as tumor cells. To\naddress this problem, we employ attention-based multiple instance learning,\nwhich enables us to focus on tumor-specific regions when the similarity between\ncases is computed. Moreover, we employ contrastive distance metric learning to\nincorporate immunohistochemical (IHC) staining patterns as useful supervised\ninformation for defining appropriate similarity between heterogeneous malignant\nlymphoma cases. In the experiment with 249 malignant lymphoma patients, we\nconfirmed that the proposed method exhibited higher evaluation measures than\nthe baseline case-based SIR methods. Furthermore, the subjective evaluation by\npathologists revealed that our similarity measure using IHC staining patterns\nis appropriate for representing the similarity of H&E-stained tissue images for\nmalignant lymphoma.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 04:50:37 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 03:16:21 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Hashimoto", "Noriaki", ""], ["Takagi", "Yusuke", ""], ["Masuda", "Hiroki", ""], ["Miyoshi", "Hiroaki", ""], ["Kohno", "Kei", ""], ["Nagaishi", "Miharu", ""], ["Sato", "Kensaku", ""], ["Takeuchi", "Mai", ""], ["Furuta", "Takuya", ""], ["Kawamoto", "Keisuke", ""], ["Yamada", "Kyohei", ""], ["Moritsubo", "Mayuko", ""], ["Inoue", "Kanako", ""], ["Shimasaki", "Yasumasa", ""], ["Ogura", "Yusuke", ""], ["Imamoto", "Teppei", ""], ["Mishina", "Tatsuzo", ""], ["Ohshima", "Koichi", ""], ["Hontani", "Hidekata", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2107.03606", "submitter": "Shuji Oishi", "authors": "Shuji Oishi, Kenji Koide, Masashi Yokozuka, Atsuhiko Banno", "title": "4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a framework for capturing human attention in the\nspatio-temporal domain using eye-tracking glasses. Attention mapping is a key\ntechnology for human perceptual activity analysis or Human-Robot Interaction\n(HRI) to support human visual cognition; however, measuring human attention in\ndynamic environments is challenging owing to the difficulty in localizing the\nsubject and dealing with moving objects. To address this, we present a\ncomprehensive framework, 4D Attention, for unified gaze mapping onto static and\ndynamic objects. Specifically, we estimate the glasses pose by leveraging a\nloose coupling of direct visual localization and Inertial Measurement Unit\n(IMU) values. Further, by installing reconstruction components into our\nframework, dynamic objects not captured in the 3D environment map are\ninstantiated based on the input images. Finally, a scene rendering component\nsynthesizes a first-person view with identification (ID) textures and performs\ndirect 2D-3D gaze association. Quantitative evaluations showed the\neffectiveness of our framework. Additionally, we demonstrated the applications\nof 4D Attention through experiments in real situations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 04:55:18 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Oishi", "Shuji", ""], ["Koide", "Kenji", ""], ["Yokozuka", "Masashi", ""], ["Banno", "Atsuhiko", ""]]}, {"id": "2107.03609", "submitter": "Lingyun Wu", "authors": "Lingyun Wu, Zhiqiang Hu, Yuanfeng Ji, Ping Luo, Shaoting Zhang", "title": "Multi-frame Collaboration for Effective Endoscopic Video Polyp Detection\n  via Spatial-Temporal Feature Transformation", "comments": "Accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precise localization of polyp is crucial for early cancer screening in\ngastrointestinal endoscopy. Videos given by endoscopy bring both richer\ncontextual information as well as more challenges than still images. The\ncamera-moving situation, instead of the common camera-fixed-object-moving one,\nleads to significant background variation between frames. Severe internal\nartifacts (e.g. water flow in the human body, specular reflection by tissues)\ncan make the quality of adjacent frames vary considerately. These factors\nhinder a video-based model to effectively aggregate features from neighborhood\nframes and give better predictions. In this paper, we present Spatial-Temporal\nFeature Transformation (STFT), a multi-frame collaborative framework to address\nthese issues. Spatially, STFT mitigates inter-frame variations in the\ncamera-moving situation with feature alignment by proposal-guided deformable\nconvolutions. Temporally, STFT proposes a channel-aware attention module to\nsimultaneously estimate the quality and correlation of adjacent frames for\nadaptive feature aggregation. Empirical studies and superior results\ndemonstrate the effectiveness and stability of our method. For example, STFT\nimproves the still image baseline FCOS by 10.6% and 20.6% on the comprehensive\nF1-score of the polyp localization task in CVC-Clinic and ASUMayo datasets,\nrespectively, and outperforms the state-of-the-art video-based method by 3.6%\nand 8.0%, respectively. Code is available at\n\\url{https://github.com/lingyunwu14/STFT}.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 05:17:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wu", "Lingyun", ""], ["Hu", "Zhiqiang", ""], ["Ji", "Yuanfeng", ""], ["Luo", "Ping", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2107.03610", "submitter": "Guangming Wang", "authors": "Guangming Wang, Shuaiqi Ren, and Hesheng Wang", "title": "NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from\n  Geometry", "comments": "10 pages, 7 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation is a fundamental problem of computer vision and has\nmany applications in the fields of robot learning and autonomous driving. This\npaper reveals novel geometric laws of optical flow based on the insight and\ndetailed definition of non-occlusion. Then, two novel loss functions are\nproposed for the unsupervised learning of optical flow based on the geometric\nlaws of non-occlusion. Specifically, after the occlusion part of the images are\nmasked, the flowing process of pixels is carefully considered and geometric\nconstraints are conducted based on the geometric laws of optical flow. First,\nneighboring pixels in the first frame will not intersect during the pixel\ndisplacement to the second frame. Secondly, when the cluster containing\nadjacent four pixels in the first frame moves to the second frame, no other\npixels will flow into the quadrilateral formed by them. According to the two\ngeometrical constraints, the optical flow non-intersection loss and the optical\nflow non-blocking loss in the non-occlusion regions are proposed. Two loss\nfunctions punish the irregular and inexact optical flows in the non-occlusion\nregions. The experiments on datasets demonstrated that the proposed\nunsupervised losses of optical flow based on the geometric laws in\nnon-occlusion regions make the estimated optical flow more refined in detail,\nand improve the performance of unsupervised learning of optical flow. In\naddition, the experiments training on synthetic data and evaluating on real\ndata show that the generalization ability of optical flow network is improved\nby our proposed unsupervised approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 05:19:54 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Guangming", ""], ["Ren", "Shuaiqi", ""], ["Wang", "Hesheng", ""]]}, {"id": "2107.03640", "submitter": "Ningyuan Xu", "authors": "Ningyuan Xu, Jiayan Zhuang, Yaojun Wu, Jiangjian Xiao", "title": "A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep\n  Learing", "comments": "7pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angular measurements is essential to make a resonable treatment for Hallux\nvalgus (HV), a common forefoot deformity. However, it still depends on manual\nlabeling and measurement, which is time-consuming and sometimes unreliable.\nAutomating this process is a thing of concern. However, it lack of dataset and\nthe keypoints based method which made a great success in pose estimation is not\nsuitable for this field.To solve the problems, we made a dataset and developed\nan algorithm based on deep learning and linear regression. It shows great\nfitting ability to the ground truth.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:11:12 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Xu", "Ningyuan", ""], ["Zhuang", "Jiayan", ""], ["Wu", "Yaojun", ""], ["Xiao", "Jiangjian", ""]]}, {"id": "2107.03642", "submitter": "Ningyuan Xu", "authors": "Ningyuan Xu, Jiayan Zhuang, Jiangjian Xiao and Chengbin Peng", "title": "Regional Differential Information Entropy for Super-Resolution Image\n  Quality Assessment", "comments": "8 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PSNR and SSIM are the most widely used metrics in super-resolution problems,\nbecause they are easy to use and can evaluate the similarities between\ngenerated images and reference images. However, single image super-resolution\nis an ill-posed problem, there are multiple corresponding high-resolution\nimages for the same low-resolution image. The similarities can't totally\nreflect the restoration effect. The perceptual quality of generated images is\nalso important, but PSNR and SSIM do not reflect perceptual quality well. To\nsolve the problem, we proposed a method called regional differential\ninformation entropy to measure both of the similarities and perceptual quality.\nTo overcome the problem that traditional image information entropy can't\nreflect the structure information, we proposed to measure every region's\ninformation entropy with sliding window. Considering that the human visual\nsystem is more sensitive to the brightness difference at low brightness, we\ntake $\\gamma$ quantization rather than linear quantization. To accelerate the\nmethod, we reorganized the calculation procedure of information entropy with a\nneural network. Through experiments on our IQA dataset and PIPAL, this paper\nproves that RDIE can better quantify perceptual quality of images especially\nGAN-based images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:12:55 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Xu", "Ningyuan", ""], ["Zhuang", "Jiayan", ""], ["Xiao", "Jiangjian", ""], ["Peng", "Chengbin", ""]]}, {"id": "2107.03648", "submitter": "Dr. Mohammed Javed", "authors": "Shrikant Temburwar, Bulla Rajesh and Mohammed Javed", "title": "Deep Learning Based Image Retrieval in the JPEG Compressed Domain", "comments": "Accepted in MISP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) systems on pixel domain use low-level\nfeatures, such as colour, texture and shape, to retrieve images. In this\ncontext, two types of image representations i.e. local and global image\nfeatures have been studied in the literature. Extracting these features from\npixel images and comparing them with images from the database is very\ntime-consuming. Therefore, in recent years, there has been some effort to\naccomplish image analysis directly in the compressed domain with lesser\ncomputations. Furthermore, most of the images in our daily transactions are\nstored in the JPEG compressed format. Therefore, it would be ideal if we could\nretrieve features directly from the partially decoded or compressed data and\nuse them for retrieval. Here, we propose a unified model for image retrieval\nwhich takes DCT coefficients as input and efficiently extracts global and local\nfeatures directly in the JPEG compressed domain for accurate image retrieval.\nThe experimental findings indicate that our proposed model performed similarly\nto the current DELG model which takes RGB features as an input with reference\nto mean average precision while having a faster training and retrieval speed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:30:03 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Temburwar", "Shrikant", ""], ["Rajesh", "Bulla", ""], ["Javed", "Mohammed", ""]]}, {"id": "2107.03651", "submitter": "Daniel Bar-David", "authors": "Daniel Bar-David, Laura Bar-David, Yinon Shapira, Rina Leibu, Dalia\n  Dori, Ronit Schneor, Anath Fischer, Shiri Soudry", "title": "Elastic deformation of optical coherence tomography images of diabetic\n  macular edema for deep-learning models training: how far to go?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To explore the clinical validity of elastic deformation of optical coherence\ntomography (OCT) images for data augmentation in the development of\ndeep-learning model for detection of diabetic macular edema (DME).\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:35:34 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:43:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bar-David", "Daniel", ""], ["Bar-David", "Laura", ""], ["Shapira", "Yinon", ""], ["Leibu", "Rina", ""], ["Dori", "Dalia", ""], ["Schneor", "Ronit", ""], ["Fischer", "Anath", ""], ["Soudry", "Shiri", ""]]}, {"id": "2107.03665", "submitter": "Zhaoyi Yan", "authors": "Zhaoyi Yan, Ruimao Zhang, Hongzhi Zhang, Qingfu Zhang, and Wangmeng\n  Zuo", "title": "Crowd Counting via Perspective-Guided Fractional-Dilation Convolution", "comments": "Accepted by T-MM 2021", "journal-ref": "T-MM 2021", "doi": "10.1109/TMM.2021.3086709", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting is critical for numerous video surveillance scenarios. One of\nthe main issues in this task is how to handle the dramatic scale variations of\npedestrians caused by the perspective effect. To address this issue, this paper\nproposes a novel convolution neural network-based crowd counting method, termed\nPerspective-guided Fractional-Dilation Network (PFDNet). By modeling the\ncontinuous scale variations, the proposed PFDNet is able to select the proper\nfractional dilation kernels for adapting to different spatial locations. It\nsignificantly improves the flexibility of the state-of-the-arts that only\nconsider the discrete representative scales. In addition, by avoiding the\nmulti-scale or multi-column architecture that used in other methods, it is\ncomputationally more efficient. In practice, the proposed PFDNet is constructed\nby stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC)\non a VGG16-BN backbone. By introducing a novel generalized dilation convolution\noperation, the PFC can handle fractional dilation ratios in the spatial domain\nunder the guidance of perspective annotations, achieving continuous scales\nmodeling of pedestrians. To deal with the problem of unavailable perspective\ninformation in some cases, we further introduce an effective perspective\nestimation branch to the proposed PFDNet, which can be trained in either\nsupervised or weakly-supervised setting once the branch has been pre-trained.\nExtensive experiments show that the proposed PFDNet outperforms\nstate-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo'10,\nUCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3,\n205.8, and 3.06 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:57:00 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yan", "Zhaoyi", ""], ["Zhang", "Ruimao", ""], ["Zhang", "Hongzhi", ""], ["Zhang", "Qingfu", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2107.03670", "submitter": "Ruian He", "authors": "Ruian He, Zhen Xing, Weimin Tan, Bo Yan", "title": "Feature Pyramid Network for Multi-task Affective Analysis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective Analysis is not a single task, and the valence-arousal value,\nexpression class, and action unit can be predicted at the same time. Previous\nresearches did not pay enough attention to the entanglement and hierarchical\nrelation of these three facial attributes. We propose a novel model named\nfeature pyramid networks for multi-task affect analysis. The hierarchical\nfeatures are extracted to predict three labels and we apply a teacher-student\ntraining strategy to learn from pretrained single-task models. Extensive\nexperiment results demonstrate the proposed model outperforms other models.\nThis is a submission to The 2nd Workshop and Competition on Affective Behavior\nAnalysis in the wild (ABAW). The code and model are available for research\npurposes at https://github.com/ryanhe312/ABAW2-FPNMAA.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 08:10:04 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 11:14:03 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 06:38:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["He", "Ruian", ""], ["Xing", "Zhen", ""], ["Tan", "Weimin", ""], ["Yan", "Bo", ""]]}, {"id": "2107.03688", "submitter": "Longyu Ma", "authors": "Longyu Ma, Chiu-Wing Sham, Chun Yan Lo, and Xinchao Zhong", "title": "An Embedded Iris Recognition System Optimization using Dynamically\n  ReconfigurableDecoder with LDPC Codes", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting and analyzing iris textures for biometric recognition has been\nextensively studied. As the transition of iris recognition from lab technology\nto nation-scale applications, most systems are facing high complexity in either\ntime or space, leading to unfitness for embedded devices. In this paper, the\nproposed design includes a minimal set of computer vision modules and\nmulti-mode QC-LDPC decoder which can alleviate variability and noise caused by\niris acquisition and follow-up process. Several classes of QC-LDPC code from\nIEEE 802.16 are tested for the validity of accuracy improvement. Some of the\ncodes mentioned above are used for further QC-LDPC decoder quantization,\nvalidation and comparison to each other. We show that we can apply Dynamic\nPartial Reconfiguration technology to implement the multi-mode QC-LDPC decoder\nfor the iris recognition system. The results show that the implementation is\npower-efficient and good for edge applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 09:04:11 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ma", "Longyu", ""], ["Sham", "Chiu-Wing", ""], ["Lo", "Chun Yan", ""], ["Zhong", "Xinchao", ""]]}, {"id": "2107.03700", "submitter": "Ayushe Gangal", "authors": "Ayushe Gangal, Peeyush Kumar and Sunita Kumari", "title": "Complete Scanning Application Using OpenCv", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the following paper, we have combined the various basic functionalities\nprovided by the NumPy library and OpenCv library, which is an open source for\nComputer Vision applications, like conversion of colored images to grayscale,\ncalculating threshold, finding contours and using those contour points to take\nperspective transform of the image inputted by the user, using Python version\n3.7. Additional features include cropping, rotating and saving as well. All\nthese functions and features, when implemented step by step, results in a\ncomplete scanning application. The applied procedure involves the following\nsteps: Finding contours, applying Perspective transform and brightening the\nimage, Adaptive Thresholding and applying filters for noise cancellation, and\nRotation features and perspective transform for a special cropping algorithm.\nThe described technique is implemented on various samples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 09:21:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Gangal", "Ayushe", ""], ["Kumar", "Peeyush", ""], ["Kumari", "Sunita", ""]]}, {"id": "2107.03708", "submitter": "Keyu Chen", "authors": "Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng Zhang, Yu Ding", "title": "Prior Aided Streaming Network for Multi-task Affective Recognitionat the\n  2nd ABAW2 Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic affective recognition has been an important research topic in human\ncomputer interaction (HCI) area. With recent development of deep learning\ntechniques and large scale in-the-wild annotated datasets, the facial emotion\nanalysis is now aimed at challenges in the real world settings. In this paper,\nwe introduce our submission to the 2nd Affective Behavior Analysis in-the-wild\n(ABAW2) Competition. In dealing with different emotion representations,\nincluding Categorical Emotions (CE), Action Units (AU), and Valence Arousal\n(VA), we propose a multi-task streaming network by a heuristic that the three\nrepresentations are intrinsically associated with each other. Besides, we\nleverage an advanced facial expression embedding as prior knowledge, which is\ncapable of capturing identity-invariant expression features while preserving\nthe expression similarities, to aid the down-streaming recognition tasks. The\nextensive quantitative evaluations as well as ablation studies on the Aff-Wild2\ndataset prove the effectiveness of our proposed prior aided streaming network\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 09:35:08 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhang", "Wei", ""], ["Guo", "Zunhu", ""], ["Chen", "Keyu", ""], ["Li", "Lincheng", ""], ["Zhang", "Zhimeng", ""], ["Ding", "Yu", ""]]}, {"id": "2107.03742", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, G\\\"okhan Yildirim, Christian Bracher, Roland Vollgraf", "title": "Grid Partitioned Attention: Efficient TransformerApproximation with\n  Inductive Bias for High Resolution Detail Generation", "comments": "code available at https://github.com/zalandoresearch/gpa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 10:37:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Yildirim", "G\u00f6khan", ""], ["Bracher", "Christian", ""], ["Vollgraf", "Roland", ""]]}, {"id": "2107.03751", "submitter": "Luis Lucas", "authors": "Luis Lucas and David Tomas and Jose Garcia-Rodriguez", "title": "Exploiting the relationship between visual and textual features in\n  social networks for image classification with zero-shot deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main issues related to unsupervised machine learning is the cost\nof processing and extracting useful information from large datasets. In this\nwork, we propose a classifier ensemble based on the transferable learning\ncapabilities of the CLIP neural network architecture in multimodal environments\n(image and text) from social media. For this purpose, we used the InstaNY100K\ndataset and proposed a validation approach based on sampling techniques. Our\nexperiments, based on image classification tasks according to the labels of the\nPlaces dataset, are performed by first considering only the visual part, and\nthen adding the associated texts as support. The results obtained demonstrated\nthat trained neural networks such as CLIP can be successfully applied to image\nclassification with little fine-tuning, and considering the associated texts to\nthe images can help to improve the accuracy depending on the goal. The results\ndemonstrated what seems to be a promising research direction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 10:54:59 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lucas", "Luis", ""], ["Tomas", "David", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "2107.03758", "submitter": "Lei Liu", "authors": "Lei Liu and Li Liu", "title": "Investigate the Essence of Long-Tailed Recognition from a Unified\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the data scale grows, deep recognition models often suffer from\nlong-tailed data distributions due to the heavy imbalanced sample number across\ncategories. Indeed, real-world data usually exhibit some similarity relation\namong different categories (e.g., pigeons and sparrows), called category\nsimilarity in this work. It is doubly difficult when the imbalance occurs\nbetween such categories with similar appearances. However, existing solutions\nmainly focus on the sample number to re-balance data distribution. In this\nwork, we systematically investigate the essence of the long-tailed problem from\na unified perspective. Specifically, we demonstrate that long-tailed\nrecognition suffers from both sample number and category similarity.\nIntuitively, using a toy example, we first show that sample number is not the\nunique influence factor for performance dropping of long-tailed recognition.\nTheoretically, we demonstrate that (1) category similarity, as an inevitable\nfactor, would also influence the model learning under long-tailed distribution\nvia similar samples, (2) using more discriminative representation methods\n(e.g., self-supervised learning) for similarity reduction, the classifier bias\ncan be further alleviated with greatly improved performance. Extensive\nexperiments on several long-tailed datasets verify the rationality of our\ntheoretical analysis, and show that based on existing state-of-the-arts\n(SOTAs), the performance could be further improved by similarity reduction. Our\ninvestigations highlight the essence behind the long-tailed problem, and claim\nseveral feasible directions for future work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:08:40 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Liu", "Lei", ""], ["Liu", "Li", ""]]}, {"id": "2107.03769", "submitter": "Martin Knoche", "authors": "Martin Knoche, Stefan H\\\"ormann, Gerhard Rigoll", "title": "Image Resolution Susceptibility of Face Recognition Models", "comments": "19 pages, 15 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition approaches often rely on equal image resolution for\nverification faces on two images. However, in practical applications, those\nimage resolutions are usually not in the same range due to different image\ncapture mechanisms or sources. In this work, we first analyze the impact of\nimage resolutions on the face verification performance with a state-of-the-art\nface recognition model. For images, synthetically reduced to $5\\, \\times 5\\,\n\\mathrm{px}$ resolution, the verification performance drops from $99.23\\%$\nincreasingly down to almost $55\\%$. Especially, for cross-resolution image\npairs (one high- and one low-resolution image), the verification accuracy\ndecreases even further. We investigate this behavior more in-depth by looking\nat the feature distances for every 2-image test pair. To tackle this problem,\nwe propose the following two methods: 1) Train a state-of-the-art\nface-recognition model straightforward with $50\\%$ low-resolution images\ndirectly within each batch. \\\\ 2) Train a siamese-network structure and adding\na cosine distance feature loss between high- and low-resolution features. Both\nmethods show an improvement for cross-resolution scenarios and can increase the\naccuracy at very low resolution to approximately $70\\%$. However, a\ndisadvantage is that a specific model needs to be trained for every\nresolution-pair ...\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:30:27 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Knoche", "Martin", ""], ["H\u00f6rmann", "Stefan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2107.03774", "submitter": "Jos\\'e Cano", "authors": "Martina Lofqvist, Jos\\'e Cano", "title": "Optimizing Data Processing in Space for Object Detection in Satellite\n  Imagery", "comments": "Published as a workshop paper at SmallSat 2021 - The 35th Annual\n  Small Satellite Conference. 9 pages, 10 figures. arXiv admin note: text\n  overlap with arXiv:2007.11089", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:37:24 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lofqvist", "Martina", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2107.03783", "submitter": "Berkay Kopru", "authors": "Berkay K\\\"opr\\\"u, Engin Erzin", "title": "Use of Affective Visual Information for Summarization of Human-Centric\n  Videos", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing volume of user-generated human-centric video content and their\napplications, such as video retrieval and browsing, require compact\nrepresentations that are addressed by the video summarization literature.\nCurrent supervised studies formulate video summarization as a\nsequence-to-sequence learning problem and the existing solutions often neglect\nthe surge of human-centric view, which inherently contains affective content.\nIn this study, we investigate the affective-information enriched supervised\nvideo summarization task for human-centric videos. First, we train a visual\ninput-driven state-of-the-art continuous emotion recognition model (CER-NET) on\nthe RECOLA dataset to estimate emotional attributes. Then, we integrate the\nestimated emotional attributes and the high-level representations from the\nCER-NET with the visual information to define the proposed affective video\nsummarization architectures (AVSUM). In addition, we investigate the use of\nattention to improve the AVSUM architectures and propose two new architectures\nbased on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We\nconduct video summarization experiments on the TvSum database. The proposed\nAVSUM-GRU architecture with an early fusion of high level GRU embeddings and\nthe temporal attention based TA-AVSUM architecture attain competitive video\nsummarization performances by bringing strong performance improvements for the\nhuman-centric videos compared to the state-of-the-art in terms of F-score and\nself-defined face recall metrics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:46:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["K\u00f6pr\u00fc", "Berkay", ""], ["Erzin", "Engin", ""]]}, {"id": "2107.03815", "submitter": "Zhao Zhong", "authors": "Yikang Zhang, Zhuo Chen, Zhao Zhong", "title": "Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with\n  100M FLOPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:44:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhang", "Yikang", ""], ["Chen", "Zhuo", ""], ["Zhong", "Zhao", ""]]}, {"id": "2107.03824", "submitter": "Nian Liu", "authors": "Nian Liu, Long Li, Wangbo Zhao, Junwei Han, Ling Shao", "title": "Instance-Level Relative Saliency Ranking with Graph Reasoning", "comments": "TPAMI under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional salient object detection models cannot differentiate the\nimportance of different salient objects. Recently, two works have been proposed\nto detect saliency ranking by assigning different degrees of saliency to\ndifferent objects. However, one of these models cannot differentiate object\ninstances and the other focuses more on sequential attention shift order\ninference. In this paper, we investigate a practical problem setting that\nrequires simultaneously segment salient instances and infer their relative\nsaliency rank order. We present a novel unified model as the first end-to-end\nsolution, where an improved Mask R-CNN is first used to segment salient\ninstances and a saliency ranking branch is then added to infer the relative\nsaliency. For relative saliency ranking, we build a new graph reasoning module\nby combining four graphs to incorporate the instance interaction relation,\nlocal contrast, global contrast, and a high-level semantic prior, respectively.\nA novel loss function is also proposed to effectively train the saliency\nranking branch. Besides, a new dataset and an evaluation metric are proposed\nfor this task, aiming at pushing forward this field of research. Finally,\nexperimental results demonstrate that our proposed model is more effective than\nprevious methods. We also show an example of its practical usage on adaptive\nimage retargeting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:10:42 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Liu", "Nian", ""], ["Li", "Long", ""], ["Zhao", "Wangbo", ""], ["Han", "Junwei", ""], ["Shao", "Ling", ""]]}, {"id": "2107.03846", "submitter": "Lucas Fidon", "authors": "Lucas Fidon, Michael Aertsen, Doaa Emam, Nada Mufti, Fr\\'ed\\'eric\n  Guffens, Thomas Deprest, Philippe Demaerel, Anna L. David, Andrew Melbourne,\n  S\\'ebastien Ourselin, Jan Deprest, Tom Vercauteren", "title": "Label-set Loss Functions for Partial Supervision: Application to Fetal\n  Brain 3D MRI Parcellation", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have increased the accuracy of automatic segmentation,\nhowever, their accuracy depends on the availability of a large number of fully\nsegmented images. Methods to train deep neural networks using images for which\nsome, but not all, regions of interest are segmented are necessary to make\nbetter use of partially annotated datasets. In this paper, we propose the first\naxiomatic definition of label-set loss functions that are the loss functions\nthat can handle partially segmented images. We prove that there is one and only\none method to convert a classical loss function for fully segmented images into\na proper label-set loss function. Our theory also allows us to define the\nleaf-Dice loss, a label-set generalization of the Dice loss particularly suited\nfor partial supervision with only missing labels. Using the leaf-Dice loss, we\nset a new state of the art in partially supervised learning for fetal brain 3D\nMRI segmentation. We achieve a deep neural network able to segment white\nmatter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,\ndeep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of\nanatomically normal fetuses or with open spina bifida. Our implementation of\nthe proposed label-set loss functions is available at\nhttps://github.com/LucasFidon/label-set-loss-functions\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:53:56 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 15:44:06 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Fidon", "Lucas", ""], ["Aertsen", "Michael", ""], ["Emam", "Doaa", ""], ["Mufti", "Nada", ""], ["Guffens", "Fr\u00e9d\u00e9ric", ""], ["Deprest", "Thomas", ""], ["Demaerel", "Philippe", ""], ["David", "Anna L.", ""], ["Melbourne", "Andrew", ""], ["Ourselin", "S\u00e9bastien", ""], ["Deprest", "Jan", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2107.03886", "submitter": "Geesung Oh", "authors": "Geesung Oh, Euiseok Jeong and Sejoon Lim", "title": "Causal affect prediction model using a facial image sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among human affective behavior research, facial expression recognition\nresearch is improving in performance along with the development of deep\nlearning. However, for improved performance, not only past images but also\nfuture images should be used along with corresponding facial images, but there\nare obstacles to the application of this technique to real-time environments.\nIn this paper, we propose the causal affect prediction network (CAPNet), which\nuses only past facial images to predict corresponding affective valence and\narousal. We train CAPNet to learn causal inference between past images and\ncorresponding affective valence and arousal through supervised learning by\npairing the sequence of past images with the current label using the Aff-Wild2\ndataset. We show through experiments that the well-trained CAPNet outperforms\nthe baseline of the second challenge of the Affective Behavior Analysis\nin-the-wild (ABAW2) Competition by predicting affective valence and arousal\nonly with past facial images one-third of a second earlier. Therefore, in\nreal-time application, CAPNet can reliably predict affective valence and\narousal only with past data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:13:50 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Oh", "Geesung", ""], ["Jeong", "Euiseok", ""], ["Lim", "Sejoon", ""]]}, {"id": "2107.03887", "submitter": "Shuo Wang", "authors": "Shuo Wang, Chen Qin, Nicolo Savioli, Chen Chen, Declan O'Regan, Stuart\n  Cook, Yike Guo, Daniel Rueckert and Wenjia Bai", "title": "Joint Motion Correction and Super Resolution for Cardiac Segmentation\n  via Latent Optimisation", "comments": "The paper is early accepted to MICCAI 2021. The codes are available\n  at https://github.com/shuowang26/SRHeart", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cardiac magnetic resonance (CMR) imaging, a 3D high-resolution\nsegmentation of the heart is essential for detailed description of its\nanatomical structures. However, due to the limit of acquisition duration and\nrespiratory/cardiac motion, stacks of multi-slice 2D images are acquired in\nclinical routine. The segmentation of these images provides a low-resolution\nrepresentation of cardiac anatomy, which may contain artefacts caused by\nmotion. Here we propose a novel latent optimisation framework that jointly\nperforms motion correction and super resolution for cardiac image\nsegmentations. Given a low-resolution segmentation as input, the framework\naccounts for inter-slice motion in cardiac MR imaging and super-resolves the\ninput into a high-resolution segmentation consistent with input. A multi-view\nloss is incorporated to leverage information from both short-axis view and\nlong-axis view of cardiac imaging. To solve the inverse problem, iterative\noptimisation is performed in a latent space, which ensures the anatomical\nplausibility. This alleviates the need of paired low-resolution and\nhigh-resolution images for supervised learning. Experiments on two cardiac MR\ndatasets show that the proposed framework achieves high performance, comparable\nto state-of-the-art super-resolution approaches and with better cross-domain\ngeneralisability and anatomical plausibility.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:14:00 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Shuo", ""], ["Qin", "Chen", ""], ["Savioli", "Nicolo", ""], ["Chen", "Chen", ""], ["O'Regan", "Declan", ""], ["Cook", "Stuart", ""], ["Guo", "Yike", ""], ["Rueckert", "Daniel", ""], ["Bai", "Wenjia", ""]]}, {"id": "2107.03890", "submitter": "Alexander Vakhitov", "authors": "Alexander Vakhitov, Luis Ferraz Colomina, Antonio Agudo, Francesc\n  Moreno-Noguer", "title": "Uncertainty-Aware Camera Pose Estimation from Points and Lines", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perspective-n-Point-and-Line (P$n$PL) algorithms aim at fast, accurate, and\nrobust camera localization with respect to a 3D model from 2D-3D feature\ncorrespondences, being a major part of modern robotic and AR/VR systems.\nCurrent point-based pose estimation methods use only 2D feature detection\nuncertainties, and the line-based methods do not take uncertainties into\naccount. In our setup, both 3D coordinates and 2D projections of the features\nare considered uncertain. We propose PnP(L) solvers based on EPnP and DLS for\nthe uncertainty-aware pose estimation. We also modify motion-only bundle\nadjustment to take 3D uncertainties into account. We perform exhaustive\nsynthetic and real experiments on two different visual odometry datasets. The\nnew PnP(L) methods outperform the state-of-the-art on real data in isolation,\nshowing an increase in mean translation accuracy by 18% on a representative\nsubset of KITTI, while the new uncertain refinement improves pose accuracy for\nmost of the solvers, e.g. decreasing mean translation error for the EPnP by 16%\ncompared to the standard refinement on the same dataset. The code is available\nat https://alexandervakhitov.github.io/uncertain-pnp/.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:19:36 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Vakhitov", "Alexander", ""], ["Colomina", "Luis Ferraz", ""], ["Agudo", "Antonio", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2107.03891", "submitter": "I-Hsuan Li", "authors": "Hong-Xia Xie, I-Hsuan Li, Ling Lo, Hong-Han Shuai, and Wen-Huang Cheng", "title": "Technical Report for Valence-Arousal Estimation in ABAW2 Challenge", "comments": "arXiv admin note: substantial text overlap with arXiv:2105.01502", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe our method for tackling the valence-arousal\nestimation challenge from ABAW2 ICCV-2021 Competition. The competition\norganizers provide an in-the-wild Aff-Wild2 dataset for participants to analyze\naffective behavior in real-life settings. We use a two stream model to learn\nemotion features from appearance and action respectively. To solve data\nimbalanced problem, we apply label distribution smoothing (LDS) to re-weight\nlabels. Our proposed method achieves Concordance Correlation Coefficient (CCC)\nof 0.591 and 0.617 for valence and arousal on the validation set of Aff-wild2\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:21:38 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Xie", "Hong-Xia", ""], ["Li", "I-Hsuan", ""], ["Lo", "Ling", ""], ["Shuai", "Hong-Han", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2107.03904", "submitter": "Shuang Liang", "authors": "Shuang Liang", "title": "A hybrid deep learning framework for Covid-19 detection via 3D Chest CT\n  Images", "comments": "5 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a hybrid deep learning framework named CTNet which\ncombines convolutional neural network and transformer together for the\ndetection of COVID-19 via 3D chest CT images. It consists of a CNN feature\nextractor module with SE attention to extract sufficient features from CT\nscans, together with a transformer model to model the discriminative features\nof the 3D CT scans. Compared to previous works, CTNet provides an effective and\nefficient method to perform COVID-19 diagnosis via 3D CT scans with data\nresampling strategy. Advanced results on a large and public benchmarks,\nCOV19-CT-DB database was achieved by the proposed CTNet, over the\nstate-of-the-art baseline approachproposed together with the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:37:46 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 11:16:14 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liang", "Shuang", ""]]}, {"id": "2107.03909", "submitter": "Robin Dupont", "authors": "Robin Dupont, Hichem Sahbi, Guillaume Michel", "title": "Weight Reparametrization for Budget-Aware Network Pruning", "comments": "Accepted at International Conference on Image Processing (ICIP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning seeks to design lightweight architectures by removing redundant\nweights in overparameterized networks. Most of the existing techniques first\nremove structured sub-networks (filters, channels,...) and then fine-tune the\nresulting networks to maintain a high accuracy. However, removing a whole\nstructure is a strong topological prior and recovering the accuracy, with\nfine-tuning, is highly cumbersome. In this paper, we introduce an \"end-to-end\"\nlightweight network design that achieves training and pruning simultaneously\nwithout fine-tuning. The design principle of our method relies on\nreparametrization that learns not only the weights but also the topological\nstructure of the lightweight sub-network. This reparametrization acts as a\nprior (or regularizer) that defines pruning masks implicitly from the weights\nof the underlying network, without increasing the number of training\nparameters. Sparsity is induced with a budget loss that provides an accurate\npruning. Extensive experiments conducted on the CIFAR10 and the TinyImageNet\ndatasets, using standard architectures (namely Conv4, VGG19 and ResNet18), show\ncompelling results without fine-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:40:16 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 14:15:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Dupont", "Robin", ""], ["Sahbi", "Hichem", ""], ["Michel", "Guillaume", ""]]}, {"id": "2107.03949", "submitter": "Patrick Godau", "authors": "Patrick Godau and Lena Maier-Hein", "title": "Task Fingerprinting for Meta Learning in Biomedical Image Analysis", "comments": "Medical Image Computing and Computer Assisted Interventions (MICCAI)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortage of annotated data is one of the greatest bottlenecks in biomedical\nimage analysis. Meta learning studies how learning systems can increase in\nefficiency through experience and could thus evolve as an important concept to\novercome data sparsity. However, the core capability of meta learning-based\napproaches is the identification of similar previous tasks given a new task - a\nchallenge largely unexplored in the biomedical imaging domain. In this paper,\nwe address the problem of quantifying task similarity with a concept that we\nrefer to as task fingerprinting. The concept involves converting a given task,\nrepresented by imaging data and corresponding labels, to a fixed-length vector\nrepresentation. In fingerprint space, different tasks can be directly compared\nirrespective of their data set sizes, types of labels or specific resolutions.\nAn initial feasibility study in the field of surgical data science (SDS) with\n26 classification tasks from various medical and non-medical domains suggests\nthat task fingerprinting could be leveraged for both (1) selecting appropriate\ndata sets for pretraining and (2) selecting appropriate architectures for a new\ntask. Task fingerprinting could thus become an important tool for meta learning\nin SDS and other fields of biomedical image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:20:28 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Godau", "Patrick", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2107.03964", "submitter": "Sibendu Paul", "authors": "Sibendu Paul, Kunal Rao, Giuseppe Coviello, Murugan Sankaradas, Oliver\n  Po, Y. Charlie Hu, Srimat T. Chakradhar", "title": "CamTuner: Reinforcement-Learning based System for Camera Parameter\n  Tuning to enhance Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex sensors like video cameras include tens of configurable parameters,\nwhich can be set by end-users to customize the sensors to specific application\nscenarios. Although parameter settings significantly affect the quality of the\nsensor output and the accuracy of insights derived from sensor data, most\nend-users use a fixed parameter setting because they lack the skill or\nunderstanding to appropriately configure these parameters. We propose CamTuner,\nwhich is a system to automatically, and dynamically adapt the complex sensor to\nchanging environments. CamTuner includes two key components. First, a bespoke\nanalytics quality estimator, which is a deep-learning model to automatically\nand continuously estimate the quality of insights from an analytics unit as the\nenvironment around a sensor change. Second, a reinforcement learning (RL)\nmodule, which reacts to the changes in quality, and automatically adjusts the\ncamera parameters to enhance the accuracy of insights. We improve the training\ntime of the RL module by an order of magnitude by designing virtual models to\nmimic essential behavior of the camera: we design virtual knobs that can be set\nto different values to mimic the effects of assigning different values to the\ncamera's configurable parameters, and we design a virtual camera model that\nmimics the output from a video camera at different times of the day. These\nvirtual models significantly accelerate training because (a) frame rates from a\nreal camera are limited to 25-30 fps while the virtual models enable processing\nat 300 fps, (b) we do not have to wait until the real camera sees different\nenvironments, which could take weeks or months, and (c) virtual knobs can be\nupdated instantly, while it can take 200-500 ms to change the camera parameter\nsettings. Our dynamic tuning approach results in up to 12% improvement in the\naccuracy of insights from several video analytics tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:43:02 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Paul", "Sibendu", ""], ["Rao", "Kunal", ""], ["Coviello", "Giuseppe", ""], ["Sankaradas", "Murugan", ""], ["Po", "Oliver", ""], ["Hu", "Y. Charlie", ""], ["Chakradhar", "Srimat T.", ""]]}, {"id": "2107.03983", "submitter": "Subhranil Bagchi", "authors": "Subhranil Bagchi and Deepti R. Bathula", "title": "EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli\n  Classification", "comments": "Preprint and Supplementary material. 17 pages, 13 figures and 4\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different categories of visual stimuli activate different responses in the\nhuman brain. These signals can be captured with EEG for utilization in\napplications such as Brain-Computer Interface (BCI). However, accurate\nclassification of single-trial data is challenging due to low signal-to-noise\nratio of EEG. This work introduces an EEG-ConvTranformer network that is based\non multi-headed self-attention. Unlike other transformers, the model\nincorporates self-attention to capture inter-region interactions. It further\nextends to adjunct convolutional filters with multi-head attention as a single\nmodule to learn temporal patterns. Experimental results demonstrate that\nEEG-ConvTransformer achieves improved classification accuracy over the\nstate-of-the-art techniques across five different visual stimuli classification\ntasks. Finally, quantitative analysis of inter-head diversity also shows low\nsimilarity in representational subspaces, emphasizing the implicit diversity of\nmulti-head attention.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:22:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bagchi", "Subhranil", ""], ["Bathula", "Deepti R.", ""]]}, {"id": "2107.03987", "submitter": "Jianing Wang", "authors": "Jianing Wang, Dingjie Su, Yubo Fan, Srijata Chakravorti, Jack H.\n  Noble, and Benoit M. Dawant", "title": "Atlas-Based Segmentation of Intracochlear Anatomy in Metal Artifact\n  Affected CT Images of the Ear with Co-trained Deep Neural Networks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an atlas-based method to segment the intracochlear anatomy (ICA)\nin the post-implantation CT (Post-CT) images of cochlear implant (CI)\nrecipients that preserves the point-to-point correspondence between the meshes\nin the atlas and the segmented volumes. To solve this problem, which is\nchallenging because of the strong artifacts produced by the implant, we use a\npair of co-trained deep networks that generate dense deformation fields (DDFs)\nin opposite directions. One network is tasked with registering an atlas image\nto the Post-CT images and the other network is tasked with registering the\nPost-CT images to the atlas image. The networks are trained using loss\nfunctions based on voxel-wise labels, image content, fiducial registration\nerror, and cycle-consistency constraint. The segmentation of the ICA in the\nPost-CT images is subsequently obtained by transferring the predefined\nsegmentation meshes of the ICA in the atlas image to the Post-CT images using\nthe corresponding DDFs generated by the trained registration networks. Our\nmodel can learn the underlying geometric features of the ICA even though they\nare obscured by the metal artifacts. We show that our end-to-end network\nproduces results that are comparable to the current state of the art (SOTA)\nthat relies on a two-steps approach that first uses conditional generative\nadversarial networks to synthesize artifact-free images from the Post-CT images\nand then uses an active shape model-based method to segment the ICA in the\nsynthetic images. Our method requires a fraction of the time needed by the\nSOTA, which is important for end-user acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:26:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 15:00:52 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Jianing", ""], ["Su", "Dingjie", ""], ["Fan", "Yubo", ""], ["Chakravorti", "Srijata", ""], ["Noble", "Jack H.", ""], ["Dawant", "Benoit M.", ""]]}, {"id": "2107.03996", "submitter": "Ruihan Yang", "authors": "Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, Xiaolong Wang", "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with\n  Cross-Modal Transformers", "comments": "Our project page with videos is at\n  https://RchalYang.github.io/LocoTransformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to address quadrupedal locomotion tasks using Reinforcement\nLearning (RL) with a Transformer-based model that learns to combine\nproprioceptive information and high-dimensional depth sensor inputs. While\nlearning-based locomotion has made great advances using RL, most methods still\nrely on domain randomization for training blind agents that generalize to\nchallenging terrains. Our key insight is that proprioceptive states only offer\ncontact measurements for immediate reaction, whereas an agent equipped with\nvisual sensory observations can learn to proactively maneuver environments with\nobstacles and uneven terrain by anticipating changes in the environment many\nsteps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL\nmethod for quadrupedal locomotion that leverages a Transformer-based model for\nfusing proprioceptive states and visual observations. We evaluate our method in\nchallenging simulated environments with different obstacles and uneven terrain.\nWe show that our method obtains significant improvements over policies with\nonly proprioceptive state inputs, and that Transformer-based models further\nimprove generalization across environments. Our project page with videos is at\nhttps://RchalYang.github.io/LocoTransformer .\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:41:55 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yang", "Ruihan", ""], ["Zhang", "Minghao", ""], ["Hansen", "Nicklas", ""], ["Xu", "Huazhe", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2107.04000", "submitter": "Siddharth Ancha", "authors": "Siddharth Ancha, Gaurav Pathak, Srinivasa G. Narasimhan, David Held", "title": "Active Safety Envelopes using Light Curtains with Probabilistic\n  Guarantees", "comments": "18 pages, Published at Robotics: Science and Systems (RSS) 2021", "journal-ref": null, "doi": "10.15607/rss.2021.xvii.045", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To safely navigate unknown environments, robots must accurately perceive\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\nsensor, we explore the use of a much cheaper and higher resolution sensor:\nprogrammable light curtains. Light curtains are controllable depth sensors that\nsense only along a surface that a user selects. We use light curtains to\nestimate the safety envelope of a scene: a hypothetical surface that separates\nthe robot from all obstacles. We show that generating light curtains that sense\nrandom locations (from a particular distribution) can quickly discover the\nsafety envelope for scenes with unknown objects. Importantly, we produce\ntheoretical safety guarantees on the probability of detecting an obstacle using\nrandom curtains. We combine random curtains with a machine learning based model\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\nmethod accurately estimates safety envelopes while providing probabilistic\nsafety guarantees that can be used to certify the efficacy of a robot\nperception system to detect and avoid dynamic obstacles. We evaluate our\napproach in a simulated urban driving environment and a real-world environment\nwith moving pedestrians using a light curtain device and show that we can\nestimate safety envelopes efficiently and effectively. Project website:\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:46:05 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ancha", "Siddharth", ""], ["Pathak", "Gaurav", ""], ["Narasimhan", "Srinivasa G.", ""], ["Held", "David", ""]]}, {"id": "2107.04004", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio\n  Torralba", "title": "3D Neural Scene Representations for Visuomotor Control", "comments": "First two authors contributed equally. Project Page:\n  https://3d-representation-learning.github.io/nerf-dy/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have a strong intuitive understanding of the 3D environment around us.\nThe mental model of the physics in our brain applies to objects of different\nmaterials and enables us to perform a wide range of manipulation tasks that are\nfar beyond the reach of current robots. In this work, we desire to learn models\nfor dynamic 3D scenes purely from 2D visual observations. Our model combines\nNeural Radiance Fields (NeRF) and time contrastive learning with an\nautoencoding framework, which learns viewpoint-invariant 3D-aware scene\nrepresentations. We show that a dynamics model, constructed over the learned\nrepresentation space, enables visuomotor control for challenging manipulation\ntasks involving both rigid bodies and fluids, where the target is specified in\na viewpoint different from what the robot operates on. When coupled with an\nauto-decoding framework, it can even support goal specification from camera\nviewpoints that are outside the training distribution. We further demonstrate\nthe richness of the learned 3D dynamics model by performing future prediction\nand novel view synthesis. Finally, we provide detailed ablation studies\nregarding different system designs and qualitative analysis of the learned\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:49:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Li", "Yunzhu", ""], ["Li", "Shuang", ""], ["Sitzmann", "Vincent", ""], ["Agrawal", "Pulkit", ""], ["Torralba", "Antonio", ""]]}, {"id": "2107.04008", "submitter": "Asifullah Khan", "authors": "Muhammad Asam, Saddam Hussain Khan, Tauseef Jamal, Umme Zahoora,\n  Asifullah Khan", "title": "Malware Classification Using Deep Boosted Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious activities in cyberspace have gone further than simply hacking\nmachines and spreading viruses. It has become a challenge for a nations\nsurvival and hence has evolved to cyber warfare. Malware is a key component of\ncyber-crime, and its analysis is the first line of defence against attack. This\nwork proposes a novel deep boosted hybrid learning-based malware classification\nframework and named as Deep boosted Feature Space-based Malware classification\n(DFS-MC). In the proposed framework, the discrimination power is enhanced by\nfusing the feature spaces of the best performing customized CNN architectures\nmodels and its discrimination by an SVM for classification. The discrimination\ncapacity of the proposed classification framework is assessed by comparing it\nagainst the standard customized CNNs. The customized CNN models are implemented\nin two ways: softmax classifier and deep hybrid learning-based malware\nclassification. In the hybrid learning, Deep features are extracted from\ncustomized CNN architectures and fed into the conventional machine learning\nclassifier to improve the classification performance. We also introduced the\nconcept of transfer learning in a customized CNN architecture based malware\nclassification framework through fine-tuning. The performance of the proposed\nmalware classification approaches are validated on the MalImg malware dataset\nusing the hold-out cross-validation technique. Experimental comparisons were\nconducted by employing innovative, customized CNN, trained from scratch and\nfine-tuning the customized CNN using transfer learning. The proposed\nclassification framework DFS-MC showed improved results, Accuracy: 98.61%,\nF-score: 0.96, Precision: 0.96, and Recall: 0.96.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:53:33 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Asam", "Muhammad", ""], ["Khan", "Saddam Hussain", ""], ["Jamal", "Tauseef", ""], ["Zahoora", "Umme", ""], ["Khan", "Asifullah", ""]]}, {"id": "2107.04013", "submitter": "Jinhyung Park", "authors": "Jinhyung Park, Xinshuo Weng, Yunze Man, Kris Kitani", "title": "Multi-Modality Task Cascade for 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds and RGB images are naturally complementary modalities for 3D\nvisual understanding - the former provides sparse but accurate locations of\npoints on objects, while the latter contains dense color and texture\ninformation. Despite this potential for close sensor fusion, many methods train\ntwo models in isolation and use simple feature concatenation to represent 3D\nsensor data. This separated training scheme results in potentially sub-optimal\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\noften useful on their own. To provide a more integrated approach, we propose a\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\nproposals to improve 2D segmentation predictions, which are then used to\nfurther refine the 3D boxes. We show that including a 2D network between two\nstages of 3D modules significantly improves both 2D and 3D task performance.\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\npredictions, we propose a dual-head 2D segmentation training and inference\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\ndataset, we improve upon state-of-the-art results of both single modality and\nfusion networks by a large margin ($\\textbf{+3.8}$ mAP@0.5). Code will be\nreleased $\\href{https://github.com/Divadi/MTC_RCNN}{\\text{here.}}$\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:55:01 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Park", "Jinhyung", ""], ["Weng", "Xinshuo", ""], ["Man", "Yunze", ""], ["Kitani", "Kris", ""]]}, {"id": "2107.04020", "submitter": "Xuejing Lei", "authors": "Xuejing Lei, Ganning Zhao, Kaitai Zhang, C.-C. Jay Kuo", "title": "TGHop: An Explainable, Efficient and Lightweight Method for Texture\n  Generation", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.01376", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explainable, efficient and lightweight method for texture generation,\ncalled TGHop (an acronym of Texture Generation PixelHop), is proposed in this\nwork. Although synthesis of visually pleasant texture can be achieved by deep\nneural networks, the associated models are large in size, difficult to explain\nin theory, and computationally expensive in training. In contrast, TGHop is\nsmall in its model size, mathematically transparent, efficient in training and\ninference, and able to generate high quality texture. Given an exemplary\ntexture, TGHop first crops many sample patches out of it to form a collection\nof sample patches called the source. Then, it analyzes pixel statistics of\nsamples from the source and obtains a sequence of fine-to-coarse subspaces for\nthese patches by using the PixelHop++ framework. To generate texture patches\nwith TGHop, we begin with the coarsest subspace, which is called the core, and\nattempt to generate samples in each subspace by following the distribution of\nreal samples. Finally, texture patches are stitched to form texture images of a\nlarge size. It is demonstrated by experimental results that TGHop can generate\ntexture images of superior quality with a small model size and at a fast speed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:56:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lei", "Xuejing", ""], ["Zhao", "Ganning", ""], ["Zhang", "Kaitai", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2107.04032", "submitter": "Vladislav Golyanik", "authors": "Marcel Seelbach Benkner and Vladislav Golyanik and Christian Theobalt\n  and Michael Moeller", "title": "Adiabatic Quantum Graph Matching with Permutation Matrix Constraints", "comments": "18 pages, 14 figures, 2 tables; project webpage:\n  http://gvv.mpi-inf.mpg.de/projects/QGM/", "journal-ref": "Published at 3DV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching problems on 3D shapes and images are challenging as they are\nfrequently formulated as combinatorial quadratic assignment problems (QAPs)\nwith permutation matrix constraints, which are NP-hard. In this work, we\naddress such problems with emerging quantum computing technology and propose\nseveral reformulations of QAPs as unconstrained problems suitable for efficient\nexecution on quantum hardware. We investigate several ways to inject\npermutation matrix constraints in a quadratic unconstrained binary optimization\nproblem which can be mapped to quantum hardware. We focus on obtaining a\nsufficient spectral gap, which further increases the probability to measure\noptimal solutions and valid permutation matrices in a single run. We perform\nour experiments on the quantum computer D-Wave 2000Q (2^11 qubits, adiabatic).\nDespite the observed discrepancy between simulated adiabatic quantum computing\nand execution on real quantum hardware, our reformulation of permutation matrix\nconstraints increases the robustness of the numerical computations over other\npenalty approaches in our experiments. The proposed algorithm has the potential\nto scale to higher dimensions on future quantum computing architectures, which\nopens up multiple new directions for solving matching problems in 3D computer\nvision and graphics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:59:55 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Benkner", "Marcel Seelbach", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""], ["Moeller", "Michael", ""]]}, {"id": "2107.04034", "submitter": "Deepak Pathak", "authors": "Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik", "title": "RMA: Rapid Motor Adaptation for Legged Robots", "comments": "RSS 2021. Webpage at https://ashish-kmr.github.io/rma-legged-robots/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful real-world deployment of legged robots would require them to adapt\nin real-time to unseen scenarios like changing terrains, changing payloads,\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped robots. RMA\nconsists of two components: a base policy and an adaptation module. The\ncombination of these components enables the robot to adapt to novel situations\nin fractions of a second. RMA is trained completely in simulation without using\nany domain knowledge like reference trajectories or predefined foot trajectory\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\ndeploy it on a variety of difficult terrains including rocky, slippery,\ndeformable surfaces in environments with grass, long vegetation, concrete,\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulation experiments. Video results at\nhttps://ashish-kmr.github.io/rma-legged-robots/\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:59:59 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Kumar", "Ashish", ""], ["Fu", "Zipeng", ""], ["Pathak", "Deepak", ""], ["Malik", "Jitendra", ""]]}, {"id": "2107.04055", "submitter": "Haibo Qi", "authors": "Haibo Qi, Yuhan Wang, Xinyu Liu", "title": "3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a 3D-RegNet-based neural network is proposed for diagnosing\nthe physical condition of patients with coronavirus (Covid-19) infection. In\nthe application of clinical medicine, lung CT images are utilized by\npractitioners to determine whether a patient is infected with coronavirus.\nHowever, there are some laybacks can be considered regarding to this diagnostic\nmethod, such as time consuming and low accuracy. As a relatively large organ of\nhuman body, important spatial features would be lost if the lungs were\ndiagnosed utilizing two dimensional slice image. Therefore, in this paper, a\ndeep learning model with 3D image was designed. The 3D image as input data was\ncomprised of two-dimensional pulmonary image sequence and from which relevant\ncoronavirus infection 3D features were extracted and classified. The results\nshow that the test set of the 3D model, the result: f1 score of 0.8379 and AUC\nvalue of 0.8807 have been achieved.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 18:10:07 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Qi", "Haibo", ""], ["Wang", "Yuhan", ""], ["Liu", "Xinyu", ""]]}, {"id": "2107.04062", "submitter": "Andre Mastmeyer", "authors": "Nico Zettler and Andre Mastmeyer", "title": "Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT\n  images", "comments": "9 pages, 6 figure, 2 tables", "journal-ref": "International Conference on Computer Graphics, Visualization and\n  Computer Vision 2021 - WSCG", "doi": null, "report-no": "H61", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-step concept for 3D segmentation on 5 abdominal organs inside\nvolumetric CT images is presented. First each relevant organ's volume of\ninterest is extracted as bounding box. The extracted volume acts as input for a\nsecond stage, wherein two compared U-Nets with different architectural\ndimensions re-construct an organ segmentation as label mask. In this work, we\nfocus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results\nindicate Dice improvements of about 6\\% at maximum. In this study to our\nsurprise, liver and kidneys for instance were tackled significantly better\nusing the faster and GPU-memory saving 2D U-Nets. For other abdominal key\norgans, there were no significant differences, but we observe highly\nsignificant advantages for the 2D U-Net in terms of GPU computational efforts\nfor all organs under study.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 18:35:15 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zettler", "Nico", ""], ["Mastmeyer", "Andre", ""]]}, {"id": "2107.04099", "submitter": "Andrea Liew", "authors": "Andrea Liew, Chun Cheng Lee, Boon Leong Lan, Maxine Tan", "title": "CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention\n  Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have been used quite successfully for\nsemantic segmentation of brain tumors. However, current CNNs and attention\nmechanisms are stochastic in nature and neglect the morphological indicators\nused by radiologists to manually annotate regions of interest. In this paper,\nwe introduce a channel and spatial wise asymmetric attention (CASPIAN) by\nleveraging the inherent structure of tumors to detect regions of saliency. To\ndemonstrate the efficacy of our proposed layer, we integrate this into a\nwell-established convolutional neural network (CNN) architecture to achieve\nhigher Dice scores, with less GPU resources. Also, we investigate the inclusion\nof auxiliary multiscale and multiplanar attention branches to increase the\nspatial context crucial in semantic segmentation tasks. The resulting\narchitecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole\ntumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven\nby the scarcity of brain tumor data, we investigate the Noisy Student method\nfor segmentation tasks. Our new Noisy Student Curriculum Learning paradigm,\nwhich infuses noise incrementally to increase the complexity of the training\nimages exposed to the network, further boosts the enhancing tumor region to\n81.53%. Additional validation performed on the BraTS2020 data shows that the\nNoisy Student Curriculum Learning method works well without any additional\ntraining or finetuning.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 20:35:17 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liew", "Andrea", ""], ["Lee", "Chun Cheng", ""], ["Lan", "Boon Leong", ""], ["Tan", "Maxine", ""]]}, {"id": "2107.04127", "submitter": "Manh Tu Vu", "authors": "Manh Tu Vu, Marie Beurton-Aimar", "title": "Multitask Multi-database Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we introduce our submission to the 2nd Affective Behavior\nAnalysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning\nmodel on multi-databases to perform two tasks: seven basic facial expressions\nprediction and valence-arousal estimation. Since these databases do not\ncontains labels for all the two tasks, we have applied the distillation\nknowledge technique to train two networks: one teacher and one student model.\nThe student model will be trained using both ground truth labels and soft\nlabels derived from the pretrained teacher model. During the training, we add\none more task, which is the combination of the two mentioned tasks, for better\nexploiting inter-task correlations. We also exploit the sharing videos between\nthe two tasks of the AffWild2 database that is used in the competition, to\nfurther improve the performance of the network. Experiment results shows that\nthe network have achieved promising results on the validation set of the\nAffWild2 database. Code and pretrained model are publicly available at\nhttps://github.com/glmanhtu/multitask-abaw-2021\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:57:58 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 15:36:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Vu", "Manh Tu", ""], ["Beurton-Aimar", "Marie", ""]]}, {"id": "2107.04133", "submitter": "Muhammad Ali Farooq", "authors": "Muhammad Ali Farooq, Ammar Ali Khan, Ansar Ahmad, Rana Hammad Raza", "title": "Effectiveness of State-of-the-Art Super Resolution Algorithms in\n  Surveillance Environment", "comments": null, "journal-ref": "Springer, Part of the Advances in Intelligent Systems and\n  Computing book series (AISC, volume 1376), 2021", "doi": "10.1007/978-3-030-74728-2_8", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image Super Resolution (SR) finds applications in areas where images need to\nbe closely inspected by the observer to extract enhanced information. One such\nfocused application is an offline forensic analysis of surveillance feeds. Due\nto the limitations of camera hardware, camera pose, limited bandwidth, varying\nillumination conditions, and occlusions, the quality of the surveillance feed\nis significantly degraded at times, thereby compromising monitoring of\nbehavior, activities, and other sporadic information in the scene. For the\nproposed research work, we have inspected the effectiveness of four\nconventional yet effective SR algorithms and three deep learning-based SR\nalgorithms to seek the finest method that executes well in a surveillance\nenvironment with limited training data op-tions. These algorithms generate an\nenhanced resolution output image from a sin-gle low-resolution (LR) input\nimage. For performance analysis, a subset of 220 images from six surveillance\ndatasets has been used, consisting of individuals with varying distances from\nthe camera, changing illumination conditions, and complex backgrounds. The\nperformance of these algorithms has been evaluated and compared using both\nqualitative and quantitative metrics. These SR algo-rithms have also been\ncompared based on face detection accuracy. By analyzing and comparing the\nperformance of all the algorithms, a Convolutional Neural Network (CNN) based\nSR technique using an external dictionary proved to be best by achieving robust\nface detection accuracy and scoring optimal quantitative metric results under\ndifferent surveillance conditions. This is because the CNN layers progressively\nlearn more complex features using an external dictionary.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 22:28:48 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Farooq", "Muhammad Ali", ""], ["Khan", "Ammar Ali", ""], ["Ahmad", "Ansar", ""], ["Raza", "Rana Hammad", ""]]}, {"id": "2107.04144", "submitter": "Alexander Wong", "authors": "Saad Abbasi, Mohammad Javad Shafiee, Ellick Chan, and Alexander Wong", "title": "Does Form Follow Function? An Empirical Exploration of the Impact of\n  Deep Neural Network Architecture Design on Hardware-Specific Acceleration", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fine-grained relationship between form and function with respect to deep\nneural network architecture design and hardware-specific acceleration is one\narea that is not well studied in the research literature, with form often\ndictated by accuracy as opposed to hardware function. In this study, a\ncomprehensive empirical exploration is conducted to investigate the impact of\ndeep neural network architecture design on the degree of inference speedup that\ncan be achieved via hardware-specific acceleration. More specifically, we\nempirically study the impact of a variety of commonly used macro-architecture\ndesign patterns across different architectural depths through the lens of\nOpenVINO microprocessor-specific and GPU-specific acceleration. Experimental\nresults showed that while leveraging hardware-specific acceleration achieved an\naverage inference speed-up of 380%, the degree of inference speed-up varied\ndrastically depending on the macro-architecture design pattern, with the\ngreatest speedup achieved on the depthwise bottleneck convolution design\npattern at 550%. Furthermore, we conduct an in-depth exploration of the\ncorrelation between FLOPs requirement, level 3 cache efficacy, and network\nlatency with increasing architectural depth and width. Finally, we analyze the\ninference time reductions using hardware-specific acceleration when compared to\nnative deep learning frameworks across a wide variety of hand-crafted deep\nconvolutional neural network architecture designs as well as ones found via\nneural architecture search strategies. We found that the DARTS-derived\narchitecture to benefit from the greatest improvement from hardware-specific\nsoftware acceleration (1200%) while the depthwise bottleneck convolution-based\nMobileNet-V2 to have the lowest overall inference time of around 2.4 ms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 23:05:39 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Abbasi", "Saad", ""], ["Shafiee", "Mohammad Javad", ""], ["Chan", "Ellick", ""], ["Wong", "Alexander", ""]]}, {"id": "2107.04174", "submitter": "Jacob Donley", "authors": "Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark Broyles, Hao\n  Jiang, Jie Shen, Maja Pantic, Vamsi Krishna Ithapu, Ravish Mehra", "title": "EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy\n  Communication in Noisy Environments", "comments": "Dataset is available at:\n  https://github.com/facebookresearch/EasyComDataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) as a platform has the potential to facilitate the\nreduction of the cocktail party effect. Future AR headsets could potentially\nleverage information from an array of sensors spanning many different\nmodalities. Training and testing signal processing and machine learning\nalgorithms on tasks such as beam-forming and speech enhancement require high\nquality representative data. To the best of the author's knowledge, as of\npublication there are no available datasets that contain synchronized\negocentric multi-channel audio and video with dynamic movement and\nconversations in a noisy environment. In this work, we describe, evaluate and\nrelease a dataset that contains over 5 hours of multi-modal data useful for\ntraining and testing algorithms for the application of improving conversations\nfor an AR glasses wearer. We provide speech intelligibility, quality and\nsignal-to-noise ratio improvement results for a baseline method and show\nimprovements across all tested metrics. The dataset we are releasing contains\nAR glasses egocentric multi-channel microphone array audio, wide field-of-view\nRGB video, speech source pose, headset microphone audio, annotated voice\nactivity, speech transcriptions, head bounding boxes, target of speech and\nsource identification labels. We have created and are releasing this dataset to\nfacilitate research in multi-modal AR solutions to the cocktail party problem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 02:00:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Donley", "Jacob", ""], ["Tourbabin", "Vladimir", ""], ["Lee", "Jung-Suk", ""], ["Broyles", "Mark", ""], ["Jiang", "Hao", ""], ["Shen", "Jie", ""], ["Pantic", "Maja", ""], ["Ithapu", "Vamsi Krishna", ""], ["Mehra", "Ravish", ""]]}, {"id": "2107.04187", "submitter": "Yue Jin", "authors": "Yue Jin, Tianqing Zheng, Chao Gao, Guoqiang Xu", "title": "A Multi-modal and Multi-task Learning Method for Action Unit and\n  Expression Recognition", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Analyzing human affect is vital for human-computer interaction systems. Most\nmethods are developed in restricted scenarios which are not practical for\nin-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021\nContest provides a benchmark for this in-the-wild problem. In this paper, we\nintroduce a multi-modal and multi-task learning method by using both visual and\naudio information. We use both AU and expression annotations to train the model\nand apply a sequence model to further extract associations between video\nframes. We achieve an AU score of 0.712 and an expression score of 0.477 on the\nvalidation set. These results demonstrate the effectiveness of our approach in\nimproving model performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 03:28:17 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 11:14:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Jin", "Yue", ""], ["Zheng", "Tianqing", ""], ["Gao", "Chao", ""], ["Xu", "Guoqiang", ""]]}, {"id": "2107.04192", "submitter": "Hung Hoang Manh", "authors": "Phan Tran Dac Thinh, Hoang Manh Hung, Hyung-Jeong Yang, Soo-Hyung Kim,\n  and Guee-Sang Lee", "title": "Emotion Recognition with Incomplete Labels Using Modified Multi-task\n  Learning Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of predicting affective information in the wild such as seven basic\nemotions or action units from human faces has gradually become more interesting\ndue to the accessibility and availability of massive annotated datasets. In\nthis study, we propose a method that utilizes the association between seven\nbasic emotions and twelve action units from the AffWild2 dataset. The method\nbased on the architecture of ResNet50 involves the multi-task learning\ntechnique for the incomplete labels of the two tasks. By combining the\nknowledge for two correlated tasks, both performances are improved by a large\nmargin compared to those with the model employing only one kind of label.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 03:43:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Thinh", "Phan Tran Dac", ""], ["Hung", "Hoang Manh", ""], ["Yang", "Hyung-Jeong", ""], ["Kim", "Soo-Hyung", ""], ["Lee", "Guee-Sang", ""]]}, {"id": "2107.04220", "submitter": "Mayank Goswami", "authors": "Mayank Goswami", "title": "Deep Learning models for benign and malign Ocular Tumor Growth\n  Estimation", "comments": "22 Page, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.TO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Relatively abundant availability of medical imaging data has provided\nsignificant support in the development and testing of Neural Network based\nimage processing methods. Clinicians often face issues in selecting suitable\nimage processing algorithm for medical imaging data. A strategy for the\nselection of a proper model is presented here. The training data set comprises\noptical coherence tomography (OCT) and angiography (OCT-A) images of 50 mice\neyes with more than 100 days follow-up. The data contains images from treated\nand untreated mouse eyes. Four deep learning variants are tested for automatic\n(a) differentiation of tumor region with healthy retinal layer and (b)\nsegmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of\ndeep learning models is performed with respect to the number of training and\ntesting images using 8 eight performance indices to study accuracy,\nreliability/reproducibility, and speed. U-net with UVgg16 is best for malign\ntumor data set with treatment (having considerable variation) and U-net with\nInception backbone for benign tumor data (with minor variation). Loss value and\nroot mean square error (R.M.S.E.) are found most and least sensitive\nperformance indices, respectively. The performance (via indices) is found to be\nexponentially improving regarding a number of training images. The segmented\nOCT-Angiography data shows that neovascularization drives the tumor volume.\nImage analysis shows that photodynamic imaging-assisted tumor treatment\nprotocol is transforming an aggressively growing tumor into a cyst. An\nempirical expression is obtained to help medical professionals to choose a\nparticular model given the number of images and types of characteristics. We\nrecommend that the presented exercise should be taken as standard practice\nbefore employing a particular deep learning model for biomedical image\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 05:40:25 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Goswami", "Mayank", ""]]}, {"id": "2107.04225", "submitter": "Lingfeng Wang", "authors": "Lingfeng Wang, Shisen Wang", "title": "A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective Behavior Analysis is an important part in human-computer\ninteraction. Existing successful affective behavior analysis method such as\nTSAV[9] suffer from challenge of incomplete labeled datasets. To boost its\nperformance, this paper presents a multi-task mean teacher model for\nsemi-supervised Affective Behavior Analysis to learn from missing labels and\nexploring the learning of multiple correlated task simultaneously. To be\nspecific, we first utilize TSAV as baseline model to simultaneously recognize\nthe three tasks. We have modified the preprocessing method of rendering mask to\nprovide better semantics information. After that, we extended TSAV model to\nsemi-supervised model using mean teacher, which allow it to be benefited from\nunlabeled data. Experimental results on validation datasets show that our\nmethod achieves better performance than TSAV model, which verifies that the\nproposed network can effectively learn additional unlabeled data to boost the\naffective behavior analysis performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 05:48:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 05:15:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Lingfeng", ""], ["Wang", "Shisen", ""]]}, {"id": "2107.04228", "submitter": "Liangming Chen", "authors": "Mei Liu, Liangming Chen, Xiaohao Du, Long Jin, and Mingsheng Shang", "title": "Activated Gradients for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks often suffer from poor performance or even training\nfailure due to the ill-conditioned problem, the vanishing/exploding gradient\nproblem, and the saddle point problem. In this paper, a novel method by acting\nthe gradient activation function (GAF) on the gradient is proposed to handle\nthese challenges. Intuitively, the GAF enlarges the tiny gradients and\nrestricts the large gradient. Theoretically, this paper gives conditions that\nthe GAF needs to meet, and on this basis, proves that the GAF alleviates the\nproblems mentioned above. In addition, this paper proves that the convergence\nrate of SGD with the GAF is faster than that without the GAF under some\nassumptions. Furthermore, experiments on CIFAR, ImageNet, and PASCAL visual\nobject classes confirm the GAF's effectiveness. The experimental results also\ndemonstrate that the proposed method is able to be adopted in various deep\nneural networks to improve their performance. The source code is publicly\navailable at\nhttps://github.com/LongJin-lab/Activated-Gradients-for-Deep-Neural-Networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:00:55 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Mei", ""], ["Chen", "Liangming", ""], ["Du", "Xiaohao", ""], ["Jin", "Long", ""], ["Shang", "Mingsheng", ""]]}, {"id": "2107.04231", "submitter": "Vinod K Kurmi", "authors": "Vinod K Kurmi and Venkatesh K Subramanian and Vinay P. Namboodiri", "title": "Exploring Dropout Discriminator for Domain Adaptation", "comments": "This work is an extension of our BMVC-2019 paper (arXiv:1907.10628)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adaptation of a classifier to new domains is one of the challenging problems\nin machine learning. This has been addressed using many deep and non-deep\nlearning based methods. Among the methodologies used, that of adversarial\nlearning is widely applied to solve many deep learning problems along with\ndomain adaptation. These methods are based on a discriminator that ensures\nsource and target distributions are close. However, here we suggest that rather\nthan using a point estimate obtaining by a single discriminator, it would be\nuseful if a distribution based on ensembles of discriminators could be used to\nbridge this gap. This could be achieved using multiple classifiers or using\ntraditional ensemble methods. In contrast, we suggest that a Monte Carlo\ndropout based ensemble discriminator could suffice to obtain the distribution\nbased discriminator. Specifically, we propose a curriculum based dropout\ndiscriminator that gradually increases the variance of the sample based\ndistribution and the corresponding reverse gradients are used to align the\nsource and target feature representations. An ensemble of discriminators helps\nthe model to learn the data distribution efficiently. It also provides a better\ngradient estimates to train the feature extractor. The detailed results and\nthorough ablation analysis show that our model outperforms state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:11:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Subramanian", "Venkatesh K", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2107.04240", "submitter": "Sharon Xiaolei Huang", "authors": "Yuan Xue, Yuan-Chen Guo, Han Zhang, Tao Xu, Song-Hai Zhang, Xiaolei\n  Huang", "title": "Deep Image Synthesis from Intuitive User Input: A Review and\n  Perspectives", "comments": "26 pages, 7 figures, 1 table", "journal-ref": "Computational Visual Media 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many applications of computer graphics, art and design, it is desirable\nfor a user to provide intuitive non-image input, such as text, sketch, stroke,\ngraph or layout, and have a computer system automatically generate\nphoto-realistic images that adhere to the input content. While classic works\nthat allow such automatic image content generation have followed a framework of\nimage retrieval and composition, recent advances in deep generative models such\nas generative adversarial networks (GANs), variational autoencoders (VAEs), and\nflow-based methods have enabled more powerful and versatile image generation\ntasks. This paper reviews recent works for image synthesis given intuitive user\ninput, covering advances in input versatility, image generation methodology,\nbenchmark datasets, and evaluation metrics. This motivates new perspectives on\ninput representation and interactivity, cross pollination between major image\ngeneration paradigms, and evaluation and comparison of generation methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:31:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Xue", "Yuan", ""], ["Guo", "Yuan-Chen", ""], ["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Zhang", "Song-Hai", ""], ["Huang", "Xiaolei", ""]]}, {"id": "2107.04259", "submitter": "You-Cyuan Jhang", "authors": "Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan\n  Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh, Bowen Li, Steven Leal, Pete\n  Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, Nupur Yadav", "title": "Unity Perception: Generate Synthetic Data for Computer Vision", "comments": "We corrected tasks supported by NVISII platform. For the Unity\n  perception package, see\n  https://github.com/Unity-Technologies/com.unity.perception", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Unity Perception package which aims to simplify and\naccelerate the process of generating synthetic datasets for computer vision\ntasks by offering an easy-to-use and highly customizable toolset. This\nopen-source package extends the Unity Editor and engine components to generate\nperfectly annotated examples for several common computer vision tasks.\nAdditionally, it offers an extensible Randomization framework that lets the\nuser quickly construct and configure randomized simulation parameters in order\nto introduce variation into the generated datasets. We provide an overview of\nthe provided tools and how they work, and demonstrate the value of the\ngenerated synthetic datasets by training a 2D object detection model. The model\ntrained with mostly synthetic data outperforms the model trained using only\nreal data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:09:00 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 19:12:32 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Borkman", "Steve", ""], ["Crespi", "Adam", ""], ["Dhakad", "Saurav", ""], ["Ganguly", "Sujoy", ""], ["Hogins", "Jonathan", ""], ["Jhang", "You-Cyuan", ""], ["Kamalzadeh", "Mohsen", ""], ["Li", "Bowen", ""], ["Leal", "Steven", ""], ["Parisi", "Pete", ""], ["Romero", "Cesar", ""], ["Smith", "Wesley", ""], ["Thaman", "Alex", ""], ["Warren", "Samuel", ""], ["Yadav", "Nupur", ""]]}, {"id": "2107.04261", "submitter": "Qiegen Liu", "authors": "Jin Li, Wanyun Li, Zichen Xu, Yuhao Wang, Qiegen Liu", "title": "Wavelet Transform-assisted Adaptive Generative Modeling for Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised deep learning has recently demonstrated the promise to produce\nhigh-quality samples. While it has tremendous potential to promote the image\ncolorization task, the performance is limited owing to the manifold hypothesis\nin machine learning. This study presents a novel scheme that exploiting the\nscore-based generative model in wavelet domain to address the issue. By taking\nadvantage of the multi-scale and multi-channel representation via wavelet\ntransform, the proposed model learns the priors from stacked wavelet\ncoefficient components, thus learns the image characteristics under coarse and\ndetail frequency spectrums jointly and effectively. Moreover, such a highly\nflexible generative model without adversarial optimization can execute\ncolorization tasks better under dual consistency terms in wavelet domain,\nnamely data-consistency and structure-consistency. Specifically, in the\ntraining phase, a set of multi-channel tensors consisting of wavelet\ncoefficients are used as the input to train the network by denoising score\nmatching. In the test phase, samples are iteratively generated via annealed\nLangevin dynamics with data and structure consistencies. Experiments\ndemonstrated remarkable improvements of the proposed model on colorization\nquality, particularly on colorization robustness and diversity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:12:39 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Li", "Jin", ""], ["Li", "Wanyun", ""], ["Xu", "Zichen", ""], ["Wang", "Yuhao", ""], ["Liu", "Qiegen", ""]]}, {"id": "2107.04263", "submitter": "Laura Daza", "authors": "Laura Daza, Juan C. P\\'erez, Pablo Arbel\\'aez", "title": "Towards Robust General Medical Image Segmentation", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of Deep Learning systems depends on their accuracy but also\non their robustness against adversarial perturbations to the input data.\nSeveral attacks and defenses have been proposed to improve the performance of\nDeep Neural Networks under the presence of adversarial noise in the natural\nimage domain. However, robustness in computer-aided diagnosis for volumetric\ndata has only been explored for specific tasks and with limited attacks. We\npropose a new framework to assess the robustness of general medical image\nsegmentation systems. Our contributions are two-fold: (i) we propose a new\nbenchmark to evaluate robustness in the context of the Medical Segmentation\nDecathlon (MSD) by extending the recent AutoAttack natural image classification\nframework to the domain of volumetric data segmentation, and (ii) we present a\nnovel lattice architecture for RObust Generic medical image segmentation (ROG).\nOur results show that ROG is capable of generalizing across different tasks of\nthe MSD and largely surpasses the state-of-the-art under sophisticated\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:17:05 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Daza", "Laura", ""], ["P\u00e9rez", "Juan C.", ""], ["Arbel\u00e1ez", "Pablo", ""]]}, {"id": "2107.04277", "submitter": "Xueying Wang", "authors": "Xueying Wang, Yudong Guo, Zhongqi Yang and Juyong Zhang", "title": "Prior-Guided Multi-View 3D Head Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a 3D head model including the complete face and hair regions is\nstill a challenging problem in computer vision and graphics. In this paper, we\nconsider this problem with a few multi-view portrait images as input. Previous\nmulti-view stereo methods, either based on the optimization strategies or deep\nlearning techniques, suffer from low-frequency geometric structures such as\nunclear head structures and inaccurate reconstruction in hair regions. To\ntackle this problem, we propose a prior-guided implicit neural rendering\nnetwork. Specifically, we model the head geometry with a learnable signed\ndistance field (SDF) and optimize it via an implicit differentiable renderer\nwith the guidance of some human head priors, including the facial prior\nknowledge, head semantic segmentation information and 2D hair orientation maps.\nThe utilization of these priors can improve the reconstruction accuracy and\nrobustness, leading to a high-quality integrated 3D head model. Extensive\nablation studies and comparisons with state-of-the-art methods demonstrate that\nour method could produce high-fidelity 3D head geometries with the guidance of\nthese priors.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:43:56 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Xueying", ""], ["Guo", "Yudong", ""], ["Yang", "Zhongqi", ""], ["Zhang", "Juyong", ""]]}, {"id": "2107.04279", "submitter": "Yu Siyue", "authors": "Siyue Yu, Jimin Xiao, BingFeng Zhang, Eng Gee Lim", "title": "Fast Pixel-Matching for Video Object Segmentation", "comments": "Accepted by Signal Processing: Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video object segmentation, aiming to segment the foreground objects given the\nannotation of the first frame, has been attracting increasing attentions. Many\nstate-of-the-art approaches have achieved great performance by relying on\nonline model updating or mask-propagation techniques. However, most online\nmodels require high computational cost due to model fine-tuning during\ninference. Most mask-propagation based models are faster but with relatively\nlow performance due to failure to adapt to object appearance variation. In this\npaper, we are aiming to design a new model to make a good balance between speed\nand performance. We propose a model, called NPMCA-net, which directly localizes\nforeground objects based on mask-propagation and non-local technique by\nmatching pixels in reference and target frames. Since we bring in information\nof both first and previous frames, our network is robust to large object\nappearance variation, and can better adapt to occlusions. Extensive experiments\nshow that our approach can achieve a new state-of-the-art performance with a\nfast speed at the same time (86.5% IoU on DAVIS-2016 and 72.2% IoU on\nDAVIS-2017, with speed of 0.11s per frame) under the same level comparison.\nSource code is available at https://github.com/siyueyu/NPMCA-net.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:46:46 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Yu", "Siyue", ""], ["Xiao", "Jimin", ""], ["Zhang", "BingFeng", ""], ["Lim", "Eng Gee", ""]]}, {"id": "2107.04281", "submitter": "Qing Guo", "authors": "Qing Guo and Xiaoguang Li and Felix Juefei-Xu and Hongkai Yu and Yang\n  Liu and Song wang", "title": "JPGNet: Joint Predictive Filtering and Generative Network for Image\n  Inpainting", "comments": "This work has been accepted to ACM-MM 2021", "journal-ref": null, "doi": "10.1145/3474085.3475170", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting aims to restore the missing regions and make the recovery\nresults identical to the originally complete image, which is different from the\ncommon generative task emphasizing the naturalness of generated images.\nNevertheless, existing works usually regard it as a pure generation problem and\nemploy cutting-edge generative techniques to address it. The generative\nnetworks fill the main missing parts with realistic contents but usually\ndistort the local structures. In this paper, we formulate image inpainting as a\nmix of two problems, i.e., predictive filtering and deep generation. Predictive\nfiltering is good at preserving local structures and removing artifacts but\nfalls short to complete the large missing regions. The deep generative network\ncan fill the numerous missing pixels based on the understanding of the whole\nscene but hardly restores the details identical to the original ones. To make\nuse of their respective advantages, we propose the joint predictive filtering\nand generative network (JPGNet) that contains three branches: predictive\nfiltering & uncertainty network (PFUNet), deep generative network, and\nuncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict\npixel-wise kernels for filtering-based inpainting according to the input image\nand output an uncertainty map. This map indicates the pixels should be\nprocessed by filtering or generative networks, which is further fed to the\nUAFNet for a smart combination between filtering and generative results. Note\nthat, our method as a novel framework for the image inpainting problem can\nbenefit any existing generation-based methods. We validate our method on three\npublic datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our\nmethod can enhance three state-of-the-art generative methods (i.e., StructFlow,\nEdgeConnect, and RFRNet) significantly with the slightly extra time cost.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:49:52 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 04:12:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Guo", "Qing", ""], ["Li", "Xiaoguang", ""], ["Juefei-Xu", "Felix", ""], ["Yu", "Hongkai", ""], ["Liu", "Yang", ""], ["wang", "Song", ""]]}, {"id": "2107.04282", "submitter": "Dewei Hu", "authors": "Dewei Hu, Can Cui, Hao Li, Kathleen E. Larson, Yuankai K. Tao and Ipek\n  Oguz", "title": "LIFE: A Generalizable Autodidactic Pipeline for 3D OCT-A Vessel\n  Segmentation", "comments": "Accepted by International Conference on Medical Image Computing and\n  Computer Assisted Intervention (MICCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical coherence tomography (OCT) is a non-invasive imaging technique widely\nused for ophthalmology. It can be extended to OCT angiography (OCT-A), which\nreveals the retinal vasculature with improved contrast. Recent deep learning\nalgorithms produced promising vascular segmentation results; however, 3D\nretinal vessel segmentation remains difficult due to the lack of manually\nannotated training data. We propose a learning-based method that is only\nsupervised by a self-synthesized modality named local intensity fusion (LIF).\nLIF is a capillary-enhanced volume computed directly from the input OCT-A. We\nthen construct the local intensity fusion encoder (LIFE) to map a given OCT-A\nvolume and its LIF counterpart to a shared latent space. The latent space of\nLIFE has the same dimensions as the input data and it contains features common\nto both modalities. By binarizing this latent space, we obtain a volumetric\nvessel segmentation. Our method is evaluated in a human fovea OCT-A and three\nzebrafish OCT-A volumes with manual labels. It yields a Dice score of 0.7736 on\nhuman data and 0.8594 +/- 0.0275 on zebrafish data, a dramatic improvement over\nexisting unsupervised algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:51:33 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Hu", "Dewei", ""], ["Cui", "Can", ""], ["Li", "Hao", ""], ["Larson", "Kathleen E.", ""], ["Tao", "Yuankai K.", ""], ["Oguz", "Ipek", ""]]}, {"id": "2107.04286", "submitter": "Yilin Liu", "authors": "Yilin Liu and Fuyou Xue and Hui Huang", "title": "UrbanScene3D: A Large Scale Urban Scene Dataset and Simulator", "comments": "Project page: https://vcc.tech/UrbanScene3D/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perceive the environments in different ways is essential to\nrobotic research. This involves the analysis of both 2D and 3D data sources. We\npresent a large scale urban scene dataset associated with a handy simulator\nbased on Unreal Engine 4 and AirSim, which consists of both man-made and\nreal-world reconstruction scenes in different scales, referred to as\nUrbanScene3D. Unlike previous works that purely based on 2D information or\nman-made 3D CAD models, UrbanScene3D contains both compact man-made models and\ndetailed real-world models reconstructed by aerial images. Each building has\nbeen manually extracted from the entire scene model and then has been assigned\nwith a unique label, forming an instance segmentation map. The provided 3D\nground-truth textured models with instance segmentation labels in UrbanScene3D\nallow users to obtain all kinds of data they would like to have: instance\nsegmentation map, depth map in arbitrary resolution, 3D point cloud/mesh in\nboth visible and invisible places, etc. In addition, with the help of AirSim,\nusers can also simulate the robots (cars/drones)to test a variety of autonomous\ntasks in the proposed city environment. Please refer to our paper and\nwebsite(https://vcc.tech/UrbanScene3D/) for further details and applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:56:46 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Yilin", ""], ["Xue", "Fuyou", ""], ["Huang", "Hui", ""]]}, {"id": "2107.04288", "submitter": "Dewei Hu", "authors": "Dewei Hu, Joseph D. Malone, Yigit Atay, Yuankai K. Tao and Ipek Oguz", "title": "Retinal OCT Denoising with Pseudo-Multimodal Fusion Network", "comments": "Accepted by International Workshop on Ophthalmic Medical Image\n  Analysis (OMIA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical coherence tomography (OCT) is a prevalent imaging technique for\nretina. However, it is affected by multiplicative speckle noise that can\ndegrade the visibility of essential anatomical structures, including blood\nvessels and tissue layers. Although averaging repeated B-scan frames can\nsignificantly improve the signal-to-noise-ratio (SNR), this requires longer\nacquisition time, which can introduce motion artifacts and cause discomfort to\npatients. In this study, we propose a learning-based method that exploits\ninformation from the single-frame noisy B-scan and a pseudo-modality that is\ncreated with the aid of the self-fusion method. The pseudo-modality provides\ngood SNR for layers that are barely perceptible in the noisy B-scan but can\nover-smooth fine features such as small vessels. By using a fusion network,\ndesired features from each modality can be combined, and the weight of their\ncontribution is adjustable. Evaluated by intensity-based and structural\nmetrics, the result shows that our method can effectively suppress the speckle\nnoise and enhance the contrast between retina layers while the overall\nstructure and small blood vessels are preserved. Compared to the single\nmodality network, our method improves the structural similarity with low noise\nB-scan from 0.559 +\\- 0.033 to 0.576 +\\- 0.031.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 08:00:20 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Hu", "Dewei", ""], ["Malone", "Joseph D.", ""], ["Atay", "Yigit", ""], ["Tao", "Yuankai K.", ""], ["Oguz", "Ipek", ""]]}, {"id": "2107.04291", "submitter": "Yiqun Lin", "authors": "Yiqun Lin, Lichang Chen, Haibin Huang, Chongyang Ma, Xiaoguang Han and\n  Shuguang Cui", "title": "Beyond Farthest Point Sampling in Point-Wise Analysis", "comments": "12 pages, 13 figures and 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sampling, grouping, and aggregation are three important components in the\nmulti-scale analysis of point clouds. In this paper, we present a novel\ndata-driven sampler learning strategy for point-wise analysis tasks. Unlike the\nwidely used sampling technique, Farthest Point Sampling (FPS), we propose to\nlearn sampling and downstream applications jointly. Our key insight is that\nuniform sampling methods like FPS are not always optimal for different tasks:\nsampling more points around boundary areas can make the point-wise\nclassification easier for segmentation. Towards the end, we propose a novel\nsampler learning strategy that learns sampling point displacement supervised by\ntask-related ground truth information and can be trained jointly with the\nunderlying tasks. We further demonstrate our methods in various point-wise\nanalysis architectures, including semantic part segmentation, point cloud\ncompletion, and keypoint detection. Our experiments show that jointly learning\nof the sampler and task brings remarkable improvement over previous baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 08:08:44 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 03:04:09 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lin", "Yiqun", ""], ["Chen", "Lichang", ""], ["Huang", "Haibin", ""], ["Ma", "Chongyang", ""], ["Han", "Xiaoguang", ""], ["Cui", "Shuguang", ""]]}, {"id": "2107.04296", "submitter": "Moritz Knolle", "authors": "Moritz Knolle, Alexander Ziller, Dmitrii Usynin, Rickmer Braren,\n  Marcus R. Makowski, Daniel Rueckert, Georgios Kaissis", "title": "Differentially private training of neural networks with Langevin\n  dynamics forcalibrated predictive uncertainty", "comments": "Accepted to the ICML 2021 Theory and Practice of Differential Privacy\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that differentially private stochastic gradient descent (DP-SGD) can\nyield poorly calibrated, overconfident deep learning models. This represents a\nserious issue for safety-critical applications, e.g. in medical diagnosis. We\nhighlight and exploit parallels between stochastic gradient Langevin dynamics,\na scalable Bayesian inference technique for training deep neural networks, and\nDP-SGD, in order to train differentially private, Bayesian neural networks with\nminor adjustments to the original (DP-SGD) algorithm. Our approach provides\nconsiderably more reliable uncertainty estimates than DP-SGD, as demonstrated\nempirically by a reduction in expected calibration error (MNIST $\\sim{5}$-fold,\nPediatric Pneumonia Dataset $\\sim{2}$-fold).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 08:14:45 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Knolle", "Moritz", ""], ["Ziller", "Alexander", ""], ["Usynin", "Dmitrii", ""], ["Braren", "Rickmer", ""], ["Makowski", "Marcus R.", ""], ["Rueckert", "Daniel", ""], ["Kaissis", "Georgios", ""]]}, {"id": "2107.04306", "submitter": "Wenting Jiang", "authors": "Wenting Jiang, Yicheng Jiang, Lu Zhang, Changmiao Wang, Xiaoguang Han,\n  Shuixing Zhang, Xiang Wan, Shuguang Cui", "title": "Hepatocellular Carcinoma Segmentation fromDigital Subtraction\n  Angiography Videos usingLearnable Temporal Difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of hepatocellular carcinoma (HCC)in Digital\nSubtraction Angiography (DSA) videos can assist radiologistsin efficient\ndiagnosis of HCC and accurate evaluation of tumors in clinical practice. Few\nstudies have investigated HCC segmentation from DSAvideos. It shows great\nchallenging due to motion artifacts in filming, ambiguous boundaries of tumor\nregions and high similarity in imaging toother anatomical tissues. In this\npaper, we raise the problem of HCCsegmentation in DSA videos, and build our own\nDSA dataset. We alsopropose a novel segmentation network called DSA-LTDNet,\nincluding asegmentation sub-network, a temporal difference learning (TDL)\nmoduleand a liver region segmentation (LRS) sub-network for providing\nadditional guidance. DSA-LTDNet is preferable for learning the latent\nmotioninformation from DSA videos proactively and boosting segmentation\nperformance. All of experiments are conducted on our self-collected\ndataset.Experimental results show that DSA-LTDNet increases the DICE scoreby\nnearly 4% compared to the U-Net baseline.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 08:35:37 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Jiang", "Wenting", ""], ["Jiang", "Yicheng", ""], ["Zhang", "Lu", ""], ["Wang", "Changmiao", ""], ["Han", "Xiaoguang", ""], ["Zhang", "Shuixing", ""], ["Wan", "Xiang", ""], ["Cui", "Shuguang", ""]]}, {"id": "2107.04313", "submitter": "Yuki Asano", "authors": "Hannah Rose Kirk, Yennie Jun, Paulius Rauba, Gal Wachtel, Ruining Li,\n  Xingjian Bai, Noah Broestl, Martin Doff-Sotta, Aleksandar Shtedritski, Yuki\n  M. Asano", "title": "Memes in the Wild: Assessing the Generalizability of the Hateful Memes\n  Challenge Dataset", "comments": "Accepted paper at ACL WOAH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hateful memes pose a unique challenge for current machine learning systems\nbecause their message is derived from both text- and visual-modalities. To this\neffect, Facebook released the Hateful Memes Challenge, a dataset of memes with\npre-extracted text captions, but it is unclear whether these synthetic examples\ngeneralize to `memes in the wild'. In this paper, we collect hateful and\nnon-hateful memes from Pinterest to evaluate out-of-sample performance on\nmodels pre-trained on the Facebook dataset. We find that memes in the wild\ndiffer in two key aspects: 1) Captions must be extracted via OCR, injecting\nnoise and diminishing performance of multimodal models, and 2) Memes are more\ndiverse than `traditional memes', including screenshots of conversations or\ntext on a plain background. This paper thus serves as a reality check for the\ncurrent benchmark of hateful meme detection and its applicability for detecting\nreal world hate.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:04:05 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kirk", "Hannah Rose", ""], ["Jun", "Yennie", ""], ["Rauba", "Paulius", ""], ["Wachtel", "Gal", ""], ["Li", "Ruining", ""], ["Bai", "Xingjian", ""], ["Broestl", "Noah", ""], ["Doff-Sotta", "Martin", ""], ["Shtedritski", "Aleksandar", ""], ["Asano", "Yuki M.", ""]]}, {"id": "2107.04324", "submitter": "Haoxian Tan", "authors": "Haoxian Tan, Sheng Guo, Yujie Zhong, Weilin Huang", "title": "Mutually-aware Sub-Graphs Differentiable Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiable architecture search is prevalent in the field of NAS because\nof its simplicity and efficiency, where two paradigms, multi-path algorithms\nand single-path methods, are dominated. Multi-path framework (e.g. DARTS) is\nintuitive but suffers from memory usage and training collapse. Single-path\nmethods (e.g.GDAS and ProxylessNAS) mitigate the memory issue and shrink the\ngap between searching and evaluation but sacrifice the performance. In this\npaper, we propose a conceptually simple yet efficient method to bridge these\ntwo paradigms, referred as Mutually-aware Sub-Graphs Differentiable\nArchitecture Search (MSG-DAS). The core of our framework is a differentiable\nGumbel-TopK sampler that produces multiple mutually exclusive single-path\nsub-graphs. To alleviate the severer skip-connect issue brought by multiple\nsub-graphs setting, we propose a Dropblock-Identity module to stabilize the\noptimization. To make best use of the available models (super-net and\nsub-graphs), we introduce a memory-efficient super-net guidance distillation to\nimprove training. The proposed framework strikes a balance between flexible\nmemory usage and searching quality. We demonstrate the effectiveness of our\nmethods on ImageNet and CIFAR10, where the searched models show a comparable\nperformance as the most recent approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:31:31 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 09:46:24 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tan", "Haoxian", ""], ["Guo", "Sheng", ""], ["Zhong", "Yujie", ""], ["Huang", "Weilin", ""]]}, {"id": "2107.04326", "submitter": "Floris Naber", "authors": "Floris Naber", "title": "Semantic Segmentation on Multiple Visual Domains", "comments": "Graduation project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic segmentation models only perform well on the domain they are trained\non and datasets for training are scarce and often have a small label-spaces,\nbecause the pixel level annotations required are expensive to make. Thus\ntraining models on multiple existing domains is desired to increase the output\nlabel-space. Current research shows that there is potential to improve accuracy\nacross datasets by using multi-domain training, but this has not yet been\nsuccessfully extended to datasets of three different non-overlapping domains\nwithout manual labelling. In this paper a method for this is proposed for the\ndatasets Cityscapes, SUIM and SUN RGB-D, by creating a label-space that spans\nall classes of the datasets. Duplicate classes are merged and discrepant\ngranularity is solved by keeping classes separate. Results show that accuracy\nof the multi-domain model has higher accuracy than all baseline models\ntogether, if hardware performance is equalized, as resources are not limitless,\nshowing that models benefit from additional data even from domains that have\nnothing in common.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:34:51 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Naber", "Floris", ""]]}, {"id": "2107.04327", "submitter": "Nuri Benbarka", "authors": "Nuri Benbarka, Jona Schr\\\"oder, Andreas Zell", "title": "Score refinement for confidence-based 3D multi-object tracking", "comments": "Accepted at IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-object tracking is a critical component in autonomous navigation, as it\nprovides valuable information for decision-making. Many researchers tackled the\n3D multi-object tracking task by filtering out the frame-by-frame 3D\ndetections; however, their focus was mainly on finding useful features or\nproper matching metrics. Our work focuses on a neglected part of the tracking\nsystem: score refinement and tracklet termination. We show that manipulating\nthe scores depending on time consistency while terminating the tracklets\ndepending on the tracklet score improves tracking results. We do this by\nincreasing the matched tracklets' score with score update functions and\ndecreasing the unmatched tracklets' score. Compared to count-based methods, our\nmethod consistently produces better AMOTA and MOTA scores when utilizing\nvarious detectors and filtering algorithms on different datasets. The\nimprovements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our\nmethod as a late-fusion ensembling method, and it performed better than\nvoting-based ensemble methods by a solid margin. It achieved an AMOTA score of\n67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art\ntrackers. Code is publicly available at:\n\\url{https://github.com/cogsys-tuebingen/CBMOT}.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:40:07 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Benbarka", "Nuri", ""], ["Schr\u00f6der", "Jona", ""], ["Zell", "Andreas", ""]]}, {"id": "2107.04331", "submitter": "Yucheol Jung", "authors": "Wonjong Jang, Gwangjin Ju, Yucheol Jung, Jiaolong Yang, Xin Tong,\n  Seungyong Lee", "title": "StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation", "comments": "Accepted to SIGGRAPH 2021. For supplementary material, see\n  http://cg.postech.ac.kr/papers/2021_StyleCariGAN_supp.zip", "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 116. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459860", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a caricature generation framework based on shape and style\nmanipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically\ncreates a realistic and detailed caricature from an input photo with optional\ncontrols on shape exaggeration degree and color stylization type. The key\ncomponent of our method is shape exaggeration blocks that are used for\nmodulating coarse layer feature maps of StyleGAN to produce desirable\ncaricature shape exaggerations. We first build a layer-mixed StyleGAN for\nphoto-to-caricature style conversion by swapping fine layers of the StyleGAN\nfor photos to the corresponding layers of the StyleGAN trained to generate\ncaricatures. Given an input photo, the layer-mixed model produces detailed\ncolor stylization for a caricature but without shape exaggerations. We then\nappend shape exaggeration blocks to the coarse layers of the layer-mixed model\nand train the blocks to create shape exaggerations while preserving the\ncharacteristic appearances of the input. Experimental results show that our\nStyleCariGAN generates realistic and detailed caricatures compared to the\ncurrent state-of-the-art methods. We demonstrate StyleCariGAN also supports\nother StyleGAN-based image manipulations, such as facial expression control.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:49:31 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Jang", "Wonjong", ""], ["Ju", "Gwangjin", ""], ["Jung", "Yucheol", ""], ["Yang", "Jiaolong", ""], ["Tong", "Xin", ""], ["Lee", "Seungyong", ""]]}, {"id": "2107.04357", "submitter": "Sanket Biswas", "authors": "Sanket Biswas, Pau Riba, Josep Llad\\'os, and Umapada Pal", "title": "Graph-based Deep Generative Modelling for Document Layout Generation", "comments": "Accepted by ICDAR Workshops-GLESDO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the major prerequisites for any deep learning approach is the\navailability of large-scale training data. When dealing with scanned document\nimages in real world scenarios, the principal information of its content is\nstored in the layout itself. In this work, we have proposed an automated deep\ngenerative model using Graph Neural Networks (GNNs) to generate synthetic data\nwith highly variable and plausible document layouts that can be used to train\ndocument interpretation systems, in this case, specially in digital mailroom\napplications. It is also the first graph-based approach for document layout\ngeneration task experimented on administrative document images, in this case,\ninvoices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 10:49:49 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Biswas", "Sanket", ""], ["Riba", "Pau", ""], ["Llad\u00f3s", "Josep", ""], ["Pal", "Umapada", ""]]}, {"id": "2107.04362", "submitter": "Chenhao Wang", "authors": "Chenhao Wang, Hongxiang Cai, Yuxin Zou, Yichao Xiong", "title": "RGB Stream Is Enough for Temporal Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art temporal action detectors to date are based on two-stream\ninput including RGB frames and optical flow. Although combining RGB frames and\noptical flow boosts performance significantly, optical flow is a hand-designed\nrepresentation which not only requires heavy computation, but also makes it\nmethodologically unsatisfactory that two-stream methods are often not learned\nend-to-end jointly with the flow. In this paper, we argue that optical flow is\ndispensable in high-accuracy temporal action detection and image level data\naugmentation (ILDA) is the key solution to avoid performance degradation when\noptical flow is removed. To evaluate the effectiveness of ILDA, we design a\nsimple yet efficient one-stage temporal action detector based on single RGB\nstream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has\ncomparable accuracy with all existing state-of-the-art two-stream detectors\nwhile surpassing the inference speed of previous methods by a large margin and\nthe inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is\navailable at \\url{https://github.com/Media-Smart/vedatad}.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 11:10:11 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wang", "Chenhao", ""], ["Cai", "Hongxiang", ""], ["Zou", "Yuxin", ""], ["Xiong", "Yichao", ""]]}, {"id": "2107.04386", "submitter": "Shaowu Chen", "authors": "Shaowu Chen, Jiahao Zhou, Weize Sun, Lei Huang", "title": "Joint Matrix Decomposition for Deep Convolutional Neural Networks\n  Compression", "comments": "Code is publicly available on GitHub:\n  https://github.com/ShaowuChen/JointSVD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) with a large number of parameters\nrequires huge computational resources, which has limited the application of\nCNNs on resources constrained appliances. Decomposition-based methods,\ntherefore, have been utilized to compress CNNs in recent years. However, since\nthe compression factor and performance are negatively correlated, the\nstate-of-the-art works either suffer from severe performance degradation or\nhave limited low compression factors. To overcome these problems, unlike\nprevious works compressing layers separately, we propose to compress CNNs and\nalleviate performance degradation via joint matrix decomposition. The idea is\ninspired by the fact that there are lots of repeated modules in CNNs, and by\nprojecting weights with the same structures into the same subspace, networks\ncan be further compressed and even accelerated. In particular, three joint\nmatrix decomposition schemes are developed, and the corresponding optimization\napproaches based on Singular Values Decomposition are proposed. Extensive\nexperiments are conducted across three challenging compact CNNs and 3 benchmark\ndata sets to demonstrate the superior performance of our proposed algorithms.\nAs a result, our methods can compress the size of ResNet-34 by 22x with\nslighter accuracy degradation compared with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 12:32:10 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 03:06:42 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Shaowu", ""], ["Zhou", "Jiahao", ""], ["Sun", "Weize", ""], ["Huang", "Lei", ""]]}, {"id": "2107.04388", "submitter": "Jessica Cooper", "authors": "Jessica Cooper, In Hwa Um, Ognjen Arandjelovi\\'c and David J Harrison", "title": "Hoechst Is All You Need: Lymphocyte Classification with Deep Learning", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiplex immunofluorescence and immunohistochemistry benefit patients by\nallowing cancer pathologists to identify several proteins expressed on the\nsurface of cells, enabling cell classification, better understanding of the\ntumour micro-environment, more accurate diagnoses, prognoses, and tailored\nimmunotherapy based on the immune status of individual patients. However, they\nare expensive and time consuming processes which require complex staining and\nimaging techniques by expert technicians. Hoechst staining is much cheaper and\neasier to perform, but is not typically used in this case as it binds to DNA\nrather than to the proteins targeted by immunofluorescent techniques, and it\nwas not previously thought possible to differentiate cells expressing these\nproteins based only on DNA morphology. In this work we show otherwise, training\na deep convolutional neural network to identify cells expressing three proteins\n(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with\ngreater than 90% precision and recall, from Hoechst 33342 stained tissue only.\nOur model learns previously unknown morphological features associated with\nexpression of these proteins which can be used to accurately differentiate\nlymphocyte subtypes for use in key prognostic metrics such as assessment of\nimmune cell infiltration,and thereby predict and improve patient outcomes\nwithout the need for costly multiplex immunofluorescence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 12:33:22 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 13:43:59 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Cooper", "Jessica", ""], ["Um", "In Hwa", ""], ["Arandjelovi\u0107", "Ognjen", ""], ["Harrison", "David J", ""]]}, {"id": "2107.04389", "submitter": "Zhilei Liu", "authors": "Chenggong Zhang and Juan Song and Qingyang Zhang and Weilong Dong and\n  Ruomeng Ding and Zhilei Liu", "title": "Action Unit Detection with Joint Adaptive Attention and Graph Relation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an approach to the facial action unit (AU) detection. In\nthis work, we present our submission to the Field Affective Behavior Analysis\n(ABAW) 2021 competition. The proposed method uses the pre-trained JAA model as\nthe feature extractor, and extracts global features, face alignment features\nand AU local features on the basis of multi-scale features. We take the AU\nlocal features as the input of the graph convolution to further consider the\ncorrelation between AU, and finally use the fused features to classify AU. The\ndetected accuracy was evaluated by 0.5*accuracy + 0.5*F1. Our model achieves\n0.674 on the challenging Aff-Wild2 database.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 12:33:38 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Chenggong", ""], ["Song", "Juan", ""], ["Zhang", "Qingyang", ""], ["Dong", "Weilong", ""], ["Ding", "Ruomeng", ""], ["Liu", "Zhilei", ""]]}, {"id": "2107.04396", "submitter": "Milan Aggarwal", "authors": "Milan Aggarwal, Mausoom Sarkar, Hiresh Gupta, Balaji Krishnamurthy", "title": "Multi-Modal Association based Grouping for Form Structure Extraction", "comments": "This work has been accepted and presented at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document structure extraction has been a widely researched area for decades.\nRecent work in this direction has been deep learning-based, mostly focusing on\nextracting structure using fully convolution NN through semantic segmentation.\nIn this work, we present a novel multi-modal approach for form structure\nextraction. Given simple elements such as textruns and widgets, we extract\nhigher-order structures such as TextBlocks, Text Fields, Choice Fields, and\nChoice Groups, which are essential for information collection in forms. To\nachieve this, we obtain a local image patch around each low-level element\n(reference) by identifying candidate elements closest to it. We process textual\nand spatial representation of candidates sequentially through a BiLSTM to\nobtain context-aware representations and fuse them with image patch features\nobtained by processing it through a CNN. Subsequently, the sequential decoder\ntakes this fused feature vector to predict the association type between\nreference and candidates. These predicted associations are utilized to\ndetermine larger structures through connected components analysis. Experimental\nresults show the effectiveness of our approach achieving a recall of 90.29%,\n73.80%, 83.12%, and 52.72% for the above structures, respectively,\noutperforming semantic segmentation baselines significantly. We show the\nefficacy of our method through ablations, comparing it against using individual\nmodalities. We also introduce our new rich human-annotated Forms Dataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 12:49:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Aggarwal", "Milan", ""], ["Sarkar", "Mausoom", ""], ["Gupta", "Hiresh", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2107.04435", "submitter": "Oliver De Candido", "authors": "Tobias Uelwer, Felix Michels, Oliver De Candido", "title": "Learning to Detect Adversarial Examples Based on Class Scores", "comments": "Accepted at the 44th German Conference on Artificial Intelligence (KI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the increasing threat of adversarial attacks on deep neural networks\n(DNNs), research on efficient detection methods is more important than ever. In\nthis work, we take a closer look at adversarial attack detection based on the\nclass scores of an already trained classification model. We propose to train a\nsupport vector machine (SVM) on the class scores to detect adversarial\nexamples. Our method is able to detect adversarial examples generated by\nvarious attacks, and can be easily adopted to a plethora of deep classification\nmodels. We show that our approach yields an improved detection rate compared to\nan existing method, whilst being easy to implement. We perform an extensive\nempirical analysis on different deep classification models, investigating\nvarious state-of-the-art adversarial attacks. Moreover, we observe that our\nproposed method is better at detecting a combination of adversarial attacks.\nThis work indicates the potential of detecting various adversarial attacks\nsimply by using the class scores of an already trained classification model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 13:29:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Uelwer", "Tobias", ""], ["Michels", "Felix", ""], ["De Candido", "Oliver", ""]]}, {"id": "2107.04440", "submitter": "Xiang Chen", "authors": "Xiang Chen, Nishant Ravikumar, Yan Xia, Alejandro F Frangi", "title": "A Deep Discontinuity-Preserving Image Registration Network", "comments": "Provisional accepted in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration aims to establish spatial correspondence across pairs, or\ngroups of images, and is a cornerstone of medical image computing and\ncomputer-assisted-interventions. Currently, most deep learning-based\nregistration methods assume that the desired deformation fields are globally\nsmooth and continuous, which is not always valid for real-world scenarios,\nespecially in medical image registration (e.g. cardiac imaging and abdominal\nimaging). Such a global constraint can lead to artefacts and increased errors\nat discontinuous tissue interfaces. To tackle this issue, we propose a\nweakly-supervised Deep Discontinuity-preserving Image Registration network\n(DDIR), to obtain better registration performance and realistic deformation\nfields. We demonstrate that our method achieves significant improvements in\nregistration accuracy and predicts more realistic deformations, in registration\nexperiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging\nStudy (UKBB), than state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 13:35:59 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Chen", "Xiang", ""], ["Ravikumar", "Nishant", ""], ["Xia", "Yan", ""], ["Frangi", "Alejandro F", ""]]}, {"id": "2107.04452", "submitter": "Xiaoxue Zang", "authors": "Xiaoxue Zang, Ying Xu, Jindong Chen", "title": "Multimodal Icon Annotation For Mobile Applications", "comments": "11 pages, MobileHCI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Annotating user interfaces (UIs) that involves localization and\nclassification of meaningful UI elements on a screen is a critical step for\nmany mobile applications such as screen readers and voice control of devices.\nAnnotating object icons, such as menu, search, and arrow backward, is\nespecially challenging due to the lack of explicit labels on screens, their\nsimilarity to pictures, and their diverse shapes. Existing studies either use\nview hierarchy or pixel based methods to tackle the task. Pixel based\napproaches are more popular as view hierarchy features on mobile platforms are\noften incomplete or inaccurate, however it leaves out instructional information\nin the view hierarchy such as resource-ids or content descriptions. We propose\na novel deep learning based multi-modal approach that combines the benefits of\nboth pixel and view hierarchy features as well as leverages the\nstate-of-the-art object detection techniques. In order to demonstrate the\nutility provided, we create a high quality UI dataset by manually annotating\nthe most commonly used 29 icons in Rico, a large scale mobile design dataset\nconsisting of 72k UI screenshots. The experimental results indicate the\neffectiveness of our multi-modal approach. Our model not only outperforms a\nwidely used object classification baseline but also pixel based object\ndetection models. Our study sheds light on how to combine view hierarchy with\npixel features for annotating UI elements.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 13:57:37 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zang", "Xiaoxue", ""], ["Xu", "Ying", ""], ["Chen", "Jindong", ""]]}, {"id": "2107.04458", "submitter": "Eng-Jon Ong", "authors": "Eng-Jon Ong, Sameed Husain, Miroslaw Bober", "title": "Understanding the Distributions of Aggregation Layers in Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The process of aggregation is ubiquitous in almost all deep nets models. It\nfunctions as an important mechanism for consolidating deep features into a more\ncompact representation, whilst increasing robustness to overfitting and\nproviding spatial invariance in deep nets. In particular, the proximity of\nglobal aggregation layers to the output layers of DNNs mean that aggregated\nfeatures have a direct influence on the performance of a deep net. A better\nunderstanding of this relationship can be obtained using information theoretic\nmethods. However, this requires the knowledge of the distributions of the\nactivations of aggregation layers. To achieve this, we propose a novel\nmathematical formulation for analytically modelling the probability\ndistributions of output values of layers involved with deep feature\naggregation. An important outcome is our ability to analytically predict the\nKL-divergence of output nodes in a DNN. We also experimentally verify our\ntheoretical predictions against empirical observations across a range of\ndifferent classification tasks and datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 14:23:57 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Ong", "Eng-Jon", ""], ["Husain", "Sameed", ""], ["Bober", "Miroslaw", ""]]}, {"id": "2107.04461", "submitter": "Dario Fontanel", "authors": "Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Barbara Caputo", "title": "On the Challenges of Open World Recognitionunder Shifting Visual Domains", "comments": "RAL/ICRA 2021", "journal-ref": null, "doi": "10.1109/LRA.2020.3047777", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic visual systems operating in the wild must act in unconstrained\nscenarios, under different environmental conditions while facing a variety of\nsemantic concepts, including unknown ones. To this end, recent works tried to\nempower visual object recognition methods with the capability to i) detect\nunseen concepts and ii) extended their knowledge over time, as images of new\nsemantic classes arrive. This setting, called Open World Recognition (OWR), has\nthe goal to produce systems capable of breaking the semantic limits present in\nthe initial training set. However, this training set imposes to the system not\nonly its own semantic limits, but also environmental ones, due to its bias\ntoward certain acquisition conditions that do not necessarily reflect the high\nvariability of the real-world. This discrepancy between training and test\ndistribution is called domain-shift. This work investigates whether OWR\nalgorithms are effective under domain-shift, presenting the first benchmark\nsetup for assessing fairly the performances of OWR algorithms, with and without\ndomain-shift. We then use this benchmark to conduct analyses in various\nscenarios, showing how existing OWR algorithms indeed suffer a severe\nperformance degradation when train and test distributions differ. Our analysis\nshows that this degradation is only slightly mitigated by coupling OWR with\ndomain generalization techniques, indicating that the mere plug-and-play of\nexisting algorithms is not enough to recognize new and unknown categories in\nunseen domains. Our results clearly point toward open issues and future\nresearch directions, that need to be investigated for building robot visual\nsystems able to function reliably under these challenging yet very real\nconditions. Code available at\nhttps://github.com/DarioFontanel/OWR-VisualDomains\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 14:25:45 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Fontanel", "Dario", ""], ["Cermelli", "Fabio", ""], ["Mancini", "Massimiliano", ""], ["Caputo", "Barbara", ""]]}, {"id": "2107.04474", "submitter": "Wen Shen", "authors": "Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Jiaqi Fan, Ping\n  Zhao, Quanshi Zhang", "title": "Interpretable Compositional Convolutional Neural Networks", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reasonable definition of semantic interpretability presents the core\nchallenge in explainable AI. This paper proposes a method to modify a\ntraditional convolutional neural network (CNN) into an interpretable\ncompositional CNN, in order to learn filters that encode meaningful visual\npatterns in intermediate convolutional layers. In a compositional CNN, each\nfilter is supposed to consistently represent a specific compositional object\npart or image region with a clear meaning. The compositional CNN learns from\nimage labels for classification without any annotations of parts or regions for\nsupervision. Our method can be broadly applied to different types of CNNs.\nExperiments have demonstrated the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:01:24 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Shen", "Wen", ""], ["Wei", "Zhihua", ""], ["Huang", "Shikun", ""], ["Zhang", "Binbin", ""], ["Fan", "Jiaqi", ""], ["Zhao", "Ping", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2107.04476", "submitter": "Alexander Sch\\\"afer", "authors": "Alexander Sch\\\"afer, Tomoko Isomura, Gerd Reis, Katsumi Watanabe,\n  Didier Stricker", "title": "MutualEyeContact: A conversation analysis tool with focus on eye contact", "comments": null, "journal-ref": null, "doi": "10.1145/3379156.3391340", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye contact between individuals is particularly important for understanding\nhuman behaviour. To further investigate the importance of eye contact in social\ninteractions, portable eye tracking technology seems to be a natural choice.\nHowever, the analysis of available data can become quite complex. Scientists\nneed data that is calculated quickly and accurately. Additionally, the relevant\ndata must be automatically separated to save time. In this work, we propose a\ntool called MutualEyeContact which excels in those tasks and can help\nscientists to understand the importance of (mutual) eye contact in social\ninteractions. We combine state-of-the-art eye tracking with face recognition\nbased on machine learning and provide a tool for analysis and visualization of\nsocial interaction sessions. This work is a joint collaboration of computer\nscientists and cognitive scientists. It combines the fields of social and\nbehavioural science with computer vision and deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:05:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sch\u00e4fer", "Alexander", ""], ["Isomura", "Tomoko", ""], ["Reis", "Gerd", ""], ["Watanabe", "Katsumi", ""], ["Stricker", "Didier", ""]]}, {"id": "2107.04481", "submitter": "Mustafa Shukor", "authors": "Mustafa Shukor, Xu Yao, Bharath Bhushan Damodaran, Pierre Hellier", "title": "Semantic and Geometric Unfolding of StyleGAN Latent Space", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have proven to be surprisingly\nefficient for image editing by inverting and manipulating the latent code\ncorresponding to a natural image. This property emerges from the disentangled\nnature of the latent space. In this paper, we identify two geometric\nlimitations of such latent space: (a) euclidean distances differ from image\nperceptual distance, and (b) disentanglement is not optimal and facial\nattribute separation using linear model is a limiting hypothesis. We thus\npropose a new method to learn a proxy latent representation using normalizing\nflows to remedy these limitations, and show that this leads to a more efficient\nspace for face image editing.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:12:55 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Shukor", "Mustafa", ""], ["Yao", "Xu", ""], ["Damodaran", "Bharath Bhushan", ""], ["Hellier", "Pierre", ""]]}, {"id": "2107.04510", "submitter": "Anastasia Zvezdakova", "authors": "Maksim Siniukov, Anastasia Antsiferova, Dmitriy Kulikov, Dmitriy\n  Vatolin", "title": "Hacking VMAF and VMAF NEG: metrics vulnerability to different\n  preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality measurement plays a critical role in the development of video\nprocessing applications. In this paper, we show how popular quality metrics\nVMAF and its tuning-resistant version VMAF NEG can be artificially increased by\nvideo preprocessing. We propose a pipeline for tuning parameters of processing\nalgorithms that allows increasing VMAF by up to 218.8%. A subjective comparison\nof preprocessed videos showed that with the majority of methods visual quality\ndrops down or stays unchanged. We show that VMAF NEG scores can also be\nincreased by some preprocessing methods by up to 23.6%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:53:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Siniukov", "Maksim", ""], ["Antsiferova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "2107.04517", "submitter": "Tobias Riedlinger", "authors": "Tobias Riedlinger, Matthias Rottmann, Marius Schubert, Hanno\n  Gottschalk", "title": "Gradient-Based Quantification of Epistemic Uncertainty for Deep Object\n  Detectors", "comments": "20 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliable epistemic uncertainty estimation is an essential component for\nbackend applications of deep object detectors in safety-critical environments.\nModern network architectures tend to give poorly calibrated confidences with\nlimited predictive power. Here, we introduce novel gradient-based uncertainty\nmetrics and investigate them for different object detection architectures.\nExperiments on the MS COCO, PASCAL VOC and the KITTI dataset show significant\nimprovements in true positive / false positive discrimination and prediction of\nintersection over union as compared to network confidence. We also find\nimprovement over Monte-Carlo dropout uncertainty metrics and further\nsignificant boosts by aggregating different sources of uncertainty metrics.The\nresulting uncertainty models generate well-calibrated confidences in all\ninstances. Furthermore, we implement our uncertainty quantification models into\nobject detection pipelines as a means to discern true against false\npredictions, replacing the ordinary score-threshold-based decision rule. In our\nexperiments, we achieve a significant boost in detection performance in terms\nof mean average precision. With respect to computational complexity, we find\nthat computing gradient uncertainty metrics results in floating point operation\ncounts similar to those of Monte-Carlo dropout.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:04:11 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Riedlinger", "Tobias", ""], ["Rottmann", "Matthias", ""], ["Schubert", "Marius", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "2107.04523", "submitter": "Niklas Hanselmann", "authors": "Niklas Hanselmann, Nick Schneider, Benedikt Ortelt and Andreas Geiger", "title": "Learning Cascaded Detection Tasks with Weakly-Supervised Domain\n  Adaptation", "comments": "Accepted to IEEE IV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to handle the challenges of autonomous driving, deep learning has\nproven to be crucial in tackling increasingly complex tasks, such as 3D\ndetection or instance segmentation. State-of-the-art approaches for image-based\ndetection tasks tackle this complexity by operating in a cascaded fashion: they\nfirst extract a 2D bounding box based on which additional attributes, e.g.\ninstance masks, are inferred. While these methods perform well, a key challenge\nremains the lack of accurate and cheap annotations for the growing variety of\ntasks. Synthetic data presents a promising solution but, despite the effort in\ndomain adaptation research, the gap between synthetic and real data remains an\nopen problem. In this work, we propose a weakly supervised domain adaptation\nsetting which exploits the structure of cascaded detection tasks. In\nparticular, we learn to infer the attributes solely from the source domain\nwhile leveraging 2D bounding boxes as weak labels in both domains to explain\nthe domain shift. We further encourage domain-invariant features through\nclass-wise feature alignment using ground-truth class information, which is not\navailable in the unsupervised setting. As our experiments demonstrate, the\napproach is competitive with fully supervised settings while outperforming\nunsupervised adaptation approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:18:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Hanselmann", "Niklas", ""], ["Schneider", "Nick", ""], ["Ortelt", "Benedikt", ""], ["Geiger", "Andreas", ""]]}, {"id": "2107.04536", "submitter": "Pak Hong Chui", "authors": "Jason Chui, Simon Klenk, Daniel Cremers", "title": "Event-Based Feature Tracking in Continuous Time with Sliding Window\n  Optimization", "comments": "9 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel method for continuous-time feature tracking in event\ncameras. To this end, we track features by aligning events along an estimated\ntrajectory in space-time such that the projection on the image plane results in\nmaximally sharp event patch images. The trajectory is parameterized by $n^{th}$\norder B-splines, which are continuous up to $(n-2)^{th}$ derivative. In\ncontrast to previous work, we optimize the curve parameters in a sliding window\nfashion. On a public dataset we experimentally confirm that the proposed\nsliding-window B-spline optimization leads to longer and more accurate feature\ntracks than in previous work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:41:20 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Chui", "Jason", ""], ["Klenk", "Simon", ""], ["Cremers", "Daniel", ""]]}, {"id": "2107.04537", "submitter": "Narinder Singh Punn", "authors": "Narinder Singh Punn, Sonali Agarwal", "title": "Modality specific U-Net variants for biomedical image segmentation: A\n  survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of advancements in deep learning approaches, such as deep\nconvolution neural network, residual neural network, adversarial network; U-Net\narchitectures are most widely utilized in biomedical image segmentation to\naddress the automation in identification and detection of the target regions or\nsub-regions. In recent studies, U-Net based approaches have illustrated\nstate-of-the-art performance in different applications for the development of\ncomputer-aided diagnosis systems for early diagnosis and treatment of diseases\nsuch as brain tumor, lung cancer, alzheimer, breast cancer, etc. This article\ncontributes to present the success of these approaches by describing the U-Net\nframework, followed by the comprehensive analysis of the U-Net variants for\ndifferent medical imaging or modalities such as magnetic resonance imaging,\nX-ray, computerized tomography/computerized axial tomography, ultrasound,\npositron emission tomography, etc. Besides, this article also highlights the\ncontribution of U-Net based frameworks in the on-going pandemic, severe acute\nrespiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:41:40 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Punn", "Narinder Singh", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2107.04548", "submitter": "Xinrui Song", "authors": "Xinrui Song, Hengtao Guo, Xuanang Xu, Hanqing Chao, Sheng Xu, Baris\n  Turkbey, Bradford J. Wood, Ge Wang, Pingkun Yan", "title": "Cross-modal Attention for MRI and Ultrasound Volume Registration", "comments": "This paper has been accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer biopsy benefits from accurate fusion of transrectal\nultrasound (TRUS) and magnetic resonance (MR) images. In the past few years,\nconvolutional neural networks (CNNs) have been proved powerful in extracting\nimage features crucial for image registration. However, challenging\napplications and recent advances in computer vision suggest that CNNs are quite\nlimited in its ability to understand spatial correspondence between features, a\ntask in which the self-attention mechanism excels. This paper aims to develop a\nself-attention mechanism specifically for cross-modal image registration. Our\nproposed cross-modal attention block effectively maps each of the features in\none volume to all features in the corresponding volume. Our experimental\nresults demonstrate that a CNN network designed with the cross-modal attention\nblock embedded outperforms an advanced CNN network 10 times of its size. We\nalso incorporated visualization techniques to improve the interpretability of\nour network. The source code of our work is available at\nhttps://github.com/DIAL-RPI/Attention-Reg .\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:04:02 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 01:46:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Song", "Xinrui", ""], ["Guo", "Hengtao", ""], ["Xu", "Xuanang", ""], ["Chao", "Hanqing", ""], ["Xu", "Sheng", ""], ["Turkbey", "Baris", ""], ["Wood", "Bradford J.", ""], ["Wang", "Ge", ""], ["Yan", "Pingkun", ""]]}, {"id": "2107.04551", "submitter": "Amey Thakur", "authors": "Amey Thakur, Hasan Rizvi, Mega Satish", "title": "White-Box Cartoonization Using An Extended GAN Framework", "comments": "5 pages, 6 figures. International Journal of Engineering Applied\n  Sciences and Technology, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present study, we propose to implement a new framework for estimating\ngenerative models via an adversarial process to extend an existing GAN\nframework and develop a white-box controllable image cartoonization, which can\ngenerate high-quality cartooned images/videos from real-world photos and\nvideos. The learning purposes of our system are based on three distinct\nrepresentations: surface representation, structure representation, and texture\nrepresentation. The surface representation refers to the smooth surface of the\nimages. The structure representation relates to the sparse colour blocks and\ncompresses generic content. The texture representation shows the texture,\ncurves, and features in cartoon images. Generative Adversarial Network (GAN)\nframework decomposes the images into different representations and learns from\nthem to generate cartoon images. This decomposition makes the framework more\ncontrollable and flexible which allows users to make changes based on the\nrequired output. This approach overcomes any previous system in terms of\nmaintaining clarity, colours, textures, shapes of images yet showing the\ncharacteristics of cartoon images.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:09:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Thakur", "Amey", ""], ["Rizvi", "Hasan", ""], ["Satish", "Mega", ""]]}, {"id": "2107.04569", "submitter": "Satnam Singh", "authors": "Satnam Singh, Doris Schicker", "title": "Seven Basic Expression Recognition Using ResNet-18", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use a ResNet-18 architecture that was pre-trained on the FER+\ndataset for tackling the problem of affective behavior analysis in-the-wild\n(ABAW) for classification of the seven basic expressions, namely, neutral,\nanger, disgust, fear, happiness, sadness and surprise. As part of the second\nworkshop and competition on affective behavior analysis in-the-wild (ABAW2), a\ndatabase consisting of 564 videos with around 2.8M frames is provided along\nwith labels for these seven basic expressions. We resampled the dataset to\ncounter class-imbalances by under-sampling the over-represented classes and\nover-sampling the under-represented classes along with class-wise weights. To\navoid overfitting we performed data-augmentation and used L2 regularisation.\nOur classifier reaches an ABAW2 score of 0.4 and therefore exceeds the baseline\nresults provided by the hosts of the competition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:40:57 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Singh", "Satnam", ""], ["Schicker", "Doris", ""]]}, {"id": "2107.04570", "submitter": "Francisco Eiras", "authors": "Francisco Eiras, Motasem Alfarra, M. Pawan Kumar, Philip H. S. Torr,\n  Puneet K. Dokania, Bernard Ghanem, Adel Bibi", "title": "ANCER: Anisotropic Certification via Sample-wise Volume Maximization", "comments": "First two authors and the last one contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, i.e., it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Code used in our experiments is available in\nhttps://github.com/MotasemAlfarra/ANCER.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:42:38 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 10:08:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Eiras", "Francisco", ""], ["Alfarra", "Motasem", ""], ["Kumar", "M. Pawan", ""], ["Torr", "Philip H. S.", ""], ["Dokania", "Puneet K.", ""], ["Ghanem", "Bernard", ""], ["Bibi", "Adel", ""]]}, {"id": "2107.04589", "submitter": "Kwonjoon Lee", "authors": "Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu", "title": "ViTGAN: Training GANs with Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Vision Transformers (ViTs) have shown competitive performance on\nimage recognition while requiring less vision-specific inductive biases. In\nthis paper, we investigate if such observation can be extended to image\ngeneration. To this end, we integrate the ViT architecture into generative\nadversarial networks (GANs). We observe that existing regularization methods\nfor GANs interact poorly with self-attention, causing serious instability\nduring training. To resolve this issue, we introduce novel regularization\ntechniques for training GANs with ViTs. Empirically, our approach, named\nViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2\non CIFAR-10, CelebA, and LSUN bedroom datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:59:30 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Lee", "Kwonjoon", ""], ["Chang", "Huiwen", ""], ["Jiang", "Lu", ""], ["Zhang", "Han", ""], ["Tu", "Zhuowen", ""], ["Liu", "Ce", ""]]}, {"id": "2107.04618", "submitter": "Reshad Hosseini", "authors": "Seyed-Mahdi Nasiri, Reshad Hosseini, Hadi Moradi", "title": "Optimal Triangulation Method is Not Really Optimal", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triangulation refers to the problem of finding a 3D point from its 2D\nprojections on multiple camera images. For solving this problem, it is the\ncommon practice to use so-called optimal triangulation method, which we call\nthe L2 method in this paper. But, the method can be optimal only if we assume\nno uncertainty in the camera parameters. Through extensive comparison on\nsynthetic and real data, we observed that the L2 method is actually not the\nbest choice when there is uncertainty in the camera parameters. Interestingly,\nit can be observed that the simple mid-point method outperforms other methods.\nApart from its high performance, the mid-point method has a simple closed\nformed solution for multiple camera images while the L2 method is hard to be\nused for more than two camera images. Therefore, in contrast to the common\npractice, we argue that the simple mid-point method should be used in\nstructure-from-motion applications where there is uncertainty in camera\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 18:14:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nasiri", "Seyed-Mahdi", ""], ["Hosseini", "Reshad", ""], ["Moradi", "Hadi", ""]]}, {"id": "2107.04619", "submitter": "Gaurav Shrivastava", "authors": "Gaurav Shrivastava and Abhinav Shrivastava", "title": "Diverse Video Generation using a Gaussian Process Trigger", "comments": "International Conference on Learning Representations, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generating future frames given a few context (or past) frames is a\nchallenging task. It requires modeling the temporal coherence of videos and\nmulti-modality in terms of diversity in the potential future states. Current\nvariational approaches for video generation tend to marginalize over\nmulti-modal future outcomes. Instead, we propose to explicitly model the\nmulti-modality in the future outcomes and leverage it to sample diverse\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\nlearn priors on future states given the past and maintains a probability\ndistribution over possible futures given a particular sample. In addition, we\nleverage the changes in this distribution over time to control the sampling of\ndiverse future states by estimating the end of ongoing sequences. That is, we\nuse the variance of GP over the output function space to trigger a change in an\naction sequence. We achieve state-of-the-art results on diverse future frame\ngeneration in terms of reconstruction quality and diversity of the generated\nsequences.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 18:15:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shrivastava", "Gaurav", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2107.04622", "submitter": "Myron Brown", "authors": "Shea Hagstrom, Hee Won Pak, Stephanie Ku, Sean Wang, Gregory Hager,\n  Myron Brown", "title": "Cumulative Assessment for Urban 3D Modeling", "comments": "Published in IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban 3D modeling from satellite images requires accurate semantic\nsegmentation to delineate urban features, multiple view stereo for 3D\nreconstruction of surface heights, and 3D model fitting to produce compact\nmodels with accurate surface slopes. In this work, we present a cumulative\nassessment metric that succinctly captures error contributions from each of\nthese components. We demonstrate our approach by providing challenging public\ndatasets and extending two open source projects to provide an end-to-end 3D\nmodeling baseline solution to stimulate further research and evaluation with a\npublic leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 18:29:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hagstrom", "Shea", ""], ["Pak", "Hee Won", ""], ["Ku", "Stephanie", ""], ["Wang", "Sean", ""], ["Hager", "Gregory", ""], ["Brown", "Myron", ""]]}, {"id": "2107.04644", "submitter": "Baoru Huang", "authors": "Baoru Huang, Jianqing Zheng, Anh Nguyen, David Tuch, Kunal Vyas,\n  Stamatia Giannarou, Daniel S. Elson", "title": "Self-Supervised Generative Adversarial Network for Depth Estimation in\n  Laparoscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth estimation and 3D reconstruction of a surgical scene are crucial\nsteps in computer assisted surgery. Recent work has shown that depth estimation\nfrom a stereo images pair could be solved with convolutional neural networks.\nHowever, most recent depth estimation models were trained on datasets with\nper-pixel ground truth. Such data is especially rare for laparoscopic imaging,\nmaking it hard to apply supervised depth estimation to real surgical\napplications. To overcome this limitation, we propose SADepth, a new\nself-supervised depth estimation method based on Generative Adversarial\nNetworks. It consists of an encoder-decoder generator and a discriminator to\nincorporate geometry constraints during training. Multi-scale outputs from the\ngenerator help to solve the local minima caused by the photometric reprojection\nloss, while the adversarial learning improves the framework generation quality.\nExtensive experiments on two public datasets show that SADepth outperforms\nrecent state-of-the-art unsupervised methods by a large margin, and reduces the\ngap between supervised and unsupervised depth estimation in laparoscopic\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:40:20 GMT"}], "update_date": "2021-07-24", "authors_parsed": [["Huang", "Baoru", ""], ["Zheng", "Jianqing", ""], ["Nguyen", "Anh", ""], ["Tuch", "David", ""], ["Vyas", "Kunal", ""], ["Giannarou", "Stamatia", ""], ["Elson", "Daniel S.", ""]]}, {"id": "2107.04648", "submitter": "Marwan Dhuheir", "authors": "Marwan Dhuheir, Emna Baccour, Aiman Erbad, Sinan Sabeeh, Mounir Hamdi", "title": "Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs\n  and Convolutional Networks", "comments": "conference paper accepted and presented at 17th Int. Wireless\n  Communications & Mobile Computing Conference - IWCMC 2021, Harbin, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention\ndue to their outstanding ability to be used in different sectors and serve in\ndifficult and dangerous areas. Moreover, the advancements in computer vision\nand artificial intelligence have increased the use of UAVs in various\napplications and solutions, such as forest fires detection and borders\nmonitoring. However, using deep neural networks (DNNs) with UAVs introduces\nseveral challenges of processing deeper networks and complex models, which\nrestricts their on-board computation. In this work, we present a strategy\naiming at distributing inference requests to a swarm of resource-constrained\nUAVs that classifies captured images on-board and finds the minimum\ndecision-making latency. We formulate the model as an optimization problem that\nminimizes the latency between acquiring images and making the final decisions.\nThe formulated optimization solution is an NP-hard problem. Hence it is not\nadequate for online resource allocation. Therefore, we introduce an online\nheuristic solution, namely DistInference, to find the layers placement strategy\nthat gives the best latency among the available UAVs. The proposed approach is\ngeneral enough to be used for different low decision-latency applications as\nwell as for all CNN types organized into the pipeline of layers (e.g., VGG) or\nbased on residual blocks (e.g., ResNet).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:47:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dhuheir", "Marwan", ""], ["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Sabeeh", "Sinan", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2107.04688", "submitter": "Lihan Yao", "authors": "Richard Lau, Lihan Yao, Todd Huster, William Johnson, Stephen Arleth,\n  Justin Wong, Devin Ridge, Michael Fletcher, William C. Headley", "title": "Scaled-Time-Attention Robust Edge Network", "comments": "20 pages, 22 figures, 9 tables, Darpa Distribution Statement A.\n  Approved for public release. Distribution Unlimited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a systematic approach towards building a new family of\nneural networks based on a delay-loop version of a reservoir neural network.\nThe resulting architecture, called Scaled-Time-Attention Robust Edge (STARE)\nnetwork, exploits hyper dimensional space and non-multiply-and-add computation\nto achieve a simpler architecture, which has shallow layers, is simple to\ntrain, and is better suited for Edge applications, such as Internet of Things\n(IoT), over traditional deep neural networks. STARE incorporates new AI\nconcepts such as Attention and Context, and is best suited for temporal feature\nextraction and classification. We demonstrate that STARE is applicable to a\nvariety of applications with improved performance and lower implementation\ncomplexity. In particular, we showed a novel way of applying a dual-loop\nconfiguration to detection and identification of drone vs bird in a counter\nUnmanned Air Systems (UAS) detection application by exploiting both spatial\n(video frame) and temporal (trajectory) information. We also demonstrated that\nthe STARE performance approaches that of a State-of-the-Art deep neural network\nin classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in\na special case of Mackey Glass time series prediction. To demonstrate hardware\nefficiency, we designed and developed an FPGA implementation of the STARE\nalgorithm to demonstrate its low-power and high-throughput operations. In\naddition, we illustrate an efficient structure for integrating a massively\nparallel implementation of the STARE algorithm for ASIC implementation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 21:24:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lau", "Richard", ""], ["Yao", "Lihan", ""], ["Huster", "Todd", ""], ["Johnson", "William", ""], ["Arleth", "Stephen", ""], ["Wong", "Justin", ""], ["Ridge", "Devin", ""], ["Fletcher", "Michael", ""], ["Headley", "William C.", ""]]}, {"id": "2107.04689", "submitter": "Fei Ye", "authors": "Fei Ye and Adrian G. Bors", "title": "Lifelong Teacher-Student Network Learning", "comments": "18 pages, 18 figures. in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3092677", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A unique cognitive capability of humans consists in their ability to acquire\nnew knowledge and skills from a sequence of experiences. Meanwhile, artificial\nintelligence systems are good at learning only the last given task without\nbeing able to remember the databases learnt in the past. We propose a novel\nlifelong learning methodology by employing a Teacher-Student network framework.\nWhile the Student module is trained with a new given database, the Teacher\nmodule would remind the Student about the information learnt in the past. The\nTeacher, implemented by a Generative Adversarial Network (GAN), is trained to\npreserve and replay past knowledge corresponding to the probabilistic\nrepresentations of previously learn databases. Meanwhile, the Student module is\nimplemented by a Variational Autoencoder (VAE) which infers its latent variable\nrepresentation from both the output of the Teacher module as well as from the\nnewly available database. Moreover, the Student module is trained to capture\nboth continuous and discrete underlying data representations across different\ndomains. The proposed lifelong learning framework is applied in supervised,\nsemi-supervised and unsupervised training. The code is available~:\n\\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 21:25:56 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ye", "Fei", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2107.04694", "submitter": "Fei Ye", "authors": "Fei Ye and Adrian G. Bors", "title": "Lifelong Mixture of Variational Autoencoders", "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose an end-to-end lifelong learning mixture of experts.\nEach expert is implemented by a Variational Autoencoder (VAE). The experts in\nthe mixture system are jointly trained by maximizing a mixture of individual\ncomponent evidence lower bounds (MELBO) on the log-likelihood of the given\ntraining samples. The mixing coefficients in the mixture, control the\ncontributions of each expert in the goal representation. These are sampled from\na Dirichlet distribution whose parameters are determined through non-parametric\nestimation during lifelong learning. The model can learn new tasks fast when\nthese are similar to those previously learnt. The proposed Lifelong mixture of\nVAE (L-MVAE) expands its architecture with new components when learning a\ncompletely new task. After the training, our model can automatically determine\nthe relevant expert to be used when fed with new data samples. This mechanism\nbenefits both the memory efficiency and the required computational cost as only\none expert is used during the inference. The L-MVAE inference model is able to\nperform interpolation in the joint latent space across the data domains\nassociated with different tasks and is shown to be efficient for disentangled\nlearning representation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:07:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ye", "Fei", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2107.04705", "submitter": "Fei Ye", "authors": "Fei Ye and Adrian G. Bors", "title": "InfoVAEGAN : learning joint interpretable representations by information\n  maximization and maximum likelihood", "comments": "Accepted at International Conference on Image Processing (ICIP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning disentangled and interpretable representations is an important step\ntowards accomplishing comprehensive data representations on the manifold. In\nthis paper, we propose a novel representation learning algorithm which combines\nthe inference abilities of Variational Autoencoders (VAE) with the\ngeneralization capability of Generative Adversarial Networks (GAN). The\nproposed model, called InfoVAEGAN, consists of three networks~: Encoder,\nGenerator and Discriminator. InfoVAEGAN aims to jointly learn discrete and\ncontinuous interpretable representations in an unsupervised manner by using two\ndifferent data-free log-likelihood functions onto the variables sampled from\nthe generator's distribution. We propose a two-stage algorithm for optimizing\nthe inference network separately from the generator training. Moreover, we\nenforce the learning of interpretable representations through the maximization\nof the mutual information between the existing latent variables and those\ncreated through generative and inference processes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:38:10 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ye", "Fei", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2107.04708", "submitter": "Fei Ye", "authors": "Fei Ye and Adrian G. Bors", "title": "Lifelong Twin Generative Adversarial Networks", "comments": "Accepted at International Conference on Image Processing (ICIP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a new continuously learning generative model,\ncalled the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs\nlearns a sequence of tasks from several databases and its architecture consists\nof three components: two identical generators, namely the Teacher and\nAssistant, and one Discriminator. In order to allow for the LT-GANs to learn\nnew concepts without forgetting, we introduce a new lifelong training approach,\nnamely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the\nTeacher and Assistant to alternately teach each other, while learning a new\ndatabase. This training approach favours transferring knowledge from a more\nknowledgeable player to another player which knows less information about a\npreviously given task.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:52:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ye", "Fei", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2107.04713", "submitter": "Ronghang Zhu", "authors": "Ronghang Zhu and Zhiqiang Tao and Yaliang Li and Sheng Li", "title": "Automated Graph Learning via Population Based Self-Tuning GCN", "comments": "This manuscript has been accepted by the SIGIR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Owing to the remarkable capability of extracting effective graph embeddings,\ngraph convolutional network (GCN) and its variants have been successfully\napplied to a broad range of tasks, such as node classification, link\nprediction, and graph classification. Traditional GCN models suffer from the\nissues of overfitting and oversmoothing, while some recent techniques like\nDropEdge could alleviate these issues and thus enable the development of deep\nGCN. However, training GCN models is non-trivial, as it is sensitive to the\nchoice of hyperparameters such as dropout rate and learning weight decay,\nespecially for deep GCN models. In this paper, we aim to automate the training\nof GCN models through hyperparameter optimization. To be specific, we propose a\nself-tuning GCN approach with an alternate training algorithm, and further\nextend our approach by incorporating the population based training scheme.\nExperimental results on three benchmark datasets demonstrate the effectiveness\nof our approaches on optimizing multi-layer GCN, compared with several\nrepresentative baselines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 23:05:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhu", "Ronghang", ""], ["Tao", "Zhiqiang", ""], ["Li", "Yaliang", ""], ["Li", "Sheng", ""]]}, {"id": "2107.04714", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Colby Wight, Sarah Akers, Scott Howland, Woongjo Choi,\n  Xiaolong Ma, Luke Gosink, Elizabeth Jurrus, Keerti Kappagantula, Tegan H.\n  Emerson", "title": "A Topological-Framework to Improve Analysis of Machine Learning Model\n  Performance", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As both machine learning models and the datasets on which they are evaluated\nhave grown in size and complexity, the practice of using a few summary\nstatistics to understand model performance has become increasingly problematic.\nThis is particularly true in real-world scenarios where understanding model\nfailure on certain subpopulations of the data is of critical importance. In\nthis paper we propose a topological framework for evaluating machine learning\nmodels in which a dataset is treated as a \"space\" on which a model operates.\nThis provides us with a principled way to organize information about model\nperformance at both the global level (over the entire test set) and also the\nlocal level (on specific subpopulations). Finally, we describe a topological\ndata structure, presheaves, which offer a convenient way to store and analyze\nmodel performance between different subpopulations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 23:11:13 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kvinge", "Henry", ""], ["Wight", "Colby", ""], ["Akers", "Sarah", ""], ["Howland", "Scott", ""], ["Choi", "Woongjo", ""], ["Ma", "Xiaolong", ""], ["Gosink", "Luke", ""], ["Jurrus", "Elizabeth", ""], ["Kappagantula", "Keerti", ""], ["Emerson", "Tegan H.", ""]]}, {"id": "2107.04715", "submitter": "Madhusudhanan Balasubramanian", "authors": "Ali Salehi, Madhusudhanan Balasubramanian", "title": "DDCNet: Deep Dilated Convolutional Neural Network for Dense Prediction", "comments": "32 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense pixel matching problems such as optical flow and disparity estimation\nare among the most challenging tasks in computer vision. Recently, several deep\nlearning methods designed for these problems have been successful. A\nsufficiently larger effective receptive field (ERF) and a higher resolution of\nspatial features within a network are essential for providing higher-resolution\ndense estimates. In this work, we present a systemic approach to design network\narchitectures that can provide a larger receptive field while maintaining a\nhigher spatial feature resolution. To achieve a larger ERF, we utilized dilated\nconvolutional layers. By aggressively increasing dilation rates in the deeper\nlayers, we were able to achieve a sufficiently larger ERF with a significantly\nfewer number of trainable parameters. We used optical flow estimation problem\nas the primary benchmark to illustrate our network design strategy. The\nbenchmark results (Sintel, KITTI, and Middlebury) indicate that our compact\nnetworks can achieve comparable performance in the class of lightweight\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 23:15:34 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Salehi", "Ali", ""], ["Balasubramanian", "Madhusudhanan", ""]]}, {"id": "2107.04721", "submitter": "Michael Beyeler", "authors": "Shuyun Tang, Ziming Qi, Jacob Granley and Michael Beyeler", "title": "U-Net with Hierarchical Bottleneck Attention for Landmark Detection in\n  Fundus Images of the Degenerated Retina", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fundus photography has routinely been used to document the presence and\nseverity of retinal degenerative diseases such as age-related macular\ndegeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical\npractice, for which the fovea and optic disc (OD) are important retinal\nlandmarks. However, the occurrence of lesions, drusen, and other retinal\nabnormalities during retinal degeneration severely complicates automatic\nlandmark detection and segmentation. Here we propose HBA-U-Net: a U-Net\nbackbone enriched with hierarchical bottleneck attention. The network consists\nof a novel bottleneck attention block that combines and refines self-attention,\nchannel attention, and relative-position attention to highlight retinal\nabnormalities that may be important for fovea and OD segmentation in the\ndegenerated retina. HBA-U-Net achieved state-of-the-art results on fovea\ndetection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of\n25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for\nAMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:\nED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for\nlandmark detection in the presence of a variety of retinal degenerative\ndiseases.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 23:57:51 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tang", "Shuyun", ""], ["Qi", "Ziming", ""], ["Granley", "Jacob", ""], ["Beyeler", "Michael", ""]]}, {"id": "2107.04735", "submitter": "Jinpeng Li", "authors": "Jinpeng Li, Yichao Yan, Shengcai Liao, Xiaokang Yang, Ling Shao", "title": "Local-to-Global Self-Attention in Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have demonstrated great potential in computer vision tasks. To\navoid dense computations of self-attentions in high-resolution visual data,\nsome recent Transformer models adopt a hierarchical design, where\nself-attentions are only computed within local windows. This design\nsignificantly improves the efficiency but lacks global feature reasoning in\nearly stages. In this work, we design a multi-path structure of the\nTransformer, which enables local-to-global reasoning at multiple granularities\nin each stage. The proposed framework is computationally efficient and highly\neffective. With a marginal increasement in computational overhead, our model\nachieves notable improvements in both image classification and semantic\nsegmentation. Code is available at https://github.com/ljpadam/LG-Transformer\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 02:34:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Jinpeng", ""], ["Yan", "Yichao", ""], ["Liao", "Shengcai", ""], ["Yang", "Xiaokang", ""], ["Shao", "Ling", ""]]}, {"id": "2107.04746", "submitter": "Darshan Gera", "authors": "Darshan Gera, S. Balasubramanian", "title": "Consensual Collaborative Training And Knowledge Distillation Based\n  Facial Expression Recognition Under Noisy Annotations", "comments": "11 pages, 6 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT), Codes:\n  https://github.com/1980x/CCT", "journal-ref": "International Journal of Engineering Trends and Technology\n  69.7(2021):244-254", "doi": "10.14445/22315381/IJETT-V69I7P231", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presence of noise in the labels of large scale facial expression datasets has\nbeen a key challenge towards Facial Expression Recognition (FER) in the wild.\nDuring early learning stage, deep networks fit on clean data. Then, eventually,\nthey start overfitting on noisy labels due to their memorization ability, which\nlimits FER performance. This work proposes an effective training strategy in\nthe presence of noisy labels, called as Consensual Collaborative Training (CCT)\nframework. CCT co-trains three networks jointly using a convex combination of\nsupervision loss and consistency loss, without making any assumption about the\nnoise distribution. A dynamic transition mechanism is used to move from\nsupervision loss in early learning to consistency loss for consensus of\npredictions among networks in the later stage. Inference is done using a single\nnetwork based on a simple knowledge distillation scheme. Effectiveness of the\nproposed framework is demonstrated on synthetic as well as real noisy FER\ndatasets. In addition, a large test subset of around 5K images is annotated\nfrom the FEC dataset using crowd wisdom of 16 different annotators and reliable\nlabels are inferred. CCT is also validated on it. State-of-the-art performance\nis reported on the benchmark FER datasets RAFDB (90.84%) FERPlus (89.99%) and\nAffectNet (66%). Our codes are available at https://github.com/1980x/CCT.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 03:37:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gera", "Darshan", ""], ["Balasubramanian", "S.", ""]]}, {"id": "2107.04749", "submitter": "Mohammad Nayeem Teli", "authors": "Mohammad Nayeem Teli and Seungwon Oh", "title": "Resilience of Autonomous Vehicle Object Category Detection to Universal\n  Adversarial Perturbations", "comments": null, "journal-ref": "2021 IEEE International IOT, Electronics and Mechatronics\n  Conference (IEMTRONICS), 2021, pp. 1-6", "doi": "10.1109/IEMTRONICS52119.2021.9422616", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the vulnerability of deep neural networks to adversarial examples,\nnumerous works on adversarial attacks and defenses have been burgeoning over\nthe past several years. However, there seem to be some conventional views\nregarding adversarial attacks and object detection approaches that most\nresearchers take for granted. In this work, we bring a fresh perspective on\nthose procedures by evaluating the impact of universal perturbations on object\ndetection at a class-level. We apply it to a carefully curated data set related\nto autonomous driving. We use Faster-RCNN object detector on images of five\ndifferent categories: person, car, truck, stop sign and traffic light from the\nCOCO data set, while carefully perturbing the images using Universal Dense\nObject Suppression algorithm. Our results indicate that person, car, traffic\nlight, truck and stop sign are resilient in that order (most to least) to\nuniversal perturbations. To the best of our knowledge, this is the first time\nsuch a ranking has been established which is significant for the security of\nthe data sets pertaining to autonomous vehicles and object detection in\ngeneral.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 03:40:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Teli", "Mohammad Nayeem", ""], ["Oh", "Seungwon", ""]]}, {"id": "2107.04767", "submitter": "Mayur Parate", "authors": "Mayur R. Parate, Kishor M. Bhurchandi, Ashwin G. Kothari", "title": "Anomaly Detection in Residential Video Surveillance on Edge Devices in\n  IoT Framework", "comments": "7 Pages, 7 Figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Intelligent resident surveillance is one of the most essential smart\ncommunity services. The increasing demand for security needs surveillance\nsystems to be able to detect anomalies in surveillance scenes. Employing\nhigh-capacity computational devices for intelligent surveillance in residential\nsocieties is costly and not feasible. Therefore, we propose anomaly detection\nfor intelligent surveillance using CPU-only edge devices. A modular framework\nto capture object-level inferences and tracking is developed. To cope with\npartial occlusions, posture deformations, and complex scenes we employed\nfeature encoding and trajectory associations. Elements of the anomaly detection\nframework are optimized to run on CPU-only edge devices with sufficient FPS.\nThe experimental results indicate the proposed method is feasible and achieves\nsatisfactory results in real-life scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 05:52:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Parate", "Mayur R.", ""], ["Bhurchandi", "Kishor M.", ""], ["Kothari", "Ashwin G.", ""]]}, {"id": "2107.04768", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Bing-Kun Bao, Changsheng Xu", "title": "DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering", "comments": "12 pages, 12 figures", "journal-ref": "IEEE Transactions on Multimedia 2021", "doi": "10.1109/TMM.2021.3097171", "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video question answering is a challenging task, which requires agents to be\nable to understand rich video contents and perform spatial-temporal reasoning.\nHowever, existing graph-based methods fail to perform multi-step reasoning\nwell, neglecting two properties of VideoQA: (1) Even for the same video,\ndifferent questions may require different amount of video clips or objects to\ninfer the answer with relational reasoning; (2) During reasoning, appearance\nand motion features have complicated interdependence which are correlated and\ncomplementary to each other. Based on these observations, we propose a\nDual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an\nend-to-end fashion. The first contribution of our DualVGR is the design of an\nexplainable Query Punishment Module, which can filter out irrelevant visual\nfeatures through multiple cycles of reasoning. The second contribution is the\nproposed Video-based Multi-view Graph Attention Network, which captures the\nrelations between appearance and motion features. Our DualVGR network achieves\nstate-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and\ndemonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is\navailable at https://github.com/MMIR/DualVGR-VideoQA.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 06:08:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Jianyu", ""], ["Bao", "Bing-Kun", ""], ["Xu", "Changsheng", ""]]}, {"id": "2107.04782", "submitter": "Shuyuan Li", "authors": "Shuyuan Li, Huabin Liu, Rui Qian, Yuxi Li, John See, Mengjuan Fei,\n  Xiaoyuan Yu, Weiyao Lin", "title": "TTAN: Two-Stage Temporal Alignment Network for Few-shot Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Few-shot action recognition aims to recognize novel action classes (query)\nusing just a few samples (support). The majority of current approaches follow\nthe metric learning paradigm, which learns to compare the similarity between\nvideos. Recently, it has been observed that directly measuring this similarity\nis not ideal since different action instances may show distinctive temporal\ndistribution, resulting in severe misalignment issues across query and support\nvideos. In this paper, we arrest this problem from two distinct aspects --\naction duration misalignment and motion evolution misalignment. We address them\nsequentially through a Two-stage Temporal Alignment Network (TTAN). The first\nstage performs temporal transformation with the predicted affine warp\nparameters, while the second stage utilizes a cross-attention mechanism to\ncoordinate the features of the support and query to a consistent evolution.\nBesides, we devise a novel multi-shot fusion strategy, which takes the\nmisalignment among support samples into consideration. Ablation studies and\nvisualizations demonstrate the role played by both stages in addressing the\nmisalignment. Extensive experiments on benchmark datasets show the potential of\nthe proposed method in achieving state-of-the-art performance for few-shot\naction recognition.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 07:22:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Shuyuan", ""], ["Liu", "Huabin", ""], ["Qian", "Rui", ""], ["Li", "Yuxi", ""], ["See", "John", ""], ["Fei", "Mengjuan", ""], ["Yu", "Xiaoyuan", ""], ["Lin", "Weiyao", ""]]}, {"id": "2107.04795", "submitter": "MingCai Chen", "authors": "Mingcai Chen, Yuntao Du, Yi Zhang, Shuwei Qian, Chongjun Wang", "title": "Semi-Supervised Learning with Multi-Head Co-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. It works at the cost of training extra classifiers,\nwhere the algorithm should be delicately designed to prevent individual\nclassifiers from collapsing into each other. In this paper, we present a simple\nand efficient co-training algorithm, named Multi-Head Co-Training, for\nsemi-supervised image classification. By integrating base learners into a\nmulti-head structure, the model is in a minimal amount of extra parameters.\nEvery classification head in the unified model interacts with its peers through\na \"Weak and Strong Augmentation\" strategy, achieving single-view co-training\nwithout promoting diversity explicitly. The effectiveness of Multi-Head\nCo-Training is demonstrated in an empirical study on standard semi-supervised\nlearning benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 08:53:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Mingcai", ""], ["Du", "Yuntao", ""], ["Zhang", "Yi", ""], ["Qian", "Shuwei", ""], ["Wang", "Chongjun", ""]]}, {"id": "2107.04805", "submitter": "Shaohua Li", "authors": "Shaohua Li, Xiuchao Sui, Jie Fu, Huazhu Fu, Xiangde Luo, Yangqin Feng,\n  Xinxing Xu, Yong Liu, Daniel Ting, Rick Siow Mong Goh", "title": "Few-Shot Domain Adaptation with Polymorphic Transformers", "comments": "MICCAI'2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) trained on one set of medical images often\nexperience severe performance drop on unseen test images, due to various domain\ndiscrepancy between the training images (source domain) and the test images\n(target domain), which raises a domain adaptation issue. In clinical settings,\nit is difficult to collect enough annotated target domain data in a short\nperiod. Few-shot domain adaptation, i.e., adapting a trained model with a\nhandful of annotations, is highly practical and useful in this case. In this\npaper, we propose a Polymorphic Transformer (Polyformer), which can be\nincorporated into any DNN backbones for few-shot domain adaptation.\nSpecifically, after the polyformer layer is inserted into a model trained on\nthe source domain, it extracts a set of prototype embeddings, which can be\nviewed as a \"basis\" of the source-domain features. On the target domain, the\npolyformer layer adapts by only updating a projection layer which controls the\ninteractions between image features and the prototype embeddings. All other\nmodel weights (except BatchNorm parameters) are frozen during adaptation. Thus,\nthe chance of overfitting the annotations is greatly reduced, and the model can\nperform robustly on the target domain after being trained on a few annotated\nimages. We demonstrate the effectiveness of Polyformer on two medical\nsegmentation tasks (i.e., optic disc/cup segmentation, and polyp segmentation).\nThe source code of Polyformer is released at\nhttps://github.com/askerlee/segtran.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 10:08:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Shaohua", ""], ["Sui", "Xiuchao", ""], ["Fu", "Jie", ""], ["Fu", "Huazhu", ""], ["Luo", "Xiangde", ""], ["Feng", "Yangqin", ""], ["Xu", "Xinxing", ""], ["Liu", "Yong", ""], ["Ting", "Daniel", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2107.04806", "submitter": "Shijing Si", "authors": "Shijing Si, Jianzong Wang, Xiaoyang Qu, Ning Cheng, Wenqi Wei, Xinghua\n  Zhu and Jing Xiao", "title": "Speech2Video: Cross-Modal Distillation for Speech to Video Generation", "comments": "Accepted by InterSpeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates a novel task of talking face video generation solely\nfrom speeches. The speech-to-video generation technique can spark interesting\napplications in entertainment, customer service, and human-computer-interaction\nindustries. Indeed, the timbre, accent and speed in speeches could contain rich\ninformation relevant to speakers' appearance. The challenge mainly lies in\ndisentangling the distinct visual attributes from audio signals. In this\narticle, we propose a light-weight, cross-modal distillation method to extract\ndisentangled emotional and identity information from unlabelled video inputs.\nThe extracted features are then integrated by a generative adversarial network\ninto talking face video clips. With carefully crafted discriminators, the\nproposed framework achieves realistic generation results. Experiments with\nobserved individuals demonstrated that the proposed framework captures the\nemotional expressions solely from speeches, and produces spontaneous facial\nmotion in the video output. Compared to the baseline method where speeches are\ncombined with a static image of the speaker, the results of the proposed\nframework is almost indistinguishable. User studies also show that the proposed\nmethod outperforms the existing algorithms in terms of emotion expression in\nthe generated videos.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 10:27:26 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Si", "Shijing", ""], ["Wang", "Jianzong", ""], ["Qu", "Xiaoyang", ""], ["Cheng", "Ning", ""], ["Wei", "Wenqi", ""], ["Zhu", "Xinghua", ""], ["Xiao", "Jing", ""]]}, {"id": "2107.04808", "submitter": "Mihaela Breaban", "authors": "Radu Miron, Cosmin Moisii, Sergiu Dinu, Mihaela Breaban", "title": "COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents a comparative analysis of three distinct approaches based\non deep learning for COVID-19 detection in chest CTs. The first approach is a\nvolumetric one, involving 3D convolutions, while the other two approaches\nperform at first slice-wise classification and then aggregate the results at\nthe volume level. The experiments are carried on the COV19-CT-DB dataset, with\nthe aim of addressing the challenge raised by the MIA-COV19D Competition within\nICCV 2021. Our best results on the validation subset reach a macro-F1 score of\n0.92, which improves considerably the baseline score of 0.70 set by the\norganizers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 10:39:18 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 08:45:22 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Miron", "Radu", ""], ["Moisii", "Cosmin", ""], ["Dinu", "Sergiu", ""], ["Breaban", "Mihaela", ""]]}, {"id": "2107.04810", "submitter": "Fangqiu Yi", "authors": "Fangqiu Yi and Tingting Jiang", "title": "Not End-to-End: Explore Multi-Stage Architecture for Online Surgical\n  Phase Recognition", "comments": "Not accepted by M2CAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical phase recognition is of particular interest to computer assisted\nsurgery systems, in which the goal is to predict what phase is occurring at\neach frame for a surgery video. Networks with multi-stage architecture have\nbeen widely applied in many computer vision tasks with rich patterns, where a\npredictor stage first outputs initial predictions and an additional refinement\nstage operates on the initial predictions to perform further refinement.\nExisting works show that surgical video contents are well ordered and contain\nrich temporal patterns, making the multi-stage architecture well suited for the\nsurgical phase recognition task. However, we observe that when simply applying\nthe multi-stage architecture to the surgical phase recognition task, the\nend-to-end training manner will make the refinement ability fall short of its\nwishes. To address the problem, we propose a new non end-to-end training\nstrategy and explore different designs of multi-stage architecture for surgical\nphase recognition task. For the non end-to-end training strategy, the\nrefinement stage is trained separately with proposed two types of disturbed\nsequences. Meanwhile, we evaluate three different choices of refinement models\nto show that our analysis and solution are robust to the choices of specific\nmulti-stage models. We conduct experiments on two public benchmarks, the\nM2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that\nmulti-stage architecture trained with our strategy largely boosts the\nperformance of the current state-of-the-art single-stage model. Code is\navailable at \\url{https://github.com/ChinaYi/casual_tcn}.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 11:00:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Yi", "Fangqiu", ""], ["Jiang", "Tingting", ""]]}, {"id": "2107.04813", "submitter": "Dr. Mohammed Javed", "authors": "Atul Sharma, Bulla Rajesh and Mohammed Javed", "title": "Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain\n  using Transfer Learning Technique", "comments": "Accepted in MISP 2021 3rd International Conference On Machine\n  Intelligence And Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant leaf diseases pose a significant danger to food security and they cause\ndepletion in quality and volume of production. Therefore accurate and timely\ndetection of leaf disease is very important to check the loss of the crops and\nmeet the growing food demand of the people. Conventional techniques depend on\nlab investigation and human skills which are generally costly and inaccessible.\nRecently, Deep Neural Networks have been exceptionally fruitful in image\nclassification. In this research paper, plant leaf disease detection employing\ntransfer learning is explored in the JPEG compressed domain. Here, the JPEG\ncompressed stream consisting of DCT coefficients is, directly fed into the\nNeural Network to improve the efficiency of classification. The experimental\nresults on JPEG compressed leaf dataset demonstrate the efficacy of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 11:10:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sharma", "Atul", ""], ["Rajesh", "Bulla", ""], ["Javed", "Mohammed", ""]]}, {"id": "2107.04819", "submitter": "Wang Jiabao", "authors": "Fang Gao, Jiabao Wang, Jun Yu, Yaoxiong Wang, Feng Shuang", "title": "A Weakly-Supervised Depth Estimation Network Using Attention Mechanism", "comments": "8 pages, 8 figures, IJCAI-WSRL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation (MDE) is a fundamental task in many applications\nsuch as scene understanding and reconstruction. However, most of the existing\nmethods rely on accurately labeled datasets. A weakly-supervised framework\nbased on attention nested U-net (ANU) named as ANUW is introduced in this paper\nfor cases with wrong labels. The ANUW is trained end-to-end to convert an input\nsingle RGB image into a depth image. It consists of a dense residual network\nstructure, an adaptive weight channel attention (AWCA) module, a patch second\nnon-local (PSNL) module and a soft label generation method. The dense residual\nnetwork is the main body of the network to encode and decode the input. The\nAWCA module can adaptively adjust the channel weights to extract important\nfeatures. The PSNL module implements the spatial attention mechanism through a\nsecond-order non-local method. The proposed soft label generation method uses\nthe prior knowledge of the dataset to produce soft labels to replace false\nones. The proposed ANUW is trained on a defective monocular depth dataset and\nthe trained model is tested on three public datasets, and the results\ndemonstrate the superiority of ANUW in comparison with the state-of-the-art MDE\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 11:40:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gao", "Fang", ""], ["Wang", "Jiabao", ""], ["Yu", "Jun", ""], ["Wang", "Yaoxiong", ""], ["Shuang", "Feng", ""]]}, {"id": "2107.04823", "submitter": "Li Lin", "authors": "Li Lin, Zhonghua Wang, Jiewei Wu, Yijin Huang, Junyan Lyu, Pujin\n  Cheng, Jiong Wu, Xiaoying Tang", "title": "BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework\n  for Segmenting and Classifying OCTA Images", "comments": "12 pages, 4 figures, MICCAI2021 [Student Travel Award]", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical coherence tomography angiography (OCTA) is a novel non-invasive\nimaging technique that allows visualizations of vasculature and foveal\navascular zone (FAZ) across retinal layers. Clinical researches suggest that\nthe morphology and contour irregularity of FAZ are important biomarkers of\nvarious ocular pathologies. Therefore, precise segmentation of FAZ has great\nclinical interest. Also, there is no existing research reporting that FAZ\nfeatures can improve the performance of deep diagnostic classification\nnetworks. In this paper, we propose a novel multi-level boundary shape and\ndistance aware joint learning framework, named BSDA-Net, for FAZ segmentation\nand diagnostic classification from OCTA images. Two auxiliary branches, namely\nboundary heatmap regression and signed distance map reconstruction branches,\nare constructed in addition to the segmentation branch to improve the\nsegmentation performance, resulting in more accurate FAZ contours and fewer\noutliers. Moreover, both low-level and high-level features from the\naforementioned three branches, including shape, size, boundary, and signed\ndirectional distance map of FAZ, are fused hierarchically with features from\nthe diagnostic classifier. Through extensive experiments, the proposed BSDA-Net\nis found to yield state-of-the-art segmentation and classification results on\nthe OCTA-500, OCTAGON, and FAZID datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 12:16:23 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:34:19 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lin", "Li", ""], ["Wang", "Zhonghua", ""], ["Wu", "Jiewei", ""], ["Huang", "Yijin", ""], ["Lyu", "Junyan", ""], ["Cheng", "Pujin", ""], ["Wu", "Jiong", ""], ["Tang", "Xiaoying", ""]]}, {"id": "2107.04827", "submitter": "Shoaib Ahmed Siddiqui", "authors": "Shoaib Ahmed Siddiqui, Thomas Breuel", "title": "Identifying Layers Susceptible to Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common neural network architectures are susceptible to attack by adversarial\nsamples. Neural network architectures are commonly thought of as divided into\nlow-level feature extraction layers and high-level classification layers;\nsusceptibility of networks to adversarial samples is often thought of as a\nproblem related to classification rather than feature extraction. We test this\nidea by selectively retraining different portions of VGG and ResNet\narchitectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and\nadversarial data. Our experimental results show that susceptibility to\nadversarial samples is associated with low-level feature extraction layers.\nTherefore, retraining high-level layers is insufficient for achieving\nrobustness. This phenomenon could have two explanations: either, adversarial\nattacks yield outputs from early layers that are indistinguishable from\nfeatures found in the attack classes, or adversarial attacks yield outputs from\nearly layers that differ statistically from features for non-adversarial\nsamples and do not permit consistent classification by subsequent layers. We\ntest this question by large-scale non-linear dimensionality reduction and\ndensity modeling on distributions of feature vectors in hidden layers and find\nthat the feature distributions between non-adversarial and adversarial samples\ndiffer substantially. Our results provide new insights into the statistical\norigins of adversarial samples and possible defenses.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 12:38:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Siddiqui", "Shoaib Ahmed", ""], ["Breuel", "Thomas", ""]]}, {"id": "2107.04829", "submitter": "Yu-Min Zhang", "authors": "Yu-Ming Zhang, Chun-Chieh Lee, Jun-Wei Hsieh, Kuo-Chin Fan", "title": "CSL-YOLO: A New Lightweight Object Detection System for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of lightweight object detectors is essential due to the\nlimited computation resources. To reduce the computation cost, how to generate\nredundant features plays a significant role. This paper proposes a new\nlightweight Convolution method Cross-Stage Lightweight (CSL) Module, to\ngenerate redundant features from cheap operations. In the intermediate\nexpansion stage, we replaced Pointwise Convolution with Depthwise Convolution\nto produce candidate features. The proposed CSL-Module can reduce the\ncomputation cost significantly. Experiments conducted at MS-COCO show that the\nproposed CSL-Module can approximate the fitting ability of Convolution-3x3.\nFinally, we use the module to construct a lightweight detector CSL-YOLO,\nachieving better detection performance with only 43% FLOPs and 52% parameters\nthan Tiny-YOLOv4.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 12:56:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Yu-Ming", ""], ["Lee", "Chun-Chieh", ""], ["Hsieh", "Jun-Wei", ""], ["Fan", "Kuo-Chin", ""]]}, {"id": "2107.04834", "submitter": "Hailan Huang", "authors": "Yuan Tai, Yihua Tan, Wei Gong, Hailan Huang", "title": "Bayesian Convolutional Neural Networks for Seven Basic Facial Expression\n  Classifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seven basic facial expression classifications are a basic way to express\ncomplex human emotions and are an important part of artificial intelligence\nresearch. Based on the traditional Bayesian neural network framework, the\nResNet18_BNN network constructed in this paper has been improved in the\nfollowing three aspects: (1) A new objective function is proposed, which is\ncomposed of the KL loss of uncertain parameters and the intersection of\nspecific parameters. Entropy loss composition. (2) Aiming at a special\nobjective function, a training scheme for alternately updating these two\nparameters is proposed. (3) Only model the parameters of the last convolution\ngroup. Through testing on the FER2013 test set, we achieved 71.5% and 73.1%\naccuracy in PublicTestSet and PrivateTestSet, respectively. Compared with\ntraditional Bayesian neural networks, our method brings the highest\nclassification accuracy gain.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 13:09:13 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:05:36 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Tai", "Yuan", ""], ["Tan", "Yihua", ""], ["Gong", "Wei", ""], ["Huang", "Hailan", ""]]}, {"id": "2107.04847", "submitter": "Zhuangzhuang Zhang", "authors": "Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Weixiong Zhang, Baozhou\n  Sun", "title": "Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method\n  for Organs-at-risk Segmentation in Head and Neck CT Images", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radiotherapy planning, manual contouring is labor-intensive and\ntime-consuming. Accurate and robust automated segmentation models improve the\nefficiency and treatment outcome. We aim to develop a novel hybrid deep\nlearning approach, combining convolutional neural networks (CNNs) and the\nself-attention mechanism, for rapid and accurate multi-organ segmentation on\nhead and neck computed tomography (CT) images. Head and neck CT images with\nmanual contours of 115 patients were retrospectively collected and used. We set\nthe training/validation/testing ratio to 81/9/25 and used the 10-fold\ncross-validation strategy to select the best model parameters. The proposed\nhybrid model segmented ten organs-at-risk (OARs) altogether for each case. The\nperformance of the model was evaluated by three metrics, i.e., the Dice\nSimilarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface\ndistance (MSD). We also tested the performance of the model on the Head and\nNeck 2015 challenge dataset and compared it against several state-of-the-art\nautomated segmentation algorithms. The proposed method generated contours that\nclosely resemble the ground truth for ten OARs. Our results of the new Weaving\nAttention U-net demonstrate superior or similar performance on the segmentation\nof head and neck CT images.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 14:27:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Zhuangzhuang", ""], ["Zhao", "Tianyu", ""], ["Gay", "Hiram", ""], ["Zhang", "Weixiong", ""], ["Sun", "Baozhou", ""]]}, {"id": "2107.04852", "submitter": "Arul Selvam Periyasamy", "authors": "Arul Selvam Periyasamy, Max Schwarz, and Sven Behnke", "title": "SynPick: A Dataset for Dynamic Bin Picking Scene Understanding", "comments": "Accepted for 17th IEEE International Conference on Automation Science\n  and Engineering (CASE), Lyon, France, August 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present SynPick, a synthetic dataset for dynamic scene understanding in\nbin-picking scenarios. In contrast to existing datasets, our dataset is both\nsituated in a realistic industrial application domain -- inspired by the\nwell-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with\nauthentic picking actions as chosen by our picking heuristic developed for the\nARC 2017. The dataset is compatible with the popular BOP dataset format. We\ndescribe the dataset generation process in detail, including object arrangement\ngeneration and manipulation simulation using the NVIDIA PhysX physics engine.\nTo cover a large action space, we perform untargeted and targeted picking\nactions, as well as random moving actions. To establish a baseline for object\nperception, a state-of-the-art pose estimation approach is evaluated on the\ndataset. We demonstrate the usefulness of tracking poses during manipulation\ninstead of single-shot estimation even with a naive filtering approach. The\ngenerator source code and dataset are publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 14:58:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Periyasamy", "Arul Selvam", ""], ["Schwarz", "Max", ""], ["Behnke", "Sven", ""]]}, {"id": "2107.04867", "submitter": "AnChieh Cheng", "authors": "An-Chieh Cheng, Xueting Li, Min Sun, Ming-Hsuan Yang, Sifei Liu", "title": "Learning 3D Dense Correspondence via Canonical Point Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a canonical point autoencoder (CPAE) that predicts dense\ncorrespondences between 3D shapes of the same category. The autoencoder\nperforms two key functions: (a) encoding an arbitrarily ordered point cloud to\na canonical primitive, e.g., a sphere, and (b) decoding the primitive back to\nthe original input instance shape. As being placed in the bottleneck, this\nprimitive plays a key role to map all the unordered point clouds on the\ncanonical surface and to be reconstructed in an ordered fashion. Once trained,\npoints from different shape instances that are mapped to the same locations on\nthe primitive surface are determined to be a pair of correspondence. Our method\ndoes not require any form of annotation or self-supervised part segmentation\nnetwork and can handle unaligned input point clouds. Experimental results on 3D\nsemantic keypoint transfer and part segmentation transfer show that our model\nperforms favorably against state-of-the-art correspondence learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 15:54:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cheng", "An-Chieh", ""], ["Li", "Xueting", ""], ["Sun", "Min", ""], ["Yang", "Ming-Hsuan", ""], ["Liu", "Sifei", ""]]}, {"id": "2107.04882", "submitter": "Anisie Uwimana", "authors": "Anisie Uwimana1, Ransalu Senanayake", "title": "Out of Distribution Detection and Adversarial Attacks on Deep Neural\n  Networks for Robust Medical Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models have become a popular choice for medical image analysis.\nHowever, the poor generalization performance of deep learning models limits\nthem from being deployed in the real world as robustness is critical for\nmedical applications. For instance, the state-of-the-art Convolutional Neural\nNetworks (CNNs) fail to detect adversarial samples or samples drawn\nstatistically far away from the training distribution. In this work, we\nexperimentally evaluate the robustness of a Mahalanobis distance-based\nconfidence score, a simple yet effective method for detecting abnormal input\nsamples, in classifying malaria parasitized cells and uninfected cells. Results\nindicated that the Mahalanobis confidence score detector exhibits improved\nperformance and robustness of deep learning models, and achieves\nstateof-the-art performance on both out-of-distribution (OOD) and adversarial\nsamples.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 18:00:40 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Uwimana1", "Anisie", ""], ["Senanayake", "Ransalu", ""]]}, {"id": "2107.04886", "submitter": "Hao Zheng", "authors": "Hao Zheng, Jun Han, Hongxiao Wang, Lin Yang, Zhuo Zhao, Chaoli Wang,\n  Danny Z. Chen", "title": "Hierarchical Self-Supervised Learning for Medical Image Segmentation\n  Based on Multi-Domain Data Aggregation", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A large labeled dataset is a key to the success of supervised deep learning,\nbut for medical image segmentation, it is highly challenging to obtain\nsufficient annotated images for model training. In many scenarios, unannotated\nimages are abundant and easy to acquire. Self-supervised learning (SSL) has\nshown great potentials in exploiting raw data information and representation\nlearning. In this paper, we propose Hierarchical Self-Supervised Learning\n(HSSL), a new self-supervised framework that boosts medical image segmentation\nby making good use of unannotated data. Unlike the current literature on\ntask-specific self-supervised pretraining followed by supervised fine-tuning,\nwe utilize SSL to learn task-agnostic knowledge from heterogeneous data for\nvarious medical image segmentation tasks. Specifically, we first aggregate a\ndataset from several medical challenges, then pre-train the network in a\nself-supervised manner, and finally fine-tune on labeled data. We develop a new\nloss function by combining contrastive loss and classification loss and\npretrain an encoder-decoder architecture for segmentation tasks. Our extensive\nexperiments show that multi-domain joint pre-training benefits downstream\nsegmentation tasks and outperforms single-domain pre-training significantly.\nCompared to learning from scratch, our new method yields better performance on\nvarious tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated\ndata). With limited amounts of training data, our method can substantially\nbridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of\nannotated data).\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 18:17:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zheng", "Hao", ""], ["Han", "Jun", ""], ["Wang", "Hongxiao", ""], ["Yang", "Lin", ""], ["Zhao", "Zhuo", ""], ["Wang", "Chaoli", ""], ["Chen", "Danny Z.", ""]]}, {"id": "2107.04902", "submitter": "Iuliia Kotseruba", "authors": "Iuliia Kotseruba, Manos Papagelis, John K. Tsotsos", "title": "Industry and Academic Research in Computer Vision", "comments": "8 pages, 9 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to study the dynamic between research in the industry and\nacademia in computer vision. The results are demonstrated on a set of top-5\nvision conferences that are representative of the field. Since data for such\nanalysis was not readily available, significant effort was spent on gathering\nand processing meta-data from the original publications. First, this study\nquantifies the share of industry-sponsored research. Specifically, it shows\nthat the proportion of papers published by industry-affiliated researchers is\nincreasing and that more academics join companies or collaborate with them.\nNext, the possible impact of industry presence is further explored, namely in\nthe distribution of research topics and citation patterns. The results indicate\nthat the distribution of the research topics is similar in industry and\nacademic papers. However, there is a strong preference towards citing industry\npapers. Finally, possible reasons for citation bias, such as code availability\nand influence, are investigated.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 20:09:52 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:59:37 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 23:30:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kotseruba", "Iuliia", ""], ["Papagelis", "Manos", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2107.04914", "submitter": "Ivan Zakazov", "authors": "Ivan Zakazov, Boris Shirokikh, Alexey Chernyavskiy and Mikhail Belyaev", "title": "Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation", "comments": "Accepted for MICCAI-2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain Adaptation (DA) methods are widely used in medical image segmentation\ntasks to tackle the problem of differently distributed train (source) and test\n(target) data. We consider the supervised DA task with a limited number of\nannotated samples from the target domain. It corresponds to one of the most\nrelevant clinical setups: building a sufficiently accurate model on the minimum\npossible amount of annotated data. Existing methods mostly fine-tune specific\nlayers of the pretrained Convolutional Neural Network (CNN). However, there is\nno consensus on which layers are better to fine-tune, e.g. the first layers for\nimages with low-level domain shift or the deeper layers for images with\nhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecture\nthat automatically chooses the layers which should be optimally fine-tuned.\nMore specifically, on the target domain, our method additionally learns the\npolicy that indicates whether a specific layer should be fine-tuned or reused\nfrom the pretrained network. We show that our method performs at the same level\nas the best of the nonflexible fine-tuning methods even under the extreme\nscarcity of annotated data. Secondly, we show that SpotTUnet policy provides a\nlayer-wise visualization of the domain shift impact on the network, which could\nbe further used to develop robust domain generalization methods. In order to\nextensively evaluate SpotTUnet performance, we use a publicly available dataset\nof brain MR images (CC359), characterized by explicit domain shift. We release\na reproducible experimental pipeline.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 21:13:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zakazov", "Ivan", ""], ["Shirokikh", "Boris", ""], ["Chernyavskiy", "Alexey", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "2107.04921", "submitter": "Antea Hadviger", "authors": "Antea Hadviger, Igor Cvi\\v{s}i\\'c, Ivan Markovi\\'c, Sacha\n  Vra\\v{z}i\\'c, Ivan Petrovi\\'c", "title": "Feature-based Event Stereo Visual Odometry", "comments": "Submitted to Accepted to European Conference on Mobile Robots (ECMR)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event-based cameras are biologically inspired sensors that output events,\ni.e., asynchronous pixel-wise brightness changes in the scene. Their high\ndynamic range and temporal resolution of a microsecond makes them more reliable\nthan standard cameras in environments of challenging illumination and in\nhigh-speed scenarios, thus developing odometry algorithms based solely on event\ncameras offers exciting new possibilities for autonomous systems and robots. In\nthis paper, we propose a novel stereo visual odometry method for event cameras\nbased on feature detection and matching with careful feature management, while\npose estimation is done by reprojection error minimization. We evaluate the\nperformance of the proposed method on two publicly available datasets: MVSEC\nsequences captured by an indoor flying drone and DSEC outdoor driving\nsequences. MVSEC offers accurate ground truth from motion capture, while for\nDSEC, which does not offer ground truth, in order to obtain a reference\ntrajectory on the standard camera frames we used our SOFT visual odometry, one\nof the highest ranking algorithms on the KITTI scoreboards. We compared our\nmethod to the ESVO method, which is the first and still the only stereo event\nodometry method, showing on par performance on the MVSEC sequences, while on\nthe DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving\nscenario with default parameters. Furthermore, two important advantages of our\nmethod over ESVO are that it adapts tracking frequency to the asynchronous\nevent rate and does not require initialization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 22:36:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hadviger", "Antea", ""], ["Cvi\u0161i\u0107", "Igor", ""], ["Markovi\u0107", "Ivan", ""], ["Vra\u017ei\u0107", "Sacha", ""], ["Petrovi\u0107", "Ivan", ""]]}, {"id": "2107.04930", "submitter": "Mohammad Nayeem Teli", "authors": "Mohammad Nayeem Teli", "title": "TeliNet, a simple and shallow Convolution Neural Network (CNN) to\n  Classify CT Scans of COVID-19 patients", "comments": "5 pages, 3 figures, ICCVW", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hundreds of millions of cases and millions of deaths have occurred worldwide\ndue to COVID-19. The fight against this pandemic is on-going on multiple\nfronts. While vaccinations are picking up speed, there are still billions of\nunvaccinated people. In this fight diagnosis of the disease and isolation of\nthe patients to prevent any spreads play a huge role. Machine Learning\napproaches have assisted the diagnosis of COVID-19 cases by analyzing chest\nX-ray and CT-scan images of patients. In this research we present a simple and\nshallow Convolutional Neural Network based approach, TeliNet, to classify\nCT-scan images of COVID-19 patients. Our results outperform the F1 score of\nVGGNet and the benchmark approaches. Our proposed solution is also more\nlightweight in comparison to the other methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 23:46:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Teli", "Mohammad Nayeem", ""]]}, {"id": "2107.04932", "submitter": "Yuecong Xu", "authors": "Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin, Simon\n  See", "title": "Aligning Correlation Information for Domain Adaptation in Action\n  Recognition", "comments": "The dataset HMDB-ARID is available at\n  https://xuyu0010.github.io/vuda.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Domain adaptation (DA) approaches address domain shift and enable networks to\nbe applied to different scenarios. Although various image DA approaches have\nbeen proposed in recent years, there is limited research towards video DA. This\nis partly due to the complexity in adapting the different modalities of\nfeatures in videos, which includes the correlation features extracted as\nlong-term dependencies of pixels across spatiotemporal dimensions. The\ncorrelation features are highly associated with action classes and proven their\neffectiveness in accurate video feature extraction through the supervised\naction recognition task. Yet correlation features of the same action would\ndiffer across domains due to domain shift. Therefore we propose a novel\nAdversarial Correlation Adaptation Network (ACAN) to align action videos by\naligning pixel correlations. ACAN aims to minimize the distribution of\ncorrelation information, termed as Pixel Correlation Discrepancy (PCD).\nAdditionally, video DA research is also limited by the lack of cross-domain\nvideo datasets with larger domain shifts. We, therefore, introduce a novel\nHMDB-ARID dataset with a larger domain shift caused by a larger statistical\ndifference between domains. This dataset is built in an effort to leverage\ncurrent datasets for dark video classification. Empirical results demonstrate\nthe state-of-the-art performance of our proposed ACAN for both existing and the\nnew video DA datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 00:13:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xu", "Yuecong", ""], ["Yang", "Jianfei", ""], ["Cao", "Haozhi", ""], ["Mao", "Kezhi", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""]]}, {"id": "2107.04934", "submitter": "Euijoon Ahn", "authors": "Euijoon Ahn, Dagan Feng and Jinman Kim", "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image\n  Segmentation", "comments": "Accepted at Medical Image Computing and Computer Assisted\n  Interventions (MICCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of medical images is a fundamental step in automated\nclinical decision support systems. Existing medical image segmentation methods\nbased on supervised deep learning, however, remain problematic because of their\nreliance on large amounts of labelled training data. Although medical imaging\ndata repositories continue to expand, there has not been a commensurate\nincrease in the amount of annotated data. Hence, we propose a new spatial\nguided self-supervised clustering network (SGSCN) for medical image\nsegmentation, where we introduce multiple loss functions designed to aid in\ngrouping image pixels that are spatially connected and have similar feature\nrepresentations. It iteratively learns feature representations and clustering\nassignment of each pixel in an end-to-end fashion from a single image. We also\npropose a context-based consistency loss that better delineates the shape and\nboundaries of image regions. It enforces all the pixels belonging to a cluster\nto be spatially close to the cluster centre. We evaluated our method on 2\npublic medical image datasets and compared it to existing conventional and\nself-supervised clustering methods. Experimental results show that our method\nwas most accurate for medical image segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 00:40:40 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ahn", "Euijoon", ""], ["Feng", "Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "2107.04937", "submitter": "Senthil Yogamani", "authors": "Hazem Rashed, Mariam Essam, Maha Mohamed, Ahmad El Sallab and Senthil\n  Yogamani", "title": "BEV-MODNet: Monocular Camera based Bird's Eye View Moving Object\n  Detection for Autonomous Driving", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of moving objects is a very important task in autonomous driving\nsystems. After the perception phase, motion planning is typically performed in\nBird's Eye View (BEV) space. This would require projection of objects detected\non the image plane to top view BEV plane. Such a projection is prone to errors\ndue to lack of depth information and noisy mapping in far away areas. CNNs can\nleverage the global context in the scene to project better. In this work, we\nexplore end-to-end Moving Object Detection (MOD) on the BEV map directly using\nmonocular images as input. To the best of our knowledge, such a dataset does\nnot exist and we create an extended KITTI-raw dataset consisting of 12.9k\nimages with annotations of moving object masks in BEV space for five classes.\nThe dataset is intended to be used for class agnostic motion cue based object\ndetection and classes are provided as meta-data for better tuning. We design\nand implement a two-stream RGB and optical flow fusion architecture which\noutputs motion segmentation directly in BEV space. We compare it with inverse\nperspective mapping of state-of-the-art motion segmentation predictions on the\nimage plane. We observe a significant improvement of 13% in mIoU using the\nsimple baseline implementation. This demonstrates the ability to directly learn\nmotion segmentation output in BEV space. Qualitative results of our baseline\nand the dataset annotations can be found in\nhttps://sites.google.com/view/bev-modnet.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 01:11:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Rashed", "Hazem", ""], ["Essam", "Mariam", ""], ["Mohamed", "Maha", ""], ["Sallab", "Ahmad El", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2107.04938", "submitter": "Chen Yuqian", "authors": "Yuqian Chen, Chaoyi Zhang, Yang Song, Nikos Makris, Yogesh Rathi,\n  Weidong Cai, Fan Zhang, Lauren J. O'Donnell", "title": "Deep Fiber Clustering: Anatomically Informed Unsupervised Deep Learning\n  for Fast and Effective White Matter Parcellation", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White matter fiber clustering (WMFC) enables parcellation of white matter\ntractography for applications such as disease classification and anatomical\ntract segmentation. However, the lack of ground truth and the ambiguity of\nfiber data (the points along a fiber can equivalently be represented in forward\nor reverse order) pose challenges to this task. We propose a novel WMFC\nframework based on unsupervised deep learning. We solve the unsupervised\nclustering problem as a self-supervised learning task. Specifically, we use a\nconvolutional neural network to learn embeddings of input fibers, using\npairwise fiber distances as pseudo annotations. This enables WMFC that is\ninsensitive to fiber point ordering. In addition, anatomical coherence of fiber\nclusters is improved by incorporating brain anatomical segmentation data. The\nproposed framework enables outlier removal in a natural way by rejecting fibers\nwith low cluster assignment probability. We train and evaluate our method using\n200 datasets from the Human Connectome Project. Results demonstrate superior\nperformance and efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 01:36:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Yuqian", ""], ["Zhang", "Chaoyi", ""], ["Song", "Yang", ""], ["Makris", "Nikos", ""], ["Rathi", "Yogesh", ""], ["Cai", "Weidong", ""], ["Zhang", "Fan", ""], ["O'Donnell", "Lauren J.", ""]]}, {"id": "2107.04941", "submitter": "Yuecong Xu", "authors": "Yuecong Xu, Jianfei Yang, Haozhi Cao, Qi Li, Kezhi Mao, Zhenghua Chen", "title": "Partial Video Domain Adaptation with Partial Adversarial Temporal\n  Attentive Network", "comments": "The new datasets for PVDA: HMDB-ARID(partial), MiniKinetics-UCF,\n  HMDB-ARID(partial) can be downloaded from\n  https://xuyu0010.github.io/pvda.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Partial Domain Adaptation (PDA) is a practical and general domain adaptation\nscenario, which relaxes the fully shared label space assumption such that the\nsource label space subsumes the target one. The key challenge of PDA is the\nissue of negative transfer caused by source-only classes. For videos, such\nnegative transfer could be triggered by both spatial and temporal features,\nwhich leads to a more challenging Partial Video Domain Adaptation (PVDA)\nproblem. In this paper, we propose a novel Partial Adversarial Temporal\nAttentive Network (PATAN) to address the PVDA problem by utilizing both spatial\nand temporal features for filtering source-only classes. Besides, PATAN\nconstructs effective overall temporal features by attending to local temporal\nfeatures that contribute more toward the class filtration process. We further\nintroduce new benchmarks to facilitate research on PVDA problems, covering a\nwide range of PVDA scenarios. Empirical results demonstrate the\nstate-of-the-art performance of our proposed PATAN across the multiple PVDA\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 02:17:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xu", "Yuecong", ""], ["Yang", "Jianfei", ""], ["Cao", "Haozhi", ""], ["Li", "Qi", ""], ["Mao", "Kezhi", ""], ["Chen", "Zhenghua", ""]]}, {"id": "2107.04943", "submitter": "Jianping Zhang", "authors": "Xiaohong Fan, Yin Yang, Jianping Zhang", "title": "Deep Geometric Distillation Network for Compressive Sensing MRI", "comments": "Accepted by IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI), 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is an efficient method to reconstruct MR image from\nsmall sampled data in $k$-space and accelerate the acquisition of MRI. In this\nwork, we propose a novel deep geometric distillation network which combines the\nmerits of model-based and deep learning-based CS-MRI methods, it can be\ntheoretically guaranteed to improve geometric texture details of a linear\nreconstruction. Firstly, we unfold the model-based CS-MRI optimization problem\ninto two sub-problems that consist of image linear approximation and image\ngeometric compensation. Secondly, geometric compensation sub-problem for\ndistilling lost texture details in approximation stage can be expanded by\nTaylor expansion to design a geometric distillation module fusing features of\ndifferent geometric characteristic domains. Additionally, we use a learnable\nversion with adaptive initialization of the step-length parameter, which allows\nmodel more flexibility that can lead to convergent smoothly. Numerical\nexperiments verify its superiority over other state-of-the-art CS-MRI\nreconstruction approaches. The source code will be available at\n\\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 02:24:55 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fan", "Xiaohong", ""], ["Yang", "Yin", ""], ["Zhang", "Jianping", ""]]}, {"id": "2107.04952", "submitter": "Gaurav Bhatt", "authors": "Gaurav Bhatt, Shivam Chandhok and Vineeth N Balasubramanian", "title": "Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with\n  Limited Supervision", "comments": "Accepted at IJCAI'21 workshop on Weakly Supervised Representation\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common problem with most zero and few-shot learning approaches is they\nsuffer from bias towards seen classes resulting in sub-optimal performance.\nExisting efforts aim to utilize unlabeled images from unseen classes (i.e\ntransductive zero-shot) during training to enable generalization. However, this\nlimits their use in practical scenarios where data from target unseen classes\nis unavailable or infeasible to collect. In this work, we present a practical\nsetting of inductive zero and few-shot learning, where unlabeled images from\nother out-of-data classes, that do not belong to seen or unseen categories, can\nbe used to improve generalization in any-shot learning. We leverage a\nformulation based on product-of-experts and introduce a new AUD module that\nenables us to use unlabeled samples from out-of-data classes which are usually\neasily available and practically entail no annotation cost. In addition, we\nalso demonstrate the applicability of our model to address a more practical and\nchallenging, Generalized Zero-shot under a limited supervision setting, where\neven base seen classes do not have sufficient annotated samples.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 03:23:20 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 01:28:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bhatt", "Gaurav", ""], ["Chandhok", "Shivam", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2107.04974", "submitter": "Boris Kovalerchuk", "authors": "Rose McDonald, Boris Kovalerchuk", "title": "Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates", "comments": "29 pages, 29 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging for humans to enable visual knowledge discovery in data\nwith more than 2-3 dimensions with a naked eye. This chapter explores the\nefficiency of discovering predictive machine learning models interactively\nusing new Elliptic Paired coordinates (EPC) visualizations. It is shown that\nEPC are capable to visualize multidimensional data and support visual machine\nlearning with preservation of multidimensional information in 2-D. Relative to\nparallel and radial coordinates, EPC visualization requires only a half of the\nvisual elements for each n-D point. An interactive software system EllipseVis,\nwhich is developed in this work, processes high-dimensional datasets, creates\nEPC visualizations, and produces predictive classification models by\ndiscovering dominance rules in EPC. By using interactive and automatic\nprocesses it discovers zones in EPC with a high dominance of a single class.\nThe EPC methodology has been successful in discovering non-linear predictive\nmodels with high coverage and precision in the computational experiments. This\ncan benefit multiple domains by producing visually appealing dominance rules.\nThis chapter presents results of successful testing the EPC non-linear\nmethodology in experiments using real and simulated data, EPC generalized to\nthe Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of\ncoordinates to optimize the visual discovery, introduction of an alternative\nEPC design and introduction of the concept of incompact machine learning\nmethodology based on EPC/DEPC.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 05:53:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["McDonald", "Rose", ""], ["Kovalerchuk", "Boris", ""]]}, {"id": "2107.04983", "submitter": "Jack Lynch", "authors": "Jack Lynch and Sam Wookey", "title": "Leveraging Domain Adaptation for Low-Resource Geospatial Machine\n  Learning", "comments": "Tackling Climate Change with Machine Learning Workshop at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning in remote sensing has matured alongside a proliferation in\navailability and resolution of geospatial imagery, but its utility is\nbottlenecked by the need for labeled data. What's more, many labeled geospatial\ndatasets are specific to certain regions, instruments, or extreme weather\nevents. We investigate the application of modern domain-adaptation to multiple\nproposed geospatial benchmarks, uncovering unique challenges and proposing\nsolutions to them.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 06:47:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lynch", "Jack", ""], ["Wookey", "Sam", ""]]}, {"id": "2107.04991", "submitter": "Ferhat Ozgur Catak", "authors": "Ferhat Ozgur Catak, Tao Yue, Shaukat Ali", "title": "Prediction Surface Uncertainty Quantification in Object Detection Models\n  for Autonomous Driving", "comments": "Accepted in AITest 2021, The Third IEEE International Conference On\n  Artificial Intelligence Testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in autonomous cars is commonly based on camera images and\nLidar inputs, which are often used to train prediction models such as deep\nartificial neural networks for decision making for object recognition,\nadjusting speed, etc. A mistake in such decision making can be damaging; thus,\nit is vital to measure the reliability of decisions made by such prediction\nmodels via uncertainty measurement. Uncertainty, in deep learning models, is\noften measured for classification problems. However, deep learning models in\nautonomous driving are often multi-output regression models. Hence, we propose\na novel method called PURE (Prediction sURface uncErtainty) for measuring\nprediction uncertainty of such regression models. We formulate the object\nrecognition problem as a regression model with more than one outputs for\nfinding object locations in a 2-dimensional camera view. For evaluation, we\nmodified three widely-applied object recognition models (i.e., YoLo, SSD300 and\nSSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET\ndatasets. Results showed the statistically significant negative correlation\nbetween prediction surface uncertainty and prediction accuracy suggesting that\nuncertainty significantly impacts the decisions made by autonomous driving.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 08:31:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Catak", "Ferhat Ozgur", ""], ["Yue", "Tao", ""], ["Ali", "Shaukat", ""]]}, {"id": "2107.05005", "submitter": "Yi-Geng Hong", "authors": "Yi-Geng Hong, Hui-Chu Xiao, Wan-Lei Zhao", "title": "Towards Accurate Localization by Instance Search", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 10:03:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hong", "Yi-Geng", ""], ["Xiao", "Hui-Chu", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "2107.05023", "submitter": "Sang Dinh", "authors": "Phan Ngoc Lan, Nguyen Sy An, Dao Viet Hang, Dao Van Long, Tran Quang\n  Trung, Nguyen Thi Thuy, Dinh Viet Sang", "title": "NeoUNet: Towards accurate colon polyp segmentation and neoplasm\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic polyp segmentation has proven to be immensely helpful for endoscopy\nprocedures, reducing the missing rate of adenoma detection for endoscopists\nwhile increasing efficiency. However, classifying a polyp as being neoplasm or\nnot and segmenting it at the pixel level is still a challenging task for\ndoctors to perform in a limited time. In this work, we propose a fine-grained\nformulation for the polyp segmentation problem. Our formulation aims to not\nonly segment polyp regions, but also identify those at high risk of malignancy\nwith high accuracy. In addition, we present a UNet-based neural network\narchitecture called NeoUNet, along with a hybrid loss function to solve this\nproblem. Experiments show highly competitive results for NeoUNet on our\nbenchmark dataset compared to existing polyp segmentation models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 11:10:12 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lan", "Phan Ngoc", ""], ["An", "Nguyen Sy", ""], ["Hang", "Dao Viet", ""], ["Van Long", "Dao", ""], ["Trung", "Tran Quang", ""], ["Thuy", "Nguyen Thi", ""], ["Sang", "Dinh Viet", ""]]}, {"id": "2107.05025", "submitter": "Young Kyun Jang", "authors": "Young Kyun Jang, Nam Ik Cho", "title": "Similarity Guided Deep Face Image Retrieval", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Face image retrieval, which searches for images of the same identity from the\nquery input face image, is drawing more attention as the size of the image\ndatabase increases rapidly. In order to conduct fast and accurate retrieval, a\ncompact hash code-based methods have been proposed, and recently, deep face\nimage hashing methods with supervised classification training have shown\noutstanding performance. However, classification-based scheme has a\ndisadvantage in that it cannot reveal complex similarities between face images\ninto the hash code learning. In this paper, we attempt to improve the face\nimage retrieval quality by proposing a Similarity Guided Hashing (SGH) method,\nwhich gently considers self and pairwise-similarity simultaneously. SGH employs\nvarious data augmentations designed to explore elaborate similarities between\nface images, solving both intra and inter identity-wise difficulties. Extensive\nexperimental results on the protocols with existing benchmarks and an\nadditionally proposed large scale higher resolution face image dataset\ndemonstrate that our SGH delivers state-of-the-art retrieval performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 11:32:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jang", "Young Kyun", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2107.05031", "submitter": "FangYuan Zhang", "authors": "Fangyuan Zhang, Tianxiang Pan, Bin Wang", "title": "Semi-Supervised Object Detection with Adaptive Class-Rebalancing\n  Self-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study delves into semi-supervised object detection (SSOD) to improve\ndetector performance with additional unlabeled data. State-of-the-art SSOD\nperformance has been achieved recently by self-training, in which training\nsupervision consists of ground truths and pseudo-labels. In current studies, we\nobserve that class imbalance in SSOD severely impedes the effectiveness of\nself-training. To address the class imbalance, we propose adaptive\nclass-rebalancing self-training (ACRST) with a novel memory module called\nCropBank. ACRST adaptively rebalances the training data with foreground\ninstances extracted from the CropBank, thereby alleviating the class imbalance.\nOwing to the high complexity of detection tasks, we observe that both\nself-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.\nTherefore, we propose a novel two-stage filtering algorithm to generate\naccurate pseudo-labels. Our method achieves satisfactory improvements on\nMS-COCO and VOC benchmarks. When using only 1\\% labeled data in MS-COCO, our\nmethod achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP\nimprovement compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 12:14:42 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Fangyuan", ""], ["Pan", "Tianxiang", ""], ["Wang", "Bin", ""]]}, {"id": "2107.05033", "submitter": "Zhongzhan Huang", "authors": "Wei He, Zhongzhan Huang, Mingfu Liang, Senwei Liang, Haizhao Yang", "title": "Blending Pruning Criteria for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of convolutional neural networks (CNNs) on various vision\napplications has attracted lots of attention. Yet the majority of CNNs are\nunable to satisfy the strict requirement for real-world deployment. To overcome\nthis, the recent popular network pruning is an effective method to reduce the\nredundancy of the models. However, the ranking of filters according to their\n\"importance\" on different pruning criteria may be inconsistent. One filter\ncould be important according to a certain criterion, while it is unnecessary\naccording to another one, which indicates that each criterion is only a partial\nview of the comprehensive \"importance\". From this motivation, we propose a\nnovel framework to integrate the existing filter pruning criteria by exploring\nthe criteria diversity. The proposed framework contains two stages: Criteria\nClustering and Filters Importance Calibration. First, we condense the pruning\ncriteria via layerwise clustering based on the rank of \"importance\" score.\nSecond, within each cluster, we propose a calibration factor to adjust their\nsignificance for each selected blending candidates and search for the optimal\nblending criterion via Evolutionary Algorithm. Quantitative results on the\nCIFAR-100 and ImageNet benchmarks show that our framework outperforms the\nstate-of-the-art baselines, regrading to the compact model performance after\npruning.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 12:34:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["He", "Wei", ""], ["Huang", "Zhongzhan", ""], ["Liang", "Mingfu", ""], ["Liang", "Senwei", ""], ["Yang", "Haizhao", ""]]}, {"id": "2107.05037", "submitter": "Hamidreza Bolhasani", "authors": "Pouya Hallaj Zavareh, Atefeh Safayari, Hamidreza Bolhasani", "title": "BCNet: A Deep Convolutional Neural Network for Breast Cancer Grading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer has become one of the most prevalent cancers by which people\nall over the world are affected and is posed serious threats to human beings,\nin a particular woman. In order to provide effective treatment or prevention of\nthis cancer, disease diagnosis in the early stages would be of high importance.\nThere have been various methods to detect this disorder in which using images\nhave to play a dominant role. Deep learning has been recently adopted widely in\ndifferent areas of science, especially medicine. In breast cancer detection\nproblems, some diverse deep learning techniques have been developed on\ndifferent datasets and resulted in good accuracy. In this article, we aimed to\npresent a deep neural network model to classify histopathological images from\nthe Databiox image dataset as the first application on this image database. Our\nproposed model named BCNet has taken advantage of the transfer learning\napproach in which VGG16 is selected from available pertained models as a\nfeature extractor. Furthermore, to address the problem of insufficient data, we\nemployed the data augmentation technique to expand the input dataset. All\nimplementations in this research, ranging from pre-processing actions to\ndepicting the diagram of the model architecture, have been carried out using\ntf.keras API. As a consequence of the proposed model execution, the significant\nvalidation accuracy of 88% and evaluation accuracy of 72% obtained.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 12:55:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zavareh", "Pouya Hallaj", ""], ["Safayari", "Atefeh", ""], ["Bolhasani", "Hamidreza", ""]]}, {"id": "2107.05043", "submitter": "Daisuke Iwai", "authors": "Kenta Yamamoto, Daisuke Iwai, Kosuke Sato", "title": "A Projector-Camera System Using Hybrid Pixels with Projection and\n  Capturing Capabilities", "comments": "Author's version of a paper published at IDW (International Display\n  Workshops) 2020", "journal-ref": "In Proceedings of the International Display Workshops, pp.\n  655-658, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel projector-camera system (ProCams) in which each pixel has\nboth projection and capturing capabilities. Our proposed ProCams solves the\ndifficulty of obtaining precise pixel correspondence between the projector and\nthe camera. We implemented a proof-of-concept ProCams prototype and\ndemonstrated its applicability to a dynamic projection mapping.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 13:27:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Yamamoto", "Kenta", ""], ["Iwai", "Daisuke", ""], ["Sato", "Kosuke", ""]]}, {"id": "2107.05047", "submitter": "Weina Jin", "authors": "Weina Jin, Xiaoxiao Li, Ghassan Hamarneh", "title": "One Map Does Not Fit All: Evaluating Saliency Map Explanation on\n  Multi-Modal Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Being able to explain the prediction to clinical end-users is a necessity to\nleverage the power of AI models for clinical decision support. For medical\nimages, saliency maps are the most common form of explanation. The maps\nhighlight important features for AI model's prediction. Although many saliency\nmap methods have been proposed, it is unknown how well they perform on\nexplaining decisions on multi-modal medical images, where each modality/channel\ncarries distinct clinical meanings of the same underlying biomedical\nphenomenon. Understanding such modality-dependent features is essential for\nclinical users' interpretation of AI decisions. To tackle this clinically\nimportant but technically ignored problem, we propose the MSFI\n(Modality-Specific Feature Importance) metric to examine whether saliency maps\ncan highlight modality-specific important features. MSFI encodes the clinical\nrequirements on modality prioritization and modality-specific feature\nlocalization. Our evaluations on 16 commonly used saliency map methods,\nincluding a clinician user study, show that although most saliency map methods\ncaptured modality importance information in general, most of them failed to\nhighlight modality-specific important features consistently and precisely. The\nevaluation results guide the choices of saliency map methods and provide\ninsights to propose new ones targeting clinical applications.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 13:43:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jin", "Weina", ""], ["Li", "Xiaoxiao", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2107.05073", "submitter": "Xiangzhu Meng", "authors": "Xiangzhu Meng, Wei Wei, Wenzhe Liu", "title": "Locality Relationship Constrained Multi-view Clustering Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In most practical applications, it's common to utilize multiple features from\ndifferent views to represent one object. Among these works, multi-view\nsubspace-based clustering has gained extensive attention from many researchers,\nwhich aims to provide clustering solutions to multi-view data. However, most\nexisting methods fail to take full use of the locality geometric structure and\nsimilarity relationship among samples under the multi-view scenario. To solve\nthese issues, we propose a novel multi-view learning method with locality\nrelationship constraint to explore the problem of multi-view clustering, called\nLocality Relationship Constrained Multi-view Clustering Framework (LRC-MCF).\nLRC-MCF aims to explore the diversity, geometric, consensus and complementary\ninformation among different views, by capturing the locality relationship\ninformation and the common similarity relationships among multiple views.\nMoreover, LRC-MCF takes sufficient consideration to weights of different views\nin finding the common-view locality structure and straightforwardly produce the\nfinal clusters. To effectually reduce the redundancy of the learned\nrepresentations, the low-rank constraint on the common similarity matrix is\nconsidered additionally. To solve the minimization problem of LRC-MCF, an\nAlternating Direction Minimization (ADM) method is provided to iteratively\ncalculate all variables LRC-MCF. Extensive experimental results on seven\nbenchmark multi-view datasets validate the effectiveness of the LRC-MCF method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 15:45:10 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Meng", "Xiangzhu", ""], ["Wei", "Wei", ""], ["Liu", "Wenzhe", ""]]}, {"id": "2107.05078", "submitter": "Zheyi Ma", "authors": "Zheyi Ma, Hao Li, Wen Fang, Qingwen Liu, Bin Zhou and Zhiyong Bu", "title": "A Cloud-Edge-Terminal Collaborative System for Temperature Measurement\n  in COVID-19 Prevention", "comments": "6 pages, 8 figures, INFOCOMW ICCN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary\ntemperature measurement and mask detection in public areas are conducted.\nHowever, the existing temperature measurement methods face the problems of\nsafety and deployment. In this paper, to realize safe and accurate temperature\nmeasurement even when a person's face is partially obscured, we propose a\ncloud-edge-terminal collaborative system with a lightweight infrared\ntemperature measurement model. A binocular camera with an RGB lens and a\nthermal lens is utilized to simultaneously capture image pairs. Then, a mobile\ndetection model based on a multi-task cascaded convolutional network (MTCNN) is\nproposed to realize face alignment and mask detection on the RGB images. For\naccurate temperature measurement, we transform the facial landmarks on the RGB\nimages to the thermal images by an affine transformation and select a more\naccurate temperature measurement area on the forehead. The collected\ninformation is uploaded to the cloud in real time for COVID-19 prevention.\nExperiments show that the detection model is only 6.1M and the average\ndetection speed is 257ms. At a distance of 1m, the error of indoor temperature\nmeasurement is about 3%. That is, the proposed system can realize real-time\ntemperature measurement in public areas.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:15:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ma", "Zheyi", ""], ["Li", "Hao", ""], ["Fang", "Wen", ""], ["Liu", "Qingwen", ""], ["Zhou", "Bin", ""], ["Bu", "Zhiyong", ""]]}, {"id": "2107.05080", "submitter": "Xuan Kan", "authors": "Xuan Kan, Hejie Cui, Carl Yang", "title": "Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge\n  Integration", "comments": "This paper has been accepted for presentation in the Research Track\n  of ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation prediction among entities in images is an important step in scene\ngraph generation (SGG), which further impacts various visual understanding and\nreasoning tasks. Existing SGG frameworks, however, require heavy training yet\nare incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we\nstress that such incapability is due to the lack of commonsense reasoning,i.e.,\nthe ability to associate similar entities and infer similar relations based on\ngeneral understanding of the world. To fill this gap, we propose\nCommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to\nintegrate commonsense knowledge for SGG, especially for zero-shot relation\nprediction. Specifically, we develop novel graph mining pipelines to model the\nneighborhoods and paths around entities in an external commonsense knowledge\ngraph, and integrate them on top of state-of-the-art SGG frameworks. Extensive\nquantitative evaluations and qualitative case studies on both original and\nmanipulated datasets from Visual Genome demonstrate the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:22:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kan", "Xuan", ""], ["Cui", "Hejie", ""], ["Yang", "Carl", ""]]}, {"id": "2107.05085", "submitter": "Gorkem Polat", "authors": "Gorkem Polat, Yesim Dogrusoz Serinagaoglu, Ugur Halici", "title": "Effect of Input Size on the Classification of Lung Nodules Using\n  Convolutional Neural Networks", "comments": "4 pages, in Turkish language, 2018 26th Signal Processing and\n  Communications Applications Conference (SIU)", "journal-ref": null, "doi": "10.1109/SIU.2018.8404659", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have shown that lung cancer screening using annual low-dose\ncomputed tomography (CT) reduces lung cancer mortality by 20% compared to\ntraditional chest radiography. Therefore, CT lung screening has started to be\nused widely all across the world. However, analyzing these images is a serious\nburden for radiologists. The number of slices in a CT scan can be up to 600.\nTherefore, computer-aided-detection (CAD) systems are very important for faster\nand more accurate assessment of the data. In this study, we proposed a\nframework that analyzes CT lung screenings using convolutional neural networks\n(CNNs) to reduce false positives. We trained our model with different volume\nsizes and showed that volume size plays a critical role in the performance of\nthe system. We also used different fusions in order to show their power and\neffect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D\nconvolutional operations applied to 3D data could result in information loss.\nThe proposed framework has been tested on the dataset provided by the LUNA16\nChallenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:52:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Polat", "Gorkem", ""], ["Serinagaoglu", "Yesim Dogrusoz", ""], ["Halici", "Ugur", ""]]}, {"id": "2107.05087", "submitter": "Joshua Mathew", "authors": "Joshua Mathew, Xin Tian, Min Wu, Chau-Wai Wong", "title": "Remote Blood Oxygen Estimation From Videos Using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory\nfunctionality and is receiving increasing attention during the COVID-19\npandemic. Clinical findings show that it is possible for COVID-19 patients to\nhave significantly low SpO$_2$ before any obvious symptoms. The prevalence of\ncameras has motivated researchers to investigate methods for monitoring SpO$_2$\nusing videos. Most prior schemes involving smartphones are contact-based: They\nrequire a fingertip to cover the phone's camera and the nearby light source to\ncapture re-emitted light from the illuminated tissue. In this paper, we propose\nthe first convolutional neural network based noncontact SpO$_2$ estimation\nscheme using smartphone cameras. The scheme analyzes the videos of a\nparticipant's hand for physiological sensing, which is convenient and\ncomfortable, and can protect their privacy and allow for keeping face masks on.\nWe design our neural network architectures inspired by the optophysiological\nmodels for SpO$_2$ measurement and demonstrate the explainability by\nvisualizing the weights for channel combination. Our proposed models outperform\nthe state-of-the-art model that is designed for contact-based SpO$_2$\nmeasurement, showing the potential of our proposed method to contribute to\npublic health. We also analyze the impact of skin type and the side of a hand\non SpO$_2$ estimation performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:59:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mathew", "Joshua", ""], ["Tian", "Xin", ""], ["Wu", "Min", ""], ["Wong", "Chau-Wai", ""]]}, {"id": "2107.05093", "submitter": "Shuo-En Chang", "authors": "Shuo-En Chang, Yi-Cheng Yang, En-Ting Lin, Pei-Yung Hsiao, Li-Chen Fu", "title": "SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation\n  Network", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been a panoptic segmentation task combining semantic and\ninstance segmentation, in which the goal is to classify each pixel with the\ncorresponding instance ID. In this work, we propose a solution to tackle the\npanoptic segmentation task. The overall structure combines the bottom-up method\nand the top-down method. Therefore, not only can there be better performance,\nbut also the execution speed can be maintained. The network mainly pays\nattention to the quality of the mask. In the previous work, we can see that the\nuneven contour of the object is more likely to appear, resulting in low-quality\nprediction. Accordingly, we propose enhancement features and corresponding loss\nfunctions for the silhouette of objects and backgrounds to improve the mask.\nMeanwhile, we use the new proposed confidence score to solve the occlusion\nproblem and make the network tend to use higher quality masks as prediction\nresults. To verify our research, we used the COCO dataset and CityScapes\ndataset to do experiments and obtained competitive results with fast inference\ntime.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 17:20:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chang", "Shuo-En", ""], ["Yang", "Yi-Cheng", ""], ["Lin", "En-Ting", ""], ["Hsiao", "Pei-Yung", ""], ["Fu", "Li-Chen", ""]]}, {"id": "2107.05097", "submitter": "Hejie Cui", "authors": "Hejie Cui, Wei Dai, Yanqiao Zhu, Xiaoxiao Li, Lifang He, Carl Yang", "title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for\n  Brain Network based Disease Analysis", "comments": "This paper has been accepted to ICML 2021 Workshop on Interpretable\n  Machine Learning in Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable brain network models for disease prediction are of great value\nfor the advancement of neuroscience. GNNs are promising to model complicated\nnetwork data, but they are prone to overfitting and suffer from poor\ninterpretability, which prevents their usage in decision-critical scenarios\nlike healthcare. To bridge this gap, we propose BrainNNExplainer, an\ninterpretable GNN framework for brain network analysis. It is mainly composed\nof two jointly learned modules: a backbone prediction model that is\nspecifically designed for brain networks and an explanation generator that\nhighlights disease-specific prominent brain network connections. Extensive\nexperimental results with visualizations on two challenging disease prediction\ndatasets demonstrate the unique interpretability and outstanding performance of\nBrainNNExplainer.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 17:33:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cui", "Hejie", ""], ["Dai", "Wei", ""], ["Zhu", "Yanqiao", ""], ["Li", "Xiaoxiao", ""], ["He", "Lifang", ""], ["Yang", "Carl", ""]]}, {"id": "2107.05113", "submitter": "Sushobhan Ghosh", "authors": "Sushobhan Ghosh, Zhaoyang Lv, Nathan Matsuda, Lei Xiao, Andrew\n  Berkovich, Oliver Cossairt", "title": "LiveView: Dynamic Target-Centered MPI for View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Multi-Plane Image (MPI) based view-synthesis methods generate an MPI\naligned with the input view using a fixed number of planes in one forward pass.\nThese methods produce fast, high-quality rendering of novel views, but rely on\nslow and computationally expensive MPI generation methods unsuitable for\nreal-time applications. In addition, most MPI techniques use fixed\ndepth/disparity planes which cannot be modified once the training is complete,\nhence offering very little flexibility at run-time.\n  We propose LiveView - a novel MPI generation and rendering technique that\nproduces high-quality view synthesis in real-time. Our method can also offer\nthe flexibility to select scene-dependent MPI planes (number of planes and\nspacing between them) at run-time. LiveView first warps input images to target\nview (target-centered) and then learns to generate a target view centered MPI,\none depth plane at a time (dynamically). The method generates high-quality\nrenderings, while also enabling fast MPI generation and novel view synthesis.\nAs a result, LiveView enables real-time view synthesis applications where an\nMPI needs to be updated frequently based on a video stream of input views. We\ndemonstrate that LiveView improves the quality of view synthesis while being 70\ntimes faster at run-time compared to state-of-the-art MPI-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 19:01:56 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ghosh", "Sushobhan", ""], ["Lv", "Zhaoyang", ""], ["Matsuda", "Nathan", ""], ["Xiao", "Lei", ""], ["Berkovich", "Andrew", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2107.05115", "submitter": "Basit Alawode", "authors": "Basit O. Alawode, Mudassir Masood, Tarig Ballal, and Tareq Al-Naffouri", "title": "Details Preserving Deep Collaborative Filtering-Based Method for Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In spite of the improvements achieved by the several denoising algorithms\nover the years, many of them still fail at preserving the fine details of the\nimage after denoising. This is as a result of the smooth-out effect they have\non the images. Most neural network-based algorithms have achieved better\nquantitative performance than the classical denoising algorithms. However, they\nalso suffer from qualitative (visual) performance as a result of the smooth-out\neffect. In this paper, we propose an algorithm to address this shortcoming. We\npropose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image\ndenoising. This algorithm performs collaborative denoising of image patches in\nthe sparse domain using a set of optimized neural network models. This results\nin a fast algorithm that is able to excellently obtain a trade-off between\nnoise removal and details preservation. Extensive experiments show that the\nDeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and\nqualitatively (visually) better than many of the state-of-the-art denoising\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 19:02:36 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 16:53:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Alawode", "Basit O.", ""], ["Masood", "Mudassir", ""], ["Ballal", "Tarig", ""], ["Al-Naffouri", "Tareq", ""]]}, {"id": "2107.05121", "submitter": "Naoki Saito", "authors": "Naoki Saito and Yiqun Shao", "title": "eGHWT: The extended Generalized Haar-Walsh Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.IT cs.NA math.CO math.IT math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extending computational harmonic analysis tools from the classical setting of\nregular lattices to the more general setting of graphs and networks is very\nimportant and much research has been done recently. The Generalized Haar-Walsh\nTransform (GHWT) developed by Irion and Saito (2014) is a multiscale transform\nfor signals on graphs, which is a generalization of the classical Haar and\nWalsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh\nTransform (eGHWT), which is a generalization of the adapted time-frequency\ntilings of Thiele and Villemoes (1996). The eGHWT examines not only the\nefficiency of graph-domain partitions but also that of \"sequency-domain\"\npartitions simultaneously. Consequently, the eGHWT and its associated\nbest-basis selection algorithm for graph signals significantly improve the\nperformance of the previous GHWT with the similar computational cost, $O(N \\log\nN)$, where $N$ is the number of nodes of an input graph. While the GHWT\nbest-basis algorithm seeks the most suitable orthonormal basis for a given task\namong more than $(1.5)^N$ possible orthonormal bases in $\\mathbb{R}^N$, the\neGHWT best-basis algorithm can find a better one by searching through more than\n$0.618\\cdot(1.84)^N$ possible orthonormal bases in $\\mathbb{R}^N$. This article\ndescribes the details of the eGHWT best-basis algorithm and demonstrates its\nsuperiority using several examples including genuine graph signals as well as\nconventional digital images viewed as graph signals. Furthermore, we also show\nhow the eGHWT can be extended to 2D signals and matrix-form data by viewing\nthem as a tensor product of graphs generated from their columns and rows and\ndemonstrate its effectiveness on applications such as image approximation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 19:39:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Saito", "Naoki", ""], ["Shao", "Yiqun", ""]]}, {"id": "2107.05122", "submitter": "He Zhao", "authors": "He Zhao, Richard P. Wildes", "title": "Interpretable Deep Feature Propagation for Early Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early action recognition (action prediction) from limited preliminary\nobservations plays a critical role for streaming vision systems that demand\nreal-time inference, as video actions often possess elongated temporal spans\nwhich cause undesired latency. In this study, we address action prediction by\ninvestigating how action patterns evolve over time in a spatial feature space.\nThere are three key components to our system. First, we work with\nintermediate-layer ConvNet features, which allow for abstraction from raw data,\nwhile retaining spatial layout. Second, instead of propagating features per se,\nwe propagate their residuals across time, which allows for a compact\nrepresentation that reduces redundancy. Third, we employ a Kalman filter to\ncombat error build-up and unify across prediction start times. Extensive\nexperimental results on multiple benchmarks show that our approach leads to\ncompetitive performance in action prediction. Notably, we investigate the\nlearned components of our system to shed light on their otherwise opaque\nnatures in two ways. First, we document that our learned feature propagation\nmodule works as a spatial shifting mechanism under convolution to propagate\ncurrent observations into the future. Thus, it captures flow-based image motion\ninformation. Second, the learned Kalman filter adaptively updates prior\nestimation to aid the sequence learning process.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 19:40:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhao", "He", ""], ["Wildes", "Richard P.", ""]]}, {"id": "2107.05140", "submitter": "He Zhao", "authors": "He Zhao, Richard P. Wildes", "title": "Review of Video Predictive Understanding: Early Action Recognition and\n  Future Action Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video predictive understanding encompasses a wide range of efforts that are\nconcerned with the anticipation of the unobserved future from the current as\nwell as historical video observations. Action prediction is a major sub-area of\nvideo predictive understanding and is the focus of this review. This sub-area\nhas two major subdivisions: early action recognition and future action\nprediction. Early action recognition is concerned with recognizing an ongoing\naction as soon as possible. Future action prediction is concerned with the\nanticipation of actions that follow those previously observed. In either case,\nthe \\textbf{\\textit{causal}} relationship between the past, current, and\npotential future information is the main focus. Various mathematical tools such\nas Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian\nrecursive filtering are widely adopted jointly with computer vision techniques\nfor these two tasks. However, these approaches face challenges such as the\ncurse of dimensionality, poor generalization, and constraints from\ndomain-specific knowledge. Recently, structures that rely on deep convolutional\nneural networks and recurrent neural networks have been extensively proposed\nfor improving the performance of existing vision tasks, in general, and action\nprediction tasks, in particular. However, they have their own shortcomings, \\eg\nreliance on massive training data and lack of strong theoretical underpinnings.\nIn this survey, we start by introducing the major sub-areas of the broad area\nof video predictive understanding, which recently have received intensive\nattention and proven to have practical value. Next, a thorough review of\nvarious early action recognition and future action prediction algorithms are\nprovided with suitably organized divisions. Finally, we conclude our discussion\nwith future research directions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 22:46:52 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 19:34:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhao", "He", ""], ["Wildes", "Richard P.", ""]]}, {"id": "2107.05150", "submitter": "Ramin Nabati", "authors": "Ramin Nabati, Landon Harris, Hairong Qi", "title": "CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object\n  Tracking", "comments": "2021 IEEE Intelligent Vehicles Symposium, 3D-Deep Learning for\n  Autonomous Driving Workshop (WS15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D multi-object tracking is a crucial component in the perception system of\nautonomous driving vehicles. Tracking all dynamic objects around the vehicle is\nessential for tasks such as obstacle avoidance and path planning. Autonomous\nvehicles are usually equipped with different sensor modalities to improve\naccuracy and reliability. While sensor fusion has been widely used in object\ndetection networks in recent years, most existing multi-object tracking\nalgorithms either rely on a single input modality, or do not fully exploit the\ninformation provided by multiple sensing modalities. In this work, we propose\nan end-to-end network for joint object detection and tracking based on radar\nand camera sensor fusion. Our proposed method uses a center-based radar-camera\nfusion algorithm for object detection and utilizes a greedy algorithm for\nobject association. The proposed greedy algorithm uses the depth, velocity and\n2D displacement of the detected objects to associate them through time. This\nmakes our tracking algorithm very robust to occluded and overlapping objects,\nas the depth and velocity information can help the network in distinguishing\nthem. We evaluate our method on the challenging nuScenes dataset, where it\nachieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the\nbenchmark, as well as the baseline LiDAR-based method. Our method is online\nwith a runtime of 35ms per image, making it very suitable for autonomous\ndriving applications.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 23:56:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nabati", "Ramin", ""], ["Harris", "Landon", ""], ["Qi", "Hairong", ""]]}, {"id": "2107.05160", "submitter": "Xinqi Fan", "authors": "Shuyi Mao, Xinqi Fan, Xiaojiang Peng", "title": "Spatial and Temporal Networks for Facial Expression Recognition in the\n  Wild Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper describes our proposed methodology for the seven basic expression\nclassification track of Affective Behavior Analysis in-the-wild (ABAW)\nCompetition 2021. In this task, facial expression recognition (FER) methods aim\nto classify the correct expression category from a diverse background, but\nthere are several challenges. First, to adapt the model to in-the-wild\nscenarios, we use the knowledge from pre-trained large-scale face recognition\ndata. Second, we propose an ensemble model with a convolution neural network\n(CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer\n(CNN-Transformer), to incorporate both spatial and temporal information. Our\nensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as\n0.4821 on the validation set.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 01:41:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mao", "Shuyi", ""], ["Fan", "Xinqi", ""], ["Peng", "Xiaojiang", ""]]}, {"id": "2107.05176", "submitter": "Guangyue Xu", "authors": "Guangyue Xu, Parisa Kordjamshidi, Joyce Y. Chai", "title": "Zero-Shot Compositional Concept Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the problem of recognizing compositional\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\npropose an episode-based cross-attention (EpiCA) network which combines merits\nof cross-attention mechanism and episode-based training strategy to recognize\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\ncorrelate concept-visual information and utilizes the gated pooling layer to\nbuild contextualized representations for both images and concepts. The updated\nrepresentations are used for a more in-depth multi-modal relevance calculation\nfor concept recognition. Secondly, a two-phase episode training strategy,\nespecially the transductive phase, is adopted to utilize unlabeled test\nexamples to alleviate the low-resource learning problem. Experiments on two\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\ndemonstrated the effectiveness of the model compared with recent approaches on\nboth conventional and generalized ZSCL settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 03:31:56 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xu", "Guangyue", ""], ["Kordjamshidi", "Parisa", ""], ["Chai", "Joyce Y.", ""]]}, {"id": "2107.05186", "submitter": "Joerg Wolf", "authors": "Joerg Christian Wolf", "title": "Early warning of pedestrians and cyclists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art motor vehicles are able to break for pedestrians in an\nemergency. We investigate what it would take to issue an early warning to the\ndriver so he/she has time to react. We have identified that predicting the\nintention of a pedestrian reliably by position is a particularly hard\nchallenge. This paper describes an early pedestrian warning demonstration\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 04:02:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wolf", "Joerg Christian", ""]]}, {"id": "2107.05188", "submitter": "Menghan Hu", "authors": "Yao Chang, Hu Menghan, Zhai Guangtao, Zhang Xiao-Ping", "title": "TransClaw U-Net: Claw U-Net with Transformers for Medical Image\n  Segmentation", "comments": "8 page, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, computer-aided diagnosis has become an increasingly popular\ntopic. Methods based on convolutional neural networks have achieved good\nperformance in medical image segmentation and classification. Due to the\nlimitations of the convolution operation, the long-term spatial features are\noften not accurately obtained. Hence, we propose a TransClaw U-Net network\nstructure, which combines the convolution operation with the transformer\noperation in the encoding part. The convolution part is applied for extracting\nthe shallow spatial features to facilitate the recovery of the image resolution\nafter upsampling. The transformer part is used to encode the patches, and the\nself-attention mechanism is used to obtain global information between\nsequences. The decoding part retains the bottom upsampling structure for better\ndetail segmentation performance. The experimental results on Synapse\nMulti-organ Segmentation Datasets show that the performance of TransClaw U-Net\nis better than other network structures. The ablation experiments also prove\nthe generalization performance of TransClaw U-Net.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 04:17:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chang", "Yao", ""], ["Menghan", "Hu", ""], ["Guangtao", "Zhai", ""], ["Xiao-Ping", "Zhang", ""]]}, {"id": "2107.05190", "submitter": "Xinyu Gao", "authors": "Xinyu Gao, Tianlang Wang, Jing Yang, Jinchao Tao, Yanqing Qiu, Yanlong\n  Meng, Banging Mao, Pengwei Zhou, and Yi Li", "title": "Deep-learning-based Hyperspectral imaging through a RGB camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral image (HSI) contains both spatial pattern and spectral\ninformation which has been widely used in food safety, remote sensing, and\nmedical detection. However, the acquisition of hyperspectral images is usually\ncostly due to the complicated apparatus for the acquisition of optical\nspectrum. Recently, it has been reported that HSI can be reconstructed from\nsingle RGB image using convolution neural network (CNN) algorithms. Compared\nwith the traditional hyperspectral cameras, the method based on CNN algorithms\nis simple, portable and low cost. In this study, we focused on the influence of\nthe RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated\nwith a monochromator were used as the standard light source to calibrate the\nCSS. And the experimental results show that the CSS plays a significant role in\nthe reconstruction accuracy of an HSI. In addition, we proposed a new HSI\nreconstruction network where the dimensional structure of the original\nhyperspectral datacube was modified by 3D matrix transpose to improve the\nreconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 04:23:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gao", "Xinyu", ""], ["Wang", "Tianlang", ""], ["Yang", "Jing", ""], ["Tao", "Jinchao", ""], ["Qiu", "Yanqing", ""], ["Meng", "Yanlong", ""], ["Mao", "Banging", ""], ["Zhou", "Pengwei", ""], ["Li", "Yi", ""]]}, {"id": "2107.05202", "submitter": "Daniil Pakhomov", "authors": "Sanchit Hira, Ritwik Das, Abhinav Modi, Daniil Pakhomov", "title": "Delta Sampling R-BERT for limited data and low-light action recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach to perform supervised action recognition in the dark.\nIn this work, we present our results on the ARID dataset. Most previous works\nonly evaluate performance on large, well illuminated datasets like Kinetics and\nHMDB51. We demonstrate that our work is able to achieve a very low error rate\nwhile being trained on a much smaller dataset of dark videos. We also explore a\nvariety of training and inference strategies including domain transfer\nmethodologies and also propose a simple but useful frame selection strategy.\nOur empirical results demonstrate that we beat previously published baseline\nmodels by 11%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 05:35:51 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hira", "Sanchit", ""], ["Das", "Ritwik", ""], ["Modi", "Abhinav", ""], ["Pakhomov", "Daniil", ""]]}, {"id": "2107.05214", "submitter": "ZhenRong Zhang", "authors": "Zhenrong Zhang, Jianshu Zhang and Jun Du", "title": "Split, embed and merge: An accurate table structure recognizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Table structure recognition is an essential part for making machines\nunderstand tables. Its main task is to recognize the internal structure of a\ntable. However, due to the complexity and diversity in their structure and\nstyle, it is very difficult to parse the tabular data into the structured\nformat which machines can understand easily, especially for complex tables. In\nthis paper, we introduce Split, Embed and Merge (SEM), an accurate table\nstructure recognizer. Our model takes table images as input and can correctly\nrecognize the structure of tables, whether they are simple or a complex tables.\nSEM is mainly composed of three parts, splitter, embedder and merger. In the\nfirst stage, we apply the splitter to predict the potential regions of the\ntable row (column) separators, and obtain the fine grid structure of the table.\nIn the second stage, by taking a full consideration of the textual information\nin the table, we fuse the output features for each table grid from both vision\nand language modalities. Moreover, we achieve a higher precision in our\nexperiments through adding additional semantic features. Finally, we process\nthe merging of these basic table grids in a self-regression manner. The\ncorrespondent merging results is learned through the attention mechanism. In\nour experiments, SEM achieves an average F1-Measure of 97.11% on the SciTSR\ndataset which outperforms other methods by a large margin. We also won the\nfirst place in the complex table and third place in all tables in ICDAR 2021\nCompetition on Scientific Literature Parsing, Task-B. Extensive experiments on\nother publicly available datasets demonstrate that our model achieves\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 06:26:19 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 13:18:55 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhang", "Zhenrong", ""], ["Zhang", "Jianshu", ""], ["Du", "Jun", ""]]}, {"id": "2107.05238", "submitter": "Sho Ozaki", "authors": "Sho Ozaki, Shizuo Kaji, Kanabu Nawa, Toshikazu Imae, Atsushi Aoki,\n  Takahiro Nakamoto, Takeshi Ohta, Yuki Nozawa, Hideomi Yamashita, Akihiro\n  Haga, Keiichi Nakagawa", "title": "Training deep cross-modality conversion models with a small amount of\n  data and its application to MVCT to kVCT conversion", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning-based image processing has emerged as a valuable tool in recent\nyears owing to its high performance. However, the quality of\ndeep-learning-based methods relies heavily on the amount of training data, and\nthe cost of acquiring a large amount of data is often prohibitive in medical\nfields. Therefore, we performed CT modality conversion based on deep learning\nrequiring only a small number of unsupervised images. The proposed method is\nbased on generative adversarial networks (GANs) with several extensions\ntailored for CT images. This method emphasizes the preservation of the\nstructure in the processed images and reduction in the amount of training data.\nThis method was applied to realize the conversion of mega-voltage computed\ntomography (MVCT) to kilo-voltage computed tomography (kVCT) images. Training\nwas performed using several datasets acquired from patients with head and neck\ncancer. The size of the datasets ranged from 16 slices (for two patients) to\n2745 slices (for 137 patients) of MVCT and 2824 slices of kVCT for 98 patients.\nThe quality of the processed MVCT images was considerably enhanced, and the\nstructural changes in the images were minimized. With an increase in the size\nof training data, the image quality exhibited a satisfactory convergence from a\nfew hundred slices. In addition to statistical and visual evaluations, these\nresults were clinically evaluated by medical doctors in terms of the accuracy\nof contouring. We developed an MVCT to kVCT conversion model based on deep\nlearning, which can be trained using a few hundred unpaired images. The\nstability of the model against the change in the data size was demonstrated.\nThis research promotes the reliable use of deep learning in clinical medicine\nby partially answering the commonly asked questions: \"Is our data enough? How\nmuch data must we prepare?\"\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 07:43:41 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ozaki", "Sho", ""], ["Kaji", "Shizuo", ""], ["Nawa", "Kanabu", ""], ["Imae", "Toshikazu", ""], ["Aoki", "Atsushi", ""], ["Nakamoto", "Takahiro", ""], ["Ohta", "Takeshi", ""], ["Nozawa", "Yuki", ""], ["Yamashita", "Hideomi", ""], ["Haga", "Akihiro", ""], ["Nakagawa", "Keiichi", ""]]}, {"id": "2107.05241", "submitter": "Vinod K Kurmi", "authors": "Blessen George and Vinod K. Kurmi and Vinay P. Namboodiri", "title": "Prb-GAN: A Probabilistic Framework for GAN Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative adversarial networks (GANs) are very popular to generate realistic\nimages, but they often suffer from the training instability issues and the\nphenomenon of mode loss. In order to attain greater diversity in GAN\nsynthesized data, it is critical to solving the problem of mode loss. Our work\nexplores probabilistic approaches to GAN modelling that could allow us to\ntackle these issues. We present Prb-GANs, a new variation that uses dropout to\ncreate a distribution over the network parameters with the posterior learnt\nusing variational inference. We describe theoretically and validate\nexperimentally using simple and complex datasets the benefits of such an\napproach. We look into further improvements using the concept of uncertainty\nmeasures. Through a set of further modifications to the loss functions for each\nnetwork of the GAN, we are able to get results that show the improvement of GAN\nperformance. Our methods are extremely simple and require very little\nmodification to existing GAN architecture.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 08:04:13 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["George", "Blessen", ""], ["Kurmi", "Vinod K.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2107.05250", "submitter": "Zipeng Xu", "authors": "Zipeng Xu, Fandong Meng, Xiaojie Wang, Duo Zheng, Chenxu Lv and Jie\n  Zhou", "title": "Modeling Explicit Concerning States for Reinforcement Learning in Visual\n  Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of\nReinforcement Learning has been proven potential. In Reinforcement Learning, it\nis crucial to represent states and assign rewards based on the action-caused\ntransitions of states. However, the state representation in previous Visual\nDialogue works uses the textual information only and its transitions are\nimplicit. In this paper, we propose Explicit Concerning States (ECS) to\nrepresent what visual contents are concerned at each round and what have been\nconcerned throughout the Visual Dialogue. ECS is modeled from multimodal\ninformation and is represented explicitly. Based on ECS, we formulate two\nintuitive and interpretable rewards to encourage the Visual Dialogue agents to\nconverse on diverse and informative visual information. Experimental results on\nthe VisDial v1.0 dataset show our method enables the Visual Dialogue agents to\ngenerate more visual coherent, less repetitive and more visual informative\ndialogues compared with previous methods, according to multiple automatic\nmetrics, human study and qualitative analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 08:15:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xu", "Zipeng", ""], ["Meng", "Fandong", ""], ["Wang", "Xiaojie", ""], ["Zheng", "Duo", ""], ["Lv", "Chenxu", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.05255", "submitter": "Sophia Bano", "authors": "Sophia Bano, Brian Dromey, Francisco Vasconcelos, Raffaele Napolitano,\n  Anna L. David, Donald M. Peebles, Danail Stoyanov", "title": "AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound\n  Planes", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During pregnancy, ultrasound examination in the second trimester can assess\nfetal size according to standardized charts. To achieve a reproducible and\naccurate measurement, a sonographer needs to identify three standard 2D planes\nof the fetal anatomy (head, abdomen, femur) and manually mark the key\nanatomical landmarks on the image for accurate biometry and fetal weight\nestimation. This can be a time-consuming operator-dependent task, especially\nfor a trainee sonographer. Computer-assisted techniques can help in automating\nthe fetal biometry computation process. In this paper, we present a unified\nautomated framework for estimating all measurements needed for the fetal weight\nassessment. The proposed framework semantically segments the key fetal\nanatomies using state-of-the-art segmentation models, followed by region\nfitting and scale recovery for the biometry estimation. We present an ablation\nstudy of segmentation algorithms to show their robustness through 4-fold\ncross-validation on a dataset of 349 ultrasound standard plane images from 42\npregnancies. Moreover, we show that the network with the best segmentation\nperformance tends to be more accurate for biometry estimation. Furthermore, we\ndemonstrate that the error between clinically measured and predicted fetal\nbiometry is lower than the permissible error during routine clinical\nmeasurements.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 08:42:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bano", "Sophia", ""], ["Dromey", "Brian", ""], ["Vasconcelos", "Francisco", ""], ["Napolitano", "Raffaele", ""], ["David", "Anna L.", ""], ["Peebles", "Donald M.", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2107.05270", "submitter": "Or Bar-Shira", "authors": "Or Bar-Shira, Ahuva Grubstein, Yael Rapson, Dror Suhami, Eli Atar,\n  Keren Peri-Hanania, Ronnie Rosen, Yonina C. Eldar", "title": "Learned super resolution ultrasound for improved breast lesion\n  characterization", "comments": "to be published in MICCAI 2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common malignancy in women. Mammographic findings\nsuch as microcalcifications and masses, as well as morphologic features of\nmasses in sonographic scans, are the main diagnostic targets for tumor\ndetection. However, improved specificity of these imaging modalities is\nrequired. A leading alternative target is neoangiogenesis. When pathological,\nit contributes to the development of numerous types of tumors, and the\nformation of metastases. Hence, demonstrating neoangiogenesis by visualization\nof the microvasculature may be of great importance. Super resolution ultrasound\nlocalization microscopy enables imaging of the microvasculature at the\ncapillary level. Yet, challenges such as long reconstruction time, dependency\non prior knowledge of the system Point Spread Function (PSF), and separability\nof the Ultrasound Contrast Agents (UCAs), need to be addressed for translation\nof super-resolution US into the clinic. In this work we use a deep neural\nnetwork architecture that makes effective use of signal structure to address\nthese challenges. We present in vivo human results of three different breast\nlesions acquired with a clinical US scanner. By leveraging our trained network,\nthe microvasculature structure is recovered in a short time, without prior PSF\nknowledge, and without requiring separability of the UCAs. Each of the\nrecoveries exhibits a different structure that corresponds with the known\nhistological structure. This study demonstrates the feasibility of in vivo\nhuman super resolution, based on a clinical scanner, to increase US specificity\nfor different breast lesions and promotes the use of US in the diagnosis of\nbreast pathologies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:04:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bar-Shira", "Or", ""], ["Grubstein", "Ahuva", ""], ["Rapson", "Yael", ""], ["Suhami", "Dror", ""], ["Atar", "Eli", ""], ["Peri-Hanania", "Keren", ""], ["Rosen", "Ronnie", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "2107.05274", "submitter": "Bingzhi Chen", "authors": "Bingzhi Chen, Yishu Liu, Zheng Zhang, Guangming Lu, David Zhang", "title": "TransAttUnet: Multi-level Attention-guided U-Net with Transformer for\n  Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep encoder-decoder architectures and large-scale\nannotated medical datasets, great progress has been achieved in the development\nof automatic medical image segmentation. Due to the stacking of convolution\nlayers and the consecutive sampling operations, existing standard models\ninevitably encounter the information recession problem of feature\nrepresentations, which fails to fully model the global contextual feature\ndependencies. To overcome the above challenges, this paper proposes a novel\nTransformer based medical image semantic segmentation framework called\nTransAttUnet, in which the multi-level guided attention and multi-scale skip\nconnection are jointly designed to effectively enhance the functionality and\nflexibility of traditional U-shaped architecture. Inspired by Transformer, a\nnovel self-aware attention (SAA) module with both Transformer Self Attention\n(TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to\neffectively learn the non-local interactions between encoder features. In\nparticular, we also establish additional multi-scale skip connections between\ndecoder blocks to aggregate the different semantic-scale upsampling features.\nIn this way, the representation ability of multi-scale context information is\nstrengthened to generate discriminative features. Benefitting from these\ncomplementary components, the proposed TransAttUnet can effectively alleviate\nthe loss of fine details caused by the information recession problem, improving\nthe diagnostic sensitivity and segmentation quality of medical image analysis.\nExtensive experiments on multiple medical image segmentation datasets of\ndifferent imaging demonstrate that our method consistently outperforms the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:17:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Bingzhi", ""], ["Liu", "Yishu", ""], ["Zhang", "Zheng", ""], ["Lu", "Guangming", ""], ["Zhang", "David", ""]]}, {"id": "2107.05276", "submitter": "Wenyuan Li", "authors": "Wenyuan Li, Keyan Chen, Hao Chen and Zhenwei Shi", "title": "Geographical Knowledge-driven Representation Learning for Remote Sensing\n  Images", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of remote sensing satellites has resulted in a massive\namount of remote sensing images. However, due to human and material resource\nconstraints, the vast majority of remote sensing images remain unlabeled. As a\nresult, it cannot be applied to currently available deep learning methods. To\nfully utilize the remaining unlabeled images, we propose a Geographical\nKnowledge-driven Representation learning method for remote sensing images\n(GeoKR), improving network performance and reduce the demand for annotated\ndata. The global land cover products and geographical location associated with\neach remote sensing image are regarded as geographical knowledge to provide\nsupervision for representation learning and network pre-training. An efficient\npre-training framework is proposed to eliminate the supervision noises caused\nby imaging times and resolutions difference between remote sensing images and\ngeographical knowledge. A large scale pre-training dataset Levir-KR is proposed\nto support network pre-training. It contains 1,431,950 remote sensing images\nfrom Gaofen series satellites with various resolutions. Experimental results\ndemonstrate that our proposed method outperforms ImageNet pre-training and\nself-supervised representation learning methods and significantly reduces the\nburden of data annotation on downstream tasks such as scene classification,\nsemantic segmentation, object detection, and cloud / snow detection. It\ndemonstrates that our proposed method can be used as a novel paradigm for\npre-training neural networks. Codes will be available on\nhttps://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:23:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Wenyuan", ""], ["Chen", "Keyan", ""], ["Chen", "Hao", ""], ["Shi", "Zhenwei", ""]]}, {"id": "2107.05279", "submitter": "Chun Chet Ng", "authors": "Chun Chet Ng, Akmalul Khairi Bin Nazaruddin, Yeong Khang Lee, Xinyu\n  Wang, Yuliang Liu, Chee Seng Chan, Lianwen Jin, Yipeng Sun, and Lixin Fan", "title": "ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic\n  Assessment", "comments": "Technical report of ICDAR 2021 Competition on Integrated Circuit Text\n  Spotting and Aesthetic Assessment", "journal-ref": "International Conference on Document Analysis and Recognition\n  (ICDAR) 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With hundreds of thousands of electronic chip components are being\nmanufactured every day, chip manufacturers have seen an increasing demand in\nseeking a more efficient and effective way of inspecting the quality of printed\ntexts on chip components. The major problem that deters this area of research\nis the lacking of realistic text on chips datasets to act as a strong\nfoundation. Hence, a text on chips dataset, ICText is used as the main target\nfor the proposed Robust Reading Challenge on Integrated Circuit Text Spotting\nand Aesthetic Assessment (RRC-ICText) 2021 to encourage the research on this\nproblem. Throughout the entire competition, we have received a total of 233\nsubmissions from 10 unique teams/individuals. Details of the competition and\nsubmission results are presented in this report.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:29:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ng", "Chun Chet", ""], ["Nazaruddin", "Akmalul Khairi Bin", ""], ["Lee", "Yeong Khang", ""], ["Wang", "Xinyu", ""], ["Liu", "Yuliang", ""], ["Chan", "Chee Seng", ""], ["Jin", "Lianwen", ""], ["Sun", "Yipeng", ""], ["Fan", "Lixin", ""]]}, {"id": "2107.05287", "submitter": "Stefan Ainetter", "authors": "Stefan Ainetter and Friedrich Fraundorfer", "title": "End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and\n  Semantic Segmentation from RGB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel, end-to-end trainable CNN-based\narchitecture to deliver high quality results for grasp detection suitable for a\nparallel-plate gripper, and semantic segmentation. Utilizing this, we propose a\nnovel refinement module that takes advantage of previously calculated grasp\ndetection and semantic segmentation and further increases grasp detection\naccuracy. Our proposed network delivers state-of-the-art accuracy on two\npopular grasp dataset, namely Cornell and Jacquard. As additional contribution,\nwe provide a novel dataset extension for the OCID dataset, making it possible\nto evaluate grasp detection in highly challenging scenes. Using this dataset,\nwe show that semantic segmentation can additionally be used to assign grasp\ncandidates to object classes, which can be used to pick specific objects in the\nscene.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:45:13 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ainetter", "Stefan", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2107.05307", "submitter": "Chengcheng Wang", "authors": "Yanpeng Cao, Chengcheng Wang, Changjun Song, Yongming Tang, He Li", "title": "Real-Time Super-Resolution System of 4K-Video Based on Deep Learning", "comments": "8 pages, 7 figures, ASAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) technology excels in reconstructing low-quality\nvideo, avoiding unpleasant blur effect caused by interpolation-based\nalgorithms. However, vast computation complexity and memory occupation hampers\nthe edge of deplorability and the runtime inference in real-life applications,\nespecially for large-scale VSR task. This paper explores the possibility of\nreal-time VSR system and designs an efficient and generic VSR network, termed\nEGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for\ntemporal coherence. In order to pursue faster VSR processing ability up to 4K\nresolution, this paper tries to choose lightweight network structure and\nefficient upsampling method to reduce the computation required by EGVSR network\nunder the guarantee of high visual quality. Besides, we implement the batch\nnormalization computation fusion, convolutional acceleration algorithm and\nother neural network acceleration techniques on the actual hardware platform to\noptimize the inference process of EGVSR network. Finally, our EGVSR achieves\nthe real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the\nmost advanced VSR network at present, we achieve 85.04% reduction of\ncomputation density and 7.92x performance speedups. In terms of visual quality,\nthe proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,\netc.) on the public test dataset Vid4 and surpasses other state-of-the-art\nmethods in overall performance score. The source code of this project can be\nfound on https://github.com/Thmen/EGVSR.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 10:35:05 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:42:34 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Cao", "Yanpeng", ""], ["Wang", "Chengcheng", ""], ["Song", "Changjun", ""], ["Tang", "Yongming", ""], ["Li", "He", ""]]}, {"id": "2107.05318", "submitter": "Rongkai Zhang", "authors": "Rongkai Zhang, Jiang Zhu, Zhiyuan Zha, Justin Dauwels and Bihan Wen", "title": "R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks\n  for Image Denoising via Residual Recovery", "comments": "Accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image denoisers exploit various types of deep neural\nnetworks via deterministic training. Alternatively, very recent works utilize\ndeep reinforcement learning for restoring images with diverse or unknown\ncorruptions. Though deep reinforcement learning can generate effective policy\nnetworks for operator selection or architecture search in image restoration,\nhow it is connected to the classic deterministic training in solving inverse\nproblems remains unclear. In this work, we propose a novel image denoising\nscheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show\nthat R3L is equivalent to a deep recurrent neural network that is trained using\na stochastic reward, in contrast to many popular denoisers using supervised\nlearning with deterministic losses. To benchmark the effectiveness of\nreinforcement learning in R3L, we train a recurrent neural network with the\nsame architecture for residual recovery using the deterministic loss, thus to\nanalyze how the two different training strategies affect the denoising\nperformance. With such a unified benchmarking system, we demonstrate that the\nproposed R3L has better generalizability and robustness in image denoising when\nthe estimated noise level varies, comparing to its counterparts using\ndeterministic training, as well as various state-of-the-art image denoising\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 11:12:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Rongkai", ""], ["Zhu", "Jiang", ""], ["Zha", "Zhiyuan", ""], ["Dauwels", "Justin", ""], ["Wen", "Bihan", ""]]}, {"id": "2107.05319", "submitter": "Andrew Gilbert", "authors": "Joseph Chrol-Cannon, Andrew Gilbert, Ranko Lazic, Adithya\n  Madhusoodanan, Frank Guerin", "title": "Human-like Relational Models for Activity Recognition in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video activity recognition by deep neural networks is impressive for many\nclasses. However, it falls short of human performance, especially for\nchallenging to discriminate activities. Humans differentiate these complex\nactivities by recognising critical spatio-temporal relations among explicitly\nrecognised objects and parts, for example, an object entering the aperture of a\ncontainer. Deep neural networks can struggle to learn such critical\nrelationships effectively. Therefore we propose a more human-like approach to\nactivity recognition, which interprets a video in sequential temporal phases\nand extracts specific relationships among objects and hands in those phases.\nRandom forest classifiers are learnt from these extracted relationships. We\napply the method to a challenging subset of the something-something dataset and\nachieve a more robust performance against neural network baselines on\nchallenging activities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 11:13:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chrol-Cannon", "Joseph", ""], ["Gilbert", "Andrew", ""], ["Lazic", "Ranko", ""], ["Madhusoodanan", "Adithya", ""], ["Guerin", "Frank", ""]]}, {"id": "2107.05334", "submitter": "Chih-Chung Hsu", "authors": "Chih-Chung Hsu, Guan-Lin Chen, and Mei-Hsuan Wu", "title": "Visual Transformer with Statistical Test for COVID-19 Classification", "comments": "this is a draft for MIA-Competition/ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the massive damage in the world caused by Coronavirus Disease 2019\nSARS-CoV-2 (COVID-19), many related research topics have been proposed in the\npast two years. The Chest Computed Tomography (CT) scans are the most valuable\nmaterials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19\nclassification of Chest CT scan is based on a single-slice level, implying that\nthe most critical CT slice should be selected from the original CT scan volume\nmanually. We simultaneously propose 2-D and 3-D models to predict the COVID-19\nof CT scan to tickle this issue. In our 2-D model, we introduce the Deep\nWilcoxon signed-rank test (DWCC) to determine the importance of each slice of a\nCT scan to overcome the issue mentioned previously. Furthermore, a\nConvolutional CT scan-Aware Transformer (CCAT) is proposed to discover the\ncontext of the slices fully. The frame-level feature is extracted from each CT\nslice based on any backbone network and followed by feeding the features to our\nwithin-slice-Transformer (WST) to discover the context information in the pixel\ndimension. The proposed Between-Slice-Transformer (BST) is used to aggregate\nthe extracted spatial-context features of every CT slice. A simple classifier\nis then used to judge whether the Spatio-temporal features are COVID-19 or\nnon-COVID-19. The extensive experiments demonstrated that the proposed CCAT and\nDWCC significantly outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 11:48:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Chen", "Guan-Lin", ""], ["Wu", "Mei-Hsuan", ""]]}, {"id": "2107.05342", "submitter": "Numan Celik", "authors": "Numan Celik, Sharib Ali, Soumya Gupta, Barbara Braden and Jens\n  Rittscher", "title": "EndoUDA: A modality independent segmentation approach for endoscopy\n  imaging", "comments": "10 pages, 3 figures, 3 tables. Accepted for MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gastrointestinal (GI) cancer precursors require frequent monitoring for risk\nstratification of patients. Automated segmentation methods can help to assess\nrisk areas more accurately, and assist in therapeutic procedures or even\nremoval. In clinical practice, addition to the conventional white-light imaging\n(WLI), complimentary modalities such as narrow-band imaging (NBI) and\nfluorescence imaging are used. While, today most segmentation approaches are\nsupervised and only concentrated on a single modality dataset, this work\nexploits to use a target-independent unsupervised domain adaptation (UDA)\ntechnique that is capable to generalize to an unseen target modality. In this\ncontext, we propose a novel UDA-based segmentation method that couples the\nvariational autoencoder and U-Net with a common EfficientNet-B4 backbone, and\nuses a joint loss for latent-space optimization for target samples. We show\nthat our model can generalize to unseen target NBI (target) modality when\ntrained using only WLI (source) modality. Our experiments on both upper and\nlower GI endoscopy data show the effectiveness of our approach compared to\nnaive supervised approach and state-of-the-art UDA segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 11:57:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Celik", "Numan", ""], ["Ali", "Sharib", ""], ["Gupta", "Soumya", ""], ["Braden", "Barbara", ""], ["Rittscher", "Jens", ""]]}, {"id": "2107.05384", "submitter": "Fangyi Zhang", "authors": "Ya Wang, Hesen Chen, Fangyi Zhang, Yaohua Wang, Xiuyu Sun, Ming Lin,\n  Hao Li", "title": "Fine-Grained AutoAugmentation for Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a commonly used approach to improving the generalization\nof deep learning models. Recent works show that learned data augmentation\npolicies can achieve better generalization than hand-crafted ones. However,\nmost of these works use unified augmentation policies for all samples in a\ndataset, which is observed not necessarily beneficial for all labels in\nmulti-label classification tasks, i.e., some policies may have negative impacts\non some labels while benefitting the others. To tackle this problem, we propose\na novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,\nwhere augmentation policies are generated with respect to labels by an\naugmentation-policy network. The policies are learned via reinforcement\nlearning using policy gradient methods, providing a mapping from instance\nlabels to their optimal augmentation policies. Numerical experiments show that\nour LB-Aug outperforms previous state-of-the-art augmentation methods by large\nmargins in multiple benchmarks on image and video classification.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:47:16 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:38:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Ya", ""], ["Chen", "Hesen", ""], ["Zhang", "Fangyi", ""], ["Wang", "Yaohua", ""], ["Sun", "Xiuyu", ""], ["Lin", "Ming", ""], ["Li", "Hao", ""]]}, {"id": "2107.05399", "submitter": "Aoran Xiao", "authors": "Aoran Xiao, Jiaxing Huang, Dayan Guan, Fangneng Zhan, Shijian Lu", "title": "SynLiDAR: Learning From Synthetic LiDAR Sequential Point Cloud for\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning from synthetic to real data has been proved an effective\nway of mitigating data annotation constraints in various computer vision tasks.\nHowever, the developments focused on 2D images but lag far behind for 3D point\nclouds due to the lack of large-scale high-quality synthetic point cloud data\nand effective transfer methods. We address this issue by collecting SynLiDAR, a\nsynthetic LiDAR point cloud dataset that contains large-scale point-wise\nannotated point cloud with accurate geometric shapes and comprehensive semantic\nclasses, and designing PCT-Net, a point cloud translation network that aims to\nnarrow down the gap with real-world point cloud data. For SynLiDAR, we leverage\ngraphic tools and professionals who construct multiple realistic virtual\nenvironments with rich scene types and layouts where annotated LiDAR points can\nbe generated automatically. On top of that, PCT-Net disentangles\nsynthetic-to-real gaps into an appearance component and a sparsity component\nand translates SynLiDAR by aligning the two components with real-world data\nseparately. Extensive experiments over multiple data augmentation and\nsemi-supervised semantic segmentation tasks show very positive outcomes -\nincluding SynLiDAR can either train better models or reduce real-world\nannotated data without sacrificing performance, and PCT-Net translated data\nfurther improve model performance consistently.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:51:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xiao", "Aoran", ""], ["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Zhan", "Fangneng", ""], ["Lu", "Shijian", ""]]}, {"id": "2107.05426", "submitter": "Mark Stamp", "authors": "Xinxin Yang and Mark Stamp", "title": "Computer-Aided Diagnosis of Low Grade Endometrial Stromal Sarcoma\n  (LGESS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low grade endometrial stromal sarcoma (LGESS) is rare form of cancer,\naccounting for about 0.2% of all uterine cancer cases. Approximately 75% of\nLGESS patients are initially misdiagnosed with leiomyoma, which is a type of\nbenign tumor, also known as fibroids. In this research, uterine tissue biopsy\nimages of potential LGESS patients are preprocessed using segmentation and\nstaining normalization algorithms. A variety of classic machine learning and\nleading deep learning models are then applied to classify tissue images as\neither benign or cancerous. For the classic techniques considered, the highest\nclassification accuracy we attain is about 0.85, while our best deep learning\nmodel achieves an accuracy of approximately 0.87. These results indicate that\nproperly trained learning algorithms can play a useful role in the diagnosis of\nLGESS.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 00:41:18 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Yang", "Xinxin", ""], ["Stamp", "Mark", ""]]}, {"id": "2107.05445", "submitter": "Yipeng Zhang", "authors": "Yipeng Zhang, Tyler L. Hayes, Christopher Kanan", "title": "Disentangling Transfer and Interference in Multi-Domain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial or these domains could\ninterfere with each other given limited network capacity. In this work, we\ndecipher the conditions where interference and knowledge transfer occur in\nmulti-domain learning. We propose new metrics disentangling interference and\ntransfer and set up experimental protocols. We further examine the roles of\nnetwork capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer. We demonstrate our findings on the\nCIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 01:30:36 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 01:14:21 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Zhang", "Yipeng", ""], ["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2107.05446", "submitter": "Cian Eastwood", "authors": "Cian Eastwood, Ian Mason, Christopher K. I. Williams, Bernhard\n  Sch\\\"olkopf", "title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature\n  Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source-free domain adaptation (SFDA) aims to adapt a model trained on\nlabelled data in a source domain to unlabelled data in a target domain without\naccess to the source-domain data during adaptation. Existing methods for SFDA\nleverage entropy-minimization techniques which: (i) apply only to\nclassification; (ii) destroy model calibration; and (iii) rely on the source\nmodel achieving a good level of feature-space class-separation in the target\ndomain. We address these issues for a particularly pervasive type of domain\nshift called measurement shift, characterized by a change in measurement system\n(e.g. a change in sensor or lighting). In the source domain, we store a\nlightweight and flexible approximation of the feature distribution under the\nsource data. In the target domain, we adapt the feature-extractor such that the\napproximate feature distribution under the target data realigns with that saved\non the source. We call this method Feature Restoration (FR) as it seeks to\nextract features with the same semantics from the target domain as were\npreviously extracted from the source. We additionally propose Bottom-Up Feature\nRestoration (BUFR), a bottom-up training scheme for FR which boosts performance\nby preserving learnt structure in the later layers of a network. Through\nexperiments we demonstrate that BUFR often outperforms existing SFDA methods in\nterms of accuracy, calibration, and data efficiency, while being less reliant\non the performance of the source model in the target domain.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:21:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Eastwood", "Cian", ""], ["Mason", "Ian", ""], ["Williams", "Christopher K. I.", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2107.05448", "submitter": "Rajat Koner", "authors": "Rajat Koner, Poulami Sinhamahapatra, Volker Tresp", "title": "Scenes and Surroundings: Scene Graph Generation using Relation\n  Transformer", "comments": "arXiv admin note: text overlap with arXiv:2004.06193", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying objects in an image and their mutual relationships as a scene\ngraph leads to a deep understanding of image content. Despite the recent\nadvancement in deep learning, the detection and labeling of visual object\nrelationships remain a challenging task. This work proposes a novel\nlocal-context aware architecture named relation transformer, which exploits\ncomplex global objects to object and object to edge (relation) interactions.\nOur hierarchical multi-head attention-based approach efficiently captures\ncontextual dependencies between objects and predicts their relationships. In\ncomparison to state-of-the-art approaches, we have achieved an overall mean\n\\textbf{4.85\\%} improvement and a new benchmark across all the scene graph\ngeneration tasks on the Visual Genome dataset.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:22:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Koner", "Rajat", ""], ["Sinhamahapatra", "Poulami", ""], ["Tresp", "Volker", ""]]}, {"id": "2107.05451", "submitter": "Donglai Wei Mr.", "authors": "Donglai Wei, Kisuk Lee, Hanyu Li, Ran Lu, J. Alexander Bae, Zequan\n  Liu, Lifu Zhang, M\\'arcia dos Santos, Zudi Lin, Thomas Uram, Xueying Wang,\n  Ignacio Arganda-Carreras, Brian Matejek, Narayanan Kasthuri, Jeff Lichtman,\n  Hanspeter Pfister", "title": "AxonEM Dataset: 3D Axon Instance Segmentation of Brain Cortical Regions", "comments": "The two first authors contributed equally. To be published in the\n  proceedings of MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electron microscopy (EM) enables the reconstruction of neural circuits at the\nlevel of individual synapses, which has been transformative for scientific\ndiscoveries. However, due to the complex morphology, an accurate reconstruction\nof cortical axons has become a major challenge. Worse still, there is no\npublicly available large-scale EM dataset from the cortex that provides dense\nground truth segmentation for axons, making it difficult to develop and\nevaluate large-scale axon reconstruction methods. To address this, we introduce\nthe AxonEM dataset, which consists of two 30x30x30 um^3 EM image volumes from\nthe human and mouse cortex, respectively. We thoroughly proofread over 18,000\naxon instances to provide dense 3D axon instance segmentation, enabling\nlarge-scale evaluation of axon reconstruction methods. In addition, we densely\nannotate nine ground truth subvolumes for training, per each data volume. With\nthis, we reproduce two published state-of-the-art methods and provide their\nevaluation results as a baseline. We publicly release our code and data at\nhttps://connectomics-bazaar.github.io/proj/AxonEM/index.html to foster the\ndevelopment of advanced methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:24:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wei", "Donglai", ""], ["Lee", "Kisuk", ""], ["Li", "Hanyu", ""], ["Lu", "Ran", ""], ["Bae", "J. Alexander", ""], ["Liu", "Zequan", ""], ["Zhang", "Lifu", ""], ["Santos", "M\u00e1rcia dos", ""], ["Lin", "Zudi", ""], ["Uram", "Thomas", ""], ["Wang", "Xueying", ""], ["Arganda-Carreras", "Ignacio", ""], ["Matejek", "Brian", ""], ["Kasthuri", "Narayanan", ""], ["Lichtman", "Jeff", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2107.05468", "submitter": "Shaoyu Cai", "authors": "Shaoyu Cai, Kening Zhu, Yuki Ban, Takuji Narumi", "title": "Visual-Tactile Cross-Modal Data Generation using Residue-Fusion GAN with\n  Feature-Matching and Perceptual Losses", "comments": "8 pages, 6 figures, Accepted by IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2021.3095925", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing psychophysical studies have revealed that the cross-modal\nvisual-tactile perception is common for humans performing daily activities.\nHowever, it is still challenging to build the algorithmic mapping from one\nmodality space to another, namely the cross-modal visual-tactile data\ntranslation/generation, which could be potentially important for robotic\noperation. In this paper, we propose a deep-learning-based approach for\ncross-modal visual-tactile data generation by leveraging the framework of the\ngenerative adversarial networks (GANs). Our approach takes the visual image of\na material surface as the visual data, and the accelerometer signal induced by\nthe pen-sliding movement on the surface as the tactile data. We adopt the\nconditional-GAN (cGAN) structure together with the residue-fusion (RF) module,\nand train the model with the additional feature-matching (FM) and perceptual\nlosses to achieve the cross-modal data generation. The experimental results\nshow that the inclusion of the RF module, and the FM and the perceptual losses\nsignificantly improves cross-modal data generation performance in terms of the\nclassification accuracy upon the generated data and the visual similarity\nbetween the ground-truth and the generated data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:36:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cai", "Shaoyu", ""], ["Zhu", "Kening", ""], ["Ban", "Yuki", ""], ["Narumi", "Takuji", ""]]}, {"id": "2107.05471", "submitter": "Vishwesh Nath", "authors": "Vishwesh Nath, Dong Yang, Ali Hatamizadeh, Anas A. Abidin, Andriy\n  Myronenko, Holger Roth, Daguang Xu", "title": "The Power of Proxy Data and Proxy Networks for Hyper-Parameter\n  Optimization in Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning models for medical image segmentation are primarily\ndata-driven. Models trained with more data lead to improved performance and\ngeneralizability. However, training is a computationally expensive process\nbecause multiple hyper-parameters need to be tested to find the optimal setting\nfor best performance. In this work, we focus on accelerating the estimation of\nhyper-parameters by proposing two novel methodologies: proxy data and proxy\nnetworks. Both can be useful for estimating hyper-parameters more efficiently.\nWe test the proposed techniques on CT and MR imaging modalities using\nwell-known public datasets. In both cases using one dataset for building proxy\ndata and another data source for external evaluation. For CT, the approach is\ntested on spleen segmentation with two datasets. The first dataset is from the\nmedical segmentation decathlon (MSD), where the proxy data is constructed, the\nsecondary dataset is utilized as an external validation dataset. Similarly, for\nMR, the approach is evaluated on prostate segmentation where the first dataset\nis from MSD and the second dataset is PROSTATEx. First, we show higher\ncorrelation to using full data for training when testing on the external\nvalidation set using smaller proxy data than a random selection of the proxy\ndata. Second, we show that a high correlation exists for proxy networks when\ncompared with the full network on validation Dice score. Third, we show that\nthe proposed approach of utilizing a proxy network can speed up an AutoML\nframework for hyper-parameter search by 3.3x, and by 4.4x if proxy data and\nproxy network are utilized together.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:37:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nath", "Vishwesh", ""], ["Yang", "Dong", ""], ["Hatamizadeh", "Ali", ""], ["Abidin", "Anas A.", ""], ["Myronenko", "Andriy", ""], ["Roth", "Holger", ""], ["Xu", "Daguang", ""]]}, {"id": "2107.05475", "submitter": "Fei Shen", "authors": "Fei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, and Huanqiang Zeng", "title": "GiT: Graph Interactive Transformer for Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are more and more popular in computer vision, which treat an\nimage as a sequence of patches and learn robust global features from the\nsequence. However, a suitable vehicle re-identification method should consider\nboth robust global features and discriminative local features. In this paper,\nwe propose a graph interactive transformer (GiT) for vehicle re-identification.\nOn the whole, we stack multiple GiT blocks to build a competitive vehicle\nre-identification model, in where each GiT block employs a novel local\ncorrelation graph (LCG) module to extract discriminative local features within\npatches and uses a transformer layer to extract robust global features among\npatches. In detail, in the current GiT block, the LCG module learns local\nfeatures from local and global features resulting from the LCG module and\ntransformer layer of the previous GiT block. Similarly, the transformer layer\nlearns global features from the global features generated by the transformer\nlayer of the previous GiT block and the new local features outputted via the\nLCG module of the current GiT block. Therefore, LCG modules and transformer\nlayers are in a coupled status, bringing effective cooperation between local\nand global features. This is the first work to combine graphs and transformers\nfor vehicle re-identification to the best of our knowledge. Extensive\nexperiments on three large-scale vehicle re-identification datasets demonstrate\nthat our method is superior to state-of-the-art approaches. The code will be\navailable soon.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:43:44 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shen", "Fei", ""], ["Xie", "Yi", ""], ["Zhu", "Jianqing", ""], ["Zhu", "Xiaobin", ""], ["Zeng", "Huanqiang", ""]]}, {"id": "2107.05482", "submitter": "Bo Zhou", "authors": "Bo Zhou, Chi Liu, James S. Duncan", "title": "Anatomy-Constrained Contrastive Learning for Synthetic Segmentation\n  without Ground-truth", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A large amount of manual segmentation is typically required to train a robust\nsegmentation network so that it can segment objects of interest in a new\nimaging modality. The manual efforts can be alleviated if the manual\nsegmentation in one imaging modality (e.g., CT) can be utilized to train a\nsegmentation network in another imaging modality (e.g., CBCT/MRI/PET). In this\nwork, we developed an anatomy-constrained contrastive synthetic segmentation\nnetwork (AccSeg-Net) to train a segmentation network for a target imaging\nmodality without using its ground truth. Specifically, we proposed to use\nanatomy-constraint and patch contrastive learning to ensure the anatomy\nfidelity during the unsupervised adaptation, such that the segmentation network\ncan be trained on the adapted image with correct anatomical structure/content.\nThe training data for our AccSeg-Net consists of 1) imaging data paired with\nsegmentation ground-truth in source modality, and 2) unpaired source and target\nmodality imaging data. We demonstrated successful applications on CBCT, MRI,\nand PET imaging data, and showed superior segmentation performances as compared\nto previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:54:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhou", "Bo", ""], ["Liu", "Chi", ""], ["Duncan", "James S.", ""]]}, {"id": "2107.05491", "submitter": "Bo Zhou", "authors": "Bo Zhou, Rui Wang, Ming-Kai Chen, Adam P. Mecca, Ryan S. O'Dell,\n  Christopher H. Van Dyck, Richard E. Carson, James S. Duncan, Chi Liu", "title": "Synthesizing Multi-Tracer PET Images for Alzheimer's Disease Patients\n  using a 3D Unified Anatomy-aware Cyclic Adversarial Network", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Positron Emission Tomography (PET) is an important tool for studying\nAlzheimer's disease (AD). PET scans can be used as diagnostics tools, and to\nprovide molecular characterization of patients with cognitive disorders.\nHowever, multiple tracers are needed to measure glucose metabolism (18F-FDG),\nsynaptic vesicle protein (11C-UCB-J), and $\\beta$-amyloid (11C-PiB).\nAdministering multiple tracers to patient will lead to high radiation dose and\ncost. In addition, access to PET scans using new or less-available tracers with\nsophisticated production methods and short half-life isotopes may be very\nlimited. Thus, it is desirable to develop an efficient multi-tracer PET\nsynthesis model that can generate multi-tracer PET from single-tracer PET.\nPrevious works on medical image synthesis focus on one-to-one fixed domain\ntranslations, and cannot simultaneously learn the feature from multi-tracer\ndomains. Given 3 or more tracers, relying on previous methods will also create\na heavy burden on the number of models to be trained. To tackle these issues,\nwe propose a 3D unified anatomy-aware cyclic adversarial network (UCAN) for\ntranslating multi-tracer PET volumes with one unified generative model, where\nMR with anatomical information is incorporated. Evaluations on a multi-tracer\nPET dataset demonstrate the feasibility that our UCAN can generate high-quality\nmulti-tracer PET volumes, with NMSE less than 15% for all PET tracers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:10:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhou", "Bo", ""], ["Wang", "Rui", ""], ["Chen", "Ming-Kai", ""], ["Mecca", "Adam P.", ""], ["O'Dell", "Ryan S.", ""], ["Van Dyck", "Christopher H.", ""], ["Carson", "Richard E.", ""], ["Duncan", "James S.", ""], ["Liu", "Chi", ""]]}, {"id": "2107.05509", "submitter": "Giorgos Karvounas", "authors": "Giorgos Karvounas, Nikolaos Kyriazis, Iason Oikonomidis, Aggeliki\n  Tsoli, Antonis A. Argyros", "title": "Multi-view Image-based Hand Geometry Refinement using Differentiable\n  Monte Carlo Ray Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount and quality of datasets and tools available in the research field\nof hand pose and shape estimation act as evidence to the significant progress\nthat has been made. We find that there is still room for improvement in both\nfronts, and even beyond. Even the datasets of the highest quality, reported to\ndate, have shortcomings in annotation. There are tools in the literature that\ncan assist in that direction and yet they have not been considered, so far. To\ndemonstrate how these gaps can be bridged, we employ such a publicly available,\nmulti-camera dataset of hands (InterHand2.6M), and perform effective\nimage-based refinement to improve on the imperfect ground truth annotations,\nyielding a better dataset. The image-based refinement is achieved through\nraytracing, a method that has not been employed so far to relevant problems and\nis hereby shown to be superior to the approximative alternatives that have been\nemployed in the past. To tackle the lack of reliable ground truth, we resort to\nrealistic synthetic data, to show that the improvement we induce is indeed\nsignificant, qualitatively, and quantitatively, too.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:35:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Karvounas", "Giorgos", ""], ["Kyriazis", "Nikolaos", ""], ["Oikonomidis", "Iason", ""], ["Tsoli", "Aggeliki", ""], ["Argyros", "Antonis A.", ""]]}, {"id": "2107.05532", "submitter": "Ping Wang", "authors": "Ping Wang and Jizong Peng and Marco Pedersoli and Yuanfeng Zhou and\n  Caiming Zhang and Christian Desrosiers", "title": "Context-aware virtual adversarial training for anatomically-plausible\n  segmentation", "comments": "This paper is accepted at MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their outstanding accuracy, semi-supervised segmentation methods\nbased on deep neural networks can still yield predictions that are considered\nanatomically impossible by clinicians, for instance, containing holes or\ndisconnected regions. To solve this problem, we present a Context-aware Virtual\nAdversarial Training (CaVAT) method for generating anatomically plausible\nsegmentation. Unlike approaches focusing solely on accuracy, our method also\nconsiders complex topological constraints like connectivity which cannot be\neasily modeled in a differentiable loss function. We use adversarial training\nto generate examples violating the constraints, so the network can learn to\navoid making such incorrect predictions on new examples, and employ the\nReinforce algorithm to handle non-differentiable segmentation constraints. The\nproposed method offers a generic and efficient way to add any constraint on top\nof any segmentation network. Experiments on two clinically-relevant datasets\nshow our method to produce segmentations that are both accurate and\nanatomically-plausible in terms of region connectivity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:01:27 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:36:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Ping", ""], ["Peng", "Jizong", ""], ["Pedersoli", "Marco", ""], ["Zhou", "Yuanfeng", ""], ["Zhang", "Caiming", ""], ["Desrosiers", "Christian", ""]]}, {"id": "2107.05533", "submitter": "Weijie Gan", "authors": "Weijie Gan, Yu Sun, Cihat Eldeniz, Jiaming Liu, Hongyu An and Ulugbek\n  S. Kamilov", "title": "MoDIR: Motion-Compensated Training for Deep Image Reconstruction without\n  Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks for medical image reconstruction are traditionally\ntrained using high-quality ground-truth images as training targets. Recent work\nonNoise2Noise (N2N) has shown the potential of using multiple noisy\nmeasurements of the same object as an alternative to having a ground truth.\nHowever, existing N2N-based methods cannot exploit information from various\nmotion states, limiting their ability to learn on moving objects. This paper\naddresses this issue by proposing a novel motion-compensated deep image\nreconstruction (MoDIR) method that can use information from several\nunregistered and noisy measurements for training. MoDIR deals with object\nmotion by including a deep registration module jointly trained with the deep\nreconstruction network without any ground-truth supervision. We validate MoDIR\non both simulated and experimentally collected magnetic resonance imaging (MRI)\ndata and show that it significantly improves imaging quality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:01:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gan", "Weijie", ""], ["Sun", "Yu", ""], ["Eldeniz", "Cihat", ""], ["Liu", "Jiaming", ""], ["An", "Hongyu", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "2107.05534", "submitter": "Xianbiao Qi", "authors": "Yuxiang Zhong, Xianbiao Qi, Shanjun Li, Dengyi Gu, Yihao Chen, Peiyang\n  Ning, Rong Xiao", "title": "1st Place Solution for ICDAR 2021 Competition on Mathematical Formula\n  Detection", "comments": "1st Place Solution for ICDAR 2021 Competition on Mathematical Formula\n  Detection. http://transcriptorium.eu/~htrcontest/MathsICDAR2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this technical report, we present our 1st place solution for the ICDAR\n2021 competition on mathematical formula detection (MFD). The MFD task has\nthree key challenges including a large scale span, large variation of the ratio\nbetween height and width, and rich character set and mathematical expressions.\nConsidering these challenges, we used Generalized Focal Loss (GFL), an\nanchor-free method, instead of the anchor-based method, and prove the Adaptive\nTraining Sampling Strategy (ATSS) and proper Feature Pyramid Network (FPN) can\nwell solve the important issue of scale variation. Meanwhile, we also found\nsome tricks, e.g., Deformable Convolution Network (DCN), SyncBN, and Weighted\nBox Fusion (WBF), were effective in MFD task. Our proposed method ranked 1st in\nthe final 15 teams.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:03:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhong", "Yuxiang", ""], ["Qi", "Xianbiao", ""], ["Li", "Shanjun", ""], ["Gu", "Dengyi", ""], ["Chen", "Yihao", ""], ["Ning", "Peiyang", ""], ["Xiao", "Rong", ""]]}, {"id": "2107.05548", "submitter": "Xi Zhang", "authors": "Xi Zhang and Xiaolin Wu", "title": "Multi-modality Deep Restoration of Extremely Compressed Face Videos", "comments": "Extension of DAVD-Net in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguably the most common and salient object in daily video communications is\nthe talking head, as encountered in social media, virtual classrooms,\nteleconferences, news broadcasting, talk shows, etc. When communication\nbandwidth is limited by network congestions or cost effectiveness, compression\nartifacts in talking head videos are inevitable. The resulting video quality\ndegradation is highly visible and objectionable due to high acuity of human\nvisual system to faces. To solve this problem, we develop a multi-modality deep\nconvolutional neural network method for restoring face videos that are\naggressively compressed. The main innovation is a new DCNN architecture that\nincorporates known priors of multiple modalities: the video-synchronized speech\nsignal and semantic elements of the compression code stream, including motion\nvectors, code partition map and quantization parameters. These priors strongly\ncorrelate with the latent video and hence they are able to enhance the\ncapability of deep learning to remove compression artifacts. Ample empirical\nevidences are presented to validate the superior performance of the proposed\nDCNN method on face videos over the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:29:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2107.05583", "submitter": "Yuan Zhou", "authors": "Yuan Zhou and Yanrong Guo and Shijie Hao and Richang Hong and Zhen\n  junzha and Meng Wang", "title": "Few-shot Learning with Global Relatedness Decoupled-Distillation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success that metric learning based approaches have achieved in\nfew-shot learning, recent works reveal the ineffectiveness of their episodic\ntraining mode. In this paper, we point out two potential reasons for this\nproblem: 1) the random episodic labels can only provide limited supervision\ninformation, while the relatedness information between the query and support\nsamples is not fully exploited; 2) the meta-learner is usually constrained by\nthe limited contextual information of the local episode. To overcome these\nproblems, we propose a new Global Relatedness Decoupled-Distillation (GRDD)\nmethod using the global category knowledge and the Relatedness\nDecoupled-Distillation (RDD) strategy. Our GRDD learns new visual concepts\nquickly by imitating the habit of humans, i.e. learning from the deep knowledge\ndistilled from the teacher. More specifically, we first train a global learner\non the entire base subset using category labels as supervision to leverage the\nglobal context information of the categories. Then, the well-trained global\nlearner is used to simulate the query-support relatedness in global\ndependencies. Finally, the distilled global query-support relatedness is\nexplicitly used to train the meta-learner using the RDD strategy, with the goal\nof making the meta-learner more discriminative. The RDD strategy aims to\ndecouple the dense query-support relatedness into the groups of sparse\ndecoupled relatedness. Moreover, only the relatedness of a single support\nsample with other query samples is considered in each group. By distilling the\nsparse decoupled relatedness group by group, sharper relatedness can be\neffectively distilled to the meta-learner, thereby facilitating the learning of\na discriminative meta-learner. We conduct extensive experiments on the\nminiImagenet and CIFAR-FS datasets, which show the state-of-the-art performance\nof our GRDD method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:01:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhou", "Yuan", ""], ["Guo", "Yanrong", ""], ["Hao", "Shijie", ""], ["Hong", "Richang", ""], ["junzha", "Zhen", ""], ["Wang", "Meng", ""]]}, {"id": "2107.05605", "submitter": "Alina Jade Barnett", "authors": "Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen,\n  Yinhao Ren, Joseph Y. Lo, Cynthia Rudin", "title": "Interpretable Mammographic Image Classification using Cased-Based\n  Reasoning and Deep Learning", "comments": "10 pages, 6 figures, accepted for oral presentation at the IJCAI-21\n  Workshop on Deep Learning, Case-Based Reasoning, and AutoML: Present and\n  Future Synergies. arXiv admin note: substantial text overlap with\n  arXiv:2103.12308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we deploy machine learning models in high-stakes medical settings, we\nmust ensure these models make accurate predictions that are consistent with\nknown medical science. Inherently interpretable networks address this need by\nexplaining the rationale behind each decision while maintaining equal or higher\naccuracy compared to black-box models. In this work, we present a novel\ninterpretable neural network algorithm that uses case-based reasoning for\nmammography. Designed to aid a radiologist in their decisions, our network\npresents both a prediction of malignancy and an explanation of that prediction\nusing known medical features. In order to yield helpful explanations, the\nnetwork is designed to mimic the reasoning processes of a radiologist: our\nnetwork first detects the clinically relevant semantic features of each image\nby comparing each new image with a learned set of prototypical image parts from\nthe training images, then uses those clinical features to predict malignancy.\nCompared to other methods, our model detects clinical features (mass margins)\nwith equal or higher accuracy, provides a more detailed explanation of its\nprediction, and is better able to differentiate the classification-relevant\nparts of the image.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:42:09 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Barnett", "Alina Jade", ""], ["Schwartz", "Fides Regina", ""], ["Tao", "Chaofan", ""], ["Chen", "Chaofan", ""], ["Ren", "Yinhao", ""], ["Lo", "Joseph Y.", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2107.05612", "submitter": "Valts Blukis", "authors": "Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi", "title": "A Persistent Spatial Semantic Representation for High-level Natural\n  Language Instruction Execution", "comments": "Submitted to CoRL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language provides an accessible and expressive interface to specify\nlong-term tasks for robotic agents. However, non-experts are likely to specify\nsuch tasks with high-level instructions, which abstract over specific robot\nactions through several layers of abstraction. We propose that key to bridging\nthis gap between language and robot actions over long execution horizons are\npersistent representations. We propose a persistent spatial semantic\nrepresentation method, and show how it enables building an agent that performs\nhierarchical reasoning to effectively execute long-term tasks. We evaluate our\napproach on the ALFRED benchmark and achieve state-of-the-art results, despite\ncompletely avoiding the commonly used step-by-step instructions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:47:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Blukis", "Valts", ""], ["Paxton", "Chris", ""], ["Fox", "Dieter", ""], ["Garg", "Animesh", ""], ["Artzi", "Yoav", ""]]}, {"id": "2107.05617", "submitter": "Alina Roitberg", "authors": "Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold,\n  Simon Rei{\\ss}, Rainer Stiefelhagen", "title": "Let's Play for Action: Recognizing Activities of Daily Living by\n  Learning from Life Simulation Video Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Activities of Daily Living (ADL) is a vital process for\nintelligent assistive robots, but collecting large annotated datasets requires\ntime-consuming temporal labeling and raises privacy concerns, e.g., if the data\nis collected in a real household. In this work, we explore the concept of\nconstructing training examples for ADL recognition by playing life simulation\nvideo games and introduce the SIMS4ACTION dataset created with the popular\ncommercial game THE SIMS 4. We build Sims4Action by specifically executing\nactions-of-interest in a \"top-down\" manner, while the gaming circumstances\nallow us to freely switch between environments, camera angles and subject\nappearances. While ADL recognition on gaming data is interesting from the\ntheoretical perspective, the key challenge arises from transferring it to the\nreal-world applications, such as smart-homes or assistive robotics. To meet\nthis requirement, Sims4Action is accompanied with a GamingToReal benchmark,\nwhere the models are evaluated on real videos derived from an existing ADL\ndataset. We integrate two modern algorithms for video-based activity\nrecognition in our framework, revealing the value of life simulation video\ngames as an inexpensive and far less intrusive source of training data.\nHowever, our results also indicate that tasks involving a mixture of gaming and\nreal data are challenging, opening a new research direction. We will make our\ndataset publicly available at https://github.com/aroitberg/sims4action.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:53:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Roitberg", "Alina", ""], ["Schneider", "David", ""], ["Djamal", "Aulia", ""], ["Seibold", "Constantin", ""], ["Rei\u00df", "Simon", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.05622", "submitter": "Shivam Chandhok", "authors": "Shivam Chandhok, Sanath Narayan, Hisham Cholakkal, Rao Muhammad Anwer,\n  Vineeth N Balasubramanian, Fahad Shahbaz Khan, Ling Shao", "title": "Structured Latent Embeddings for Recognizing Unseen Classes in Unseen\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need to address the scarcity of task-specific annotated data has resulted\nin concerted efforts in recent years for specific settings such as zero-shot\nlearning (ZSL) and domain generalization (DG), to separately address the issues\nof semantic shift and domain shift, respectively. However, real-world\napplications often do not have constrained settings and necessitate handling\nunseen classes in unseen domains -- a setting called Zero-shot Domain\nGeneralization, which presents the issues of domain and semantic shifts\nsimultaneously. In this work, we propose a novel approach that learns\ndomain-agnostic structured latent embeddings by projecting images from\ndifferent domains as well as class-specific semantic text-based representations\nto a common latent space. In particular, our method jointly strives for the\nfollowing objectives: (i) aligning the multimodal cues from visual and\ntext-based semantic concepts; (ii) partitioning the common latent space\naccording to the domain-agnostic class-level semantic concepts; and (iii)\nlearning a domain invariance w.r.t the visual-semantic joint distribution for\ngeneralizing to unseen classes in unseen domains. Our experiments on the\nchallenging DomainNet and DomainNet-LS benchmarks show the superiority of our\napproach over existing methods, with significant gains on difficult domains\nlike quickdraw and sketch.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:57:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chandhok", "Shivam", ""], ["Narayan", "Sanath", ""], ["Cholakkal", "Hisham", ""], ["Anwer", "Rao Muhammad", ""], ["Balasubramanian", "Vineeth N", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "2107.05624", "submitter": "Yi-Wen Chen", "authors": "Yi-Wen Chen, Yi-Hsuan Tsai, Ming-Hsuan Yang", "title": "End-to-end Multi-modal Video Temporal Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of text-guided video temporal grounding, which aims to\nidentify the time interval of certain event based on a natural language\ndescription. Different from most existing methods that only consider RGB images\nas visual features, we propose a multi-modal framework to extract complementary\ninformation from videos. Specifically, we adopt RGB images for appearance,\noptical flow for motion, and depth maps for image structure. While RGB images\nprovide abundant visual cues of certain event, the performance may be affected\nby background clutters. Therefore, we use optical flow to focus on large motion\nand depth maps to infer the scene configuration when the action is related to\nobjects recognizable with their shapes. To integrate the three modalities more\neffectively and enable inter-modal learning, we design a dynamic fusion scheme\nwith transformers to model the interactions between modalities. Furthermore, we\napply intra-modal self-supervised learning to enhance feature representations\nacross videos for each modality, which also facilitates multi-modal learning.\nWe conduct extensive experiments on the Charades-STA and ActivityNet Captions\ndatasets, and show that the proposed method performs favorably against\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:58:10 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Yi-Wen", ""], ["Tsai", "Yi-Hsuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2107.05627", "submitter": "Deepak Pathak", "authors": "Shikhar Bahl, Abhinav Gupta, Deepak Pathak", "title": "Hierarchical Neural Dynamic Policies", "comments": "Accepted at RSS 2021. Videos and code at\n  https://shikharbahl.github.io/hierarchical-ndps/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of generalization to unseen configurations for dynamic\ntasks in the real world while learning from high-dimensional image input. The\nfamily of nonlinear dynamical system-based methods have successfully\ndemonstrated dynamic robot behaviors but have difficulty in generalizing to\nunseen configurations as well as learning from image inputs. Recent works\napproach this issue by using deep network policies and reparameterize actions\nto embed the structure of dynamical systems but still struggle in domains with\ndiverse configurations of image goals, and hence, find it difficult to\ngeneralize. In this paper, we address this dichotomy by leveraging embedding\nthe structure of dynamical systems in a hierarchical deep policy learning\nframework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of\nfitting deep dynamical systems to diverse data directly, H-NDPs form a\ncurriculum by learning local dynamical system-based policies on small regions\nin state-space and then distill them into a global dynamical system-based\npolicy that operates only from high-dimensional images. H-NDPs additionally\nprovide smooth trajectories, a strong safety benefit in the real world. We\nperform extensive experiments on dynamic tasks both in the real world (digit\nwriting, scooping, and pouring) and simulation (catching, throwing, picking).\nWe show that H-NDPs are easily integrated with both imitation as well as\nreinforcement learning setups and achieve state-of-the-art results. Video\nresults are at https://shikharbahl.github.io/hierarchical-ndps/\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:59:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bahl", "Shikhar", ""], ["Gupta", "Abhinav", ""], ["Pathak", "Deepak", ""]]}, {"id": "2107.05634", "submitter": "Madhusudhanan Balasubramanian", "authors": "Ali Salehi, Madhusudhanan Balasubramanian", "title": "DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN\n  for Dense Prediction", "comments": "27 pages, 10 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:2107.04715", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense optical flow estimation is challenging when there are large\ndisplacements in a scene with heterogeneous motion dynamics, occlusion, and\nscene homogeneity. Traditional approaches to handle these challenges include\nhierarchical and multiresolution processing methods. Learning-based optical\nflow methods typically use a multiresolution approach with image warping when a\nbroad range of flow velocities and heterogeneous motion is present. Accuracy of\nsuch coarse-to-fine methods is affected by the ghosting artifacts when images\nare warped across multiple resolutions and by the vanishing problem in smaller\nscene extents with higher motion contrast. Previously, we devised strategies\nfor building compact dense prediction networks guided by the effective\nreceptive field (ERF) characteristics of the network (DDCNet). The DDCNet\ndesign was intentionally simple and compact allowing it to be used as a\nbuilding block for designing more complex yet compact networks. In this work,\nwe extend the DDCNet strategies to handle heterogeneous motion dynamics by\ncascading DDCNet based sub-nets with decreasing extents of their ERF. Our\nDDCNet with multiresolution capability (DDCNet-Multires) is compact without any\nspecialized network layers. We evaluate the performance of the DDCNet-Multires\nnetwork using standard optical flow benchmark datasets. Our experiments\ndemonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and\nprovides optical flow estimates with accuracy comparable to similar lightweight\nlearning-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 17:28:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Salehi", "Ali", ""], ["Balasubramanian", "Madhusudhanan", ""]]}, {"id": "2107.05637", "submitter": "Chenglin Yang", "authors": "Chenglin Yang, Siyuan Qiao, Adam Kortylewski, Alan Yuille", "title": "Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and\n  Context Terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-Attention has become prevalent in computer vision models. Inspired by\nfully connected Conditional Random Fields (CRFs), we decompose it into local\nand context terms. They correspond to the unary and binary terms in CRF and are\nimplemented by attention mechanisms with projection matrices. We observe that\nthe unary terms only make small contributions to the outputs, and meanwhile\nstandard CNNs that rely solely on the unary terms achieve great performances on\na variety of tasks. Therefore, we propose Locally Enhanced Self-Attention\n(LESA), which enhances the unary term by incorporating it with convolutions,\nand utilizes a fusion module to dynamically couple the unary and binary\noperations. In our experiments, we replace the self-attention modules with\nLESA. The results on ImageNet and COCO show the superiority of LESA over\nconvolution and self-attention baselines for the tasks of image recognition,\nobject detection, and instance segmentation. The code is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:00:00 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Yang", "Chenglin", ""], ["Qiao", "Siyuan", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2107.05680", "submitter": "Arda Sahiner", "authors": "Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly,\n  Morteza Mardani, Mert Pilanci", "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models\n  with Closed-Form Solutions", "comments": "First two authors contributed equally to this work; 30 pages, 11\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are commonly used for modeling complex\ndistributions of data. Both the generators and discriminators of GANs are often\nmodeled by neural networks, posing a non-transparent optimization problem which\nis non-convex and non-concave over the generator and discriminator,\nrespectively. Such networks are often heuristically optimized with gradient\ndescent-ascent (GDA), but it is unclear whether the optimization problem\ncontains any saddle points, or whether heuristic methods can find them in\npractice. In this work, we analyze the training of Wasserstein GANs with\ntwo-layer neural network discriminators through the lens of convex duality, and\nfor a variety of generators expose the conditions under which Wasserstein GANs\ncan be solved exactly with convex optimization approaches, or can be\nrepresented as convex-concave games. Using this convex duality interpretation,\nwe further demonstrate the impact of different activation functions of the\ndiscriminator. Our observations are verified with numerical results\ndemonstrating the power of the convex interpretation, with applications in\nprogressive training of convex architectures corresponding to linear generators\nand quadratic-activation discriminators for CelebA image generation. The code\nfor our experiments is available at https://github.com/ardasahiner/ProCoGAN.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:33:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sahiner", "Arda", ""], ["Ergen", "Tolga", ""], ["Ozturkler", "Batu", ""], ["Bartan", "Burak", ""], ["Pauly", "John", ""], ["Mardani", "Morteza", ""], ["Pilanci", "Mert", ""]]}, {"id": "2107.05698", "submitter": "Jian Wang", "authors": "Jian Wang, Miaomiao Zhang", "title": "Bayesian Atlas Building with Hierarchical Priors for Subject-specific\n  Regularization", "comments": "11 pages, 2 figures", "journal-ref": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI 2021)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel hierarchical Bayesian model for unbiased atlas\nbuilding with subject-specific regularizations of image registration. We\ndevelop an atlas construction process that automatically selects parameters to\ncontrol the smoothness of diffeomorphic transformation according to individual\nimage data. To achieve this, we introduce a hierarchical prior distribution on\nregularization parameters that allows multiple penalties on images with various\ndegrees of geometric transformations. We then treat the regularization\nparameters as latent variables and integrate them out from the model by using\nthe Monte Carlo Expectation Maximization (MCEM) algorithm. Another advantage of\nour algorithm is that it eliminates the need for manual parameter tuning, which\ncan be tedious and infeasible. We demonstrate the effectiveness of our model on\n3D brain MR images. Experimental results show that our model provides a sharper\natlas compared to the current atlas building algorithms with single-penalty\nregularizations. Our code is publicly available at\nhttps://github.com/jw4hv/HierarchicalBayesianAtlasBuild.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 19:32:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Jian", ""], ["Zhang", "Miaomiao", ""]]}, {"id": "2107.05736", "submitter": "Darshan Gera", "authors": "Darshan Gera, S Balasubramanian", "title": "Affect Expression Behaviour Analysis in the Wild using Consensual\n  Collaborative Training", "comments": "7 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2009.14440. substantial text overlap with arXiv:2107.04746", "journal-ref": "International Journal of Engineering Trends and Technology\n  69.7(2021):244-254", "doi": "10.14445/22315381/IJETT-V69I7P231", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition (FER) in the wild is crucial for building\nreliable human-computer interactive systems. However, annotations of large\nscale datasets in FER has been a key challenge as these datasets suffer from\nnoise due to various factors like crowd sourcing, subjectivity of annotators,\npoor quality of images, automatic labelling based on key word search etc. Such\nnoisy annotations impede the performance of FER due to the memorization ability\nof deep networks. During early learning stage, deep networks fit on clean data.\nThen, eventually, they start overfitting on noisy labels due to their\nmemorization ability, which limits FER performance. This report presents\nConsensual Collaborative Training (CCT) framework used in our submission to\nexpression recognition track of the Affective Behaviour Analysis in-the-wild\n(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex\ncombination of supervision loss and consistency loss, without making any\nassumption about the noise distribution. A dynamic transition mechanism is used\nto move from supervision loss in early learning to consistency loss for\nconsensus of predictions among networks in the later stage. Co-training reduces\noverall error, and consistency loss prevents overfitting to noisy samples. The\nperformance of the model is validated on challenging Aff-Wild2 dataset for\ncategorical expression classification. Our code is made publicly available at\nhttps://github.com/1980x/ABAW2021DMACS.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 04:28:21 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 05:28:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gera", "Darshan", ""], ["Balasubramanian", "S", ""]]}, {"id": "2107.05747", "submitter": "Timoleon Moraitis", "authors": "Timoleon Moraitis, Dmitry Toichkin, Yansong Chua, Qinghai Guo", "title": "SoftHebb: Bayesian inference in unsupervised Hebbian soft\n  winner-take-all networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art artificial neural networks (ANNs) require labelled data or\nfeedback between layers, are often biologically implausible, and are vulnerable\nto adversarial attacks that humans are not susceptible to. On the other hand,\nHebbian learning in winner-take-all (WTA) networks, is unsupervised,\nfeed-forward, and biologically plausible. However, an objective optimization\ntheory for WTA networks has been missing, except under very limiting\nassumptions. Here we derive formally such a theory, based on biologically\nplausible but generic ANN elements. Through Hebbian learning, network\nparameters maintain a Bayesian generative model of the data. There is no\nsupervisory loss function, but the network does minimize cross-entropy between\nits activations and the input distribution. The key is a \"soft\" WTA where there\nis no absolute \"hard\" winner neuron, and a specific type of Hebbian-like\nplasticity of weights and biases. We confirm our theory in practice, where, in\nhandwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,\nminimizes cross-entropy without having access to it, and outperforms the more\nfrequently used, hard-WTA-based method. Strikingly, it even outperforms\nsupervised end-to-end backpropagation, under certain conditions. Specifically,\nin a two-layered network, SoftHebb outperforms backpropagation when the\ntraining dataset is only presented once, when the testing data is noisy, and\nunder gradient-based adversarial attacks. Adversarial attacks that confuse\nSoftHebb are also confusing to the human eye. Finally, the model can generate\ninterpolations of objects from its input distribution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 21:34:45 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Moraitis", "Timoleon", ""], ["Toichkin", "Dmitry", ""], ["Chua", "Yansong", ""], ["Guo", "Qinghai", ""]]}, {"id": "2107.05754", "submitter": "Andrei Ilie", "authors": "Andrei Ilie, Marius Popescu, Alin Stefanescu", "title": "EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown how easily white-box adversarial attacks can be applied\nto state-of-the-art image classifiers. However, real-life scenarios resemble\nmore the black-box adversarial conditions, lacking transparency and usually\nimposing natural, hard constraints on the query budget.\n  We propose $\\textbf{EvoBA}$, a black-box adversarial attack based on a\nsurprisingly simple evolutionary search strategy. $\\textbf{EvoBA}$ is\nquery-efficient, minimizes $L_0$ adversarial perturbations, and does not\nrequire any form of training.\n  $\\textbf{EvoBA}$ shows efficiency and efficacy through results that are in\nline with much more complex state-of-the-art black-box attacks such as\n$\\textbf{AutoZOOM}$. It is more query-efficient than $\\textbf{SimBA}$, a simple\nand powerful baseline black-box attack, and has a similar level of complexity.\nTherefore, we propose it both as a new strong baseline for black-box\nadversarial attacks and as a fast and general tool for gaining empirical\ninsight into how robust image classifiers are with respect to $L_0$ adversarial\nperturbations.\n  There exist fast and reliable $L_2$ black-box attacks, such as\n$\\textbf{SimBA}$, and $L_{\\infty}$ black-box attacks, such as\n$\\textbf{DeepSearch}$. We propose $\\textbf{EvoBA}$ as a query-efficient $L_0$\nblack-box adversarial attack which, together with the aforementioned methods,\ncan serve as a generic tool to assess the empirical robustness of image\nclassifiers. The main advantages of such methods are that they run fast, are\nquery-efficient, and can easily be integrated in image classifiers development\npipelines.\n  While our attack minimises the $L_0$ adversarial perturbation, we also report\n$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$\nblack-box attack, $\\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,\n$\\textbf{SimBA}$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 21:55:01 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ilie", "Andrei", ""], ["Popescu", "Marius", ""], ["Stefanescu", "Alin", ""]]}, {"id": "2107.05768", "submitter": "Hanjun Dai", "authors": "Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale\n  Schuurmans, Bo Dai", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 22:43:11 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ren", "Hongyu", ""], ["Dai", "Hanjun", ""], ["Dai", "Zihang", ""], ["Yang", "Mengjiao", ""], ["Leskovec", "Jure", ""], ["Schuurmans", "Dale", ""], ["Dai", "Bo", ""]]}, {"id": "2107.05775", "submitter": "Pengsheng Guo", "authors": "Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel\n  Ulbricht, Joshua M. Susskind, Qi Shan", "title": "Fast and Explicit Neural View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of novel view synthesis of a scene comprised of 3D\nobjects. We propose a simple yet effective approach that is neither continuous\nnor implicit, challenging recent trends on view synthesis. We demonstrate that\nalthough continuous radiance field representations have gained a lot of\nattention due to their expressive power, our simple approach obtains comparable\nor even better novel view reconstruction quality comparing with\nstate-of-the-art baselines while increasing rendering speed by over 400x. Our\nmodel is trained in a category-agnostic manner and does not require\nscene-specific optimization. Therefore, it is able to generalize novel view\nsynthesis to object categories not seen during training. In addition, we show\nthat with our simple formulation, we can use view synthesis as a\nself-supervision signal for efficient learning of 3D geometry without explicit\n3D supervision.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 23:24:53 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Guo", "Pengsheng", ""], ["Bautista", "Miguel Angel", ""], ["Colburn", "Alex", ""], ["Yang", "Liang", ""], ["Ulbricht", "Daniel", ""], ["Susskind", "Joshua M.", ""], ["Shan", "Qi", ""]]}, {"id": "2107.05780", "submitter": "Sid Ahmed Fezza", "authors": "Anouar Kherchouche, Sid Ahmed Fezza, Wassim Hamidouche", "title": "Detect and Defense Against Adversarial Examples in Deep Learning using\n  Natural Scene Statistics and Adaptive Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the enormous performance of deepneural networks (DNNs), recent\nstudies have shown theirvulnerability to adversarial examples (AEs), i.e.,\ncare-fully perturbed inputs designed to fool the targetedDNN. Currently, the\nliterature is rich with many ef-fective attacks to craft such AEs. Meanwhile,\nmany de-fenses strategies have been developed to mitigate thisvulnerability.\nHowever, these latter showed their effec-tiveness against specific attacks and\ndoes not general-ize well to different attacks. In this paper, we proposea\nframework for defending DNN classifier against ad-versarial samples. The\nproposed method is based on atwo-stage framework involving a separate detector\nanda denoising block. The detector aims to detect AEs bycharacterizing them\nthrough the use of natural scenestatistic (NSS), where we demonstrate that\nthese statis-tical features are altered by the presence of\nadversarialperturbations. The denoiser is based on block matching3D (BM3D)\nfilter fed by an optimum threshold valueestimated by a convolutional neural\nnetwork (CNN) toproject back the samples detected as AEs into theirdata\nmanifold. We conducted a complete evaluation onthree standard datasets namely\nMNIST, CIFAR-10 andTiny-ImageNet. The experimental results show that\ntheproposed defense method outperforms the state-of-the-art defense techniques\nby improving the robustnessagainst a set of attacks under black-box, gray-box\nand white-box settings. The source code is available at:\nhttps://github.com/kherchouche-anouar/2DAE\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 23:45:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kherchouche", "Anouar", ""], ["Fezza", "Sid Ahmed", ""], ["Hamidouche", "Wassim", ""]]}, {"id": "2107.05789", "submitter": "Ashwin Balakrishna", "authors": "Shivin Devgon and Jeffrey Ichnowski and Michael Danielczuk and Daniel\n  S. Brown and Ashwin Balakrishna and Shirin Joshi and Eduardo M. C. Rocha and\n  Eugen Solowjow and Ken Goldberg", "title": "Kit-Net: Self-Supervised Learning to Kit Novel 3D Objects into Novel 3D\n  Cavities", "comments": null, "journal-ref": "Conference on Automation Science and Engineering (CASE) 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In industrial part kitting, 3D objects are inserted into cavities for\ntransportation or subsequent assembly. Kitting is a critical step as it can\ndecrease downstream processing and handling times and enable lower storage and\nshipping costs. We present Kit-Net, a framework for kitting previously unseen\n3D objects into cavities given depth images of both the target cavity and an\nobject held by a gripper in an unknown initial orientation. Kit-Net uses\nself-supervised deep learning and data augmentation to train a convolutional\nneural network (CNN) to robustly estimate 3D rotations between objects and\nmatching concave or convex cavities using a large training dataset of simulated\ndepth images pairs. Kit-Net then uses the trained CNN to implement a controller\nto orient and position novel objects for insertion into novel prismatic and\nconformal 3D cavities. Experiments in simulation suggest that Kit-Net can\norient objects to have a 98.9% average intersection volume between the object\nmesh and that of the target cavity. Physical experiments with industrial\nobjects succeed in 18% of trials using a baseline method and in 63% of trials\nwith Kit-Net. Video, code, and data are available at\nhttps://github.com/BerkeleyAutomation/Kit-Net.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 00:21:23 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Devgon", "Shivin", ""], ["Ichnowski", "Jeffrey", ""], ["Danielczuk", "Michael", ""], ["Brown", "Daniel S.", ""], ["Balakrishna", "Ashwin", ""], ["Joshi", "Shirin", ""], ["Rocha", "Eduardo M. C.", ""], ["Solowjow", "Eugen", ""], ["Goldberg", "Ken", ""]]}, {"id": "2107.05790", "submitter": "Shuyang Sun", "authors": "Shuyang Sun*, Xiaoyu Yue*, Song Bai, Philip Torr", "title": "Visual Parser: Representing Part-whole Hierarchies with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human vision is able to capture the part-whole hierarchical information from\nthe entire scene. This paper presents the Visual Parser (ViP) that explicitly\nconstructs such a hierarchy with transformers. ViP divides visual\nrepresentations into two levels, the part level and the whole level.\nInformation of each part represents a combination of several independent\nvectors within the whole. To model the representations of the two levels, we\nfirst encode the information from the whole into part vectors through an\nattention mechanism, then decode the global information within the part vectors\nback into the whole representation. By iteratively parsing the two levels with\nthe proposed encoder-decoder interaction, the model can gradually refine the\nfeatures on both levels. Experimental results demonstrate that ViP can achieve\nvery competitive performance on three major tasks e.g. classification,\ndetection and instance segmentation. In particular, it can surpass the previous\nstate-of-the-art CNN backbones by a large margin on object detection. The tiny\nmodel of the ViP family with $7.2\\times$ fewer parameters and $10.9\\times$\nfewer FLOPS can perform comparably with the largest model\nResNeXt-101-64$\\times$4d of ResNe(X)t family. Visualization results also\ndemonstrate that the learnt parts are highly informative of the predicting\nclass, making ViP more explainable than previous fundamental architectures.\nCode is available at https://github.com/kevin-ssy/ViP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 00:27:01 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sun*", "Shuyang", ""], ["Yue*", "Xiaoyu", ""], ["Bai", "Song", ""], ["Torr", "Philip", ""]]}, {"id": "2107.05804", "submitter": "Zhongzhan Huang", "authors": "Zhongzhan Huang, Mingfu Liang, Senwei Liang, Wei He", "title": "AlterSGD: Finding Flat Minima for Continual Learning by Alternative\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks suffer from catastrophic forgetting when learning\nmultiple knowledge sequentially, and a growing number of approaches have been\nproposed to mitigate this problem. Some of these methods achieved considerable\nperformance by associating the flat local minima with forgetting mitigation in\ncontinual learning. However, they inevitably need (1) tedious hyperparameters\ntuning, and (2) additional computational cost. To alleviate these problems, in\nthis paper, we propose a simple yet effective optimization method, called\nAlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we\nconduct gradient descent and ascent alternatively when the network tends to\nconverge at each session of learning new knowledge. Moreover, we theoretically\nprove that such a strategy can encourage the optimization to converge to a flat\nminima. We verify AlterSGD on continual learning benchmark for semantic\nsegmentation and the empirical results show that we can significantly mitigate\nthe forgetting and outperform the state-of-the-art methods with a large margin\nunder challenging continual learning protocols.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 01:43:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Huang", "Zhongzhan", ""], ["Liang", "Mingfu", ""], ["Liang", "Senwei", ""], ["He", "Wei", ""]]}, {"id": "2107.05819", "submitter": "Xi Li", "authors": "Jiabao Cui, Pengyi Zhang, Songyuan Li, Liangli Zheng, Cuizhu Bao,\n  Jupeng Xia, Xi Li", "title": "Multitask Identity-Aware Image Steganography via Minimax Optimization", "comments": "Accepted to Transaction of Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-capacity image steganography, aimed at concealing a secret image in a\ncover image, is a technique to preserve sensitive data, e.g., faces and\nfingerprints. Previous methods focus on the security during transmission and\nsubsequently run a risk of privacy leakage after the restoration of secret\nimages at the receiving end. To address this issue, we propose a framework,\ncalled Multitask Identity-Aware Image Steganography (MIAIS), to achieve direct\nrecognition on container images without restoring secret images. The key issue\nof the direct recognition is to preserve identity information of secret images\ninto container images and make container images look similar to cover images at\nthe same time. Thus, we introduce a simple content loss to preserve the\nidentity information, and design a minimax optimization to deal with the\ncontradictory aspects. We demonstrate that the robustness results can be\ntransferred across different cover datasets. In order to be flexible for the\nsecret image restoration in some cases, we incorporate an optional restoration\nnetwork into our method, providing a multitask framework. The experiments under\nthe multitask scenario show the effectiveness of our framework compared with\nother visual information hiding methods and state-of-the-art high-capacity\nimage steganography methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 02:53:38 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cui", "Jiabao", ""], ["Zhang", "Pengyi", ""], ["Li", "Songyuan", ""], ["Zheng", "Liangli", ""], ["Bao", "Cuizhu", ""], ["Xia", "Jupeng", ""], ["Li", "Xi", ""]]}, {"id": "2107.05821", "submitter": "Chenqi Kong", "authors": "Chenqi Kong, Baoliang Chen, Haoliang Li, Shiqi Wang, Anderson Rocha,\n  and Sam Kwong", "title": "Detect and Locate: A Face Anti-Manipulation Approach with Semantic and\n  Noise-level Supervision", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technological advancements of deep learning have enabled sophisticated\nface manipulation schemes, raising severe trust issues and security concerns in\nmodern society. Generally speaking, detecting manipulated faces and locating\nthe potentially altered regions are challenging tasks. Herein, we propose a\nconceptually simple but effective method to efficiently detect forged faces in\nan image while simultaneously locating the manipulated regions. The proposed\nscheme relies on a segmentation map that delivers meaningful high-level\nsemantic information clues about the image. Furthermore, a noise map is\nestimated, playing a complementary role in capturing low-level clues and\nsubsequently empowering decision-making. Finally, the features from these two\nmodules are combined to distinguish fake faces. Extensive experiments show that\nthe proposed model achieves state-of-the-art detection accuracy and remarkable\nlocalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 02:59:31 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kong", "Chenqi", ""], ["Chen", "Baoliang", ""], ["Li", "Haoliang", ""], ["Wang", "Shiqi", ""], ["Rocha", "Anderson", ""], ["Kwong", "Sam", ""]]}, {"id": "2107.05828", "submitter": "Hawzhin Mohammed", "authors": "Hawzhin Mohammed, Tolulope A. Odetola, Nan Guo, Syed Rafay Hasan", "title": "Dynamic Distribution of Edge Intelligence at the Node Level for Internet\n  of Things", "comments": "5 pages, 4 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, dynamic deployment of Convolutional Neural Network (CNN)\narchitecture is proposed utilizing only IoT-level devices. By partitioning and\npipelining the CNN, it horizontally distributes the computation load among\nresource-constrained devices (called horizontal collaboration), which in turn\nincreases the throughput. Through partitioning, we can decrease the computation\nand energy consumption on individual IoT devices and increase the throughput\nwithout sacrificing accuracy. Also, by processing the data at the generation\npoint, data privacy can be achieved. The results show that throughput can be\nincreased by 1.55x to 1.75x for sharing the CNN into two and three\nresource-constrained devices, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 03:26:36 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Mohammed", "Hawzhin", ""], ["Odetola", "Tolulope A.", ""], ["Guo", "Nan", ""], ["Hasan", "Syed Rafay", ""]]}, {"id": "2107.05830", "submitter": "Rongkai Zhang", "authors": "Rongkai Zhang, Lanqing Guo, Siyu Huang and Bihan Wen", "title": "ReLLIE: Deep Reinforcement Learning for Customized Low-Light Image\n  Enhancement", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement (LLIE) is a pervasive yet challenging problem,\nsince: 1) low-light measurements may vary due to different imaging conditions\nin practice; 2) images can be enlightened subjectively according to diverse\npreferences by each individual. To tackle these two challenges, this paper\npresents a novel deep reinforcement learning based method, dubbed ReLLIE, for\ncustomized low-light enhancement. ReLLIE models LLIE as a markov decision\nprocess, i.e., estimating the pixel-wise image-specific curves sequentially and\nrecurrently. Given the reward computed from a set of carefully crafted\nnon-reference loss functions, a lightweight network is proposed to estimate the\ncurves for enlightening of a low-light image input. As ReLLIE learns a policy\ninstead of one-one image translation, it can handle various low-light\nmeasurements and provide customized enhanced outputs by flexibly applying the\npolicy different times. Furthermore, ReLLIE can enhance real-world images with\nhybrid corruptions, e.g., noise, by using a plug-and-play denoiser easily.\nExtensive experiments on various benchmarks demonstrate the advantages of\nReLLIE, comparing to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 03:36:30 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhang", "Rongkai", ""], ["Guo", "Lanqing", ""], ["Huang", "Siyu", ""], ["Wen", "Bihan", ""]]}, {"id": "2107.05840", "submitter": "Zudi Lin", "authors": "Zudi Lin, Donglai Wei, Mariela D. Petkova, Yuelong Wu, Zergham Ahmed,\n  Krishna Swaroop K, Silin Zou, Nils Wendt, Jonathan Boulanger-Weill, Xueying\n  Wang, Nagaraju Dhanyasi, Ignacio Arganda-Carreras, Florian Engert, Jeff\n  Lichtman, Hanspeter Pfister", "title": "NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic\n  Millimeter Scale", "comments": "The two first authors contributed equally. To be published in the\n  proceedings of MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmenting 3D cell nuclei from microscopy image volumes is critical for\nbiological and clinical analysis, enabling the study of cellular expression\npatterns and cell lineages. However, current datasets for neuronal nuclei\nusually contain volumes smaller than $10^{\\text{-}3}\\ mm^3$ with fewer than 500\ninstances per volume, unable to reveal the complexity in large brain regions\nand restrict the investigation of neuronal structures. In this paper, we have\npushed the task forward to the sub-cubic millimeter scale and curated the NucMM\ndataset with two fully annotated volumes: one $0.1\\ mm^3$ electron microscopy\n(EM) volume containing nearly the entire zebrafish brain with around 170,000\nnuclei; and one $0.25\\ mm^3$ micro-CT (uCT) volume containing part of a mouse\nvisual cortex with about 7,000 nuclei. With two imaging modalities and\nsignificantly increased volume size and instance numbers, we discover a great\ndiversity of neuronal nuclei in appearance and density, introducing new\nchallenges to the field. We also perform a statistical analysis to illustrate\nthose challenges quantitatively. To tackle the challenges, we propose a novel\nhybrid-representation learning model that combines the merits of foreground\nmask, contour map, and signed distance transform to produce high-quality 3D\nmasks. The benchmark comparisons on the NucMM dataset show that our proposed\nmethod significantly outperforms state-of-the-art nuclei segmentation\napproaches. Code and data are available at\nhttps://connectomics-bazaar.github.io/proj/nucMM/index.html.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 04:29:58 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lin", "Zudi", ""], ["Wei", "Donglai", ""], ["Petkova", "Mariela D.", ""], ["Wu", "Yuelong", ""], ["Ahmed", "Zergham", ""], ["K", "Krishna Swaroop", ""], ["Zou", "Silin", ""], ["Wendt", "Nils", ""], ["Boulanger-Weill", "Jonathan", ""], ["Wang", "Xueying", ""], ["Dhanyasi", "Nagaraju", ""], ["Arganda-Carreras", "Ignacio", ""], ["Engert", "Florian", ""], ["Lichtman", "Jeff", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2107.05856", "submitter": "Jiangbo Yuan", "authors": "Jiangbo Yuan, An-Ti Chiang, Wen Tang, Antonio Haro", "title": "eProduct: A Million-Scale Visual Search Benchmark to Address Product\n  Recognition Challenges", "comments": "This paper was accepted at FGVC8 CVPR2021 as a competition paper\n  (https://sites.google.com/view/fgvc8/papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale product recognition is one of the major applications of computer\nvision and machine learning in the e-commerce domain. Since the number of\nproducts is typically much larger than the number of categories of products,\nimage-based product recognition is often cast as a visual search rather than a\nclassification problem. It is also one of the instances of super fine-grained\nrecognition, where there are many products with slight or subtle visual\ndifferences. It has always been a challenge to create a benchmark dataset for\ntraining and evaluation on various visual search solutions in a real-world\nsetting. This motivated creation of eProduct, a dataset consisting of 2.5\nmillion product images towards accelerating development in the areas of\nself-supervised learning, weakly-supervised learning, and multimodal learning,\nfor fine-grained recognition. We present eProduct as a training set and an\nevaluation set, where the training set contains 1.3M+ listing images with\ntitles and hierarchical category labels, for model development, and the\nevaluation set includes 10,000 query and 1.1 million index images for visual\nsearch evaluation. We will present eProduct's construction steps, provide\nanalysis about its diversity and cover the performance of baseline models\ntrained on it.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 05:28:34 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Yuan", "Jiangbo", ""], ["Chiang", "An-Ti", ""], ["Tang", "Wen", ""], ["Haro", "Antonio", ""]]}, {"id": "2107.05869", "submitter": "Weiqing Min", "authors": "Weiqing Min, Chunlin Liu, Shuqiang Jiang", "title": "Towards Building a Food Knowledge Graph for Internet of Food", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The deployment of various networks (e.g., Internet of Things\n(IoT) and mobile networks) and databases (e.g., nutrition tables and food\ncompositional databases) in the food system generates massive information silos\ndue to the well-known data harmonization problem. The food knowledge graph\nprovides a unified and standardized conceptual terminology and their\nrelationships in a structured form and thus can transform these information\nsilos across the whole food system to a more reusable globally digitally\nconnected Internet of Food, enabling every stage of the food system from\nfarm-to-fork.\n  Scope and approach: We review the evolution of food knowledge organization,\nfrom food classification, food ontology to food knowledge graphs. We then\ndiscuss the progress in food knowledge graphs from several representative\napplications. We finally discuss the main challenges and future directions.\n  Key findings and conclusions: Our comprehensive summary of current research\non food knowledge graphs shows that food knowledge graphs play an important\nrole in food-oriented applications, including food search and Question\nAnswering (QA), personalized dietary recommendation, food analysis and\nvisualization, food traceability, and food machinery intelligent manufacturing.\nFuture directions for food knowledge graphs cover several fields such as\nmultimodal food knowledge graphs and food intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 06:26:53 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Min", "Weiqing", ""], ["Liu", "Chunlin", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2107.05887", "submitter": "Eslam Bakr Mohamed", "authors": "Eslam Mohamed and Ahmad El-Sallab", "title": "ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer", "comments": "arXiv admin note: substantial text overlap with arXiv:2106.11401", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for\nobject detection from a sequence of temporal frames. We treat the temporal\nframes as sequences in both space and time and employ the full attention\nmechanisms to take advantage of the features correlations over both dimensions.\nThis treatment enables us to deal with frames sequence as temporal object\nfeatures traces over every location in the space. We explore two possible\napproaches; the early spatial features aggregation over the temporal dimension,\nand the late temporal aggregation of object query spatial features. Moreover,\nwe propose a novel Temporal Positional Embedding technique to encode the time\nsequence information. To evaluate our approach, we choose the Moving Object\nDetection (MOD)task, since it is a perfect candidate to showcase the importance\nof the temporal dimension. Results show a significant 5% mAP improvement on the\nKITTI MOD dataset over the 1-step spatial baseline.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:38:08 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 19:47:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mohamed", "Eslam", ""], ["El-Sallab", "Ahmad", ""]]}, {"id": "2107.05893", "submitter": "Aihua Mao", "authors": "Aihua Mao, Zihui Du, Junhui Hou, Yaqi Duan, Yong-jin Liu, Ying He", "title": "PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud upsampling aims to generate dense point clouds from given sparse\nones, which is a challenging task due to the irregular and unordered nature of\npoint sets. To address this issue, we present a novel deep learning-based\nmodel, called PU-Flow,which incorporates normalizing flows and feature\ninterpolation techniques to produce dense points uniformly distributed on the\nunderlying surface. Specifically, we formulate the upsampling process as point\ninterpolation in a latent space, where the interpolation weights are adaptively\nlearned from local geometric context, and exploit the invertible\ncharacteristics of normalizing flows to transform points between Euclidean and\nlatent spaces. We evaluate PU-Flow on a wide range of 3D models with sharp\nfeatures and high-frequency details. Qualitative and quantitative results show\nthat our method outperforms state-of-the-art deep learning-based approaches in\nterms of reconstruction quality, proximity-to-surface accuracy, and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:45:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Mao", "Aihua", ""], ["Du", "Zihui", ""], ["Hou", "Junhui", ""], ["Duan", "Yaqi", ""], ["Liu", "Yong-jin", ""], ["He", "Ying", ""]]}, {"id": "2107.05894", "submitter": "Eric Fiege", "authors": "Eric Fiege, Salima Houta, Pinar Bisgin, Rainer Surges, Falk Howar", "title": "Automatic Seizure Detection Using the Pulse Transit Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Documentation of epileptic seizures plays an essential role in planning\nmedical therapy. Solutions for automated epileptic seizure detection can help\nimprove the current problem of incomplete and erroneous manual documentation of\nepileptic seizures. In recent years, a number of wearable sensors have been\ntested for this purpose. However, detecting seizures with subtle symptoms\nremains difficult and current solutions tend to have a high false alarm rate.\nSeizures can also affect the patient's arterial blood pressure, which has not\nyet been studied for detection with sensors. The pulse transit time (PTT)\nprovides a noninvasive estimate of arterial blood pressure. It can be obtained\nby using to two sensors, which are measuring the time differences between\narrivals of the pulse waves. Due to separated time chips a clock drift emerges,\nwhich is strongly influencing the PTT. In this work, we present an algorithm\nwhich responds to alterations in the PTT, considering the clock drift and\nenabling the noninvasive monitoring of blood pressure alterations using\nseparated sensors. Furthermore we investigated whether seizures can be detected\nusing the PTT. Our results indicate that using the algorithm, it is possible to\ndetect seizures with a Random Forest. Using the PTT along with other signals in\na multimodal approach, the detection of seizures with subtle symptoms could\nthereby be improved.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:46:47 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Fiege", "Eric", ""], ["Houta", "Salima", ""], ["Bisgin", "Pinar", ""], ["Surges", "Rainer", ""], ["Howar", "Falk", ""]]}, {"id": "2107.05904", "submitter": "Ling Zhou", "authors": "Qirong Mao, Ling Zhou, Wenming Zheng, Xiuyan Shao, Xiaohua Huang", "title": "Region attention and graph embedding network for occlusion objective\n  class-based micro-expression recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Micro-expression recognition (\\textbf{MER}) has attracted lots of\nresearchers' attention in a decade. However, occlusion will occur for MER in\nreal-world scenarios. This paper deeply investigates an interesting but\nunexplored challenging issue in MER, \\ie, occlusion MER. First, to research MER\nunder real-world occlusion, synthetic occluded micro-expression databases are\ncreated by using various mask for the community. Second, to suppress the\ninfluence of occlusion, a \\underline{R}egion-inspired \\underline{R}elation\n\\underline{R}easoning \\underline{N}etwork (\\textbf{RRRN}) is proposed to model\nrelations between various facial regions. RRRN consists of a backbone network,\nthe Region-Inspired (\\textbf{RI}) module and Relation Reasoning (\\textbf{RR})\nmodule. More specifically, the backbone network aims at extracting feature\nrepresentations from different facial regions, RI module computing an adaptive\nweight from the region itself based on attention mechanism with respect to the\nunobstructedness and importance for suppressing the influence of occlusion, and\nRR module exploiting the progressive interactions among these regions by\nperforming graph convolutions. Experiments are conducted on handout-database\nevaluation and composite database evaluation tasks of MEGC 2018 protocol.\nExperimental results show that RRRN can significantly explore the importance of\nfacial regions and capture the cooperative complementary relationship of facial\nregions for MER. The results also demonstrate RRRN outperforms the\nstate-of-the-art approaches, especially on occlusion, and RRRN acts more robust\nto occlusion.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 08:04:03 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Mao", "Qirong", ""], ["Zhou", "Ling", ""], ["Zheng", "Wenming", ""], ["Shao", "Xiuyan", ""], ["Huang", "Xiaohua", ""]]}, {"id": "2107.05938", "submitter": "Konstantinos Kamnitsas", "authors": "Gregory Filbrandt, Konstantinos Kamnitsas, David Bernstein, Alexandra\n  Taylor, Ben Glocker", "title": "Learning from Partially Overlapping Labels: Image Segmentation under\n  Annotation Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of high quality annotated images remains a limiting factor for\ntraining accurate image segmentation models. While more and more annotated\ndatasets become publicly available, the number of samples in each individual\ndatabase is often small. Combining different databases to create larger amounts\nof training data is appealing yet challenging due to the heterogeneity as a\nresult of differences in data acquisition and annotation processes, often\nyielding incompatible or even conflicting information. In this paper, we\ninvestigate and propose several strategies for learning from partially\noverlapping labels in the context of abdominal organ segmentation. We find that\ncombining a semi-supervised approach with an adaptive cross entropy loss can\nsuccessfully exploit heterogeneously annotated data and substantially improve\nsegmentation accuracy compared to baseline and alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:22:24 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Filbrandt", "Gregory", ""], ["Kamnitsas", "Konstantinos", ""], ["Bernstein", "David", ""], ["Taylor", "Alexandra", ""], ["Glocker", "Ben", ""]]}, {"id": "2107.05940", "submitter": "Veronika Cheplygina", "authors": "Irma van den Brandt, Floris Fok, Bas Mulders, Joaquin Vanschoren,\n  Veronika Cheplygina", "title": "Cats, not CAT scans: a study of dataset similarity in transfer learning\n  for 2D medical image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning is a commonly used strategy for medical image\nclassification, especially via pretraining on source data and fine-tuning on\ntarget data. There is currently no consensus on how to choose appropriate\nsource data, and in the literature we can find both evidence of favoring large\nnatural image datasets such as ImageNet, and evidence of favoring more\nspecialized medical datasets. In this paper we perform a systematic study with\nnine source datasets with natural or medical images, and three target medical\ndatasets, all with 2D images. We find that ImageNet is the source leading to\nthe highest performances, but also that larger datasets are not necessarily\nbetter. We also study different definitions of data similarity. We show that\ncommon intuitions about similarity may be inaccurate, and therefore not\nsufficient to predict an appropriate source a priori. Finally, we discuss\nseveral steps needed for further research in this field, especially with regard\nto other types (for example 3D) medical images. Our experiments and pretrained\nmodels are available via \\url{https://www.github.com/vcheplygina/cats-scans}\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:24:34 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Brandt", "Irma van den", ""], ["Fok", "Floris", ""], ["Mulders", "Bas", ""], ["Vanschoren", "Joaquin", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2107.05942", "submitter": "Suranjan Goswami", "authors": "Suranjan Goswami, IEEE Student Member, Satish Kumar Singh, Senior\n  Member, IEEE and Bidyut B. Chaudhuri, Life Fellow, IEEE", "title": "A Novel Deep Learning Method for Thermal to Annotated Thermal-Optical\n  Fused Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thermal Images profile the passive radiation of objects and capture them in\ngrayscale images. Such images have a very different distribution of data\ncompared to optical colored images. We present here a work that produces a\ngrayscale thermo-optical fused mask given a thermal input. This is a deep\nlearning based pioneering work since to the best of our knowledge, there exists\nno other work on thermal-optical grayscale fusion. Our method is also unique in\nthe sense that the deep learning method we are proposing here works on the\nDiscrete Wavelet Transform (DWT) domain instead of the gray level domain. As a\npart of this work, we also present a new and unique database for obtaining the\nregion of interest in thermal images based on an existing thermal visual paired\ndatabase, containing the Region of Interest on 5 different classes of data.\nFinally, we are proposing a simple low cost overhead statistical measure for\nidentifying the region of interest in the fused images, which we call as the\nRegion of Fusion (RoF). Experiments on the database show encouraging results in\nidentifying the region of interest in the fused images. We also show that they\ncan be processed better in the mixed form rather than with only thermal images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:29:12 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Goswami", "Suranjan", ""], ["Member", "IEEE Student", ""], ["Singh", "Satish Kumar", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["Chaudhuri", "Bidyut B.", ""], ["Fellow", "Life", ""], ["IEEE", "", ""]]}, {"id": "2107.05945", "submitter": "Tao Sheng", "authors": "Tao Sheng, Jie Chen, Zhouhui Lian", "title": "CentripetalText: An Efficient Text Instance Representation for Scene\n  Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scene text detection remains a grand challenge due to the variation in text\ncurvatures, orientations, and aspect ratios. One of the most intractable\nproblems is how to represent text instances of arbitrary shapes. Although many\nstate-of-the-art methods have been proposed to model irregular texts in a\nflexible manner, most of them lose simplicity and robustness. Their complicated\npost-processings and the regression under Dirac delta distribution undermine\nthe detection performance and the generalization ability. In this paper, we\npropose an efficient text instance representation named CentripetalText (CT),\nwhich decomposes text instances into the combination of text kernels and\ncentripetal shifts. Specifically, we utilize the centripetal shifts to\nimplement the pixel aggregation, which guide the external text pixels to the\ninternal text kernels. The relaxation operation is integrated into the dense\nregression for centripetal shifts, allowing the correct prediction in a range,\nnot a specific value. The convenient reconstruction of the text contours and\nthe tolerance of the prediction errors in our method guarantee the high\ndetection accuracy and the fast inference speed respectively. Besides, we\nshrink our text detector into a proposal generation module, namely\nCentripetalText Proposal Network (CPN), replacing SPN in Mask TextSpotter v3\nand producing more accurate proposals. To validate the effectiveness of our\ndesigns, we conduct experiments on several commonly used scene text benchmarks,\nincluding both curved and multi-oriented text datasets. For the task of scene\ntext detection, our approach achieves superior or competitive performance\ncompared to other existing methods, e.g., F-measure of 86.3% at 40.0 FPS on\nTotal-Text, F-measure of 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of\nend-to-end scene text recognition, we outperform Mask TextSpotter v3 by 1.1% on\nTotal-Text.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:34:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sheng", "Tao", ""], ["Chen", "Jie", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2107.05946", "submitter": "Pingping Zhang Dr", "authors": "Guowen Zhang and Pingping Zhang and Jinqing Qi and Huchuan Lu", "title": "HAT: Hierarchical Aggregation Transformers for Person Re-identification", "comments": "This work has been accepted by ACM International Conference on\n  Multimedia 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the advance of deep Convolutional Neural Networks (CNNs),\nperson Re-Identification (Re-ID) has witnessed great success in various\napplications. However, with limited receptive fields of CNNs, it is still\nchallenging to extract discriminative representations in a global view for\npersons under non-overlapped cameras. Meanwhile, Transformers demonstrate\nstrong abilities of modeling long-range dependencies for spatial and sequential\ndata. In this work, we take advantages of both CNNs and Transformers, and\npropose a novel learning framework named Hierarchical Aggregation Transformer\n(HAT) for image-based person Re-ID with high performance. To achieve this goal,\nwe first propose a Deeply Supervised Aggregation (DSA) to recurrently aggregate\nhierarchical features from CNN backbones. With multi-granularity supervisions,\nthe DSA can enhance multi-scale features for person retrieval, which is very\ndifferent from previous methods. Then, we introduce a Transformer-based Feature\nCalibration (TFC) to integrate low-level detail information as the global prior\nfor high-level semantic information. The proposed TFC is inserted to each level\nof hierarchical features, resulting in great performance improvements. To our\nbest knowledge, this work is the first to take advantages of both CNNs and\nTransformers for image-based person Re-ID. Comprehensive experiments on four\nlarge-scale Re-ID benchmarks demonstrate that our method shows better results\nthan several state-of-the-art methods. The code is released at\nhttps://github.com/AI-Zhpp/HAT.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:34:54 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 01:42:35 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhang", "Guowen", ""], ["Zhang", "Pingping", ""], ["Qi", "Jinqing", ""], ["Lu", "Huchuan", ""]]}, {"id": "2107.05948", "submitter": "Qinglin Li", "authors": "Qinglin Li, Bin Li, Jonathan M Garibaldi, Guoping Qiu", "title": "On Designing Good Representation Learning Models", "comments": "15 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:39:43 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Qinglin", ""], ["Li", "Bin", ""], ["Garibaldi", "Jonathan M", ""], ["Qiu", "Guoping", ""]]}, {"id": "2107.05975", "submitter": "Camila Gonzalez", "authors": "Camila Gonzalez, Karol Gotkowski, Andreas Bucher, Ricarda Fischbach,\n  Isabel Kaltenborn, Anirban Mukhopadhyay", "title": "Detecting when pre-trained nnU-Net models fail silently for Covid-19\n  lung lesion segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic segmentation of lung lesions in computer tomography has the\npotential to ease the burden of clinicians during the Covid-19 pandemic. Yet\npredictive deep learning models are not trusted in the clinical routine due to\nfailing silently in out-of-distribution (OOD) data. We propose a lightweight\nOOD detection method that exploits the Mahalanobis distance in the feature\nspace. The proposed approach can be seamlessly integrated into state-of-the-art\nsegmentation pipelines without requiring changes in model architecture or\ntraining procedure, and can therefore be used to assess the suitability of\npre-trained models to new data. We validate our method with a patch-based\nnnU-Net architecture trained with a multi-institutional dataset and find that\nit effectively detects samples that the model segments incorrectly.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 10:48:08 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 11:45:47 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Gonzalez", "Camila", ""], ["Gotkowski", "Karol", ""], ["Bucher", "Andreas", ""], ["Fischbach", "Ricarda", ""], ["Kaltenborn", "Isabel", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2107.05990", "submitter": "Sebastian P\\\"olsterl", "authors": "Sebastian P\\\"olsterl and Tom Nuno Wolf and Christian Wachinger", "title": "Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map\n  Transform", "comments": "Accepted at 2021 International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on diagnosing Alzheimer's disease from magnetic resonance images\nof the brain established that convolutional neural networks (CNNs) can leverage\nthe high-dimensional image information for classifying patients. However,\nlittle research focused on how these models can utilize the usually\nlow-dimensional tabular information, such as patient demographics or laboratory\nmeasurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a\ngeneral-purpose module for CNNs that dynamically rescales and shifts the\nfeature maps of a convolutional layer, conditional on a patient's tabular\nclinical information. We show that DAFT is highly effective in combining 3D\nimage and tabular information for diagnosis and time-to-dementia prediction,\nwhere it outperforms competing CNNs with a mean balanced accuracy of 0.622 and\nmean c-index of 0.748, respectively. Our extensive ablation study provides\nvaluable insights into the architectural properties of DAFT. Our implementation\nis available at https://github.com/ai-med/DAFT.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 11:18:22 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["P\u00f6lsterl", "Sebastian", ""], ["Wolf", "Tom Nuno", ""], ["Wachinger", "Christian", ""]]}, {"id": "2107.05997", "submitter": "Sebastian P\\\"olsterl", "authors": "Sebastian P\\\"olsterl and Christina Aigner and Christian Wachinger", "title": "Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from\n  Heterogeneous Data", "comments": "Accepted at 2021 International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 11:25:54 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["P\u00f6lsterl", "Sebastian", ""], ["Aigner", "Christina", ""], ["Wachinger", "Christian", ""]]}, {"id": "2107.06011", "submitter": "Pierre Marza", "authors": "Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf", "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object affordances. In classical Reinforcement\nLearning (RL) setups, this capacity is learned from reward alone. We introduce\nsupplementary supervision in the form of auxiliary tasks designed to favor the\nemergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 12:01:05 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Marza", "Pierre", ""], ["Matignon", "Laetitia", ""], ["Simonin", "Olivier", ""], ["Wolf", "Christian", ""]]}, {"id": "2107.06018", "submitter": "Ryan Webster", "authors": "Ryan Webster and Julien Rabin and Loic Simon and Frederic Jurie", "title": "This Person (Probably) Exists. Identity Membership Attacks Against GAN\n  Generated Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, generative adversarial networks (GANs) have achieved stunning\nrealism, fooling even human observers. Indeed, the popular tongue-in-cheek\nwebsite {\\small \\url{ http://thispersondoesnotexist.com}}, taunts users with\nGAN generated images that seem too real to believe. On the other hand, GANs do\nleak information about their training data, as evidenced by membership attacks\nrecently demonstrated in the literature. In this work, we challenge the\nassumption that GAN faces really are novel creations, by constructing a\nsuccessful membership attack of a new kind. Unlike previous works, our attack\ncan accurately discern samples sharing the same identity as training samples\nwithout being the same samples. We demonstrate the interest of our attack\nacross several popular face datasets and GAN training procedures. Notably, we\nshow that even in the presence of significant dataset diversity, an over\nrepresented person can pose a privacy concern.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 12:11:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Webster", "Ryan", ""], ["Rabin", "Julien", ""], ["Simon", "Loic", ""], ["Jurie", "Frederic", ""]]}, {"id": "2107.06028", "submitter": "Emanuel Laude", "authors": "Hartmut Bauermeister and Emanuel Laude and Thomas M\\\"ollenhoff and\n  Michael Moeller and Daniel Cremers", "title": "Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable\n  Approach for Continuous Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dual decomposition approaches in nonconvex optimization may suffer from a\nduality gap. This poses a challenge when applying them directly to nonconvex\nproblems such as MAP-inference in a Markov random field (MRF) with continuous\nstate spaces. To eliminate such gaps, this paper considers a reformulation of\nthe original nonconvex task in the space of measures. This infinite-dimensional\nreformulation is then approximated by a semi-infinite one, which is obtained\nvia a piecewise polynomial discretization in the dual. We provide a geometric\nintuition behind the primal problem induced by the dual discretization and draw\nconnections to optimization over moment spaces. In contrast to existing\ndiscretizations which suffer from a grid bias, we show that a piecewise\npolynomial discretization better preserves the continuous nature of our\nproblem. Invoking results from optimal transport theory and convex algebraic\ngeometry we reduce the semi-infinite program to a finite one and provide a\npractical implementation based on semidefinite programming. We show,\nexperimentally and in theory, that the approach successfully reduces the\nduality gap. To showcase the scalability of our approach, we apply it to the\nstereo matching problem between two images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 12:31:06 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bauermeister", "Hartmut", ""], ["Laude", "Emanuel", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Moeller", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "2107.06050", "submitter": "Guangjie Leng", "authors": "Guangjie Leng, Yekun Zhu and Zhi-Qin John Xu", "title": "Force-in-domain GAN inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Empirical works suggest that various semantics emerge in the latent space of\nGenerative Adversarial Networks (GANs) when being trained to generate images.\nTo perform real image editing, it requires an accurate mapping from the real\nimage to the latent space to leveraging these learned semantics, which is\nimportant yet difficult. An in-domain GAN inversion approach is recently\nproposed to constraint the inverted code within the latent space by forcing the\nreconstructed image obtained from the inverted code within the real image\nspace. Empirically, we find that the inverted code by the in-domain GAN can\ndeviate from the latent space significantly. To solve this problem, we propose\na force-in-domain GAN based on the in-domain GAN, which utilizes a\ndiscriminator to force the inverted code within the latent space. The\nforce-in-domain GAN can also be interpreted by a cycle-GAN with slight\nmodification. Extensive experiments show that our force-in-domain GAN not only\nreconstructs the target image at the pixel level, but also align the inverted\ncode with the latent space well for semantic editing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:03:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 01:42:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Leng", "Guangjie", ""], ["Zhu", "Yekun", ""], ["Xu", "Zhi-Qin John", ""]]}, {"id": "2107.06098", "submitter": "Sumedha Singla", "authors": "Sumedha Singla, Stephen Wallace, Sofia Triantafillou, Kayhan\n  Batmanghelich", "title": "Using Causal Analysis for Conceptual Deep Learning Explanation", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Model explainability is essential for the creation of trustworthy Machine\nLearning models in healthcare. An ideal explanation resembles the\ndecision-making process of a domain expert and is expressed using concepts or\nterminology that is meaningful to the clinicians. To provide such an\nexplanation, we first associate the hidden units of the classifier to\nclinically relevant concepts. We take advantage of radiology reports\naccompanying the chest X-ray images to define concepts. We discover sparse\nassociations between concepts and hidden units using a linear sparse logistic\nregression. To ensure that the identified units truly influence the\nclassifier's outcome, we adopt tools from Causal Inference literature and, more\nspecifically, mediation analysis through counterfactual interventions. Finally,\nwe construct a low-depth decision tree to translate all the discovered concepts\ninto a straightforward decision rule, expressed to the radiologist. We\nevaluated our approach on a large chest x-ray dataset, where our model produces\na global explanation consistent with clinical knowledge.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 00:01:45 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Singla", "Sumedha", ""], ["Wallace", "Stephen", ""], ["Triantafillou", "Sofia", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "2107.06125", "submitter": "Nisarg Shah", "authors": "Sourya Dipta Das, Nisarg A. Shah, Saikat Dutta", "title": "MSR-Net: Multi-Scale Relighting Network for One-to-One Relighting", "comments": "Workshop on Differentiable Vision, Graphics, and Physics in Machine\n  Learning at NeurIPS 2020. arXiv admin note: text overlap with\n  arXiv:2102.09242", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image relighting allows photo enhancement by illumination-specific\nretouching without human effort and so it is getting much interest lately. Most\nof the existing popular methods available for relighting are run-time intensive\nand memory inefficient. Keeping these issues in mind, we propose the use of\nStacked Deep Multi-Scale Hierarchical Network, which aggregates features from\neach image at different scales. Our solution is differentiable and robust for\ntranslating image illumination setting from input image to target image.\nAdditionally, we have also shown that using a multi-step training approach to\nthis problem with two different loss functions can significantly boost\nperformance and can achieve a high quality reconstruction of a relighted image.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:25:05 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Dutta", "Saikat", ""]]}, {"id": "2107.06129", "submitter": "Tao Sheng", "authors": "Tao Sheng, Zhouhui Lian", "title": "Bidirectional Regression for Arbitrary-Shaped Text Detection", "comments": "Accepted at ICDAR 2021, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-shaped text detection has recently attracted increasing interests\nand witnessed rapid development with the popularity of deep learning\nalgorithms. Nevertheless, existing approaches often obtain inaccurate detection\nresults, mainly due to the relatively weak ability to utilize context\ninformation and the inappropriate choice of offset references. This paper\npresents a novel text instance expression which integrates both foreground and\nbackground information into the pipeline, and naturally uses the pixels near\ntext boundaries as the offset starts. Besides, a corresponding post-processing\nalgorithm is also designed to sequentially combine the four prediction results\nand reconstruct the text instance accurately. We evaluate our method on several\nchallenging scene text benchmarks, including both curved and multi-oriented\ntext datasets. Experimental results demonstrate that the proposed approach\nobtains superior or competitive performance compared to other state-of-the-art\nmethods, e.g., 83.4% F-score for Total-Text, 82.4% F-score for MSRA-TD500, etc.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:29:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sheng", "Tao", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2107.06130", "submitter": "Raphael Sulzer", "authors": "Raphael Sulzer, Loic Landrieu, Renaud Marlet, Bruno Vallet", "title": "Scalable Surface Reconstruction with Delaunay-Graph Neural Networks", "comments": "The presentation of this work at SGP 2021 is available at\n  https://youtu.be/KIrCDGhS10o", "journal-ref": "Computer Graphics Forum 2021", "doi": "10.1111/cgf.14364", "report-no": "40-Issue 5", "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel learning-based, visibility-aware, surface reconstruction\nmethod for large-scale, defect-laden point clouds. Our approach can cope with\nthe scale and variety of point cloud defects encountered in real-life\nMulti-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay\ntetrahedralization whose cells are classified as inside or outside the surface\nby a graph neural network and an energy model solvable with a graph cut. Our\nmodel, making use of both local geometric attributes and line-of-sight\nvisibility information, is able to learn a visibility model from a small amount\nof synthetic training data and generalizes to real-life acquisitions. Combining\nthe efficiency of deep learning methods and the scalability of energy based\nmodels, our approach outperforms both learning and non learning-based\nreconstruction algorithms on two publicly available reconstruction benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:30:32 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 16:01:59 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Sulzer", "Raphael", ""], ["Landrieu", "Loic", ""], ["Marlet", "Renaud", ""], ["Vallet", "Bruno", ""]]}, {"id": "2107.06132", "submitter": "Antonio Di Pilato", "authors": "Antonio Di Pilato, Nicol\\`o Taggio, Alexis Pompili, Michele\n  Iacobellis, Adriano Di Florio, Davide Passarelli, Sergio Samarelli", "title": "Deep learning approaches to Earth Observation change detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The interest for change detection in the field of remote sensing has\nincreased in the last few years. Searching for changes in satellite images has\nmany useful applications, ranging from land cover and land use analysis to\nanomaly detection. In particular, urban change detection provides an efficient\ntool to study urban spread and growth through several years of observation. At\nthe same time, change detection is often a computationally challenging and\ntime-consuming task, which requires innovative methods to guarantee optimal\nresults with unquestionable value and within reasonable time. In this paper we\npresent two different approaches to change detection (semantic segmentation and\nclassification) that both exploit convolutional neural networks to achieve good\nresults, which can be further refined and used in a post-processing workflow\nfor a large variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:34:59 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Di Pilato", "Antonio", ""], ["Taggio", "Nicol\u00f2", ""], ["Pompili", "Alexis", ""], ["Iacobellis", "Michele", ""], ["Di Florio", "Adriano", ""], ["Passarelli", "Davide", ""], ["Samarelli", "Sergio", ""]]}, {"id": "2107.06149", "submitter": "Jia Zheng", "authors": "Haocheng Ren and Hao Zhang and Jia Zheng and Jiaxiang Zheng and Rui\n  Tang and Rui Wang and Hujun Bao", "title": "MINERVAS: Massive INterior EnviRonments VirtuAl Synthesis", "comments": "The two first authors contribute equally. Project pape:\n  https://coohom.github.io/MINERVAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of data-driven techniques, data has played an\nessential role in various computer vision tasks. Many realistic and synthetic\ndatasets have been proposed to address different problems. However, there are\nlots of unresolved challenges: (1) the creation of dataset is usually a tedious\nprocess with manual annotations, (2) most datasets are only designed for a\nsingle specific task, (3) the modification or randomization of the 3D scene is\ndifficult, and (4) the release of commercial 3D data may encounter copyright\nissue. This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl\nSynthesis system, to facilitate the 3D scene modification and the 2D image\nsynthesis for various vision tasks. In particular, we design a programmable\npipeline with Domain-Specific Language, allowing users to (1) select scenes\nfrom the commercial indoor scene database, (2) synthesize scenes for different\ntasks with customized rules, and (3) render various imagery data, such as\nvisual color, geometric structures, semantic label. Our system eases the\ndifficulty of customizing massive numbers of scenes for different tasks and\nrelieves users from manipulating fine-grained scene configurations by providing\nuser-controllable randomness using multi-level samplers. Most importantly, it\nempowers users to access commercial scene databases with millions of indoor\nscenes and protects the copyright of core data assets, e.g., 3D CAD models. We\ndemonstrate the validity and flexibility of our system by using our synthesized\ndata to improve the performance on different kinds of computer vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:53:01 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:21:45 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ren", "Haocheng", ""], ["Zhang", "Hao", ""], ["Zheng", "Jia", ""], ["Zheng", "Jiaxiang", ""], ["Tang", "Rui", ""], ["Wang", "Rui", ""], ["Bao", "Hujun", ""]]}, {"id": "2107.06154", "submitter": "Shuhao Cui", "authors": "Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang and Qi\n  Tian", "title": "Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain\n  Adaptation", "comments": "TPAMI under revivew. arXiv admin note: text overlap with\n  arXiv:2003.12237", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the domain discrepancy in visual domain adaptation, the performance of\nsource model degrades when bumping into the high data density near decision\nboundary in target domain. A common solution is to minimize the Shannon Entropy\nto push the decision boundary away from the high density area. However, entropy\nminimization also leads to severe reduction of prediction diversity, and\nunfortunately brings harm to the domain adaptation. In this paper, we\ninvestigate the prediction discriminability and diversity by studying the\nstructure of the classification output matrix of a randomly selected data\nbatch. We find by theoretical analysis that the prediction discriminability and\ndiversity could be separately measured by the Frobenius-norm and rank of the\nbatch output matrix. The nuclear-norm is an upperbound of the former, and a\nconvex approximation of the latter. Accordingly, we propose Batch Nuclear-norm\nMaximization and Minimization, which performs nuclear-norm maximization on the\ntarget output matrix to enhance the target prediction ability, and nuclear-norm\nminimization on the source batch output matrix to increase applicability of the\nsource domain knowledge. We further approximate the nuclear-norm by\nL_{1,2}-norm, and design multi-batch optimization for stable solution on large\nnumber of categories. The fast approximation method achieves O(n^2)\ncomputational complexity and better convergence property. Experiments show that\nour method could boost the adaptation accuracy and robustness under three\ntypical domain adaptation scenarios. The code is available at\nhttps://github.com/cuishuhao/BNM.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:08:32 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 15:25:38 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 13:15:12 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Cui", "Shuhao", ""], ["Wang", "Shuhui", ""], ["Zhuo", "Junbao", ""], ["Li", "Liang", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2107.06165", "submitter": "Albert Matveev", "authors": "Albert Matveev, Alexey Artemov, Denis Zorin and Evgeny Burnaev", "title": "3D Parametric Wireframe Extraction Based on Distance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a pipeline for parametric wireframe extraction from densely\nsampled point clouds. Our approach processes a scalar distance field that\nrepresents proximity to the nearest sharp feature curve. In intermediate\nstages, it detects corners, constructs curve segmentation, and builds a\ntopological graph fitted to the wireframe. As an output, we produce parametric\nspline curves that can be edited and sampled arbitrarily. We evaluate our\nmethod on 50 complex 3D shapes and compare it to the novel deep learning-based\ntechnique, demonstrating superior quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:25:14 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Matveev", "Albert", ""], ["Artemov", "Alexey", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2107.06179", "submitter": "Javad Hassannataj Joloudari", "authors": "Javad Hassannataj Joloudari, Sanaz Mojrian, Issa Nodehi, Amir\n  Mashmool, Zeynab Kiani Zadegan, Sahar Khanjani Shirkharkolaie, Tahereh\n  Tamadon, Samiyeh Khosravi, Mitra Akbari, Edris Hassannataj, Roohallah\n  Alizadehsani, Danial Sharifrazi, and Amir Mosavi", "title": "A Survey of Applications of Artificial Intelligence for Myocardial\n  Infarction Disease Diagnosis", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Myocardial infarction disease (MID) is caused to the rapid progress of\nundiagnosed coronary artery disease (CAD) that indicates the injury of a heart\ncell by decreasing the blood flow to the cardiac muscles. MID is the leading\ncause of death in middle-aged and elderly subjects all over the world. In\ngeneral, raw Electrocardiogram (ECG) signals are tested for MID identification\nby clinicians that is exhausting, time-consuming, and expensive. Artificial\nintelligence-based methods are proposed to handle the problems to diagnose MID\non the ECG signals automatically. Hence, in this survey paper, artificial\nintelligence-based methods, including machine learning and deep learning, are\nreview for MID diagnosis on the ECG signals. Using the methods demonstrate that\nthe feature extraction and selection of ECG signals required to be handcrafted\nin the ML methods. In contrast, these tasks are explored automatically in the\nDL methods. Based on our best knowledge, Deep Convolutional Neural Network\n(DCNN) methods are highly required methods developed for the early diagnosis of\nMID on the ECG signals. Most researchers have tended to use DCNN methods, and\nno studies have surveyed using artificial intelligence methods for MID\ndiagnosis on the ECG signals.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:15:06 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Joloudari", "Javad Hassannataj", ""], ["Mojrian", "Sanaz", ""], ["Nodehi", "Issa", ""], ["Mashmool", "Amir", ""], ["Zadegan", "Zeynab Kiani", ""], ["Shirkharkolaie", "Sahar Khanjani", ""], ["Tamadon", "Tahereh", ""], ["Khosravi", "Samiyeh", ""], ["Akbari", "Mitra", ""], ["Hassannataj", "Edris", ""], ["Alizadehsani", "Roohallah", ""], ["Sharifrazi", "Danial", ""], ["Mosavi", "Amir", ""]]}, {"id": "2107.06187", "submitter": "Mai Lan Ha", "authors": "Mai Lan Ha and Volker Blanz", "title": "Deep Ranking with Adaptive Margin Triplet Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a simple modification from a fixed margin triplet loss to an\nadaptive margin triplet loss. While the original triplet loss is used widely in\nclassification problems such as face recognition, face re-identification and\nfine-grained similarity, our proposed loss is well suited for rating datasets\nin which the ratings are continuous values. In contrast to original triplet\nloss where we have to sample data carefully, in out method, we can generate\ntriplets using the whole dataset, and the optimization can still converge\nwithout frequently running into a model collapsing issue. The adaptive margins\nonly need to be computed once before the training, which is much less expensive\nthan generating triplets after every epoch as in the fixed margin case. Besides\nsubstantially improved training stability (the proposed model never collapsed\nin our experiments compared to a couple of times that the training collapsed on\nexisting triplet loss), we achieved slightly better performance than the\noriginal triplet loss on various rating datasets and network architectures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:37:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ha", "Mai Lan", ""], ["Blanz", "Volker", ""]]}, {"id": "2107.06197", "submitter": "Abdelhak Lemkhenter", "authors": "Abdelhak Lemkhenter, Adam Bielski, Alp Eren Sari, Paolo Favaro", "title": "Generative Adversarial Learning via Kernel Density Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for\ngenerative adversarial learning. KDD GAN formulates the training as a\nlikelihood ratio optimization problem where the data distributions are written\nexplicitly via (local) Kernel Density Estimates (KDE). This is inspired by the\nrecent progress in contrastive learning and its relation to KDE. We define the\nKDEs directly in feature space and forgo the requirement of invertibility of\nthe kernel feature mappings. In our approach, features are no longer optimized\nfor linear separability, as in the original GAN formulation, but for the more\ngeneral discrimination of distributions in the feature space. We analyze the\ngradient of our loss with respect to the feature representation and show that\nit is better behaved than that of the original hinge loss. We perform\nexperiments with the proposed KDE-based loss, used either as a training loss or\na regularization term, on both CIFAR10 and scaled versions of ImageNet. We use\nBigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the\narchitecture of the networks. We show a boost in the quality of generated\nsamples with respect to FID from 10% to 40% compared to the baseline. Code will\nbe made available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:52:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lemkhenter", "Abdelhak", ""], ["Bielski", "Adam", ""], ["Sari", "Alp Eren", ""], ["Favaro", "Paolo", ""]]}, {"id": "2107.06209", "submitter": "Mai Lan Ha", "authors": "Mai Lan Ha, Gianni Franchi, Emanuel Aldea and Volker Blanz", "title": "Learning a Discriminant Latent Space with Neural Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Discriminative features play an important role in image and object\nclassification and also in other fields of research such as semi-supervised\nlearning, fine-grained classification, out of distribution detection. Inspired\nby Linear Discriminant Analysis (LDA), we propose an optimization called Neural\nDiscriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA\ntransforms deep features to become more discriminative and, therefore, improves\nthe performances in various tasks. Our proposed optimization has two primary\ngoals for inter- and intra-class variances. The first one is to minimize\nvariances within each individual class. The second goal is to maximize pairwise\ndistances between features coming from different classes. We evaluate our NDA\noptimization in different research fields: general supervised classification,\nfine-grained classification, semi-supervised learning, and out of distribution\ndetection. We achieve performance improvements in all the fields compared to\nbaseline methods that do not use NDA. Besides, using NDA, we also surpass the\nstate of the art on the four tasks on various testing datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:06:07 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ha", "Mai Lan", ""], ["Franchi", "Gianni", ""], ["Aldea", "Emanuel", ""], ["Blanz", "Volker", ""]]}, {"id": "2107.06211", "submitter": "Jie Chen", "authors": "Jie Chen, Zaifeng Yang, Tsz Nam Chan, Hui Li, Junhui Hou, and Lap-Pui\n  Chau", "title": "Attention-Guided Progressive Neural Texture Fusion for High Dynamic\n  Range Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Dynamic Range (HDR) imaging via multi-exposure fusion is an important\ntask for most modern imaging platforms. In spite of recent developments in both\nhardware and algorithm innovations, challenges remain over content association\nambiguities caused by saturation, motion, and various artifacts introduced\nduring multi-exposure fusion such as ghosting, noise, and blur. In this work,\nwe propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion)\nHDR restoration model which aims to address these issues within one framework.\nAn efficient two-stream structure is proposed which separately focuses on\ntexture feature transfer over saturated regions and multi-exposure tonal and\ntexture feature fusion. A neural feature transfer mechanism is proposed which\nestablishes spatial correspondence between different exposures based on\nmulti-scale VGG features in the masked saturated HDR domain for discriminative\ncontextual clues over the ambiguous image areas. A progressive texture blending\nmodule is designed to blend the encoded two-stream features in a multi-scale\nand progressive manner. In addition, we introduce several novel attention\nmechanisms, i.e., the motion attention module detects and suppresses the\ncontent discrepancies among the reference images; the saturation attention\nmodule facilitates differentiating the misalignment caused by saturation from\nthose caused by motion; and the scale attention module ensures texture blending\nconsistency between different coder/decoder scales. We carry out comprehensive\nqualitative and quantitative evaluations and ablation studies, which validate\nthat these novel modules work coherently under the same framework and\noutperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:07:00 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chen", "Jie", ""], ["Yang", "Zaifeng", ""], ["Chan", "Tsz Nam", ""], ["Li", "Hui", ""], ["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2107.06212", "submitter": "Bharadwaj Manda", "authors": "Bharadwaj Manda, Shubham Dhayarkar, Sai Mitheran, V.K. Viekash,\n  Ramanathan Muthuganapathy", "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval\n  with Deep Neural Networks", "comments": "Computers & Graphics Journal, Special Section on 3DOR 2021", "journal-ref": "Computers & Graphics, Volume 99, 2021, Pages 100-113, ISSN\n  0097-8493", "doi": "10.1016/j.cag.2021.07.001", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:10:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 06:26:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Manda", "Bharadwaj", ""], ["Dhayarkar", "Shubham", ""], ["Mitheran", "Sai", ""], ["Viekash", "V. K.", ""], ["Muthuganapathy", "Ramanathan", ""]]}, {"id": "2107.06219", "submitter": "Xingxuan Zhang", "authors": "Xingxuan Zhang, Linjun Zhou, Renzhe Xu, Peng Cui, Zheyan Shen, Haoxin\n  Liu", "title": "Domain-Irrelevant Representation Learning for Unsupervised Domain\n  Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) aims to help models trained on a set of source\ndomains generalize better on unseen target domains. The performances of current\nDG methods largely rely on sufficient labeled data, which however are usually\ncostly or unavailable. While unlabeled data are far more accessible, we seek to\nexplore how unsupervised learning can help deep models generalizes across\ndomains. Specifically, we study a novel generalization problem called\nunsupervised domain generalization, which aims to learn generalizable models\nwith unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised\nLearning (DIUL) method to cope with the significant and misleading\nheterogeneity within unlabeled data and severe distribution shifts between\nsource and target data. Surprisingly we observe that DIUL can not only\ncounterbalance the scarcity of labeled data but also further strengthen the\ngeneralization ability of models when the labeled data are sufficient. As a\npretraining approach, DIUL shows superior to ImageNet pretraining protocol even\nwhen the available data are unlabeled and of a greatly smaller amount compared\nto ImageNet. Extensive experiments clearly demonstrate the effectiveness of our\nmethod compared with state-of-the-art unsupervised learning counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:20:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhang", "Xingxuan", ""], ["Zhou", "Linjun", ""], ["Xu", "Renzhe", ""], ["Cui", "Peng", ""], ["Shen", "Zheyan", ""], ["Liu", "Haoxin", ""]]}, {"id": "2107.06235", "submitter": "Fabrizio Julian Piva", "authors": "Fabrizio J. Piva, Gijs Dubbelman", "title": "Exploiting Image Translations via Ensemble Self-Supervised Learning for\n  Unsupervised Domain Adaptation", "comments": "Manuscript under review at Computer Vision and Image Understanding\n  (CVIU) journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an unsupervised domain adaption (UDA) strategy that combines\nmultiple image translations, ensemble learning and self-supervised learning in\none coherent approach. We focus on one of the standard tasks of UDA in which a\nsemantic segmentation model is trained on labeled synthetic data together with\nunlabeled real-world data, aiming to perform well on the latter. To exploit the\nadvantage of using multiple image translations, we propose an ensemble learning\napproach, where three classifiers calculate their prediction by taking as input\nfeatures of different image translations, making each classifier learn\nindependently, with the purpose of combining their outputs by sparse\nMultinomial Logistic Regression. This regression layer known as meta-learner\nhelps to reduce the bias during pseudo label generation when performing\nself-supervised learning and improves the generalizability of the model by\ntaking into consideration the contribution of each classifier. We evaluate our\nmethod on the standard UDA benchmarks, i.e. adapting GTA V and Synthia to\nCityscapes, and achieve state-of-the-art results in the mean intersection over\nunion metric. Extensive ablation experiments are reported to highlight the\nadvantageous properties of our proposed UDA strategy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:43:02 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Piva", "Fabrizio J.", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "2107.06239", "submitter": "Srikrishna Karanam", "authors": "Ren Li and Meng Zheng and Srikrishna Karanam and Terrence Chen and\n  Ziyan Wu", "title": "Everybody Is Unique: Towards Unbiased Human Mesh Recovery", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of obese human mesh recovery, i.e., fitting a\nparametric human mesh to images of obese people. Despite obese person mesh\nfitting being an important problem with numerous applications (e.g.,\nhealthcare), much recent progress in mesh recovery has been restricted to\nimages of non-obese people. In this work, we identify this crucial gap in the\ncurrent literature by presenting and discussing limitations of existing\nalgorithms. Next, we present a simple baseline to address this problem that is\nscalable and can be easily used in conjunction with existing algorithms to\nimprove their performance. Finally, we present a generalized human mesh\noptimization algorithm that substantially improves the performance of existing\nmethods on both obese person images as well as community-standard benchmark\ndatasets. A key innovation of this technique is that it does not rely on\nsupervision from expensive-to-create mesh parameters. Instead, starting from\nwidely and cheaply available 2D keypoints annotations, our method automatically\ngenerates mesh parameters that can in turn be used to re-train and fine-tune\nany existing mesh estimation algorithm. This way, we show our method acts as a\ndrop-in to improve the performance of a wide variety of contemporary mesh\nestimation methods. We conduct extensive experiments on multiple datasets\ncomprising both standard and obese person images and demonstrate the efficacy\nof our proposed techniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:52:55 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Ren", ""], ["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Wu", "Ziyan", ""]]}, {"id": "2107.06256", "submitter": "Min Jin Chong", "authors": "Min Jin Chong, Wen-Sheng Chu, Abhishek Kumar, David Forsyth", "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval", "comments": "Code is here https://github.com/mchong6/RetrieveInStyle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Retrieve in Style (RIS), an unsupervised framework for\nfine-grained facial feature transfer and retrieval on real images. Recent work\nshows that it is possible to learn a catalog that allows local semantic\ntransfers of facial features on generated images by capitalizing on the\ndisentanglement property of the StyleGAN latent space. RIS improves existing\nart on: 1) feature disentanglement and allows for challenging transfers (i.e.,\nhair and pose) that were not shown possible in SoTA methods. 2) eliminating the\nneed for per-image hyperparameter tuning, and for computing a catalog over a\nlarge batch of images. 3) enabling face retrieval using the proposed facial\nfeatures (e.g., eyes), and to our best knowledge, is the first work to retrieve\nface images at the fine-grained level. 4) robustness and natural application to\nreal images. Our qualitative and quantitative analyses show RIS achieves both\nhigh-fidelity feature transfers and accurate fine-grained retrievals on real\nimages. We discuss the responsible application of RIS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:31:34 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 08:56:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chong", "Min Jin", ""], ["Chu", "Wen-Sheng", ""], ["Kumar", "Abhishek", ""], ["Forsyth", "David", ""]]}, {"id": "2107.06257", "submitter": "Daniel Wilson", "authors": "Daniel Wilson, Thayer Alshaabi, Colin Van Oort, Xiaohan Zhang,\n  Jonathan Nelson, Safwan Wshah", "title": "Object Tracking and Geo-localization from Street Images", "comments": "28 pages, 7 figures, to be submitted to Elsevier Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-localizing static objects from street images is challenging but also very\nimportant for road asset mapping and autonomous driving. In this paper we\npresent a two-stage framework that detects and geolocalizes traffic signs from\nlow frame rate street videos. Our proposed system uses a modified version of\nRetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign\nrelative to the camera, in addition to performing the standard classification\nand bounding box regression. Candidate sign detections from GPS-RetinaNet are\ncondensed into geolocalized signs by our custom tracker, which consists of a\nlearned metric network and a variant of the Hungarian Algorithm. Our metric\nnetwork estimates the similarity between pairs of detections, then the\nHungarian Algorithm matches detections across images using the similarity\nscores provided by the metric network. Our models were trained using an updated\nversion of the ARTS dataset, which contains 25,544 images and 47.589 sign\nannotations ~\\cite{arts}. The proposed dataset covers a diverse set of\nenvironments gathered from a broad selection of roads. Each annotaiton contains\na sign class label, its geospatial location, an assembly label, a side of road\nindicator, and unique identifiers that aid in the evaluation. This dataset will\nsupport future progress in the field, and the proposed system demonstrates how\nto take advantage of some of the unique characteristics of a realistic\ngeolocalization dataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:32:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wilson", "Daniel", ""], ["Alshaabi", "Thayer", ""], ["Van Oort", "Colin", ""], ["Zhang", "Xiaohan", ""], ["Nelson", "Jonathan", ""], ["Wshah", "Safwan", ""]]}, {"id": "2107.06262", "submitter": "Qingyuan Zheng", "authors": "Qingyuan Zheng, Zhuoru Li, Adam Bargteil", "title": "Learning Aesthetic Layouts via Visual Guidance", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:46:42 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zheng", "Qingyuan", ""], ["Li", "Zhuoru", ""], ["Bargteil", "Adam", ""]]}, {"id": "2107.06263", "submitter": "Jianyuan Guo", "authors": "Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu and\n  Yunhe Wang", "title": "CMT: Convolutional Neural Networks Meet Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformers have been successfully applied to image recognition tasks\ndue to their ability to capture long-range dependencies within an image.\nHowever, there are still gaps in both performance and computational cost\nbetween transformers and existing convolutional neural networks (CNNs). In this\npaper, we aim to address this issue and develop a network that can outperform\nnot only the canonical transformers, but also the high-performance\nconvolutional models. We propose a new transformer based hybrid network by\ntaking advantage of transformers to capture long-range dependencies, and of\nCNNs to model local features. Furthermore, we scale it to obtain a family of\nmodels, called CMTs, obtaining much better accuracy and efficiency than\nprevious convolution and transformer based models. In particular, our CMT-S\nachieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S\nalso generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),\nand other challenging vision datasets such as COCO (44.3% mAP), with\nconsiderably less computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:47:19 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 06:22:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Guo", "Jianyuan", ""], ["Han", "Kai", ""], ["Wu", "Han", ""], ["Xu", "Chang", ""], ["Tang", "Yehui", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2107.06276", "submitter": "Sudhir Kumar Suman", "authors": "Sudhir Suman, Gagandeep Singh, Nicole Sakla, Rishabh Gattu, Jeremy\n  Green, Tej Phatak, Dimitris Samaras, Prateek Prasanna", "title": "Attention based CNN-LSTM Network for Pulmonary Embolism Prediction on\n  Chest Computed Tomography Pulmonary Angiograms", "comments": "This work will be presented at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more than 60,000 deaths annually in the United States, Pulmonary\nEmbolism (PE) is among the most fatal cardiovascular diseases. It is caused by\nan artery blockage in the lung; confirming its presence is time-consuming and\nis prone to over-diagnosis. The utilization of automated PE detection systems\nis critical for diagnostic accuracy and efficiency. In this study we propose a\ntwo-stage attention-based CNN-LSTM network for predicting PE, its associated\ntype (chronic, acute) and corresponding location (leftsided, rightsided or\ncentral) on computed tomography (CT) examinations. We trained our model on the\nlargest available public Computed Tomography Pulmonary Angiogram PE dataset\n(RSNA-STR Pulmonary Embolism CT (RSPECT) Dataset, N=7279 CT studies) and tested\nit on an in-house curated dataset of N=106 studies. Our framework mirrors the\nradiologic diagnostic process via a multi-slice approach so that the accuracy\nand pathologic sequela of true pulmonary emboli may be meticulously assessed,\nenabling physicians to better appraise the morbidity of a PE when present. Our\nproposed method outperformed a baseline CNN classifier and a single-stage\nCNN-LSTM network, achieving an AUC of 0.95 on the test set for detecting the\npresence of PE in the study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:58:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Suman", "Sudhir", ""], ["Singh", "Gagandeep", ""], ["Sakla", "Nicole", ""], ["Gattu", "Rishabh", ""], ["Green", "Jeremy", ""], ["Phatak", "Tej", ""], ["Samaras", "Dimitris", ""], ["Prasanna", "Prateek", ""]]}, {"id": "2107.06278", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Alexander G. Schwing and Alexander Kirillov", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation", "comments": "Project page: https://bowenc0221.github.io/maskformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern approaches typically formulate semantic segmentation as a per-pixel\nclassification task, while instance-level segmentation is handled with an\nalternative mask classification. Our key insight: mask classification is\nsufficiently general to solve both semantic- and instance-level segmentation\ntasks in a unified manner using the exact same model, loss, and training\nprocedure. Following this observation, we propose MaskFormer, a simple mask\nclassification model which predicts a set of binary masks, each associated with\na single global class label prediction. Overall, the proposed mask\nclassification-based method simplifies the landscape of effective approaches to\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\nIn particular, we observe that MaskFormer outperforms per-pixel classification\nbaselines when the number of classes is large. Our mask classification-based\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\nand panoptic segmentation (52.7 PQ on COCO) models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:59:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cheng", "Bowen", ""], ["Schwing", "Alexander G.", ""], ["Kirillov", "Alexander", ""]]}, {"id": "2107.06281", "submitter": "Islem Rekik", "authors": "Islem Mhiri and Ahmed Nebli and Mohamed Ali Mahjoub and Islem Rekik", "title": "Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic\n  Brain Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Brain graph synthesis marked a new era for predicting a target brain graph\nfrom a source one without incurring the high acquisition cost and processing\ntime of neuroimaging data. However, existing multi-modal graph synthesis\nframeworks have several limitations. First, they mainly focus on generating\ngraphs from the same domain (intra-modality), overlooking the rich multimodal\nrepresentations of brain connectivity (inter-modality). Second, they can only\nhandle isomorphic graph generation tasks, limiting their generalizability to\nsynthesizing target graphs with a different node size and topological structure\nfrom those of the source one. More importantly, both target and source domains\nmight have different distributions, which causes a domain fracture between them\n(i.e., distribution misalignment). To address such challenges, we propose an\ninter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to\ninfer a target graph modality based on a given modality. Our three core\ncontributions lie in (i) predicting a target graph (e.g., functional) from a\nsource graph (e.g., morphological) based on a novel graph generative\nadversarial network (gGAN); (ii) using non-isomorphic graphs for both source\nand target domains with a different number of nodes, edges and structure; and\n(iii) enforcing the predicted target distribution to match that of the ground\ntruth graphs using a graph autoencoder to relax the designed loss oprimization.\nTo handle the unstable behavior of gGAN, we design a new Ground\nTruth-Preserving (GT-P) loss function to guide the generator in learning the\ntopological structure of ground truth brain graphs. Our comprehensive\nexperiments on predicting functional from morphological graphs demonstrate the\noutperformance of IMANGraphNet in comparison with its variants. This can be\nfurther leveraged for integrative and holistic brain mapping in health and\ndisease.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:59:55 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Mhiri", "Islem", ""], ["Nebli", "Ahmed", ""], ["Mahjoub", "Mohamed Ali", ""], ["Rekik", "Islem", ""]]}, {"id": "2107.06304", "submitter": "Xin Dong", "authors": "Xin Dong, Hongxu Yin, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov", "title": "Deep Neural Networks are Surprisingly Reversible: A Baseline for\n  Zero-Shot Inversion", "comments": "A new inversion method to reverse neural networks and get input from\n  intermediate feature maps. Works without original data for classifiers and\n  GANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behavior and vulnerability of pre-trained deep neural\nnetworks (DNNs) can help to improve them. Analysis can be performed via\nreversing the network's flow to generate inputs from internal representations.\nMost existing work relies on priors or data-intensive optimization to invert a\nmodel, yet struggles to scale to deep architectures and complex datasets. This\npaper presents a zero-shot direct model inversion framework that recovers the\ninput to the trained model given only the internal representation. The crux of\nour method is to inverse the DNN in a divide-and-conquer manner while\nre-syncing the inverted layers via cycle-consistency guidance with the help of\nsynthesized data. As a result, we obtain a single feed-forward model capable of\ninversion with a single forward pass without seeing any real data of the\noriginal task. With the proposed approach, we scale zero-shot direct inversion\nto deep architectures and complex datasets. We empirically show that modern\nclassification models on ImageNet can, surprisingly, be inverted, allowing an\napproximate recovery of the original 224x224px images from a representation\nafter more than 20 layers. Moreover, inversion of generators in GANs unveils\nlatent code of a given synthesized face image at 128x128px, which can even, in\nturn, improve defective synthesized images from GANs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 18:01:43 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Dong", "Xin", ""], ["Yin", "Hongxu", ""], ["Alvarez", "Jose M.", ""], ["Kautz", "Jan", ""], ["Molchanov", "Pavlo", ""]]}, {"id": "2107.06307", "submitter": "Qi Li", "authors": "Qi Li, Yue Wang, Yilun Wang, Hang Zhao", "title": "HDMapNet: An Online HD Map Construction and Evaluation Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-definition map (HD map) construction is a crucial problem for autonomous\ndriving. This problem typically involves collecting high-quality point clouds,\nfusing multiple point clouds of the same scene, annotating map elements, and\nupdating maps constantly. This pipeline, however, requires a vast amount of\nhuman efforts and resources which limits its scalability. Additionally,\ntraditional HD maps are coupled with centimeter-level accurate localization\nwhich is unreliable in many scenarios. In this paper, we argue that online map\nlearning, which dynamically constructs the HD maps based on local sensor\nobservations, is a more scalable way to provide semantic and geometry priors to\nself-driving vehicles than traditional pre-annotated HD maps. Meanwhile, we\nintroduce an online map learning method, titled HDMapNet. It encodes image\nfeatures from surrounding cameras and/or point clouds from LiDAR, and predicts\nvectorized map elements in the bird's-eye view. We benchmark HDMapNet on the\nnuScenes dataset and show that in all settings, it performs better than\nbaseline methods. Of note, our fusion-based HDMapNet outperforms existing\nmethods by more than 50% in all metrics. To accelerate future research, we\ndevelop customized metrics to evaluate map learning performance, including both\nsemantic-level and instance-level ones. By introducing this method and metrics,\nwe invite the community to study this novel map learning problem. We will\nrelease our code and evaluation kit to facilitate future development.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 18:06:46 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 01:54:14 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Li", "Qi", ""], ["Wang", "Yue", ""], ["Wang", "Yilun", ""], ["Zhao", "Hang", ""]]}, {"id": "2107.06325", "submitter": "Rajat Koner", "authors": "Rajat Koner, Hang Li, Marcel Hildebrandt, Deepan Das, Volker Tresp,\n  Stephan G\\\"unnemann", "title": "Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question\n  Answering", "comments": "arXiv admin note: text overlap with arXiv:2007.01072", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Question Answering (VQA) is concerned with answering free-form\nquestions about an image. Since it requires a deep semantic and linguistic\nunderstanding of the question and the ability to associate it with various\nobjects that are present in the image, it is an ambitious task and requires\nmulti-modal reasoning from both computer vision and natural language\nprocessing. We propose Graphhopper, a novel method that approaches the task by\nintegrating knowledge graph reasoning, computer vision, and natural language\nprocessing techniques. Concretely, our method is based on performing\ncontext-driven, sequential reasoning based on the scene entities and their\nsemantic and spatial relationships. As a first step, we derive a scene graph\nthat describes the objects in the image, as well as their attributes and their\nmutual relationships. Subsequently, a reinforcement learning agent is trained\nto autonomously navigate in a multi-hop manner over the extracted scene graph\nto generate reasoning paths, which are the basis for deriving answers. We\nconduct an experimental study on the challenging dataset GQA, based on both\nmanually curated and automatically generated scene graphs. Our results show\nthat we keep up with a human performance on manually curated scene graphs.\nMoreover, we find that Graphhopper outperforms another state-of-the-art scene\ngraph reasoning model on both manually curated and automatically generated\nscene graphs by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 18:33:04 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Koner", "Rajat", ""], ["Li", "Hang", ""], ["Hildebrandt", "Marcel", ""], ["Das", "Deepan", ""], ["Tresp", "Volker", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "2107.06351", "submitter": "Andrei Costin", "authors": "Tuomo Lahtinen, Hannu Turtiainen, Andrei Costin", "title": "BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image annotation and large annotated datasets are crucial parts within the\nComputer Vision and Artificial Intelligence fields.At the same time, it is\nwell-known and acknowledged by the research community that the image annotation\nprocess is challenging, time-consuming and hard to scale. Therefore, the\nresearchers and practitioners are always seeking ways to perform the\nannotations easier, faster, and at higher quality. Even though several widely\nused tools exist and the tools' landscape evolved considerably, most of the\ntools still require intricate technical setups and high levels of technical\nsavviness from its operators and crowdsource contributors.\n  In order to address such challenges, we develop and present BRIMA -- a\nflexible and open-source browser extension that allows BRowser-only IMage\nAnnotation at considerably lower overheads. Once added to the browser, it\ninstantly allows the user to annotate images easily and efficiently directly\nfrom the browser without any installation or setup on the client-side. It also\nfeatures cross-browser and cross-platform functionality thus presenting itself\nas a neat tool for researchers within the Computer Vision, Artificial\nIntelligence, and privacy-related fields.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:23:13 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lahtinen", "Tuomo", ""], ["Turtiainen", "Hannu", ""], ["Costin", "Andrei", ""]]}, {"id": "2107.06356", "submitter": "Anas Al Shaghouri Mr.", "authors": "Anas Al Shaghouri, Rami Alkhatib, Samir Berjaoui", "title": "Real-Time Pothole Detection Using Deep Learning", "comments": "10 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roads are connecting line between different places, and used daily. Roads'\nperiodic maintenance keeps them safe and functional. Detecting and reporting\nthe existence of potholes to responsible departments can help in eliminating\nthem. This study deployed and tested on different deep learning architecture to\ndetect potholes. The images used for training were collected by cellphone\nmounted on the windshield of the car, in addition to many images downloaded\nfrom the internet to increase the size and variability of the database. Second,\nvarious object detection algorithms are employed and compared to detect\npotholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.\nYOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%\nmean Average Precision (mAP). The speed of processing was 20 frame per second.\nThe system was able to detect potholes from a range on 100 meters away from the\ncamera. The system can increase the safety of drivers and improve the\nperformance of self-driving cars by detecting pothole time ahead.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:36:34 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shaghouri", "Anas Al", ""], ["Alkhatib", "Rami", ""], ["Berjaoui", "Samir", ""]]}, {"id": "2107.06360", "submitter": "Stanislav Lukyanenko", "authors": "Stanislav Lukyanenko, Won-Dong Jang, Donglai Wei, Robbert Struyven,\n  Yoon Kim, Brian Leahy, Helen Yang, Alexander Rush, Dalit Ben-Yosef, Daniel\n  Needleman and Hanspeter Pfister", "title": "Developmental Stage Classification of Embryos Using Two-Stream Neural\n  Network with Linear-Chain Conditional Random Field", "comments": "8.5 pages, to appear in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The developmental process of embryos follows a monotonic order. An embryo can\nprogressively cleave from one cell to multiple cells and finally transform to\nmorula and blastocyst. For time-lapse videos of embryos, most existing\ndevelopmental stage classification methods conduct per-frame predictions using\nan image frame at each time step. However, classification using only images\nsuffers from overlapping between cells and imbalance between stages. Temporal\ninformation can be valuable in addressing this problem by capturing movements\nbetween neighboring frames. In this work, we propose a two-stream model for\ndevelopmental stage classification. Unlike previous methods, our two-stream\nmodel accepts both temporal and image information. We develop a linear-chain\nconditional random field (CRF) on top of neural network features extracted from\nthe temporal and image streams to make use of both modalities. The linear-chain\nCRF formulation enables tractable training of global sequential models over\nmultiple frames while also making it possible to inject monotonic development\norder constraints into the learning process explicitly. We demonstrate our\nalgorithm on two time-lapse embryo video datasets: i) mouse and ii) human\nembryo datasets. Our method achieves 98.1 % and 80.6 % for mouse and human\nembryo stage classification, respectively. Our approach will enable more\nprofound clinical and biological studies and suggests a new direction for\ndevelopmental stage classification by utilizing temporal information.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:56:01 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lukyanenko", "Stanislav", ""], ["Jang", "Won-Dong", ""], ["Wei", "Donglai", ""], ["Struyven", "Robbert", ""], ["Kim", "Yoon", ""], ["Leahy", "Brian", ""], ["Yang", "Helen", ""], ["Rush", "Alexander", ""], ["Ben-Yosef", "Dalit", ""], ["Needleman", "Daniel", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2107.06383", "submitter": "Liunian Harold Li", "authors": "Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach,\n  Kai-Wei Chang, Zhewei Yao, Kurt Keutzer", "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Vision-and-Language (V&L) models rely on pre-trained visual\nencoders, using a relatively small set of manually-annotated data (as compared\nto web-crawled data), to perceive the visual world. However, it has been\nobserved that large-scale pretraining usually can result in better\ngeneralization performance, e.g., CLIP (Contrastive Language-Image\nPre-training), trained on a massive amount of image-caption pairs, has shown a\nstrong zero-shot capability on various vision tasks. To further study the\nadvantage brought by CLIP, we propose to use CLIP as the visual encoder in\nvarious V&L models in two typical scenarios: 1) plugging CLIP into\ntask-specific fine-tuning; 2) combining CLIP with V&L pre-training and\ntransferring to downstream tasks. We show that CLIP significantly outperforms\nwidely-used visual encoders trained with in-domain annotated data, such as\nBottomUp-TopDown. We achieve competitive or better results on diverse V&L\ntasks, while establishing new state-of-the-art results on Visual Question\nAnswering, Visual Entailment, and V&L Navigation tasks. We release our code at\nhttps://github.com/clip-vil/CLIP-ViL.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 20:48:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shen", "Sheng", ""], ["Li", "Liunian Harold", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""], ["Rohrbach", "Anna", ""], ["Chang", "Kai-Wei", ""], ["Yao", "Zhewei", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2107.06393", "submitter": "Tuan Anh Le", "authors": "Tuan Anh Le, Katherine M. Collins, Luke Hewitt, Kevin Ellis, Siddharth\n  N, Samuel J. Gershman, Joshua B. Tenenbaum", "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the\n  Discrete-Continuous Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling complex phenomena typically involves the use of both discrete and\ncontinuous variables. Such a setting applies across a wide range of problems,\nfrom identifying trends in time-series data to performing effective\ncompositional scene understanding in images. Here, we propose Hybrid Memoised\nWake-Sleep (HMWS), an algorithm for effective inference in such hybrid\ndiscrete-continuous models. Prior approaches to learning suffer as they need to\nperform repeated expensive inner-loop discrete inference. We build on a recent\napproach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by\nmemoising discrete variables, and extend it to allow for a principled and\neffective way to handle continuous variables by learning a separate recognition\nmodel used for importance-sampling based approximate inference and\nmarginalization. We evaluate HMWS in the GP-kernel learning and 3D scene\nunderstanding domains, and show that it outperforms current state-of-the-art\ninference methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 00:57:14 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Le", "Tuan Anh", ""], ["Collins", "Katherine M.", ""], ["Hewitt", "Luke", ""], ["Ellis", "Kevin", ""], ["N", "Siddharth", ""], ["Gershman", "Samuel J.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2107.06394", "submitter": "Sandip Roy", "authors": "Sandip Roy", "title": "Compressive Representations of Weather Scenes for Strategic Air Traffic\n  Flow Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terse representation of high-dimensional weather scene data is explored, in\nsupport of strategic air traffic flow management objectives. Specifically, we\nconsider whether aviation-relevant weather scenes are compressible, in the\nsense that each scene admits a possibly-different sparse representation in a\nbasis of interest. Here, compression of weather scenes extracted from METAR\ndata (including temperature, flight categories, and visibility profiles for the\ncontiguous United States) is examined, for the graph-spectral basis. The scenes\nare found to be compressible, with 75-95% of the scene content captured using\n0.5-4% of the basis vectors. Further, the dominant basis vectors for each scene\nare seen to identify time-varying spatial characteristics of the weather, and\nreconstruction from the compressed representation is demonstrated. Finally,\npotential uses of the compressive representations in strategic TFM design are\nbriefly scoped.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:16:28 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Roy", "Sandip", ""]]}, {"id": "2107.06397", "submitter": "Mitchell Doughty", "authors": "Mitchell Doughty, Karan Singh, and Nilesh R. Ghugre", "title": "SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based\n  Augmented Reality for Surgical Guidance", "comments": "Accepted at MICCAI 2021; 11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SurgeonAssist-Net: a lightweight framework making\naction-and-workflow-driven virtual assistance, for a set of predefined surgical\ntasks, accessible to commercially available optical see-through head-mounted\ndisplays (OST-HMDs). On a widely used benchmark dataset for laparoscopic\nsurgical workflow, our implementation competes with state-of-the-art approaches\nin prediction accuracy for automated task recognition, and yet requires 7.4x\nfewer parameters, 10.2x fewer floating point operations per second (FLOPS), is\n7.0x faster for inference on a CPU, and is capable of near real-time\nperformance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use\nof an efficient convolutional neural network (CNN) backbone to extract\ndiscriminative features from image data, and a low-parameter recurrent neural\nnetwork (RNN) architecture to learn long-term temporal dependencies. To\ndemonstrate the feasibility of our approach for inference on the HoloLens 2 we\ncreated a sample dataset that included video of several surgical tasks recorded\nfrom a user-centric point-of-view. After training, we deployed our model and\ncataloged its performance in an online simulated surgical scenario for the\nprediction of the current surgical task. The utility of our approach is\nexplored in the discussion of several relevant clinical use-cases. Our code is\npublicly available at https://github.com/doughtmw/surgeon-assist-net.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 21:12:34 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Doughty", "Mitchell", ""], ["Singh", "Karan", ""], ["Ghugre", "Nilesh R.", ""]]}, {"id": "2107.06409", "submitter": "Xavier Boix", "authors": "Vanessa D'Amario, Sanjana Srivastava, Tomotake Sasaki, Xavier Boix", "title": "The Foes of Neural Network's Data Efficiency Among Unnecessary Input\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets often contain input dimensions that are unnecessary to predict the\noutput label, e.g. background in object recognition, which lead to more\ntrainable parameters. Deep Neural Networks (DNNs) are robust to increasing the\nnumber of parameters in the hidden layers, but it is unclear whether this holds\ntrue for the input layer. In this letter, we investigate the impact of\nunnecessary input dimensions on a central issue of DNNs: their data efficiency,\nie. the amount of examples needed to achieve certain generalization\nperformance. Our results show that unnecessary input dimensions that are\ntask-unrelated substantially degrade data efficiency. This highlights the need\nfor mechanisms that remove {task-unrelated} dimensions to enable data\nefficiency gains.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 21:52:02 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["D'Amario", "Vanessa", ""], ["Srivastava", "Sanjana", ""], ["Sasaki", "Tomotake", ""], ["Boix", "Xavier", ""]]}, {"id": "2107.06442", "submitter": "Jinpeng Li", "authors": "Baolian Qi, Gangming Zhao, Xin Wei, Chaowei Fang, Chengwei Pan,\n  Jinpeng Li, Huiguang He, and Licheng Jiao", "title": "GREN: Graph-Regularized Embedding Network for Weakly-Supervised Disease\n  Localization in X-ray images", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating diseases in chest X-ray images with few careful annotations saves\nlarge human effort. Recent works approached this task with innovative\nweakly-supervised algorithms such as multi-instance learning (MIL) and class\nactivation maps (CAM), however, these methods often yield inaccurate or\nincomplete regions. One of the reasons is the neglection of the pathological\nimplications hidden in the relationship across anatomical regions within each\nimage and the relationship across images. In this paper, we argue that the\ncross-region and cross-image relationship, as contextual and compensating\ninformation, is vital to obtain more consistent and integral regions. To model\nthe relationship, we propose the Graph Regularized Embedding Network (GREN),\nwhich leverages the intra-image and inter-image information to locate diseases\non chest X-ray images. GREN uses a pre-trained U-Net to segment the lung lobes,\nand then models the intra-image relationship between the lung lobes using an\nintra-image graph to compare different regions. Meanwhile, the relationship\nbetween in-batch images is modeled by an inter-image graph to compare multiple\nimages. This process mimics the training and decision-making process of a\nradiologist: comparing multiple regions and images for diagnosis. In order for\nthe deep embedding layers of the neural network to retain structural\ninformation (important in the localization task), we use the Hash coding and\nHamming distance to compute the graphs, which are used as regularizers to\nfacilitate training. By means of this, our approach achieves the\nstate-of-the-art result on NIH chest X-ray dataset for weakly-supervised\ndisease localization. Our codes are accessible online.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 01:27:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Qi", "Baolian", ""], ["Zhao", "Gangming", ""], ["Wei", "Xin", ""], ["Fang", "Chaowei", ""], ["Pan", "Chengwei", ""], ["Li", "Jinpeng", ""], ["He", "Huiguang", ""], ["Jiao", "Licheng", ""]]}, {"id": "2107.06445", "submitter": "Meiqi Pei", "authors": "Meiqi Pei", "title": "MSFNet:Multi-scale features network for monocular depth estimation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, monocular depth estimation is applied to understand the\nsurrounding 3D environment and has made great progress. However, there is an\nill-posed problem on how to gain depth information directly from a single\nimage. With the rapid development of deep learning, this problem is possible to\nbe solved. Although more and more approaches are proposed one after another,\nmost of existing methods inevitably lost details due to continuous downsampling\nwhen mapping from RGB space to depth space. To the end, we design a Multi-scale\nFeatures Network (MSFNet), which consists of Enhanced Diverse Attention (EDA)\nmodule and Upsample-Stage Fusion (USF) module. The EDA module employs the\nspatial attention method to learn significant spatial information, while USF\nmodule complements low-level detail information with high-level semantic\ninformation from the perspective of multi-scale feature fusion to improve the\npredicted effect. In addition, since the simple samples are always trained to a\nbetter effect first, the hard samples are difficult to converge. Therefore, we\ndesign a batch-loss to assign large loss factors to the harder samples in a\nbatch. Experiments on NYU-Depth V2 dataset and KITTI dataset demonstrate that\nour proposed approach is more competitive with the state-of-the-art methods in\nboth qualitative and quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 01:38:29 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Pei", "Meiqi", ""]]}, {"id": "2107.06449", "submitter": "Hengtao Guo", "authors": "Hengtao Guo, Xuanang Xu, Sheng Xu, Bradford J. Wood, Pingkun Yan", "title": "End-to-end Ultrasound Frame to Volume Registration", "comments": "Early accepted by MICCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fusing intra-operative 2D transrectal ultrasound (TRUS) image with\npre-operative 3D magnetic resonance (MR) volume to guide prostate biopsy can\nsignificantly increase the yield. However, such a multimodal 2D/3D registration\nproblem is a very challenging task. In this paper, we propose an end-to-end\nframe-to-volume registration network (FVR-Net), which can efficiently bridge\nthe previous research gaps by aligning a 2D TRUS frame with a 3D TRUS volume\nwithout requiring hardware tracking. The proposed FVR-Net utilizes a\ndual-branch feature extraction module to extract the information from TRUS\nframe and volume to estimate transformation parameters. We also introduce a\ndifferentiable 2D slice sampling module which allows gradients backpropagating\nfrom an unsupervised image similarity loss for content correspondence learning.\nOur model shows superior efficiency for real-time interventional guidance with\nhighly competitive registration accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 01:59:42 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Guo", "Hengtao", ""], ["Xu", "Xuanang", ""], ["Xu", "Sheng", ""], ["Wood", "Bradford J.", ""], ["Yan", "Pingkun", ""]]}, {"id": "2107.06456", "submitter": "Eunjung Lee", "authors": "Duhun Hwang, Eunjung Lee, Wonjong Rhee", "title": "AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense", "comments": null, "journal-ref": "ICML 2021 Workshop on Adversarial Machine Learning", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an AID-purifier that can boost the robustness of\nadversarially-trained networks by purifying their inputs. AID-purifier is an\nauxiliary network that works as an add-on to an already trained main\nclassifier. To keep it computationally light, it is trained as a discriminator\nwith a binary cross-entropy loss. To obtain additionally useful information\nfrom the adversarial examples, the architecture design is closely related to\ninformation maximization principles where two layers of the main classification\nnetwork are piped to the auxiliary network. To assist the iterative\noptimization procedure of purification, the auxiliary network is trained with\nAVmixup. AID-purifier can be used together with other purifiers such as\nPixelDefend for an extra enhancement. The overall results indicate that the\nbest performing adversarially-trained networks can be enhanced by the best\nperforming purification networks, where AID-purifier is a competitive candidate\nthat is light and robust.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 02:39:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hwang", "Duhun", ""], ["Lee", "Eunjung", ""], ["Rhee", "Wonjong", ""]]}, {"id": "2107.06463", "submitter": "Haisheng Fu", "authors": "Haisheng Fu and Feng Liang and Jianping Lin and Bing Li and Mohammad\n  Akbari and Jie Liang and Guohe Zhang and Dong Liu and Chengjie Tu and\n  Jingning Han", "title": "Learned Image Compression with Discretized Gaussian-Laplacian-Logistic\n  Mixture Model and Concatenated Residual Modules", "comments": "Submitted to IEEE Transactions On Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently deep learning-based image compression methods have achieved\nsignificant achievements and gradually outperformed traditional approaches\nincluding the latest standard Versatile Video Coding (VVC) in both PSNR and\nMS-SSIM metrics. Two key components of learned image compression frameworks are\nthe entropy model of the latent representations and the encoding/decoding\nnetwork architectures. Various models have been proposed, such as\nautoregressive, softmax, logistic mixture, Gaussian mixture, and Laplacian.\nExisting schemes only use one of these models. However, due to the vast\ndiversity of images, it is not optimal to use one model for all images, even\ndifferent regions of one image. In this paper, we propose a more flexible\ndiscretized Gaussian-Laplacian-Logistic mixture model (GLLMM) for the latent\nrepresentations, which can adapt to different contents in different images and\ndifferent regions of one image more accurately. Besides, in the\nencoding/decoding network design part, we propose a concatenated residual\nblocks (CRB), where multiple residual blocks are serially connected with\nadditional shortcut connections. The CRB can improve the learning ability of\nthe network, which can further improve the compression performance.\nExperimental results using the Kodak and Tecnick datasets show that the\nproposed scheme outperforms all the state-of-the-art learning-based methods and\nexisting compression standards including VVC intra coding (4:4:4 and 4:2:0) in\nterms of the PSNR and MS-SSIM. The project page is at\n\\url{https://github.com/fengyurenpingsheng/Learned-image-compression-with-GLLMM}\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 02:54:22 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 10:12:58 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fu", "Haisheng", ""], ["Liang", "Feng", ""], ["Lin", "Jianping", ""], ["Li", "Bing", ""], ["Akbari", "Mohammad", ""], ["Liang", "Jie", ""], ["Zhang", "Guohe", ""], ["Liu", "Dong", ""], ["Tu", "Chengjie", ""], ["Han", "Jingning", ""]]}, {"id": "2107.06475", "submitter": "Patryk Orzechowski", "authors": "Patryk Orzechowski and Jason H. Moore", "title": "Generative and reproducible benchmarks for comprehensive evaluation of\n  machine learning classifiers", "comments": "12 pages, 3 figures with subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the strengths and weaknesses of machine learning (ML)\nalgorithms is crucial for determine their scope of application. Here, we\nintroduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of\nsynthetic datasets for comprehensive, reproducible, and interpretable\nbenchmarking of machine learning algorithms for classification of binary\noutcomes. The DIGEN resource consists of 40 mathematical functions which map\ncontinuous features to discrete endpoints for creating synthetic datasets.\nThese 40 functions were discovered using a heuristic algorithm designed to\nmaximize the diversity of performance among multiple popular machine learning\nalgorithms thus providing a useful test suite for evaluating and comparing new\nmethods. Access to the generative functions facilitates understanding of why a\nmethod performs poorly compared to other algorithms thus providing ideas for\nimprovement. The resource with extensive documentation and analyses is\nopen-source and available on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 03:58:02 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Orzechowski", "Patryk", ""], ["Moore", "Jason H.", ""]]}, {"id": "2107.06481", "submitter": "Bharadwaj Manda", "authors": "Bharadwaj Manda, Pranjal Bhaskare, Ramanathan Muthuganapathy", "title": "A Convolutional Neural Network Approach to the Classification of\n  Engineering Models", "comments": null, "journal-ref": "in IEEE Access, vol. 9, pp. 22711-22723, 2021", "doi": "10.1109/ACCESS.2021.3055826", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a deep learning approach for the classification of\nEngineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to\nthe availability of large annotated datasets and also enough computational\npower in the form of GPUs, many deep learning-based solutions for object\nclassification have been proposed of late, especially in the domain of images\nand graphical models. Nevertheless, very few solutions have been proposed for\nthe task of functional classification of CAD models. Hence, for this research,\nCAD models have been collected from Engineering Shape Benchmark (ESB), National\nDesign Repository (NDR) and augmented with newer models created using a\nmodelling software to form a dataset - 'CADNET'. It is proposed to use a\nresidual network architecture for CADNET, inspired by the popular ResNet. A\nweighted Light Field Descriptor (LFD) scheme is chosen as the method of feature\nextraction, and the generated images are fed as inputs to the CNN. The problem\nof class imbalance in the dataset is addressed using a class weights approach.\nExperiments have been conducted with other signatures such as geodesic distance\netc. using deep networks as well as other network architectures on the CADNET.\nThe LFD-based CNN approach using the proposed network architecture, along with\ngradient boosting yielded the best classification accuracy on CADNET.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 04:33:50 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Manda", "Bharadwaj", ""], ["Bhaskare", "Pranjal", ""], ["Muthuganapathy", "Ramanathan", ""]]}, {"id": "2107.06492", "submitter": "Hoang Trinh Man", "authors": "Trinh Man Hoang, Jinjia Zhou", "title": "RCLC: ROI-based joint conventional and learning video compression", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 leads to the high demand for remote interactive systems ever seen.\nOne of the key elements of these systems is video streaming, which requires a\nvery high network bandwidth due to its specific real-time demand, especially\nwith high-resolution video. Existing video compression methods are struggling\nin the trade-off between video quality and the speed requirement. Addressed\nthat the background information rarely changes in most remote meeting cases, we\nintroduce a Region-Of-Interests (ROI) based video compression framework (named\nRCLC) that leverages the cutting-edge learning-based and conventional\ntechnologies. In RCLC, each coming frame is marked as a background-updating\n(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the\nBU frame is compressed with low-quality and high-compression, while the ROI\nfrom RU-frame is compressed with high-quality and low-compression. The\nlearning-based methods are applied to detect the ROI, blend background-ROI, and\nenhance video quality. The experimental results show that our RCLC can reduce\nup to 32.55\\% BD-rate for the ROI region compared to H.265 video codec under a\nsimilar compression time with 1080p resolution.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:38:37 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hoang", "Trinh Man", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2107.06501", "submitter": "Qing Guo", "authors": "Yihao Huang and Qing Guo and Felix Juefei-Xu and Lei Ma and Weikai\n  Miao and Yang Liu and Geguang Pu", "title": "AdvFilter: Predictive Perturbation-aware Filtering against Adversarial\n  Attack via Multi-domain Learning", "comments": "This work has been accepted to ACM-MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level representation-guided pixel denoising and adversarial training are\nindependent solutions to enhance the robustness of CNNs against adversarial\nattacks by pre-processing input data and re-training models, respectively. Most\nrecently, adversarial training techniques have been widely studied and improved\nwhile the pixel denoising-based method is getting less attractive. However, it\nis still questionable whether there exists a more advanced pixel\ndenoising-based method and whether the combination of the two solutions\nbenefits each other. To this end, we first comprehensively investigate two\nkinds of pixel denoising methods for adversarial robustness enhancement (i.e.,\nexisting additive-based and unexplored filtering-based methods) under the loss\nfunctions of image-level and semantic-level restorations, respectively, showing\nthat pixel-wise filtering can obtain much higher image quality (e.g., higher\nPSNR) as well as higher robustness (e.g., higher accuracy on adversarial\nexamples) than existing pixel-wise additive-based method. However, we also\nobserve that the robustness results of the filtering-based method rely on the\nperturbation amplitude of adversarial examples used for training. To address\nthis problem, we propose predictive perturbation-aware pixel-wise filtering,\nwhere dual-perturbation filtering and an uncertainty-aware fusion module are\ndesigned and employed to automatically perceive the perturbation amplitude\nduring the training and testing process. The proposed method is termed as\nAdvFilter. Moreover, we combine adversarial pixel denoising methods with three\nadversarial training-based methods, hinting that considering data and models\njointly is able to achieve more robust CNNs. The experiments conduct on\nNeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over\nenhancing CNNs' robustness, high generalization to different models, and noise\nlevels.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 06:08:48 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Huang", "Yihao", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Ma", "Lei", ""], ["Miao", "Weikai", ""], ["Liu", "Yang", ""], ["Pu", "Geguang", ""]]}, {"id": "2107.06505", "submitter": "Anqi Pang", "authors": "Anqi Pang, Xin Chen, Haimin Luo, Minye Wu, Jingyi Yu, Lan Xu", "title": "Few-shot Neural Human Performance Rendering from Sparse RGBD Videos", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent neural rendering approaches for human activities achieve remarkable\nview synthesis results, but still rely on dense input views or dense training\nwith all the capture frames, leading to deployment difficulty and inefficient\ntraining overload. However, existing advances will be ill-posed if the input is\nboth spatially and temporally sparse. To fill this gap, in this paper we\npropose a few-shot neural human rendering approach (FNHR) from only sparse RGBD\ninputs, which exploits the temporal and spatial redundancy to generate\nphoto-realistic free-view output of human activities. Our FNHR is trained only\non the key-frames which expand the motion manifold in the input sequences. We\nintroduce a two-branch neural blending to combine the neural point render and\nclassical graphics texturing pipeline, which integrates reliable observations\nover sparse key-frames. Furthermore, we adopt a patch-based adversarial\ntraining process to make use of the local redundancy and avoids over-fitting to\nthe key-frames, which generates fine-detailed rendering results. Extensive\nexperiments demonstrate the effectiveness of our approach to generate\nhigh-quality free view-point results for challenging human performances under\nthe sparse setting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 06:28:16 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Pang", "Anqi", ""], ["Chen", "Xin", ""], ["Luo", "Haimin", ""], ["Wu", "Minye", ""], ["Yu", "Jingyi", ""], ["Xu", "Lan", ""]]}, {"id": "2107.06530", "submitter": "Suneung Kim", "authors": "Suneung-Kim, Seong-Whan Lee", "title": "Detection of Abnormal Behavior with Self-Supervised Gaze Estimation", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent outbreak of COVID-19, many classes, exams, and meetings\nhave been conducted non-face-to-face. However, the foundation for video\nconferencing solutions is still insufficient. So this technology has become an\nimportant issue. In particular, these technologies are essential for\nnon-face-to-face testing, and technology dissemination is urgent. In this\npaper, we present a single video conferencing solution using gaze estimation in\npreparation for these problems. Gaze is an important cue for the tasks such as\nanalysis of human behavior. Hence, numerous studies have been proposed to solve\ngaze estimation using deep learning, which is one of the most prominent methods\nup to date. We use these gaze estimation methods to detect abnormal behavior of\nvideo conferencing participants. Our contribution is as follows. i) We find and\napply the optimal network for the gaze estimation method and apply a\nself-supervised method to improve accuracy. ii) For anomaly detection, we\npresent a new dataset that aggregates the values of a new gaze, head pose, etc.\niii) We train newly created data on Multi Layer Perceptron (MLP) models to\ndetect anomaly behavior based on deep learning. We demonstrate the robustness\nof our method through experiments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 07:58:59 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Suneung-Kim", "", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.06532", "submitter": "Yong Li", "authors": "Yong Li, Lingjie Lao, Zhen Cui, Shiguang Shan, Jian Yang", "title": "Graph Jigsaw Learning for Cartoon Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cartoon face recognition is challenging as they typically have smooth color\nregions and emphasized edges, the key to recognize cartoon faces is to\nprecisely perceive their sparse and critical shape patterns. However, it is\nquite difficult to learn a shape-oriented representation for cartoon face\nrecognition with convolutional neural networks (CNNs). To mitigate this issue,\nwe propose the GraphJigsaw that constructs jigsaw puzzles at various stages in\nthe classification network and solves the puzzles with the graph convolutional\nnetwork (GCN) in a progressive manner. Solving the puzzles requires the model\nto spot the shape patterns of the cartoon faces as the texture information is\nquite limited. The key idea of GraphJigsaw is constructing a jigsaw puzzle by\nrandomly shuffling the intermediate convolutional feature maps in the spatial\ndimension and exploiting the GCN to reason and recover the correct layout of\nthe jigsaw fragments in a self-supervised manner. The proposed GraphJigsaw\navoids training the classification model with the deconstructed images that\nwould introduce noisy patterns and are harmful for the final classification.\nSpecially, GraphJigsaw can be incorporated at various stages in a top-down\nmanner within the classification model, which facilitates propagating the\nlearned shape patterns gradually. GraphJigsaw does not rely on any extra manual\nannotation during the training process and incorporates no extra computation\nburden at inference time. Both quantitative and qualitative experimental\nresults have verified the feasibility of our proposed GraphJigsaw, which\nconsistently outperforms other face recognition or jigsaw-based methods on two\npopular cartoon face datasets with considerable improvements.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:01:06 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Li", "Yong", ""], ["Lao", "Lingjie", ""], ["Cui", "Zhen", ""], ["Shan", "Shiguang", ""], ["Yang", "Jian", ""]]}, {"id": "2107.06536", "submitter": "Meng Xu", "authors": "Meng Xu, Zhihao Wang, Jiasong Zhu, Xiuping Jia, Sen Jia", "title": "Multi-Attention Generative Adversarial Network for Remote Sensing Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image super-resolution (SR) methods can generate remote sensing images with\nhigh spatial resolution without increasing the cost, thereby providing a\nfeasible way to acquire high-resolution remote sensing images, which are\ndifficult to obtain due to the high cost of acquisition equipment and complex\nweather. Clearly, image super-resolution is a severe ill-posed problem.\nFortunately, with the development of deep learning, the powerful fitting\nability of deep neural networks has solved this problem to some extent. In this\npaper, we propose a network based on the generative adversarial network (GAN)\nto generate high resolution remote sensing images, named the multi-attention\ngenerative adversarial network (MA-GAN). We first designed a GAN-based\nframework for the image SR task. The core to accomplishing the SR task is the\nimage generator with post-upsampling that we designed. The main body of the\ngenerator contains two blocks; one is the pyramidal convolution in the\nresidual-dense block (PCRDB), and the other is the attention-based upsample\n(AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB\nblock is a module that combines multi-scale convolution and channel attention\nto automatically learn and adjust the scaling of the residuals for better\nresults. The AUP block is a module that combines pixel attention (PA) to\nperform arbitrary multiples of upsampling. These two blocks work together to\nhelp generate better quality images. For the loss function, we design a loss\nfunction based on pixel loss and introduce both adversarial loss and feature\nloss to guide the generator learning. We have compared our method with several\nstate-of-the-art methods on a remote sensing scene image dataset, and the\nexperimental results consistently demonstrate the effectiveness of the proposed\nMA-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:06:19 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Xu", "Meng", ""], ["Wang", "Zhihao", ""], ["Zhu", "Jiasong", ""], ["Jia", "Xiuping", ""], ["Jia", "Sen", ""]]}, {"id": "2107.06538", "submitter": "Xinda Liu", "authors": "Xinda Liu, Lili Wang, Xiaoguang Han", "title": "Transformer with Peak Suppression and Knowledge Guidance for\n  Fine-grained Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image recognition is challenging because discriminative clues\nare usually fragmented, whether from a single image or multiple images. Despite\ntheir significant improvements, most existing methods still focus on the most\ndiscriminative parts from a single image, ignoring informative details in other\nregions and lacking consideration of clues from other associated images. In\nthis paper, we analyze the difficulties of fine-grained image recognition from\na new perspective and propose a transformer architecture with the peak\nsuppression module and knowledge guidance module, which respects the\ndiversification of discriminative features in a single image and the\naggregation of discriminative clues among multiple images. Specifically, the\npeak suppression module first utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention\nresponse generated by the transformer encoder. This module penalizes the\nattention to the most discriminative parts in the feature learning process,\ntherefore, enhancing the information exploitation of the neglected regions. The\nknowledge guidance module compares the image-based representation generated\nfrom the peak suppression module with the learnable knowledge embedding set to\nobtain the knowledge response coefficients. Afterwards, it formalizes the\nknowledge learning as a classification problem using response coefficients as\nthe classification scores. Knowledge embeddings and image-based representations\nare updated during training so that the knowledge embedding includes\ndiscriminative clues for different images. Finally, we incorporate the acquired\nknowledge embeddings into the image-based representations as comprehensive\nrepresentations, leading to significantly higher performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:07:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Xinda", ""], ["Wang", "Lili", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2107.06552", "submitter": "Young Eun Kim", "authors": "Young Eun Kim and Seong-Whan Lee", "title": "Domain Generalization with Pseudo-Domain Label for Face Anti-Spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) plays an important role in protecting face\nrecognition systems from face representation attacks. Many recent studies in\nFAS have approached this problem with domain generalization technique. Domain\ngeneralization aims to increase generalization performance to better detect\nvarious types of attacks and unseen attacks. However, previous studies in this\narea have defined each domain simply as an anti-spoofing datasets and focused\non developing learning techniques. In this paper, we proposed a method that\nenables network to judge its domain by itself with the clustered convolutional\nfeature statistics from intermediate layers of the network, without labeling\ndomains as datasets. We obtained pseudo-domain labels by not only using the\nnetwork extracting features, but also using depth estimators, which were\npreviously used only as an auxiliary task in FAS. In our experiments, we\ntrained with three datasets and evaluated the performance with the remaining\none dataset to demonstrate the effectiveness of the proposed method by\nconducting a total of four sets of experiments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:35:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kim", "Young Eun", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.06563", "submitter": "Farah Shamout", "authors": "Nasir Hayat, Hazem Lashen, Farah E. Shamout", "title": "Multi-Label Generalized Zero Shot Learning for the Classification of\n  Disease in Chest Radiographs", "comments": "Accepted to the Machine Learning for Healthcare Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep neural networks in chest X-ray (CXR) diagnosis,\nsupervised learning only allows the prediction of disease classes that were\nseen during training. At inference, these networks cannot predict an unseen\ndisease class. Incorporating a new class requires the collection of labeled\ndata, which is not a trivial task, especially for less frequently-occurring\ndiseases. As a result, it becomes inconceivable to build a model that can\ndiagnose all possible disease classes. Here, we propose a multi-label\ngeneralized zero shot learning (CXR-ML-GZSL) network that can simultaneously\npredict multiple seen and unseen diseases in CXR images. Given an input image,\nCXR-ML-GZSL learns a visual representation guided by the input's corresponding\nsemantics extracted from a rich medical text corpus. Towards this ambitious\ngoal, we propose to map both visual and semantic modalities to a latent feature\nspace using a novel learning objective. The objective ensures that (i) the most\nrelevant labels for the query image are ranked higher than irrelevant labels,\n(ii) the network learns a visual representation that is aligned with its\nsemantics in the latent feature space, and (iii) the mapped semantics preserve\ntheir original inter-class representation. The network is end-to-end trainable\nand requires no independent pre-training for the offline feature extractor.\nExperiments on the NIH Chest X-ray dataset show that our network outperforms\ntwo strong baselines in terms of recall, precision, f1 score, and area under\nthe receiver operating characteristic curve. Our code is publicly available at:\nhttps://github.com/nyuad-cai/CXR-ML-GZSL.git\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 09:04:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hayat", "Nasir", ""], ["Lashen", "Hazem", ""], ["Shamout", "Farah E.", ""]]}, {"id": "2107.06564", "submitter": "Jie Xu", "authors": "Jie Xu, Xingyu Chen, Xuguang Lan and Nanning Zheng", "title": "Probabilistic Human Motion Prediction via A Bayesian Neural Network", "comments": "Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is an important and challenging topic that has\npromising prospects in efficient and safe human-robot-interaction systems.\nCurrently, the majority of the human motion prediction algorithms are based on\ndeterministic models, which may lead to risky decisions for robots. To solve\nthis problem, we propose a probabilistic model for human motion prediction in\nthis paper. The key idea of our approach is to extend the conventional\ndeterministic motion prediction neural network to a Bayesian one. On one hand,\nour model could generate several future motions when given an observed motion\nsequence. On the other hand, by calculating the Epistemic Uncertainty and the\nHeteroscedastic Aleatoric Uncertainty, our model could tell the robot if the\nobservation has been seen before and also give the optimal result among all\npossible predictions. We extensively validate our approach on a large scale\nbenchmark dataset Human3.6m. The experiments show that our approach performs\nbetter than deterministic methods. We further evaluate our approach in a\nHuman-Robot-Interaction (HRI) scenario. The experimental results show that our\napproach makes the interaction more efficient and safer.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 09:05:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Xu", "Jie", ""], ["Chen", "Xingyu", ""], ["Lan", "Xuguang", ""], ["Zheng", "Nanning", ""]]}, {"id": "2107.06618", "submitter": "Daniel C. Castro", "authors": "Shruthi Bannur, Ozan Oktay, Melanie Bernhardt, Anton Schwaighofer,\n  Rajesh Jena, Besmira Nushi, Sharan Wadhwani, Aditya Nori, Kal Natarajan,\n  Shazad Ashraf, Javier Alvarez-Valle, Daniel C. Castro", "title": "Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs", "comments": "Presented at ICML 2021 Workshop on Interpretable Machine Learning in\n  Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest radiography has been a recommended procedure for patient triaging and\nresource management in intensive care units (ICUs) throughout the COVID-19\npandemic. The machine learning efforts to augment this workflow have been long\nchallenged due to deficiencies in reporting, model evaluation, and failure mode\nanalysis. To address some of those shortcomings, we model radiological features\nwith a human-interpretable class hierarchy that aligns with the radiological\ndecision process. Also, we propose the use of a data-driven error analysis\nmethodology to uncover the blind spots of our model, providing further\ntransparency on its clinical utility. For example, our experiments show that\nmodel failures highly correlate with ICU imaging conditions and with the\ninherent difficulty in distinguishing certain types of radiological features.\nAlso, our hierarchical interpretation and analysis facilitates the comparison\nwith respect to radiologists' findings and inter-variability, which in return\nhelps us to better assess the clinical applicability of models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 11:37:28 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bannur", "Shruthi", ""], ["Oktay", "Ozan", ""], ["Bernhardt", "Melanie", ""], ["Schwaighofer", "Anton", ""], ["Jena", "Rajesh", ""], ["Nushi", "Besmira", ""], ["Wadhwani", "Sharan", ""], ["Nori", "Aditya", ""], ["Natarajan", "Kal", ""], ["Ashraf", "Shazad", ""], ["Alvarez-Valle", "Javier", ""], ["Castro", "Daniel C.", ""]]}, {"id": "2107.06652", "submitter": "Rhydian Windsor", "authors": "Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman", "title": "Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging", "comments": "Accepted as a full paper to MICCAI 2021. Code will be made publicly\n  available before September 27th 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the use of self-supervised deep learning in medical\nimaging in cases where two scan modalities are available for the same subject.\nSpecifically, we use a large publicly-available dataset of over 20,000 subjects\nfrom the UK Biobank with both whole body Dixon technique magnetic resonance\n(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three\ncontributions: (i) We introduce a multi-modal image-matching contrastive\nframework, that is able to learn to match different-modality scans of the same\nsubject with high accuracy. (ii) Without any adaption, we show that the\ncorrespondences learnt during this contrastive training step can be used to\nperform automatic cross-modal scan registration in a completely unsupervised\nmanner. (iii) Finally, we use these registrations to transfer segmentation maps\nfrom the DXA scans to the MR scans where they are used to train a network to\nsegment anatomical regions without requiring ground-truth MR examples. To aid\nfurther research, our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:35:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Windsor", "Rhydian", ""], ["Jamaludin", "Amir", ""], ["Kadir", "Timor", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2107.06681", "submitter": "Boyun Li", "authors": "Boyun Li, Yijie Lin, Xiao Liu, Peng Hu, Jiancheng Lv and Xi Peng", "title": "Unsupervised Neural Rendering for Image Hazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hazing aims to render a hazy image from a given clean one, which could\nbe applied to a variety of practical applications such as gaming, filming,\nphotographic filtering, and image dehazing. To generate plausible haze, we\nstudy two less-touched but challenging problems in hazy image rendering,\nnamely, i) how to estimate the transmission map from a single image without\nauxiliary information, and ii) how to adaptively learn the airlight from\nexemplars, i.e., unpaired real hazy images. To this end, we propose a neural\nrendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN\nis a knowledge-driven neural network which estimates the transmission map by\nleveraging a new prior, i.e., there exists the structure similarity (e.g.,\ncontour and luminance) between the transmission map and the input clean image.\nTo adaptively learn the airlight, we build a neural module based on another new\nprior, i.e., the rendered hazy image and the exemplar are similar in the\nairlight distribution. To the best of our knowledge, this could be the first\nattempt to deeply rendering hazy images in an unsupervised fashion. Comparing\nwith existing haze generation methods, HazeGEN renders the hazy images in an\nunsupervised, learnable, and controllable manner, thus avoiding the\nlabor-intensive efforts in paired data collection and the domain-shift issue in\nhaze generation. Extensive experiments show the promising performance of our\nmethod comparing with some baselines in both qualitative and quantitative\ncomparisons. The code will be released on GitHub after acceptance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:15:14 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Li", "Boyun", ""], ["Lin", "Yijie", ""], ["Liu", "Xiao", ""], ["Hu", "Peng", ""], ["Lv", "Jiancheng", ""], ["Peng", "Xi", ""]]}, {"id": "2107.06707", "submitter": "Ning Ma", "authors": "Ning Ma, Jiajun Bu, Zhen Zhang, Sheng Zhou", "title": "Uncertainty-Guided Mixup for Semi-Supervised Domain Adaptation without\n  Source Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Present domain adaptation methods usually perform explicit representation\nalignment by simultaneously accessing the source data and target data. However,\nthe source data are not always available due to the privacy preserving\nconsideration or bandwidth limitation. Source-free domain adaptation aims to\nsolve the above problem by performing domain adaptation without accessing the\nsource data. The adaptation paradigm is receiving more and more attention in\nrecent years, and multiple works have been proposed for unsupervised\nsource-free domain adaptation. However, without utilizing any supervised signal\nand source data at the adaptation stage, the optimization of the target model\nis unstable and fragile. To alleviate the problem, we focus on semi-supervised\ndomain adaptation under source-free setting. More specifically, we propose\nuncertainty-guided Mixup to reduce the representation's intra-domain\ndiscrepancy and perform inter-domain alignment without directly accessing the\nsource data. Finally, we conduct extensive semi-supervised domain adaptation\nexperiments on various datasets. Our method outperforms the recent\nsemi-supervised baselines and the unsupervised variant also achieves\ncompetitive performance. The experiment codes will be released in the future.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:54:02 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ma", "Ning", ""], ["Bu", "Jiajun", ""], ["Zhang", "Zhen", ""], ["Zhou", "Sheng", ""]]}, {"id": "2107.06709", "submitter": "Patrick Mangat", "authors": "Laurenz Reichardt, Patrick Mangat, Oliver Wasenm\\\"uller", "title": "DVMN: Dense Validity Mask Network for Depth Completion", "comments": "This paper has been accepted at IEEE Intelligent Transportation\n  Systems Conference (ITSC), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR depth maps provide environmental guidance in a variety of applications.\nHowever, such depth maps are typically sparse and insufficient for complex\ntasks such as autonomous navigation. State of the art methods use image guided\nneural networks for dense depth completion. We develop a guided convolutional\nneural network focusing on gathering dense and valid information from sparse\ndepth maps. To this end, we introduce a novel layer with spatially variant and\ncontent-depended dilation to include additional data from sparse input.\nFurthermore, we propose a sparsity invariant residual bottleneck block. We\nevaluate our Dense Validity Mask Network (DVMN) on the KITTI depth completion\nbenchmark and achieve state of the art results. At the time of submission, our\nnetwork is the leading method using sparsity invariant convolution.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:57:44 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Reichardt", "Laurenz", ""], ["Mangat", "Patrick", ""], ["Wasenm\u00fcller", "Oliver", ""]]}, {"id": "2107.06711", "submitter": "Patrick Mangat", "authors": "Dennis Teutscher, Patrick Mangat, Oliver Wasenm\\\"uller", "title": "PDC: Piecewise Depth Completion utilizing Superpixels", "comments": "This paper has been accepted at IEEE Intelligent Transportation\n  Systems Conference (ITSC), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion from sparse LiDAR and high-resolution RGB data is one of the\nfoundations for autonomous driving techniques. Current approaches often rely on\nCNN-based methods with several known drawbacks: flying pixel at depth\ndiscontinuities, overfitting to both a given data set as well as error metric,\nand many more. Thus, we propose our novel Piecewise Depth Completion (PDC),\nwhich works completely without deep learning. PDC segments the RGB image into\nsuperpixels corresponding the regions with similar depth value. Superpixels\ncorresponding to same objects are gathered using a cost map. At the end, we\nreceive detailed depth images with state of the art accuracy. In our\nevaluation, we can show both the influence of the individual proposed\nprocessing steps and the overall performance of our method on the challenging\nKITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:58:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Teutscher", "Dennis", ""], ["Mangat", "Patrick", ""], ["Wasenm\u00fcller", "Oliver", ""]]}, {"id": "2107.06735", "submitter": "Ning Ma", "authors": "Ning Ma, Jiajun Bu, Lixian Lu, Jun Wen, Zhen Zhang, Sheng Zhou, Xifeng\n  Yan", "title": "Semi-Supervised Hypothesis Transfer for Source-Free Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain Adaptation has been widely used to deal with the distribution shift in\nvision, language, multimedia etc. Most domain adaptation methods learn\ndomain-invariant features with data from both domains available. However, such\na strategy might be infeasible in practice when source data are unavailable due\nto data-privacy concerns. To address this issue, we propose a novel adaptation\nmethod via hypothesis transfer without accessing source data at adaptation\nstage. In order to fully use the limited target data, a semi-supervised mutual\nenhancement method is proposed, in which entropy minimization and augmented\nlabel propagation are used iteratively to perform inter-domain and intra-domain\nalignments. Compared with state-of-the-art methods, the experimental results on\nthree public datasets demonstrate that our method gets up to 19.9% improvements\non semi-supervised adaptation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:26:09 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ma", "Ning", ""], ["Bu", "Jiajun", ""], ["Lu", "Lixian", ""], ["Wen", "Jun", ""], ["Zhang", "Zhen", ""], ["Zhou", "Sheng", ""], ["Yan", "Xifeng", ""]]}, {"id": "2107.06747", "submitter": "Arkadiusz Sitek", "authors": "Arkadiusz Sitek, Sangtae Ahn, Evren Asma, Adam Chandler, Alvin Ihsani,\n  Sven Prevrhal, Arman Rahmim, Babak Saboury, Kris Thielemans", "title": "Artificial Intelligence in PET: an Industry Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has significant potential to positively impact\nand advance medical imaging, including positron emission tomography (PET)\nimaging applications. AI has the ability to enhance and optimize all aspects of\nthe PET imaging chain from patient scheduling, patient setup, protocoling, data\nacquisition, detector signal processing, reconstruction, image processing and\ninterpretation. AI poses industry-specific challenges which will need to be\naddressed and overcome to maximize the future potentials of AI in PET. This\npaper provides an overview of these industry-specific challenges for the\ndevelopment, standardization, commercialization, and clinical adoption of AI,\nand explores the potential enhancements to PET imaging brought on by AI in the\nnear future. In particular, the combination of on-demand image reconstruction,\nAI, and custom designed data processing workflows may open new possibilities\nfor innovation which would positively impact the industry and ultimately\npatients.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:47:24 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Sitek", "Arkadiusz", ""], ["Ahn", "Sangtae", ""], ["Asma", "Evren", ""], ["Chandler", "Adam", ""], ["Ihsani", "Alvin", ""], ["Prevrhal", "Sven", ""], ["Rahmim", "Arman", ""], ["Saboury", "Babak", ""], ["Thielemans", "Kris", ""]]}, {"id": "2107.06749", "submitter": "Yifu Wang", "authors": "Kun Huang, Yifu Wang and Laurent Kneip", "title": "Dynamic Event Camera Calibration", "comments": "accepted in the 2021 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera calibration is an important prerequisite towards the solution of 3D\ncomputer vision problems. Traditional methods rely on static images of a\ncalibration pattern. This raises interesting challenges towards the practical\nusage of event cameras, which notably require image change to produce\nsufficient measurements. The current standard for event camera calibration\ntherefore consists of using flashing patterns. They have the advantage of\nsimultaneously triggering events in all reprojected pattern feature locations,\nbut it is difficult to construct or use such patterns in the field. We present\nthe first dynamic event camera calibration algorithm. It calibrates directly\nfrom events captured during relative motion between camera and calibration\npattern. The method is propelled by a novel feature extraction mechanism for\ncalibration patterns, and leverages existing calibration tools before\noptimizing all parameters through a multi-segment continuous-time formulation.\nAs demonstrated through our results on real data, the obtained calibration\nmethod is highly convenient and reliably calibrates from data sequences\nspanning less than 10 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:52:58 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 14:37:11 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 03:40:02 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Huang", "Kun", ""], ["Wang", "Yifu", ""], ["Kneip", "Laurent", ""]]}, {"id": "2107.06768", "submitter": "Hao Chang", "authors": "Hao Chang, Guochen Xie, Jun Yu, Qiang Ling", "title": "BiSTF: Bilateral-Branch Self-Training Framework for Semi-Supervised\n  Large-scale Fine-Grained Recognition", "comments": "arXiv admin note: text overlap with arXiv:2102.09559 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised Fine-Grained Recognition is a challenge task due to the\ndifficulty of data imbalance, high inter-class similarity and domain mismatch.\nRecent years, this field has witnessed great progress and many methods has\ngained great performance. However, these methods can hardly generalize to the\nlarge-scale datasets, such as Semi-iNat, as they are prone to suffer from noise\nin unlabeled data and the incompetence for learning features from imbalanced\nfine-grained data. In this work, we propose Bilateral-Branch Self-Training\nFramework (BiSTF), a simple yet effective framework to improve existing\nsemi-supervised learning methods on class-imbalanced and domain-shifted\nfine-grained data. By adjusting the update frequency through stochastic epoch\nupdate, BiSTF iteratively retrains a baseline SSL model with a labeled set\nexpanded by selectively adding pseudo-labeled samples from an unlabeled set,\nwhere the distribution of pseudo-labeled samples are the same as the labeled\ndata. We show that BiSTF outperforms the existing state-of-the-art SSL\nalgorithm on Semi-iNat dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:28:54 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chang", "Hao", ""], ["Xie", "Guochen", ""], ["Yu", "Jun", ""], ["Ling", "Qiang", ""]]}, {"id": "2107.06777", "submitter": "Christian Bartz", "authors": "Christian Bartz, Hendrik R\\\"atz, Haojin Yang, Joseph Bethge, Christoph\n  Meinel", "title": "Synthesis in Style: Semantic Segmentation of Historical Documents using\n  Synthetic Data", "comments": "Code available at: https://github.com/Bartzi/synthesis-in-style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most pressing problems in the automated analysis of historical\ndocuments is the availability of annotated training data. In this paper, we\npropose a novel method for the synthesis of training data for semantic\nsegmentation of document images. We utilize clusters found in intermediate\nfeatures of a StyleGAN generator for the synthesis of RGB and label images at\nthe same time. Our model can be applied to any dataset of scanned documents\nwithout the need for manual annotation of individual images, as each model is\ncustom-fit to the dataset. In our experiments, we show that models trained on\nour synthetic data can reach competitive performance on open benchmark datasets\nfor line segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:36:47 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bartz", "Christian", ""], ["R\u00e4tz", "Hendrik", ""], ["Yang", "Haojin", ""], ["Bethge", "Joseph", ""], ["Meinel", "Christoph", ""]]}, {"id": "2107.06780", "submitter": "Dan Jia", "authors": "Dan Jia, Bastian Leibe", "title": "Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud", "comments": "accepted as an extended abstract in JRDB-ACT Workshop at CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this preliminary work we attempt to apply submanifold sparse convolution\nto the task of 3D person detection. In particular, we present Person-MinkUNet,\na single-stage 3D person detection network based on Minkowski Engine with U-Net\narchitecture. The network achieves a 76.4% average precision (AP) on the JRDB\n3D detection benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 09:41:53 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Jia", "Dan", ""], ["Leibe", "Bastian", ""]]}, {"id": "2107.06808", "submitter": "Hong Wang", "authors": "Hong Wang, Qi Xie, Qian Zhao, Yong Liang, Deyu Meng", "title": "RCDNet: An Interpretable Rain Convolutional Dictionary Network for\n  Single Image Deraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a common weather, rain streaks adversely degrade the image quality. Hence,\nremoving rains from an image has become an important issue in the field. To\nhandle such an ill-posed single image deraining task, in this paper, we\nspecifically build a novel deep architecture, called rain convolutional\ndictionary network (RCDNet), which embeds the intrinsic priors of rain streaks\nand has clear interpretability. In specific, we first establish a RCD model for\nrepresenting rain streaks and utilize the proximal gradient descent technique\nto design an iterative algorithm only containing simple operators for solving\nthe model. By unfolding it, we then build the RCDNet in which every network\nmodule has clear physical meanings and corresponds to each operation involved\nin the algorithm. This good interpretability greatly facilitates an easy\nvisualization and analysis on what happens inside the network and why it works\nwell in inference process. Moreover, taking into account the domain gap issue\nin real scenarios, we further design a novel dynamic RCDNet, where the rain\nkernels can be dynamically inferred corresponding to input rainy images and\nthen help shrink the space for rain layer estimation with few rain maps so as\nto ensure a fine generalization performance in the inconsistent scenarios of\nrain types between training and testing data. By end-to-end training such an\ninterpretable network, all involved rain kernels and proximal operators can be\nautomatically extracted, faithfully characterizing the features of both rain\nand clean background layers, and thus naturally lead to better deraining\nperformance. Comprehensive experiments substantiate the superiority of our\nmethod, especially on its well generality to diverse testing scenarios and good\ninterpretability for all its modules. Code is available in\n\\emph{\\url{https://github.com/hongwang01/DRCDNet}}.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:08:11 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Hong", ""], ["Xie", "Qi", ""], ["Zhao", "Qian", ""], ["Liang", "Yong", ""], ["Meng", "Deyu", ""]]}, {"id": "2107.06812", "submitter": "Amit More", "authors": "Amit More and Subhasis Chaudhuri", "title": "Deep Learning based Novel View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting novel views of a scene from real-world images has always been a\nchallenging task. In this work, we propose a deep convolutional neural network\n(CNN) which learns to predict novel views of a scene from given collection of\nimages. In comparison to prior deep learning based approaches, which can handle\nonly a fixed number of input images to predict novel view, proposed approach\nworks with different numbers of input images. The proposed model explicitly\nperforms feature extraction and matching from a given pair of input images and\nestimates, at each pixel, the probability distribution (pdf) over possible\ndepth levels in the scene. This pdf is then used for estimating the novel view.\nThe model estimates multiple predictions of novel view, one estimate per input\nimage pair, from given image collection. The model also estimates an occlusion\nmask and combines multiple novel view estimates in to a single optimal\nprediction. The finite number of depth levels used in the analysis may cause\noccasional blurriness in the estimated view. We mitigate this issue with simple\nmulti-resolution analysis which improves the quality of the estimates. We\nsubstantiate the performance on different datasets and show competitive\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:15:36 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["More", "Amit", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2107.06825", "submitter": "Ilya Tolstikhin", "authors": "Ibrahim Alabdulmohsin, Larisa Markeeva, Daniel Keysers, Ilya\n  Tolstikhin", "title": "A Generalized Lottery Ticket Hypothesis", "comments": "Workshop on Sparsity in Neural Networks: Advancing Understanding and\n  Practice (SNN'21). Updates: New curve on Figure 2(left) and discussion on Li\n  et al", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generalization to the lottery ticket hypothesis in which the\nnotion of \"sparsity\" is relaxed by choosing an arbitrary basis in the space of\nparameters. We present evidence that the original results reported for the\ncanonical basis continue to hold in this broader setting. We describe how\nstructured pruning methods, including pruning units or factorizing\nfully-connected layers into products of low-rank matrices, can be cast as\nparticular instances of this \"generalized\" lottery ticket hypothesis. The\ninvestigations reported here are preliminary and are provided to encourage\nfurther research along this direction.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 20:01:24 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 09:28:51 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Alabdulmohsin", "Ibrahim", ""], ["Markeeva", "Larisa", ""], ["Keysers", "Daniel", ""], ["Tolstikhin", "Ilya", ""]]}, {"id": "2107.06831", "submitter": "Jinglin Liu", "authors": "Jinglin Liu, Zhiying Zhu, Yi Ren and Zhou Zhao", "title": "High-Speed and High-Quality Text-to-Lip Generation", "comments": "Author draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a novel parallel decoding model for\nhigh-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we\npredict the duration of the encoded linguistic features and model the target\nlip frames conditioned on the encoded linguistic features with their duration\nin a non-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L\ngenerates lip movements with competitive quality compared with the\nstate-of-the-art AR T2L model DualLip and exceeds the baseline AR model\nTransformerT2L by a notable margin benefiting from the mitigation of the error\npropagation problem; and 2) exhibits distinct superiority in inference speed\n(an average speedup of 19$\\times$ than DualLip on TCD-TIMIT).\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:44:04 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Jinglin", ""], ["Zhu", "Zhiying", ""], ["Ren", "Yi", ""], ["Zhao", "Zhou", ""]]}, {"id": "2107.06847", "submitter": "Tiago Roxo", "authors": "Tiago Roxo and Hugo Proen\\c{c}a", "title": "Faces in the Wild: Efficient Gender Recognition in Surveillance\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Soft biometrics inference in surveillance scenarios is a topic of interest\nfor various applications, particularly in security-related areas. However, soft\nbiometric analysis is not extensively reported in wild conditions. In\nparticular, previous works on gender recognition report their results in face\ndatasets, with relatively good image quality and frontal poses. Given the\nuncertainty of the availability of the facial region in wild conditions, we\nconsider that these methods are not adequate for surveillance settings. To\novercome these limitations, we: 1) present frontal and wild face versions of\nthree well-known surveillance datasets; and 2) propose a model that effectively\nand dynamically combines facial and body information, which makes it suitable\nfor gender recognition in wild conditions. The frontal and wild face datasets\nderive from widely used Pedestrian Attribute Recognition (PAR) sets (PETA,\nPA-100K, and RAP), using a pose-based approach to filter the frontal samples\nand facial regions. This approach retrieves the facial region of images with\nvarying image/subject conditions, where the state-of-the-art face detectors\noften fail. Our model combines facial and body information through a learnable\nfusion matrix and a channel-attention sub-network, focusing on the most\ninfluential body parts according to the specific image/subject features. We\ncompare it with five PAR methods, consistently obtaining state-of-the-art\nresults on gender recognition, and reducing the prediction errors by up to 24%\nin frontal samples. The announced PAR datasets versions and model serve as the\nbasis for wild soft biometrics classification and are available in\nhttps://github.com/Tiago-Roxo.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:02:23 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Roxo", "Tiago", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2107.06862", "submitter": "Eyvind Niklasson", "authors": "Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson", "title": "Differentiable Programming of Reaction-Diffusion Patterns", "comments": "ALIFE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reaction-Diffusion (RD) systems provide a computational framework that\ngoverns many pattern formation processes in nature. Current RD system design\npractices boil down to trial-and-error parameter search. We propose a\ndifferentiable optimization method for learning the RD system parameters to\nperform example-based texture synthesis on a 2D plane. We do this by\nrepresenting the RD system as a variant of Neural Cellular Automata and using\ntask-specific differentiable loss functions. RD systems generated by our method\nexhibit robust, non-trivial 'life-like' behavior.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:38:34 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Mordvintsev", "Alexander", ""], ["Randazzo", "Ettore", ""], ["Niklasson", "Eyvind", ""]]}, {"id": "2107.06912", "submitter": "Marcella Cornia", "authors": "Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia\n  Cascianelli, Giuseppe Fiameni, Rita Cucchiara", "title": "From Show to Tell: A Survey on Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connecting Vision and Language plays an essential role in Generative\nIntelligence. For this reason, in the last few years, a large research effort\nhas been devoted to image captioning, i.e. the task of describing images with\nsyntactically and semantically meaningful sentences. Starting from 2015 the\ntask has generally been addressed with pipelines composed of a visual encoding\nstep and a language model for text generation. During these years, both\ncomponents have evolved considerably through the exploitation of object\nregions, attributes, and relationships and the introduction of multi-modal\nconnections, fully-attentive approaches, and BERT-like early-fusion strategies.\nHowever, regardless of the impressive results obtained, research in image\ncaptioning has not reached a conclusive answer yet. This work aims at providing\na comprehensive overview and categorization of image captioning approaches,\nfrom visual encoding and text generation to training strategies, used datasets,\nand evaluation metrics. In this respect, we quantitatively compare many\nrelevant state-of-the-art approaches to identify the most impactful technical\ninnovations in image captioning architectures and training strategies.\nMoreover, many variants of the problem and its open challenges are analyzed and\ndiscussed. The final goal of this work is to serve as a tool for understanding\nthe existing state-of-the-art and highlighting the future directions for an\narea of research where Computer Vision and Natural Language Processing can find\nan optimal synergy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:00:54 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Stefanini", "Matteo", ""], ["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cascianelli", "Silvia", ""], ["Fiameni", "Giuseppe", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2107.06916", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Bohong Chen, Fei Chao, Jianzhuang Liu, Wei\n  Zeng, Yonghong Tian, Qi Tian", "title": "Training Compact CNNs for Image Classification using Dynamic-coded\n  Filter Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mainstream approach for filter pruning is usually either to force a\nhard-coded importance estimation upon a computation-heavy pretrained model to\nselect \"important\" filters, or to impose a hyperparameter-sensitive sparse\nconstraint on the loss objective to regularize the network training. In this\npaper, we present a novel filter pruning method, dubbed dynamic-coded filter\nfusion (DCFF), to derive compact CNNs in a computation-economical and\nregularization-free manner for efficient image classification. Each filter in\nour DCFF is firstly given an inter-similarity distribution with a temperature\nparameter as a filter proxy, on top of which, a fresh Kullback-Leibler\ndivergence based dynamic-coded criterion is proposed to evaluate the filter\nimportance. In contrast to simply keeping high-score filters in other methods,\nwe propose the concept of filter fusion, i.e., the weighted averages using the\nassigned proxies, as our preserved filters. We obtain a one-hot\ninter-similarity distribution as the temperature parameter approaches infinity.\nThus, the relative importance of each filter can vary along with the training\nof the compact CNN, leading to dynamically changeable fused filters without\nboth the dependency on the pretrained model and the introduction of sparse\nconstraints. Extensive experiments on classification benchmarks demonstrate the\nsuperiority of our DCFF over the compared counterparts. For example, our DCFF\nderives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while\nreaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained\nwith 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1\naccuracy on ILSVRC-2012. Our code, narrower models and training logs are\navailable at https://github.com/lmbxmu/DCFF.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:07:38 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Chen", "Bohong", ""], ["Chao", "Fei", ""], ["Liu", "Jianzhuang", ""], ["Zeng", "Wei", ""], ["Tian", "Yonghong", ""], ["Tian", "Qi", ""]]}, {"id": "2107.06921", "submitter": "Efstratios Kakaletsis", "authors": "Efstratios Kakaletsis, Nikos Nikolaidis", "title": "Potential UAV Landing Sites Detection through Digital Elevation Models\n  Analysis", "comments": "Proceedings of the 2019 27th European Signal Processing Conference\n  (EUSIPCO) satellite workshop \"Signal Processing Computer vision and Deep\n  Learning for Autonomous Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)\npotential landing site detection using terrain information through\nidentification of flat areas, is presented. The algorithm utilizes digital\nelevation models (DEM) that represent the height distribution of an area. Flat\nareas which constitute appropriate landing zones for UAVs in normal or\nemergency situations result by thresholding the image gradient magnitude of the\ndigital surface model (DSM). The proposed technique also uses connected\ncomponents evaluation on the thresholded gradient image in order to discover\nconnected regions of sufficient size for landing. Moreover, man-made structures\nand vegetation areas are detected and excluded from the potential landing\nsites. Quantitative performance evaluation of the proposed landing site\ndetection algorithm in a number of areas on real world and synthetic datasets,\naccompanied by a comparison with a state-of-the-art algorithm, proves its\nefficiency and superiority.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:13:35 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kakaletsis", "Efstratios", ""], ["Nikolaidis", "Nikos", ""]]}, {"id": "2107.06935", "submitter": "Nikolai Ufer", "authors": "Nikolai Ufer, Sabine Lang, Bj\\\"orn Ommer", "title": "Object Retrieval and Localization in Large Art Collections using Deep\n  Multi-Style Feature Fusion and Iterative Voting", "comments": "Accepted at ECCV 2020 Workshop Computer Vision for Art Analysis", "journal-ref": null, "doi": "10.1007/978-3-030-66096-3_12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for specific objects or motifs is essential to art history as both\nassist in decoding the meaning of artworks. Digitization has produced large art\ncollections, but manual methods prove to be insufficient to analyze them. In\nthe following, we introduce an algorithm that allows users to search for image\nregions containing specific motifs or objects and find similar regions in an\nextensive dataset, helping art historians to analyze large digitized art\ncollections. Computer vision has presented efficient methods for visual\ninstance retrieval across photographs. However, applied to art collections,\nthey reveal severe deficiencies because of diverse motifs and massive domain\nshifts induced by differences in techniques, materials, and styles. In this\npaper, we present a multi-style feature fusion approach that successfully\nreduces the domain gap and improves retrieval results without labelled data or\ncurated image collections. Our region-based voting with GPU-accelerated\napproximate nearest-neighbour search allows us to find and localize even small\nmotifs within an extensive dataset in a few seconds. We obtain state-of-the-art\nresults on the Brueghel dataset and demonstrate its generalization to\ninhomogeneous collections with a large number of distractors.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:40:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ufer", "Nikolai", ""], ["Lang", "Sabine", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2107.06941", "submitter": "Lalith Sharan", "authors": "Lalith Sharan, Gabriele Romano, Sven Koehler, Halvar Kelm, Matthias\n  Karck, Raffaele De Simone and Sandy Engelhardt", "title": "Mutually improved endoscopic image synthesis and landmark detection in\n  unpaired image-to-image translation", "comments": "Submitted to IEEE JBHI 2021, 13 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The CycleGAN framework allows for unsupervised image-to-image translation of\nunpaired data. In a scenario of surgical training on a physical surgical\nsimulator, this method can be used to transform endoscopic images of phantoms\ninto images which more closely resemble the intra-operative appearance of the\nsame surgical target structure. This can be viewed as a novel augmented reality\napproach, which we coined Hyperrealism in previous work. In this use case, it\nis of paramount importance to display objects like needles, sutures or\ninstruments consistent in both domains while altering the style to a more\ntissue-like appearance. Segmentation of these objects would allow for a direct\ntransfer, however, contouring of these, partly tiny and thin foreground objects\nis cumbersome and perhaps inaccurate. Instead, we propose to use landmark\ndetection on the points when sutures pass into the tissue. This objective is\ndirectly incorporated into a CycleGAN framework by treating the performance of\npre-trained detector models as an additional optimization goal. We show that a\ntask defined on these sparse landmark labels improves consistency of synthesis\nby the generator network in both domains. Comparing a baseline CycleGAN\narchitecture to our proposed extension (DetCycleGAN), mean precision (PPV)\nimproved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by\n+0.4743. Furthermore, it could be shown that by dataset fusion, generated\nintra-operative images can be leveraged as additional training data for the\ndetection network itself. The data is released within the scope of the AdaptOR\nMICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at\nhttps://github.com/Cardio-AI/detcyclegan_pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:09:50 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Sharan", "Lalith", ""], ["Romano", "Gabriele", ""], ["Koehler", "Sven", ""], ["Kelm", "Halvar", ""], ["Karck", "Matthias", ""], ["De Simone", "Raffaele", ""], ["Engelhardt", "Sandy", ""]]}, {"id": "2107.06943", "submitter": "Szymon P{\\l}otka", "authors": "Szymon P{\\l}otka, Tomasz W{\\l}odarczyk, Adam Klasa, Micha{\\l} Lipa,\n  Arkadiusz Sitek, Tomasz Trzci\\'nski", "title": "FetalNet: Multi-task deep learning framework for fetal ultrasound\n  biometric measurements", "comments": "Submitted to ICONIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:13:33 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["P\u0142otka", "Szymon", ""], ["W\u0142odarczyk", "Tomasz", ""], ["Klasa", "Adam", ""], ["Lipa", "Micha\u0142", ""], ["Sitek", "Arkadiusz", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2107.06964", "submitter": "Jinglu Zhang", "authors": "Jinglu Zhang, Yinyu Nie, Jian Chang, and Jian Jun Zhang", "title": "Surgical Instruction Generation with Transformers", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical instruction generation is a prerequisite towards\nintra-operative context-aware surgical assistance. However, generating\ninstructions from surgical scenes is challenging, as it requires jointly\nunderstanding the surgical activity of current view and modelling relationships\nbetween visual information and textual description. Inspired by the neural\nmachine translation and imaging captioning tasks in open domain, we introduce a\ntransformer-backboned encoder-decoder network with self-critical reinforcement\nlearning to generate instructions from surgical images. We evaluate the\neffectiveness of our method on DAISI dataset, which includes 290 procedures\nfrom various medical disciplines. Our approach outperforms the existing\nbaseline over all caption evaluation metrics. The results demonstrate the\nbenefits of the encoder-decoder structure backboned by transformer in handling\nmultimodal context.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:54:50 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 19:56:59 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhang", "Jinglu", ""], ["Nie", "Yinyu", ""], ["Chang", "Jian", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "2107.06992", "submitter": "Gilberto Ochoa-Ruiz", "authors": "Mauricio Mendez-Ruiz, Ivan Garcia Jorge Gonzalez-Zapata, Gilberto\n  Ochoa-Ruiz, Andres Mendez-Vazquez", "title": "Finding Significant Features for Few-Shot Learning using Dimensionality\n  Reduction", "comments": "This paper is currently under review for the Mexican International\n  Conference on Artificial Intelligence (MICAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning is a relatively new technique that specializes in problems\nwhere we have little amounts of data. The goal of these methods is to classify\ncategories that have not been seen before with just a handful of samples.\nRecent approaches, such as metric learning, adopt the meta-learning strategy in\nwhich we have episodic tasks conformed by support (training) data and query\n(test) data. Metric learning methods have demonstrated that simple models can\nachieve good performance by learning a similarity function to compare the\nsupport and the query data. However, the feature space learned by a given\nmetric learning approach may not exploit the information given by a specific\nfew-shot task. In this work, we explore the use of dimension reduction\ntechniques as a way to find task-significant features helping to make better\npredictions. We measure the performance of the reduced features by assigning a\nscore based on the intra-class and inter-class distance, and selecting a\nfeature reduction method in which instances of different classes are far away\nand instances of the same class are close. This module helps to improve the\naccuracy performance by allowing the similarity function, given by the metric\nlearning method, to have more discriminative features for the classification.\nOur method outperforms the metric learning baselines in the miniImageNet\ndataset by around 2% in accuracy performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:36:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Mendez-Ruiz", "Mauricio", ""], ["Gonzalez-Zapata", "Ivan Garcia Jorge", ""], ["Ochoa-Ruiz", "Gilberto", ""], ["Mendez-Vazquez", "Andres", ""]]}, {"id": "2107.06993", "submitter": "Sourav Mishra", "authors": "Sourav Mishra and Suresh Sundaram", "title": "Confidence Conditioned Knowledge Distillation", "comments": "31 pages, 41 references, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a novel confidence conditioned knowledge distillation (CCKD)\nscheme for transferring the knowledge from a teacher model to a student model\nis proposed. Existing state-of-the-art methods employ fixed loss functions for\nthis purpose and ignore the different levels of information that need to be\ntransferred for different samples. In addition to that, these methods are also\ninefficient in terms of data usage. CCKD addresses these issues by leveraging\nthe confidence assigned by the teacher model to the correct class to devise\nsample-specific loss functions (CCKD-L formulation) and targets (CCKD-T\nformulation). Further, CCKD improves the data efficiency by employing\nself-regulation to stop those samples from participating in the distillation\nprocess on which the student model learns faster. Empirical evaluations on\nseveral benchmark datasets show that CCKD methods achieve at least as much\ngeneralization performance levels as other state-of-the-art methods while being\ndata efficient in the process. Student models trained through CCKD methods do\nnot retain most of the misclassifications commited by the teacher model on the\ntraining set. Distillation through CCKD methods improves the resilience of the\nstudent models against adversarial attacks compared to the conventional KD\nmethod. Experiments show at least 3% increase in performance against\nadversarial attacks for the MNIST and the Fashion MNIST datasets, and at least\n6% increase for the CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 00:33:25 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Mishra", "Sourav", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2107.07002", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Yi Tay, Alexey A. Gritsenko, Zhe Zhao, Neil Houlsby,\n  Fernando Diaz, Donald Metzler, Oriol Vinyals", "title": "The Benchmark Lottery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world of empirical machine learning (ML) strongly relies on benchmarks in\norder to determine the relative effectiveness of different algorithms and\nmethods. This paper proposes the notion of \"a benchmark lottery\" that describes\nthe overall fragility of the ML benchmarking process. The benchmark lottery\npostulates that many factors, other than fundamental algorithmic superiority,\nmay lead to a method being perceived as superior. On multiple benchmark setups\nthat are prevalent in the ML community, we show that the relative performance\nof algorithms may be altered significantly simply by choosing different\nbenchmark tasks, highlighting the fragility of the current paradigms and\npotential fallacious interpretation derived from benchmarking ML methods. Given\nthat every benchmark makes a statement about what it perceives to be important,\nwe argue that this might lead to biased progress in the community. We discuss\nthe implications of the observed phenomena and provide recommendations on\nmitigating them using multiple machine learning domains and communities as use\ncases, including natural language processing, computer vision, information\nretrieval, recommender systems, and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 21:08:30 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Tay", "Yi", ""], ["Gritsenko", "Alexey A.", ""], ["Zhao", "Zhe", ""], ["Houlsby", "Neil", ""], ["Diaz", "Fernando", ""], ["Metzler", "Donald", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2107.07004", "submitter": "Velat Kilic", "authors": "Velat Kilic, Deepti Hegde, Vishwanath Sindagi, A. Brinton Cooper, Mark\n  A. Foster and Vishal M. Patel", "title": "Lidar Light Scattering Augmentation (LISA): Physics-based Simulation of\n  Adverse Weather Conditions for 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar-based object detectors are critical parts of the 3D perception pipeline\nin autonomous navigation systems such as self-driving cars. However, they are\nknown to be sensitive to adverse weather conditions such as rain, snow and fog\ndue to reduced signal-to-noise ratio (SNR) and signal-to-background ratio\n(SBR). As a result, lidar-based object detectors trained on data captured in\nnormal weather tend to perform poorly in such scenarios. However, collecting\nand labelling sufficient training data in a diverse range of adverse weather\nconditions is laborious and prohibitively expensive. To address this issue, we\npropose a physics-based approach to simulate lidar point clouds of scenes in\nadverse weather conditions. These augmented datasets can then be used to train\nlidar-based detectors to improve their all-weather reliability. Specifically,\nwe introduce a hybrid Monte-Carlo based approach that treats (i) the effects of\nlarge particles by placing them randomly and comparing their back reflected\npower against the target, and (ii) attenuation effects on average through\ncalculation of scattering efficiencies from the Mie theory and particle size\ndistributions. Retraining networks with this augmented data improves mean\naverage precision evaluated on real world rainy scenes and we observe greater\nimprovement in performance with our model relative to existing models from the\nliterature. Furthermore, we evaluate recent state-of-the-art detectors on the\nsimulated weather conditions and present an in-depth analysis of their\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 21:10:47 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kilic", "Velat", ""], ["Hegde", "Deepti", ""], ["Sindagi", "Vishwanath", ""], ["Cooper", "A. Brinton", ""], ["Foster", "Mark A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2107.07013", "submitter": "Thomas Langlois", "authors": "Thomas A. Langlois, H. Charles Zhao, Erin Grant, Ishita Dasgupta,\n  Thomas L. Griffiths, Nori Jacoby", "title": "Passive attention in artificial neural networks predicts human visual\n  selectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Developments in machine learning interpretability techniques over the past\ndecade have provided new tools to observe the image regions that are most\ninformative for classification and localization in artificial neural networks\n(ANNs). Are the same regions similarly informative to human observers? Using\ndata from 78 new experiments and 6,610 participants, we show that passive\nattention techniques reveal a significant overlap with human visual selectivity\nestimates derived from 6 distinct behavioral tasks including visual\ndiscrimination, spatial localization, recognizability, free-viewing,\ncued-object search, and saliency search fixations. We find that input\nvisualizations derived from relatively simple ANN architectures probed using\nguided backpropagation methods are the best predictors of a shared component in\nthe joint variability of the human measures. We validate these correlational\nresults with causal manipulations using recognition experiments. We show that\nimages masked with ANN attention maps were easier for humans to classify than\ncontrol masks in a speeded recognition experiment. Similarly, we find that\nrecognition performance in the same ANN models was likewise influenced by\nmasking input images using human visual selectivity maps. This work contributes\na new approach to evaluating the biological and psychological validity of\nleading ANNs as models of human vision: by examining their similarities and\ndifferences in terms of their visual selectivity to the information contained\nin images.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 21:21:48 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Langlois", "Thomas A.", ""], ["Zhao", "H. Charles", ""], ["Grant", "Erin", ""], ["Dasgupta", "Ishita", ""], ["Griffiths", "Thomas L.", ""], ["Jacoby", "Nori", ""]]}, {"id": "2107.07030", "submitter": "Shiyu Song", "authors": "Lei He and Shengjie Jiang and Xiaoqing Liang and Ning Wang and Shiyu\n  Song", "title": "Diff-Net: Image Feature Difference based High-Definition Map Change\n  Detection", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up-to-date High-Definition (HD) maps are essential for self-driving cars. To\nachieve constantly updated HD maps, we present a deep neural network (DNN),\nDiff-Net, to detect changes in them. Compared to traditional methods based on\nobject detectors, the essential design in our work is a parallel feature\ndifference calculation structure that infers map changes by comparing features\nextracted from the camera and rasterized images. To generate these rasterized\nimages, we project map elements onto images in the camera view, yielding\nmeaningful map representations that can be consumed by a DNN accordingly. As we\nformulate the change detection task as an object detection problem, we leverage\nthe anchor-based structure that predicts bounding boxes with different change\nstatus categories. Furthermore, rather than relying on single frame input, we\nintroduce a spatio-temporal fusion module that fuses features from history\nframes into the current, thus improving the overall performance. Finally, we\ncomprehensively validate our method's effectiveness using freshly collected\ndatasets. Results demonstrate that our Diff-Net achieves better performance\nthan the baseline methods and is ready to be integrated into a map production\npipeline maintaining an up-to-date HD map.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 22:51:30 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["He", "Lei", ""], ["Jiang", "Shengjie", ""], ["Liang", "Xiaoqing", ""], ["Wang", "Ning", ""], ["Song", "Shiyu", ""]]}, {"id": "2107.07042", "submitter": "Vincenzo Ferrero", "authors": "Vincenzo Ferrero, Kaveh Hassani, Daniele Grandi, Bryony DuPont", "title": "Classifying Component Function in Product Assemblies with Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function is defined as the ensemble of tasks that enable the product to\ncomplete the designed purpose. Functional tools, such as functional modeling,\noffer decision guidance in the early phase of product design, where explicit\ndesign decisions are yet to be made. Function-based design data is often sparse\nand grounded in individual interpretation. As such, function-based design tools\ncan benefit from automatic function classification to increase data fidelity\nand provide function representation models that enable function-based\nintelligent design agents. Function-based design data is commonly stored in\nmanually generated design repositories. These design repositories are a\ncollection of expert knowledge and interpretations of function in product\ndesign bounded by function-flow and component taxonomies. In this work, we\nrepresent a structured taxonomy-based design repository as assembly-flow\ngraphs, then leverage a graph neural network (GNN) model to perform automatic\nfunction classification. We support automated function classification by\nlearning from repository data to establish the ground truth of component\nfunction assignment. Experimental results show that our GNN model achieves a\nmicro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and\n0.783 for tier 3 (specific) functions. Given the imbalance of data features,\nthe results are encouraging. Our efforts in this paper can be a starting point\nfor more sophisticated applications in knowledge-based CAD systems and\nDesign-for-X consideration in function-based design.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:27:23 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ferrero", "Vincenzo", ""], ["Hassani", "Kaveh", ""], ["Grandi", "Daniele", ""], ["DuPont", "Bryony", ""]]}, {"id": "2107.07056", "submitter": "Zhe Huang", "authors": "Zhe Huang, Ruohua Li, Kazuki Shin, Katherine Driggs-Campbell", "title": "Learning Sparse Interaction Graphs of Partially Observed Pedestrians for\n  Trajectory Prediction", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-pedestrian trajectory prediction is an indispensable safety element of\nautonomous systems that interact with crowds in unstructured environments. Many\nrecent efforts have developed trajectory prediction algorithms with focus on\nunderstanding social norms behind pedestrian motions. Yet we observe these\nworks usually hold two assumptions that prevent them from being smoothly\napplied to robot applications: positions of all pedestrians are consistently\ntracked; the target agent pays attention to all pedestrians in the scene. The\nfirst assumption leads to biased interaction modeling with incomplete\npedestrian data, and the second assumption introduces unnecessary disturbances\nand leads to the freezing robot problem. Thus, we propose Gumbel Social\nTransformer, in which an Edge Gumbel Selector samples a sparse interaction\ngraph of partially observed pedestrians at each time step. A Node Transformer\nEncoder and a Masked LSTM encode the pedestrian features with the sampled\nsparse graphs to predict trajectories. We demonstrate that our model overcomes\nthe potential problems caused by the assumptions, and our approach outperforms\nthe related works in benchmark evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 00:45:11 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 02:32:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Huang", "Zhe", ""], ["Li", "Ruohua", ""], ["Shin", "Kazuki", ""], ["Driggs-Campbell", "Katherine", ""]]}, {"id": "2107.07058", "submitter": "Pingping Zhang Dr", "authors": "Wei Liu and Pingping Zhang and Yinjie Lei and Xiaolin Huang and Jie\n  Yang and Michael Ng", "title": "A Generalized Framework for Edge-preserving and Structure-preserving\n  Image Smoothing", "comments": "This work is accepted by TPAMI. The code is available at\n  https://github.com/wliusjtu/Generalized-Smoothing-Framework. arXiv admin\n  note: substantial text overlap with arXiv:1907.09642", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 00:55:27 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 03:30:38 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 02:43:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Wei", ""], ["Zhang", "Pingping", ""], ["Lei", "Yinjie", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Ng", "Michael", ""]]}, {"id": "2107.07067", "submitter": "Mehdi Miah", "authors": "Mehdi Miah, Guillaume-Alexandre Bilodeau and Nicolas Saunier", "title": "MeNToS: Tracklets Association with a Space-Time Memory Network", "comments": "Presented at the \"Robust Video Scene Understanding: Tracking and\n  Video Segmentation\" workshop (CVPR-W 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for multi-object tracking and segmentation (MOTS) that\ndoes not require fine-tuning or per benchmark hyperparameter selection. The\nproposed method addresses particularly the data association problem. Indeed,\nthe recently introduced HOTA metric, that has a better alignment with the human\nvisual assessment by evenly balancing detections and associations quality, has\nshown that improvements are still needed for data association. After creating\ntracklets using instance segmentation and optical flow, the proposed method\nrelies on a space-time memory network (STM) developed for one-shot video object\nsegmentation to improve the association of tracklets with temporal gaps. To the\nbest of our knowledge, our method, named MeNToS, is the first to use the STM\nnetwork to track object masks for MOTS. We took the 4th place in the RobMOTS\nchallenge. The project page is https://mehdimiah.com/mentos.html.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 01:33:21 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Miah", "Mehdi", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "2107.07089", "submitter": "Feng Shi", "authors": "Feng Shi, Chonghan Lee, Liang Qiu, Yizhou Zhao, Tianyi Shen, Shivran\n  Muralidhar, Tian Han, Song-Chun Zhu, Vijaykrishnan Narayanan", "title": "STAR: Sparse Transformer-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cognitive system for human action and behavior has evolved into a deep\nlearning regime, and especially the advent of Graph Convolution Networks has\ntransformed the field in recent years. However, previous works have mainly\nfocused on over-parameterized and complex models based on dense graph\nconvolution networks, resulting in low efficiency in training and inference.\nMeanwhile, the Transformer architecture-based model has not yet been well\nexplored for cognitive application in human action and behavior estimation.\nThis work proposes a novel skeleton-based human action recognition model with\nsparse attention on the spatial dimension and segmented linear attention on the\ntemporal dimension of data. Our model can also process the variable length of\nvideo clips grouped as a single batch. Experiments show that our model can\nachieve comparable performance while utilizing much less trainable parameters\nand achieve high speed in training and inference. Experiments show that our\nmodel achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline\nmodels at competitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 02:53:11 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Shi", "Feng", ""], ["Lee", "Chonghan", ""], ["Qiu", "Liang", ""], ["Zhao", "Yizhou", ""], ["Shen", "Tianyi", ""], ["Muralidhar", "Shivran", ""], ["Han", "Tian", ""], ["Zhu", "Song-Chun", ""], ["Narayanan", "Vijaykrishnan", ""]]}, {"id": "2107.07095", "submitter": "Xiaomeng Ye", "authors": "Xiaomeng Ye and Ziwei Zhao and David Leake and Xizi Wang and David\n  Crandall", "title": "Applying the Case Difference Heuristic to Learn Adaptations from Deep\n  Network Features", "comments": "7 pages, 2 figures, 1 table. To be published in the IJCAI-21 Workshop\n  on Deep Learning, Case-Based Reasoning, and AutoML: Present and Future\n  Synergies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The case difference heuristic (CDH) approach is a knowledge-light method for\nlearning case adaptation knowledge from the case base of a case-based reasoning\nsystem. Given a pair of cases, the CDH approach attributes the difference in\ntheir solutions to the difference in the problems they solve, and generates\nadaptation rules to adjust solutions accordingly when a retrieved case and new\nquery have similar problem differences. As an alternative to learning\nadaptation rules, several researchers have applied neural networks to learn to\npredict solution differences from problem differences. Previous work on such\napproaches has assumed that the feature set describing problems is predefined.\nThis paper investigates a two-phase process combining deep learning for feature\nextraction and neural network based adaptation learning from extracted\nfeatures. Its performance is demonstrated in a regression task on an image\ndata: predicting age given the image of a face. Results show that the combined\nprocess can successfully learn adaptation knowledge applicable to nonsymbolic\ndifferences in cases. The CBR system achieves slightly lower performance\noverall than a baseline deep network regressor, but better performance than the\nbaseline on novel queries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 03:11:56 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ye", "Xiaomeng", ""], ["Zhao", "Ziwei", ""], ["Leake", "David", ""], ["Wang", "Xizi", ""], ["Crandall", "David", ""]]}, {"id": "2107.07110", "submitter": "Jiayun Wang", "authors": "Jiayun Wang, Yubei Chen, Stella X. Yu, Brian Cheung, Yann LeCun", "title": "Recurrent Parameter Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a generic method for recurrently using the same parameters for\nmany different convolution layers to build a deep network. Specifically, for a\nnetwork, we create a recurrent parameter generator (RPG), from which the\nparameters of each convolution layer are generated. Though using recurrent\nmodels to build a deep convolutional neural network (CNN) is not entirely new,\nour method achieves significant performance gain compared to the existing\nworks. We demonstrate how to build a one-layer neural network to achieve\nsimilar performance compared to other traditional CNN models on various\napplications and datasets. Such a method allows us to build an arbitrarily\ncomplex neural network with any amount of parameters. For example, we build a\nResNet34 with model parameters reduced by more than $400$ times, which still\nachieves $41.6\\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG\ncan be applied at different scales, such as layers, blocks, or even\nsub-networks. Specifically, we use the RPG to build a ResNet18 network with the\nnumber of weights equivalent to one convolutional layer of a conventional\nResNet and show this model can achieve $67.2\\%$ ImageNet top-1 accuracy. The\nproposed method can be viewed as an inverse approach to model compression.\nRather than removing the unused parameters from a large model, it aims to\nsqueeze more information into a small number of parameters. Extensive\nexperiment results are provided to demonstrate the power of the proposed\nrecurrent parameter generator.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 04:23:59 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Jiayun", ""], ["Chen", "Yubei", ""], ["Yu", "Stella X.", ""], ["Cheung", "Brian", ""], ["LeCun", "Yann", ""]]}, {"id": "2107.07148", "submitter": "Aleksandar Jevremovic PhD", "authors": "Zona Kostic and Aleksandar Jevremovic", "title": "What Image Features Boost Housing Market Predictions?", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2020.2966890", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The attractiveness of a property is one of the most interesting, yet\nchallenging, categories to model. Image characteristics are used to describe\ncertain attributes, and to examine the influence of visual factors on the price\nor timeframe of the listing. In this paper, we propose a set of techniques for\nthe extraction of visual features for efficient numerical inclusion in\nmodern-day predictive algorithms. We discuss techniques such as Shannon's\nentropy, calculating the center of gravity, employing image segmentation, and\nusing Convolutional Neural Networks. After comparing these techniques as\napplied to a set of property-related images (indoor, outdoor, and satellite),\nwe conclude the following: (i) the entropy is the most efficient single-digit\nvisual measure for housing price prediction; (ii) image segmentation is the\nmost important visual feature for the prediction of housing lifespan; and (iii)\ndeep image features can be used to quantify interior characteristics and\ncontribute to captivation modeling. The set of 40 image features selected here\ncarries a significant amount of predictive power and outperforms some of the\nstrongest metadata predictors. Without any need to replace a human expert in a\nreal-estate appraisal process, we conclude that the techniques presented in\nthis paper can efficiently describe visible characteristics, thus introducing\nperceived attractiveness as a quantitative measure into the predictive modeling\nof housing.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 06:32:10 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kostic", "Zona", ""], ["Jevremovic", "Aleksandar", ""]]}, {"id": "2107.07153", "submitter": "Oriol Corcoll", "authors": "Oriol Corcoll", "title": "Semantic Image Cropping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic image cropping techniques are commonly used to enhance the\naesthetic quality of an image; they do it by detecting the most beautiful or\nthe most salient parts of the image and removing the unwanted content to have a\nsmaller image that is more visually pleasing. In this thesis, I introduce an\nadditional dimension to the problem of cropping, semantics. I argue that image\ncropping can also enhance the image's relevancy for a given entity by using the\nsemantic information contained in the image. I call this problem, Semantic\nImage Cropping. To support my argument, I provide a new dataset containing 100\nimages with at least two different entities per image and four ground truth\ncroppings collected using Amazon Mechanical Turk. I use this dataset to show\nthat state-of-the-art cropping algorithms that only take into account\naesthetics do not perform well in the problem of semantic image cropping.\nAdditionally, I provide a new deep learning system that takes not just\naesthetics but also semantics into account to generate image croppings, and I\nevaluate its performance using my new semantic cropping dataset, showing that\nusing the semantic information of an image can help to produce better\ncroppings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 06:54:42 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Corcoll", "Oriol", ""]]}, {"id": "2107.07154", "submitter": "Sangmin Woo", "authors": "Sangmin Woo, Junhyug Noh, Kangil Kim", "title": "What and When to Look?: Temporal Span Proposal Network for Video Visual\n  Relation Detection", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying relations between objects is central to understanding the scene.\nWhile several works have been proposed for relation modeling in the image\ndomain, there have been many constraints in the video domain due to challenging\ndynamics of spatio-temporal interactions (e.g., Between which objects are there\nan interaction? When do relations occur and end?). To date, two representative\nmethods have been proposed to tackle Video Visual Relation Detection (VidVRD):\nsegment-based and window-based. We first point out the limitations these two\nmethods have and propose Temporal Span Proposal Network (TSPN), a novel method\nwith two advantages in terms of efficiency and effectiveness. 1) TSPN tells\nwhat to look: it sparsifies relation search space by scoring relationness\n(i.e., confidence score for the existence of a relation between pair of\nobjects) of object pair. 2) TSPN tells when to look: it leverages the full\nvideo context to simultaneously predict the temporal span and categories of the\nentire relations. TSPN demonstrates its effectiveness by achieving new\nstate-of-the-art by a significant margin on two VidVRD benchmarks\n(ImageNet-VidVDR and VidOR) while also showing lower time complexity than\nexisting methods - in particular, twice as efficient as a popular segment-based\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:01:26 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Woo", "Sangmin", ""], ["Noh", "Junhyug", ""], ["Kim", "Kangil", ""]]}, {"id": "2107.07167", "submitter": "Shyh Yaw Jou", "authors": "Shi-Yao Zhou and Chung-Yen Su", "title": "An Efficient and Small Convolutional Neural Network for Pest Recognition\n  -- ExquisiteNet", "comments": "4 pages", "journal-ref": null, "doi": "10.1109/ECICE50847.2020.9301938", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, due to the rapid population expansion, food shortage has become a\ncritical issue. In order to stabilizing the food source production, preventing\ncrops from being attacked by pests is very important. In generally, farmers use\npesticides to kill pests, however, improperly using pesticides will also kill\nsome insects which is beneficial to crops, such as bees. If the number of bees\nis too few, the supplement of food in the world will be in short. Besides,\nexcessive pesticides will seriously pollute the environment. Accordingly,\nfarmers need a machine which can automatically recognize the pests. Recently,\ndeep learning is popular because its effectiveness in the field of image\nclassification. In this paper, we propose a small and efficient model called\nExquisiteNet to complete the task of recognizing the pests and we expect to\napply our model on mobile devices. ExquisiteNet mainly consists of two blocks.\nOne is double fusion with squeeze-and-excitation-bottleneck block (DFSEB\nblock), and the other is max feature expansion block (ME block). ExquisiteNet\nonly has 0.98M parameters and its computing speed is very fast almost the same\nas SqueezeNet. In order to evaluate our model's performance, we test our model\non a benchmark pest dataset called IP102. Compared to many state-of-the-art\nmodels, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet\netc., our model achieves higher accuracy, that is, 52.32% on the test set of\nIP102 without any data augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:34:45 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Zhou", "Shi-Yao", ""], ["Su", "Chung-Yen", ""]]}, {"id": "2107.07191", "submitter": "Joosoon Lee", "authors": "D. Park, J. Lee, J. Lee and K. Lee", "title": "Deep Learning based Food Instance Segmentation using Synthetic Data", "comments": "Accepted by UR2021(Ubiquitous Robots 2021) conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the process of intelligently segmenting foods in images using deep neural\nnetworks for diet management, data collection and labeling for network training\nare very important but labor-intensive tasks. In order to solve the\ndifficulties of data collection and annotations, this paper proposes a food\nsegmentation method applicable to real-world through synthetic data. To perform\nfood segmentation on healthcare robot systems, such as meal assistance robot\narm, we generate synthetic data using the open-source 3D graphics software\nBlender placing multiple objects on meal plate and train Mask R-CNN for\ninstance segmentation. Also, we build a data collection system and verify our\nsegmentation model on real-world food data. As a result, on our real-world\ndataset, the model trained only synthetic data is available to segment food\ninstances that are not trained with 52.2% mask AP@all, and improve performance\nby +6.4%p after fine-tuning comparing to the model trained from scratch. In\naddition, we also confirm the possibility and performance improvement on the\npublic dataset for fair analysis. Our code and pre-trained weights are\navaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 08:36:54 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 13:42:37 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Park", "D.", ""], ["Lee", "J.", ""], ["Lee", "J.", ""], ["Lee", "K.", ""]]}, {"id": "2107.07192", "submitter": "YaKun Ju", "authors": "Yakun Ju, Muwei Jian, Shaoxiang Guo, Yingyu Wang, Huiyu Zhou, Junyu\n  Dong", "title": "Incorporating Lambertian Priors into Surface Normals Measurement", "comments": null, "journal-ref": null, "doi": "10.1109/TIM.2021.3096282", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of photometric stereo is to measure the precise surface normal of a\n3D object from observations with various shading cues. However, non-Lambertian\nsurfaces influence the measurement accuracy due to irregular shading cues.\nDespite deep neural networks have been employed to simulate the performance of\nnon-Lambertian surfaces, the error in specularities, shadows, and crinkle\nregions is hard to be reduced. In order to address this challenge, we here\npropose a photometric stereo network that incorporates Lambertian priors to\nbetter measure the surface normal. In this paper, we use the initial normal\nunder the Lambertian assumption as the prior information to refine the normal\nmeasurement, instead of solely applying the observed shading cues to deriving\nthe surface normal. Our method utilizes the Lambertian information to\nreparameterize the network weights and the powerful fitting ability of deep\nneural networks to correct these errors caused by general reflectance\nproperties. Our explorations include: the Lambertian priors (1) reduce the\nlearning hypothesis space, making our method learn the mapping in the same\nsurface normal space and improving the accuracy of learning, and (2) provides\nthe differential features learning, improving the surfaces reconstruction of\ndetails. Extensive experiments verify the effectiveness of the proposed\nLambertian prior photometric stereo network in accurate surface normal\nmeasurement, on the challenging benchmark dataset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 08:40:55 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ju", "Yakun", ""], ["Jian", "Muwei", ""], ["Guo", "Shaoxiang", ""], ["Wang", "Yingyu", ""], ["Zhou", "Huiyu", ""], ["Dong", "Junyu", ""]]}, {"id": "2107.07201", "submitter": "Dong An", "authors": "Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, Tieniu Tan", "title": "Neighbor-view Enhanced Model for Vision and Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision and Language Navigation (VLN) requires an agent to navigate to a\ntarget location by following natural language instructions. Most of existing\nworks represent a navigation candidate by the feature of the corresponding\nsingle view where the candidate lies in. However, an instruction may mention\nlandmarks out of the single view as references, which might lead to failures of\ntextual-visual matching of existing methods. In this work, we propose a\nmulti-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate\nvisual contexts from neighbor views for better textual-visual matching.\nSpecifically, our NvEM utilizes a subject module and a reference module to\ncollect contexts from neighbor views. The subject module fuses neighbor views\nat a global level, and the reference module fuses neighbor objects at a local\nlevel. Subjects and references are adaptively determined via attention\nme'chanisms. Our model also includes an action module to utilize the strong\norientation guidance (e.g., \"turn left\") in instructions. Each module predicts\nnavigation action separately and their weighted sum is used for predicting the\nfinal action. Extensive experimental results demonstrate the effectiveness of\nthe proposed method on the R2R and R4R benchmarks against several\nstate-of-the-art navigators, and NvEM even beats some pre-training ones. Our\ncode is available at https://github.com/MarSaKi/NvEM.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 09:11:02 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 11:10:21 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 08:03:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["An", "Dong", ""], ["Qi", "Yuankai", ""], ["Huang", "Yan", ""], ["Wu", "Qi", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "2107.07224", "submitter": "Gereon Fox", "authors": "Gereon Fox and Ayush Tewari and Mohamed Elgharib and Christian\n  Theobalt", "title": "StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial models (GANs) continue to produce advances in terms of\nthe visual quality of still images, as well as the learning of temporal\ncorrelations. However, few works manage to combine these two interesting\ncapabilities for the synthesis of video content: Most methods require an\nextensive training dataset in order to learn temporal correlations, while being\nrather limited in the resolution and visual quality of their output frames. In\nthis paper, we present a novel approach to the video synthesis problem that\nhelps to greatly improve visual quality and drastically reduce the amount of\ntraining data and resources necessary for generating video content. Our\nformulation separates the spatial domain, in which individual frames are\nsynthesized, from the temporal domain, in which motion is generated. For the\nspatial domain we make use of a pre-trained StyleGAN network, the latent space\nof which allows control over the appearance of the objects it was trained for.\nThe expressive power of this model allows us to embed our training videos in\nthe StyleGAN latent space. Our temporal architecture is then trained not on\nsequences of RGB frames, but on sequences of StyleGAN latent codes. The\nadvantageous properties of the StyleGAN space simplify the discovery of\ntemporal correlations. We demonstrate that it suffices to train our temporal\narchitecture on only 10 minutes of footage of 1 subject for about 6 hours.\nAfter training, our model can not only generate new portrait videos for the\ntraining subject, but also for any random subject which can be embedded in the\nStyleGAN space.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 09:58:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Fox", "Gereon", ""], ["Tewari", "Ayush", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2107.07225", "submitter": "Jian Zhang", "authors": "Di You, Jian Zhang, Jingfen Xie, Bin Chen, Siwei Ma", "title": "COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing", "comments": "Published in IEEE Transactions on Image Processing, 2021", "journal-ref": "IEEE Transactions on Image Processing, vol. 30, pp. 6066-6080,\n  2021", "doi": "10.1109/TIP.2021.3091834", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent deep network-based compressive sensing (CS) methods have achieved\ngreat success. However, most of them regard different sampling matrices as\ndifferent independent tasks and need to train a specific model for each target\nsampling matrix. Such practices give rise to inefficiency in computing and\nsuffer from poor generalization ability. In this paper, we propose a novel\nCOntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of\narbitrary-sampling matrices (including unseen sampling matrices) with one\nsingle model. Under the optimization-inspired deep unfolding framework, our\nCOAST exhibits good interpretability. In COAST, a random projection\naugmentation (RPA) strategy is proposed to promote the training diversity in\nthe sampling space to enable arbitrary sampling, and a controllable proximal\nmapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are\nfurther developed to dynamically modulate the network features and effectively\neliminate the blocking artifacts, respectively. Extensive experiments on widely\nused benchmark datasets demonstrate that our proposed COAST is not only able to\nhandle arbitrary sampling matrices with one single model but also to achieve\nstate-of-the-art performance with fast speed. The source code is available on\nhttps://github.com/jianzhangcs/COAST.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 10:05:00 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["You", "Di", ""], ["Zhang", "Jian", ""], ["Xie", "Jingfen", ""], ["Chen", "Bin", ""], ["Ma", "Siwei", ""]]}, {"id": "2107.07235", "submitter": "Jizhizi Li", "authors": "Jizhizi Li, Jing Zhang, Dacheng Tao", "title": "Deep Automatic Natural Image Matting", "comments": "Accepted to IJCAI-21, code and dataset available at\n  https://github.com/JizhiziLi/AIM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image matting (AIM) refers to estimating the soft foreground from\nan arbitrary natural image without any auxiliary input like trimap, which is\nuseful for image editing. Prior methods try to learn semantic features to aid\nthe matting process while being limited to images with salient opaque\nforegrounds such as humans and animals. In this paper, we investigate the\ndifficulties when extending them to natural images with salient\ntransparent/meticulous foregrounds or non-salient foregrounds. To address the\nproblem, a novel end-to-end matting network is proposed, which can predict a\ngeneralized trimap for any image of the above types as a unified semantic\nrepresentation. Simultaneously, the learned semantic features guide the matting\nnetwork to focus on the transition areas via an attention mechanism. We also\nconstruct a test set AIM-500 that contains 500 diverse natural images covering\nall types along with manually labeled alpha mattes, making it feasible to\nbenchmark the generalization ability of AIM models. Results of the experiments\ndemonstrate that our network trained on available composite matting datasets\noutperforms existing methods both objectively and subjectively. The source code\nand dataset are available at https://github.com/JizhiziLi/AIM.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 10:29:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Li", "Jizhizi", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2107.07243", "submitter": "Marco Camurri", "authors": "David Wisth, Marco Camurri, Maurice Fallon", "title": "VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged\n  Robots", "comments": "Video: https://youtu.be/2318fiEB2cQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VILENS (Visual Inertial Lidar Legged Navigation System), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term which is estimated online. This bias is only\nobservable because of the tight fusion of this preintegrated velocity factor\nwith vision, lidar, and IMU factors. Extensive experimental validation on the\nANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km\ntraveled. The experiments involved dynamic locomotion over loose rocks, slopes,\nand mud; these included perceptual challenges, such as dark and dusty\nunderground caverns or open, feature-deprived areas, as well as mobility\nchallenges such as slipping and terrain deformation. We show an average\nimprovement of 62% translational and 51% rotational errors compared to a\nstate-of-the-art loosely coupled approach. To demonstrate its robustness,\nVILENS was also integrated with a perceptive controller and a local path\nplanner.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:05:00 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wisth", "David", ""], ["Camurri", "Marco", ""], ["Fallon", "Maurice", ""]]}, {"id": "2107.07259", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Xin Sun, Jimei Yang, Ruben Villegas, Jianming Zhang,\n  Zhixin Shu, Belen Masia, and Diego Gutierrez", "title": "Single-image Full-body Human Relighting", "comments": "11 pages, 12 figures", "journal-ref": "Eurographics Symposium on Rendering (EGSR), 2021", "doi": "10.2312/sr.20211300", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a single-image data-driven method to automatically relight images\nwith full-body humans in them. Our framework is based on a realistic scene\ndecomposition leveraging precomputed radiance transfer (PRT) and spherical\nharmonics (SH) lighting. In contrast to previous work, we lift the assumptions\non Lambertian materials and explicitly model diffuse and specular reflectance\nin our data. Moreover, we introduce an additional light-dependent residual term\nthat accounts for errors in the PRT-based image reconstruction. We propose a\nnew deep learning architecture, tailored to the decomposition performed in PRT,\nthat is trained using a combination of L1, logarithmic, and rendering losses.\nOur model outperforms the state of the art for full-body human relighting both\nwith synthetic images and photographs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:34:03 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lagunas", "Manuel", ""], ["Sun", "Xin", ""], ["Yang", "Jimei", ""], ["Villegas", "Ruben", ""], ["Zhang", "Jianming", ""], ["Shu", "Zhixin", ""], ["Masia", "Belen", ""], ["Gutierrez", "Diego", ""]]}, {"id": "2107.07271", "submitter": "Richard Gault", "authors": "Andrew Moyes, Richard Gault, Kun Zhang, Ji Ming, Danny Crookes, Jing\n  Wang", "title": "Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain\n  Invariant Representations of Histopathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Domain shift is a problem commonly encountered when developing automated\nhistopathology pipelines. The performance of machine learning models such as\nconvolutional neural networks within automated histopathology pipelines is\noften diminished when applying them to novel data domains due to factors\narising from differing staining and scanning protocols. The Dual-Channel\nAuto-Encoder (DCAE) model was previously shown to produce feature\nrepresentations that are less sensitive to appearance variation introduced by\ndifferent digital slide scanners. In this work, the Multi-Channel Auto-Encoder\n(MCAE) model is presented as an extension to DCAE which learns from more than\ntwo domains of data. Additionally, a synthetic dataset is generated using\nCycleGANs that contains aligned tissue images that have had their appearance\nsynthetically modified. Experimental results show that the MCAE model produces\nfeature representations that are less sensitive to inter-domain variations than\nthe comparative StaNoSA method when tested on the novel synthetic data.\nAdditionally, the MCAE and StaNoSA models are tested on a novel tissue\nclassification task. The results of this experiment show the MCAE model out\nperforms the StaNoSA model by 5 percentage-points in the f1-score. These\nresults show that the MCAE model is able to generalise better to novel data and\ntasks than existing approaches by actively learning normalised feature\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:56:41 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Moyes", "Andrew", ""], ["Gault", "Richard", ""], ["Zhang", "Kun", ""], ["Ming", "Ji", ""], ["Crookes", "Danny", ""], ["Wang", "Jing", ""]]}, {"id": "2107.07305", "submitter": "Emmanouil Sifalakis", "authors": "Amirreza Yousefzadeh, Manolis Sifalakis", "title": "Training for temporal sparsity in deep neural networks, application in\n  video processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Activation sparsity improves compute efficiency and resource utilization in\nsparsity-aware neural network accelerators. As the predominant operation in\nDNNs is multiply-accumulate (MAC) of activations with weights to compute inner\nproducts, skipping operations where (at least) one of the two operands is zero\ncan make inference more efficient in terms of latency and power. Spatial\nsparsification of activations is a popular topic in DNN literature and several\nmethods have already been established to bias a DNN for it. On the other hand,\ntemporal sparsity is an inherent feature of bio-inspired spiking neural\nnetworks (SNNs), which neuromorphic processing exploits for hardware\nefficiency. Introducing and exploiting spatio-temporal sparsity, is a topic\nmuch less explored in DNN literature, but in perfect resonance with the trend\nin DNN, to shift from static signal processing to more streaming signal\nprocessing. Towards this goal, in this paper we introduce a new DNN layer\n(called Delta Activation Layer), whose sole purpose is to promote temporal\nsparsity of activations during training. A Delta Activation Layer casts\ntemporal sparsity into spatial activation sparsity to be exploited when\nperforming sparse tensor multiplications in hardware. By employing delta\ninference and ``the usual'' spatial sparsification heuristics during training,\nthe resulting model learns to exploit not only spatial but also temporal\nactivation sparsity (for a given input data distribution). One may use the\nDelta Activation Layer either during vanilla training or during a refinement\nphase. We have implemented Delta Activation Layer as an extension of the\nstandard Tensoflow-Keras library, and applied it to train deep neural networks\non the Human Action Recognition (UCF101) dataset. We report an almost 3x\nimprovement of activation sparsity, with recoverable loss of model accuracy\nafter longer training.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:17:11 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yousefzadeh", "Amirreza", ""], ["Sifalakis", "Manolis", ""]]}, {"id": "2107.07314", "submitter": "Ivona Najdenkoska", "authors": "Ivona Najdenkoska, Xiantong Zhen, Marcel Worring and Ling Shao", "title": "Variational Topic Inference for Chest X-Ray Report Generation", "comments": "To be published in the International Conference on Medical Image\n  Computing and Computer Assisted Intervention 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automating report generation for medical imaging promises to reduce workload\nand assist diagnosis in clinical practice. Recent work has shown that deep\nlearning models can successfully caption natural images. However, learning from\nmedical data is challenging due to the diversity and uncertainty inherent in\nthe reports written by different radiologists with discrepant expertise and\nexperience. To tackle these challenges, we propose variational topic inference\nfor automatic report generation. Specifically, we introduce a set of topics as\nlatent variables to guide sentence generation by aligning image and language\nmodalities in a latent space. The topics are inferred in a conditional\nvariational inference framework, with each topic governing the generation of a\nsentence in the report. Further, we adopt a visual attention module that\nenables the model to attend to different locations in the image and generate\nmore informative descriptions. We conduct extensive experiments on two\nbenchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results\ndemonstrate that our proposed variational topic inference method can generate\nnovel reports rather than mere copies of reports used in training, while still\nachieving comparable performance to state-of-the-art methods in terms of\nstandard language generation criteria.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:34:38 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Najdenkoska", "Ivona", ""], ["Zhen", "Xiantong", ""], ["Worring", "Marcel", ""], ["Shao", "Ling", ""]]}, {"id": "2107.07330", "submitter": "Jake Deane", "authors": "Jake Deane, Sinead Kearney, Kwang In Kim, Darren Cosker", "title": "DynaDog+T: A Parametric Animal Model for Synthetic Canine Image\n  Generation", "comments": "CV4Animals Workshop in CVPR 2021. Update to correct minor spelling\n  and grammer mistakes in supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthetic data is becoming increasingly common for training computer vision\nmodels for a variety of tasks. Notably, such data has been applied in tasks\nrelated to humans such as 3D pose estimation where data is either difficult to\ncreate or obtain in realistic settings. Comparatively, there has been less work\ninto synthetic animal data and it's uses for training models. Consequently, we\nintroduce a parametric canine model, DynaDog+T, for generating synthetic canine\nimages and data which we use for a common computer vision task, binary\nsegmentation, which would otherwise be difficult due to the lack of available\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:53:10 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 14:07:12 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Deane", "Jake", ""], ["Kearney", "Sinead", ""], ["Kim", "Kwang In", ""], ["Cosker", "Darren", ""]]}, {"id": "2107.07333", "submitter": "Taimur Hassan", "authors": "Taimur Hassan and Samet Akcay and Mohammed Bennamoun and Salman Khan\n  and Naoufel Werghi", "title": "Unsupervised Anomaly Instance Segmentation for Baggage Threat\n  Recognition", "comments": "Accepted in J-AIHC, Source Code is available at\n  https://github.com/taimurhassan/anomaly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying potential threats concealed within the baggage is of prime\nconcern for the security staff. Many researchers have developed frameworks that\ncan detect baggage threats from X-ray scans. However, to the best of our\nknowledge, all of these frameworks require extensive training on large-scale\nand well-annotated datasets, which are hard to procure in the real world. This\npaper presents a novel unsupervised anomaly instance segmentation framework\nthat recognizes baggage threats, in X-ray scans, as anomalies without requiring\nany ground truth labels. Furthermore, thanks to its stylization capacity, the\nframework is trained only once, and at the inference stage, it detects and\nextracts contraband items regardless of their scanner specifications. Our\none-staged approach initially learns to reconstruct normal baggage content via\nan encoder-decoder network utilizing a proposed stylization loss function. The\nmodel subsequently identifies the abnormal regions by analyzing the disparities\nwithin the original and the reconstructed scans. The anomalous regions are then\nclustered and post-processed to fit a bounding box for their localization. In\naddition, an optional classifier can also be appended with the proposed\nframework to recognize the categories of these extracted anomalies. A thorough\nevaluation of the proposed system on four public baggage X-ray datasets,\nwithout any re-training, demonstrates that it achieves competitive performance\nas compared to the conventional fully supervised methods (i.e., the mean\naverage precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on\nOPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art\nsemi-supervised and unsupervised baggage threat detection frameworks by 67.37%,\n32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray,\nand COMPASS-XP datasets, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:56:55 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 18:24:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hassan", "Taimur", ""], ["Akcay", "Samet", ""], ["Bennamoun", "Mohammed", ""], ["Khan", "Salman", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2107.07344", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Framework for A Personalized Intelligent Assistant to Elderly People for\n  Activities of Daily Living", "comments": "arXiv admin note: text overlap with arXiv:2106.15599", "journal-ref": "International Journal of Recent Trends in Human Computer\n  Interaction (IJHCI), Volume 9, Issue 1, 2019, pp. 1-22", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:36:07 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2107.07397", "submitter": "Piotr Migda{\\l}", "authors": "Piotr Migda{\\l}, Bart{\\l}omiej Olechno, B{\\l}a\\.zej Podg\\'orski", "title": "Level generation and style enhancement -- deep learning for game\n  development overview", "comments": "16 pages, 10 figures, submitted to the 52nd International Simulation\n  and Gaming Association (ISAGA) Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present practical approaches of using deep learning to create and enhance\nlevel maps and textures for video games -- desktop, mobile, and web. We aim to\npresent new possibilities for game developers and level artists. The task of\ndesigning levels and filling them with details is challenging. It is both\ntime-consuming and takes effort to make levels rich, complex, and with a\nfeeling of being natural. Fortunately, recent progress in deep learning\nprovides new tools to accompany level designers and visual artists. Moreover,\nthey offer a way to generate infinite worlds for game replayability and adjust\neducational games to players' needs. We present seven approaches to create\nlevel maps, each using statistical methods, machine learning, or deep learning.\nIn particular, we include:\n  - Generative Adversarial Networks for creating new images from existing\nexamples (e.g. ProGAN).\n  - Super-resolution techniques for upscaling images while preserving crisp\ndetail (e.g. ESRGAN).\n  - Neural style transfer for changing visual themes.\n  - Image translation - turning semantic maps into images (e.g. GauGAN).\n  - Semantic segmentation for turning images into semantic masks (e.g. U-Net).\n  - Unsupervised semantic segmentation for extracting semantic features (e.g.\nTile2Vec).\n  - Texture synthesis - creating large patterns based on a smaller sample (e.g.\nInGAN).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 15:24:43 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Migda\u0142", "Piotr", ""], ["Olechno", "Bart\u0142omiej", ""], ["Podg\u00f3rski", "B\u0142a\u017cej", ""]]}, {"id": "2107.07431", "submitter": "Nico Lang", "authors": "Nico Lang, Konrad Schindler, Jan Dirk Wegner", "title": "High carbon stock mapping at large scale with optical satellite imagery\n  and spaceborne LIDAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand for commodities is leading to changes in land use\nworldwide. In the tropics, deforestation, which causes high carbon emissions\nand threatens biodiversity, is often linked to agricultural expansion. While\nthe need for deforestation-free global supply chains is widely recognized,\nmaking progress in practice remains a challenge. Here, we propose an automated\napproach that aims to support conservation and sustainable land use planning\ndecisions by mapping tropical landscapes at large scale and high spatial\nresolution following the High Carbon Stock (HCS) approach. A deep learning\napproach is developed that estimates canopy height for each 10 m Sentinel-2\npixel by learning from sparse GEDI LIDAR reference data, achieving an overall\nRMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are\npredictive for classifying HCS forests and degraded areas with an overall\naccuracy of 86 % and produce a first high carbon stock map for Indonesia,\nMalaysia, and the Philippines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:21:21 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lang", "Nico", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "2107.07436", "submitter": "Neil Jethani", "authors": "Neil Jethani, Mukund Sudarshan, Ian Covert, Su-In Lee, Rajesh\n  Ranganath", "title": "FastSHAP: Real-Time Shapley Value Estimation", "comments": "20 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:34:45 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Jethani", "Neil", ""], ["Sudarshan", "Mukund", ""], ["Covert", "Ian", ""], ["Lee", "Su-In", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2107.07437", "submitter": "Omer Kafri", "authors": "Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or", "title": "StyleFusion: A Generative Model for Disentangling Spatial Segments", "comments": "Code is available at: https://github.com/OmerKafri/StyleFusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present StyleFusion, a new mapping architecture for StyleGAN, which takes\nas input a number of latent codes and fuses them into a single style code.\nInserting the resulting style code into a pre-trained StyleGAN generator\nresults in a single harmonized image in which each semantic region is\ncontrolled by one of the input latent codes. Effectively, StyleFusion yields a\ndisentangled representation of the image, providing fine-grained control over\neach region of the generated image. Moreover, to help facilitate global control\nover the generated image, a special input latent code is incorporated into the\nfused representation. StyleFusion operates in a hierarchical manner, where each\nlevel is tasked with learning to disentangle a pair of image regions (e.g., the\ncar body and wheels). The resulting learned disentanglement allows one to\nmodify both local, fine-grained semantics (e.g., facial features) as well as\nmore global features (e.g., pose and background), providing improved\nflexibility in the synthesis process. As a natural extension, StyleFusion\nenables one to perform semantically-aware cross-image mixing of regions that\nare not necessarily aligned. Finally, we demonstrate how StyleFusion can be\npaired with existing editing techniques to more faithfully constrain the edit\nto the user's region of interest.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:35:21 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kafri", "Omer", ""], ["Patashnik", "Or", ""], ["Alaluf", "Yuval", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2107.07438", "submitter": "Yikun Ban", "authors": "Yikun Ban, Jingrui He", "title": "Convolutional Neural Bandit: Provable Algorithm for Visual-aware\n  Advertising", "comments": "23 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online advertising is ubiquitous in web business. Image displaying is\nconsidered as one of the most commonly used formats to interact with customers.\nContextual multi-armed bandit has shown success in the application of\nadvertising to solve the exploration-exploitation dilemma existed in the\nrecommendation procedure. Inspired by the visual-aware advertising, in this\npaper, we propose a contextual bandit algorithm, where the convolutional neural\nnetwork (CNN) is utilized to learn the reward function along with an upper\nconfidence bound (UCB) for exploration. We also prove a near-optimal regret\nbound $\\tilde{\\mathcal{O}}(\\sqrt{T})$ when the network is over-parameterized\nand establish strong connections with convolutional neural tangent kernel\n(CNTK). Finally, we evaluate the empirical performance of the proposed\nalgorithm and show that it outperforms other state-of-the-art UCB-based bandit\nalgorithms on real-world image data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 03:02:29 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ban", "Yikun", ""], ["He", "Jingrui", ""]]}, {"id": "2107.07449", "submitter": "Senthil Yogamani", "authors": "Ibrahim Sobh, Ahmed Hamed, Varun Ravi Kumar and Senthil Yogamani", "title": "Adversarial Attacks on Multi-task Visual Perception for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have accomplished impressive success in various\napplications, including autonomous driving perception tasks, in recent years.\nOn the other hand, current deep neural networks are easily fooled by\nadversarial attacks. This vulnerability raises significant concerns,\nparticularly in safety-critical applications. As a result, research into\nattacking and defending DNNs has gained much coverage. In this work, detailed\nadversarial attacks are applied on a diverse multi-task visual perception deep\nnetwork across distance estimation, semantic segmentation, motion detection,\nand object detection. The experiments consider both white and black box attacks\nfor targeted and un-targeted cases, while attacking a task and inspecting the\neffect on all the others, in addition to inspecting the effect of applying a\nsimple defense method. We conclude this paper by comparing and discussing the\nexperimental results, proposing insights and future work. The visualizations of\nthe attacks are available at https://youtu.be/R3JUV41aiPY.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:53:48 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Sobh", "Ibrahim", ""], ["Hamed", "Ahmed", ""], ["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2107.07464", "submitter": "Xunli Zeng", "authors": "Xunli Zeng and Jianqin Yin", "title": "Amodal segmentation just like doing a jigsaw", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amodal segmentation is a new direction of instance segmentation while\nconsidering the segmentation of the visible and occluded parts of the instance.\nThe existing state-of-the-art method uses multi-task branches to predict the\namodal part and the visible part separately and subtract the visible part from\nthe amodal part to obtain the occluded part. However, the amodal part contains\nvisible information. Therefore, the separated prediction method will generate\nduplicate information. Different from this method, we propose a method of\namodal segmentation based on the idea of the jigsaw. The method uses multi-task\nbranches to predict the two naturally decoupled parts of visible and occluded,\nwhich is like getting two matching jigsaw pieces. Then put the two jigsaw\npieces together to get the amodal part. This makes each branch focus on the\nmodeling of the object. And we believe that there are certain rules in the\nocclusion relationship in the real world. This is a kind of occlusion context\ninformation. This jigsaw method can better model the occlusion relationship and\nuse the occlusion context information, which is important for amodal\nsegmentation. Experiments on two widely used amodally annotated datasets prove\nthat our method exceeds existing state-of-the-art methods. The source code of\nthis work will be made public soon.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:08:53 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Zeng", "Xunli", ""], ["Yin", "Jianqin", ""]]}, {"id": "2107.07468", "submitter": "Jo\\~ao Paulo Casagrande Bertoldo", "authors": "Jo\\~ao P C Bertoldo, Etienne Decenci\\`ere, David Ryckelynck, Henry\n  Proudhon", "title": "A modular U-Net for automated segmentation of X-ray tomography images in\n  composite materials", "comments": "Submitted to Nature Machine Intelligence", "journal-ref": null, "doi": "10.21203/rs.3.rs-721240/v1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  X-ray Computed Tomography (XCT) techniques have evolved to a point that\nhigh-resolution data can be acquired so fast that classic segmentation methods\nare prohibitively cumbersome, demanding automated data pipelines capable of\ndealing with non-trivial 3D images. Deep learning has demonstrated success in\nmany image processing tasks, including material science applications, showing a\npromising alternative for a humanfree segmentation pipeline. In this paper a\nmodular interpretation of UNet (Modular U-Net) is proposed and trained to\nsegment 3D tomography images of a three-phased glass fiber-reinforced Polyamide\n66. We compare 2D and 3D versions of our model, finding that the former is\nslightly better than the latter. We observe that human-comparable results can\nbe achievied even with only 10 annotated layers and using a shallow U-Net\nyields better results than a deeper one. As a consequence, Neural Network (NN)\nshow indeed a promising venue to automate XCT data processing pipelines needing\nno human, adhoc intervention.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:15:24 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bertoldo", "Jo\u00e3o P C", ""], ["Decenci\u00e8re", "Etienne", ""], ["Ryckelynck", "David", ""], ["Proudhon", "Henry", ""]]}, {"id": "2107.07497", "submitter": "Puneet Mangla", "authors": "Puneet Mangla, Shivam Chandhok, Vineeth N Balasubramanian and Fahad\n  Shahbaz Khan", "title": "Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress towards designing models that can generalize to unseen\ndomains (i.e domain generalization) or unseen classes (i.e zero-shot learning)\nhas embarked interest towards building models that can tackle both domain-shift\nand semantic shift simultaneously (i.e zero-shot domain generalization). For\nmodels to generalize to unseen classes in unseen domains, it is crucial to\nlearn feature representation that preserves class-level (domain-invariant) as\nwell as domain-specific information. Motivated from the success of generative\nzero-shot approaches, we propose a feature generative framework integrated with\na COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly\nintegrate class-level semantic and domain-specific information. The generated\nvisual features better capture the underlying data distribution enabling us to\ngeneralize to unseen classes and domains at test-time. We thoroughly evaluate\nand analyse our approach on established large-scale benchmark - DomainNet and\ndemonstrate promising performance over baselines and state-of-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:51:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Mangla", "Puneet", ""], ["Chandhok", "Shivam", ""], ["Balasubramanian", "Vineeth N", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2107.07500", "submitter": "Narinder Singh Punn", "authors": "Sudhanshu, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal", "title": "Recommending best course of treatment based on similarities of\n  prognostic markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement in the technology sector spanning over every field, a\nhuge influx of information is inevitable. Among all the opportunities that the\nadvancements in the technology have brought, one of them is to propose\nefficient solutions for data retrieval. This means that from an enormous pile\nof data, the retrieval methods should allow the users to fetch the relevant and\nrecent data over time. In the field of entertainment and e-commerce,\nrecommender systems have been functioning to provide the aforementioned.\nEmploying the same systems in the medical domain could definitely prove to be\nuseful in variety of ways. Following this context, the goal of this paper is to\npropose collaborative filtering based recommender system in the healthcare\nsector to recommend remedies based on the symptoms experienced by the patients.\nFurthermore, a new dataset is developed consisting of remedies concerning\nvarious diseases to address the limited availability of the data. The proposed\nrecommender system accepts the prognostic markers of a patient as the input and\ngenerates the best remedy course. With several experimental trials, the\nproposed model achieved promising results in recommending the possible remedy\nfor given prognostic markers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:52:12 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 07:39:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Sudhanshu", "", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2107.07502", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu,\n  Leslie Chen, Peter Wu, Michelle A. Lee, Yuke Zhu, Ruslan Salakhutdinov,\n  Louis-Philippe Morency", "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning", "comments": "Code: https://github.com/pliang279/MultiBench and Website:\n  https://cmu-multicomp-lab.github.io/multibench/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. It is a challenging yet crucial area\nwith numerous real-world applications in multimedia, affective computing,\nrobotics, finance, human-computer interaction, and healthcare. Unfortunately,\nmultimodal research has seen limited resources to study (1) generalization\nacross domains and modalities, (2) complexity during training and inference,\nand (3) robustness to noisy and missing modalities. In order to accelerate\nprogress towards understudied modalities and tasks while ensuring real-world\nrobustness, we release MultiBench, a systematic and unified large-scale\nbenchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6\nresearch areas. MultiBench provides an automated end-to-end machine learning\npipeline that simplifies and standardizes data loading, experimental setup, and\nmodel evaluation. To enable holistic evaluation, MultiBench offers a\ncomprehensive methodology to assess (1) generalization, (2) time and space\ncomplexity, and (3) modality robustness. MultiBench introduces impactful\nchallenges for future research, including scalability to large-scale multimodal\ndatasets and robustness to realistic imperfections. To accompany this\nbenchmark, we also provide a standardized implementation of 20 core approaches\nin multimodal learning. Simply applying methods proposed in different research\nareas can improve the state-of-the-art performance on 9/15 datasets. Therefore,\nMultiBench presents a milestone in unifying disjoint efforts in multimodal\nresearch and paves the way towards a better understanding of the capabilities\nand limitations of multimodal models, all the while ensuring ease of use,\naccessibility, and reproducibility. MultiBench, our standardized code, and\nleaderboards are publicly available, will be regularly updated, and welcomes\ninputs from the community.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:54:36 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Liang", "Paul Pu", ""], ["Lyu", "Yiwei", ""], ["Fan", "Xiang", ""], ["Wu", "Zetian", ""], ["Cheng", "Yun", ""], ["Wu", "Jason", ""], ["Chen", "Leslie", ""], ["Wu", "Peter", ""], ["Lee", "Michelle A.", ""], ["Zhu", "Yuke", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2107.07539", "submitter": "Sen Wang", "authors": "Xinxin Zuo and Sen Wang and Minglun Gong and Li Cheng", "title": "Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel unsupervised approach to reconstruct human shape\nand pose from noisy point cloud. Traditional approaches search for\ncorrespondences and conduct model fitting iteratively where a good\ninitialization is critical. Relying on large amount of dataset with\nground-truth annotations, recent learning-based approaches predict\ncorrespondences for every vertice on the point cloud; Chamfer distance is\nusually used to minimize the distance between a deformed template model and the\ninput point cloud. However, Chamfer distance is quite sensitive to noise and\noutliers, thus could be unreliable to assign correspondences. To address these\nissues, we model the probability distribution of the input point cloud as\ngenerated from a parametric human model under a Gaussian Mixture Model. Instead\nof explicitly aligning correspondences, we treat the process of correspondence\nsearch as an implicit probabilistic association by updating the posterior\nprobability of the template model given the input. A novel unsupervised loss is\nfurther derived that penalizes the discrepancy between the deformed template\nand the input point cloud conditioned on the posterior probability. Our\napproach is very flexible, which works with both complete point cloud and\nincomplete ones including even a single depth image as input. Our network is\ntrained from scratch with no need to warm-up the network with supervised data.\nCompared to previous unsupervised methods, our method shows the capability to\ndeal with substantial noise and outliers. Extensive experiments conducted on\nvarious public synthetic datasets as well as a very noisy real dataset (i.e.\nCMU Panoptic) demonstrate the superior performance of our approach over the\nstate-of-the-art methods. Code can be found\n\\url{https://github.com/wangsen1312/unsupervised3dhuman.git}\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 18:07:47 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Zuo", "Xinxin", ""], ["Wang", "Sen", ""], ["Gong", "Minglun", ""], ["Cheng", "Li", ""]]}, {"id": "2107.07557", "submitter": "Saravanabalagi Ramachandran", "authors": "Saravanabalagi Ramachandran and John McDonald", "title": "OdoViz: A 3D Odometry Visualization and Processing Tool", "comments": "Accepted, ITSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OdoViz is a reactive web-based tool for 3D visualization and processing of\nautonomous vehicle datasets designed to support common tasks in visual place\nrecognition research. The system includes functionality for loading,\ninspecting, visualizing, and processing GPS/INS poses, point clouds and camera\nimages. It supports a number of commonly used driving datasets and can be\nadapted to load custom datasets with minimal effort. OdoViz's design consists\nof a slim server to serve the datasets coupled with a rich client frontend.\nThis design supports multiple deployment configurations including single user\nstand-alone installations, research group installations serving datasets\ninternally across a lab, or publicly accessible web-frontends for providing\nonline interfaces for exploring and interacting with datasets. The tool allows\nviewing complete vehicle trajectories traversed at multiple different time\nperiods simultaneously, facilitating tasks such as sub-sampling, comparing and\nfinding pose correspondences both across and within sequences. This\nsignificantly reduces the effort required in creating subsets of data from\nexisting datasets for machine learning tasks. Further to the above, the system\nalso supports adding custom extensions and plugins to extend the capabilities\nof the software for other potential data management, visualization and\nprocessing tasks. The platform has been open-sourced to promote its use and\nencourage further contributions from the research community.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 18:37:19 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ramachandran", "Saravanabalagi", ""], ["McDonald", "John", ""]]}, {"id": "2107.07576", "submitter": "Mohammad Sabik Irbaz", "authors": "Mohammad Sabik Irbaz, MD Abdullah Al Nasim, Refat E Ferdous", "title": "Real-Time Face Recognition System for Remote Employee Tracking", "comments": "Accepted in International Conference on Big Data, IoT and Machine\n  Learning (BIM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  During the COVID-19 pandemic, most of the human-to-human interactions have\nbeen stopped. To mitigate the spread of deadly coronavirus, many offices took\nthe initiative so that the employees can work from home. But, tracking the\nemployees and finding out if they are really performing what they were supposed\nto turn out to be a serious challenge for all the companies and organizations\nwho are facilitating \"Work From Home\". To deal with the challenge effectively,\nwe came up with a solution to track the employees with face recognition. We\nhave been testing this system experimentally for our office. To train the face\nrecognition module, we used FaceNet with KNN using the Labeled Faces in the\nWild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model\ninto our central system, where the employees log their time. In this paper, we\ndiscuss in brief the system we have been experimenting with and the pros and\ncons of the system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 19:21:37 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Irbaz", "Mohammad Sabik", ""], ["Nasim", "MD Abdullah Al", ""], ["Ferdous", "Refat E", ""]]}, {"id": "2107.07578", "submitter": "Mann Patel", "authors": "Mann Patel", "title": "Real-Time Violence Detection Using CNN-LSTM", "comments": "5 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Violence rates however have been brought down about 57% during the span of\nthe past 4 decades yet it doesn't change the way that the demonstration of\nviolence actually happens, unseen by the law. Violence can be mass controlled\nsometimes by higher authorities, however, to hold everything in line one must\n\"Microgovern\" over each movement occurring in every road of each square. To\naddress the butterfly effects impact in our setting, I made a unique model and\na theorized system to handle the issue utilizing deep learning. The model takes\nthe input of the CCTV video feeds and after drawing inference, recognizes if a\nviolent movement is going on. And hypothesized architecture aims towards\nprobability-driven computation of video feeds and reduces overhead from naively\ncomputing for every CCTV video feeds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 19:37:41 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Patel", "Mann", ""]]}, {"id": "2107.07596", "submitter": "Chen-Chou Lo", "authors": "Chen-Chou Lo and Patrick Vandewalle", "title": "Depth Estimation from Monocular Images and Sparse radar using Deep\n  Ordinal Regression Network", "comments": "Accepted to ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We integrate sparse radar data into a monocular depth estimation model and\nintroduce a novel preprocessing method for reducing the sparseness and limited\nfield of view provided by radar. We explore the intrinsic error of different\nradar modalities and show our proposed method results in more data points with\nreduced error. We further propose a novel method for estimating dense depth\nmaps from monocular 2D images and sparse radar measurements using deep learning\nbased on the deep ordinal regression network by Fu et al. Radar data are\nintegrated by first converting the sparse 2D points to a height-extended 3D\nmeasurement and then including it into the network using a late fusion\napproach. Experiments are conducted on the nuScenes dataset. Our experiments\ndemonstrate state-of-the-art performance in both day and night scenes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 20:17:48 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Lo", "Chen-Chou", ""], ["Vandewalle", "Patrick", ""]]}, {"id": "2107.07608", "submitter": "Qing Chen", "authors": "Qing Chen, Jian Zhang", "title": "Multi-Level Contrastive Learning for Few-Shot Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Contrastive learning is a discriminative approach that aims at grouping\nsimilar samples closer and diverse samples far from each other. It it an\nefficient technique to train an encoder generating distinguishable and\ninformative representations, and it may even increase the encoder's\ntransferability. Most current applications of contrastive learning benefit only\na single representation from the last layer of an encoder.In this paper, we\npropose a multi-level contrasitive learning approach which applies contrastive\nlosses at different layers of an encoder to learn multiple representations from\nthe encoder. Afterward, an ensemble can be constructed to take advantage of the\nmultiple representations for the downstream tasks. We evaluated the proposed\nmethod on few-shot learning problems and conducted experiments using the\nmini-ImageNet and the tiered-ImageNet datasets. Our model achieved the new\nstate-of-the-art results for both datasets, comparing to previous regular,\nensemble, and contrastive learing (single-level) based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 21:00:02 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chen", "Qing", ""], ["Zhang", "Jian", ""]]}, {"id": "2107.07647", "submitter": "Ian Colbert", "authors": "Ian Colbert, Ken Kreutz-Delgado, Srinjoy Das", "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image\n  Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 23:49:37 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 05:34:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Colbert", "Ian", ""], ["Kreutz-Delgado", "Ken", ""], ["Das", "Srinjoy", ""]]}, {"id": "2107.07651", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq\n  Joty, Caiming Xiong, Steven Hoi", "title": "Align before Fuse: Vision and Language Representation Learning with\n  Momentum Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale vision and language representation learning has shown promising\nimprovements on various vision-language tasks. Most existing methods employ a\ntransformer-based multimodal encoder to jointly model visual tokens\n(region-based image features) and word tokens. Because the visual tokens and\nword tokens are unaligned, it is challenging for the multimodal encoder to\nlearn image-text interactions. In this paper, we introduce a contrastive loss\nto ALign the image and text representations BEfore Fusing (ALBEF) them through\ncross-modal attention, which enables more grounded vision and language\nrepresentation learning. Unlike most existing methods, our method does not\nrequire bounding box annotations nor high-resolution images. In order to\nimprove learning from noisy web data, we propose momentum distillation, a\nself-training method which learns from pseudo-targets produced by a momentum\nmodel. We provide a theoretical analysis of ALBEF from a mutual information\nmaximization perspective, showing that different training tasks can be\ninterpreted as different ways to generate views for an image-text pair. ALBEF\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained\non orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves\nabsolute improvements of 2.37% and 3.84% compared to the state-of-the-art,\nwhile enjoying faster inference speed. Code and pre-trained models are\navailable at https://github.com/salesforce/ALBEF/.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 00:19:22 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Junnan", ""], ["Selvaraju", "Ramprasaath R.", ""], ["Gotmare", "Akhilesh Deepak", ""], ["Joty", "Shafiq", ""], ["Xiong", "Caiming", ""], ["Hoi", "Steven", ""]]}, {"id": "2107.07676", "submitter": "Zida Cheng", "authors": "Zida Cheng, Siheng Chen, Ya Zhang", "title": "Semi-supervised 3D Hand-Object Pose Estimation via Pose Dictionary\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand-object pose estimation is an important issue to understand the\ninteraction between human and environment. Current hand-object pose estimation\nmethods require detailed 3D labels, which are expensive and labor-intensive. To\ntackle the problem of data collection, we propose a semi-supervised 3D\nhand-object pose estimation method with two key techniques: pose dictionary\nlearning and an object-oriented coordinate system. The proposed pose dictionary\nlearning module can distinguish infeasible poses by reconstruction error,\nenabling unlabeled data to provide supervision signals. The proposed\nobject-oriented coordinate system can make 3D estimations equivariant to the\ncamera perspective. Experiments are conducted on FPHA and HO-3D datasets. Our\nmethod reduces estimation error by 19.5% / 24.9% for hands/objects compared to\nstraightforward use of labeled data on FPHA and outperforms several baseline\nmethods. Extensive experiments also validate the robustness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 02:50:17 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Cheng", "Zida", ""], ["Chen", "Siheng", ""], ["Zhang", "Ya", ""]]}, {"id": "2107.07684", "submitter": "Yasunori Ishii Mr", "authors": "Yasunori Ishii and Takayoshi Yamashita", "title": "CutDepth:Edge-aware Data Augmentation in Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is difficult to collect data on a large scale in a monocular depth\nestimation because the task requires the simultaneous acquisition of RGB images\nand depths. Data augmentation is thus important to this task. However, there\nhas been little research on data augmentation for tasks such as monocular depth\nestimation, where the transformation is performed pixel by pixel. In this\npaper, we propose a data augmentation method, called CutDepth. In CutDepth,\npart of the depth is pasted onto an input image during training. The method\nextends variations data without destroying edge features. Experiments\nobjectively and subjectively show that the proposed method outperforms\nconventional methods of data augmentation. The estimation accuracy is improved\nwith CutDepth even though there are few training data at long distances.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 03:20:49 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ishii", "Yasunori", ""], ["Yamashita", "Takayoshi", ""]]}, {"id": "2107.07695", "submitter": "Hao Wang", "authors": "Hao Wang, Euijoon Ahn, Jinman Kim", "title": "Self-Supervised Learning Framework for Remote Heart Rate Estimation\n  Using Spatiotemporal Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent supervised deep learning methods have shown that heart rate can be\nmeasured remotely using facial videos. However, the performance of these\nsupervised method are dependent on the availability of large-scale labelled\ndata and they have been limited to 2D deep learning architectures that do not\nfully exploit the 3D spatiotemporal information. To solve this problem, we\npresent a novel 3D self-supervised spatiotemporal learning framework for remote\nHR estimation on facial videos. Concretely, we propose a landmark-based spatial\naugmentation which splits the face into several informative parts based on the\nShafer's dichromatic reflection model and a novel sparsity-based temporal\naugmentation exploiting Nyquist-Shannon sampling theorem to enhance the signal\nmodelling ability. We evaluated our method on 3 public datasets and\noutperformed other self-supervised methods and achieved competitive accuracy\nwith the state-of-the-art supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:00:13 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Wang", "Hao", ""], ["Ahn", "Euijoon", ""], ["Kim", "Jinman", ""]]}, {"id": "2107.07699", "submitter": "Peng Zhao", "authors": "Peng Zhao, Chen Li, Md Mamunur Rahaman, Hechen Yang, Tao Jiang, Marcin\n  Grzegorzek", "title": "A Comparison of Deep Learning Classification Methods on Small-scale\n  Image Data set: from Convolutional Neural Networks to Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has made brilliant achievements in image\nclassification. However, image classification of small datasets is still not\nobtained good research results. This article first briefly explains the\napplication and characteristics of convolutional neural networks and visual\ntransformers. Meanwhile, the influence of small data set on classification and\nthe solution are introduced. Then a series of experiments are carried out on\nthe small datasets by using various models, and the problems of some models in\nthe experiments are discussed. Through the comparison of experimental results,\nthe recommended deep learning model is given according to the model application\nenvironment. Finally, we give directions for future work.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:13:10 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 01:57:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhao", "Peng", ""], ["Li", "Chen", ""], ["Rahaman", "Md Mamunur", ""], ["Yang", "Hechen", ""], ["Jiang", "Tao", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2107.07706", "submitter": "Chaojian Li", "authors": "Chaojian Li, Wuyang Chen, Yuchen Gu, Tianlong Chen, Yonggan Fu,\n  Zhangyang Wang, Yingyan Lin", "title": "DANCE: DAta-Network Co-optimization for Efficient Segmentation Model\n  Training and Inference", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation for scene understanding is nowadays widely demanded,\nraising significant challenges for the algorithm efficiency, especially its\napplications on resource-limited platforms. Current segmentation models are\ntrained and evaluated on massive high-resolution scene images (\"data level\")\nand suffer from the expensive computation arising from the required multi-scale\naggregation(\"network level\"). In both folds, the computational and energy costs\nin training and inference are notable due to the often desired large input\nresolutions and heavy computational burden of segmentation models. To this end,\nwe propose DANCE, general automated DAta-Network Co-optimization for Efficient\nsegmentation model training and inference. Distinct from existing efficient\nsegmentation approaches that focus merely on light-weight network design, DANCE\ndistinguishes itself as an automated simultaneous data-network co-optimization\nvia both input data manipulation and network architecture slimming.\nSpecifically, DANCE integrates automated data slimming which adaptively\ndownsamples/drops input images and controls their corresponding contribution to\nthe training loss guided by the images' spatial complexity. Such a downsampling\noperation, in addition to slimming down the cost associated with the input size\ndirectly, also shrinks the dynamic range of input object and context scales,\ntherefore motivating us to also adaptively slim the network to match the\ndownsampled data. Extensive experiments and ablating studies (on four SOTA\nsegmentation models with three popular segmentation datasets under two training\nsettings) demonstrate that DANCE can achieve \"all-win\" towards efficient\nsegmentation(reduced training cost, less expensive inference, and better mean\nIntersection-over-Union (mIoU)).\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:58:58 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Chaojian", ""], ["Chen", "Wuyang", ""], ["Gu", "Yuchen", ""], ["Chen", "Tianlong", ""], ["Fu", "Yonggan", ""], ["Wang", "Zhangyang", ""], ["Lin", "Yingyan", ""]]}, {"id": "2107.07707", "submitter": "Ming Xu", "authors": "Ming Xu, Tobias Fischer, Niko S\\\"underhauf, Michael Milford", "title": "Probabilistic Appearance-Invariant Topometric Localization with New\n  Place Awareness", "comments": "8 pages", "journal-ref": "IEEE Robotics and Automation Letters and IROS 2021", "doi": "10.1109/LRA.2021.3096745", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic state-estimation approaches offer a principled foundation for\ndesigning localization systems, because they naturally integrate sequences of\nimperfect motion and exteroceptive sensor data. Recently, probabilistic\nlocalization systems utilizing appearance-invariant visual place recognition\n(VPR) methods as the primary exteroceptive sensor have demonstrated\nstate-of-the-art performance in the presence of substantial appearance change.\nHowever, existing systems 1) do not fully utilize odometry data within the\nmotion models, and 2) are unable to handle route deviations, due to the\nassumption that query traverses exactly repeat the mapping traverse. To address\nthese shortcomings, we present a new probabilistic topometric localization\nsystem which incorporates full 3-dof odometry into the motion model and\nfurthermore, adds an \"off-map\" state within the state-estimation framework,\nallowing query traverses which feature significant route detours from the\nreference map to be successfully localized. We perform extensive evaluation on\nmultiple query traverses from the Oxford RobotCar dataset exhibiting both\nsignificant appearance change and deviations from routes previously traversed.\nIn particular, we evaluate performance on two practically relevant localization\ntasks: loop closure detection and global localization. Our approach achieves\nmajor performance improvements over both existing and improved state-of-the-art\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 05:01:40 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Xu", "Ming", ""], ["Fischer", "Tobias", ""], ["S\u00fcnderhauf", "Niko", ""], ["Milford", "Michael", ""]]}, {"id": "2107.07714", "submitter": "Evgeny Lavrik", "authors": "E. Lavrik, M. Shiroya, H.R. Schmidt, A. Toia and J.M. Heuser", "title": "Optical Inspection of the Silicon Micro-strip Sensors for the CBM\n  Experiment employing Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical inspection of 1191 silicon micro-strip sensors was performed using a\ncustom made optical inspection setup, employing a machine-learning based\napproach for the defect analysis and subsequent quality assurance. Furthermore,\nmetrological control of the sensor's surface was performed. In this manuscript,\nwe present the analysis of various sensor surface defects. Among these are\nimplant breaks, p-stop breaks, aluminium strip opens, aluminium strip shorts,\nsurface scratches, double metallization layer defects, passivation layer\ndefects, bias resistor defects as well as dust particle identification. The\ndefect detection was done using the application of Convolutional Deep Neural\nNetworks (CDNNs). From this, defective strips and defect clusters were\nidentified, as well as a 2D map of the defects using their geometrical\npositions on the sensor was performed. Based on the total number of defects\nfound on the sensor's surface, a method for the estimation of sensor's overall\nquality grade and quality score was proposed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 05:48:22 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Lavrik", "E.", ""], ["Shiroya", "M.", ""], ["Schmidt", "H. R.", ""], ["Toia", "A.", ""], ["Heuser", "J. M.", ""]]}, {"id": "2107.07746", "submitter": "Xu Luo", "authors": "Xu Luo, Longhui Wei, Liangjian Wen, Jinrong Yang, Lingxi Xie, Zenglin\n  Xu, Qi Tian", "title": "Rectifying the Shortcut Learning of Background: Shared Object\n  Concentration for Few-Shot Image Recognition", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-Shot image classification aims to utilize pretrained knowledge learned\nfrom a large-scale dataset to tackle a series of downstream classification\ntasks. Typically, each task involves only few training examples from brand-new\ncategories. This requires the pretraining models to focus on well-generalizable\nknowledge, but ignore domain-specific information. In this paper, we observe\nthat image background serves as a source of domain-specific knowledge, which is\na shortcut for models to learn in the source dataset, but is harmful when\nadapting to brand-new classes. To prevent the model from learning this shortcut\nknowledge, we propose COSOC, a novel Few-Shot Learning framework, to\nautomatically figure out foreground objects at both pretraining and evaluation\nstage. COSOC is a two-stage algorithm motivated by the observation that\nforeground objects from different images within the same class share more\nsimilar patterns than backgrounds. At the pretraining stage, for each class, we\ncluster contrastive-pretrained features of randomly cropped image patches, such\nthat crops containing only foreground objects can be identified by a single\ncluster. We then force the pretraining model to focus on found foreground\nobjects by a fusion sampling strategy; at the evaluation stage, among images in\neach training class of any few-shot task, we seek for shared contents and\nfilter out background. The recognized foreground objects of each class are used\nto match foreground of testing images. Extensive experiments tailored to\ninductive FSL tasks on two benchmarks demonstrate the state-of-the-art\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 07:46:41 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Luo", "Xu", ""], ["Wei", "Longhui", ""], ["Wen", "Liangjian", ""], ["Yang", "Jinrong", ""], ["Xie", "Lingxi", ""], ["Xu", "Zenglin", ""], ["Tian", "Qi", ""]]}, {"id": "2107.07752", "submitter": "Francesco Cognolato Mr", "authors": "Francesco Cognolato, Kieran O'Brien, Jin Jin, Simon Robinson, Frederik\n  B. Laun, Markus Barth, Steffen Bollmann", "title": "NeXtQSM -- A complete deep learning pipeline for data-consistent\n  quantitative susceptibility mapping trained with hybrid data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, outperforming traditional non-learning approaches in\nspeed and accuracy. However, many of the current deep learning approaches are\nnot data consistent, require in vivo training data or do not solve all steps of\nthe QSM processing pipeline. Here we aim to overcome these limitations and\ndeveloped a framework to solve the QSM processing steps jointly. We developed a\nnew hybrid training data generation method that enables the end-to-end training\nfor solving background field correction and dipole inversion in a\ndata-consistent fashion using a variational network that combines the QSM model\nterm and a learned regularizer. We demonstrate that NeXtQSM overcomes the\nlimitations of previous model-agnostic deep learning methods and show that\nNeXtQSM offers a complete deep learning based pipeline for computing robust,\nfast and accurate quantitative susceptibility maps.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 08:07:22 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Cognolato", "Francesco", ""], ["O'Brien", "Kieran", ""], ["Jin", "Jin", ""], ["Robinson", "Simon", ""], ["Laun", "Frederik B.", ""], ["Barth", "Markus", ""], ["Bollmann", "Steffen", ""]]}, {"id": "2107.07761", "submitter": "Francesco Ponzio", "authors": "Alessio Mascolini, Dario Cardamone, Francesco Ponzio, Santa Di\n  Cataldo, Elisa Ficarra", "title": "Exploiting generative self-supervised learning for the assessment of\n  biological images with lack of annotations: a COVID-19 case-study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computer-aided analysis of biological images typically requires extensive\ntraining on large-scale annotated datasets, which is not viable in many\nsituations. In this paper we present GAN-DL, a Discriminator Learner based on\nthe StyleGAN2 architecture, which we employ for self-supervised image\nrepresentation learning in the case of fluorescent biological images. We show\nthat Wasserstein Generative Adversarial Networks combined with linear Support\nVector Machines enable high-throughput compound screening based on raw images.\nWe demonstrate this by classifying active and inactive compounds tested for the\ninhibition of SARS-CoV-2 infection in VERO and HRCE cell lines. In contrast to\nprevious methods, our deep learning based approach does not require any\nannotation besides the one that is normally collected during the sample\npreparation process. We test our technique on the RxRx19a Sars-CoV-2 image\ncollection. The dataset consists of fluorescent images that were generated to\nassess the ability of regulatory-approved or in late-stage clinical trials\ncompound to modulate the in vitro infection from SARS-CoV-2 in both VERO and\nHRCE cell lines. We show that our technique can be exploited not only for\nclassification tasks, but also to effectively derive a dose response curve for\nthe tested treatments, in a self-supervised manner. Lastly, we demonstrate its\ngeneralization capabilities by successfully addressing a zero-shot learning\ntask, consisting in the categorization of four different cell types of the\nRxRx1 fluorescent images collection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 08:36:34 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 13:27:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mascolini", "Alessio", ""], ["Cardamone", "Dario", ""], ["Ponzio", "Francesco", ""], ["Di Cataldo", "Santa", ""], ["Ficarra", "Elisa", ""]]}, {"id": "2107.07778", "submitter": "Patrick H\\\"ubner", "authors": "Patrick H\\\"ubner, Martin Weinmann, Sven Wursthorn, Stefan Hinz", "title": "Pose Normalization of Indoor Mapping Datasets Partially Compliant to the\n  Manhattan World Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel pose normalization method for indoor\nmapping point clouds and triangle meshes that is robust against large fractions\nof the indoor mapping geometries deviating from an ideal Manhattan World\nstructure. In the case of building structures that contain multiple Manhattan\nWorld systems, the dominant Manhattan World structure supported by the largest\nfraction of geometries is determined and used for alignment. In a first step, a\nvertical alignment orienting a chosen axis to be orthogonal to horizontal floor\nand ceiling surfaces is conducted. Subsequently, a rotation around the\nresulting vertical axis is determined that aligns the dataset horizontally with\nthe coordinate axes. The proposed method is evaluated quantitatively against\nseveral publicly available indoor mapping datasets. Our implementation of the\nproposed procedure along with code for reproducing the evaluation will be made\navailable to the public upon acceptance for publication.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:07:01 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["H\u00fcbner", "Patrick", ""], ["Weinmann", "Martin", ""], ["Wursthorn", "Sven", ""], ["Hinz", "Stefan", ""]]}, {"id": "2107.07786", "submitter": "Elona Shatri Miss", "authors": "Elona Shatri and Gy\\\"orgy Fazekas", "title": "DoReMi: First glance at a universal OMR dataset", "comments": "7 pages, including 2 pages appendix. Accepted for publishing at the\n  3rd International Workshop on Reading Music Systems 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main challenges of Optical Music Recognition (OMR) come from the nature\nof written music, its complexity and the difficulty of finding an appropriate\ndata representation. This paper provides a first look at DoReMi, an OMR dataset\nthat addresses these challenges, and a baseline object detection model to\nassess its utility. Researchers often approach OMR following a set of small\nstages, given that existing data often do not satisfy broader research. We\nexamine the possibility of changing this tendency by presenting more metadata.\nOur approach complements existing research; hence DoReMi allows harmonisation\nwith two existing datasets, DeepScores and MUSCIMA++. DoReMi was generated\nusing a music notation software and includes over 6400 printed sheet music\nimages with accompanying metadata useful in OMR research. Our dataset provides\nOMR metadata, MIDI, MEI, MusicXML and PNG files, each aiding a different stage\nof OMR. We obtain 64% mean average precision (mAP) in object detection using\nhalf of the data. Further work includes re-iterating through the creation\nprocess to satisfy custom OMR models. While we do not assume to have solved the\nmain challenges in OMR, this dataset opens a new course of discussions that\nwould ultimately aid that goal.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:24:58 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Shatri", "Elona", ""], ["Fazekas", "Gy\u00f6rgy", ""]]}, {"id": "2107.07787", "submitter": "Nico Engel", "authors": "Nico Engel, Vasileios Belagiannis and Klaus Dietmayer", "title": "Attention-based Vehicle Self-Localization with HD Feature Maps", "comments": "Accepted for publication at 24th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a vehicle self-localization method using point-based deep neural\nnetworks. Our approach processes measurements and point features, i.e.\nlandmarks, from a high-definition digital map to infer the vehicle's pose. To\nlearn the best association and incorporate local information between the point\nsets, we propose an attention mechanism that matches the measurements to the\ncorresponding landmarks. Finally, we use this representation for the\npoint-cloud registration and the subsequent pose regression task. Furthermore,\nwe introduce a training simulation framework that artificially generates\nmeasurements and landmarks to facilitate the deployment process and reduce the\ncost of creating extensive datasets from real-world data. We evaluate our\nmethod on our dataset, as well as an adapted version of the Kitti odometry\ndataset, where we achieve superior performance compared to related approaches;\nand additionally show dominant generalization capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:25:25 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Engel", "Nico", ""], ["Belagiannis", "Vasileios", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2107.07789", "submitter": "Julien Tierny", "authors": "Mathieu Pont, Jules Vidal, Julie Delon and Julien Tierny", "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:27:49 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Pont", "Mathieu", ""], ["Vidal", "Jules", ""], ["Delon", "Julie", ""], ["Tierny", "Julien", ""]]}, {"id": "2107.07791", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Shreyas Kowshik and Oliver Stromann and Michael\n  Felsberg", "title": "Graph Representation Learning for Road Type Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2021.108174", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:32:58 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Gharaee", "Zahra", ""], ["Kowshik", "Shreyas", ""], ["Stromann", "Oliver", ""], ["Felsberg", "Michael", ""]]}, {"id": "2107.07797", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Changgong Zhang, Fangneng Zhan, Lei Zhang, Tien-Tsin Wong", "title": "Conditional Directed Graph Convolution for 3D Human Pose Estimation", "comments": null, "journal-ref": "ACM International Conference on Multimedia (ACM MM), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks have significantly improved 3D human pose\nestimation by representing the human skeleton as an undirected graph. However,\nthis representation fails to reflect the articulated characteristic of human\nskeletons as the hierarchical orders among the joints are not explicitly\npresented. In this paper, we propose to represent the human skeleton as a\ndirected graph with the joints as nodes and bones as edges that are directed\nfrom parent joints to child joints. By so doing, the directions of edges can\nexplicitly reflect the hierarchical relationships among the nodes. Based on\nthis representation, we adopt the spatial-temporal directed graph convolution\n(ST-DGConv) to extract features from 2D poses represented in a temporal\nsequence of directed graphs. We further propose a spatial-temporal conditional\ndirected graph convolution (ST-CondDGConv) to leverage varying non-local\ndependence for different poses by conditioning the graph topology on input\nposes. Altogether, we form a U-shaped network with ST-DGConv and ST-CondDGConv\nlayers, named U-shaped Conditional Directed Graph Convolutional Network\n(U-CondDGCN), for 3D human pose estimation from monocular videos. To evaluate\nthe effectiveness of our U-CondDGCN, we conducted extensive experiments on two\nchallenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both\nquantitative and qualitative results show that our method achieves top\nperformance. Also, ablation studies show that directed graphs can better\nexploit the hierarchy of articulated human skeletons than undirected graphs,\nand the conditional connections can yield adaptive graph topologies for\ndifferent kinds of poses.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:50:40 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Hu", "Wenbo", ""], ["Zhang", "Changgong", ""], ["Zhan", "Fangneng", ""], ["Zhang", "Lei", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "2107.07805", "submitter": "Ben Glocker", "authors": "Talha Qaiser, Stefan Winzeck, Theodore Barfoot, Tara Barwick, Simon J.\n  Doran, Martin F. Kaiser, Linda Wedlake, Nina Tunariu, Dow-Mu Koh, Christina\n  Messiou, Andrea Rockall, Ben Glocker", "title": "Multiple Instance Learning with Auxiliary Task Weighting for Multiple\n  Myeloma Classification", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole body magnetic resonance imaging (WB-MRI) is the recommended modality\nfor diagnosis of multiple myeloma (MM). WB-MRI is used to detect sites of\ndisease across the entire skeletal system, but it requires significant\nexpertise and is time-consuming to report due to the great number of images. To\naid radiological reading, we propose an auxiliary task-based multiple instance\nlearning approach (ATMIL) for MM classification with the ability to localize\nsites of disease. This approach is appealing as it only requires patient-level\nannotations where an attention mechanism is used to identify local regions with\nactive disease. We borrow ideas from multi-task learning and define an\nauxiliary task with adaptive reweighting to support and improve learning\nefficiency in the presence of data scarcity. We validate our approach on both\nsynthetic and real multi-center clinical data. We show that the MIL attention\nmodule provides a mechanism to localize bone regions while the adaptive\nreweighting of the auxiliary task considerably improves the performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:25:38 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Qaiser", "Talha", ""], ["Winzeck", "Stefan", ""], ["Barfoot", "Theodore", ""], ["Barwick", "Tara", ""], ["Doran", "Simon J.", ""], ["Kaiser", "Martin F.", ""], ["Wedlake", "Linda", ""], ["Tunariu", "Nina", ""], ["Koh", "Dow-Mu", ""], ["Messiou", "Christina", ""], ["Rockall", "Andrea", ""], ["Glocker", "Ben", ""]]}, {"id": "2107.07820", "submitter": "Puck De Haan", "authors": "Puck de Haan, Sindy L\\\"owe", "title": "Contrastive Predictive Coding for Anomaly Detection", "comments": "7 pages, ICML 2021 Workshop on Uncertainty and Robustness in Deep\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable detection of anomalies is crucial when deploying machine learning\nmodels in practice, but remains challenging due to the lack of labeled data. To\ntackle this challenge, contrastive learning approaches are becoming\nincreasingly popular, given the impressive results they have achieved in\nself-supervised representation learning settings. However, while most existing\ncontrastive anomaly detection and segmentation approaches have been applied to\nimages, none of them can use the contrastive losses directly for both anomaly\ndetection and segmentation. In this paper, we close this gap by making use of\nthe Contrastive Predictive Coding model (arXiv:1807.03748). We show that its\npatch-wise contrastive loss can directly be interpreted as an anomaly score,\nand how this allows for the creation of anomaly segmentation masks. The\nresulting model achieves promising results for both anomaly detection and\nsegmentation on the challenging MVTec-AD dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:04:35 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["de Haan", "Puck", ""], ["L\u00f6we", "Sindy", ""]]}, {"id": "2107.07826", "submitter": "Stavros Nousias", "authors": "Kostas Blekos, Stavros Nousias, Aris S Lalos", "title": "Efficient automated U-Net based tree crown delineation using UAV\n  multi-spectral imagery on embedded devices", "comments": "6 pages, 7 figures, 2 tables", "journal-ref": null, "doi": "10.1109/INDIN45582.2020.9442183", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineation approaches provide significant benefits to various domains,\nincluding agriculture, environmental and natural disasters monitoring. Most of\nthe work in the literature utilize traditional segmentation methods that\nrequire a large amount of computational and storage resources. Deep learning\nhas transformed computer vision and dramatically improved machine translation,\nthough it requires massive dataset for training and significant resources for\ninference. More importantly, energy-efficient embedded vision hardware\ndelivering real-time and robust performance is crucial in the aforementioned\napplication. In this work, we propose a U-Net based tree delineation method,\nwhich is effectively trained using multi-spectral imagery but can then\ndelineate single-spectrum images. The deep architecture that also performs\nlocalization, i.e., a class label corresponds to each pixel, has been\nsuccessfully used to allow training with a small set of segmented images. The\nground truth data were generated using traditional image denoising and\nsegmentation approaches. To be able to execute the proposed DNN efficiently in\nembedded platforms designed for deep learning approaches, we employ traditional\nmodel compression and acceleration methods. Extensive evaluation studies using\ndata collected from UAVs equipped with multi-spectral cameras demonstrate the\neffectiveness of the proposed methods in terms of delineation accuracy and\nexecution efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:17:36 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Blekos", "Kostas", ""], ["Nousias", "Stavros", ""], ["Lalos", "Aris S", ""]]}, {"id": "2107.07827", "submitter": "Sravan Danda", "authors": "Nagajothi Kannan, Sravan Danda, Aditya Challa, and Daya Sagar B S", "title": "A Theoretical Analysis of Granulometry-based Roughness Measures on\n  Cartosat DEMs", "comments": "Under review at IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of water bodies such as rivers is an important problem in the\nremote sensing community. A meaningful set of quantitative features reflecting\nthe geophysical properties help us better understand the formation and\nevolution of rivers. Typically, river sub-basins are analysed using Cartosat\nDigital Elevation Models (DEMs), obtained at regular time epochs. One of the\nuseful geophysical features of a river sub-basin is that of a roughness measure\non DEMs. However, to the best of our knowledge, there is not much literature\navailable on theoretical analysis of roughness measures. In this article, we\nrevisit the roughness measure on DEM data adapted from multiscale\ngranulometries in mathematical morphology, namely multiscale directional\ngranulometric index (MDGI). This measure was classically used to obtain\nshape-size analysis in greyscale images. In earlier works, MDGIs were\nintroduced to capture the characteristic surficial roughness of a river\nsub-basin along specific directions. Also, MDGIs can be efficiently computed\nand are known to be useful features for classification of river sub-basins. In\nthis article, we provide a theoretical analysis of a MDGI. In particular, we\ncharacterize non-trivial sufficient conditions on the structure of DEMs under\nwhich MDGIs are invariant. These properties are illustrated with some\nfictitious DEMs. We also provide connections to a discrete derivative of volume\nof a DEM. Based on these connections, we provide intuition as to why a MDGI is\nconsidered a roughness measure. Further, we experimentally illustrate on\nLower-Indus, Wardha, and Barmer river sub-basins that the proposed features\ncapture the characteristics of the river sub-basin.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:18:55 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Kannan", "Nagajothi", ""], ["Danda", "Sravan", ""], ["Challa", "Aditya", ""], ["S", "Daya Sagar B", ""]]}, {"id": "2107.07837", "submitter": "Runde Li", "authors": "Runde Li", "title": "Progressive Deep Video Dehazing without Explicit Alignment Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To solve the issue of video dehazing, there are two main tasks to attain: how\nto align adjacent frames to the reference frame; how to restore the reference\nframe. Some papers adopt explicit approaches (e.g., the Markov random field,\noptical flow, deformable convolution, 3D convolution) to align neighboring\nframes with the reference frame in feature space or image space, they then use\nvarious restoration methods to achieve the final dehazing results. In this\npaper, we propose a progressive alignment and restoration method for video\ndehazing. The alignment process aligns consecutive neighboring frames stage by\nstage without using the optical flow estimation. The restoration process is not\nonly implemented under the alignment process but also uses a refinement network\nto improve the dehazing performance of the whole network. The proposed networks\ninclude four fusion networks and one refinement network. To decrease the\nparameters of networks, three fusion networks in the first fusion stage share\nthe same parameters. Extensive experiments demonstrate that the proposed video\ndehazing method achieves outstanding performance against the-state-of-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:57:40 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Runde", ""]]}, {"id": "2107.07859", "submitter": "Hyeon Jeon", "authors": "Hyeon Jeon, Hyung-Kwon Ko, Jaemin Jo, Youngtaek Kim, and Jinwook Seo", "title": "Measuring and Explaining the Inter-Cluster Reliability of\n  Multidimensional Projections", "comments": "IEEE Transactions of Visualization and Computer Graphics (TVCG, Proc.\n  VIS 2021), to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 12:52:13 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 04:49:15 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 05:27:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jeon", "Hyeon", ""], ["Ko", "Hyung-Kwon", ""], ["Jo", "Jaemin", ""], ["Kim", "Youngtaek", ""], ["Seo", "Jinwook", ""]]}, {"id": "2107.07905", "submitter": "Hong-Xing Yu", "authors": "Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu", "title": "Unsupervised Discovery of Object Radiance Fields", "comments": "Project page: https://kovenyu.com/uorf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of inferring an object-centric scene representation from\na single image, aiming to derive a representation that explains the image\nformation process, captures the scene's 3D nature, and is learned without\nsupervision. Most existing methods on scene decomposition lack one or more of\nthese characteristics, due to the fundamental challenge in integrating the\ncomplex 3D-to-2D image formation process into powerful inference schemes like\ndeep networks. In this paper, we propose unsupervised discovery of Object\nRadiance Fields (uORF), integrating recent progresses in neural 3D scene\nrepresentations and rendering with deep inference networks for unsupervised 3D\nscene decomposition. Trained on multi-view RGB images without annotations, uORF\nlearns to decompose complex scenes with diverse, textured background from a\nsingle image. We show that uORF performs well on unsupervised 3D scene\nsegmentation, novel view synthesis, and scene editing on three datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 13:53:36 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Yu", "Hong-Xing", ""], ["Guibas", "Leonidas J.", ""], ["Wu", "Jiajun", ""]]}, {"id": "2107.07907", "submitter": "Kanglin Liu", "authors": "Kanglin Liu, Gaofeng Cao, Jiang Duan, Guoping Qiu", "title": "Lightness Modulated Deep Inverse Tone Mapping", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image HDR reconstruction or inverse tone mapping (iTM) is a\nchallenging task. In particular, recovering information in over-exposed regions\nis extremely difficult because details in such regions are almost completely\nlost. In this paper, we present a deep learning based iTM method that takes\nadvantage of the feature extraction and mapping power of deep convolutional\nneural networks (CNNs) and uses a lightness prior to modulate the CNN to better\nexploit observations in the surrounding areas of the over-exposed regions to\nenhance the quality of HDR image reconstruction. Specifically, we introduce a\nHierarchical Synthesis Network (HiSN) for inferring a HDR image from a LDR\ninput and a Lightness Adpative Modulation Network (LAMN) to incorporate the the\nlightness prior knowledge in the inferring process. The HiSN hierarchically\nsynthesizes the high-brightness component and the low-brightness component of\nthe HDR image whilst the LAMN uses a lightness adaptive mask that separates\ndetail-less saturated bright pixels from well-exposed lower light pixels to\nenable HiSN to better infer the missing information, particularly in the\ndifficult over-exposed detail-less areas. We present experimental results to\ndemonstrate the effectiveness of the new technique based on quantitative\nmeasures and visual comparisons. In addition, we present ablation studies of\nHiSN and visualization of the activation maps inside LAMN to help gain a deeper\nunderstanding of the internal working of the new iTM algorithm and explain why\nit can achieve much improved performance over state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 13:56:20 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liu", "Kanglin", ""], ["Cao", "Gaofeng", ""], ["Duan", "Jiang", ""], ["Qiu", "Guoping", ""]]}, {"id": "2107.07919", "submitter": "Simone Fabbrizzi", "authors": "Simone Fabbrizzi, Symeon Papadopoulos, Eirini Ntoutsi, Ioannis\n  Kompatsiaris", "title": "A Survey on Bias in Visual Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer Vision (CV) has achieved remarkable results, outperforming humans in\nseveral tasks. Nonetheless, it may result in major discrimination if not dealt\nwith proper care. CV systems highly depend on the data they are fed with and\ncan learn and amplify biases within such data. Thus, both the problems of\nunderstanding and discovering biases are of utmost importance. Yet, to date\nthere is no comprehensive survey on bias in visual datasets. To this end, this\nwork aims to: i) describe the biases that can affect visual datasets; ii)\nreview the literature on methods for bias discovery and quantification in\nvisual datasets; iii) discuss existing attempts to collect bias-aware visual\ndatasets. A key conclusion of our study is that the problem of bias discovery\nand quantification in visual datasets is still open and there is room for\nimprovement in terms of both methods and the range of biases that can be\naddressed; moreover, there is no such thing as a bias-free dataset, so\nscientists and practitioners must become aware of the biases in their datasets\nand make them explicit. To this end, we propose a checklist that can be used to\nspot different types of bias during visual dataset collection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 14:16:52 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Fabbrizzi", "Simone", ""], ["Papadopoulos", "Symeon", ""], ["Ntoutsi", "Eirini", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2107.07927", "submitter": "Xi Li", "authors": "Muhammed Muzammul and Xi Li", "title": "A Survey on Deep Domain Adaptation and Tiny Object Detection Challenges,\n  Techniques and Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey paper specially analyzed computer vision-based object detection\nchallenges and solutions by different techniques. We mainly highlighted object\ndetection by three different trending strategies, i.e., 1) domain adaptive deep\nlearning-based approaches (discrepancy-based, Adversarial-based,\nReconstruction-based, Hybrid). We examined general as well as tiny object\ndetection-related challenges and offered solutions by historical and\ncomparative analysis. In part 2) we mainly focused on tiny object detection\ntechniques (multi-scale feature learning, Data augmentation, Training strategy\n(TS), Context-based detection, GAN-based detection). In part 3), To obtain\nknowledge-able findings, we discussed different object detection methods, i.e.,\nconvolutions and convolutional neural networks (CNN), pooling operations with\ntrending types. Furthermore, we explained results with the help of some object\ndetection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD,\nwhich are generally considered the base bone of CV, CNN, and OD. We performed\ncomparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12,\nand ImageNet to analyze results and present findings. At the end, we showed\nfuture directions with existing challenges of the field. In the future, OD\nmethods and models can be analyzed for real-time object detection, tracking\nstrategies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 14:33:31 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Muzammul", "Muhammed", ""], ["Li", "Xi", ""]]}, {"id": "2107.07933", "submitter": "Loic Landrieu", "authors": "Vivien Sainte Fare Garnot and Loic Landrieu", "title": "Panoptic Segmentation of Satellite Image Time Series with Convolutional\n  Temporal Attention Networks", "comments": "Accepted at ICCV2021, PASTIS Dataset available at\n  https://github.com/VSainteuf/pastis-benchmark, PyTorch implementation at\n  https://github.com/VSainteuf/utae-paps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 14:49:01 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 13:49:54 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Garnot", "Vivien Sainte Fare", ""], ["Landrieu", "Loic", ""]]}, {"id": "2107.07943", "submitter": "Ryosuke Furuta", "authors": "Yugo Shimizu, Ryosuke Furuta, Delong Ouyang, Yukinobu Taniguchi, Ryota\n  Hinami, Shonosuke Ishiwatari", "title": "Painting Style-Aware Manga Colorization Based on Generative Adversarial\n  Networks", "comments": "Accepted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Japanese comics (called manga) are traditionally created in monochrome\nformat. In recent years, in addition to monochrome comics, full color comics, a\nmore attractive medium, have appeared. Unfortunately, color comics require\nmanual colorization, which incurs high labor costs. Although automatic\ncolorization methods have been recently proposed, most of them are designed for\nillustrations, not for comics. Unlike illustrations, since comics are composed\nof many consecutive images, the painting style must be consistent. To realize\nconsistent colorization, we propose here a semi-automatic colorization method\nbased on generative adversarial networks (GAN); the method learns the painting\nstyle of a specific comic from small amount of training data. The proposed\nmethod takes a pair of a screen tone image and a flat colored image as input,\nand outputs a colorized image. Experiments show that the proposed method\nachieves better performance than the existing alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:00:28 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Shimizu", "Yugo", ""], ["Furuta", "Ryosuke", ""], ["Ouyang", "Delong", ""], ["Taniguchi", "Yukinobu", ""], ["Hinami", "Ryota", ""], ["Ishiwatari", "Shonosuke", ""]]}, {"id": "2107.07975", "submitter": "Declan O'Regan", "authors": "Nicolo Savioli, Antonio de Marvao, Wenjia Bai, Shuo Wang, Stuart A.\n  Cook, Calvin W.L. Chin, Daniel Rueckert, Declan P. O'Regan", "title": "Joint Semi-supervised 3D Super-Resolution and Segmentation with Mixed\n  Adversarial Gaussian Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimising the analysis of cardiac structure and function requires accurate\n3D representations of shape and motion. However, techniques such as cardiac\nmagnetic resonance imaging are conventionally limited to acquiring contiguous\ncross-sectional slices with low through-plane resolution and potential\ninter-slice spatial misalignment. Super-resolution in medical imaging aims to\nincrease the resolution of images but is conventionally trained on features\nfrom low resolution datasets and does not super-resolve corresponding\nsegmentations. Here we propose a semi-supervised multi-task generative\nadversarial network (Gemini-GAN) that performs joint super-resolution of the\nimages and their labels using a ground truth of high resolution 3D cines and\nsegmentations, while an unsupervised variational adversarial mixture\nautoencoder (V-AMA) is used for continuous domain adaptation. Our proposed\napproach is extensively evaluated on two transnational multi-ethnic populations\nof 1,331 and 205 adults respectively, delivering an improvement on state of the\nart methods in terms of Dice index, peak signal to noise ratio, and structural\nsimilarity index measure. This framework also exceeds the performance of state\nof the art generative domain adaptation models on external validation (Dice\nindex 0.81 vs 0.74 for the left ventricle). This demonstrates how joint\nsuper-resolution and segmentation, trained on 3D ground-truth data with\ncross-domain generalization, enables robust precision phenotyping in diverse\npopulations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:42:39 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Savioli", "Nicolo", ""], ["de Marvao", "Antonio", ""], ["Bai", "Wenjia", ""], ["Wang", "Shuo", ""], ["Cook", "Stuart A.", ""], ["Chin", "Calvin W. L.", ""], ["Rueckert", "Daniel", ""], ["O'Regan", "Declan P.", ""]]}, {"id": "2107.07985", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang, Andreas Rimner, Joseph O. Deasy, and Harini Veeraraghavan", "title": "Unpaired cross-modality educed distillation (CMEDL) applied to CT lung\n  tumor segmentation", "comments": "This manuscript is current under review at IEEE Transactions on\n  Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust segmentation of lung cancers from CTs is needed to more\naccurately plan and deliver radiotherapy and to measure treatment response.\nThis is particularly difficult for tumors located close to mediastium, due to\nlow soft-tissue contrast. Therefore, we developed a new cross-modality educed\ndistillation (CMEDL) approach, using unpaired CT and MRI scans, whereby a\nteacher MRI network guides a student CT network to extract features that signal\nthe difference between foreground and background. Our contribution eliminates\ntwo requirements of distillation methods: (i) paired image sets by using an\nimage to image (I2I) translation and (ii) pre-training of the teacher network\nwith a large training set by using concurrent training of all networks. Our\nframework uses an end-to-end trained unpaired I2I translation, teacher, and\nstudent segmentation networks. Our framework can be combined with any I2I and\nsegmentation network. We demonstrate our framework's feasibility using 3\nsegmentation and 2 I2I methods. All networks were trained with 377 CT and 82\nT2w MRI from different sets of patients. Ablation tests and different\nstrategies for incorporating MRI information into CT were performed. Accuracy\nwas measured using Dice similarity (DSC), surface Dice (sDSC), and Hausdorff\ndistance at the 95$^{th}$ percentile (HD95). The CMEDL approach was\nsignificantly (p $<$ 0.001) more accurate than non-CMEDL methods,\nquantitatively and visually. It produced the highest segmentation accuracy\n(sDSC of 0.83 $\\pm$ 0.16 and HD95 of 5.20 $\\pm$ 6.86mm). CMEDL was also more\naccurate than using either pMRI's or the combination of CT's with pMRI's for\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:58:15 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jiang", "Jue", ""], ["Rimner", "Andreas", ""], ["Deasy", "Joseph O.", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "2107.07986", "submitter": "Fernando Alonso-Fernandez", "authors": "Elias Josse, Amanda Nerborg, Kevin Hernandez-Diaz, Fernando\n  Alonso-Fernandez", "title": "In-Bed Person Monitoring Using Thermal Infrared Sensors", "comments": "Accepted for publication at FedCSIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is expecting an aging population and shortage of healthcare\nprofessionals. This poses the problem of providing a safe and dignified life\nfor the elderly. Technological solutions involving cameras can contribute to\nsafety, comfort and efficient emergency responses, but they are invasive of\nprivacy. We use 'Griddy', a prototype with a Panasonic Grid-EYE, a\nlow-resolution infrared thermopile array sensor, which offers more privacy.\nMounted over a bed, it can determine if the user is on the bed or not without\nhuman interaction. For this purpose, two datasets were captured, one (480\nimages) under constant conditions, and a second one (200 images) under\ndifferent variations such as use of a duvet, sleeping with a pet, or increased\nroom temperature. We test three machine learning algorithms: Support Vector\nMachines (SVM), k-Nearest Neighbors (k-NN) and Neural Network (NN). With\n10-fold cross validation, the highest accuracy in the main dataset is for both\nSVM and k-NN (99%). The results with variable data show a lower reliability\nunder certain circumstances, highlighting the need of extra work to meet the\nchallenge of variations in the environment.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:59:07 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Josse", "Elias", ""], ["Nerborg", "Amanda", ""], ["Hernandez-Diaz", "Kevin", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "2107.07987", "submitter": "Weizhi  Lu", "authors": "Mingrui Chen, Weiyu Li, Weizhi Lu", "title": "Deep Learning to Ternary Hash Codes by Continuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been observed that {0,1,-1}-ternary codes which are simply\ngenerated from deep features by hard thresholding, tend to outperform\n{-1,1}-binary codes in image retrieval. To obtain better ternary codes, we for\nthe first time propose to jointly learn the features with the codes by\nappending a smoothed function to the networks. During training, the function\ncould evolve into a non-smoothed ternary function by a continuation method. The\nmethod circumvents the difficulty of directly training discrete functions and\nreduces the quantization errors of ternary codes. Experiments show that the\ngenerated codes indeed could achieve higher retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 16:02:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chen", "Mingrui", ""], ["Li", "Weiyu", ""], ["Lu", "Weizhi", ""]]}, {"id": "2107.07988", "submitter": "Hao Liang", "authors": "Hao Liang, Lulan Yu, Guikang Xu, Bhiksha Raj, Rita Singh", "title": "Controlled AutoEncoders to Generate Faces from Voices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multiple studies in the past have shown that there is a strong correlation\nbetween human vocal characteristics and facial features. However, existing\napproaches generate faces simply from voice, without exploring the set of\nfeatures that contribute to these observed correlations. A computational\nmethodology to explore this can be devised by rephrasing the question to: \"how\nmuch would a target face have to change in order to be perceived as the\noriginator of a source voice?\" With this in perspective, we propose a framework\nto morph a target face in response to a given voice in a way that facial\nfeatures are implicitly guided by learned voice-face correlation in this paper.\nOur framework includes a guided autoencoder that converts one face to another,\ncontrolled by a unique model-conditioning component called a gating controller\nwhich modifies the reconstructed face based on input voice recordings. We\nevaluate the framework on VoxCelab and VGGFace datasets through human subjects\nand face retrieval. Various experiments demonstrate the effectiveness of our\nproposed model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 16:04:29 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liang", "Hao", ""], ["Yu", "Lulan", ""], ["Xu", "Guikang", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "2107.08000", "submitter": "Yannis Avrithis", "authors": "Chull Hwan Song, Hye Joo Han, Yannis Avrithis", "title": "All the attention you need: Global-local, spatial-channel attention for\n  image retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address representation learning for large-scale instance-level image\nretrieval. Apart from backbone, training pipelines and loss functions, popular\napproaches have focused on different spatial pooling and attention mechanisms,\nwhich are at the core of learning a powerful global image representation. There\nare different forms of attention according to the interaction of elements of\nthe feature tensor (local and global) and the dimensions where it is applied\n(spatial and channel). Unfortunately, each study addresses only one or two\nforms of attention and applies it to different problems like classification,\ndetection or retrieval.\n  We present global-local attention module (GLAM), which is attached at the end\nof a backbone network and incorporates all four forms of attention: local and\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\npooling, we learn a powerful embedding for image retrieval. Focusing on global\ndescriptors, we provide empirical evidence of the interaction of all forms of\nattention and improve the state of the art on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 16:39:13 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Song", "Chull Hwan", ""], ["Han", "Hye Joo", ""], ["Avrithis", "Yannis", ""]]}, {"id": "2107.08031", "submitter": "Lina Achaji", "authors": "Lina Achaji, Julien Moreau, Thibault Fouqueray, Francois Aioun,\n  Francois Charpillet", "title": "Is attention to bounding boxes all you need for pedestrian action\n  prediction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human driver is no longer the only one concerned with the complexity of\nthe driving scenarios. Autonomous vehicles (AV) are similarly becoming involved\nin the process. Nowadays, the development of AV in urban places underpins\nessential safety concerns for vulnerable road users (VRUs) such as pedestrians.\nTherefore, to make the roads safer, it is critical to classify and predict\ntheir future behavior. In this paper, we present a framework based on multiple\nvariations of the Transformer models to reason attentively about the dynamic\nevolution of the pedestrians' past trajectory and predict its future actions of\ncrossing or not crossing the street. We proved that using only bounding boxes\nas input to our model can outperform the previous state-of-the-art models and\nreach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset\nup to two seconds ahead in the future. In addition, we introduced a large-size\nsimulated dataset (CP2A) using CARLA for action prediction. Our model has\nsimilarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.\nInterestingly, we showed that pre-training our Transformer model on the\nsimulated dataset and then fine-tuning it on the real dataset can be very\neffective for the action prediction task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:47:32 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Achaji", "Lina", ""], ["Moreau", "Julien", ""], ["Fouqueray", "Thibault", ""], ["Aioun", "Francois", ""], ["Charpillet", "Francois", ""]]}, {"id": "2107.08037", "submitter": "Guillaume Le Moing", "authors": "Guillaume Le Moing and Jean Ponce and Cordelia Schmid", "title": "CCVS: Context-aware Controllable Video Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This presentation introduces a self-supervised learning approach to the\nsynthesis of new video clips from old ones, with several new key elements for\nimproved spatial resolution and realism: It conditions the synthesis process on\ncontextual information for temporal continuity and ancillary information for\nfine control. The prediction model is doubly autoregressive, in the latent\nspace of an autoencoder for forecasting, and in image space for updating\ncontextual information, which is also used to enforce spatio-temporal\nconsistency through a learnable optical flow module. Adversarial training of\nthe autoencoder in the appearance and temporal domains is used to further\nimprove the realism of its output. A quantizer inserted between the encoder and\nthe transformer in charge of forecasting future frames in latent space (and its\ninverse inserted between the transformer and the decoder) adds even more\nflexibility by affording simple mechanisms for handling multimodal ancillary\ninformation for controlling the synthesis process (eg, a few sample frames, an\naudio track, a trajectory in image space) and taking into account the\nintrinsically uncertain nature of the future by allowing multiple predictions.\nExperiments with an implementation of the proposed approach give very good\nqualitative and quantitative results on multiple tasks and standard benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:57:44 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Moing", "Guillaume Le", ""], ["Ponce", "Jean", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2107.08039", "submitter": "Zhizhong Li", "authors": "Zhizhong Li, Avinash Ravichandran, Charless Fowlkes, Marzia Polito,\n  Rahul Bhotika, Stefano Soatto", "title": "Representation Consolidation for Training Expert Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, distillation has been used to train a student model to emulate\nthe input/output functionality of a teacher. A more useful goal than emulation,\nyet under-explored, is for the student to learn feature representations that\ntransfer well to future tasks. However, we observe that standard distillation\nof task-specific teachers actually *reduces* the transferability of student\nrepresentations to downstream tasks. We show that a multi-head, multi-task\ndistillation method using an unlabeled proxy dataset and a generalist teacher\nis sufficient to consolidate representations from task-specific teacher(s) and\nimprove downstream performance, outperforming the teacher(s) and the strong\nbaseline of ImageNet pretrained features. Our method can also combine the\nrepresentational knowledge of multiple teachers trained on one or multiple\ndomains into a single model, whose representation is improved on all teachers'\ndomain(s).\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:58:18 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Zhizhong", ""], ["Ravichandran", "Avinash", ""], ["Fowlkes", "Charless", ""], ["Polito", "Marzia", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "2107.08111", "submitter": "Holger Roth", "authors": "Holger R. Roth, Dong Yang, Wenqi Li, Andriy Myronenko, Wentao Zhu,\n  Ziyue Xu, Xiaosong Wang, Daguang Xu", "title": "Federated Whole Prostate Segmentation in MRI with Personalized Neural\n  Architectures", "comments": "MICCAI 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building robust deep learning-based models requires diverse training data,\nideally from several sources. However, these datasets cannot be combined easily\nbecause of patient privacy concerns or regulatory hurdles, especially if\nmedical data is involved. Federated learning (FL) is a way to train machine\nlearning models without the need for centralized datasets. Each FL client\ntrains on their local data while only sharing model parameters with a global\nserver that aggregates the parameters from all clients. At the same time, each\nclient's data can exhibit differences and inconsistencies due to the local\nvariation in the patient population, imaging equipment, and acquisition\nprotocols. Hence, the federated learned models should be able to adapt to the\nlocal particularities of a client's data. In this work, we combine FL with an\nAutoML technique based on local neural architecture search by training a\n\"supernet\". Furthermore, we propose an adaptation scheme to allow for\npersonalized model architectures at each FL client's site. The proposed method\nis evaluated on four different datasets from 3D prostate MRI and shown to\nimprove the local models' performance after adaptation through selecting an\noptimal path through the AutoML supernet.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 20:35:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Roth", "Holger R.", ""], ["Yang", "Dong", ""], ["Li", "Wenqi", ""], ["Myronenko", "Andriy", ""], ["Zhu", "Wentao", ""], ["Xu", "Ziyue", ""], ["Wang", "Xiaosong", ""], ["Xu", "Daguang", ""]]}, {"id": "2107.08120", "submitter": "Yilin Liu", "authors": "Yilin Liu, Yong Chen, Pew-Thian Yap", "title": "Real-Time Mapping of Tissue Properties for Magnetic Resonance\n  Fingerprinting", "comments": "Accepted by 2021 MICCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance Fingerprinting (MRF) is a relatively new multi-parametric\nquantitative imaging method that involves a two-step process: (i)\nreconstructing a series of time frames from highly-undersampled non-Cartesian\nspiral k-space data and (ii) pattern matching using the time frames to infer\ntissue properties (e.g., T1 and T2 relaxation times). In this paper, we\nintroduce a novel end-to-end deep learning framework to seamlessly map the\ntissue properties directly from spiral k-space MRF data, thereby avoiding\ntime-consuming processing such as the nonuniform fast Fourier transform (NUFFT)\nand the dictionary-based Fingerprint matching. Our method directly consumes the\nnon-Cartesian k- space data, performs adaptive density compensation, and\npredicts multiple tissue property maps in one forward pass. Experiments on both\n2D and 3D MRF data demonstrate that quantification accuracy comparable to\nstate-of-the-art methods can be accomplished within 0.5 second, which is 1100\nto 7700 times faster than the original MRF framework. The proposed method is\nthus promising for facilitating the adoption of MRF in clinical settings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 21:05:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Yilin", ""], ["Chen", "Yong", ""], ["Yap", "Pew-Thian", ""]]}, {"id": "2107.08142", "submitter": "Peter Ondruska", "authors": "Ashesh Jain, Luca Del Pero, Hugo Grimmett, Peter Ondruska", "title": "Autonomy 2.0: Why is self-driving always 5 years away?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 23:20:26 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 22:51:45 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jain", "Ashesh", ""], ["Del Pero", "Luca", ""], ["Grimmett", "Hugo", ""], ["Ondruska", "Peter", ""]]}, {"id": "2107.08186", "submitter": "Hengli Wang", "authors": "Hengli Wang, Rui Fan, Ming Liu", "title": "Co-Teaching: An Ark to Unsupervised Stereo Matching", "comments": "5 pages, 3 figures and 2 tables. This paper is accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is a key component of autonomous driving perception. Recent\nunsupervised stereo matching approaches have received adequate attention due to\ntheir advantage of not requiring disparity ground truth. These approaches,\nhowever, perform poorly near occlusions. To overcome this drawback, in this\npaper, we propose CoT-Stereo, a novel unsupervised stereo matching approach.\nSpecifically, we adopt a co-teaching framework where two networks interactively\nteach each other about the occlusions in an unsupervised fashion, which greatly\nimproves the robustness of unsupervised stereo matching. Extensive experiments\non the KITTI Stereo benchmarks demonstrate the superior performance of\nCoT-Stereo over all other state-of-the-art unsupervised stereo matching\napproaches in terms of both accuracy and speed. Our project webpage is\nhttps://sites.google.com/view/cot-stereo.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 05:33:39 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Hengli", ""], ["Fan", "Rui", ""], ["Liu", "Ming", ""]]}, {"id": "2107.08187", "submitter": "Hengli Wang", "authors": "Hengli Wang, Rui Fan, Ming Liu", "title": "SCV-Stereo: Learning Stereo Matching from a Sparse Cost Volume", "comments": "5 pages, 3 figures and 2 tables. This paper is accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN)-based stereo matching approaches generally\nrequire a dense cost volume (DCV) for disparity estimation. However, generating\nsuch cost volumes is computationally-intensive and memory-consuming, hindering\nCNN training and inference efficiency. To address this problem, we propose\nSCV-Stereo, a novel CNN architecture, capable of learning dense stereo matching\nfrom sparse cost volume (SCV) representations. Our inspiration is derived from\nthe fact that DCV representations are somewhat redundant and can be replaced\nwith SCV representations. Benefiting from these SCV representations, our\nSCV-Stereo can update disparity estimations in an iterative fashion for\naccurate and efficient stereo matching. Extensive experiments carried out on\nthe KITTI Stereo benchmarks demonstrate that our SCV-Stereo can significantly\nminimize the trade-off between accuracy and efficiency for stereo matching. Our\nproject page is https://sites.google.com/view/scv-stereo.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 05:45:44 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Hengli", ""], ["Fan", "Rui", ""], ["Liu", "Ming", ""]]}, {"id": "2107.08192", "submitter": "Yunqing Hu", "authors": "Yunqing Hu, Xuan Jin, Yin Zhang, Haiwen Hong, Jingfeng Zhang, Yuan He,\n  Hui Xue", "title": "RAMS-Trans: Recurrent Attention Multi-scale Transformer forFine-grained\n  Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fine-grained image recognition (FGIR), the localization and amplification\nof region attention is an important factor, which has been explored a lot by\nconvolutional neural networks (CNNs) based approaches. The recently developed\nvision transformer (ViT) has achieved promising results on computer vision\ntasks. Compared with CNNs, Image sequentialization is a brand new manner.\nHowever, ViT is limited in its receptive field size and thus lacks local\nattention like CNNs due to the fixed size of its patches, and is unable to\ngenerate multi-scale features to learn discriminative region attention. To\nfacilitate the learning of discriminative region attention without box/part\nannotations, we use the strength of the attention weights to measure the\nimportance of the patch tokens corresponding to the raw images. We propose the\nrecurrent attention multi-scale transformer (RAMS-Trans), which uses the\ntransformer's self-attention to recursively learn discriminative region\nattention in a multi-scale manner. Specifically, at the core of our approach\nlies the dynamic patch proposal module (DPPM) guided region amplification to\ncomplete the integration of multi-scale image patches. The DPPM starts with the\nfull-size image patches and iteratively scales up the region attention to\ngenerate new patches from global to local by the intensity of the attention\nweights generated at each scale as an indicator. Our approach requires only the\nattention weights that come with ViT itself and can be easily trained\nend-to-end. Extensive experiments demonstrate that RAMS-Trans performs better\nthan concurrent works, in addition to efficient CNN models, achieving\nstate-of-the-art results on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 06:22:20 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hu", "Yunqing", ""], ["Jin", "Xuan", ""], ["Zhang", "Yin", ""], ["Hong", "Haiwen", ""], ["Zhang", "Jingfeng", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "2107.08211", "submitter": "Soumyadeep Ghosh", "authors": "Soumyadeep Ghosh, Sanjay Kumar, Janu Verma and Awanish Kumar", "title": "Self Training with Ensemble of Teacher Models", "comments": null, "journal-ref": "IJCAI 2021 Workshop on Weakly Supervised Representation Learning", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to train robust deep learning models, large amounts of labelled data\nis required. However, in the absence of such large repositories of labelled\ndata, unlabeled data can be exploited for the same. Semi-Supervised learning\naims to utilize such unlabeled data for training classification models. Recent\nprogress of self-training based approaches have shown promise in this area,\nwhich leads to this study where we utilize an ensemble approach for the same. A\nby-product of any semi-supervised approach may be loss of calibration of the\ntrained model especially in scenarios where unlabeled data may contain\nout-of-distribution samples, which leads to this investigation on how to adapt\nto such effects. Our proposed algorithm carefully avoids common pitfalls in\nutilizing unlabeled data and leads to a more accurate and calibrated supervised\nmodel compared to vanilla self-training based student-teacher algorithms. We\nperform several experiments on the popular STL-10 database followed by an\nextensive analysis of our approach and study its effects on model accuracy and\ncalibration.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 09:44:09 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ghosh", "Soumyadeep", ""], ["Kumar", "Sanjay", ""], ["Verma", "Janu", ""], ["Kumar", "Awanish", ""]]}, {"id": "2107.08221", "submitter": "Lukas Schott", "authors": "Lukas Schott, Julius von K\\\"ugelgen, Frederik Tr\\\"auble, Peter Gehler,\n  Chris Russell, Matthias Bethge, Bernhard Sch\\\"olkopf, Francesco Locatello,\n  Wieland Brendel", "title": "Visual Representation Learning Does Not Generalize Strongly Within the\n  Same Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 11:24:18 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 06:48:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Schott", "Lukas", ""], ["von K\u00fcgelgen", "Julius", ""], ["Tr\u00e4uble", "Frederik", ""], ["Gehler", "Peter", ""], ["Russell", "Chris", ""], ["Bethge", "Matthias", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Locatello", "Francesco", ""], ["Brendel", "Wieland", ""]]}, {"id": "2107.08228", "submitter": "Lisha Tang", "authors": "Lisha Tang, Yi Wang, Lap-Pui Chau", "title": "Looking Twice for Partial Clues: Weakly-supervised Part-Mentored\n  Attention Network for Vehicle Re-Identification", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (Re-ID) is to retrieve images of the same vehicle\nacross different cameras. Two key challenges lie in the subtle inter-instance\ndiscrepancy caused by near-duplicate identities and the large intra-instance\nvariance caused by different views. Since the holistic appearance suffers from\nviewpoint variation and distortion, part-level feature learning has been\nintroduced to enhance vehicle description. However, existing approaches to\nlocalize and amplify significant parts often fail to handle spatial\nmisalignment as well as occlusion and require expensive annotations. In this\npaper, we propose a weakly supervised Part-Mentored Attention Network (PMANet)\ncomposed of a Part Attention Network (PANet) for vehicle part localization with\nself-attention and a Part-Mentored Network (PMNet) for mentoring the global and\nlocal feature aggregation. Firstly, PANet is introduced to predict a foreground\nmask and pinpoint $K$ prominent vehicle parts only with weak identity\nsupervision. Secondly, we propose a PMNet to learn global and part-level\nfeatures with multi-scale attention and aggregate them in $K$ main-partial\ntasks via part transfer. Like humans who first differentiate objects with\ngeneral information and then observe salient parts for more detailed clues,\nPANet and PMNet construct a two-stage attention structure to perform a\ncoarse-to-fine search among identities. Finally, we address this Re-ID issue as\na multi-task problem, including global feature learning, identity\nclassification, and part transfer. We adopt Homoscedastic Uncertainty to learn\nthe optimal weighing of different losses. Comprehensive experiments are\nconducted on two benchmark datasets. Our approach outperforms recent\nstate-of-the-art methods by averagely 2.63% in CMC@1 on VehicleID and 2.2% in\nmAP on VeRi776. Results on occluded test sets also demonstrate the\ngeneralization ability of PMANet.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 12:19:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tang", "Lisha", ""], ["Wang", "Yi", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2107.08246", "submitter": "Senthil Yogamani", "authors": "Saravanabalagi Ramachandran, Ganesh Sistu, John McDonald and Senthil\n  Yogamani", "title": "Woodscape Fisheye Semantic Segmentation for Autonomous Driving -- CVPR\n  2021 OmniCV Workshop Challenge", "comments": "Workshop on Omnidirectional Computer Vision (OmniCV) at Conference on\n  Computer Vision and Pattern Recognition (CVPR) 2021. Presentation video is\n  available at https://youtu.be/xa7Fl2mD4CA?t=12253", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the WoodScape fisheye semantic segmentation challenge for\nautonomous driving which was held as part of the CVPR 2021 Workshop on\nOmnidirectional Computer Vision (OmniCV). This challenge is one of the first\nopportunities for the research community to evaluate the semantic segmentation\ntechniques targeted for fisheye camera perception. Due to strong radial\ndistortion standard models don't generalize well to fisheye images and hence\nthe deformations in the visual appearance of objects and entities needs to be\nencoded implicitly or as explicit knowledge. This challenge served as a medium\nto investigate the challenges and new methodologies to handle the complexities\nwith perception on fisheye images. The challenge was hosted on CodaLab and used\nthe recently released WoodScape dataset comprising of 10k samples. In this\npaper, we provide a summary of the competition which attracted the\nparticipation of 71 global teams and a total of 395 submissions. The top teams\nrecorded significantly improved mean IoU and accuracy scores over the baseline\nPSPNet with ResNet-50 backbone. We summarize the methods of winning algorithms\nand analyze the failure cases. We conclude by providing future directions for\nthe research.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 14:32:58 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ramachandran", "Saravanabalagi", ""], ["Sistu", "Ganesh", ""], ["McDonald", "John", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2107.08274", "submitter": "Yijin Huang", "authors": "Yijin Huang, Li Lin, Pujin Cheng, Junyan Lyu, Xiaoying Tang", "title": "Lesion-based Contrastive Learning for Diabetic Retinopathy Grading from\n  Fundus Images", "comments": "10 pages, 2 figures, MICCAI2021 early accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manually annotating medical images is extremely expensive, especially for\nlarge-scale datasets. Self-supervised contrastive learning has been explored to\nlearn feature representations from unlabeled images. However, unlike natural\nimages, the application of contrastive learning to medical images is relatively\nlimited. In this work, we propose a self-supervised framework, namely\nlesion-based contrastive learning for automated diabetic retinopathy (DR)\ngrading. Instead of taking entire images as the input in the common contrastive\nlearning scheme, lesion patches are employed to encourage the feature extractor\nto learn representations that are highly discriminative for DR grading. We also\ninvestigate different data augmentation operations in defining our contrastive\nprediction task. Extensive experiments are conducted on the publicly-accessible\ndataset EyePACS, demonstrating that our proposed framework performs\noutstandingly on DR grading in terms of both linear evaluation and transfer\ncapacity evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 16:30:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Huang", "Yijin", ""], ["Lin", "Li", ""], ["Cheng", "Pujin", ""], ["Lyu", "Junyan", ""], ["Tang", "Xiaoying", ""]]}, {"id": "2107.08305", "submitter": "Samira Zare", "authors": "Samira Zare, Hien Van Nguyen", "title": "PICASO: Permutation-Invariant Cascaded Attentional Set Operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set-input deep networks have recently drawn much interest in computer vision\nand machine learning. This is in part due to the increasing number of important\ntasks such as meta-learning, clustering, and anomaly detection that are defined\non set inputs. These networks must take an arbitrary number of input samples\nand produce the output invariant to the input set permutation. Several\nalgorithms have been recently developed to address this urgent need. Our paper\nanalyzes these algorithms using both synthetic and real-world datasets, and\nshows that they are not effective in dealing with common data variations such\nas image translation or viewpoint change. To address this limitation, we\npropose a permutation-invariant cascaded attentional set operator (PICASO). The\ngist of PICASO is a cascade of multihead attention blocks with dynamic\ntemplates. The proposed operator is a stand-alone module that can be adapted\nand extended to serve different machine learning tasks. We demonstrate the\nutilities of PICASO in four diverse scenarios: (i) clustering, (ii) image\nclassification under novel viewpoints, (iii) image anomaly detection, and (iv)\nstate prediction. PICASO increases the SmallNORB image classification accuracy\nwith novel viewpoints by about 10% points. For set anomaly detection on CelebA\ndataset, our model improves the areas under ROC and PR curves dataset by about\n22% and 10%, respectively. For the state prediction on CLEVR dataset, it\nimproves the AP by about 40%.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 19:21:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zare", "Samira", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "2107.08311", "submitter": "Xing Di", "authors": "Xing Di, Shuowen Hu and Vishal M. Patel", "title": "Heterogeneous Face Frontalization via Domain Agnostic Learning", "comments": "This work is accepted in IEEE conference on Automatic Face and\n  Gesture Recognition 2021 (FG2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep convolutional neural networks (DCNNs) have shown\nimpressive performance improvements on thermal to visible face synthesis and\nmatching problems. However, current DCNN-based synthesis models do not perform\nwell on thermal faces with large pose variations. In order to deal with this\nproblem, heterogeneous face frontalization methods are needed in which a model\ntakes a thermal profile face image and generates a frontal visible face. This\nis an extremely difficult problem due to the large domain as well as large pose\ndiscrepancies between the two modalities. Despite its applications in\nbiometrics and surveillance, this problem is relatively unexplored in the\nliterature. We propose a domain agnostic learning-based generative adversarial\nnetwork (DAL-GAN) which can synthesize frontal views in the visible domain from\nthermal faces with pose variations. DAL-GAN consists of a generator with an\nauxiliary classifier and two discriminators which capture both local and global\ntexture discriminations for better synthesis. A contrastive constraint is\nenforced in the latent space of the generator with the help of a dual-path\ntraining strategy, which improves the feature vector discrimination. Finally, a\nmulti-purpose loss function is utilized to guide the network in synthesizing\nidentity preserving cross-domain frontalization. Extensive experimental results\ndemonstrate that DAL-GAN can generate better quality frontal views compared to\nthe other baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 20:41:41 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Di", "Xing", ""], ["Hu", "Shuowen", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2107.08323", "submitter": "Khoa Vo Ho Viet", "authors": "Viet-Khoa Vo-Ho, Ngan Le, Kashu Yamazaki, Akihiro Sugimoto, Minh-Triet\n  Tran", "title": "Agent-Environment Network for Temporal Action Proposal Generation", "comments": "Accepted in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 23:24:49 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Vo-Ho", "Viet-Khoa", ""], ["Le", "Ngan", ""], ["Yamazaki", "Kashu", ""], ["Sugimoto", "Akihiro", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "2107.08330", "submitter": "Aishik Konwer", "authors": "Aishik Konwer, Joseph Bae, Gagandeep Singh, Rishabh Gattu, Syed Ali,\n  Jeremy Green, Tej Phatak, Prateek Prasanna", "title": "Attention-based Multi-scale Gated Recurrent Encoder with Novel\n  Correlation Loss for COVID-19 Progression Prediction", "comments": "The paper is early accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 image analysis has mostly focused on diagnostic tasks using single\ntimepoint scans acquired upon disease presentation or admission. We present a\ndeep learning-based approach to predict lung infiltrate progression from serial\nchest radiographs (CXRs) of COVID-19 patients. Our method first utilizes\nconvolutional neural networks (CNNs) for feature extraction from patches within\nthe concerned lung zone, and also from neighboring and remote boundary regions.\nThe framework further incorporates a multi-scale Gated Recurrent Unit (GRU)\nwith a correlation module for effective predictions. The GRU accepts CNN\nfeature vectors from three different areas as input and generates a fused\nrepresentation. The correlation module attempts to minimize the correlation\nloss between hidden representations of concerned and neighboring area feature\nvectors, while maximizing the loss between the same from concerned and remote\nregions. Further, we employ an attention module over the output hidden states\nof each encoder timepoint to generate a context vector. This vector is used as\nan input to a decoder module to predict patch severity grades at a future\ntimepoint. Finally, we ensemble the patch classification scores to calculate\npatient-wise grades. Specifically, our framework predicts zone-wise disease\nseverity for a patient on a given day by learning representations from the\nprevious temporal CXRs. Our novel multi-institutional dataset comprises\nsequential CXR scans from N=93 patients. Our approach outperforms transfer\nlearning and radiomic feature-based baseline approaches on this dataset.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 00:37:55 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Konwer", "Aishik", ""], ["Bae", "Joseph", ""], ["Singh", "Gagandeep", ""], ["Gattu", "Rishabh", ""], ["Ali", "Syed", ""], ["Green", "Jeremy", ""], ["Phatak", "Tej", ""], ["Prasanna", "Prateek", ""]]}, {"id": "2107.08355", "submitter": "Liupeng Lin", "authors": "Liupeng Lin, Jie Li, Huanfeng Shen, Lingli Zhao, Qiangqiang Yuan,\n  Xinghua Li", "title": "Fully Polarimetric SAR and Single-Polarization SAR Image Fusion Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data fusion technology aims to aggregate the characteristics of different\ndata and obtain products with multiple data advantages. To solves the problem\nof reduced resolution of PolSAR images due to system limitations, we propose a\nfully polarimetric synthetic aperture radar (PolSAR) images and\nsingle-polarization synthetic aperture radar SAR (SinSAR) images fusion network\nto generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the\npolarimetric information of the low-resolution PolSAR (LR-PolSAR) image and the\nspatial information of the high-resolution single-polarization SAR (HR-SinSAR)\nimage, we propose a fusion framework for joint LR-PolSAR image and HR-SinSAR\nimage and design a cross-attention mechanism to extract features from the joint\ninput data. Besides, based on the physical imaging mechanism, we designed the\nPolSAR polarimetric loss function for constrained network training. The\nexperimental results confirm the superiority of fusion network over traditional\nalgorithms. The average PSNR is increased by more than 3.6db, and the average\nMAE is reduced to less than 0.07. Experiments on polarimetric decomposition and\npolarimetric signature show that it maintains polarimetric information well.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 03:51:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lin", "Liupeng", ""], ["Li", "Jie", ""], ["Shen", "Huanfeng", ""], ["Zhao", "Lingli", ""], ["Yuan", "Qiangqiang", ""], ["Li", "Xinghua", ""]]}, {"id": "2107.08369", "submitter": "Siddha Ganju", "authors": "Sayak Paul and Siddha Ganju", "title": "Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised\n  Learning", "comments": "Equal authorship. This is a work in progress and is a submission to\n  the Emerging Techniques in Computational Intelligence (ETCI) competition on\n  Flood Detection. Code and models are available on GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. Accurate and\nrobust flood detection including delineating open water flood areas and\nidentifying flood levels can aid in disaster response and mitigation. However,\nestimating flood levels remotely is of essence as physical access to flooded\nareas is limited and the ability to deploy instruments in potential flood zones\ncan be dangerous. Aligning flood extent mapping with local topography can\nprovide a plan-of-action that the disaster response team can consider. Thus,\nremote flood level estimation via satellites like Sentinel-1 can prove to be\nremedial. The Emerging Techniques in Computational Intelligence (ETCI)\ncompetition on Flood Detection tasked participants with predicting flooded\npixels after training with synthetic aperture radar (SAR) images in a\nsupervised setting. We use a cyclical approach involving two stages (1)\ntraining an ensemble model of multiple UNet architectures with available high\nand low confidence labeled data and, (2) generating pseudo labels or low\nconfidence labels on the unlabeled test dataset, and then, combining the\ngenerated labels with the previously available high confidence labeled dataset.\nThis assimilated dataset is used for the next round of training ensemble\nmodels. This cyclical process is repeated until the performance improvement\nplateaus. Additionally, we post process our results with Conditional Random\nFields. Our approach sets a high score on the public leaderboard for the ETCI\ncompetition with 0.7654 IoU. Our method, which we release with all the code\nincluding trained models, can also be used as an open science benchmark for the\nSentinel-1 released dataset on GitHub. To the best of our knowledge we believe\nthis the first works to try out semi-supervised learning to improve flood\nsegmentation models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 05:42:10 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 16:20:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Paul", "Sayak", ""], ["Ganju", "Siddha", ""]]}, {"id": "2107.08371", "submitter": "Liangqiong Qu", "authors": "Liangqiong Qu, Niranjan Balachandar and Daniel L Rubin", "title": "An Experimental Study of Data Heterogeneity in Federated Learning\n  Methods for Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables multiple institutions to collaboratively train\nmachine learning models on their local data in a privacy-preserving way.\nHowever, its distributed nature often leads to significant heterogeneity in\ndata distributions across institutions. In this paper, we investigate the\ndeleterious impact of a taxonomy of data heterogeneity regimes on federated\nlearning methods, including quantity skew, label distribution skew, and imaging\nacquisition skew. We show that the performance degrades with the increasing\ndegrees of data heterogeneity. We present several mitigation strategies to\novercome performance drops from data heterogeneity, including weighted average\nfor data quantity skew, weighted loss and batch normalization averaging for\nlabel distribution skew. The proposed optimizations to federated learning\nmethods improve their capability of handling heterogeneity across institutions,\nwhich provides valuable guidance for the deployment of federated learning in\nreal clinical applications.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 05:47:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Qu", "Liangqiong", ""], ["Balachandar", "Niranjan", ""], ["Rubin", "Daniel L", ""]]}, {"id": "2107.08382", "submitter": "Hsu-Hsun Chin", "authors": "Hsu-Hsun Chin, Ren-Song Tsay, Hsin-I Wu", "title": "A High-Performance Adaptive Quantization Approach for Edge CNN\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent convolutional neural network (CNN) development continues to advance\nthe state-of-the-art model accuracy for various applications. However, the\nenhanced accuracy comes at the cost of substantial memory bandwidth and storage\nrequirements and demanding computational resources. Although in the past the\nquantization methods have effectively reduced the deployment cost for edge\ndevices, it suffers from significant information loss when processing the\nbiased activations of contemporary CNNs. In this paper, we hence introduce an\nadaptive high-performance quantization method to resolve the issue of biased\nactivation by dynamically adjusting the scaling and shifting factors based on\nthe task loss. Our proposed method has been extensively evaluated on image\nclassification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with\nImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and\nlanguage models with PTB dataset. The results show that our 4-bit integer\n(INT4) quantization models achieve better accuracy than the state-of-the-art\n4-bit models, and in some cases, even surpass the golden full-precision models.\nThe final designs have been successfully deployed onto extremely\nresource-constrained edge devices for many practical applications.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 07:49:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chin", "Hsu-Hsun", ""], ["Tsay", "Ren-Song", ""], ["Wu", "Hsin-I", ""]]}, {"id": "2107.08391", "submitter": "Dongze Lian", "authors": "Dongze Lian, Zehao Yu, Xing Sun, Shenghua Gao", "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.\nDifferent from MLP-Mixer, where the global spatial feature is encoded for the\ninformation flow through matrix transposition and one token-mixing MLP, we pay\nmore attention to the local features communication. By axially shifting\nchannels of the feature map, AS-MLP is able to obtain the information flow from\ndifferent axial directions, which captures the local dependencies. Such an\noperation enables us to utilize a pure MLP architecture to achieve the same\nlocal receptive field as CNN-like architecture. We can also design the\nreceptive field size and dilation of blocks of AS-MLP, etc, just like designing\nthose of convolution kernels. With the proposed AS-MLP architecture, our model\nobtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the\nImageNet-1K dataset. Such a simple yet effective architecture outperforms all\nMLP-based architectures and achieves competitive performance compared to the\ntransformer-based architectures (e.g., Swin Transformer) even with slightly\nlower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be\napplied to the downstream tasks (e.g., object detection and semantic\nsegmentation). The experimental results are also impressive. Our proposed\nAS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the\nADE20K dataset, which is competitive compared to the transformer-based\narchitectures. Code is available at https://github.com/svip-lab/AS-MLP.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 08:56:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lian", "Dongze", ""], ["Yu", "Zehao", ""], ["Sun", "Xing", ""], ["Gao", "Shenghua", ""]]}, {"id": "2107.08392", "submitter": "Chunhua Shen", "authors": "Tong He, Chunhua Shen, Anton van den Hengel", "title": "Dynamic Convolution for 3D Point Cloud Instance Segmentation", "comments": "Extended version of arXiv:2011.13328", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose an approach to instance segmentation from 3D point clouds based on\ndynamic convolution. This enables it to adapt, at inference, to varying feature\nand object scales. Doing so avoids some pitfalls of bottom up approaches,\nincluding a dependence on hyper-parameter tuning and heuristic post-processing\npipelines to compensate for the inevitable variability in object sizes, even\nwithin a single scene. The representation capability of the network is greatly\nimproved by gathering homogeneous points that have identical semantic\ncategories and close votes for the geometric centroids. Instances are then\ndecoded via several simple convolution layers, where the parameters are\ngenerated conditioned on the input. The proposed approach is proposal-free, and\ninstead exploits a convolution process that adapts to the spatial and semantic\ncharacteristics of each instance. A light-weight transformer, built on the\nbottleneck layer, allows the model to capture long-range dependencies, with\nlimited computational overhead. The result is a simple, efficient, and robust\napproach that yields strong performance on various datasets: ScanNetV2, S3DIS,\nand PartNet. The consistent improvements on both voxel- and point-based\narchitectures imply the effectiveness of the proposed method. Code is available\nat: https://git.io/DyCo3D\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 09:05:16 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["He", "Tong", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2107.08394", "submitter": "Laurent Lejeune", "authors": "Laurent Lejeune, Raphael Sznitman", "title": "A Positive/Unlabeled Approach for the Segmentation of Medical Sequences\n  using Point-Wise Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to quickly annotate medical imaging data plays a critical role in\ntraining deep learning frameworks for segmentation. Doing so for image volumes\nor video sequences is even more pressing as annotating these is particularly\nburdensome. To alleviate this problem, this work proposes a new method to\nefficiently segment medical imaging volumes or videos using point-wise\nannotations only. This allows annotations to be collected extremely quickly and\nremains applicable to numerous segmentation tasks. Our approach trains a deep\nlearning model using an appropriate Positive/Unlabeled objective function using\nsparse point-wise annotations. While most methods of this kind assume that the\nproportion of positive samples in the data is known a-priori, we introduce a\nnovel self-supervised method to estimate this prior efficiently by combining a\nBayesian estimation framework and new stopping criteria. Our method iteratively\nestimates appropriate class priors and yields high segmentation quality for a\nvariety of object types and imaging modalities. In addition, by leveraging a\nspatio-temporal tracking framework, we regularize our predictions by leveraging\nthe complete data volume. We show experimentally that our approach outperforms\nstate-of-the-art methods tailored to the same problem.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 09:13:33 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lejeune", "Laurent", ""], ["Sznitman", "Raphael", ""]]}, {"id": "2107.08406", "submitter": "Qiang Fu", "authors": "Shutai Wang, Qiang Fu, Yinhao Hu, Chunhua Zhang, Wei He", "title": "A Miniature Biological Eagle-Eye Vision System for Small Target\n  Detection", "comments": "submitted to 2021 Chinese Automation Congress (CAC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Small target detection is known to be a challenging problem. Inspired by the\nstructural characteristics and physiological mechanism of eagle-eye, a\nminiature vision system is designed for small target detection in this paper.\nFirst, a hardware platform is established, which consists of a pan-tilt, a\nshort-focus camera and a long-focus camera. Then, based on the visual attention\nmechanism of eagle-eye, the cameras with different focal lengths are controlled\ncooperatively to achieve small target detection. Experimental results show that\nthe designed biological eagle-eye vision system can accurately detect small\ntargets, which has a strong adaptive ability.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 09:51:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Shutai", ""], ["Fu", "Qiang", ""], ["Hu", "Yinhao", ""], ["Zhang", "Chunhua", ""], ["He", "Wei", ""]]}, {"id": "2107.08421", "submitter": "Tianshu Xie", "authors": "Tianshu Xie, Xuan Cheng, Xiaomin Wang, Minghui Liu, Jiali Deng, Ming\n  Liu", "title": "Feature Mining: A Novel Training Strategy for Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel training strategy for convolutional neural\nnetwork(CNN) named Feature Mining, that aims to strengthen the network's\nlearning of the local feature. Through experiments, we find that semantic\ncontained in different parts of the feature is different, while the network\nwill inevitably lose the local information during feedforward propagation. In\norder to enhance the learning of local feature, Feature Mining divides the\ncomplete feature into two complementary parts and reuse these divided feature\nto make the network learn more local information, we call the two steps as\nfeature segmentation and feature reusing. Feature Mining is a parameter-free\nmethod and has plug-and-play nature, and can be applied to any CNN models.\nExtensive experiments demonstrate the wide applicability, versatility, and\ncompatibility of our method.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 12:14:42 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Xie", "Tianshu", ""], ["Cheng", "Xuan", ""], ["Wang", "Xiaomin", ""], ["Liu", "Minghui", ""], ["Deng", "Jiali", ""], ["Liu", "Ming", ""]]}, {"id": "2107.08430", "submitter": "Songtao Liu", "authors": "Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun", "title": "YOLOX: Exceeding YOLO Series in 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 12:55:11 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ge", "Zheng", ""], ["Liu", "Songtao", ""], ["Wang", "Feng", ""], ["Li", "Zeming", ""], ["Sun", "Jian", ""]]}, {"id": "2107.08440", "submitter": "Hang Duong Thi Thuy", "authors": "Hang Duong Thi Thuy, Tuan Nguyen Minh, Phi Nguyen Van, Long Tran Quoc", "title": "Fully Automated Machine Learning Pipeline for Echocardiogram\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, cardiac diagnosis largely depends on left ventricular function\nassessment. With the help of the segmentation deep learning model, the\nassessment of the left ventricle becomes more accessible and accurate. However,\ndeep learning technique still faces two main obstacles: the difficulty in\nacquiring sufficient training data and time-consuming in developing quality\nmodels. In the ordinary data acquisition process, the dataset was selected\nrandomly from a large pool of unlabeled images for labeling, leading to massive\nlabor time to annotate those images. Besides that, hand-designed model\ndevelopment is laborious and also costly. This paper introduces a pipeline that\nrelies on Active Learning to ease the labeling work and utilizes Neural\nArchitecture Search's idea to design the adequate deep learning model\nautomatically. We called this Fully automated machine learning pipeline for\nechocardiogram segmentation. The experiment results show that our method\nobtained the same IOU accuracy with only two-fifths of the original training\ndataset, and the searched model got the same accuracy as the hand-designed\nmodel given the same training dataset.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:15:46 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Thuy", "Hang Duong Thi", ""], ["Minh", "Tuan Nguyen", ""], ["Van", "Phi Nguyen", ""], ["Quoc", "Long Tran", ""]]}, {"id": "2107.08470", "submitter": "Yung-Han Ho", "authors": "Yung-Han Ho, Chih-Chun Chan, Wen-Hsiao Peng, Hsueh-Ming Hang, Marek\n  Domanski", "title": "ANFIC: Image Compression Using Augmented Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 15:02:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ho", "Yung-Han", ""], ["Chan", "Chih-Chun", ""], ["Peng", "Wen-Hsiao", ""], ["Hang", "Hsueh-Ming", ""], ["Domanski", "Marek", ""]]}, {"id": "2107.08471", "submitter": "Dengshan Li", "authors": "Dengshan Li, Rujing Wang, Chengjun Xie", "title": "A stepped sampling method for video detection using LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 15:04:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Dengshan", ""], ["Wang", "Rujing", ""], ["Xie", "Chengjun", ""]]}, {"id": "2107.08543", "submitter": "Talgat Saparov", "authors": "Talgat Saparov, Anvar Kurmukov, Boris Shirokih, Mikhail Belyaev", "title": "Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back\n  Projection Augmentation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 21:46:49 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Saparov", "Talgat", ""], ["Kurmukov", "Anvar", ""], ["Shirokih", "Boris", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "2107.08565", "submitter": "Zhenpeng Chen", "authors": "Zhenpeng Chen", "title": "Learning point embedding for 3D data processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 00:25:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Zhenpeng", ""]]}, {"id": "2107.08579", "submitter": "Basura Fernando", "authors": "Yan Bin Ng, Basura Fernando", "title": "Action Forecasting with Feature-wise Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new architecture for human action forecasting from videos. A\ntemporal recurrent encoder captures temporal information of input videos while\na self-attention model is used to attend on relevant feature dimensions of the\ninput space. To handle temporal variations in observed video data, a feature\nmasking techniques is employed. We classify observed actions accurately using\nan auxiliary classifier which helps to understand what has happened so far.\nThen the decoder generates actions for the future based on the output of the\nrecurrent encoder and the self-attention model. Experimentally, we validate\neach component of our architecture where we see that the impact of\nself-attention to identify relevant feature dimensions, temporal masking, and\nobserved auxiliary classifier. We evaluate our method on two standard action\nforecasting benchmarks and obtain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 01:55:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ng", "Yan Bin", ""], ["Fernando", "Basura", ""]]}, {"id": "2107.08580", "submitter": "Di Yang", "authors": "Di Yang, Yaohui Wang, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero\n  Francesca, Francois Bremond", "title": "UNIK: A Unified Framework for Real-world Skeleton-based Action\n  Recognition", "comments": "Code is available at: https://github.com/YangDi666/UNIK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Action recognition based on skeleton data has recently witnessed increasing\nattention and progress. State-of-the-art approaches adopting Graph\nConvolutional networks (GCNs) can effectively extract features on human\nskeletons relying on the pre-defined human topology. Despite associated\nprogress, GCN-based methods have difficulties to generalize across domains,\nespecially with different human topological structures. In this context, we\nintroduce UNIK, a novel skeleton-based action recognition method that is not\nonly effective to learn spatio-temporal features on human skeleton sequences\nbut also able to generalize across datasets. This is achieved by learning an\noptimal dependency matrix from the uniform distribution based on a multi-head\nattention mechanism. Subsequently, to study the cross-domain generalizability\nof skeleton-based action recognition in real-world videos, we re-evaluate\nstate-of-the-art approaches as well as the proposed UNIK in light of a novel\nPosetics dataset. This dataset is created from Kinetics-400 videos by\nestimating, refining and filtering poses. We provide an analysis on how much\nperformance improves on smaller benchmark datasets after pre-training on\nPosetics for the action classification task. Experimental results show that the\nproposed UNIK, with pre-training on Posetics, generalizes well and outperforms\nstate-of-the-art when transferred onto four target action classification\ndatasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:00:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yang", "Di", ""], ["Wang", "Yaohui", ""], ["Dantcheva", "Antitza", ""], ["Garattoni", "Lorenzo", ""], ["Francesca", "Gianpiero", ""], ["Bremond", "Francois", ""]]}, {"id": "2107.08581", "submitter": "Songlin Yang", "authors": "Songlin Yang, Wei Wang, Yuehua Cheng and Jing Dong", "title": "A Systematical Solution for Face De-identification", "comments": "accepted by the 15th Chinese Conference on Biometrics Recognition\n  (CCBR2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the identity information in face data more closely related to personal\ncredit and property security, people pay increasing attention to the protection\nof face data privacy. In different tasks, people have various requirements for\nface de-identification (De-ID), so we propose a systematical solution\ncompatible for these De-ID operations. Firstly, an attribute disentanglement\nand generative network is constructed to encode two parts of the face, which\nare the identity (facial features like mouth, nose and eyes) and expression\n(including expression, pose and illumination). Through face swapping, we can\nremove the original ID completely. Secondly, we add an adversarial vector\nmapping network to perturb the latent code of the face image, different from\nprevious traditional adversarial methods. Through this, we can construct\nunrestricted adversarial image to decrease ID similarity recognized by model.\nOur method can flexibly de-identify the face data in various ways and the\nprocessed images have high image quality.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:02:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yang", "Songlin", ""], ["Wang", "Wei", ""], ["Cheng", "Yuehua", ""], ["Dong", "Jing", ""]]}, {"id": "2107.08585", "submitter": "Jo Plested", "authors": "Jo Plested, Xuyang Shen, and Tom Gedeon", "title": "Non-binary deep transfer learning for imageclassification", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current standard for a variety of computer vision tasks using smaller\nnumbers of labelled training examples is to fine-tune from weights pre-trained\non a large image classification dataset such as ImageNet. The application of\ntransfer learning and transfer learning methods tends to be rigidly binary. A\nmodel is either pre-trained or not pre-trained. Pre-training a model either\nincreases performance or decreases it, the latter being defined as negative\ntransfer. Application of L2-SP regularisation that decays the weights towards\ntheir pre-trained values is either applied or all weights are decayed towards\n0. This paper re-examines these assumptions. Our recommendations are based on\nextensive empirical evaluation that demonstrate the application of a non-binary\napproach to achieve optimal results. (1) Achieving best performance on each\nindividual dataset requires careful adjustment of various transfer learning\nhyperparameters not usually considered, including number of layers to transfer,\ndifferent learning rates for different layers and different combinations of\nL2SP and L2 regularization. (2) Best practice can be achieved using a number of\nmeasures of how well the pre-trained weights fit the target dataset to guide\noptimal hyperparameters. We present methods for non-binary transfer learning\nincluding combining L2SP and L2 regularization and performing non-traditional\nfine-tuning hyperparameter searches. Finally we suggest heuristics for\ndetermining the optimal transfer learning hyperparameters. The benefits of\nusing a non-binary approach are supported by final results that come close to\nor exceed state of the art performance on a variety of tasks that have\ntraditionally been more difficult for transfer learning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:34:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Plested", "Jo", ""], ["Shen", "Xuyang", ""], ["Gedeon", "Tom", ""]]}, {"id": "2107.08591", "submitter": "Yingchao Feng", "authors": "Yingchao Feng, Xian Sun, Wenhui Diao, Jihao Li, Xin Gao", "title": "Double Similarity Distillation for Semantic Image Segmentation", "comments": "Published in IEEE Transaction on Image Processing (TIP) 2021, Volume:\n  30", "journal-ref": null, "doi": "10.1109/TIP.2021.3083113", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balance between high accuracy and high speed has always been a\nchallenging task in semantic image segmentation. Compact segmentation networks\nare more widely used in the case of limited resources, while their performances\nare constrained. In this paper, motivated by the residual learning and global\naggregation, we propose a simple yet general and effective knowledge\ndistillation framework called double similarity distillation (DSD) to improve\nthe classification accuracy of all existing compact networks by capturing the\nsimilarity knowledge in pixel and category dimensions, respectively.\nSpecifically, we propose a pixel-wise similarity distillation (PSD) module that\nutilizes residual attention maps to capture more detailed spatial dependencies\nacross multiple layers. Compared with exiting methods, the PSD module greatly\nreduces the amount of calculation and is easy to expand. Furthermore,\nconsidering the differences in characteristics between semantic segmentation\ntask and other computer vision tasks, we propose a category-wise similarity\ndistillation (CSD) module, which can help the compact segmentation network\nstrengthen the global category correlation by constructing the correlation\nmatrix. Combining these two modules, DSD framework has no extra parameters and\nonly a minimal increase in FLOPs. Extensive experiments on four challenging\ndatasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show that\nDSD outperforms current state-of-the-art methods, proving its effectiveness and\ngenerality. The code and models will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 02:45:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Feng", "Yingchao", ""], ["Sun", "Xian", ""], ["Diao", "Wenhui", ""], ["Li", "Jihao", ""], ["Gao", "Xin", ""]]}, {"id": "2107.08621", "submitter": "Qingzhong Wang", "authors": "Qingzhong Wang, Pengfei Zhang, Haoyi Xiong and Jian Zhao", "title": "Face.evoLVe: A High-Performance Face Recognition Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 05:38:50 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:24:48 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Qingzhong", ""], ["Zhang", "Pengfei", ""], ["Xiong", "Haoyi", ""], ["Zhao", "Jian", ""]]}, {"id": "2107.08623", "submitter": "He Xinwei", "authors": "Guoping Xu, Xingrong Wu, Xuan Zhang, Xinwei He", "title": "LeViT-UNet: Make Faster Encoders with Transformer for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation plays an essential role in developing\ncomputer-assisted diagnosis and therapy systems, yet still faces many\nchallenges. In the past few years, the popular encoder-decoder architectures\nbased on CNNs (e.g., U-Net) have been successfully applied in the task of\nmedical image segmentation. However, due to the locality of convolution\noperations, they demonstrate limitations in learning global context and\nlong-range spatial relations. Recently, several researchers try to introduce\ntransformers to both the encoder and decoder components with promising results,\nbut the efficiency requires further improvement due to the high computational\ncomplexity of transformers. In this paper, we propose LeViT-UNet, which\nintegrates a LeViT Transformer module into the U-Net architecture, for fast and\naccurate medical image segmentation. Specifically, we use LeViT as the encoder\nof the LeViT-UNet, which better trades off the accuracy and efficiency of the\nTransformer block. Moreover, multi-scale feature maps from transformer blocks\nand convolutional blocks of LeViT are passed into the decoder via\nskip-connection, which can effectively reuse the spatial information of the\nfeature maps. Our experiments indicate that the proposed LeViT-UNet achieves\nbetter performance comparing to various competing methods on several\nchallenging medical image segmentation benchmarks including Synapse and ACDC.\nCode and models will be publicly available at\nhttps://github.com/apple1986/LeViT_UNet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 05:48:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Xu", "Guoping", ""], ["Wu", "Xingrong", ""], ["Zhang", "Xuan", ""], ["He", "Xinwei", ""]]}, {"id": "2107.08639", "submitter": "Kazuya Nishimura", "authors": "Kazuya Nishimura and Hyeonwoo Cho and Ryoma Bise", "title": "Semi-supervised Cell Detection in Time-lapse Images Using Temporal\n  Consistency", "comments": "11 pages, 5 figures, Accepted in MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell detection is the task of detecting the approximate positions of cell\ncentroids from microscopy images. Recently, convolutional neural network-based\napproaches have achieved promising performance. However, these methods require\na certain amount of annotation for each imaging condition. This annotation is a\ntime-consuming and labor-intensive task. To overcome this problem, we propose a\nsemi-supervised cell-detection method that effectively uses a time-lapse\nsequence with one labeled image and the other images unlabeled. First, we train\na cell-detection network with a one-labeled image and estimate the unlabeled\nimages with the trained network. We then select high-confidence positions from\nthe estimations by tracking the detected cells from the labeled frame to those\nfar from it. Next, we generate pseudo-labels from the tracking results and\ntrain the network by using pseudo-labels. We evaluated our method for seven\nconditions of public datasets, and we achieved the best results relative to\nother semi-supervised methods. Our code is available at\nhttps://github.com/naivete5656/SCDTC\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 06:40:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nishimura", "Kazuya", ""], ["Cho", "Hyeonwoo", ""], ["Bise", "Ryoma", ""]]}, {"id": "2107.08640", "submitter": "Subodh Lonkar", "authors": "Subodh Lonkar", "title": "Facial Expressions Recognition with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the centuries, humans have developed and acquired a number of ways to\ncommunicate. But hardly any of them can be as natural and instinctive as facial\nexpressions. On the other hand, neural networks have taken the world by storm.\nAnd no surprises, that the area of Computer Vision and the problem of facial\nexpressions recognitions hasn't remained untouched. Although a wide range of\ntechniques have been applied, achieving extremely high accuracies and preparing\nhighly robust FER systems still remains a challenge due to heterogeneous\ndetails in human faces. In this paper, we will be deep diving into implementing\na system for recognition of facial expressions (FER) by leveraging neural\nnetworks, and more specifically, Convolutional Neural Networks (CNNs). We adopt\nthe fundamental concepts of deep learning and computer vision with various\narchitectures, fine-tune it's hyperparameters and experiment with various\noptimization methods and demonstrate a state-of-the-art single-network-accuracy\nof 70.10% on the FER2013 dataset without using any additional training data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 06:41:00 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lonkar", "Subodh", ""]]}, {"id": "2107.08645", "submitter": "Haopeng Li", "authors": "Haopeng Li, Lingbo Liu, Kunlin Yang, Shinan Liu, Junyu Gao, Bin Zhao,\n  Rui Zhang, Jun Hou", "title": "Video Crowd Localization with Multi-focus Gaussian Neighbor Attention\n  and a Large-Scale Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video crowd localization is a crucial yet challenging task, which aims to\nestimate exact locations of human heads in the given crowded videos. To model\nspatial-temporal dependencies of human mobility, we propose a multi-focus\nGaussian neighbor attention (GNA), which can effectively exploit long-range\ncorrespondences while maintaining the spatial topological structure of the\ninput videos. In particular, our GNA can also capture the scale variation of\nhuman heads well using the equipped multi-focus mechanism. Based on the\nmulti-focus GNA, we develop a unified neural network called GNANet to\naccurately locate head centers in video clips by fully aggregating\nspatial-temporal information via a scene modeling module and a context\ncross-attention module. Moreover, to facilitate future researches in this\nfield, we introduce a large-scale crowded video benchmark named SenseCrowd,\nwhich consists of 60K+ frames captured in various surveillance scenarios and\n2M+ head annotations. Finally, we conduct extensive experiments on three\ndatasets including our SenseCrowd, and the experiment results show that the\nproposed method is capable to achieve state-of-the-art performance for both\nvideo crowd localization and counting. The code and the dataset will be\nreleased.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 06:59:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 01:46:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Li", "Haopeng", ""], ["Liu", "Lingbo", ""], ["Yang", "Kunlin", ""], ["Liu", "Shinan", ""], ["Gao", "Junyu", ""], ["Zhao", "Bin", ""], ["Zhang", "Rui", ""], ["Hou", "Jun", ""]]}, {"id": "2107.08650", "submitter": "Tianyuan Yao", "authors": "Tianyuan Yao, Chang Qu, Quan Liu, Ruining Deng, Yuanhan Tian, Jiachen\n  Xu, Aadarsh Jha, Shunxing Bao, Mengyang Zhao, Agnes B. Fogo, Bennett\n  A.Landman, Catie Chang, Haichun Yang, Yuankai Huo", "title": "Compound Figure Separation of Biomedical Images with Side Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning algorithms (e.g., self-supervised learning,\nauto-encoder, contrastive learning) allow deep learning models to learn\neffective image representations from large-scale unlabeled data. In medical\nimage analysis, even unannotated data can be difficult to obtain for individual\nlabs. Fortunately, national-level efforts have been made to provide efficient\naccess to obtain biomedical image data from previous scientific publications.\nFor instance, NIH has launched the Open-i search engine that provides a\nlarge-scale image database with free access. However, the images in scientific\npublications consist of a considerable amount of compound figures with\nsubplots. To extract and curate individual subplots, many different compound\nfigure separation approaches have been developed, especially with the recent\nadvances in deep learning. However, previous approaches typically required\nresource extensive bounding box annotation to train detection models. In this\npaper, we propose a simple compound figure separation (SimCFS) framework that\nuses weak classification annotations from individual images. Our technical\ncontribution is three-fold: (1) we introduce a new side loss that is designed\nfor compound figure separation; (2) we introduce an intra-class image\naugmentation method to simulate hard cases; (3) the proposed framework enables\nan efficient deployment to new classes of images, without requiring resource\nextensive bounding box annotations. From the results, the SimCFS achieved a new\nstate-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation\nDatabase. The source code of SimCFS is made publicly available at\nhttps://github.com/hrlblab/ImageSeperation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 07:16:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yao", "Tianyuan", ""], ["Qu", "Chang", ""], ["Liu", "Quan", ""], ["Deng", "Ruining", ""], ["Tian", "Yuanhan", ""], ["Xu", "Jiachen", ""], ["Jha", "Aadarsh", ""], ["Bao", "Shunxing", ""], ["Zhao", "Mengyang", ""], ["Fogo", "Agnes B.", ""], ["Landman", "Bennett A.", ""], ["Chang", "Catie", ""], ["Yang", "Haichun", ""], ["Huo", "Yuankai", ""]]}, {"id": "2107.08653", "submitter": "Hyeonwoo Cho", "authors": "Hyeonwoo Cho, Kazuya Nishimura, Kazuhide Watanabe and Ryoma Bise", "title": "Cell Detection in Domain Shift Problem Using Pseudo-Cell-Position\n  Heatmap", "comments": "10 pages, 4 figures, Accepted in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain shift problem is an important issue in automatic cell detection. A\ndetection network trained with training data under a specific condition (source\ndomain) may not work well in data under other conditions (target domain). We\npropose an unsupervised domain adaptation method for cell detection using the\npseudo-cell-position heatmap, where a cell centroid becomes a peak with a\nGaussian distribution in the map. In the prediction result for the target\ndomain, even if a peak location is correct, the signal distribution around the\npeak often has anon-Gaussian shape. The pseudo-cell-position heatmap is\nre-generated using the peak positions in the predicted heatmap to have a clear\nGaussian shape. Our method selects confident pseudo-cell-position heatmaps\nusing a Bayesian network and adds them to the training data in the next\niteration. The method can incrementally extend the domain from the source\ndomain to the target domain in a semi-supervised manner. In the experiments\nusing 8 combinations of domains, the proposed method outperformed the existing\ndomain adaptation methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 07:22:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Cho", "Hyeonwoo", ""], ["Nishimura", "Kazuya", ""], ["Watanabe", "Kazuhide", ""], ["Bise", "Ryoma", ""]]}, {"id": "2107.08673", "submitter": "Aidana Massalimova", "authors": "Aidana Massalimova and Huseyin Atakan Varol", "title": "Input Agnostic Deep Learning for Alzheimer's Disease Classification\n  Using Multimodal MRI Images", "comments": "4 pages, submitted to EMBC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alzheimer's disease (AD) is a progressive brain disorder that causes memory\nand functional impairments. The advances in machine learning and publicly\navailable medical datasets initiated multiple studies in AD diagnosis. In this\nwork, we utilize a multi-modal deep learning approach in classifying normal\ncognition, mild cognitive impairment and AD classes on the basis of structural\nMRI and diffusion tensor imaging (DTI) scans from the OASIS-3 dataset. In\naddition to a conventional multi-modal network, we also present an input\nagnostic architecture that allows diagnosis with either sMRI or DTI scan, which\ndistinguishes our method from previous multi-modal machine learning-based\nmethods. The results show that the input agnostic model achieves 0.96 accuracy\nwhen both structural MRI and DTI scans are provided as inputs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:19:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Massalimova", "Aidana", ""], ["Varol", "Huseyin Atakan", ""]]}, {"id": "2107.08712", "submitter": "Qiang Li Capasso", "authors": "Zhaoqing Wang, Qiang Li, Guoxin Zhang, Pengfei Wan, Wen Zheng, Nannan\n  Wang, Mingming Gong, Tongliang Liu", "title": "Exploring Set Similarity for Dense Self-supervised Representation\n  Learning", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By considering the spatial correspondence, dense self-supervised\nrepresentation learning has achieved superior performance on various dense\nprediction tasks. However, the pixel-level correspondence tends to be noisy\nbecause of many similar misleading pixels, e.g., backgrounds. To address this\nissue, in this paper, we propose to explore \\textbf{set} \\textbf{sim}ilarity\n(SetSim) for dense self-supervised representation learning. We generalize\npixel-wise similarity learning to set-wise one to improve the robustness\nbecause sets contain more semantic and structure information. Specifically, by\nresorting to attentional features of views, we establish corresponding sets,\nthus filtering out noisy backgrounds that may cause incorrect correspondences.\nMeanwhile, these attentional features can keep the coherence of the same image\nacross different views to alleviate semantic inconsistency. We further search\nthe cross-view nearest neighbours of sets and employ the structured\nneighbourhood information to enhance the robustness. Empirical evaluations\ndemonstrate that SetSim is superior to state-of-the-art methods on object\ndetection, keypoint detection, instance segmentation, and semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:38:27 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Zhaoqing", ""], ["Li", "Qiang", ""], ["Zhang", "Guoxin", ""], ["Wan", "Pengfei", ""], ["Zheng", "Wen", ""], ["Wang", "Nannan", ""], ["Gong", "Mingming", ""], ["Liu", "Tongliang", ""]]}, {"id": "2107.08715", "submitter": "Dong Wei", "authors": "Cong Xie, Shilei Cao, Dong Wei, Hongyu Zhou, Kai Ma, Xianli Zhang,\n  Buyue Qian, Liansheng Wang, Yefeng Zheng", "title": "RECIST-Net: Lesion detection via grouping keypoints on RECIST-based\n  annotation", "comments": "5 pages, 3 figures, IEEE ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Universal lesion detection in computed tomography (CT) images is an important\nyet challenging task due to the large variations in lesion type, size, shape,\nand appearance. Considering that data in clinical routine (such as the\nDeepLesion dataset) are usually annotated with a long and a short diameter\naccording to the standard of Response Evaluation Criteria in Solid Tumors\n(RECIST) diameters, we propose RECIST-Net, a new approach to lesion detection\nin which the four extreme points and center point of the RECIST diameters are\ndetected. By detecting a lesion as keypoints, we provide a more conceptually\nstraightforward formulation for detection, and overcome several drawbacks\n(e.g., requiring extensive effort in designing data-appropriate anchors and\nlosing shape information) of existing bounding-box-based methods while\nexploring a single-task, one-stage approach compared to other RECIST-based\napproaches. Experiments show that RECIST-Net achieves a sensitivity of 92.49%\nat four false positives per image, outperforming other recent methods including\nthose using multi-task learning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:41:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Xie", "Cong", ""], ["Cao", "Shilei", ""], ["Wei", "Dong", ""], ["Zhou", "Hongyu", ""], ["Ma", "Kai", ""], ["Zhang", "Xianli", ""], ["Qian", "Buyue", ""], ["Wang", "Liansheng", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2107.08717", "submitter": "Jiaxiang Tang", "authors": "Jiaxiang Tang, Xiaokang Chen, Gang Zeng", "title": "Joint Implicit Image Function for Guided Depth Super-Resolution", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": "10.1145/3474085.3475584", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided depth super-resolution is a practical task where a low-resolution and\nnoisy input depth map is restored to a high-resolution version, with the help\nof a high-resolution RGB guide image. Existing methods usually view this task\nas a generalized guided filtering problem that relies on designing explicit\nfilters and objective functions, or a dense regression problem that directly\npredicts the target image via deep neural networks. These methods suffer from\neither model capability or interpretability. Inspired by the recent progress in\nimplicit neural representation, we propose to formulate the guided\nsuper-resolution as a neural implicit image interpolation problem, where we\ntake the form of a general image interpolation but use a novel Joint Implicit\nImage Function (JIIF) representation to learn both the interpolation weights\nand values. JIIF represents the target image domain with spatially distributed\nlocal latent codes extracted from the input image and the guide image, and uses\na graph attention mechanism to learn the interpolation weights at the same time\nin one unified deep implicit function. We demonstrate the effectiveness of our\nJIIF representation on guided depth super-resolution task, significantly\noutperforming state-of-the-art methods on three public benchmarks. Code can be\nfound at \\url{https://git.io/JC2sU}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:42:18 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 09:54:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tang", "Jiaxiang", ""], ["Chen", "Xiaokang", ""], ["Zeng", "Gang", ""]]}, {"id": "2107.08737", "submitter": "Minyoung Kim", "authors": "Minyoung Kim and Young J. Kim", "title": "Synthesizing Human Faces using Latent Space Factorization and Local\n  Weights (Extended Version)", "comments": "Extended version of the paper to will be published in Computer\n  Graphics International 2021 (LNCS Proceeding Papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 3D face generative model with local weights to increase the\nmodel's variations and expressiveness. The proposed model allows partial\nmanipulation of the face while still learning the whole face mesh. For this\npurpose, we address an effective way to extract local facial features from the\nentire data and explore a way to manipulate them during a holistic generation.\nFirst, we factorize the latent space of the whole face to the subspace\nindicating different parts of the face. In addition, local weights generated by\nnon-negative matrix factorization are applied to the factorized latent space so\nthat the decomposed part space is semantically meaningful. We experiment with\nour model and observe that effective facial part manipulation is possible and\nthat the model's expressiveness is improved.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 10:17:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kim", "Minyoung", ""], ["Kim", "Young J.", ""]]}, {"id": "2107.08751", "submitter": "Marius Memmel", "authors": "Marius Memmel, Camila Gonzalez, Anirban Mukhopadhyay", "title": "Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning for medical imaging suffers from temporal and privacy-related\nrestrictions on data availability. To still obtain viable models, continual\nlearning aims to train in sequential order, as and when data is available. The\nmain challenge that continual learning methods face is to prevent catastrophic\nforgetting, i.e., a decrease in performance on the data encountered earlier.\nThis issue makes continuous training of segmentation models for medical\napplications extremely difficult. Yet, often, data from at least two different\ndomains is available which we can exploit to train the model in a way that it\ndisregards domain-specific information. We propose an architecture that\nleverages the simultaneous availability of two or more datasets to learn a\ndisentanglement between the content and domain in an adversarial fashion. The\ndomain-invariant content representation then lays the base for continual\nsemantic segmentation. Our approach takes inspiration from domain adaptation\nand combines it with continual learning for hippocampal segmentation in brain\nMRI. We showcase that our method reduces catastrophic forgetting and\noutperforms state-of-the-art continual learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 10:55:21 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:43:54 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 07:10:28 GMT"}, {"version": "v4", "created": "Sun, 25 Jul 2021 14:48:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Memmel", "Marius", ""], ["Gonzalez", "Camila", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2107.08756", "submitter": "Piotr Skalski Mr", "authors": "Iker Perez, Piotr Skalski, Alec Barns-Graham, Jason Wong, David Sutton", "title": "Path Integrals for the Attribution of Model Uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enabling interpretations of model uncertainties is of key importance in\nBayesian machine learning applications. Often, this requires to meaningfully\nattribute predictive uncertainties to source features in an image, text or\ncategorical array. However, popular attribution methods are particularly\ndesigned for classification and regression scores. In order to explain\nuncertainties, state of the art alternatives commonly procure counterfactual\nfeature vectors, and proceed by making direct comparisons. In this paper, we\nleverage path integrals to attribute uncertainties in Bayesian differentiable\nmodels. We present a novel algorithm that relies on in-distribution curves\nconnecting a feature vector to some counterfactual counterpart, and we retain\ndesirable properties of interpretability methods. We validate our approach on\nbenchmark image data sets with varying resolution, and show that it\nsignificantly simplifies interpretability over the existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 11:07:34 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 14:32:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Perez", "Iker", ""], ["Skalski", "Piotr", ""], ["Barns-Graham", "Alec", ""], ["Wong", "Jason", ""], ["Sutton", "David", ""]]}, {"id": "2107.08766", "submitter": "Pengfei Zhu", "authors": "Dawei Du, Longyin Wen, Pengfei Zhu, Heng Fan, Qinghua Hu, Haibin Ling,\n  Mubarak Shah, Junwen Pan, Ali Al-Ali, Amr Mohamed, Bakour Imene, Bin Dong,\n  Binyu Zhang, Bouchali Hadia Nesma, Chenfeng Xu, Chenzhen Duan, Ciro\n  Castiello, Corrado Mencar, Dingkang Liang, Florian Kr\\\"uger, Gennaro Vessio,\n  Giovanna Castellano, Jieru Wang, Junyu Gao, Khalid Abualsaud, Laihui Ding,\n  Lei Zhao, Marco Cianciotta, Muhammad Saqib, Noor Almaadeed, Omar Elharrouss,\n  Pei Lyu, Qi Wang, Shidong Liu, Shuang Qiu, Siyang Pan, Somaya Al-Maadeed,\n  Sultan Daud Khan, Tamer Khattab, Tao Han, Thomas Golda, Wei Xu, Xiang Bai,\n  Xiaoqing Xu, Xuelong Li, Yanyun Zhao, Ye Tian, Yingnan Lin, Yongchao Xu,\n  Yuehan Yao, Zhenyu Xu, Zhijian Zhao, Zhipeng Luo, Zhiwei Wei, Zhiyuan Zhao", "title": "VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results", "comments": "The method description of A7 Mutil-Scale Aware based SFANet\n  (M-SFANet) is updated and missing references are added", "journal-ref": "European Conference on Computer Vision. Springer, Cham, 2020:\n  675-691", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting on the drone platform is an interesting topic in computer\nvision, which brings new challenges such as small object inference, background\nclutter and wide viewpoint. However, there are few algorithms focusing on crowd\ncounting on the drone-captured data due to the lack of comprehensive datasets.\nTo this end, we collect a large-scale dataset and organize the Vision Meets\nDrone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th\nEuropean Conference on Computer Vision (ECCV 2020) to promote the developments\nin the related fields. The collected dataset is formed by $3,360$ images,\nincluding $2,460$ images for training, and $900$ images for testing.\nSpecifically, we manually annotate persons with points in each video frame.\nThere are $14$ algorithms from $15$ institutes submitted to the VisDrone-CC2020\nChallenge. We provide a detailed analysis of the evaluation results and\nconclude the challenge. More information can be found at the website:\n\\url{http://www.aiskyeye.com/}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 11:48:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Du", "Dawei", ""], ["Wen", "Longyin", ""], ["Zhu", "Pengfei", ""], ["Fan", "Heng", ""], ["Hu", "Qinghua", ""], ["Ling", "Haibin", ""], ["Shah", "Mubarak", ""], ["Pan", "Junwen", ""], ["Al-Ali", "Ali", ""], ["Mohamed", "Amr", ""], ["Imene", "Bakour", ""], ["Dong", "Bin", ""], ["Zhang", "Binyu", ""], ["Nesma", "Bouchali Hadia", ""], ["Xu", "Chenfeng", ""], ["Duan", "Chenzhen", ""], ["Castiello", "Ciro", ""], ["Mencar", "Corrado", ""], ["Liang", "Dingkang", ""], ["Kr\u00fcger", "Florian", ""], ["Vessio", "Gennaro", ""], ["Castellano", "Giovanna", ""], ["Wang", "Jieru", ""], ["Gao", "Junyu", ""], ["Abualsaud", "Khalid", ""], ["Ding", "Laihui", ""], ["Zhao", "Lei", ""], ["Cianciotta", "Marco", ""], ["Saqib", "Muhammad", ""], ["Almaadeed", "Noor", ""], ["Elharrouss", "Omar", ""], ["Lyu", "Pei", ""], ["Wang", "Qi", ""], ["Liu", "Shidong", ""], ["Qiu", "Shuang", ""], ["Pan", "Siyang", ""], ["Al-Maadeed", "Somaya", ""], ["Khan", "Sultan Daud", ""], ["Khattab", "Tamer", ""], ["Han", "Tao", ""], ["Golda", "Thomas", ""], ["Xu", "Wei", ""], ["Bai", "Xiang", ""], ["Xu", "Xiaoqing", ""], ["Li", "Xuelong", ""], ["Zhao", "Yanyun", ""], ["Tian", "Ye", ""], ["Lin", "Yingnan", ""], ["Xu", "Yongchao", ""], ["Yao", "Yuehan", ""], ["Xu", "Zhenyu", ""], ["Zhao", "Zhijian", ""], ["Luo", "Zhipeng", ""], ["Wei", "Zhiwei", ""], ["Zhao", "Zhiyuan", ""]]}, {"id": "2107.08767", "submitter": "Woojeoung Nam", "authors": "Woo-Jeoung Nam, Seong-Whan Lee", "title": "Improving Interpretability of Deep Neural Networks in Medical Diagnosis\n  by Investigating the Individual Units", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As interpretability has been pointed out as the obstacle to the adoption of\nDeep Neural Networks (DNNs), there is an increasing interest in solving a\ntransparency issue to guarantee the impressive performance. In this paper, we\ndemonstrate the efficiency of recent attribution techniques to explain the\ndiagnostic decision by visualizing the significant factors in the input image.\nBy utilizing the characteristics of objectness that DNNs have learned, fully\ndecomposing the network prediction visualizes clear localization of target\nlesion. To verify our work, we conduct our experiments on Chest X-ray diagnosis\nwith publicly accessible datasets. As an intuitive assessment metric for\nexplanations, we report the performance of intersection of Union between visual\nexplanation and bounding box of lesions. Experiment results show that recently\nproposed attribution methods visualize the more accurate localization for the\ndiagnostic decision compared to the traditionally used CAM. Furthermore, we\nanalyze the inconsistency of intentions between humans and DNNs, which is\neasily obscured by high performance. By visualizing the relevant factors, it is\npossible to confirm that the criterion for decision is in line with the\nlearning strategy. Our analysis of unmasking machine intelligence represents\nthe necessity of explainability in the medical diagnostic decision.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 11:49:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nam", "Woo-Jeoung", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.08768", "submitter": "Myeong-Seok Oh", "authors": "Myeong-Seok Oh, Yong-Ju Lee, Seong-Whan Lee", "title": "Precise Aerial Image Matching based on Deep Homography Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial image registration or matching is a geometric process of aligning two\naerial images captured in different environments. Estimating the precise\ntransformation parameters is hindered by various environments such as time,\nweather, and viewpoints. The characteristics of the aerial images are mainly\ncomposed of a straight line owing to building and road. Therefore, the straight\nlines are distorted when estimating homography parameters directly between two\nimages. In this paper, we propose a deep homography alignment network to\nprecisely match two aerial images by progressively estimating the various\ntransformation parameters. The proposed network is possible to train the\nmatching network with a higher degree of freedom by progressively analyzing the\ntransformation parameters. The precision matching performances have been\nincreased by applying homography transformation. In addition, we introduce a\nmethod that can effectively learn the difficult-to-learn homography estimation\nnetwork. Since there is no published learning data for aerial image\nregistration, in this paper, a pair of images to which random homography\ntransformation is applied within a certain range is used for learning. Hence,\nwe could confirm that the deep homography alignment network shows high\nprecision matching performance compared with conventional works.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 11:52:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Oh", "Myeong-Seok", ""], ["Lee", "Yong-Ju", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.08770", "submitter": "Gun-Hee Lee", "authors": "Gun-Hee Lee, Han-Bin Ko, Seong-Whan Lee", "title": "Joint Dermatological Lesion Classification and Confidence Modeling with\n  Uncertainty Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has played a major role in the interpretation of dermoscopic\nimages for detecting skin defects and abnormalities. However, current deep\nlearning solutions for dermatological lesion analysis are typically limited in\nproviding probabilistic predictions which highlights the importance of\nconcerning uncertainties. This concept of uncertainty can provide a confidence\nlevel for each feature which prevents overconfident predictions with poor\ngeneralization on unseen data. In this paper, we propose an overall framework\nthat jointly considers dermatological classification and uncertainty estimation\ntogether. The estimated confidence of each feature to avoid uncertain feature\nand undesirable shift, which are caused by environmental difference of input\nimage, in the latent space is pooled from confidence network. Our qualitative\nresults show that modeling uncertainties not only helps to quantify model\nconfidence for each prediction but also helps classification layers to focus on\nconfident features, therefore, improving the accuracy for dermatological lesion\nclassification. We demonstrate the potential of the proposed approach in two\nstate-of-the-art dermoscopic datasets (ISIC 2018 and ISIC 2019).\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 11:54:37 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lee", "Gun-Hee", ""], ["Ko", "Han-Bin", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.08850", "submitter": "Jonathan Ganz", "authors": "Jonathan Ganz, Tobias Kirsch, Lucas Hoffmann, Christof A. Bertram,\n  Christoph Hoffmann, Andreas Maier, Katharina Breininger, Ingmar Bl\\\"umcke,\n  Samir Jabari, Marc Aubreville", "title": "Automatic and explainable grading of meningiomas from histopathology\n  images", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meningioma is one of the most prevalent brain tumors in adults. To determine\nits malignancy, it is graded by a pathologist into three grades according to\nWHO standards. This grade plays a decisive role in treatment, and yet may be\nsubject to inter-rater discordance. In this work, we present and compare three\napproaches towards fully automatic meningioma grading from histology whole\nslide images. All approaches are following a two-stage paradigm, where we first\nidentify a region of interest based on the detection of mitotic figures in the\nslide using a state-of-the-art object detection deep learning network. This\nregion of highest mitotic rate is considered characteristic for biological\ntumor behavior. In the second stage, we calculate a score corresponding to\ntumor malignancy based on information contained in this region using three\ndifferent settings. In a first approach, image patches are sampled from this\nregion and regression is based on morphological features encoded by a\nResNet-based network. We compare this to learning a logistic regression from\nthe determined mitotic count, an approach which is easily traceable and\nexplainable. Lastly, we combine both approaches in a single network. We trained\nthe pipeline on 951 slides from 341 patients and evaluated them on a separate\nset of 141 slides from 43 patients. All approaches yield a high correlation to\nthe WHO grade. The logistic regression and the combined approach had the best\nresults in our experiments, yielding correct predictions in 32 and 33 of all\ncases, respectively, with the image-based approach only predicting 25 cases\ncorrectly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It\nmay seem counterintuitive at first that morphological features provided by\nimage patches do not improve model performance. Yet, this mirrors the criteria\nof the grading scheme, where mitotic count is the only unequivocal parameter.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:05:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ganz", "Jonathan", ""], ["Kirsch", "Tobias", ""], ["Hoffmann", "Lucas", ""], ["Bertram", "Christof A.", ""], ["Hoffmann", "Christoph", ""], ["Maier", "Andreas", ""], ["Breininger", "Katharina", ""], ["Bl\u00fcmcke", "Ingmar", ""], ["Jabari", "Samir", ""], ["Aubreville", "Marc", ""]]}, {"id": "2107.08862", "submitter": "Yuanzhu Gan", "authors": "Zizhang Wu, Wenkai Zhang, Jizheng Wang, Man Wang, Yuanzhu Gan, Xinchao\n  Gou, Muqing Fang, Jing Song", "title": "Disentangling and Vectorization: A 3D Visual Perception Approach for\n  Autonomous Driving Based on Surround-View Fisheye Cameras", "comments": "Accepted by IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D visual perception for vehicles with the surround-view fisheye camera\nsystem is a critical and challenging task for low-cost urban autonomous\ndriving. While existing monocular 3D object detection methods perform not well\nenough on the fisheye images for mass production, partly due to the lack of 3D\ndatasets of such images. In this paper, we manage to overcome and avoid the\ndifficulty of acquiring the large scale of accurate 3D labeled truth data, by\nbreaking down the 3D object detection task into some sub-tasks, such as\nvehicle's contact point detection, type classification, re-identification and\nunit assembling, etc. Particularly, we propose the concept of Multidimensional\nVector to include the utilizable information generated in different dimensions\nand stages, instead of the descriptive approach for the bird's eye view (BEV)\nor a cube of eight points. The experiments of real fisheye images demonstrate\nthat our solution achieves state-of-the-art accuracy while being real-time in\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:24:21 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wu", "Zizhang", ""], ["Zhang", "Wenkai", ""], ["Wang", "Jizheng", ""], ["Wang", "Man", ""], ["Gan", "Yuanzhu", ""], ["Gou", "Xinchao", ""], ["Fang", "Muqing", ""], ["Song", "Jing", ""]]}, {"id": "2107.08892", "submitter": "Jiahuan Zhou", "authors": "Jiahuan Zhou, Yansong Tang, Bing Su, Ying Wu", "title": "Unsupervised Embedding Learning from Uncertainty Momentum Modeling", "comments": "14 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing popular unsupervised embedding learning methods focus on enhancing\nthe instance-level local discrimination of the given unlabeled images by\nexploring various negative data. However, the existed sample outliers which\nexhibit large intra-class divergences or small inter-class variations severely\nlimit their learning performance. We justify that the performance limitation is\ncaused by the gradient vanishing on these sample outliers. Moreover, the\nshortage of positive data and disregard for global discrimination consideration\nalso pose critical issues for unsupervised learning but are always ignored by\nexisting methods. To handle these issues, we propose a novel solution to\nexplicitly model and directly explore the uncertainty of the given unlabeled\nlearning samples. Instead of learning a deterministic feature point for each\nsample in the embedding space, we propose to represent a sample by a stochastic\nGaussian with the mean vector depicting its space localization and covariance\nvector representing the sample uncertainty. We leverage such uncertainty\nmodeling as momentum to the learning which is helpful to tackle the outliers.\nFurthermore, abundant positive candidates can be readily drawn from the learned\ninstance-specific distributions which are further adopted to mitigate the\naforementioned issues. Thorough rationale analyses and extensive experiments\nare presented to verify our superiority.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:06:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhou", "Jiahuan", ""], ["Tang", "Yansong", ""], ["Su", "Bing", ""], ["Wu", "Ying", ""]]}, {"id": "2107.08918", "submitter": "Kai Zhu", "authors": "Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, Zheng-Jun Zha", "title": "Self-Promoted Prototype Refinement for Few-Shot Class-Incremental\n  Learning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot class-incremental learning is to recognize the new classes given few\nsamples and not forget the old classes. It is a challenging task since\nrepresentation optimization and prototype reorganization can only be achieved\nunder little supervision. To address this problem, we propose a novel\nincremental prototype learning scheme. Our scheme consists of a random episode\nselection strategy that adapts the feature representation to various generated\nincremental episodes to enhance the corresponding extensibility, and a\nself-promoted prototype refinement mechanism which strengthens the expression\nability of the new classes by explicitly considering the dependencies among\ndifferent classes. Particularly, a dynamic relation projection module is\nproposed to calculate the relation matrix in a shared embedding space and\nleverage it as the factor for bootstrapping the update of prototypes. Extensive\nexperiments on three benchmark datasets demonstrate the above-par incremental\nperformance, outperforming state-of-the-art methods by a margin of 13%, 17% and\n11%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:31:33 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhu", "Kai", ""], ["Cao", "Yang", ""], ["Zhai", "Wei", ""], ["Cheng", "Jie", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2107.08939", "submitter": "Seung-Hun Nam", "authors": "Seung-Hun Nam, Wonhyuk Ahn, Myung-Joon Kwon, In-Jae Yu", "title": "Detection of Double Compression in MPEG-4 Videos Using Refined\n  Features-based CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double compression is accompanied by various types of video manipulation and\nits traces can be exploited to determine whether a video is a forgery. This\nLetter presents a convolutional neural network for detecting double compression\nin MPEG-4 videos. Through analysis of the intra-coding process, we utilize two\nrefined features for capturing the subtle artifacts caused by double\ncompression. The discrete cosine transform (DCT) histogram feature effectively\ndetects the change of statistical characteristics in DCT coefficients and the\nparameter-based feature is utilized as auxiliary information to help the\nnetwork learn double compression artifacts. When compared with state-of-the-art\nnetworks and forensic method, the results show that the proposed approach\nachieves a higher performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:53:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nam", "Seung-Hun", ""], ["Ahn", "Wonhyuk", ""], ["Kwon", "Myung-Joon", ""], ["Yu", "In-Jae", ""]]}, {"id": "2107.08943", "submitter": "Jongjin Park", "authors": "Jongjin Park, Sukmin Yun, Jongheon Jeong, Jinwoo Shin", "title": "OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set\n  Unlabeled Data", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern semi-supervised learning methods conventionally assume both labeled\nand unlabeled data have the same class distribution. However, unlabeled data\nmay include out-of-class samples in practice; those that cannot have one-hot\nencoded labels from a closed-set of classes in label data, i.e., unlabeled data\nis an open-set. In this paper, we introduce OpenCoS, a method for handling this\nrealistic semi-supervised learning scenario based on a recent framework of\ncontrastive learning. One of our key findings is that out-of-class samples in\nthe unlabeled dataset can be identified effectively via (unsupervised)\ncontrastive learning. OpenCoS utilizes this information to overcome the failure\nmodes in the existing state-of-the-art semi-supervised methods, e.g.,\nReMixMatch or FixMatch. It further improves the semi-supervised performance by\nutilizing soft- and pseudo-labels on open-set unlabeled data, learned from\ncontrastive learning. Our extensive experimental results show the effectiveness\nof OpenCoS, fixing the state-of-the-art semi-supervised methods to be suitable\nfor diverse scenarios involving open-set unlabeled data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 06:10:05 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Park", "Jongjin", ""], ["Yun", "Sukmin", ""], ["Jeong", "Jongheon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2107.08948", "submitter": "Carsten Ditzel", "authors": "Carsten Ditzel and Klaus Dietmayer", "title": "GenRadar: Self-supervised Probabilistic Camera Synthesis based on Radar\n  Frequencies", "comments": "concurrently submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems require a continuous and dependable environment perception\nfor navigation and decision-making, which is best achieved by combining\ndifferent sensor types. Radar continues to function robustly in compromised\ncircumstances in which cameras become impaired, guaranteeing a steady inflow of\ninformation. Yet, camera images provide a more intuitive and readily applicable\nimpression of the world. This work combines the complementary strengths of both\nsensor types in a unique self-learning fusion approach for a probabilistic\nscene reconstruction in adverse surrounding conditions. After reducing the\nmemory requirements of both high-dimensional measurements through a decoupled\nstochastic self-supervised compression technique, the proposed algorithm\nexploits similarities and establishes correspondences between both domains at\ndifferent feature levels during training. Then, at inference time, relying\nexclusively on radio frequencies, the model successively predicts camera\nconstituents in an autoregressive and self-contained process. These discrete\ntokens are finally transformed back into an instructive view of the respective\nsurrounding, allowing to visually perceive potential dangers for important\ntasks downstream.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:00:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ditzel", "Carsten", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2107.08962", "submitter": "Zenglin Shi", "authors": "Zenglin Shi, Pascal Mettes, Guoyan Zheng, and Cees Snoek", "title": "Frequency-Supervised MR-to-CT Image Synthesis", "comments": "MICCAI workshop on Deep Generative Models, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper strives to generate a synthetic computed tomography (CT) image\nfrom a magnetic resonance (MR) image. The synthetic CT image is valuable for\nradiotherapy planning when only an MR image is available. Recent approaches\nhave made large strides in solving this challenging synthesis problem with\nconvolutional neural networks that learn a mapping from MR inputs to CT\noutputs. In this paper, we find that all existing approaches share a common\nlimitation: reconstruction breaks down in and around the high-frequency parts\nof CT images. To address this common limitation, we introduce\nfrequency-supervised deep networks to explicitly enhance high-frequency\nMR-to-CT image reconstruction. We propose a frequency decomposition layer that\nlearns to decompose predicted CT outputs into low- and high-frequency\ncomponents, and we introduce a refinement module to improve high-frequency\nreconstruction through high-frequency adversarial learning. Experimental\nresults on a new dataset with 45 pairs of 3D MR-CT brain images show the\neffectiveness and potential of the proposed approach. Code is available at\n\\url{https://github.com/shizenglin/Frequency-Supervised-MR-to-CT-Image-Synthesis}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:18:36 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shi", "Zenglin", ""], ["Mettes", "Pascal", ""], ["Zheng", "Guoyan", ""], ["Snoek", "Cees", ""]]}, {"id": "2107.08964", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Stefan Winzeck, Evgenios N. Kornaropoulos,\n  Daniel Whitehouse, Cameron Englman, Poe Phyu, Norman Pao, David K. Menon,\n  Daniel Rueckert, Tilak Das, Virginia F.J. Newcombe, Ben Glocker", "title": "Transductive image segmentation: Self-training and effect of uncertainty\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) uses unlabeled data during training to learn\nbetter models. Previous studies on SSL for medical image segmentation focused\nmostly on improving model generalization to unseen data. In some applications,\nhowever, our primary interest is not generalization but to obtain optimal\npredictions on a specific unlabeled database that is fully available during\nmodel development. Examples include population studies for extracting imaging\nphenotypes. This work investigates an often overlooked aspect of SSL,\ntransduction. It focuses on the quality of predictions made on the unlabeled\ndata of interest when they are included for optimization during training,\nrather than improving generalization. We focus on the self-training framework\nand explore its potential for transduction. We analyze it through the lens of\nInformation Gain and reveal that learning benefits from the use of calibrated\nor under-confident models. Our extensive experiments on a large MRI database\nfor multi-class segmentation of traumatic brain lesions shows promising results\nwhen comparing transductive with inductive predictions. We believe this study\nwill inspire further research on transductive learning, a well-suited paradigm\nfor medical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:26:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Winzeck", "Stefan", ""], ["Kornaropoulos", "Evgenios N.", ""], ["Whitehouse", "Daniel", ""], ["Englman", "Cameron", ""], ["Phyu", "Poe", ""], ["Pao", "Norman", ""], ["Menon", "David K.", ""], ["Rueckert", "Daniel", ""], ["Das", "Tilak", ""], ["Newcombe", "Virginia F. J.", ""], ["Glocker", "Ben", ""]]}, {"id": "2107.08976", "submitter": "Rajat Koner", "authors": "Rajat Koner, Poulami Sinhamahapatra, Karsten Roscher, Stephan\n  G\\\"unnemann, Volker Tresp", "title": "OODformer: Out-Of-Distribution Detection Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A serious problem in image classification is that a trained model might\nperform well for input data that originates from the same distribution as the\ndata available for model training, but performs much worse for\nout-of-distribution (OOD) samples. In real-world safety-critical applications,\nin particular, it is important to be aware if a new data point is OOD. To date,\nOOD detection is typically addressed using either confidence scores,\nauto-encoder based reconstruction, or by contrastive learning. However, the\nglobal image context has not yet been explored to discriminate the non-local\nobjectness between in-distribution and OOD samples. This paper proposes a\nfirst-of-its-kind OOD detection architecture named OODformer that leverages the\ncontextualization capabilities of the transformer. Incorporating the\ntrans\\-former as the principal feature extractor allows us to exploit the\nobject concepts and their discriminate attributes along with their\nco-occurrence via visual attention. Using the contextualised embedding, we\ndemonstrate OOD detection using both class-conditioned latent space similarity\nand a network confidence score. Our approach shows improved generalizability\nacross various datasets. We have achieved a new state-of-the-art result on\nCIFAR-10/-100 and ImageNet30.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:46:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Koner", "Rajat", ""], ["Sinhamahapatra", "Poulami", ""], ["Roscher", "Karsten", ""], ["G\u00fcnnemann", "Stephan", ""], ["Tresp", "Volker", ""]]}, {"id": "2107.08982", "submitter": "Xing Wei", "authors": "Dahu Shi, Xing Wei, Xiaodong Yu, Wenming Tan, Ye Ren, Shiliang Pu", "title": "InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose\n  Estimation", "comments": "arXiv admin note: text overlap with arXiv:1911.07451,\n  arXiv:2003.05664, arXiv:2102.03026 by other authors", "journal-ref": "ACM International Conference on Multimedia (ACM MM), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is an attractive and challenging task. Existing\nmethods are mostly based on two-stage frameworks, which include top-down and\nbottom-up methods. Two-stage methods either suffer from high computational\nredundancy for additional person detectors or they need to group keypoints\nheuristically after predicting all the instance-agnostic keypoints. The\nsingle-stage paradigm aims to simplify the multi-person pose estimation\npipeline and receives a lot of attention. However, recent single-stage methods\nhave the limitation of low performance due to the difficulty of regressing\nvarious full-body poses from a single feature vector. Different from previous\nsolutions that involve complex heuristic designs, we present a simple yet\neffective solution by employing instance-aware dynamic networks. Specifically,\nwe propose an instance-aware module to adaptively adjust (part of) the network\nparameters for each instance. Our solution can significantly increase the\ncapacity and adaptive-ability of the network for recognizing various poses,\nwhile maintaining a compact end-to-end trainable pipeline. Extensive\nexperiments on the MS-COCO dataset demonstrate that our method achieves\nsignificant improvement over existing single-stage methods, and makes a better\nbalance of accuracy and efficiency compared to the state-of-the-art two-stage\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:56:09 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 01:34:59 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Shi", "Dahu", ""], ["Wei", "Xing", ""], ["Yu", "Xiaodong", ""], ["Tan", "Wenming", ""], ["Ren", "Ye", ""], ["Pu", "Shiliang", ""]]}, {"id": "2107.08990", "submitter": "Na Li", "authors": "Na Li and Xinbo Zhao", "title": "A Benchmark for Gait Recognition under Occlusion Collected by\n  Multi-Kinect SDAS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human gait is one of important biometric characteristics for human\nidentification at a distance. In practice, occlusion usually occurs and\nseriously affects accuracy of gait recognition. However, there is no available\ndatabase to support in-depth research of this problem, and state-of-arts gait\nrecognition methods have not paid enough attention to it, thus this paper\nfocuses on gait recognition under occlusion. We collect a new gait recognition\ndatabase called OG RGB+D database, which breaks through the limitation of other\ngait databases and includes multimodal gait data of various occlusions\n(self-occlusion, active occlusion, and passive occlusion) by our multiple\nsynchronous Azure Kinect DK sensors data acquisition system (multi-Kinect SDAS)\nthat can be also applied in security situations. Because Azure Kinect DK can\nsimultaneously collect multimodal data to support different types of gait\nrecognition algorithms, especially enables us to effectively obtain\ncamera-centric multi-person 3D poses, and multi-view is better to deal with\nocclusion than single-view. In particular, the OG RGB+D database provides\naccurate silhouettes and the optimized human 3D joints data (OJ) by fusing data\ncollected by multi-Kinects which are more accurate in human pose representation\nunder occlusion. We also use the OJ data to train an advanced 3D multi-person\npose estimation model to improve its accuracy of pose estimation under\nocclusion for universality. Besides, as human pose is less sensitive to\nocclusion than human appearance, we propose a novel gait recognition method\nSkeletonGait based on human dual skeleton model using a framework of siamese\nspatio-temporal graph convolutional networks (siamese ST-GCN). The evaluation\nresults demonstrate that SkeletonGait has competitive performance compared with\nstate-of-art gait recognition methods on OG RGB+D database and popular CAISA-B\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:01:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Na", ""], ["Zhao", "Xinbo", ""]]}, {"id": "2107.08994", "submitter": "Hidenobu Matsuki", "authors": "Hidenobu Matsuki, Raluca Scona, Jan Czarnowski and Andrew J. Davison", "title": "CodeMapping: Real-Time Dense Mapping for Sparse SLAM using Compact Scene\n  Representations", "comments": "Accepted to IEEE Robotics and Automation Letters (RA-L) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel dense mapping framework for sparse visual SLAM systems\nwhich leverages a compact scene representation. State-of-the-art sparse visual\nSLAM systems provide accurate and reliable estimates of the camera trajectory\nand locations of landmarks. While these sparse maps are useful for\nlocalization, they cannot be used for other tasks such as obstacle avoidance or\nscene understanding. In this paper we propose a dense mapping framework to\ncomplement sparse visual SLAM systems which takes as input the camera poses,\nkeyframes and sparse points produced by the SLAM system and predicts a dense\ndepth image for every keyframe. We build on CodeSLAM and use a variational\nautoencoder (VAE) which is conditioned on intensity, sparse depth and\nreprojection error images from sparse SLAM to predict an uncertainty-aware\ndense depth map. The use of a VAE then enables us to refine the dense depth\nimages through multi-view optimization which improves the consistency of\noverlapping frames. Our mapper runs in a separate thread in parallel to the\nSLAM system in a loosely coupled manner. This flexible design allows for\nintegration with arbitrary metric sparse SLAM systems without delaying the main\nSLAM process. Our dense mapper can be used not only for local mapping but also\nglobally consistent dense 3D reconstruction through TSDF fusion. We demonstrate\nour system running with ORB-SLAM3 and show accurate dense depth estimation\nwhich could enable applications such as robotics and augmented reality.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:13:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Matsuki", "Hidenobu", ""], ["Scona", "Raluca", ""], ["Czarnowski", "Jan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2107.09011", "submitter": "Vibashan V S", "authors": "Vibashan VS, Jeya Maria Jose Valanarasu, Poojan Oza and Vishal M.\n  Patel", "title": "Image Fusion Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image fusion, images obtained from different sensors are fused to generate\na single image with enhanced information. In recent years, state-of-the-art\nmethods have adopted Convolution Neural Networks (CNNs) to encode meaningful\nfeatures for image fusion. Specifically, CNN-based methods perform image fusion\nby fusing local features. However, they do not consider long-range dependencies\nthat are present in the image. Transformer-based models are designed to\novercome this by modeling the long-range dependencies with the help of\nself-attention mechanism. This motivates us to propose a novel Image Fusion\nTransformer (IFT) where we develop a transformer-based multi-scale fusion\nstrategy that attends to both local and long-range information (or global\ncontext). The proposed method follows a two-stage training approach. In the\nfirst stage, we train an auto-encoder to extract deep features at multiple\nscales. In the second stage, multi-scale features are fused using a\nSpatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of\na CNN and a transformer branch which capture local and long-range features,\nrespectively. Extensive experiments on multiple benchmark datasets show that\nthe proposed method performs better than many competitive fusion algorithms.\nFurthermore, we show the effectiveness of the proposed ST fusion strategy with\nan ablation analysis. The source code is available at:\nhttps://github.com/Vibashan/Image-Fusion-Transformer.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:42:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 15:34:03 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["VS", "Vibashan", ""], ["Valanarasu", "Jeya Maria Jose", ""], ["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2107.09046", "submitter": "Sarah Young", "authors": "Sarah Young, Jyothish Pari, Pieter Abbeel, Lerrel Pinto", "title": "Playful Interactions for Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the key challenges in visual imitation learning is collecting large\namounts of expert demonstrations for a given task. While methods for collecting\nhuman demonstrations are becoming easier with teleoperation methods and the use\nof low-cost assistive tools, we often still require 100-1000 demonstrations for\nevery task to learn a visual representation and policy. To address this, we\nturn to an alternate form of data that does not require task-specific\ndemonstrations -- play. Playing is a fundamental method children use to learn a\nset of skills and behaviors and visual representations in early learning.\nImportantly, play data is diverse, task-agnostic, and relatively cheap to\nobtain. In this work, we propose to use playful interactions in a\nself-supervised manner to learn visual representations for downstream tasks. We\ncollect 2 hours of playful data in 19 diverse environments and use\nself-predictive learning to extract visual representations. Given these\nrepresentations, we train policies using imitation learning for two downstream\ntasks: Pushing and Stacking. We demonstrate that our visual representations\ngeneralize better than standard behavior cloning and can achieve similar\nperformance with only half the number of required demonstrations. Our\nrepresentations, which are trained from scratch, compare favorably against\nImageNet pretrained representations. Finally, we provide an experimental\nanalysis on the effects of different pretraining modes on downstream task\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:54:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Young", "Sarah", ""], ["Pari", "Jyothish", ""], ["Abbeel", "Pieter", ""], ["Pinto", "Lerrel", ""]]}, {"id": "2107.09047", "submitter": "Edward Hu S.", "authors": "Edward S. Hu, Kun Huang, Oleh Rybkin, Dinesh Jayaraman", "title": "Know Thyself: Transferable Visuomotor Control Through Robot-Awareness", "comments": "Website: https://hueds.github.io/rac/ Updated typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training visuomotor robot controllers from scratch on a new robot typically\nrequires generating large amounts of robot-specific data. Could we leverage\ndata previously collected on another robot to reduce or even completely remove\nthis need for robot-specific data? We propose a \"robot-aware\" solution paradigm\nthat exploits readily available robot \"self-knowledge\" such as proprioception,\nkinematics, and camera calibration to achieve this. First, we learn modular\ndynamics models that pair a transferable, robot-agnostic world dynamics module\nwith a robot-specific, analytical robot dynamics module. Next, we set up visual\nplanning costs that draw a distinction between the robot self and the world.\nOur experiments on tabletop manipulation tasks in simulation and on real robots\ndemonstrate that these plug-in improvements dramatically boost the\ntransferability of visuomotor controllers, even permitting zero-shot transfer\nonto new robots for the very first time. Project website:\nhttps://hueds.github.io/rac/\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:56:04 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 02:57:35 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hu", "Edward S.", ""], ["Huang", "Kun", ""], ["Rybkin", "Oleh", ""], ["Jayaraman", "Dinesh", ""]]}, {"id": "2107.09060", "submitter": "Thomas K\\\"ustner", "authors": "Thomas K\\\"ustner, Jiazhen Pan, Haikun Qi, Gastao Cruz, Christopher\n  Gilliam, Thierry Blu, Bin Yang, Sergios Gatidis, Ren\\'e Botnar, Claudia\n  Prieto", "title": "LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance\n  Imaging", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2021.3096131", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physiological motion, such as cardiac and respiratory motion, during Magnetic\nResonance (MR) image acquisition can cause image artifacts. Motion correction\ntechniques have been proposed to compensate for these types of motion during\nthoracic scans, relying on accurate motion estimation from undersampled\nmotion-resolved reconstruction. A particular interest and challenge lie in the\nderivation of reliable non-rigid motion fields from the undersampled\nmotion-resolved data. Motion estimation is usually formulated in image space\nvia diffusion, parametric-spline, or optical flow methods. However, image-based\nregistration can be impaired by remaining aliasing artifacts due to the\nundersampled motion-resolved reconstruction. In this work, we describe a\nformalism to perform non-rigid registration directly in the sampled Fourier\nspace, i.e. k-space. We propose a deep-learning based approach to perform fast\nand accurate non-rigid registration from the undersampled k-space data. The\nbasic working principle originates from the Local All-Pass (LAP) technique, a\nrecently introduced optical flow-based registration. The proposed LAPNet is\ncompared against traditional and deep learning image-based registrations and\ntested on fully-sampled and highly-accelerated (with two undersampling\nstrategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients\nwith suspected liver or lung metastases and 25 healthy subjects. The proposed\nLAPNet provided consistent and superior performance to image-based approaches\nthroughout different sampling trajectories and acceleration factors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:39:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["K\u00fcstner", "Thomas", ""], ["Pan", "Jiazhen", ""], ["Qi", "Haikun", ""], ["Cruz", "Gastao", ""], ["Gilliam", "Christopher", ""], ["Blu", "Thierry", ""], ["Yang", "Bin", ""], ["Gatidis", "Sergios", ""], ["Botnar", "Ren\u00e9", ""], ["Prieto", "Claudia", ""]]}, {"id": "2107.09092", "submitter": "Manu Tom", "authors": "Manu Tom, Yuchang Jiang, Emmanuel Baltsavias, Konrad Schindler", "title": "Learning a Sensor-invariant Embedding of Satellite Data: A Case Study\n  for Lake Ice Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fusing satellite imagery acquired with different sensors has been a\nlong-standing challenge of Earth observation, particularly across different\nmodalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we\nexplore the joint analysis of imagery from different sensors in the light of\nrepresentation learning: we propose to learn a joint, sensor-invariant\nembedding (feature representation) within a deep neural network. Our\napplication problem is the monitoring of lake ice on Alpine lakes. To reach the\ntemporal resolution requirement of the Swiss Global Climate Observing System\n(GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra\nMODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains\nand between the sensor resolutions make this a challenging instance of the\nsensor fusion problem. Our approach can be classified as a feature-level fusion\nthat is learnt in a data-driven manner. The proposed network architecture has\nseparate encoding branches for each image sensor, which feed into a single\nlatent embedding. I.e., a common feature representation shared by all inputs,\nsuch that subsequent processing steps deliver comparable output irrespective of\nwhich sort of input image was used. By fusing satellite data, we map lake ice\nat a temporal resolution of <1.5 days. The network produces spatially explicit\nlake ice maps with pixel-wise accuracies >91.3% (respectively, mIoU scores\n>60.7%) and generalises well across different lakes and winters. Moreover, it\nsets a new state-of-the-art for determining the important ice-on and ice-off\ndates for the target lakes, in many cases meeting the GCOS requirement.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 18:11:55 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tom", "Manu", ""], ["Jiang", "Yuchang", ""], ["Baltsavias", "Emmanuel", ""], ["Schindler", "Konrad", ""]]}, {"id": "2107.09101", "submitter": "Erion-Vasilis Pikoulis", "authors": "Stavros Nousias, Erion-Vasilis Pikoulis, Christos Mavrokefalidis, Aris\n  S. Lalos", "title": "Accelerating deep neural networks for efficient scene understanding in\n  automotive cyber-physical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount\nof interest in the past few decades, while one of the most critical operations\nin these systems is the perception of the environment. Deep learning and,\nespecially, the use of Deep Neural Networks (DNNs) provides impressive results\nin analyzing and understanding complex and dynamic scenes from visual data. The\nprediction horizons for those perception systems are very short and inference\nmust often be performed in real time, stressing the need of transforming the\noriginal large pre-trained networks into new smaller models, by utilizing Model\nCompression and Acceleration (MCA) techniques. Our goal in this work is to\ninvestigate best practices for appropriately applying novel weight sharing\ntechniques, optimizing the available variables and the training procedures\ntowards the significant acceleration of widely adopted DNNs. Extensive\nevaluation studies carried out using various state-of-the-art DNN models in\nobject detection and tracking experiments, provide details about the type of\nerrors that manifest after the application of weight sharing techniques,\nresulting in significant acceleration gains with negligible accuracy losses.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 18:43:17 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Nousias", "Stavros", ""], ["Pikoulis", "Erion-Vasilis", ""], ["Mavrokefalidis", "Christos", ""], ["Lalos", "Aris S.", ""]]}, {"id": "2107.09106", "submitter": "Spencer Whitehead", "authors": "Spencer Whitehead, Hui Wu, Heng Ji, Rogerio Feris, Kate Saenko", "title": "Separating Skills and Concepts for Novel Visual Question Answering", "comments": "Paper at CVPR 2021. 14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization to out-of-distribution data has been a problem for Visual\nQuestion Answering (VQA) models. To measure generalization to novel questions,\nwe propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual\ntasks, such as counting or attribute recognition, and are applied to \"concepts\"\nmentioned in the question, such as objects and people. VQA methods should be\nable to compose skills and concepts in novel ways, regardless of whether the\nspecific composition has been seen in training, yet we demonstrate that\nexisting models have much to improve upon towards handling new compositions. We\npresent a novel method for learning to compose skills and concepts that\nseparates these two factors implicitly within a model by learning grounded\nconcept representations and disentangling the encoding of skills from that of\nconcepts. We enforce these properties with a novel contrastive learning\nprocedure that does not rely on external annotations and can be learned from\nunlabeled image-question pairs. Experiments demonstrate the effectiveness of\nour approach for improving compositional and grounding performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 18:55:10 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Whitehead", "Spencer", ""], ["Wu", "Hui", ""], ["Ji", "Heng", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""]]}, {"id": "2107.09118", "submitter": "Abbas Khosravi", "authors": "Donya Khaledyan, AmirReza Tajally, Ali Sarkhosh, Afshar Shamsi, Hamzeh\n  Asgharnezhad, Abbas Khosravi, Saeid Nahavandi", "title": "Confidence Aware Neural Networks for Skin Cancer Detection", "comments": "21 Pages, 7 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) models have received particular attention in medical\nimaging due to their promising pattern recognition capabilities. However, Deep\nNeural Networks (DNNs) require a huge amount of data, and because of the lack\nof sufficient data in this field, transfer learning can be a great solution.\nDNNs used for disease diagnosis meticulously concentrate on improving the\naccuracy of predictions without providing a figure about their confidence of\npredictions. Knowing how much a DNN model is confident in a computer-aided\ndiagnosis model is necessary for gaining clinicians' confidence and trust in\nDL-based solutions. To address this issue, this work presents three different\nmethods for quantifying uncertainties for skin cancer detection from images. It\nalso comprehensively evaluates and compares performance of these DNNs using\nnovel uncertainty-related metrics. The obtained results reveal that the\npredictive uncertainty estimation methods are capable of flagging risky and\nerroneous predictions with a high uncertainty estimate. We also demonstrate\nthat ensemble approaches are more reliable in capturing uncertainties through\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 19:21:57 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 15:20:54 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Khaledyan", "Donya", ""], ["Tajally", "AmirReza", ""], ["Sarkhosh", "Ali", ""], ["Shamsi", "Afshar", ""], ["Asgharnezhad", "Hamzeh", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2107.09126", "submitter": "Benjamin Spetter-Goldstein", "authors": "Benjamin Spetter-Goldstein, Nataniel Ruiz, Sarah Adel Bargal", "title": "Examining the Human Perceptibility of Black-Box Adversarial Attacks on\n  Face Recognition", "comments": "5 pages, 5 figures, submitted to AdvML @ ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern open internet contains billions of public images of human faces\nacross the web, especially on social media websites used by half the world's\npopulation. In this context, Face Recognition (FR) systems have the potential\nto match faces to specific names and identities, creating glaring privacy\nconcerns. Adversarial attacks are a promising way to grant users privacy from\nFR systems by disrupting their capability to recognize faces. Yet, such attacks\ncan be perceptible to human observers, especially under the more challenging\nblack-box threat model. In the literature, the justification for the\nimperceptibility of such attacks hinges on bounding metrics such as $\\ell_p$\nnorms. However, there is not much research on how these norms match up with\nhuman perception. Through examining and measuring both the effectiveness of\nrecent black-box attacks in the face recognition setting and their\ncorresponding human perceptibility through survey data, we demonstrate the\ntrade-offs in perceptibility that occur as attacks become more aggressive. We\nalso show how the $\\ell_2$ norm and other metrics do not correlate with human\nperceptibility in a linear fashion, thus making these norms suboptimal at\nmeasuring adversarial attack perceptibility.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 19:45:44 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Spetter-Goldstein", "Benjamin", ""], ["Ruiz", "Nataniel", ""], ["Bargal", "Sarah Adel", ""]]}, {"id": "2107.09134", "submitter": "Daniel M\\'ario Lima", "authors": "Daniel Lima, Catharine Graves, Marco Gutierrez, Bruno Brandoli, Jose\n  Rodrigues-Jr", "title": "Convolutional module for heart localization and segmentation in MRI", "comments": "Submitted to CMIG", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Magnetic resonance imaging (MRI) is a widely known medical imaging technique\nused to assess the heart function. Deep learning (DL) models perform several\ntasks in cardiac MRI (CMR) images with good efficacy, such as segmentation,\nestimation, and detection of diseases. Many DL models based on convolutional\nneural networks (CNN) were improved by detecting regions-of-interest (ROI)\neither automatically or by hand. In this paper we describe Visual-Motion-Focus\n(VMF), a module that detects the heart motion in the 4D MRI sequence, and\nhighlights ROIs by focusing a Radial Basis Function (RBF) on the estimated\nmotion field. We experimented and evaluated VMF on three CMR datasets,\nobserving that the proposed ROIs cover 99.7% of data labels (Recall score),\nimproved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI\nextraction, and improved the overall training speed by 2.5 times (+150%).\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 20:19:40 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lima", "Daniel", ""], ["Graves", "Catharine", ""], ["Gutierrez", "Marco", ""], ["Brandoli", "Bruno", ""], ["Rodrigues-Jr", "Jose", ""]]}, {"id": "2107.09136", "submitter": "Jo\\~ao Dick", "authors": "Jo\\~ao Dick, Brunno Abreu, Mateus Grellert, Sergio Bampi", "title": "Quality and Complexity Assessment of Learning-Based Image Compression\n  Solutions", "comments": "Paper accepted at ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents an analysis of state-of-the-art learning-based image\ncompression techniques. We compare 8 models available in the Tensorflow\nCompression package in terms of visual quality metrics and processing time,\nusing the KODAK data set. The results are compared with the Better Portable\nGraphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the\nlowest execution times compared with the fastest learning-based model, with a\nspeedup of 1.46x in compression and 30x in decompression. However, the\nlearning-based models achieved improvements over JPEG2000 in terms of quality,\nspecially for lower bitrates. Our findings also show that BPG is more efficient\nin terms of PSNR, but the learning models are better for other quality metrics,\nand sometimes even faster. The results indicate that learning-based techniques\nare promising solutions towards a future mainstream compression method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 20:20:38 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Dick", "Jo\u00e3o", ""], ["Abreu", "Brunno", ""], ["Grellert", "Mateus", ""], ["Bampi", "Sergio", ""]]}, {"id": "2107.09170", "submitter": "Juan Pablo De Vicente", "authors": "Juan Pablo de Vicente, Alvaro Soto", "title": "DeepSocNav: Social Navigation by Imitating Human Behaviors", "comments": "6 pages, Accepted paper at the RSS Workshop on Social Robot\n  Navigation 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current datasets to train social behaviors are usually borrowed from\nsurveillance applications that capture visual data from a bird's-eye\nperspective. This leaves aside precious relationships and visual cues that\ncould be captured through a first-person view of a scene. In this work, we\npropose a strategy to exploit the power of current game engines, such as Unity,\nto transform pre-existing bird's-eye view datasets into a first-person view, in\nparticular, a depth view. Using this strategy, we are able to generate large\nvolumes of synthetic data that can be used to pre-train a social navigation\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\nthat takes advantage of the proposed approach to generate synthetic data.\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\nan auxiliary task. This consists of predicting the next depth frame that the\nagent will face. Our experiments show the benefits of the proposed model that\nis able to outperform relevant baselines in terms of social navigation scores.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 21:51:06 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["de Vicente", "Juan Pablo", ""], ["Soto", "Alvaro", ""]]}, {"id": "2107.09179", "submitter": "Navid Mahmoudian Bidgoli", "authors": "Navid Mahmoudian Bidgoli, Roberto G. de A. Azevedo, Thomas Maugey,\n  Aline Roumy, Pascal Frossard", "title": "OSLO: On-the-Sphere Learning for Omnidirectional images and its\n  application to 360-degree image compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art 2D image compression schemes rely on the power of\nconvolutional neural networks (CNNs). Although CNNs offer promising\nperspectives for 2D image compression, extending such models to omnidirectional\nimages is not straightforward. First, omnidirectional images have specific\nspatial and statistical properties that can not be fully captured by current\nCNN models. Second, basic mathematical operations composing a CNN architecture,\ne.g., translation and sampling, are not well-defined on the sphere. In this\npaper, we study the learning of representation models for omnidirectional\nimages and propose to use the properties of HEALPix uniform sampling of the\nsphere to redefine the mathematical tools used in deep learning models for\nomnidirectional images. In particular, we: i) propose the definition of a new\nconvolution operation on the sphere that keeps the high expressiveness and the\nlow complexity of a classical 2D convolution; ii) adapt standard CNN techniques\nsuch as stride, iterative aggregation, and pixel shuffling to the spherical\ndomain; and then iii) apply our new framework to the task of omnidirectional\nimage compression. Our experiments show that our proposed on-the-sphere\nsolution leads to a better compression gain that can save 13.7% of the bit rate\ncompared to similar learned models applied to equirectangular images. Also,\ncompared to learning models based on graph convolutional networks, our solution\nsupports more expressive filters that can preserve high frequencies and provide\na better perceptual quality of the compressed images. Such results demonstrate\nthe efficiency of the proposed framework, which opens new research venues for\nother omnidirectional vision tasks to be effectively implemented on the sphere\nmanifold.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 22:14:30 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bidgoli", "Navid Mahmoudian", ""], ["Azevedo", "Roberto G. de A.", ""], ["Maugey", "Thomas", ""], ["Roumy", "Aline", ""], ["Frossard", "Pascal", ""]]}, {"id": "2107.09204", "submitter": "Vincent Wilmet", "authors": "Vincent Wilmet, Sauraj Verma, Tabea Redl, H{\\aa}kon Sandaker, Zhenning\n  Li", "title": "A Comparison of Supervised and Unsupervised Deep Learning Methods for\n  Anomaly Detection in Images", "comments": "8 pages, for FML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection in images plays a significant role for many applications\nacross all industries, such as disease diagnosis in healthcare or quality\nassurance in manufacturing. Manual inspection of images, when extended over a\nmonotonously repetitive period of time is very time consuming and can lead to\nanomalies being overlooked.Artificial neural networks have proven themselves\nvery successful on simple, repetitive tasks, in some cases even outperforming\nhumans. Therefore, in this paper we investigate different methods of deep\nlearning, including supervised and unsupervised learning, for anomaly detection\napplied to a quality assurance use case. We utilize the MVTec anomaly dataset\nand develop three different models, a CNN for supervised anomaly detection,\nKD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly\ndetection and a DCGAN for generating reconstructed images. By experiments, we\nfound that KD-CAE performs better on the anomaly datasets compared to CNN and\nNI-CAE, with NI-CAE performing the best on the Transistor dataset. We also\nimplemented a DCGAN for the creation of new training data but due to\ncomputational limitation and lack of extrapolating the mechanics of AnoGAN, we\nrestricted ourselves just to the generation of GAN based images. We conclude\nthat unsupervised methods are more powerful for anomaly detection in images,\nespecially in a setting where only a small amount of anomalous data is\navailable, or the data is unlabeled.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 00:14:12 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wilmet", "Vincent", ""], ["Verma", "Sauraj", ""], ["Redl", "Tabea", ""], ["Sandaker", "H\u00e5kon", ""], ["Li", "Zhenning", ""]]}, {"id": "2107.09211", "submitter": "Rohan Mahadev", "authors": "Rohan Mahadev, Anindya Chakravarti", "title": "Understanding Gender and Racial Disparities in Image Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Large scale image classification models trained on top of popular datasets\nsuch as Imagenet have shown to have a distributional skew which leads to\ndisparities in prediction accuracies across different subsections of population\ndemographics. A lot of approaches have been made to solve for this\ndistributional skew using methods that alter the model pre, post and during\ntraining. We investigate one such approach - which uses a multi-label softmax\nloss with cross-entropy as the loss function instead of a binary cross-entropy\non a multi-label classification problem on the Inclusive Images dataset which\nis a subset of the OpenImages V6 dataset. We use the MR2 dataset, which\ncontains images of people with self-identified gender and race attributes to\nevaluate the fairness in the model outcomes and try to interpret the mistakes\nby looking at model activations and suggest possible fixes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 01:05:31 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mahadev", "Rohan", ""], ["Chakravarti", "Anindya", ""]]}, {"id": "2107.09225", "submitter": "Shaohao Lu", "authors": "Shaohao Lu, Yuqiao Xian, Ke Yan, Yi Hu, Xing Sun, Xiaowei Guo, Feiyue\n  Huang, Wei-Shi Zheng", "title": "Discriminator-Free Generative Adversarial Attack", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": "10.1145/3474085.3475290", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Deep Neural Networks are vulnerable toadversarial exam-ples(Figure 1),\nmaking the DNNs-based systems collapsed byadding the inconspicuous\nperturbations to the images. Most of the existing works for adversarial attack\nare gradient-based and suf-fer from the latency efficiencies and the load on\nGPU memory. Thegenerative-based adversarial attacks can get rid of this\nlimitation,and some relative works propose the approaches based on GAN.However,\nsuffering from the difficulty of the convergence of train-ing a GAN, the\nadversarial examples have either bad attack abilityor bad visual quality. In\nthis work, we find that the discriminatorcould be not necessary for\ngenerative-based adversarial attack, andpropose theSymmetric Saliency-based\nAuto-Encoder (SSAE)to generate the perturbations, which is composed of the\nsaliencymap module and the angle-norm disentanglement of the featuresmodule.\nThe advantage of our proposed method lies in that it is notdepending on\ndiscriminator, and uses the generative saliency map to pay more attention to\nlabel-relevant regions. The extensive exper-iments among the various tasks,\ndatasets, and models demonstratethat the adversarial examples generated by SSAE\nnot only make thewidely-used models collapse, but also achieves good visual\nquality.The code is available at https://github.com/BravoLu/SSAE.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 01:55:21 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lu", "Shaohao", ""], ["Xian", "Yuqiao", ""], ["Yan", "Ke", ""], ["Hu", "Yi", ""], ["Sun", "Xing", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2107.09237", "submitter": "Jie Chen", "authors": "Xingxing Yang, Jie Chen, Zaifeng Yang, and Zhenghua Chen", "title": "Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic\n  and Texture Clues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near infrared (NIR) imaging has been widely applied in low-light imaging\nscenarios; however, it is difficult for human and algorithms to perceive the\nreal scene in the colorless NIR domain. While Generative Adversarial Network\n(GAN) has been widely employed in various image colorization tasks, it is\nchallenging for a direct mapping mechanism, such as a conventional GAN, to\ntransform an image from the NIR to the RGB domain with correct semantic\nreasoning, well-preserved textures, and vivid color combinations concurrently.\nIn this work, we propose a novel Attention-based NIR image colorization\nframework via Adaptive Fusion of Semantic and Texture clues, aiming at\nachieving these goals within the same framework. The tasks of texture transfer\nand semantic reasoning are carried out in two separate network blocks.\nSpecifically, the Texture Transfer Block (TTB) aims at extracting texture\nfeatures from the NIR image's Laplacian component and transferring them for\nsubsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic\nclues and maps the NIR pixel values to the RGB domain. Finally, a Fusion\nAttention Block (FAB) is proposed to adaptively fuse the features from the two\nbranches and generate an optimized colorization result. In order to enhance the\nnetwork's learning capacity in semantic reasoning as well as mapping precision\nin texture transfer, we have proposed the Residual Coordinate Attention Block\n(RCAB), which incorporates coordinate attention into a residual learning\nframework, enabling the network to capture long-range dependencies along the\nchannel direction and meanwhile precise positional information can be preserved\nalong spatial directions. RCAB is also incorporated into FAB to facilitate\naccurate texture alignment during fusion. Both quantitative and qualitative\nevaluations show that the proposed method outperforms state-of-the-art NIR\nimage colorization methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:00:51 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yang", "Xingxing", ""], ["Chen", "Jie", ""], ["Yang", "Zaifeng", ""], ["Chen", "Zhenghua", ""]]}, {"id": "2107.09240", "submitter": "Yi-Fu Wu", "authors": "Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn", "title": "Generative Video Transformer: Can Objects be the Words?", "comments": "Published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have been successful for many natural language processing tasks.\nHowever, applying transformers to the video domain for tasks such as long-term\nvideo generation and scene understanding has remained elusive due to the high\ncomputational complexity and the lack of natural tokenization. In this paper,\nwe propose the Object-Centric Video Transformer (OCVT) which utilizes an\nobject-centric approach for decomposing scenes into tokens suitable for use in\na generative video transformer. By factoring the video into objects, our fully\nunsupervised model is able to learn complex spatio-temporal dynamics of\nmultiple interacting objects in a scene and generate future frames of the\nvideo. Our model is also significantly more memory-efficient than pixel-based\nmodels and thus able to train on videos of length up to 70 frames with a single\n48GB GPU. We compare our model with previous RNN-based approaches as well as\nother possible video transformer baselines. We demonstrate OCVT performs well\nwhen compared to baselines in generating future frames. OCVT also develops\nuseful representations for video reasoning, achieving start-of-the-art\nperformance on the CATER task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:08:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wu", "Yi-Fu", ""], ["Yoon", "Jaesik", ""], ["Ahn", "Sungjin", ""]]}, {"id": "2107.09242", "submitter": "Xu Luo", "authors": "Xu Luo, Yuxuan Chen, Liangjian Wen, Lili Pan, Zenglin Xu", "title": "Boosting few-shot classification with view-learnable contrastive\n  learning", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot classification is to classify new categories with few\nlabeled examples within each class. Nowadays, the excellent performance in\nhandling few-shot classification problems is shown by metric-based\nmeta-learning methods. However, it is very hard for previous methods to\ndiscriminate the fine-grained sub-categories in the embedding space without\nfine-grained labels. This may lead to unsatisfactory generalization to\nfine-grained subcategories, and thus affects model interpretation. To tackle\nthis problem, we introduce the contrastive loss into few-shot classification\nfor learning latent fine-grained structure in the embedding space. Furthermore,\nto overcome the drawbacks of random image transformation used in current\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\nviews), we develop a learning-to-learn algorithm to automatically generate\ndifferent views of the same image. Extensive experiments on standard few-shot\nlearning benchmarks demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:13:33 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Luo", "Xu", ""], ["Chen", "Yuxuan", ""], ["Wen", "Liangjian", ""], ["Pan", "Lili", ""], ["Xu", "Zenglin", ""]]}, {"id": "2107.09244", "submitter": "Li Shen", "authors": "Li Shen, Yao Lu, Hao Chen, Hao Wei, Donghai Xie, Jiabao Yue, Rui Chen,\n  Yue Zhang, Ao Zhang, Shouye Lv, Bitao Jiang", "title": "S2Looking: A Satellite Side-Looking Dataset for Building Change\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collecting large-scale annotated satellite imagery datasets is essential for\ndeep-learning-based global building change surveillance. In particular, the\nscroll imaging mode of optical satellites enables larger observation ranges and\nshorter revisit periods, facilitating efficient global surveillance. However,\nthe images in recent satellite change detection datasets are mainly captured at\nnear-nadir viewing angles. In this paper, we introduce S2Looking, a building\nchange detection dataset that contains large-scale side-looking satellite\nimages captured at varying off-nadir angles. Our S2Looking dataset consists of\n5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel)\nof rural areas throughout the world and more than 65,920 annotated change\ninstances. We provide two label maps to separately indicate the newly built and\ndemolished building regions for each sample in the dataset. We establish a\nbenchmark task based on this dataset, i.e., identifying the pixel-level\nbuilding changes in the bi-temporal images. We test several state-of-the-art\nmethods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset.\nThe experimental results show that recent change detection methods exhibit much\npoorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking\ndataset presents three main challenges: 1) large viewing angle changes, 2)\nlarge illumination variances and 3) various complex scene characteristics\nencountered in rural areas. Our proposed dataset may promote the development of\nalgorithms for satellite image change detection and registration under\nconditions of large off-nadir angles. The dataset is available at\nhttps://github.com/AnonymousForACMMM/.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:31:00 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Shen", "Li", ""], ["Lu", "Yao", ""], ["Chen", "Hao", ""], ["Wei", "Hao", ""], ["Xie", "Donghai", ""], ["Yue", "Jiabao", ""], ["Chen", "Rui", ""], ["Zhang", "Yue", ""], ["Zhang", "Ao", ""], ["Lv", "Shouye", ""], ["Jiang", "Bitao", ""]]}, {"id": "2107.09249", "submitter": "Yifan Zhang", "authors": "Yifan Zhang, Bryan Hooi, Lanqing Hong, Jiashi Feng", "title": "Test-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse\n  Experts with Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing long-tailed recognition methods, aiming to train class-balance\nmodels from long-tailed data, generally assume the models would be evaluated on\nthe uniform test class distribution. However, the practical test class\ndistribution often violates such an assumption (e.g., being long-tailed or even\ninversely long-tailed), which would lead existing methods to fail in real-world\napplications. In this work, we study a more practical task setting, called\ntest-agnostic long-tailed recognition, where the training class distribution is\nlong-tailed while the test class distribution is unknown and can be skewed\narbitrarily. In addition to the issue of class imbalance, this task poses\nanother challenge: the class distribution shift between the training and test\nsamples is unidentified. To address this task, we propose a new method, called\nTest-time Aggregating Diverse Experts (TADE), that presents two solution\nstrategies: (1) a novel skill-diverse expert learning strategy that trains\ndiverse experts to excel at handling different test distributions from a single\nlong-tailed training distribution; (2) a novel test-time expert aggregation\nstrategy that leverages self-supervision to aggregate multiple experts for\nhandling various test distributions. Moreover, we theoretically show that our\nmethod has provable ability to simulate unknown test class distributions.\nPromising results on both vanilla and test-agnostic long-tailed recognition\nverify the effectiveness of TADE. Code is available at\nhttps://github.com/Vanint/TADE-AgnosticLT.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 04:10:31 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhang", "Yifan", ""], ["Hooi", "Bryan", ""], ["Hong", "Lanqing", ""], ["Feng", "Jiashi", ""]]}, {"id": "2107.09255", "submitter": "Shaobo Cai", "authors": "Yuanzhou Chen, Shaobo Cai, Yuxin Wang, Junchi Yan", "title": "Monocular Visual Analysis for Electronic Line Calling of Tennis Games", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electronic Line Calling is an auxiliary referee system used for tennis\nmatches based on binocular vision technology. While ELC has been widely used,\nthere are still many problems, such as complex installation and maintenance,\nhigh cost and etc. We propose a monocular vision technology based ELC method.\nThe method has the following steps. First, locate the tennis ball's trajectory.\nWe propose a multistage tennis ball positioning approach combining background\nsubtraction and color area filtering. Then we propose a bouncing point\nprediction method by minimizing the fitting loss of the uncertain point.\nFinally, we find out whether the bouncing point of the ball is out of bounds or\nnot according to the relative position between the bouncing point and the court\nside line in the two dimensional image. We collected and tagged 394 samples\nwith an accuracy rate of 99.4%, and 81.8% of the 11 samples with bouncing\npoints.The experimental results show that our method is feasible to judge if a\nball is out of the court with monocular vision and significantly reduce complex\ninstallation and costs of ELC system with binocular vision.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 04:23:11 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Yuanzhou", ""], ["Cai", "Shaobo", ""], ["Wang", "Yuxin", ""], ["Yan", "Junchi", ""]]}, {"id": "2107.09262", "submitter": "Sanchita Ghose", "authors": "Sanchita Ghose and John J. Prevost", "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based\n  Synchronous Sound Generation in Silent Videos", "comments": "This article is under review in IEEE Transaction on Multimedia. It\n  contains total 12 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning based visual to sound generation systems essentially need to be\ndeveloped particularly considering the synchronicity aspects of visual and\naudio features with time. In this research we introduce a novel task of guiding\na class conditioned generative adversarial network with the temporal visual\ninformation of a video input for visual to sound generation task adapting the\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\nmodel is capable of conditioning action sequences of visual events leading\ntowards generating visually aligned realistic sound tracks. We expand our\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\nour synthesized sound through human survey that shows noteworthy (on average\n81\\%) audio-visual synchronicity performance. Our approach also outperforms in\nstatistical experiments compared with other baseline models and audio-visual\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 04:59:26 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ghose", "Sanchita", ""], ["Prevost", "John J.", ""]]}, {"id": "2107.09270", "submitter": "Mingjie He", "authors": "Mingjie He, Jie Zhang, Shiguang Shan, Xiao Liu, Zhongqin Wu, Xilin\n  Chen", "title": "Locality-aware Channel-wise Dropout for Occluded Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition remains a challenging task in unconstrained scenarios,\nespecially when faces are partially occluded. To improve the robustness against\nocclusion, augmenting the training images with artificial occlusions has been\nproved as a useful approach. However, these artificial occlusions are commonly\ngenerated by adding a black rectangle or several object templates including\nsunglasses, scarfs and phones, which cannot well simulate the realistic\nocclusions. In this paper, based on the argument that the occlusion essentially\ndamages a group of neurons, we propose a novel and elegant occlusion-simulation\nmethod via dropping the activations of a group of neurons in some elaborately\nselected channel. Specifically, we first employ a spatial regularization to\nencourage each feature channel to respond to local and different face regions.\nIn this way, the activations affected by an occlusion in a local region are\nmore likely to be located in a single feature channel. Then, the locality-aware\nchannel-wise dropout (LCD) is designed to simulate the occlusion by dropping\nout the entire feature channel. Furthermore, by randomly dropping out several\nfeature channels, our method can well simulate the occlusion of larger area.\nThe proposed LCD can encourage its succeeding layers to minimize the\nintra-class feature variance caused by occlusions, thus leading to improved\nrobustness against occlusion. In addition, we design an auxiliary spatial\nattention module by learning a channel-wise attention vector to reweight the\nfeature channels, which improves the contributions of non-occluded regions.\nExtensive experiments on various benchmarks show that the proposed method\noutperforms state-of-the-art methods with a remarkable improvement.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 05:53:14 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["He", "Mingjie", ""], ["Zhang", "Jie", ""], ["Shan", "Shiguang", ""], ["Liu", "Xiao", ""], ["Wu", "Zhongqin", ""], ["Chen", "Xilin", ""]]}, {"id": "2107.09282", "submitter": "Shan You", "authors": "Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang,\n  Xiaogang Wang, Chang Xu", "title": "ReSSL: Relational Self-Supervised Learning with Weak Augmentation", "comments": "fixed several typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised Learning (SSL) including the mainstream contrastive learning\nhas achieved great success in learning visual representations without data\nannotations. However, most of methods mainly focus on the instance level\ninformation (\\ie, the different augmented images of the same instance should\nhave the same feature or cluster into the same class), but there is a lack of\nattention on the relationships between different instances. In this paper, we\nintroduced a novel SSL paradigm, which we term as relational self-supervised\nlearning (ReSSL) framework that learns representations by modeling the\nrelationship between different instances. Specifically, our proposed method\nemploys sharpened distribution of pairwise similarities among different\ninstances as \\textit{relation} metric, which is thus utilized to match the\nfeature embeddings of different augmentations. Moreover, to boost the\nperformance, we argue that weak augmentations matter to represent a more\nreliable relation, and leverage momentum strategy for practical efficiency.\nExperimental results show that our proposed ReSSL significantly outperforms the\nprevious state-of-the-art algorithms in terms of both performance and training\nefficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 06:53:07 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 14:24:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Zheng", "Mingkai", ""], ["You", "Shan", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Wang", "Xiaogang", ""], ["Xu", "Chang", ""]]}, {"id": "2107.09287", "submitter": "Hu Wang", "authors": "Olivia Byrnes, Wendy La, Hu Wang, Congbo Ma, Minhui Xue, Qi Wu", "title": "Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking\n  and Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data hiding is the process of embedding information into a noise-tolerant\nsignal such as a piece of audio, video, or image. Digital watermarking is a\nform of data hiding where identifying data is robustly embedded so that it can\nresist tampering and be used to identify the original owners of the media.\nSteganography, another form of data hiding, embeds data for the purpose of\nsecure and secret communication. This survey summarises recent developments in\ndeep learning techniques for data hiding for the purposes of watermarking and\nsteganography, categorising them based on model architectures and noise\ninjection methods. The objective functions, evaluation metrics, and datasets\nused for training these data hiding models are comprehensively summarised.\nFinally, we propose and discuss possible future directions for research into\ndeep data hiding techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:03:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Byrnes", "Olivia", ""], ["La", "Wendy", ""], ["Wang", "Hu", ""], ["Ma", "Congbo", ""], ["Xue", "Minhui", ""], ["Wu", "Qi", ""]]}, {"id": "2107.09289", "submitter": "Kazuma Fujii", "authors": "Kazuma Fujii, Daiki Suehiro, Kazuya Nishimura, Ryoma Bise", "title": "Cell Detection from Imperfect Annotation by Pseudo Label Selection Using\n  P-classification", "comments": "10 pages, 3 figures, Accepted in MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell detection is an essential task in cell image analysis. Recent deep\nlearning-based detection methods have achieved very promising results. In\ngeneral, these methods require exhaustively annotating the cells in an entire\nimage. If some of the cells are not annotated (imperfect annotation), the\ndetection performance significantly degrades due to noisy labels. This often\noccurs in real collaborations with biologists and even in public data-sets. Our\nproposed method takes a pseudo labeling approach for cell detection from\nimperfect annotated data. A detection convolutional neural network (CNN)\ntrained using such missing labeled data often produces over-detection. We treat\npartially labeled cells as positive samples and the detected positions except\nfor the labeled cell as unlabeled samples. Then we select reliable pseudo\nlabels from unlabeled data using recent machine learning techniques;\npositive-and-unlabeled (PU) learning and P-classification. Experiments using\nmicroscopy images for five different conditions demonstrate the effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:08:05 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 04:28:59 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Fujii", "Kazuma", ""], ["Suehiro", "Daiki", ""], ["Nishimura", "Kazuya", ""], ["Bise", "Ryoma", ""]]}, {"id": "2107.09293", "submitter": "Suzhen Wang", "authors": "Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, Xin Yu", "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural\n  Head Motion", "comments": null, "journal-ref": "IJCAI 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an audio-driven talking-head method to generate photo-realistic\ntalking-head videos from a single reference image. In this work, we tackle two\nkey challenges: (i) producing natural head motions that match speech prosody,\nand (ii) maintaining the appearance of a speaker in a large head motion while\nstabilizing the non-face regions. We first design a head pose predictor by\nmodeling rigid 6D head movements with a motion-aware recurrent neural network\n(RNN). In this way, the predicted head poses act as the low-frequency holistic\nmovements of a talking head, thus allowing our latter network to focus on\ndetailed facial movement generation. To depict the entire image motions arising\nfrom audio, we exploit a keypoint based dense motion field representation.\nThen, we develop a motion field generator to produce the dense motion fields\nfrom input audio, head poses, and a reference image. As this keypoint based\nrepresentation models the motions of facial regions, head, and backgrounds\nintegrally, our method can better constrain the spatial and temporal\nconsistency of the generated videos. Finally, an image generation network is\nemployed to render photo-realistic talking-head videos from the estimated\nkeypoint based motion fields and the input reference image. Extensive\nexperiments demonstrate that our method produces videos with plausible head\nmotions, synchronized facial expressions, and stable backgrounds and\noutperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:22:42 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Suzhen", ""], ["Li", "Lincheng", ""], ["Ding", "Yu", ""], ["Fan", "Changjie", ""], ["Yu", "Xin", ""]]}, {"id": "2107.09305", "submitter": "Wenxian Shi", "authors": "Wenxian Shi, Yuxuan Song, Hao Zhou, Bohan Li, Lei Li", "title": "Follow Your Path: a Progressive Method for Knowledge Distillation", "comments": "Accepted by ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks often have a huge number of parameters, which posts\nchallenges in deployment in application scenarios with limited memory and\ncomputation capacity. Knowledge distillation is one approach to derive compact\nmodels from bigger ones. However, it has been observed that a converged heavy\nteacher model is strongly constrained for learning a compact student network\nand could make the optimization subject to poor local optima. In this paper, we\npropose ProKT, a new model-agnostic method by projecting the supervision\nsignals of a teacher model into the student's parameter space. Such projection\nis implemented by decomposing the training objective into local intermediate\ntargets with an approximate mirror descent technique. The proposed method could\nbe less sensitive with the quirks during optimization which could result in a\nbetter local optimum. Experiments on both image and text datasets show that our\nproposed ProKT consistently achieves superior performance compared to other\nexisting knowledge distillation methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:44:33 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Shi", "Wenxian", ""], ["Song", "Yuxuan", ""], ["Zhou", "Hao", ""], ["Li", "Bohan", ""], ["Li", "Lei", ""]]}, {"id": "2107.09313", "submitter": "Moonbin Yim", "authors": "Moonbin Yim, Yoonsik Kim, Han-Cheol Cho and Sungrae Park", "title": "SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text\n  Recognition Models", "comments": "Accepted at ICDAR 2021, 16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For successful scene text recognition (STR) models, synthetic text image\ngenerators have alleviated the lack of annotated text images from the real\nworld. Specifically, they generate multiple text images with diverse\nbackgrounds, font styles, and text shapes and enable STR models to learn visual\npatterns that might not be accessible from manually annotated data. In this\npaper, we introduce a new synthetic text image generator, SynthTIGER, by\nanalyzing techniques used for text image synthesis and integrating effective\nones under a single algorithm. Moreover, we propose two techniques that\nalleviate the long-tail problem in length and character distributions of\ntraining data. In our experiments, SynthTIGER achieves better STR performance\nthan the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).\nOur ablation study demonstrates the benefits of using sub-components of\nSynthTIGER and the guideline on generating synthetic text images for STR\nmodels. Our implementation is publicly available at\nhttps://github.com/clovaai/synthtiger.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:03:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yim", "Moonbin", ""], ["Kim", "Yoonsik", ""], ["Cho", "Han-Cheol", ""], ["Park", "Sungrae", ""]]}, {"id": "2107.09362", "submitter": "Hiroki Ito", "authors": "Hiroki Ito, MaungMaung AprilPyone, Hitoshi Kiya", "title": "Protecting Semantic Segmentation Models by Using Block-wise Image\n  Encryption with Secret Key from Unauthorized Access", "comments": "To appear in 2021 International Workshop on Smart Info-Media Systems\n  in Asia (SISA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since production-level trained deep neural networks (DNNs) are of a great\nbusiness value, protecting such DNN models against copyright infringement and\nunauthorized access is in a rising demand. However, conventional model\nprotection methods focused only the image classification task, and these\nprotection methods were never applied to semantic segmentation although it has\nan increasing number of applications. In this paper, we propose to protect\nsemantic segmentation models from unauthorized access by utilizing block-wise\ntransformation with a secret key for the first time. Protected models are\ntrained by using transformed images. Experiment results show that the proposed\nprotection method allows rightful users with the correct key to access the\nmodel to full capacity and deteriorate the performance for unauthorized users.\nHowever, protected models slightly drop the segmentation performance compared\nto non-protected models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:31:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ito", "Hiroki", ""], ["AprilPyone", "MaungMaung", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2107.09372", "submitter": "Duy Minh Ho Nguyen", "authors": "Duy M. H. Nguyen, Truong T. N. Mai, Ngoc T. T. Than, Alexander Prange,\n  Daniel Sonntag", "title": "Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using\n  Vessel Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the problem of domain adaptation for diabetic\nretinopathy (DR) grading. We learn invariant target-domain features by defining\na novel self-supervised task based on retinal vessel image reconstructions,\ninspired by medical domain knowledge. Then, a benchmark of current\nstate-of-the-art unsupervised domain adaptation methods on the DR problem is\nprovided. It can be shown that our approach outperforms existing domain\nadaption strategies. Furthermore, when utilizing entire training data in the\ntarget domain, we are able to compete with several state-of-the-art approaches\nin final classification accuracy just by applying standard network\narchitectures and using image-level labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:44:07 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Nguyen", "Duy M. H.", ""], ["Mai", "Truong T. N.", ""], ["Than", "Ngoc T. T.", ""], ["Prange", "Alexander", ""], ["Sonntag", "Daniel", ""]]}, {"id": "2107.09373", "submitter": "Manoranjan Mohanty", "authors": "Waheeb Yaqub, Manoranjan Mohanty, Basem Suleiman", "title": "Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online\n  Proctoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online proctoring has become a necessity in online teaching. Video-based\ncrowd-sourced online proctoring solutions are being used, where an exam-taking\nstudent's video is monitored by third parties, leading to privacy concerns. In\nthis paper, we propose a privacy-preserving online proctoring system. The\nproposed image-hashing-based system can detect the student's excessive face and\nbody movement (i.e., anomalies) that is resulted when the student tries to\ncheat in the exam. The detection can be done even if the student's face is\nblurred or masked in video frames. Experiment with an in-house dataset shows\nthe usability of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:45:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yaqub", "Waheeb", ""], ["Mohanty", "Manoranjan", ""], ["Suleiman", "Basem", ""]]}, {"id": "2107.09391", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad, Ivan Sosnovik, Arnold Smeulders", "title": "Built-in Elastic Transformations for Improved Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion and Gaussian perturbations improves, while even improving the\nperformance on clean images slightly without performing any data augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 10:16:38 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Sosnovik", "Ivan", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2107.09405", "submitter": "Yoni Schirris", "authors": "Yoni Schirris, Efstratios Gavves, Iris Nederlof, Hugo Mark Horlings,\n  Jonas Teuwen", "title": "DeepSMILE: Self-supervised heterogeneity-aware multiple instance\n  learning for DNA damage response defect classification directly from H&E\n  whole-slide images", "comments": "Main paper: 16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Deep learning-based weak label learning method for analysing\nwhole slide images (WSIs) of Hematoxylin and Eosin (H&E) stained tumorcells not\nrequiring pixel-level or tile-level annotations using Self-supervised\npre-training and heterogeneity-aware deep Multiple Instance LEarning\n(DeepSMILE). We apply DeepSMILE to the task of Homologous recombination\ndeficiency (HRD) and microsatellite instability (MSI) prediction. We utilize\ncontrastive self-supervised learning to pre-train a feature extractor on\nhistopathology tiles of cancer tissue. Additionally, we use variability-aware\ndeep multiple instance learning to learn the tile feature aggregation function\nwhile modeling tumor heterogeneity. Compared to state-of-the-art genomic label\nclassification methods, DeepSMILE improves classification performance for HRD\nfrom $70.43\\pm4.10\\%$ to $83.79\\pm1.25\\%$ AUC and MSI from $78.56\\pm6.24\\%$ to\n$90.32\\pm3.58\\%$ AUC in a multi-center breast and colorectal cancer dataset,\nrespectively. These improvements suggest we can improve genomic label\nclassification performance without collecting larger datasets. In the future,\nthis may reduce the need for expensive genome sequencing techniques, provide\npersonalized therapy recommendations based on widely available WSIs of cancer\ntissue, and improve patient care with quicker treatment decisions - also in\nmedical centers without access to genome sequencing resources.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:00:16 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 11:55:58 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Schirris", "Yoni", ""], ["Gavves", "Efstratios", ""], ["Nederlof", "Iris", ""], ["Horlings", "Hugo Mark", ""], ["Teuwen", "Jonas", ""]]}, {"id": "2107.09427", "submitter": "Wenlong Zhang", "authors": "Wenlong Zhang, Yihao Liu, Chao Dong, Yu Qiao", "title": "RankSRGAN: Super Resolution Generative Adversarial Networks with\n  Learning to Rank", "comments": "IEEE PAMI accepted. arXiv admin note: substantial text overlap with\n  arXiv:1908.06382", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GAN) have demonstrated the potential to\nrecover realistic details for single image super-resolution (SISR). To further\nimprove the visual quality of super-resolved results, PIRM2018-SR Challenge\nemployed perceptual metrics to assess the perceptual quality, such as PI, NIQE,\nand Ma. However, existing methods cannot directly optimize these\nindifferentiable perceptual metrics, which are shown to be highly correlated\nwith human ratings. To address the problem, we propose Super-Resolution\nGenerative Adversarial Networks with Ranker (RankSRGAN) to optimize generator\nin the direction of different perceptual metrics. Specifically, we first train\na Ranker which can learn the behaviour of perceptual metrics and then introduce\na novel rank-content loss to optimize the perceptual quality. The most\nappealing part is that the proposed method can combine the strengths of\ndifferent SR methods to generate better results. Furthermore, we extend our\nmethod to multiple Rankers to provide multi-dimension constraints for the\ngenerator. Extensive experiments show that RankSRGAN achieves visually pleasing\nresults and reaches state-of-the-art performance in perceptual metrics and\nquality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:42:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhang", "Wenlong", ""], ["Liu", "Yihao", ""], ["Dong", "Chao", ""], ["Qiao", "Yu", ""]]}, {"id": "2107.09442", "submitter": "Gerda Bortsova", "authors": "Gerda Bortsova, Daniel Bos, Florian Dubost, Meike W. Vernooij, M.\n  Kamran Ikram, Gijs van Tulder, Marleen de Bruijne", "title": "Automated Segmentation and Volume Measurement of Intracranial Carotid\n  Artery Calcification on Non-Contrast CT", "comments": "Accepted for publication in Radiology: Artificial Intelligence\n  (https://pubs.rsna.org/journal/ai), which is published by the Radiological\n  Society of North America (RSNA)", "journal-ref": null, "doi": "10.1148/ryai.2021200226", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To evaluate a fully-automated deep-learning-based method for\nassessment of intracranial carotid artery calcification (ICAC). Methods: Two\nobservers manually delineated ICAC in non-contrast CT scans of 2,319\nparticipants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study,\nprospectively collected between 2003 and 2006. These data were used to\nretrospectively develop and validate a deep-learning-based method for automated\nICAC delineation and volume measurement. To evaluate the method, we compared\nmanual and automatic assessment (computed using ten-fold cross-validation) with\nrespect to 1) the agreement with an independent observer's assessment\n(available in a random subset of 47 scans); 2) the accuracy in delineating ICAC\nas judged via blinded visual comparison by an expert; 3) the association with\nfirst stroke incidence from the scan date until 2012. All method performance\nmetrics were computed using 10-fold cross-validation. Results: The automated\ndelineation of ICAC reached sensitivity of 83.8% and positive predictive value\n(PPV) of 88%. The intraclass correlation between automatic and manual ICAC\nvolume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset).\nMeasured between the assessments of independent observers, sensitivity was\n73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95;\ncomputed in the 47-scan subset). In the blinded visual comparisons, automatic\ndelineations were more accurate than manual ones (p-value = 0.01). The\nassociation of ICAC volume with incident stroke was similarly strong for both\nautomated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured\nvolumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed\nmodel was capable of automated segmentation and volume quantification of ICAC\nwith accuracy comparable to human experts.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 12:21:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bortsova", "Gerda", ""], ["Bos", "Daniel", ""], ["Dubost", "Florian", ""], ["Vernooij", "Meike W.", ""], ["Ikram", "M. Kamran", ""], ["van Tulder", "Gijs", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2107.09504", "submitter": "Olga Zatsarynna", "authors": "Olga Zatsarynna, Yazan Abu Farha and Juergen Gall", "title": "Multi-Modal Temporal Convolutional Network for Anticipating Actions in\n  Egocentric Videos", "comments": "CVPR Precognition Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating human actions is an important task that needs to be addressed\nfor the development of reliable intelligent agents, such as self-driving cars\nor robot assistants. While the ability to make future predictions with high\naccuracy is crucial for designing the anticipation approaches, the speed at\nwhich the inference is performed is not less important. Methods that are\naccurate but not sufficiently fast would introduce a high latency into the\ndecision process. Thus, this will increase the reaction time of the underlying\nsystem. This poses a problem for domains such as autonomous driving, where the\nreaction time is crucial. In this work, we propose a simple and effective\nmulti-modal architecture based on temporal convolutions. Our approach stacks a\nhierarchy of temporal convolutional layers and does not rely on recurrent\nlayers to ensure a fast prediction. We further introduce a multi-modal fusion\nmechanism that captures the pairwise interactions between RGB, flow, and object\nmodalities. Results on two large-scale datasets of egocentric videos,\nEPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves\ncomparable performance to the state-of-the-art approaches while being\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 16:21:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zatsarynna", "Olga", ""], ["Farha", "Yazan Abu", ""], ["Gall", "Juergen", ""]]}, {"id": "2107.09540", "submitter": "Andrew Melnik", "authors": "Andrew Melnik, Augustin Harter, Christian Limberg, Krishan Rana, Niko\n  Suenderhauf, Helge Ritter", "title": "Critic Guided Segmentation of Rewarding Objects in First-Person Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work discusses a learning approach to mask rewarding objects in images\nusing sparse reward signals from an imitation learning dataset. For that, we\ntrain an Hourglass network using only feedback from a critic model. The\nHourglass network learns to produce a mask to decrease the critic's score of a\nhigh score image and increase the critic's score of a low score image by\nswapping the masked areas between these two images. We trained the model on an\nimitation learning dataset from the NeurIPS 2020 MineRL Competition Track,\nwhere our model learned to mask rewarding objects in a complex interactive 3D\nenvironment with a sparse reward signal. This approach was part of the 1st\nplace winning solution in this competition. Video demonstration and code:\nhttps://rebrand.ly/critic-guided-segmentation\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 14:54:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Melnik", "Andrew", ""], ["Harter", "Augustin", ""], ["Limberg", "Christian", ""], ["Rana", "Krishan", ""], ["Suenderhauf", "Niko", ""], ["Ritter", "Helge", ""]]}, {"id": "2107.09543", "submitter": "Richard Osuala", "authors": "Richard Osuala, Kaisar Kushibar, Lidia Garrucho, Akis Linardos,\n  Zuzanna Szafranowska, Stefan Klein, Ben Glocker, Oliver Diaz, Karim Lekadir", "title": "A Review of Generative Adversarial Networks in Cancer Imaging: New\n  Applications, New Solutions", "comments": "64 pages, v1, preprint submitted to Elsevier, Oliver Diaz and Karim\n  Lekadir contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite technological and medical advances, the detection, interpretation,\nand treatment of cancer based on imaging data continue to pose significant\nchallenges. These include high inter-observer variability, difficulty of\nsmall-sized lesion detection, nodule interpretation and malignancy\ndetermination, inter- and intra-tumour heterogeneity, class imbalance,\nsegmentation inaccuracies, and treatment effect uncertainty. The recent\nadvancements in Generative Adversarial Networks (GANs) in computer vision as\nwell as in medical imaging may provide a basis for enhanced capabilities in\ncancer detection and analysis. In this review, we assess the potential of GANs\nto address a number of key challenges of cancer imaging, including data\nscarcity and imbalance, domain and dataset shifts, data access and privacy,\ndata annotation and quantification, as well as cancer detection, tumour\nprofiling and treatment planning. We provide a critical appraisal of the\nexisting literature of GANs applied to cancer imagery, together with\nsuggestions on future research directions to address these challenges. We\nanalyse and discuss 163 papers that apply adversarial training techniques in\nthe context of cancer imaging and elaborate their methodologies, advantages and\nlimitations. With this work, we strive to bridge the gap between the needs of\nthe clinical cancer imaging community and the current and prospective research\non GANs in the artificial intelligence community.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 14:57:51 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Osuala", "Richard", ""], ["Kushibar", "Kaisar", ""], ["Garrucho", "Lidia", ""], ["Linardos", "Akis", ""], ["Szafranowska", "Zuzanna", ""], ["Klein", "Stefan", ""], ["Glocker", "Ben", ""], ["Diaz", "Oliver", ""], ["Lekadir", "Karim", ""]]}, {"id": "2107.09559", "submitter": "Benjamin Billot", "authors": "Benjamin Billot, Douglas N. Greve, Oula Puonti, Axel Thielscher, Koen\n  Van Leemput, Bruce Fischl, Adrian V. Dalca, Juan Eugenio Iglesias", "title": "SynthSeg: Domain Randomisation for Segmentation of Brain MRI Scans of\n  any Contrast and Resolution", "comments": "20 pages, 11 figures, currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in data augmentation and transfer learning, convolutional\nneural networks (CNNs) have difficulties generalising to unseen target domains.\nWhen applied to segmentation of brain MRI scans, CNNs are highly sensitive to\nchanges in resolution and contrast: even within the same MR modality, decreases\nin performance can be observed across datasets. We introduce SynthSeg, the\nfirst segmentation CNN agnostic to brain MRI scans of any contrast and\nresolution. SynthSeg is trained with synthetic data sampled from a generative\nmodel inspired by Bayesian segmentation. Crucially, we adopt a \\textit{domain\nrandomisation} strategy where we fully randomise the generation parameters to\nmaximise the variability of the training data. Consequently, SynthSeg can\nsegment preprocessed and unpreprocessed real scans of any target domain,\nwithout retraining or fine-tuning. Because SynthSeg only requires segmentations\nto be trained (no images), it can learn from label maps obtained automatically\nfrom existing datasets of different populations (e.g., with atrophy and\nlesions), thus achieving robustness to a wide range of morphological\nvariability. We demonstrate SynthSeg on 5,500 scans of 6 modalities and 10\nresolutions, where it exhibits unparalleled generalisation compared to\nsupervised CNNs, test time adaptation, and Bayesian segmentation. The code and\ntrained model are available at https://github.com/BBillot/SynthSeg.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:22:16 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Billot", "Benjamin", ""], ["Greve", "Douglas N.", ""], ["Puonti", "Oula", ""], ["Thielscher", "Axel", ""], ["Van Leemput", "Koen", ""], ["Fischl", "Bruce", ""], ["Dalca", "Adrian V.", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "2107.09562", "submitter": "Timo Milbich", "authors": "Timo Milbich, Karsten Roth, Samarth Sinha, Ludwig Schmidt, Marzyeh\n  Ghassemi, Bj\\\"orn Ommer", "title": "Characterizing Generalization under Out-Of-Distribution Shifts in Deep\n  Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Metric Learning (DML) aims to find representations suitable for\nzero-shot transfer to a priori unknown test distributions. However, common\nevaluation protocols only test a single, fixed data split in which train and\ntest classes are assigned randomly. More realistic evaluations should consider\na broad spectrum of distribution shifts with potentially varying degree and\ndifficulty. In this work, we systematically construct train-test splits of\nincreasing difficulty and present the ooDML benchmark to characterize\ngeneralization under out-of-distribution shifts in DML. ooDML is designed to\nprobe the generalization performance on much more challenging, diverse\ntrain-to-test distribution shifts. Based on our new benchmark, we conduct a\nthorough empirical analysis of state-of-the-art DML methods. We find that while\ngeneralization tends to consistently degrade with difficulty, some methods are\nbetter at retaining performance as the distribution shift increases. Finally,\nwe propose few-shot DML as an efficient way to consistently improve\ngeneralization in response to unknown test shifts presented in ooDML. Code\navailable here:\nhttps://github.com/Confusezius/Characterizing_Generalization_in_DeepMetricLearning.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:26:09 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Milbich", "Timo", ""], ["Roth", "Karsten", ""], ["Sinha", "Samarth", ""], ["Schmidt", "Ludwig", ""], ["Ghassemi", "Marzyeh", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2107.09584", "submitter": "Edward Smith", "authors": "Edward J. Smith and David Meger and Luis Pineda and Roberto Calandra\n  and Jitendra Malik and Adriana Romero and Michal Drozdzal", "title": "Active 3D Shape Reconstruction from Vision and Touch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans build 3D understandings of the world through active object\nexploration, using jointly their senses of vision and touch. However, in 3D\nshape reconstruction, most recent progress has relied on static datasets of\nlimited sensory data such as RGB images, depth maps or haptic readings, leaving\nthe active exploration of the shape largely unexplored. In active touch sensing\nfor 3D reconstruction, the goal is to actively select the tactile readings that\nmaximize the improvement in shape reconstruction accuracy. However, the\ndevelopment of deep learning-based active touch models is largely limited by\nthe lack of frameworks for shape exploration. In this paper, we focus on this\nproblem and introduce a system composed of: 1) a haptic simulator leveraging\nhigh spatial resolution vision-based tactile sensors for active touching of 3D\nobjects; 2) a mesh-based 3D shape reconstruction model that relies on tactile\nor visuotactile signals; and 3) a set of data-driven solutions with either\ntactile or visuotactile priors to guide the shape exploration. Our framework\nenables the development of the first fully data-driven solutions to active\ntouch on top of learned models for object understanding. Our experiments show\nthe benefits of such solutions in the task of 3D shape understanding where our\nmodels consistently outperform natural baselines. We provide our framework as a\ntool to foster future research in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:56:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Smith", "Edward J.", ""], ["Meger", "David", ""], ["Pineda", "Luis", ""], ["Calandra", "Roberto", ""], ["Malik", "Jitendra", ""], ["Romero", "Adriana", ""], ["Drozdzal", "Michal", ""]]}, {"id": "2107.09600", "submitter": "Li Gao", "authors": "Li Gao, Jing Zhang, Lefei Zhang, Dacheng Tao", "title": "DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": "10.1145/3474085.3475186", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt\na segmentation model trained on the labeled source domain to the unlabeled\ntarget domain. Existing methods try to learn domain invariant features while\nsuffering from large domain gaps that make it difficult to correctly align\ndiscrepant features, especially in the initial training phase. To address this\nissue, we propose a novel Dual Soft-Paste (DSP) method in this paper.\nSpecifically, DSP selects some classes from a source domain image using a\nlong-tail class first sampling strategy and softly pastes the corresponding\nimage patch on both the source and target training images with a fusion weight.\nTechnically, we adopt the mean teacher framework for domain adaptation, where\nthe pasted source and target images go through the student network while the\noriginal target image goes through the teacher network. Output-level alignment\nis carried out by aligning the probability maps of the target fused image from\nboth networks using a weighted cross-entropy loss. In addition, feature-level\nalignment is carried out by aligning the feature maps of the source and target\nimages from student network using a weighted maximum mean discrepancy loss. DSP\nfacilitates the model learning domain-invariant features from the intermediate\ndomains, leading to faster convergence and better performance. Experiments on\ntwo challenging benchmarks demonstrate the superiority of DSP over\nstate-of-the-art methods. Code is available at\n\\url{https://github.com/GaoLii/DSP}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:22:40 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 15:20:09 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 15:41:13 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gao", "Li", ""], ["Zhang", "Jing", ""], ["Zhang", "Lefei", ""], ["Tao", "Dacheng", ""]]}, {"id": "2107.09602", "submitter": "Prajoy Podder", "authors": "Subrato Bharati, Prajoy Podder, M. Rubaiyat Hossain Mondal, V.B. Surya\n  Prasath", "title": "Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A\n  Comprehensive Review", "comments": "22 pages, 11 Figures", "journal-ref": "International Journal of Computer Information Systems and\n  Industrial Management Applications(ISSN 2150-7988), Volume 13, 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of\nlives and has affected all aspects of human life. This paper focuses on the\napplication of deep learning (DL) models to medical imaging and drug discovery\nfor managing COVID-19 disease. In this article, we detail various medical\nimaging-based studies such as X-rays and computed tomography (CT) images along\nwith DL methods for classifying COVID-19 affected versus pneumonia. The\napplications of DL techniques to medical images are further described in terms\nof image localization, segmentation, registration, and classification leading\nto COVID-19 detection. The reviews of recent papers indicate that the highest\nclassification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is\napplied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients\nand 365 normal people. Furthermore, it can be seen that the best classification\naccuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT\nimage dataset of 7500 samples where COVID-19 patients, lung tumor patients and\nnormal people are equal in number. Moreover, we illustrate the potential DL\ntechniques in drug or vaccine discovery in combating the coronavirus. Finally,\nwe address a number of problems, concerns and future research directions\nrelevant to DL applications for COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:49:49 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bharati", "Subrato", ""], ["Podder", "Prajoy", ""], ["Mondal", "M. Rubaiyat Hossain", ""], ["Prasath", "V. B. Surya", ""]]}, {"id": "2107.09609", "submitter": "Jie Lei", "authors": "Jie Lei, Tamara L. Berg, Mohit Bansal", "title": "QVHighlights: Detecting Moments and Highlights in Videos via Natural\n  Language Queries", "comments": "17 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting customized moments and highlights from videos given natural\nlanguage (NL) user queries is an important but under-studied topic. One of the\nchallenges in pursuing this direction is the lack of annotated data. To address\nthis issue, we present the Query-based Video Highlights (QVHighlights) dataset.\nIt consists of over 10,000 YouTube videos, covering a wide range of topics,\nfrom everyday activities and travel in lifestyle vlog videos to social and\npolitical activities in news videos. Each video in the dataset is annotated\nwith: (1) a human-written free-form NL query, (2) relevant moments in the video\nw.r.t. the query, and (3) five-point scale saliency scores for all\nquery-relevant clips. This comprehensive annotation enables us to develop and\nevaluate systems that detect relevant moments as well as salient highlights for\ndiverse, flexible user queries. We also present a strong baseline for this\ntask, Moment-DETR, a transformer encoder-decoder model that views moment\nretrieval as a direct set prediction problem, taking extracted video and query\nrepresentations as inputs and predicting moment coordinates and saliency scores\nend-to-end. While our model does not utilize any human prior, we show that it\nperforms competitively when compared to well-engineered architectures. With\nweakly supervised pretraining using ASR captions, Moment-DETR substantially\noutperforms previous methods. Lastly, we present several ablations and\nvisualizations of Moment-DETR. Data and code is publicly available at\nhttps://github.com/jayleicn/moment_detr\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:42:58 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lei", "Jie", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2107.09628", "submitter": "Carola Figueroa Flores", "authors": "Carola Figueroa-Flores, David Berga, Joost van der Weijer and Bogdan\n  Raducanu", "title": "Saliency for free: Saliency prediction as a side-effect of object\n  recognition", "comments": "Paper published to Pattern Recognition Letter", "journal-ref": "2021", "doi": "10.1016/j.patrec.2021.05.015", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Saliency is the perceptual capacity of our visual system to focus our\nattention (i.e. gaze) on relevant objects. Neural networks for saliency\nestimation require ground truth saliency maps for training which are usually\nachieved via eyetracking experiments. In the current paper, we demonstrate that\nsaliency maps can be generated as a side-effect of training an object\nrecognition deep neural network that is endowed with a saliency branch. Such a\nnetwork does not require any ground-truth saliency maps for training.Extensive\nexperiments carried out on both real and synthetic saliency datasets\ndemonstrate that our approach is able to generate accurate saliency maps,\nachieving competitive results on both synthetic and real datasets when compared\nto methods that do require ground truth data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:17:28 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Figueroa-Flores", "Carola", ""], ["Berga", "David", ""], ["van der Weijer", "Joost", ""], ["Raducanu", "Bogdan", ""]]}, {"id": "2107.09652", "submitter": "Helena Montenegro", "authors": "H. Montenegro, W. Silva, J. S. Cardoso", "title": "Towards Privacy-preserving Explanations in Medical Image Analysis", "comments": "7 pages, 5 figures, accepted at Workshop on Interpretable ML in\n  Healthcare at ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Deep Learning in the medical field is hindered by the lack of\ninterpretability. Case-based interpretability strategies can provide intuitive\nexplanations for deep learning models' decisions, thus, enhancing trust.\nHowever, the resulting explanations threaten patient privacy, motivating the\ndevelopment of privacy-preserving methods compatible with the specifics of\nmedical data. In this work, we analyze existing privacy-preserving methods and\ntheir respective capacity to anonymize medical data while preserving\ndisease-related semantic features. We find that the PPRL-VGAN deep learning\nmethod was the best at preserving the disease-related semantic features while\nguaranteeing a high level of privacy among the compared state-of-the-art\nmethods. Nevertheless, we emphasize the need to improve privacy-preserving\nmethods for medical imaging, as we identified relevant drawbacks in all\nexisting privacy-preserving approaches.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:35:36 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Montenegro", "H.", ""], ["Silva", "W.", ""], ["Cardoso", "J. S.", ""]]}, {"id": "2107.09700", "submitter": "Sungmin Hong", "authors": "Sungmin Hong, Razvan Marinescu, Adrian V. Dalca, Anna K. Bonkhoff,\n  Martin Bretzner, Natalia S. Rost, Polina Golland", "title": "3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative\n  Modeling of Three-Dimensional Medical Images", "comments": "11 pages, 6 figures, 2 tables. Provisionally Accepted at DGM4MICCAI\n  workshop in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image synthesis via Generative Adversarial Networks (GANs) of\nthree-dimensional (3D) medical images has great potential that can be extended\nto many medical applications, such as, image enhancement and disease\nprogression modeling. However, current GAN technologies for 3D medical image\nsynthesis need to be significantly improved to be readily adapted to real-world\nmedical problems. In this paper, we extend the state-of-the-art StyleGAN2\nmodel, which natively works with two-dimensional images, to enable 3D image\nsynthesis. In addition to the image synthesis, we investigate the\ncontrollability and interpretability of the 3D-StyleGAN via style vectors\ninherited form the original StyleGAN2 that are highly suitable for medical\napplications: (i) the latent space projection and reconstruction of unseen real\nimages, and (ii) style mixing. We demonstrate the 3D-StyleGAN's performance and\nfeasibility with ~12,000 three-dimensional full brain MR T1 images, although it\ncan be applied to any 3D volumetric images. Furthermore, we explore different\nconfigurations of hyperparameters to investigate potential improvement of the\nimage synthesis with larger networks. The codes and pre-trained networks are\navailable online: https://github.com/sh4174/3DStyleGAN.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:08:27 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hong", "Sungmin", ""], ["Marinescu", "Razvan", ""], ["Dalca", "Adrian V.", ""], ["Bonkhoff", "Anna K.", ""], ["Bretzner", "Martin", ""], ["Rost", "Natalia S.", ""], ["Golland", "Polina", ""]]}, {"id": "2107.09725", "submitter": "Hung La", "authors": "Ashutosh Singandhupe, Hung La, Trung Dung Ngo, Van Ho", "title": "Registration of 3D Point Sets Using Correntropy Similarity Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on Registration or Alignment of 3D point sets. Although the\nRegistration problem is a well established problem and it's solved using\nmultiple variants of Iterative Closest Point (ICP) Algorithm, most of the\napproaches in the current state of the art still suffers from misalignment when\nthe \\textit{Source} and the \\textit{Target} point sets are separated by large\nrotations and translation. In this work, we propose a variant of the Standard\nICP algorithm, where we introduce a Correntropy Relationship Matrix in the\ncomputation of rotation and translation component which attempts to solve the\nlarge rotation and translation problem between \\textit{Source} and\n\\textit{Target} point sets. This matrix is created through correntropy\ncriterion which is updated in every iteration. The correntropy criterion\ndefined in this approach maintains the relationship between the points in the\n\\textit{Source} dataset and the \\textit{Target} dataset. Through our\nexperiments and validation we verify that our approach has performed well under\nvarious rotation and translation in comparison to the other well-known state of\nthe art methods available in the Point Cloud Library (PCL) as well as other\nmethods available as open source. We have uploaded our code in the github\nrepository for the readers to validate and verify our approach\nhttps://github.com/aralab-unr/CoSM-ICP.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:56:22 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Singandhupe", "Ashutosh", ""], ["La", "Hung", ""], ["Ngo", "Trung Dung", ""], ["Ho", "Van", ""]]}, {"id": "2107.09783", "submitter": "Mrigank Rochan", "authors": "Mrigank Rochan, Shubhra Aich, Eduardo R. Corral-Soto, Amir Nabatchian,\n  Bingbing Liu", "title": "Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with\n  Self-Supervision and Gated Adapters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on a less explored, but more realistic and complex\nproblem of domain adaptation in LiDAR semantic segmentation. There is a\nsignificant drop in performance of an existing segmentation model when training\n(source domain) and testing (target domain) data originate from different LiDAR\nsensors. To overcome this shortcoming, we propose an unsupervised domain\nadaptation framework that leverages unlabeled target domain data for\nself-supervision, coupled with an unpaired mask transfer strategy to mitigate\nthe impact of domain shifts. Furthermore, we introduce gated adapter modules\nwith a small number of parameters into the network to account for target\ndomain-specific information. Experiments adapting from both real-to-real and\nsynthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the\nsignificant improvement over prior arts.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 21:57:18 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Rochan", "Mrigank", ""], ["Aich", "Shubhra", ""], ["Corral-Soto", "Eduardo R.", ""], ["Nabatchian", "Amir", ""], ["Liu", "Bingbing", ""]]}, {"id": "2107.09842", "submitter": "Yao Zhang", "authors": "Yao Zhang, Jiawei Yang, Jiang Tian, Zhongchao Shi, Cheng Zhong, Yang\n  Zhang, and Zhiqiang He", "title": "Modality-aware Mutual Learning for Multi-modal Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Liver cancer is one of the most common cancers worldwide. Due to\ninconspicuous texture changes of liver tumor, contrast-enhanced computed\ntomography (CT) imaging is effective for the diagnosis of liver cancer. In this\npaper, we focus on improving automated liver tumor segmentation by integrating\nmulti-modal CT images. To this end, we propose a novel mutual learning (ML)\nstrategy for effective and robust multi-modal liver tumor segmentation.\nDifferent from existing multi-modal methods that fuse information from\ndifferent modalities by a single model, with ML, an ensemble of\nmodality-specific models learn collaboratively and teach each other to distill\nboth the characteristics and the commonality between high-level representations\nof different modalities. The proposed ML not only enables the superiority for\nmulti-modal learning but can also handle missing modalities by transferring\nknowledge from existing modalities to missing ones. Additionally, we present a\nmodality-aware (MA) module, where the modality-specific models are\ninterconnected and calibrated with attention weights for adaptive information\nexchange. The proposed modality-aware mutual learning (MAML) method achieves\npromising results for liver tumor segmentation on a large-scale clinical\ndataset. Moreover, we show the efficacy and robustness of MAML for handling\nmissing modalities on both the liver tumor and public brain tumor (BRATS 2018)\ndatasets. Our code is available at https://github.com/YaoZhang93/MAML.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:24:31 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhang", "Yao", ""], ["Yang", "Jiawei", ""], ["Tian", "Jiang", ""], ["Shi", "Zhongchao", ""], ["Zhong", "Cheng", ""], ["Zhang", "Yang", ""], ["He", "Zhiqiang", ""]]}, {"id": "2107.09843", "submitter": "Yao Zhang", "authors": "Jiawei Yang, Yao Zhang, Yuan Liang, Yang Zhang, Lei He, and Zhiqiang\n  He", "title": "TumorCP: A Simple but Effective Object-Level Data Augmentation for Tumor\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning models are notoriously data-hungry. Thus, there is an urging\nneed for data-efficient techniques in medical image analysis, where\nwell-annotated data are costly and time consuming to collect. Motivated by the\nrecently revived \"Copy-Paste\" augmentation, we propose TumorCP, a simple but\neffective object-level data augmentation method tailored for tumor\nsegmentation. TumorCP is online and stochastic, providing unlimited\naugmentation possibilities for tumors' subjects, locations, appearances, as\nwell as morphologies. Experiments on kidney tumor segmentation task demonstrate\nthat TumorCP surpasses the strong baseline by a remarkable margin of 7.12% on\ntumor Dice. Moreover, together with image-level data augmentation, it beats the\ncurrent state-of-the-art by 2.32% on tumor Dice. Comprehensive ablation studies\nare performed to validate the effectiveness of TumorCP. Meanwhile, we show that\nTumorCP can lead to striking improvements in extremely low-data regimes.\nEvaluated with only 10% labeled data, TumorCP significantly boosts tumor Dice\nby 21.87%. To the best of our knowledge, this is the very first work exploring\nand extending the \"Copy-Paste\" design in medical imaging domain. Code is\navailable at: https://github.com/YaoZhang93/TumorCP.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:26:50 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Yang", "Jiawei", ""], ["Zhang", "Yao", ""], ["Liang", "Yuan", ""], ["Zhang", "Yang", ""], ["He", "Lei", ""], ["He", "Zhiqiang", ""]]}, {"id": "2107.09847", "submitter": "Minjung Shin", "authors": "Minjung Shin (1), Jeonghoon Kim (1 and 2), Seongho Choi (3), Yu-Jung\n  Heo (3), Donghyun Kim (1 and 4), Minsu Lee (3 and 5), Byoung-Tak Zhang (3 and\n  5) and Jeh-Kwang Ryu (1 and 4) ((1) Laboratory for Natural and Artificial\n  Kin\\\"asthese, Convergence Research Center for Artificial Intelligence\n  (CRC4AI), Dongguk University, Seoul, South Korea, (2) Department of\n  Artificial Intelligence, Dongguk University, Seoul, South Korea, (3)\n  Biointelligence Laboratory, Department of Computer Science and Engineering,\n  Seoul National University, Seoul, South Korea, (4) Department of Physical\n  Education, College of Education, Dongguk University, Seoul, South Korea, (5)\n  AI Institute of Seoul National University (AIIS), Seoul, South Korea)", "title": "CogME: A Novel Evaluation Metric for Video Understanding Intelligence", "comments": "17 pages with 3 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing video understanding intelligence is quite challenging because it\nrequires holistic integration of images, scripts, and sounds based on natural\nlanguage processing, temporal dependency, and reasoning. Recently, substantial\nattempts have been made on several video datasets with associated question\nanswering (QA) on a large scale. However, existing evaluation metrics for video\nquestion answering (VideoQA) do not provide meaningful analysis. To make\nprogress, we argue that a well-made framework, established on the way humans\nunderstand, is required to explain and evaluate the performance of\nunderstanding in detail. Then we propose a top-down evaluation system for\nVideoQA, based on the cognitive process of humans and story elements: Cognitive\nModules for Evaluation (CogME). CogME is composed of three cognitive modules:\ntargets, contents, and thinking. The interaction among the modules in the\nunderstanding procedure can be expressed in one sentence as follows: \"I\nunderstand the CONTENT of the TARGET through a way of THINKING.\" Each module\nhas sub-components derived from the story elements. We can specify the required\naspects of understanding by annotating the sub-components to individual\nquestions. CogME thus provides a framework for an elaborated specification of\nVideoQA datasets. To examine the suitability of a VideoQA dataset for\nvalidating video understanding intelligence, we evaluated the baseline model of\nthe DramaQA dataset by applying CogME. The evaluation reveals that story\nelements are unevenly reflected in the existing dataset, and the model based on\nthe dataset may cause biased predictions. Although this study has only been\nable to grasp a narrow range of stories, we expect that it offers the first\nstep in considering the cognitive process of humans on the video understanding\nintelligence of humans and AI.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:33:37 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Shin", "Minjung", "", "1 and 2"], ["Kim", "Jeonghoon", "", "1 and 2"], ["Choi", "Seongho", "", "1 and 4"], ["Heo", "Yu-Jung", "", "1 and 4"], ["Kim", "Donghyun", "", "1 and 4"], ["Lee", "Minsu", "", "3 and 5"], ["Zhang", "Byoung-Tak", "", "3 and\n  5"], ["Ryu", "Jeh-Kwang", "", "1 and 4"]]}, {"id": "2107.09858", "submitter": "Yeong-Jun Cho", "authors": "Yeong-Jun Cho", "title": "Weighted Intersection over Union (wIoU): A New Evaluation Metric for\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel evaluation metric for performance\nevaluation of semantic segmentation. In recent years, many studies have tried\nto train pixel-level classifiers on large-scale image datasets to perform\naccurate semantic segmentation. The goal of semantic segmentation is to assign\na class label of each pixel in the scene. It has various potential applications\nin computer vision fields e.g., object detection, classification, scene\nunderstanding and Etc. To validate the proposed wIoU evaluation metric, we\ntested state-of-the art methods on public benchmark datasets (e.g., KITTI)\nbased on the proposed wIoU metric and compared with other conventional\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:59:59 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Cho", "Yeong-Jun", ""]]}, {"id": "2107.09887", "submitter": "Dominik Lewy", "authors": "Dominik Lewy and Jacek Ma\\'ndziuk", "title": "An overview of mixing augmentation methods and augmentation strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks have made an incredible progress in many\nComputer Vision tasks. This progress, however, often relies on the availability\nof large amounts of the training data, required to prevent over-fitting, which\nin many domains entails significant cost of manual data labeling. An\nalternative approach is application of data augmentation (DA) techniques that\naim at model regularization by creating additional observations from the\navailable ones. This survey focuses on two DA research streams: image mixing\nand automated selection of augmentation strategies. First, the presented\nmethods are briefly described, and then qualitatively compared with respect to\ntheir key characteristics. Various quantitative comparisons are also included\nbased on the results reported in recent DA literature. This review mainly\ncovers the methods published in the materials of top-tier conferences and in\nleading journals in the years 2017-2021.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 05:58:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lewy", "Dominik", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2107.09892", "submitter": "Uddeshya Upadhyay", "authors": "Viswanath P. Sudarshan, Uddeshya Upadhyay, Gary F. Egan, Zhaolin Chen,\n  Suyash P. Awate", "title": "Towards Lower-Dose PET using Physics-Based Uncertainty-Aware Multimodal\n  Learning with Robustness to Out-of-Distribution Data", "comments": "Accepted at Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiation exposure in positron emission tomography (PET) imaging limits its\nusage in the studies of radiation-sensitive populations, e.g., pregnant women,\nchildren, and adults that require longitudinal imaging. Reducing the PET\nradiotracer dose or acquisition time reduces photon counts, which can\ndeteriorate image quality. Recent deep-neural-network (DNN) based methods for\nimage-to-image translation enable the mapping of low-quality PET images\n(acquired using substantially reduced dose), coupled with the associated\nmagnetic resonance imaging (MRI) images, to high-quality PET images. However,\nsuch DNN methods focus on applications involving test data that match the\nstatistical characteristics of the training data very closely and give little\nattention to evaluating the performance of these DNNs on new\nout-of-distribution (OOD) acquisitions. We propose a novel DNN formulation that\nmodels the (i) underlying sinogram-based physics of the PET imaging system and\n(ii) the uncertainty in the DNN output through the per-voxel heteroscedasticity\nof the residuals between the predicted and the high-quality reference images.\nOur sinogram-based uncertainty-aware DNN framework, namely, suDNN, estimates a\nstandard-dose PET image using multimodal input in the form of (i) a\nlow-dose/low-count PET image and (ii) the corresponding multi-contrast MRI\nimages, leading to improved robustness of suDNN to OOD acquisitions. Results on\nin vivo simultaneous PET-MRI, and various forms of OOD data in PET-MRI, show\nthe benefits of suDNN over the current state of the art, quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 06:18:10 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Sudarshan", "Viswanath P.", ""], ["Upadhyay", "Uddeshya", ""], ["Egan", "Gary F.", ""], ["Chen", "Zhaolin", ""], ["Awate", "Suyash P.", ""]]}, {"id": "2107.09899", "submitter": "Runnan Chen Mr.", "authors": "Runnan Chen, Yuexin Ma, Nenglun Chen, Lingjie Liu, Zhiming Cui,\n  Yanhong Lin, Wenping Wang", "title": "Structure-Aware Long Short-Term Memory Network for 3D Cephalometric\n  Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting 3D landmarks on cone-beam computed tomography (CBCT) is crucial to\nassessing and quantifying the anatomical abnormalities in 3D cephalometric\nanalysis. However, the current methods are time-consuming and suffer from large\nbiases in landmark localization, leading to unreliable diagnosis results. In\nthis work, we propose a novel Structure-Aware Long Short-Term Memory framework\n(SA-LSTM) for efficient and accurate 3D landmark detection. To reduce the\ncomputational burden, SA-LSTM is designed in two stages. It first locates the\ncoarse landmarks via heatmap regression on a down-sampled CBCT volume and then\nprogressively refines landmarks by attentive offset regression using\nhigh-resolution cropped patches. To boost accuracy, SA-LSTM captures\nglobal-local dependence among the cropping patches via self-attention.\nSpecifically, a graph attention module implicitly encodes the landmark's global\nstructure to rationalize the predicted position. Furthermore, a novel\nattention-gated module recursively filters irrelevant local features and\nmaintains high-confident local predictions for aggregating the final result.\nExperiments show that our method significantly outperforms state-of-the-art\nmethods in terms of efficiency and accuracy on an in-house dataset and a public\ndataset, achieving 1.64 mm and 2.37 mm average errors, respectively, and using\nonly 0.5 seconds for inferring the whole CBCT volume of resolution 768*768*576.\nMoreover, all predicted landmarks are within 8 mm error, which is vital for\nacceptable cephalometric analysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 06:35:52 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chen", "Runnan", ""], ["Ma", "Yuexin", ""], ["Chen", "Nenglun", ""], ["Liu", "Lingjie", ""], ["Cui", "Zhiming", ""], ["Lin", "Yanhong", ""], ["Wang", "Wenping", ""]]}, {"id": "2107.09903", "submitter": "Xing Wei", "authors": "Ning Li, Kaitao Jiang, Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong\n  Gong", "title": "Anomaly Detection via Self-organizing Map", "comments": "International Conference on Image Processing(ICIP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection plays a key role in industrial manufacturing for product\nquality control. Traditional methods for anomaly detection are rule-based with\nlimited generalization ability. Recent methods based on supervised deep\nlearning are more powerful but require large-scale annotated datasets for\ntraining. In practice, abnormal products are rare thus it is very difficult to\ntrain a deep model in a fully supervised way. In this paper, we propose a novel\nunsupervised anomaly detection approach based on Self-organizing Map (SOM). Our\nmethod, Self-organizing Map for Anomaly Detection (SOMAD) maintains normal\ncharacteristics by using topological memory based on multi-scale features.\nSOMAD achieves state-of the-art performance on unsupervised anomaly detection\nand localization on the MVTec dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 06:56:57 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Ning", ""], ["Jiang", "Kaitao", ""], ["Ma", "Zhiheng", ""], ["Wei", "Xing", ""], ["Hong", "Xiaopeng", ""], ["Gong", "Yihong", ""]]}, {"id": "2107.09909", "submitter": "Haiwen Hong", "authors": "Haiwen Hong, Xuan Jin, Yin Zhang, Yunqing Hu, Jingfeng Zhang, Yuan He,\n  Hui Xue", "title": "DRDF: Determining the Importance of Different Multimodal Information\n  with Dual-Router Dynamic Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multimodal tasks, we find that the importance of text and image modal\ninformation is different for different input cases, and for this motivation, we\npropose a high-performance and highly general Dual-Router Dynamic Framework\n(DRDF), consisting of Dual-Router, MWF-Layer, experts and expert fusion unit.\nThe text router and image router in Dual-Router accept text modal information\nand image modal information, and use MWF-Layer to determine the importance of\nmodal information. Based on the result of the determination, MWF-Layer\ngenerates fused weights for the fusion of experts. Experts are model backbones\nthat match the current task. DRDF has high performance and high generality, and\nwe have tested 12 backbones such as Visual BERT on multimodal dataset Hateful\nmemes, unimodal dataset CIFAR10, CIFAR100, and TinyImagenet. Our DRDF\noutperforms all the baselines. We also verified the components of DRDF in\ndetail by ablations, compared and discussed the reasons and ideas of DRDF\ndesign.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 07:19:33 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hong", "Haiwen", ""], ["Jin", "Xuan", ""], ["Zhang", "Yin", ""], ["Hu", "Yunqing", ""], ["Zhang", "Jingfeng", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "2107.09923", "submitter": "Shuqiang Wang", "authors": "Bowen Hu, Baiying Lei, Yanyan Shen, Yong Liu, Shuqiang Wang", "title": "A Point Cloud Generative Model via Tree-Structured Graph Convolutions\n  for 3D Brain Shape Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing medical images and the corresponding 3D shape representation can\nprovide complementary information and microstructure details to improve the\noperational performance and accuracy in brain surgery. However, compared to the\nsubstantial image data, it is almost impossible to obtain the intraoperative 3D\nshape information by using physical methods such as sensor scanning, especially\nin minimally invasive surgery and robot-guided surgery. In this paper, a\ngeneral generative adversarial network (GAN) architecture based on graph\nconvolutional networks is proposed to reconstruct the 3D point clouds (PCs) of\nbrains by using one single 2D image, thus relieving the limitation of acquiring\n3D shape data during surgery. Specifically, a tree-structured generative\nmechanism is constructed to use the latent vector effectively and transfer\nfeatures between hidden layers accurately. With the proposed generative model,\na spontaneous image-to-PC conversion is finished in real-time. Competitive\nqualitative and quantitative experimental results have been achieved on our\nmodel. In multiple evaluation methods, the proposed model outperforms another\ncommon point cloud generative model PointOutNet.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 07:57:37 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hu", "Bowen", ""], ["Lei", "Baiying", ""], ["Shen", "Yanyan", ""], ["Liu", "Yong", ""], ["Wang", "Shuqiang", ""]]}, {"id": "2107.09928", "submitter": "Shuqiang Wang", "authors": "Qiankun Zuo, Baiying Lei, Yanyan Shen, Yong Liu, Zhiguang Feng,\n  Shuqiang Wang", "title": "Multimodal Representations Learning and Adversarial Hypergraph Fusion\n  for Early Alzheimer's Disease Prediction", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal neuroimage can provide complementary information about the\ndementia, but small size of complete multimodal data limits the ability in\nrepresentation learning. Moreover, the data distribution inconsistency from\ndifferent modalities may lead to ineffective fusion, which fails to\nsufficiently explore the intra-modal and inter-modal interactions and\ncompromises the disease diagnosis performance. To solve these problems, we\nproposed a novel multimodal representation learning and adversarial hypergraph\nfusion (MRL-AHF) framework for Alzheimer's disease diagnosis using complete\ntrimodal images. First, adversarial strategy and pre-trained model are\nincorporated into the MRL to extract latent representations from multimodal\ndata. Then two hypergraphs are constructed from the latent representations and\nthe adversarial network based on graph convolution is employed to narrow the\ndistribution difference of hyperedge features. Finally, the hyperedge-invariant\nfeatures are fused for disease prediction by hyperedge convolution. Experiments\non the public Alzheimer's Disease Neuroimaging Initiative(ADNI) database\ndemonstrate that our model achieves superior performance on Alzheimer's disease\ndetection compared with other related models and provides a possible way to\nunderstand the underlying mechanisms of disorder's progression by analyzing the\nabnormal brain connections.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 08:08:05 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zuo", "Qiankun", ""], ["Lei", "Baiying", ""], ["Shen", "Yanyan", ""], ["Liu", "Yong", ""], ["Feng", "Zhiguang", ""], ["Wang", "Shuqiang", ""]]}, {"id": "2107.09953", "submitter": "Shuqiang Wang", "authors": "Junren Pan, Baiying Lei, Yanyan Shen, Yong Liu, Zhiguang Feng,\n  Shuqiang Wang", "title": "Characterization Multimodal Connectivity of Brain Network by Hypergraph\n  GAN for Alzheimer's Disease Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using multimodal neuroimaging data to characterize brain network is currently\nan advanced technique for Alzheimer's disease(AD) Analysis. Over recent years\nthe neuroimaging community has made tremendous progress in the study of\nresting-state functional magnetic resonance imaging (rs-fMRI) derived from\nblood-oxygen-level-dependent (BOLD) signals and Diffusion Tensor Imaging (DTI)\nderived from white matter fiber tractography. However, Due to the heterogeneity\nand complexity between BOLD signals and fiber tractography, Most existing\nmultimodal data fusion algorithms can not sufficiently take advantage of the\ncomplementary information between rs-fMRI and DTI. To overcome this problem, a\nnovel Hypergraph Generative Adversarial Networks(HGGAN) is proposed in this\npaper, which utilizes Interactive Hyperedge Neurons module (IHEN) and Optimal\nHypergraph Homomorphism algorithm(OHGH) to generate multimodal connectivity of\nBrain Network from rs-fMRI combination with DTI. To evaluate the performance of\nthis model, We use publicly available data from the ADNI database to\ndemonstrate that the proposed model not only can identify discriminative brain\nregions of AD but also can effectively improve classification performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:02:29 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Pan", "Junren", ""], ["Lei", "Baiying", ""], ["Shen", "Yanyan", ""], ["Liu", "Yong", ""], ["Feng", "Zhiguang", ""], ["Wang", "Shuqiang", ""]]}, {"id": "2107.09957", "submitter": "Deep Patel", "authors": "Deep Patel and P.S. Sastry", "title": "Memorization in Deep Neural Networks: Does the Loss Function matter?", "comments": "Accepted at PAKDD 2021. 12 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks, often owing to the overparameterization, are shown to\nbe capable of exactly memorizing even randomly labelled data. Empirical studies\nhave also shown that none of the standard regularization techniques mitigate\nsuch overfitting. We investigate whether the choice of the loss function can\naffect this memorization. We empirically show, with benchmark data sets MNIST\nand CIFAR-10, that a symmetric loss function, as opposed to either\ncross-entropy or squared error loss, results in significant improvement in the\nability of the network to resist such overfitting. We then provide a formal\ndefinition for robustness to memorization and provide a theoretical explanation\nas to why the symmetric losses provide this robustness. Our results clearly\nbring out the role loss functions alone can play in this phenomenon of\nmemorization.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:08:51 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 05:36:24 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Patel", "Deep", ""], ["Sastry", "P. S.", ""]]}, {"id": "2107.09965", "submitter": "James Noeckel", "authors": "James Noeckel, Haisen Zhao, Brian Curless, Adriana Schulz", "title": "Fabrication-Aware Reverse Engineering for Carpentry", "comments": "24 pages, plus 6 pages of supplemental material. 14 figures. To be\n  published in Eurographics Symposium on Geometry Processing, Volume 40 (2021),\n  Number 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to generate fabrication blueprints from images of\ncarpentered items. While 3D reconstruction from images is a well-studied\nproblem, typical approaches produce representations that are ill-suited for\ncomputer-aided design and fabrication applications. Our key insight is that\nfabrication processes define and constrain the design space for carpentered\nobjects, and can be leveraged to develop novel reconstruction methods. Our\nmethod makes use of domain-specific constraints to recover not just valid\ngeometry, but a semantically valid assembly of parts, using a combination of\nimage-based and geometric optimization techniques.\n  We demonstrate our method on a variety of wooden objects and furniture, and\nshow that we can automatically obtain designs that are both easy to edit and\naccurate recreations of the ground truth. We further illustrate how our method\ncan be used to fabricate a physical replica of the captured object as well as a\ncustomized version, which can be produced by directly editing the reconstructed\nmodel in CAD software.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:25:15 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Noeckel", "James", ""], ["Zhao", "Haisen", ""], ["Curless", "Brian", ""], ["Schulz", "Adriana", ""]]}, {"id": "2107.09989", "submitter": "Guang Yang A", "authors": "Guangyuan Li, Jun Lv, Xiangrong Tong, Chengyan Wang, Guang Yang", "title": "High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial\n  Network with Attention and Cyclic Loss", "comments": "21 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Magnetic resonance imaging (MRI) is an important medical imaging modality,\nbut its acquisition speed is quite slow due to the physiological limitations.\nRecently, super-resolution methods have shown excellent performance in\naccelerating MRI. In some circumstances, it is difficult to obtain\nhigh-resolution images even with prolonged scan time. Therefore, we proposed a\nnovel super-resolution method that uses a generative adversarial network (GAN)\nwith cyclic loss and attention mechanism to generate high-resolution MR images\nfrom low-resolution MR images by a factor of 2. We implemented our model on\npelvic images from healthy subjects as training and validation data, while\nthose data from patients were used for testing. The MR dataset was obtained\nusing different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four\nmethods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison.\nStructural similarity, peak signal to noise ratio, root mean square error, and\nvariance inflation factor were used as calculation indicators to evaluate the\nperformances of the proposed method. Various experimental results showed that\nour method can better restore the details of the high-resolution MR image as\ncompared to the other methods. In addition, the reconstructed high-resolution\nMR image can provide better lesion textures in the tumor patients, which is\npromising to be used in clinical diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 10:07:22 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Guangyuan", ""], ["Lv", "Jun", ""], ["Tong", "Xiangrong", ""], ["Wang", "Chengyan", ""], ["Yang", "Guang", ""]]}, {"id": "2107.10004", "submitter": "Srikrishna Jaganathan", "authors": "Srikrishna Jaganathan, Jian Wang, Anja Borsdorf, Karthik Shetty,\n  Andreas Maier", "title": "Deep Iterative 2D/3D Registration", "comments": "10 pages,2 figures, Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning-based 2D/3D registration methods are highly robust but often\nlack the necessary registration accuracy for clinical application. A refinement\nstep using the classical optimization-based 2D/3D registration method applied\nin combination with Deep Learning-based techniques can provide the required\naccuracy. However, it also increases the runtime. In this work, we propose a\nnovel Deep Learning driven 2D/3D registration framework that can be used\nend-to-end for iterative registration tasks without relying on any further\nrefinement step. We accomplish this by learning the update step of the 2D/3D\nregistration framework using Point-to-Plane Correspondences. The update step is\nlearned using iterative residual refinement-based optical flow estimation, in\ncombination with the Point-to-Plane correspondence solver embedded as a known\noperator. Our proposed method achieves an average runtime of around 8s, a mean\nre-projection distance error of 0.60 $\\pm$ 0.40 mm with a success ratio of 97\npercent and a capture range of 60 mm. The combination of high registration\naccuracy, high robustness, and fast runtime makes our solution ideal for\nclinical applications.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 10:51:29 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Jaganathan", "Srikrishna", ""], ["Wang", "Jian", ""], ["Borsdorf", "Anja", ""], ["Shetty", "Karthik", ""], ["Maier", "Andreas", ""]]}, {"id": "2107.10006", "submitter": "Mola Ayenew", "authors": "Nils Nordmark and Mola Ayenew", "title": "Window Detection In Facade Imagery: A Deep Learning Approach Using Mask\n  R-CNN", "comments": "13 pages, 65 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The parsing of windows in building facades is a long-desired but challenging\ntask in computer vision. It is crucial to urban analysis, semantic\nreconstruction, lifecycle analysis, digital twins, and scene parsing amongst\nother building-related tasks that require high-quality semantic data. This\narticle investigates the usage of the mask R-CNN framework to be used for\nwindow detection of facade imagery input. We utilize transfer learning to train\nour proposed method on COCO weights with our own collected dataset of street\nview images of facades to produce instance segmentations of our new window\nclass. Experimental results show that our suggested approach with a relatively\nsmall dataset trains the network only with transfer learning and augmentation\nachieves results on par with prior state-of-the-art window detection\napproaches, even without post-optimization techniques.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:00:01 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Nordmark", "Nils", ""], ["Ayenew", "Mola", ""]]}, {"id": "2107.10050", "submitter": "Alexandra Dana", "authors": "Alexandra Dana, Maor Shutman, Yotam Perlitz, Ran Vitek, Tomer Peleg,\n  Roy Jevnisek", "title": "You Better Look Twice: a new perspective for designing accurate\n  detectors with reduced computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General object detectors use powerful backbones that uniformly extract\nfeatures from images for enabling detection of a vast amount of object types.\nHowever, utilization of such backbones in object detection applications\ndeveloped for specific object types can unnecessarily over-process an extensive\namount of background. In addition, they are agnostic to object scales, thus\nredundantly process all image regions at the same resolution. In this work we\nintroduce BLT-net, a new low-computation two-stage object detection\narchitecture designed to process images with a significant amount of background\nand objects of variate scales. BLT-net reduces computations by separating\nobjects from background using a very lite first-stage. BLT-net then efficiently\nmerges obtained proposals to further decrease processed background and then\ndynamically reduces their resolution to minimize computations. Resulting image\nproposals are then processed in the second-stage by a highly accurate model. We\ndemonstrate our architecture on the pedestrian detection problem, where objects\nare of different sizes, images are of high resolution and object detection is\nrequired to run in real-time. We show that our design reduces computations by a\nfactor of x4-x7 on the Citypersons and Caltech datasets with respect to leading\npedestrian detectors, on account of a small accuracy degradation. This method\ncan be applied on other object detection applications in scenes with a\nconsiderable amount of background and variate object sizes to reduce\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:39:51 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Dana", "Alexandra", ""], ["Shutman", "Maor", ""], ["Perlitz", "Yotam", ""], ["Vitek", "Ran", ""], ["Peleg", "Tomer", ""], ["Jevnisek", "Roy", ""]]}, {"id": "2107.10060", "submitter": "Liang Hou", "authors": "Liang Hou, Qi Cao, Huawei Shen, Xueqi Cheng", "title": "cGANs with Auxiliary Discriminative Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative models aim to learn the underlying joint distribution\nof data and labels, and thus realize conditional generation. Among them,\nauxiliary classifier generative adversarial networks (AC-GAN) have been widely\nused, but suffer from the issue of low intra-class diversity on generated\nsamples. In this paper, we point out that the fundamental reason is that the\nclassifier of AC-GAN is generator-agnostic, and thus cannot provide informative\nguidance to the generator to approximate the target joint distribution, leading\nto a minimization of conditional entropy that decreases the intra-class\ndiversity. Based on this finding, we propose novel cGANs with auxiliary\ndiscriminative classifier (ADC-GAN) to address the issue of AC-GAN.\nSpecifically, the auxiliary discriminative classifier becomes generator-aware\nby distinguishing between the real and fake data while recognizing their\nlabels. We then optimize the generator based on the auxiliary classifier along\nwith the original discriminator to match the joint and marginal distributions\nof the generated samples with those of the real samples. We provide theoretical\nanalysis and empirical evidence on synthetic and real-world datasets to\ndemonstrate the superiority of the proposed ADC-GAN compared to competitive\ncGANs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 13:06:32 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 06:16:51 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Hou", "Liang", ""], ["Cao", "Qi", ""], ["Shen", "Huawei", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2107.10064", "submitter": "Mohamed Ali Souibgui", "authors": "Mohamed Ali Souibgui, Alicia Forn\\'es, Yousri Kessentini, Be\\'ata\n  Megyesi", "title": "Few Shots Is All You Need: A Progressive Few Shot Learning Approach for\n  Low Resource Handwriting Recognition", "comments": "Under Revision at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten text recognition in low resource scenarios, such as manuscripts\nwith rare alphabets, is a challenging problem. The main difficulty comes from\nthe very few annotated data and the limited linguistic information (e.g.\ndictionaries and language models). Thus, we propose a few-shot learning-based\nhandwriting recognition approach that significantly reduces the human labor\nannotation process, requiring only few images of each alphabet symbol. First,\nour model detects all symbols of a given alphabet in a textline image, then a\ndecoding step maps the symbol similarity scores to the final sequence of\ntranscribed symbols. Our model is first pretrained on synthetic line images\ngenerated from any alphabet, even though different from the target domain. A\nsecond training step is then applied to diminish the gap between the source and\ntarget data. Since this retraining would require annotation of thousands of\nhandwritten symbols together with their bounding boxes, we propose to avoid\nsuch human effort through an unsupervised progressive learning approach that\nautomatically assigns pseudo-labels to the non-annotated data. The evaluation\non different manuscript datasets show that our model can lead to competitive\nresults with a significant reduction in human effort.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 13:18:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Souibgui", "Mohamed Ali", ""], ["Forn\u00e9s", "Alicia", ""], ["Kessentini", "Yousri", ""], ["Megyesi", "Be\u00e1ta", ""]]}, {"id": "2107.10068", "submitter": "Ning Shuliang", "authors": "Mengcheng Lan, Shuliang Ning, Yanran Li, Qian Chen, Xunlai Chen,\n  Xiaoguang Han, Shuguang Cui", "title": "From Single to Multiple: Leveraging Multi-level Prediction Spaces for\n  Video Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite video forecasting has been a widely explored topic in recent years,\nthe mainstream of the existing work still limits their models with a single\nprediction space but completely neglects the way to leverage their model with\nmulti-prediction spaces. This work fills this gap. For the first time, we\ndeeply study numerous strategies to perform video forecasting in\nmulti-prediction spaces and fuse their results together to boost performance.\nThe prediction in the pixel space usually lacks the ability to preserve the\nsemantic and structure content of the video however the prediction in the\nhigh-level feature space is prone to generate errors in the reduction and\nrecovering process. Therefore, we build a recurrent connection between\ndifferent feature spaces and incorporate their generations in the upsampling\nprocess. Rather surprisingly, this simple idea yields a much more significant\nperformance boost than PhyDNet (performance improved by 32.1% MAE on MNIST-2\ndataset, and 21.4% MAE on KTH dataset). Both qualitative and quantitative\nevaluations on four datasets demonstrate the generalization ability and\neffectiveness of our approach. We show that our model significantly reduces the\ntroublesome distortions and blurry artifacts and brings remarkable improvements\nto the accuracy in long term video prediction. The code will be released soon.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 13:23:16 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lan", "Mengcheng", ""], ["Ning", "Shuliang", ""], ["Li", "Yanran", ""], ["Chen", "Qian", ""], ["Chen", "Xunlai", ""], ["Han", "Xiaoguang", ""], ["Cui", "Shuguang", ""]]}, {"id": "2107.10073", "submitter": "Guillaume Jaume", "authors": "Guillaume Jaume, Pushpak Pati, Valentin Anklin, Antonio Foncubierta,\n  Maria Gabrani", "title": "HistoCartography: A Toolkit for Graph Analytics in Digital Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in entity-graph based analysis of histopathology images have brought\nin a new paradigm to describe tissue composition, and learn the tissue\nstructure-to-function relationship. Entity-graphs offer flexible and scalable\nrepresentations to characterize tissue organization, while allowing the\nincorporation of prior pathological knowledge to further support model\ninterpretability and explainability. However, entity-graph analysis requires\nprerequisites for image-to-graph translation and knowledge of state-of-the-art\nmachine learning algorithms applied to graph-structured data, which can\npotentially hinder their adoption. In this work, we aim to alleviate these\nissues by developing HistoCartography, a standardized python API with necessary\npreprocessing, machine learning and explainability tools to facilitate\ngraph-analytics in computational pathology. Further, we have benchmarked the\ncomputational time and performance on multiple datasets across different\nimaging types and histopathology tasks to highlight the applicability of the\nAPI for building computational pathology workflows.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 13:34:14 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Jaume", "Guillaume", ""], ["Pati", "Pushpak", ""], ["Anklin", "Valentin", ""], ["Foncubierta", "Antonio", ""], ["Gabrani", "Maria", ""]]}, {"id": "2107.10100", "submitter": "Zhitong Gao", "authors": "Shuailin Li, Zhitong Gao, Xuming He", "title": "Superpixel-guided Iterative Learning from Noisy Labels for Medical Image\n  Segmentation", "comments": "To appear in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning segmentation from noisy labels is an important task for medical\nimage analysis due to the difficulty in acquiring highquality annotations. Most\nexisting methods neglect the pixel correlation and structural prior in\nsegmentation, often producing noisy predictions around object boundaries. To\naddress this, we adopt a superpixel representation and develop a robust\niterative learning strategy that combines noise-aware training of segmentation\nnetwork and noisy label refinement, both guided by the superpixels. This design\nenables us to exploit the structural constraints in segmentation labels and\neffectively mitigate the impact of label noise in learning. Experiments on two\nbenchmarks show that our method outperforms recent state-of-the-art approaches,\nand achieves superior robustness in a wide range of label noises. Code is\navailable at https://github.com/gaozhitong/SP_guided_Noisy_Label_Seg.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 14:27:36 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Shuailin", ""], ["Gao", "Zhitong", ""], ["He", "Xuming", ""]]}, {"id": "2107.10140", "submitter": "Viraj Prabhu", "authors": "Viraj Prabhu, Shivam Khare, Deeksha Kartik, Judy Hoffman", "title": "S4T: Source-free domain adaptation for semantic segmentation via\n  self-supervised selective self-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most modern approaches for domain adaptive semantic segmentation rely on\ncontinued access to source data during adaptation, which may be infeasible due\nto computational or privacy constraints. We focus on source-free domain\nadaptation for semantic segmentation, wherein a source model must adapt itself\nto a new target domain given only unlabeled target data. We propose\nSelf-Supervised Selective Self-Training (S4T), a source-free adaptation\nalgorithm that first uses the model's pixel-level predictive consistency across\ndiverse views of each target image along with model confidence to classify\npixel predictions as either reliable or unreliable. Next, the model is\nself-trained, using predicted pseudolabels for reliable predictions and\npseudolabels inferred via a selective interpolation strategy for unreliable\nones. S4T matches or improves upon the state-of-the-art in source-free\nadaptation on 3 standard benchmarks for semantic segmentation within a single\nepoch of adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:18:01 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Prabhu", "Viraj", ""], ["Khare", "Shivam", ""], ["Kartik", "Deeksha", ""], ["Hoffman", "Judy", ""]]}, {"id": "2107.10161", "submitter": "Wentao Bao", "authors": "Wentao Bao, Qi Yu, Yu Kong", "title": "Evidential Deep Learning for Open Set Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a real-world scenario, human actions are typically out of the distribution\nfrom training data, which requires a model to both recognize the known actions\nand reject the unknown. Different from image data, video actions are more\nchallenging to be recognized in an open-set setting due to the uncertain\ntemporal dynamics and static bias of human actions. In this paper, we propose a\nDeep Evidential Action Recognition (DEAR) method to recognize actions in an\nopen testing set. Specifically, we formulate the action recognition problem\nfrom the evidential deep learning (EDL) perspective and propose a novel model\ncalibration method to regularize the EDL training. Besides, to mitigate the\nstatic bias of video representation, we propose a plug-and-play module to\ndebias the learned representation through contrastive learning. Experimental\nresults show that our DEAR method achieves consistent performance gain on\nmultiple mainstream action recognition models and benchmarks. Codes and\npre-trained weights will be made available upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:45:37 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bao", "Wentao", ""], ["Yu", "Qi", ""], ["Kong", "Yu", ""]]}, {"id": "2107.10180", "submitter": "Dennis Eschweiler", "authors": "Dennis Eschweiler, Malte Rethwisch, Mareike Jarchow, Simon Koppers,\n  Johannes Stegmaier", "title": "3D fluorescence microscopy data synthesis for segmentation and\n  benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated image processing approaches are indispensable for many biomedical\nexperiments and help to cope with the increasing amount of microscopy image\ndata in a fast and reproducible way. Especially state-of-the-art deep\nlearning-based approaches most often require large amounts of annotated\ntraining data to produce accurate and generalist outputs, but they are often\ncompromised by the general lack of those annotated data sets. In this work, we\npropose how conditional generative adversarial networks can be utilized to\ngenerate realistic image data for 3D fluorescence microscopy from annotation\nmasks of 3D cellular structures. In combination with mask simulation\napproaches, we demonstrate the generation of fully-annotated 3D microscopy data\nsets that we make publicly available for training or benchmarking. An\nadditional positional conditioning of the cellular structures enables the\nreconstruction of position-dependent intensity characteristics and allows to\ngenerate image data of different quality levels. A patch-wise working principle\nand a subsequent full-size reassemble strategy is used to generate image data\nof arbitrary size and different organisms. We present this as a\nproof-of-concept for the automated generation of fully-annotated training data\nsets requiring only a minimum of manual interaction to alleviate the need of\nmanual annotations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:08:56 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Eschweiler", "Dennis", ""], ["Rethwisch", "Malte", ""], ["Jarchow", "Mareike", ""], ["Koppers", "Simon", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "2107.10189", "submitter": "Wentao Bao", "authors": "Wentao Bao, Qi Yu, Yu Kong", "title": "DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic accident anticipation aims to accurately and promptly predict the\noccurrence of a future accident from dashcam videos, which is vital for a\nsafety-guaranteed self-driving system. To encourage an early and accurate\ndecision, existing approaches typically focus on capturing the cues of spatial\nand temporal context before a future accident occurs. However, their\ndecision-making lacks visual explanation and ignores the dynamic interaction\nwith the environment. In this paper, we propose Deep ReInforced accident\nanticipation with Visual Explanation, named DRIVE. The method simulates both\nthe bottom-up and top-down visual attention mechanism in a dashcam observation\nenvironment so that the decision from the proposed stochastic multi-task agent\ncan be visually explained by attentive regions. Moreover, the proposed dense\nanticipation reward and sparse fixation reward are effective in training the\nDRIVE model with our improved reinforcement learning algorithm. Experimental\nresults show that the DRIVE model achieves state-of-the-art performance on\nmultiple real-world traffic accident datasets. The code and pre-trained model\nwill be available upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:33:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bao", "Wentao", ""], ["Yu", "Qi", ""], ["Kong", "Yu", ""]]}, {"id": "2107.10220", "submitter": "Anastasia Antsiferova", "authors": "Anastasia Antsiferova, Alexander Yakovenko, Nickolay Safonov, Dmitriy\n  Kulikov, Alexander Gushin, and Dmitriy Vatolin", "title": "Objective video quality metrics application to video codecs comparisons:\n  choosing the best for subjective quality estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment plays a key role in creating and comparing video\ncompression algorithms. Despite the development of a large number of new\nmethods for assessing quality, generally accepted and well-known codecs\ncomparisons mainly use the classical methods like PSNR, SSIM and new method\nVMAF. These methods can be calculated following different rules: they can use\ndifferent frame-by-frame averaging techniques or different summation of color\ncomponents. In this paper, a fundamental comparison of various versions of\ngenerally accepted metrics is carried out to find the most relevant and\nrecommended versions of video quality metrics to be used in codecs comparisons.\nFor comparison, we used a set of videos encoded with video codecs of different\nstandards, and visual quality scores collected for the resulting set of streams\nsince 2018 until 2021\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:18:11 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Antsiferova", "Anastasia", ""], ["Yakovenko", "Alexander", ""], ["Safonov", "Nickolay", ""], ["Kulikov", "Dmitriy", ""], ["Gushin", "Alexander", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "2107.10224", "submitter": "Shoufa Chen", "authors": "Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, Ping Luo", "title": "CycleMLP: A MLP-like Architecture for Dense Prediction", "comments": "Technical report. Code: \\url{https://github.com/ShoufaChen/CycleMLP}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple MLP-like architecture, CycleMLP, which is a\nversatile backbone for visual recognition and dense predictions, unlike modern\nMLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose architectures are\ncorrelated to image size and thus are infeasible in object detection and\nsegmentation. CycleMLP has two advantages compared to modern approaches. (1) It\ncan cope with various image sizes. (2) It achieves linear computational\ncomplexity to image size by using local windows. In contrast, previous MLPs\nhave quadratic computations because of their fully spatial connections. We\nbuild a family of models that surpass existing MLPs and achieve a comparable\naccuracy (83.2%) on ImageNet-1K classification compared to the state-of-the-art\nTransformer such as Swin Transformer (83.3%) but using fewer parameters and\nFLOPs. We expand the MLP-like models' applicability, making them a versatile\nbackbone for dense prediction tasks. CycleMLP aims to provide a competitive\nbaseline on object detection, instance segmentation, and semantic segmentation\nfor MLP models. In particular, CycleMLP achieves 45.1 mIoU on ADE20K val,\ncomparable to Swin (45.2 mIOU). Code is available at\n\\url{https://github.com/ShoufaChen/CycleMLP}.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:23:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chen", "Shoufa", ""], ["Xie", "Enze", ""], ["Ge", "Chongjian", ""], ["Liang", "Ding", ""], ["Luo", "Ping", ""]]}, {"id": "2107.10296", "submitter": "Minghan Zhu", "authors": "Minghan Zhu, Maani Ghaffari, Huei Peng", "title": "Correspondence-Free Point Cloud Registration with SO(3)-Equivariant\n  Implicit Shape Representations", "comments": "7 pages. 2 figures. Submitted to CoRL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a correspondence-free method for point cloud rotational\nregistration. We learn an embedding for each point cloud in a feature space\nthat preserves the SO(3)-equivariance property, enabled by recent developments\nin equivariant neural networks. The proposed shape registration method achieves\nthree major advantages through combining equivariant feature learning with\nimplicit shape models. First, the necessity of data association is removed\nbecause of the permutation-invariant property in network architectures similar\nto PointNet. Second, the registration in feature space can be solved in\nclosed-form using Horn's method due to the SO(3)-equivariance property. Third,\nthe registration is robust to noise in the point cloud because of implicit\nshape learning. The experimental results show superior performance compared\nwith existing correspondence-free deep registration methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:18:21 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhu", "Minghan", ""], ["Ghaffari", "Maani", ""], ["Peng", "Huei", ""]]}, {"id": "2107.10297", "submitter": "Boris Ivanovic", "authors": "Boris Ivanovic and Marco Pavone", "title": "Rethinking Trajectory Forecasting Evaluation", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the behavior of other agents is an integral part of the modern\nrobotic autonomy stack, especially in safety-critical scenarios with\nhuman-robot interaction, such as autonomous driving. In turn, there has been a\nsignificant amount of interest and research in trajectory forecasting,\nresulting in a wide variety of approaches. Common to all works, however, is the\nuse of the same few accuracy-based evaluation metrics, e.g., displacement error\nand log-likelihood. While these metrics are informative, they are task-agnostic\nand predictions that are evaluated as equal can lead to vastly different\noutcomes, e.g., in downstream planning and decision making. In this work, we\ntake a step back and critically evaluate current trajectory forecasting\nmetrics, proposing task-aware metrics as a better measure of performance in\nsystems where prediction is being deployed. We additionally present one example\nof such a metric, incorporating planning-awareness within existing trajectory\nforecasting metrics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:20:03 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ivanovic", "Boris", ""], ["Pavone", "Marco", ""]]}, {"id": "2107.10300", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha and Vinija Jain", "title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural\n  Language with Interpretability", "comments": "12 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:56:34 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chadha", "Aman", ""], ["Jain", "Vinija", ""]]}, {"id": "2107.10327", "submitter": "Arindam Sengupta", "authors": "Arindam Sengupta and Siyang Cao", "title": "mmPose-NLP: A Natural Language Processing Approach to Precise Skeletal\n  Pose Estimation using mmWave Radars", "comments": "Submitted to IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we presented mmPose-NLP, a novel Natural Language Processing\n(NLP) inspired Sequence-to-Sequence (Seq2Seq) skeletal key-point estimator\nusing millimeter-wave (mmWave) radar data. To the best of the author's\nknowledge, this is the first method to precisely estimate upto 25 skeletal\nkey-points using mmWave radar data alone. Skeletal pose estimation is critical\nin several applications ranging from autonomous vehicles, traffic monitoring,\npatient monitoring, gait analysis, to defense security forensics, and aid both\npreventative and actionable decision making. The use of mmWave radars for this\ntask, over traditionally employed optical sensors, provide several advantages,\nprimarily its operational robustness to scene lighting and adverse weather\nconditions, where optical sensor performance degrade significantly. The mmWave\nradar point-cloud (PCL) data is first voxelized (analogous to tokenization in\nNLP) and $N$ frames of the voxelized radar data (analogous to a text paragraph\nin NLP) is subjected to the proposed mmPose-NLP architecture, where the voxel\nindices of the 25 skeletal key-points (analogous to keyword extraction in NLP)\nare predicted. The voxel indices are converted back to real world 3-D\ncoordinates using the voxel dictionary used during the tokenization process.\nMean Absolute Error (MAE) metrics were used to measure the accuracy of the\nproposed system against the ground truth, with the proposed mmPose-NLP offering\n<3 cm localization errors in the depth, horizontal and vertical axes. The\neffect of the number of input frames vs performance/accuracy was also studied\nfor N = {1,2,..,10}. A comprehensive methodology, results, discussions and\nlimitations are presented in this paper. All the source codes and results are\nmade available on GitHub for furthering research and development in this\ncritical yet emerging domain of skeletal key-point estimation using mmWave\nradars.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 19:45:17 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Sengupta", "Arindam", ""], ["Cao", "Siyang", ""]]}, {"id": "2107.10356", "submitter": "Judy Gichoya", "authors": "Imon Banerjee, Ananth Reddy Bhimireddy, John L. Burns, Leo Anthony\n  Celi, Li-Ching Chen, Ramon Correa, Natalie Dullerud, Marzyeh Ghassemi,\n  Shih-Cheng Huang, Po-Chih Kuo, Matthew P Lungren, Lyle Palmer, Brandon J\n  Price, Saptarshi Purkayastha, Ayis Pyrros, Luke Oakden-Rayner, Chima\n  Okechukwu, Laleh Seyyed-Kalantari, Hari Trivedi, Ryan Wang, Zachary Zaiman,\n  Haoran Zhang, Judy W Gichoya", "title": "Reading Race: AI Recognises Patient's Racial Identity In Medical Images", "comments": "Submitted to the Lancet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: In medical imaging, prior studies have demonstrated disparate AI\nperformance by race, yet there is no known correlation for race on medical\nimaging that would be obvious to the human expert interpreting the images.\n  Methods: Using private and public datasets we evaluate: A) performance\nquantification of deep learning models to detect race from medical images,\nincluding the ability of these models to generalize to external environments\nand across multiple imaging modalities, B) assessment of possible confounding\nanatomic and phenotype population features, such as disease distribution and\nbody habitus as predictors of race, and C) investigation into the underlying\nmechanism by which AI models can recognize race.\n  Findings: Standard deep learning models can be trained to predict race from\nmedical images with high performance across multiple imaging modalities. Our\nfindings hold under external validation conditions, as well as when models are\noptimized to perform clinically motivated tasks. We demonstrate this detection\nis not due to trivial proxies or imaging-related surrogate covariates for race,\nsuch as underlying disease distribution. Finally, we show that performance\npersists over all anatomical regions and frequency spectrum of the images\nsuggesting that mitigation efforts will be challenging and demand further\nstudy.\n  Interpretation: We emphasize that model ability to predict self-reported race\nis itself not the issue of importance. However, our findings that AI can\ntrivially predict self-reported race -- even from corrupted, cropped, and\nnoised medical images -- in a setting where clinical experts cannot, creates an\nenormous risk for all model deployments in medical imaging: if an AI model\nsecretly used its knowledge of self-reported race to misclassify all Black\npatients, radiologists would not be able to tell using the same data the model\nhas access to.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 21:10:16 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Banerjee", "Imon", ""], ["Bhimireddy", "Ananth Reddy", ""], ["Burns", "John L.", ""], ["Celi", "Leo Anthony", ""], ["Chen", "Li-Ching", ""], ["Correa", "Ramon", ""], ["Dullerud", "Natalie", ""], ["Ghassemi", "Marzyeh", ""], ["Huang", "Shih-Cheng", ""], ["Kuo", "Po-Chih", ""], ["Lungren", "Matthew P", ""], ["Palmer", "Lyle", ""], ["Price", "Brandon J", ""], ["Purkayastha", "Saptarshi", ""], ["Pyrros", "Ayis", ""], ["Oakden-Rayner", "Luke", ""], ["Okechukwu", "Chima", ""], ["Seyyed-Kalantari", "Laleh", ""], ["Trivedi", "Hari", ""], ["Wang", "Ryan", ""], ["Zaiman", "Zachary", ""], ["Zhang", "Haoran", ""], ["Gichoya", "Judy W", ""]]}, {"id": "2107.10373", "submitter": "Johannes Bayer", "authors": "Felix Thoma, Johannes Bayer, Yakun Li", "title": "A Public Ground-Truth Dataset for Handwritten Circuit Diagram Images", "comments": "6 pages, 3 figures, raw version as submitted to GREC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of digitization methods for line drawings (especially in the\narea of electrical engineering) relies on the availability of publicly\navailable training and evaluation data. This paper presents such an image set\nalong with annotations. The dataset consists of 1152 images of 144 circuits by\n12 drafters and 48 563 annotations. Each of these images depicts an electrical\ncircuit diagram, taken by consumer grade cameras under varying lighting\nconditions and perspectives. A variety of different pencil types and surface\nmaterials has been used. For each image, all individual electrical components\nare annotated with bounding boxes and one out of 45 class labels. In order to\nsimplify a graph extraction process, different helper symbols like junction\npoints and crossovers are introduced, while texts are annotated as well. The\ngeometric and taxonomic problems arising from this task as well as the classes\nthemselves and statistics of their appearances are stated. The performance of a\nstandard Faster RCNN on the dataset is provided as an object detection\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 22:10:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Thoma", "Felix", ""], ["Bayer", "Johannes", ""], ["Li", "Yakun", ""]]}, {"id": "2107.10404", "submitter": "Keivan Nalaie", "authors": "Keivan Nalaie, Rong Zheng", "title": "DeepScale: An Online Frame Size Adaptation Framework to Accelerate\n  Visual Multi-object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surveillance and search and rescue applications, it is important to\nperform multi-target tracking (MOT) in real-time on low-end devices. Today's\nMOT solutions employ deep neural networks, which tend to have high computation\ncomplexity. Recognizing the effects of frame sizes on tracking performance, we\npropose DeepScale, a model agnostic frame size selection approach that operates\non top of existing fully convolutional network-based trackers to accelerate\ntracking throughput. In the training stage, we incorporate detectability scores\ninto a one-shot tracker architecture so that DeepScale can learn representation\nestimations for different frame sizes in a self-supervised manner. During\ninference, based on user-controlled parameters, it can find a suitable\ntrade-off between tracking accuracy and speed by adapting frame sizes at run\ntime. Extensive experiments and benchmark tests on MOT datasets demonstrate the\neffectiveness and flexibility of DeepScale. Compared to a state-of-the-art\ntracker, DeepScale++, a variant of DeepScale achieves 1.57X accelerated with\nonly moderate degradation (~ 2.4) in tracking accuracy on the MOT15 dataset in\none configuration.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 00:12:58 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Nalaie", "Keivan", ""], ["Zheng", "Rong", ""]]}, {"id": "2107.10419", "submitter": "Wenbin Li", "authors": "Wenbin Li, Xuesong Yang, Meihao Kong, Lei Wang, Jing Huo, Yang Gao and\n  Jiebo Luo", "title": "Triplet is All You Need with Random Mappings for Unsupervised Visual\n  Representation Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive self-supervised learning (SSL) has achieved great success in\nunsupervised visual representation learning by maximizing the similarity\nbetween two augmented views of the same image (positive pairs) and\nsimultaneously contrasting other different images (negative pairs). However,\nthis type of methods, such as SimCLR and MoCo, relies heavily on a large number\nof negative pairs and thus requires either large batches or memory banks. In\ncontrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,\nattempt to discard negative pairs by introducing asymmetry and show remarkable\nperformance. Unfortunately, to avoid collapsed solutions caused by not using\nnegative pairs, these methods require sophisticated asymmetry designs. In this\npaper, we argue that negative pairs are still necessary but one is sufficient,\ni.e., triplet is all you need. A simple triplet-based loss can achieve\nsurprisingly good performance without requiring large batches or asymmetry.\nMoreover, we observe that unsupervised visual representation learning can gain\nsignificantly from randomness. Based on this observation, we propose a simple\nplug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other\nspaces and enforcing these randomly projected samples to satisfy the same\ncorrelation requirement. The proposed ROMA strategy not only achieves the\nstate-of-the-art performance in conjunction with the triplet-based loss, but\nalso can further effectively boost other SSL methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 02:06:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Li", "Wenbin", ""], ["Yang", "Xuesong", ""], ["Kong", "Meihao", ""], ["Wang", "Lei", ""], ["Huo", "Jing", ""], ["Gao", "Yang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2107.10433", "submitter": "Xiao Wang", "authors": "Xiao Wang, Xiujun Shu, Shiliang Zhang, Bo Jiang, Yaowei Wang, Yonghong\n  Tian, Feng Wu", "title": "MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking", "comments": "In Peer Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many RGB-T trackers attempt to attain robust feature representation by\nutilizing an adaptive weighting scheme (or attention mechanism). Different from\nthese works, we propose a new dynamic modality-aware filter generation module\n(named MFGNet) to boost the message communication between visible and thermal\ndata by adaptively adjusting the convolutional kernels for various input images\nin practical tracking. Given the image pairs as input, we first encode their\nfeatures with the backbone network. Then, we concatenate these feature maps and\ngenerate dynamic modality-aware filters with two independent networks. The\nvisible and thermal filters will be used to conduct a dynamic convolutional\noperation on their corresponding input feature maps respectively. Inspired by\nresidual connection, both the generated visible and thermal feature maps will\nbe summarized with input feature maps. The augmented feature maps will be fed\ninto the RoI align module to generate instance-level features for subsequent\nclassification. To address issues caused by heavy occlusion, fast motion, and\nout-of-view, we propose to conduct a joint local and global search by\nexploiting a new direction-aware target-driven attention mechanism. The spatial\nand temporal recurrent neural network is used to capture the direction-aware\ncontext for accurate global attention prediction. Extensive experiments on\nthree large-scale RGB-T tracking benchmark datasets validated the effectiveness\nof our proposed algorithm. The project page of this paper is available at\nhttps://sites.google.com/view/mfgrgbttrack/.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 03:10:51 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Xiao", ""], ["Shu", "Xiujun", ""], ["Zhang", "Shiliang", ""], ["Jiang", "Bo", ""], ["Wang", "Yaowei", ""], ["Tian", "Yonghong", ""], ["Wu", "Feng", ""]]}, {"id": "2107.10449", "submitter": "Zhendong Chu", "authors": "Zhendong Chu, Hongning Wang", "title": "Improve Learning from Crowds via Generative Augmentation", "comments": "KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467409", "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourcing provides an efficient label collection schema for supervised\nmachine learning. However, to control annotation cost, each instance in the\ncrowdsourced data is typically annotated by a small number of annotators. This\ncreates a sparsity issue and limits the quality of machine learning models\ntrained on such data. In this paper, we study how to handle sparsity in\ncrowdsourced data using data augmentation. Specifically, we propose to directly\nlearn a classifier by augmenting the raw sparse annotations. We implement two\nprinciples of high-quality augmentation using Generative Adversarial Networks:\n1) the generated annotations should follow the distribution of authentic ones,\nwhich is measured by a discriminator; 2) the generated annotations should have\nhigh mutual information with the ground-truth labels, which is measured by an\nauxiliary network. Extensive experiments and comparisons against an array of\nstate-of-the-art learning from crowds methods on three real-world datasets\nproved the effectiveness of our data augmentation framework. It shows the\npotential of our algorithm for low-budget crowdsourcing in general.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 04:14:30 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chu", "Zhendong", ""], ["Wang", "Hongning", ""]]}, {"id": "2107.10456", "submitter": "Hyukseong Kwon", "authors": "Hyukseong Kwon, Amir Rahimi, Kevin G. Lee, Amit Agarwal, Rajan\n  Bhattacharyya", "title": "CogSense: A Cognitively Inspired Framework for Perception Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes the CogSense system, which is inspired by sense-making\ncognition and perception in the mammalian brain to perform perception error\ndetection and perception parameter adaptation using probabilistic signal\ntemporal logic. As a specific application, a contrast-based perception adaption\nmethod is presented and validated. The proposed method evaluates perception\nerrors using heterogeneous probe functions computed from the detected objects\nand subsequently solves a contrast optimization problem to correct perception\nerrors. The CogSense probe functions utilize the characteristics of geometry,\ndynamics, and detected blob image quality of the objects to develop axioms in a\nprobabilistic signal temporal logic framework. By evaluating these axioms, we\ncan formally verify whether the detections are valid or erroneous. Further,\nusing the CogSense axioms, we generate the probabilistic signal temporal\nlogic-based constraints to finally solve the contrast-based optimization\nproblem to reduce false positives and false negatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 05:01:05 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kwon", "Hyukseong", ""], ["Rahimi", "Amir", ""], ["Lee", "Kevin G.", ""], ["Agarwal", "Amit", ""], ["Bhattacharyya", "Rajan", ""]]}, {"id": "2107.10466", "submitter": "Chenyu Tian", "authors": "Chenyu Tian, Ran Yu, Xinyuan Zhao, Weihao Xia, Haoqian Wang, Yujiu\n  Yang", "title": "PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current methods of multi-person pose estimation typically treat the\nlocalization and the association of body joints separately. It is convenient\nbut inefficient, leading to additional computation and a waste of time. This\npaper, however, presents a novel framework PoseDet (Estimating Pose by\nDetection) to localize and associate body joints simultaneously at higher\ninference speed. Moreover, we propose the keypoint-aware pose embedding to\nrepresent an object in terms of the locations of its keypoints. The proposed\npose embedding contains semantic and geometric information, allowing us to\naccess discriminative and informative features efficiently. It is utilized for\ncandidate classification and body joint localization in PoseDet, leading to\nrobust predictions of various poses. This simple framework achieves an\nunprecedented speed and a competitive accuracy on the COCO benchmark compared\nwith state-of-the-art methods. Extensive experiments on the CrowdPose benchmark\nshow the robustness in the crowd scenes. Source code is available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 05:54:00 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:23:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Tian", "Chenyu", ""], ["Yu", "Ran", ""], ["Zhao", "Xinyuan", ""], ["Xia", "Weihao", ""], ["Wang", "Haoqian", ""], ["Yang", "Yujiu", ""]]}, {"id": "2107.10476", "submitter": "Yiqing Shen", "authors": "Yufei Wang and Yiqing Shen and Meng Yuan and Jing Xu and Bin Yang and\n  Chi Liu and Wenjia Cai and Weijing Cheng and Wei Wang", "title": "A Deep Learning-based Quality Assessment and Segmentation System with a\n  Large-scale Benchmark Dataset for Optical Coherence Tomographic Angiography\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Coherence Tomography Angiography (OCTA) is a non-invasive and\nnon-contacting imaging technique providing visualization of microvasculature of\nretina and optic nerve head in human eyes in vivo. The adequate image quality\nof OCTA is the prerequisite for the subsequent quantification of retinal\nmicrovasculature. Traditionally, the image quality score based on signal\nstrength is used for discriminating low quality. However, it is insufficient\nfor identifying artefacts such as motion and off-centration, which rely\nspecialized knowledge and need tedious and time-consuming manual\nidentification. One of the most primary issues in OCTA analysis is to sort out\nthe foveal avascular zone (FAZ) region in the retina, which highly correlates\nwith any visual acuity disease. However, the variations in OCTA visual quality\naffect the performance of deep learning in any downstream marginally. Moreover,\nfiltering the low-quality OCTA images out is both labor-intensive and\ntime-consuming. To address these issues, we develop an automated computer-aided\nOCTA image processing system using deep neural networks as the classifier and\nsegmentor to help ophthalmologists in clinical diagnosis and research. This\nsystem can be an assistive tool as it can process OCTA images of different\nformats to assess the quality and segment the FAZ area. The source code is\nfreely available at https://github.com/shanzha09/COIPS.git.\n  Another major contribution is the large-scale OCTA dataset, namely\nOCTA-25K-IQA-SEG we publicize for performance evaluation. It is comprised of\nfour subsets, namely sOCTA-3$\\times$3-10k, sOCTA-6$\\times$6-14k,\nsOCTA-3$\\times$3-1.1k-seg, and dOCTA-6$\\times$6-1.1k-seg, which contains a\ntotal number of 25,665 images. The large-scale OCTA dataset is available at\nhttps://doi.org/10.5281/zenodo.5111975, https://doi.org/10.5281/zenodo.5111972.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 06:32:10 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Yufei", ""], ["Shen", "Yiqing", ""], ["Yuan", "Meng", ""], ["Xu", "Jing", ""], ["Yang", "Bin", ""], ["Liu", "Chi", ""], ["Cai", "Wenjia", ""], ["Cheng", "Weijing", ""], ["Wang", "Wei", ""]]}, {"id": "2107.10477", "submitter": "Zhengxiong Luo", "authors": "Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan and\n  Erjin Zhou", "title": "Adaptive Dilated Convolution For Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing human pose estimation (HPE) methods exploit multi-scale\ninformation by fusing feature maps of four different spatial sizes, \\ie $1/4$,\n$1/8$, $1/16$, and $1/32$ of the input image. There are two drawbacks of this\nstrategy: 1) feature maps of different spatial sizes may be not well aligned\nspatially, which potentially hurts the accuracy of keypoint location; 2) these\nscales are fixed and inflexible, which may restrict the generalization ability\nover various human sizes. Towards these issues, we propose an adaptive dilated\nconvolution (ADC). It can generate and fuse multi-scale features of the same\nspatial sizes by setting different dilation rates for different channels. More\nimportantly, these dilation rates are generated by a regression module. It\nenables ADC to adaptively adjust the fused scales and thus ADC may generalize\nbetter to various human sizes. ADC can be end-to-end trained and easily plugged\ninto existing methods. Extensive experiments show that ADC can bring consistent\nimprovements to various HPE methods. The source codes will be released for\nfurther research.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 06:38:04 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Luo", "Zhengxiong", ""], ["Wang", "Zhicheng", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""], ["Zhou", "Erjin", ""]]}, {"id": "2107.10479", "submitter": "Cheng Yang", "authors": "Cheng Yang", "title": "Copy and Paste method based on Pose for Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-identification (ReID) aims at matching objects in surveillance cameras\nwith different viewpoints. It's developing very fast, but there is no\nprocessing method for the ReID task in multiple scenarios at this stage.\nHowever, this dose happen all the time in real life, such as the security\nscenarios. This paper explores a new scenario of Re-identification, which\ndiffers in perspective, background, and pose(walking or cycling). Obviously,\nordinary ReID processing methods cannot handle this scenario well. As we all\nknow, the best way to deal with that it is to introduce image datasets in this\nscanario, But this one is very expensive.\n  To solve this problem, this paper proposes a simple and effective way to\ngenerate images in some new scenario, which is named Copy and Paste method\nbased on Pose(CPP). The CPP is a method based on key point detection, using\ncopy and paste, to composite a new semantic image dataset in two different\nsemantic image datasets. Such as, we can use pedestrians and bicycles to\ngenerate some images that shows the same person rides on different bicycles.\nThe CPP is suitable for ReID tasks in new scenarios and it outperforms\nstate-of-the-art on the original datasets in original ReID tasks. Specifically,\nit can also have better generalization performance for third-party public\ndatasets. Code and Datasets which composited by the CPP will be available in\nthe future.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 06:51:34 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 15:47:00 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yang", "Cheng", ""]]}, {"id": "2107.10480", "submitter": "Gihyuk Ko", "authors": "Gihyuk Ko, Gyumin Lim", "title": "Unsupervised Detection of Adversarial Examples with Model Explanations", "comments": "AdvML@KDD'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) have shown remarkable performance in a diverse\nrange of machine learning applications. However, it is widely known that DNNs\nare vulnerable to simple adversarial perturbations, which causes the model to\nincorrectly classify inputs. In this paper, we propose a simple yet effective\nmethod to detect adversarial examples, using methods developed to explain the\nmodel's behavior. Our key observation is that adding small, humanly\nimperceptible perturbations can lead to drastic changes in the model\nexplanations, resulting in unusual or irregular forms of explanations. From\nthis insight, we propose an unsupervised detection of adversarial examples\nusing reconstructor networks trained only on model explanations of benign\nexamples. Our evaluations with MNIST handwritten dataset show that our method\nis capable of detecting adversarial examples generated by the state-of-the-art\nalgorithms with high confidence. To the best of our knowledge, this work is the\nfirst in suggesting unsupervised defense method using model explanations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 06:54:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ko", "Gihyuk", ""], ["Lim", "Gyumin", ""]]}, {"id": "2107.10493", "submitter": "Sihyun Yu", "authors": "Sihyun Yu, Sangwoo Mo, Sungsoo Ahn, Jinwoo Shin", "title": "Abstract Reasoning via Logic-guided Generation", "comments": "ICML 2021 Workshop on Self-Supervised Learning for Reasoning and\n  Perception (Spotlight Talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract reasoning, i.e., inferring complicated patterns from given\nobservations, is a central building block of artificial general intelligence.\nWhile humans find the answer by either eliminating wrong candidates or first\nconstructing the answer, prior deep neural network (DNN)-based methods focus on\nthe former discriminative approach. This paper aims to design a framework for\nthe latter approach and bridge the gap between artificial and human\nintelligence. To this end, we propose logic-guided generation (LoGe), a novel\ngenerative DNN framework that reduces abstract reasoning as an optimization\nproblem in propositional logic. LoGe is composed of three steps: extract\npropositional variables from images, reason the answer variables with a logic\nlayer, and reconstruct the answer image from the variables. We demonstrate that\nLoGe outperforms the black box DNN frameworks for generative abstract reasoning\nunder the RAVEN benchmark, i.e., reconstructing answers based on capturing\ncorrect rules of various attributes from observations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 07:28:24 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Yu", "Sihyun", ""], ["Mo", "Sangwoo", ""], ["Ahn", "Sungsoo", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2107.10504", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge, Christopher D. Toole, Joseph A. Maestri, and Jose C.\n  Principe", "title": "External-Memory Networks for Low-Shot Learning of Targets in\n  Forward-Looking-Sonar Imagery", "comments": "Submitted to IEEE Journal of Oceanic Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a memory-based framework for real-time, data-efficient target\nanalysis in forward-looking-sonar (FLS) imagery. Our framework relies on first\nremoving non-discriminative details from the imagery using a small-scale\nDenseNet-inspired network. Doing so simplifies ensuing analyses and permits\ngeneralizing from few labeled examples. We then cascade the filtered imagery\ninto a novel NeuralRAM-based convolutional matching network, NRMN, for low-shot\ntarget recognition. We employ a small-scale FlowNet, LFN to align and register\nFLS imagery across local temporal scales. LFN enables target label consensus\nvoting across images and generally improves target detection and recognition\nrates.\n  We evaluate our framework using real-world FLS imagery with multiple broad\ntarget classes that have high intra-class variability and rich sub-class\nstructure. We show that few-shot learning, with anywhere from ten to thirty\nclass-specific exemplars, performs similarly to supervised deep networks\ntrained on hundreds of samples per class. Effective zero-shot learning is also\npossible. High performance is realized from the inductive-transfer properties\nof NRMNs when distractor elements are removed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 07:50:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Toole", "Christopher D.", ""], ["Maestri", "Joseph A.", ""], ["Principe", "Jose C.", ""]]}, {"id": "2107.10524", "submitter": "Takashi Shibata", "authors": "Takashi Shibata, Masayuki Tanaka, Masatoshi Okutomi", "title": "Geometric Data Augmentation Based on Feature Map Ensemble", "comments": "Accepted to ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have become the mainstream in computer vision\napplications. Although CNNs have been successful in many computer vision tasks,\nit is not free from drawbacks. The performance of CNN is dramatically degraded\nby geometric transformation, such as large rotations. In this paper, we propose\na novel CNN architecture that can improve the robustness against geometric\ntransformations without modifying the existing backbones of their CNNs. The key\nis to enclose the existing backbone with a geometric transformation (and the\ncorresponding reverse transformation) and a feature map ensemble. The proposed\nmethod can inherit the strengths of existing CNNs that have been presented so\nfar. Furthermore, the proposed method can be employed in combination with\nstate-of-the-art data augmentation algorithms to improve their performance. We\ndemonstrate the effectiveness of the proposed method using standard datasets\nsuch as CIFAR, CUB-200, and Mnist-rot-12k.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 08:48:54 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Shibata", "Takashi", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2107.10563", "submitter": "Kelvin Chelli", "authors": "Thorsten Herfet, Kelvin Chelli, Tobias Lange and Robin Kremer", "title": "Fristograms: Revealing and Exploiting Light Field Internals", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, light field (LF) capture and processing has become an\nintegral part of media production. The richness of information available in LFs\nhas enabled novel applications like post-capture depth-of-field editing, 3D\nreconstruction, segmentation and matting, saliency detection, object detection\nand recognition, and mixed reality. The efficacy of such applications depends\non certain underlying requirements, which are often ignored. For example, some\noperations such as noise-reduction, or hyperfan-filtering are only possible if\na scene point Lambertian radiator. Some other operations such as the removal of\nobstacles or looking behind objects are only possible if there is at least one\nray capturing the required scene point. Consequently, the ray distribution\nrepresenting a certain scene point is an important characteristic for\nevaluating processing possibilities. The primary idea in this paper is to\nestablish a relation between the capturing setup and the rays of the LF. To\nthis end, we discretize the view frustum. Traditionally, a uniform\ndiscretization of the view frustum results in voxels that represents a single\nsample on a regularly spaced, 3-D grid. Instead, we use frustum-shaped voxels\n(froxels), by using depth and capturing-setup dependent discretization of the\nview frustum. Based on such discretization, we count the number of rays mapping\nto the same pixel on the capturing device(s). By means of this count, we\npropose histograms of ray-counts over the froxels (fristograms). Fristograms\ncan be used as a tool to analyze and reveal interesting aspects of the\nunderlying LF, like the number of rays originating from a scene point and the\ncolor distribution of these rays. As an example, we show its ability by\nsignificantly reducing the number of rays which enables noise reduction while\nmaintaining the realistic rendering of non-Lambertian or partially occluded\nregions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:33:13 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Herfet", "Thorsten", ""], ["Chelli", "Kelvin", ""], ["Lange", "Tobias", ""], ["Kremer", "Robin", ""]]}, {"id": "2107.10607", "submitter": "Moritz Ibing", "authors": "Moritz Ibing, Isaak Lim, Leif Kobbelt", "title": "3D Shape Generation with Grid-based Implicit Functions", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches to generate shapes in a 3D setting train a GAN on the\nlatent space of an autoencoder (AE). Even though this produces convincing\nresults, it has two major shortcomings. As the GAN is limited to reproduce the\ndataset the AE was trained on, we cannot reuse a trained AE for novel data.\nFurthermore, it is difficult to add spatial supervision into the generation\nprocess, as the AE only gives us a global representation. To remedy these\nissues, we propose to train the GAN on grids (i.e. each cell covers a part of a\nshape). In this representation each cell is equipped with a latent vector\nprovided by an AE. This localized representation enables more expressiveness\n(since the cell-based latent vectors can be combined in novel ways) as well as\nspatial control of the generation process (e.g. via bounding boxes). Our method\noutperforms the current state of the art on all established evaluation\nmeasures, proposed for quantitatively evaluating the generative capabilities of\nGANs. We show limitations of these measures and propose the adaptation of a\nrobust criterion from statistical analysis as an alternative.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 12:23:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ibing", "Moritz", ""], ["Lim", "Isaak", ""], ["Kobbelt", "Leif", ""]]}, {"id": "2107.10624", "submitter": "Pavlo Molchanov", "authors": "Pavlo Molchanov and Jimmy Hall and Hongxu Yin and Jan Kautz and Nicolo\n  Fusi and Arash Vahdat", "title": "HANT: Hardware-Aware Network Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a trained network, how can we accelerate it to meet efficiency needs\nfor deployment on particular hardware? The commonly used hardware-aware network\ncompression techniques address this question with pruning, kernel fusion,\nquantization and lowering precision. However, these approaches do not change\nthe underlying network operations. In this paper, we propose hardware-aware\nnetwork transformation (HANT), which accelerates a network by replacing\ninefficient operations with more efficient alternatives using a neural\narchitecture search like approach. HANT tackles the problem in two phase: In\nthe first phase, a large number of alternative operations per every layer of\nthe teacher model is trained using layer-wise feature map distillation. In the\nsecond phase, the combinatorial selection of efficient operations is relaxed to\nan integer optimization problem that can be solved in a few seconds. We extend\nHANT with kernel fusion and quantization to improve throughput even further.\nOur experimental results on accelerating the EfficientNet family show that HANT\ncan accelerate them by up to 3.6x with <0.4% drop in the top-1 accuracy on the\nImageNet dataset. When comparing the same latency level, HANT can accelerate\nEfficientNet-B4 to the same latency as EfficientNet-B1 while having 3% higher\naccuracy. We examine a large pool of operations, up to 197 per layer, and we\nprovide insights into the selected operations and final architectures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:46:34 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Molchanov", "Pavlo", ""], ["Hall", "Jimmy", ""], ["Yin", "Hongxu", ""], ["Kautz", "Jan", ""], ["Fusi", "Nicolo", ""], ["Vahdat", "Arash", ""]]}, {"id": "2107.10628", "submitter": "Ke-Yue Zhang", "authors": "Ke-Yue Zhang, Taiping Yao, Jian Zhang, Shice Liu, Bangjie Yin,\n  Shouhong Ding, Jilin Li", "title": "Structure Destruction and Content Combination for Face Anti-Spoofing", "comments": "Accepted by IJCB2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pursuit of consolidating the face verification systems, prior face\nanti-spoofing studies excavate the hidden cues in original images to\ndiscriminate real persons and diverse attack types with the assistance of\nauxiliary supervision. However, limited by the following two inherent\ndisturbances in their training process: 1) Complete facial structure in a\nsingle image. 2) Implicit subdomains in the whole dataset, these methods are\nprone to stick on memorization of the entire training dataset and show\nsensitivity to nonhomologous domain distribution. In this paper, we propose\nStructure Destruction Module and Content Combination Module to address these\ntwo imitations separately. The former mechanism destroys images into patches to\nconstruct a non-structural input, while the latter mechanism recombines patches\nfrom different subdomains or classes into a mixup construct. Based on this\nsplitting-and-splicing operation, Local Relation Modeling Module is further\nproposed to model the second-order relationship between patches. We evaluate\nour method on extensive public datasets and promising experimental results to\ndemonstrate the reliability of our method against state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:08:46 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhang", "Ke-Yue", ""], ["Yao", "Taiping", ""], ["Zhang", "Jian", ""], ["Liu", "Shice", ""], ["Yin", "Bangjie", ""], ["Ding", "Shouhong", ""], ["Li", "Jilin", ""]]}, {"id": "2107.10638", "submitter": "Songuel Polat", "authors": "Songuel Polat, Alain Tremeau, Frank Boochs", "title": "Rule-Based Classification of Hyperspectral Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its high spatial and spectral information content, hyperspectral\nimaging opens up new possibilities for a better understanding of data and\nscenes in a wide variety of applications. An essential part of this process of\nunderstanding is the classification part. In this article we present a general\nclassification approach based on the shape of spectral signatures. In contrast\nto classical classification approaches (e.g. SVM, KNN), not only reflectance\nvalues are considered, but also parameters such as curvature points, curvature\nvalues, and the curvature behavior of spectral signatures are used to develop\nshape-describing rules in order to use them for classification by a rule-based\nprocedure using IF-THEN queries. The flexibility and efficiency of the\nmethodology is demonstrated using datasets from two different application\nfields and leads to convincing results with good performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 10:11:41 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Polat", "Songuel", ""], ["Tremeau", "Alain", ""], ["Boochs", "Frank", ""]]}, {"id": "2107.10685", "submitter": "Ben Saunders", "authors": "Ben Saunders, Necati Cihan Camgoz, Richard Bowden", "title": "AnonySIGN: Novel Human Appearance Synthesis for Sign Language Video\n  Anonymisation", "comments": null, "journal-ref": "Face and Gesture Conference 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual anonymisation of sign language data is an essential task to\naddress privacy concerns raised by large-scale dataset collection. Previous\nanonymisation techniques have either significantly affected sign comprehension\nor required manual, labour-intensive work.\n  In this paper, we formally introduce the task of Sign Language Video\nAnonymisation (SLVA) as an automatic method to anonymise the visual appearance\nof a sign language video whilst retaining the meaning of the original sign\nlanguage sequence. To tackle SLVA, we propose AnonySign, a novel automatic\napproach for visual anonymisation of sign language data. We first extract pose\ninformation from the source video to remove the original signer appearance. We\nnext generate a photo-realistic sign language video of a novel appearance from\nthe pose sequence, using image-to-image translation methods in a conditional\nvariational autoencoder framework. An approximate posterior style distribution\nis learnt, which can be sampled from to synthesise novel human appearances. In\naddition, we propose a novel \\textit{style loss} that ensures style consistency\nin the anonymised sign language videos.\n  We evaluate AnonySign for the SLVA task with extensive quantitative and\nqualitative experiments highlighting both realism and anonymity of our novel\nhuman appearance synthesis. In addition, we formalise an anonymity perceptual\nstudy as an evaluation criteria for the SLVA task and showcase that video\nanonymisation using AnonySign retains the original sign language content.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:42:18 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 16:10:18 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2107.10712", "submitter": "Xiaofeng Liu", "authors": "Wanqing Xie, Lizhong Liang, Yao Lu, Hui Luo, Xiaofeng Liu", "title": "Deep 3D-CNN for Depression Diagnosis with Facial Video Recording of\n  Self-Rating Depression Scale Questionnaire", "comments": "43rd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Self-Rating Depression Scale (SDS) questionnaire is commonly utilized for\neffective depression preliminary screening. The uncontrolled self-administered\nmeasure, on the other hand, maybe readily influenced by insouciant or dishonest\nresponses, yielding different findings from the clinician-administered\ndiagnostic. Facial expression (FE) and behaviors are important in\nclinician-administered assessments, but they are underappreciated in\nself-administered evaluations. We use a new dataset of 200 participants to\ndemonstrate the validity of self-rating questionnaires and their accompanying\nquestion-by-question video recordings in this study. We offer an end-to-end\nsystem to handle the face video recording that is conditioned on the\nquestionnaire answers and the responding time to automatically interpret\nsadness from the SDS assessment and the associated video. We modified a 3D-CNN\nfor temporal feature extraction and compared various state-of-the-art temporal\nmodeling techniques. The superior performance of our system shows the validity\nof combining facial video recording with the SDS score for more accurate\nself-diagnose.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 14:37:00 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Xie", "Wanqing", ""], ["Liang", "Lizhong", ""], ["Lu", "Yao", ""], ["Luo", "Hui", ""], ["Liu", "Xiaofeng", ""]]}, {"id": "2107.10718", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Hanna K. Gaggin, Weichung Wang, C.-C. Jay\n  Kuo, Georges El Fakhri, Jonghye Woo", "title": "Segmentation of Cardiac Structures via Successive Subspace Learning with\n  Saab Transform from Cine MRI", "comments": "43rd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessment of cardiovascular disease (CVD) with cine magnetic resonance\nimaging (MRI) has been used to non-invasively evaluate detailed cardiac\nstructure and function. Accurate segmentation of cardiac structures from cine\nMRI is a crucial step for early diagnosis and prognosis of CVD, and has been\ngreatly improved with convolutional neural networks (CNN). There, however, are\na number of limitations identified in CNN models, such as limited\ninterpretability and high complexity, thus limiting their use in clinical\npractice. In this work, to address the limitations, we propose a lightweight\nand interpretable machine learning model, successive subspace learning with the\nsubspace approximation with adjusted bias (Saab) transform, for accurate and\nefficient segmentation from cine MRI. Specifically, our segmentation framework\nis comprised of the following steps: (1) sequential expansion of near-to-far\nneighborhood at different resolutions; (2) channel-wise subspace approximation\nusing the Saab transform for unsupervised dimension reduction; (3) class-wise\nentropy guided feature selection for supervised dimension reduction; (4)\nconcatenation of features and pixel-wise classification with gradient boost;\nand (5) conditional random field for post-processing. Experimental results on\nthe ACDC 2017 segmentation database, showed that our framework performed better\nthan state-of-the-art U-Net models with 200$\\times$ fewer parameters in\ndelineating the left ventricle, right ventricle, and myocardium, thus showing\nits potential to be used in clinical practice.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 14:50:48 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Gaggin", "Hanna K.", ""], ["Wang", "Weichung", ""], ["Kuo", "C. -C. Jay", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2107.10756", "submitter": "Manan Oza", "authors": "Manan Oza, Sukalpa Chanda and David Doermann", "title": "Semantic Text-to-Face GAN -ST^2FG", "comments": "arXiv admin note: text overlap with arXiv:2010.12136 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Faces generated using generative adversarial networks (GANs) have reached\nunprecedented realism. These faces, also known as \"Deep Fakes\", appear as\nrealistic photographs with very little pixel-level distortions. While some work\nhas enabled the training of models that lead to the generation of specific\nproperties of the subject, generating a facial image based on a natural\nlanguage description has not been fully explored. For security and criminal\nidentification, the ability to provide a GAN-based system that works like a\nsketch artist would be incredibly useful. In this paper, we present a novel\napproach to generate facial images from semantic text descriptions. The learned\nmodel is provided with a text description and an outline of the type of face,\nwhich the model uses to sketch the features. Our models are trained using an\nAffine Combination Module (ACM) mechanism to combine the text embedding from\nBERT and the GAN latent space using a self-attention matrix. This avoids the\nloss of features due to inadequate \"attention\", which may happen if text\nembedding and latent vector are simply concatenated. Our approach is capable of\ngenerating images that are very accurately aligned to the exhaustive textual\ndescriptions of faces with many fine detail features of the face and helps in\ngenerating better images. The proposed method is also capable of making\nincremental changes to a previously generated image if it is provided with\nadditional textual descriptions or sentences.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:42:25 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Oza", "Manan", ""], ["Chanda", "Sukalpa", ""], ["Doermann", "David", ""]]}, {"id": "2107.10771", "submitter": "Yuan Tian", "authors": "Yuan Tian, Yichao Yan, Xiongkuo Min, Guo Lu, Guangtao Zhai, Guodong\n  Guo, and Zhiyong Gao", "title": "EAN: Event Adaptive Network for Enhanced Action Recognition", "comments": "Submitted to TIP. Codes are available at:\n  https://github.com/tianyuan168326/EAN-Pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently modeling spatial-temporal information in videos is crucial for\naction recognition. To achieve this goal, state-of-the-art methods typically\nemploy the convolution operator and the dense interaction modules such as\nnon-local blocks. However, these methods cannot accurately fit the diverse\nevents in videos. On the one hand, the adopted convolutions are with fixed\nscales, thus struggling with events of various scales. On the other hand, the\ndense interaction modeling paradigm only achieves sub-optimal performance as\naction-irrelevant parts bring additional noises for the final prediction. In\nthis paper, we propose a unified action recognition framework to investigate\nthe dynamic nature of video content by introducing the following designs.\nFirst, when extracting local cues, we generate the spatial-temporal kernels of\ndynamic-scale to adaptively fit the diverse events. Second, to accurately\naggregate these cues into a global video representation, we propose to mine the\ninteractions only among a few selected foreground objects by a Transformer,\nwhich yields a sparse paradigm. We call the proposed framework as Event\nAdaptive Network (EAN) because both key designs are adaptive to the input video\ncontent. To exploit the short-term motions within local segments, we propose a\nnovel and efficient Latent Motion Code (LMC) module, further improving the\nperformance of the framework. Extensive experiments on several large-scale\nvideo datasets, e.g., Something-to-Something V1&V2, Kinetics, and Diving48,\nverify that our models achieve state-of-the-art or competitive performances at\nlow FLOPs. Codes are available at:\nhttps://github.com/tianyuan168326/EAN-Pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:57:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Tian", "Yuan", ""], ["Yan", "Yichao", ""], ["Min", "Xiongkuo", ""], ["Lu", "Guo", ""], ["Zhai", "Guangtao", ""], ["Guo", "Guodong", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2107.10806", "submitter": "Alvaro Fernandez-Quilez", "authors": "Alvaro Fernandez-Quilez, Trygve Eftest{\\o}l, Morten Goodwin, Svein\n  Reidar Kjosavik, Ketil Oppedal", "title": "Self-transfer learning via patches: A prostate cancer triage approach\n  based on bi-parametric MRI", "comments": "13 pages. Article under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer (PCa) is the second most common cancer diagnosed among men\nworldwide. The current PCa diagnostic pathway comes at the cost of substantial\noverdiagnosis, leading to unnecessary treatment and further testing.\nBi-parametric magnetic resonance imaging (bp-MRI) based on apparent diffusion\ncoefficient maps (ADC) and T2-weighted (T2w) sequences has been proposed as a\ntriage test to differentiate between clinically significant (cS) and\nnon-clinically significant (ncS) prostate lesions. However, analysis of the\nsequences relies on expertise, requires specialized training, and suffers from\ninter-observer variability. Deep learning (DL) techniques hold promise in tasks\nsuch as classification and detection. Nevertheless, they rely on large amounts\nof annotated data which is not common in the medical field. In order to\npalliate such issues, existing works rely on transfer learning (TL) and\nImageNet pre-training, which has been proven to be sub-optimal for the medical\nimaging domain. In this paper, we present a patch-based pre-training strategy\nto distinguish between cS and ncS lesions which exploit the region of interest\n(ROI) of the patched source domain to efficiently train a classifier in the\nfull-slice target domain which does not require annotations by making use of\ntransfer learning (TL). We provide a comprehensive comparison between several\nCNNs architectures and different settings which are presented as a baseline.\nMoreover, we explore cross-domain TL which exploits both MRI modalities and\nimproves single modality results. Finally, we show how our approaches\noutperform the standard approaches by a considerable margin\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:02:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fernandez-Quilez", "Alvaro", ""], ["Eftest\u00f8l", "Trygve", ""], ["Goodwin", "Morten", ""], ["Kjosavik", "Svein Reidar", ""], ["Oppedal", "Ketil", ""]]}, {"id": "2107.10833", "submitter": "Xintao Wang", "authors": "Xintao Wang, Liangbin Xie, Chao Dong, Ying Shan", "title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure\n  Synthetic Data", "comments": "Tech Report. Training codes, testing codes, and executable files are\n  in https://github.com/xinntao/Real-ESRGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though many attempts have been made in blind super-resolution to restore\nlow-resolution images with unknown and complex degradations, they are still far\nfrom addressing general real-world degraded images. In this work, we extend the\npowerful ESRGAN to a practical restoration application (namely, Real-ESRGAN),\nwhich is trained with pure synthetic data. Specifically, a high-order\ndegradation modeling process is introduced to better simulate complex\nreal-world degradations. We also consider the common ringing and overshoot\nartifacts in the synthesis process. In addition, we employ a U-Net\ndiscriminator with spectral normalization to increase discriminator capability\nand stabilize the training dynamics. Extensive comparisons have shown its\nsuperior visual performance than prior works on various real datasets. We also\nprovide efficient implementations to synthesize training pairs on the fly.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:43:24 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Xintao", ""], ["Xie", "Liangbin", ""], ["Dong", "Chao", ""], ["Shan", "Ying", ""]]}, {"id": "2107.10834", "submitter": "Shilong Liu", "authors": "Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, Jun Zhu", "title": "Query2Label: A Simple Transformer Way to Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple and effective approach to solving the\nmulti-label classification problem. The proposed approach leverages Transformer\ndecoders to query the existence of a class label. The use of Transformer is\nrooted in the need of extracting local discriminative features adaptively for\ndifferent labels, which is a strongly desired property due to the existence of\nmultiple objects in one image. The built-in cross-attention module in the\nTransformer decoder offers an effective way to use label embeddings as queries\nto probe and pool class-related features from a feature map computed by a\nvision backbone for subsequent binary classifications. Compared with prior\nworks, the new framework is simple, using standard Transformers and vision\nbackbones, and effective, consistently outperforming all previous works on five\nmulti-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE,\nand Visual Genome. Particularly, we establish $91.3\\%$ mAP on MS-COCO. We hope\nits compact structure, simple implementation, and superior performance serve as\na strong baseline for multi-label classification tasks and future studies. The\ncode will be available soon at https://github.com/SlongLiu/query2labels.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:49:25 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Liu", "Shilong", ""], ["Zhang", "Lei", ""], ["Yang", "Xiao", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2107.10844", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu, Tomas Jakab, Christian Rupprecht, Andrea Vedaldi", "title": "DOVE: Learning Deformable 3D Objects by Watching Videos", "comments": "Project Page: https://dove3d.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning deformable 3D objects from 2D images is an extremely ill-posed\nproblem. Existing methods rely on explicit supervision to establish multi-view\ncorrespondences, such as template shape models and keypoint annotations, which\nrestricts their applicability on objects \"in the wild\". In this paper, we\npropose to use monocular videos, which naturally provide correspondences across\ntime, allowing us to learn 3D shapes of deformable object categories without\nexplicit keypoints or template shapes. Specifically, we present DOVE, which\nlearns to predict 3D canonical shape, deformation, viewpoint and texture from a\nsingle 2D image of a bird, given a bird video collection as well as\nautomatically obtained silhouettes and optical flows as training data. Our\nmethod reconstructs temporally consistent 3D shape and deformation, which\nallows us to animate and re-render the bird from arbitrary viewpoints from a\nsingle image.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:58:10 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wu", "Shangzhe", ""], ["Jakab", "Tomas", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2107.10873", "submitter": "Linyi Li", "authors": "Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, Bo Li", "title": "On the Certified Robustness for Ensemble Models and Beyond", "comments": "57 pages, 11 pages for main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent studies show that deep neural networks (DNN) are vulnerable to\nadversarial examples, which aim to mislead DNNs by adding perturbations with\nsmall magnitude. To defend against such attacks, both empirical and theoretical\ndefense approaches have been extensively studied for a single ML model. In this\nwork, we aim to analyze and provide the certified robustness for ensemble ML\nmodels, together with the sufficient and necessary conditions of robustness for\ndifferent ensemble protocols. Although ensemble models are shown more robust\nthan a single model empirically; surprisingly, we find that in terms of the\ncertified robustness the standard ensemble models only achieve marginal\nimprovement compared to a single model. Thus, to explore the conditions that\nguarantee to provide certifiably robust ensemble ML models, we first prove that\ndiversified gradient and large confidence margin are sufficient and necessary\nconditions for certifiably robust ensemble models under the model-smoothness\nassumption. We then provide the bounded model-smoothness analysis based on the\nproposed Ensemble-before-Smoothing strategy. We also prove that an ensemble\nmodel can always achieve higher certified robustness than a single base model\nunder mild conditions. Inspired by the theoretical findings, we propose the\nlightweight Diversity Regularized Training (DRT) to train certifiably robust\nensemble ML models. Extensive experiments show that our DRT enhanced ensembles\ncan consistently achieve higher certified robustness than existing single and\nensemble ML models, demonstrating the state-of-the-art certified L2-robustness\non MNIST, CIFAR-10, and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 18:10:41 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yang", "Zhuolin", ""], ["Li", "Linyi", ""], ["Xu", "Xiaojun", ""], ["Kailkhura", "Bhavya", ""], ["Xie", "Tao", ""], ["Li", "Bo", ""]]}, {"id": "2107.10894", "submitter": "Michael Mommert", "authors": "Michael Mommert, Linus Scheibenreif, Jo\\\"elle Hanna, Damian Borth", "title": "Power Plant Classification from Remote Imaging with Deep Learning", "comments": "Presented at the 2021 IEEE International Geoscience and Remote\n  Sensing Symposium (IGARSS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite remote imaging enables the detailed study of land use patterns on a\nglobal scale. We investigate the possibility to improve the information content\nof traditional land use classification by identifying the nature of industrial\nsites from medium-resolution remote sensing images. In this work, we focus on\nclassifying different types of power plants from Sentinel-2 imaging data. Using\na ResNet-50 deep learning model, we are able to achieve a mean accuracy of\n90.0% in distinguishing 10 different power plant types and a background class.\nFurthermore, we are able to identify the cooling mechanisms utilized in thermal\npower plants with a mean accuracy of 87.5%. Our results enable us to\nqualitatively investigate the energy mix from Sentinel-2 imaging data, and\nprove the feasibility to classify industrial sites on a global scale from\nfreely available satellite imagery.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 19:36:07 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Mommert", "Michael", ""], ["Scheibenreif", "Linus", ""], ["Hanna", "Jo\u00eblle", ""], ["Borth", "Damian", ""]]}, {"id": "2107.10895", "submitter": "Arnav Malawade", "authors": "Arnav Malawade, Mohanad Odema, Sebastien Lajeunesse-DeGroot, Mohammad\n  Abdullah Al Faruque", "title": "SAGE: A Split-Architecture Methodology for Efficient End-to-End\n  Autonomous Vehicle Control", "comments": "This article appears as part of the ESWEEK-TECS special issue and was\n  presented in the International Conference on Hardware/Software Codesign and\n  System Synthesis (CODES+ISSS), 2021", "journal-ref": null, "doi": "10.1145/3477006", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AV) are expected to revolutionize transportation and\nimprove road safety significantly. However, these benefits do not come without\ncost; AVs require large Deep-Learning (DL) models and powerful hardware\nplatforms to operate reliably in real-time, requiring between several hundred\nwatts to one kilowatt of power. This power consumption can dramatically reduce\nvehicles' driving range and affect emissions. To address this problem, we\npropose SAGE: a methodology for selectively offloading the key energy-consuming\nmodules of DL architectures to the cloud to optimize edge energy usage while\nmeeting real-time latency constraints. Furthermore, we leverage Head Network\nDistillation (HND) to introduce efficient bottlenecks within the DL\narchitecture in order to minimize the network overhead costs of offloading with\nalmost no degradation in the model's performance. We evaluate SAGE using an\nNvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge\ndevices and demonstrate that our offloading strategy is practical for a wide\nrange of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi\ntechnologies. Compared to edge-only computation, SAGE reduces energy\nconsumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one\nlow-resolution camera, one high-resolution camera, and three high-resolution\ncameras, respectively. SAGE also reduces upload data size by up to 98.40%\ncompared to direct camera offloading.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 19:40:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Malawade", "Arnav", ""], ["Odema", "Mohanad", ""], ["Lajeunesse-DeGroot", "Sebastien", ""], ["Faruque", "Mohammad Abdullah Al", ""]]}, {"id": "2107.10898", "submitter": "Max Coenen", "authors": "Max Coenen and Franz Rottensteiner", "title": "Pose Estimation and 3D Reconstruction of Vehicles from Stereo-Images\n  Using a Subcategory-Aware Shape Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The 3D reconstruction of objects is a prerequisite for many highly relevant\napplications of computer vision such as mobile robotics or autonomous driving.\nTo deal with the inverse problem of reconstructing 3D objects from their 2D\nprojections, a common strategy is to incorporate prior object knowledge into\nthe reconstruction approach by establishing a 3D model and aligning it to the\n2D image plane. However, current approaches are limited due to inadequate shape\npriors and the insufficiency of the derived image observations for a reliable\nalignment with the 3D model. The goal of this paper is to show how 3D object\nreconstruction can profit from a more sophisticated shape prior and from a\ncombined incorporation of different observation types inferred from the images.\nWe introduce a subcategory-aware deformable vehicle model that makes use of a\nprediction of the vehicle type for a more appropriate regularisation of the\nvehicle shape. A multi-branch CNN is presented to derive predictions of the\nvehicle type and orientation. This information is also introduced as prior\ninformation for model fitting. Furthermore, the CNN extracts vehicle keypoints\nand wireframes, which are well-suited for model-to-image association and model\nfitting. The task of pose estimation and reconstruction is addressed by a\nversatile probabilistic model. Extensive experiments are conducted using two\nchallenging real-world data sets on both of which the benefit of the developed\nshape prior can be shown. A comparison to state-of-the-art methods for vehicle\npose estimation shows that the proposed approach performs on par or better,\nconfirming the suitability of the developed shape prior and probabilistic model\nfor vehicle reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 19:47:49 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Coenen", "Max", ""], ["Rottensteiner", "Franz", ""]]}, {"id": "2107.10912", "submitter": "Bas van der Velden", "authors": "Bas H.M. van der Velden, Hugo J. Kuijf, Kenneth G.A. Gilhuijs, Max A.\n  Viergever", "title": "Explainable artificial intelligence (XAI) in deep learning-based medical\n  image analysis", "comments": "Submitted for publication. Comments welcome by email to first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With an increase in deep learning-based methods, the call for explainability\nof such methods grows, especially in high-stakes decision making areas such as\nmedical image analysis. This survey presents an overview of eXplainable\nArtificial Intelligence (XAI) used in deep learning-based medical image\nanalysis. A framework of XAI criteria is introduced to classify deep\nlearning-based medical image analysis methods. Papers on XAI techniques in\nmedical image analysis are then surveyed and categorized according to the\nframework and according to anatomical location. The paper concludes with an\noutlook of future opportunities for XAI in medical image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:16:34 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["van der Velden", "Bas H. M.", ""], ["Kuijf", "Hugo J.", ""], ["Gilhuijs", "Kenneth G. A.", ""], ["Viergever", "Max A.", ""]]}, {"id": "2107.10931", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Bo Hu, Linghao Jin, Xu Han, Fangxu Xing, Jinsong Ouyang,\n  Jun Lu, Georges EL Fakhri, Jonghye Woo", "title": "Domain Generalization under Conditional and Label Shifts via Variational\n  Bayesian Inference", "comments": "30th International Joint Conference on Artificial Intelligence\n  (IJCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a domain generalization (DG) approach to learn on\nseveral labeled source domains and transfer knowledge to a target domain that\nis inaccessible in training. Considering the inherent conditional and label\nshifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the\nwidely used domain invariant feature learning (IFL) methods relies on aligning\nthe marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic\nassumption that $p(y)$ is invariant across domains. We thereby propose a novel\nvariational Bayesian inference framework to enforce the conditional\ndistribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a\nlatent space, which also takes the marginal label shift w.r.t. $p(y)$ into\nconsideration with the posterior alignment. Extensive experiments on various\nbenchmarks demonstrate that our framework is robust to the label shift and the\ncross-domain accuracy is significantly improved, thereby achieving superior\nperformance over the conventional IFL counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 21:19:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Hu", "Bo", ""], ["Jin", "Linghao", ""], ["Han", "Xu", ""], ["Xing", "Fangxu", ""], ["Ouyang", "Jinsong", ""], ["Lu", "Jun", ""], ["Fakhri", "Georges EL", ""], ["Woo", "Jonghye", ""]]}, {"id": "2107.10950", "submitter": "Henry Nelson", "authors": "Henry J. Nelson and Nikolaos Papanikolopoulos", "title": "Pre-Clustering Point Clouds of Crop Fields Using Scalable Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to apply the recent successes of automated plant phenotyping and\nmachine learning on a large scale, efficient and general algorithms must be\ndesigned to intelligently split crop fields into small, yet actionable,\nportions that can then be processed by more complex algorithms. In this paper\nwe notice a similarity between the current state-of-the-art for this problem\nand a commonly used density-based clustering algorithm, Quickshift. Exploiting\nthis similarity we propose a number of novel, application specific algorithms\nwith the goal of producing a general and scalable plant segmentation algorithm.\nThe novel algorithms proposed in this work are shown to produce quantitatively\nbetter results than the current state-of-the-art while being less sensitive to\ninput parameters and maintaining the same algorithmic time complexity. When\nincorporated into field-scale phenotyping systems, the proposed algorithms\nshould work as a drop in replacement that can greatly improve the accuracy of\nresults while ensuring that performance and scalability remain undiminished.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 22:47:22 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nelson", "Henry J.", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "2107.10963", "submitter": "Andrey Zhmoginov", "authors": "Andrey Zhmoginov, Dina Bashkirova and Mark Sandler", "title": "Compositional Models: Multi-Task Learning and Knowledge Transfer with\n  Modular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conditional computation and modular networks have been recently proposed for\nmultitask learning and other problems as a way to decompose problem solving\ninto multiple reusable computational blocks. We propose a new approach for\nlearning modular networks based on the isometric version of ResNet with all\nresidual blocks having the same configuration and the same number of\nparameters. This architectural choice allows adding, removing and changing the\norder of residual blocks. In our method, the modules can be invoked repeatedly\nand allow knowledge transfer to novel tasks by adjusting the order of\ncomputation. This allows soft weight sharing between tasks with only a small\nincrease in the number of parameters. We show that our method leads to\ninterpretable self-organization of modules in case of multi-task learning,\ntransfer learning and domain adaptation while achieving competitive results on\nthose tasks. From practical perspective, our approach allows to: (a) reuse\nexisting modules for learning new task by adjusting the computation order, (b)\nuse it for unsupervised multi-source domain adaptation to illustrate that\nadaptation to unseen data can be achieved by only manipulating the order of\npretrained modules, (c) show how our approach can be used to increase accuracy\nof existing architectures for image classification tasks such as ImageNet,\nwithout any parameter increase, by reusing the same block multiple times.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 00:05:55 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Zhmoginov", "Andrey", ""], ["Bashkirova", "Dina", ""], ["Sandler", "Mark", ""]]}, {"id": "2107.10981", "submitter": "Shitong Luo", "authors": "Shitong Luo, Wei Hu", "title": "Score-Based Point Cloud Denoising", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds acquired from scanning devices are often perturbed by noise,\nwhich affects downstream tasks such as surface reconstruction and analysis. The\ndistribution of a noisy point cloud can be viewed as the distribution of a set\nof noise-free samples $p(x)$ convolved with some noise model $n$, leading to\n$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy\npoint cloud, we propose to increase the log-likelihood of each point from $p *\nn$ via gradient ascent -- iteratively updating each point's position. Since $p\n* n$ is unknown at test-time, and we only need the score (i.e., the gradient of\nthe log-probability function) to perform gradient ascent, we propose a neural\nnetwork architecture to estimate the score of $p * n$ given only noisy point\nclouds as input. We derive objective functions for training the network and\ndevelop a denoising algorithm leveraging on the estimated scores. Experiments\ndemonstrate that the proposed model outperforms state-of-the-art methods under\na variety of noise models, and shows the potential to be applied in other tasks\nsuch as point cloud upsampling.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 01:13:03 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Luo", "Shitong", ""], ["Hu", "Wei", ""]]}, {"id": "2107.10984", "submitter": "Kun Wu", "authors": "Kun Wu, Chengxiang Yin, Zhengping Che, Bo Jiang, Jian Tang, Zheng Guan\n  and Gangyi Ding", "title": "Human Pose Transfer with Disentangled Feature Consistency", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. However,\nmost existing approaches explicitly leverage the pose information extracted\nfrom the source images as a conditional input for the generative networks.\nMeanwhile, they usually focus on the visual fidelity of the synthesized images\nbut neglect the inherent consistency, which further confines their performance\nof pose transfer. To alleviate the current limitations and improve the quality\nof the synthesized images, we propose a pose transfer network with Disentangled\nFeature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair\nof images containing the source and target person, DFC-Net extracts pose and\nstatic information from the source and target respectively, then synthesizes an\nimage of the target person with the desired pose from the source. Moreover,\nDFC-Net leverages disentangled feature consistency losses in the adversarial\ntraining to strengthen the transfer coherence and integrates the keypoint\namplifier to enhance the pose feature extraction. Additionally, an unpaired\nsupport dataset Mixamo-Sup providing more extra pose information has been\nfurther utilized during the training to improve the generality and robustness\nof DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have\ndemonstrated DFC-Net achieves state-of-the-art performance on pose transfer.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 01:25:07 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wu", "Kun", ""], ["Yin", "Chengxiang", ""], ["Che", "Zhengping", ""], ["Jiang", "Bo", ""], ["Tang", "Jian", ""], ["Guan", "Zheng", ""], ["Ding", "Gangyi", ""]]}, {"id": "2107.10990", "submitter": "Libo Long", "authors": "Libo Long, Jochen Lang", "title": "Detail Preserving Residual Feature Pyramid Modules for Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Feature pyramids and iterative refinement have recently led to great progress\nin optical flow estimation. However, downsampling in feature pyramids can cause\nblending of foreground objects with the background, which will mislead\nsubsequent decisions in the iterative processing. The results are missing\ndetails especially in the flow of thin and of small structures. We propose a\nnovel Residual Feature Pyramid Module (RFPM) which retains important details in\nthe feature map without changing the overall iterative refinement design of the\noptical flow estimation. RFPM incorporates a residual structure between\nmultiple feature pyramids into a downsampling module that corrects the blending\nof objects across boundaries. We demonstrate how to integrate our module with\ntwo state-of-the-art iterative refinement architectures. Results show that our\nRFPM visibly reduces flow errors and improves state-of-art performance in the\nclean pass of Sintel, and is one of the top-performing methods in KITTI.\nAccording to the particular modular structure of RFPM, we introduce a special\ntransfer learning approach that can dramatically decrease the training time\ncompared to a typical full optical flow training schedule on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 01:53:04 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Long", "Libo", ""], ["Lang", "Jochen", ""]]}, {"id": "2107.10997", "submitter": "Touqeer Ahmad", "authors": "Touqeer Ahmad, Ebrahim Emami, Martin \\v{C}ad\\'ik, George Bebis", "title": "Resource Efficient Mountainous Skyline Extraction using Shallow Learning", "comments": "Accepted at International Joint Conference on Neural Networks, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline plays a pivotal role in mountainous visual geo-localization and\nlocalization/navigation of planetary rovers/UAVs and virtual/augmented reality\napplications. We present a novel mountainous skyline detection approach where\nwe adapt a shallow learning approach to learn a set of filters to discriminate\nbetween edges belonging to sky-mountain boundary and others coming from\ndifferent regions. Unlike earlier approaches, which either rely on extraction\nof explicit feature descriptors and their classification, or fine-tuning\ngeneral scene parsing deep networks for sky segmentation, our approach learns\nlinear filters based on local structure analysis. At test time, for every\ncandidate edge pixel, a single filter is chosen from the set of learned filters\nbased on pixel's structure tensor, and then applied to the patch around it. We\nthen employ dynamic programming to solve the shortest path problem for the\nresultant multistage graph to get the sky-mountain boundary. The proposed\napproach is computationally faster than earlier methods while providing\ncomparable performance and is more suitable for resource constrained platforms\ne.g., mobile devices, planetary rovers and UAVs. We compare our proposed\napproach against earlier skyline detection methods using four different data\nsets. Our code is available at\n\\url{https://github.com/TouqeerAhmad/skyline_detection}.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 02:14:17 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ahmad", "Touqeer", ""], ["Emami", "Ebrahim", ""], ["\u010cad\u00edk", "Martin", ""], ["Bebis", "George", ""]]}, {"id": "2107.10998", "submitter": "Dan Liu", "authors": "Dan Liu, Xi Chen, Jie Fu, Xue Liu", "title": "Pruning Ternary Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose pruning ternary quantization (PTQ), a simple, yet effective,\nsymmetric ternary quantization method. The method significantly compresses\nneural network weights to a sparse ternary of [-1,0,1] and thus reduces\ncomputational, storage, and memory footprints. We show that PTQ can convert\nregular weights to ternary orthonormal bases by simply using pruning and L2\nprojection. In addition, we introduce a refined straight-through estimator to\nfinalize and stabilize the quantized weights. Our method can provide at most\n46x compression ratio on the ResNet-18 structure, with an acceptable accuracy\nof 65.36%, outperforming leading methods. Furthermore, PTQ can compress a\nResNet-18 model from 46 MB to 955KB (~48x) and a ResNet-50 model from 99 MB to\n3.3MB (~30x), while the top-1 accuracy on ImageNet drops slightly from 69.7% to\n65.3% and from 76.15% to 74.47%, respectively. Our method unifies pruning and\nquantization and thus provides a range of size-accuracy trade-off.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 02:18:00 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Liu", "Dan", ""], ["Chen", "Xi", ""], ["Fu", "Jie", ""], ["Liu", "Xue", ""]]}, {"id": "2107.11001", "submitter": "Bhavya Goyal", "authors": "Bhavya Goyal, Mohit Gupta", "title": "Photon-Starved Scene Inference using Single Photon Cameras", "comments": "ICCV 2021 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Scene understanding under low-light conditions is a challenging problem. This\nis due to the small number of photons captured by the camera and the resulting\nlow signal-to-noise ratio (SNR). Single-photon cameras (SPCs) are an emerging\nsensing modality that are capable of capturing images with high sensitivity.\nDespite having minimal read-noise, images captured by SPCs in photon-starved\nconditions still suffer from strong shot noise, preventing reliable scene\ninference. We propose photon scale-space a collection of high-SNR images\nspanning a wide range of photons-per-pixel (PPP) levels (but same scene\ncontent) as guides to train inference model on low photon flux images. We\ndevelop training techniques that push images with different illumination levels\ncloser to each other in feature representation space. The key idea is that\nhaving a spectrum of different brightness levels during training enables\neffective guidance, and increases robustness to shot noise even in extreme\nnoise cases. Based on the proposed approach, we demonstrate, via simulations\nand real experiments with a SPAD camera, high-performance on various inference\ntasks such as image classification and monocular depth estimation under ultra\nlow-light, down to < 1 PPP.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 02:27:03 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 03:26:36 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Goyal", "Bhavya", ""], ["Gupta", "Mohit", ""]]}, {"id": "2107.11004", "submitter": "Dayan Guan", "authors": "Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu", "title": "Domain Adaptive Video Segmentation via Temporal Consistency\n  Regularization", "comments": "Accepted to ICCV 2021. Code is available at\n  https://github.com/Dayan-Guan/DA-VSN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video semantic segmentation is an essential task for the analysis and\nunderstanding of videos. Recent efforts largely focus on supervised video\nsegmentation by learning from fully annotated data, but the learnt models often\nexperience clear performance drop while applied to videos of a different\ndomain. This paper presents DA-VSN, a domain adaptive video segmentation\nnetwork that addresses domain gaps in videos by temporal consistency\nregularization (TCR) for consecutive frames of target-domain videos. DA-VSN\nconsists of two novel and complementary designs. The first is cross-domain TCR\nthat guides the prediction of target frames to have similar temporal\nconsistency as that of source frames (learnt from annotated source data) via\nadversarial learning. The second is intra-domain TCR that guides unconfident\npredictions of target frames to have similar temporal consistency as confident\npredictions of target frames. Extensive experiments demonstrate the superiority\nof our proposed domain adaptive video segmentation network which outperforms\nmultiple baselines consistently by large margins.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 02:50:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Guan", "Dayan", ""], ["Huang", "Jiaxing", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""]]}, {"id": "2107.11007", "submitter": "Yixiao Yang", "authors": "Yixiao Yang, Ran Tao, Kaixuan Wei, Ying Fu", "title": "Dynamic Proximal Unrolling Network for Compressive Sensing Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering an underlying image from under-sampled measurements, Compressive\nSensing Imaging (CSI) is a challenging problem and has many practical\napplications. Recently, deep neural networks have been applied to this problem\nwith promising results, owing to its implicitly learned prior to alleviate the\nill-poseness of CSI. However, existing neural network approaches require\nseparate models for each imaging parameter like sampling ratios, leading to\ntraining difficulties and overfitting to specific settings. In this paper, we\npresent a dynamic proximal unrolling network (dubbed DPUNet), which can handle\na variety of measurement matrices via one single model without retraining.\nSpecifically, DPUNet can exploit both embedded physical model via gradient\ndescent and imposing image prior with learned dynamic proximal mapping leading\nto joint reconstruction. A key component of DPUNet is a dynamic proximal\nmapping module, whose parameters can be dynamically adjusted at inference stage\nand make it adapt to any given imaging setting. Experimental results\ndemonstrate that the proposed DPUNet can effectively handle multiple CSI\nmodalities under varying sampling ratios and noise levels with only one model,\nand outperform the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 03:04:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yang", "Yixiao", ""], ["Tao", "Ran", ""], ["Wei", "Kaixuan", ""], ["Fu", "Ying", ""]]}, {"id": "2107.11008", "submitter": "Mehdi Mousavi", "authors": "Mehdi Mousavi, Rolando Estrada", "title": "SuperCaustics: Real-time, open-source simulation of transparent objects\n  for deep learning applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transparent objects are a very challenging problem in computer vision. They\nare hard to segment or classify due to their lack of precise boundaries, and\nthere is limited data available for training deep neural networks. As such,\ncurrent solutions for this problem employ rigid synthetic datasets, which lack\nflexibility and lead to severe performance degradation when deployed on\nreal-world scenarios. In particular, these synthetic datasets omit features\nsuch as refraction, dispersion and caustics due to limitations in the rendering\npipeline. To address this issue, we present SuperCaustics, a real-time,\nopen-source simulation of transparent objects designed for deep learning\napplications. SuperCaustics features extensive modules for stochastic\nenvironment creation; uses hardware ray-tracing to support caustics,\ndispersion, and refraction; and enables generating massive datasets with\nmulti-modal, pixel-perfect ground truth annotations. To validate our proposed\nsystem, we trained a deep neural network from scratch to segment transparent\nobjects in difficult lighting scenarios. Our neural network achieved\nperformance comparable to the state-of-the-art on a real-world dataset using\nonly 10% of the training data and in a fraction of the training time. Further\nexperiments show that a model trained with SuperCaustics can segment different\ntypes of caustics, even in images with multiple overlapping transparent\nobjects. To the best of our knowledge, this is the first such result for a\nmodel trained on synthetic data. Both our open-source code and experimental\ndata are freely available online.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 03:11:47 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Mousavi", "Mehdi", ""], ["Estrada", "Rolando", ""]]}, {"id": "2107.11010", "submitter": "Shuqiang Wang", "authors": "Bowen Hu, Baiying Lei, Yong Liu, Min Gan, Bingchuan Wang, Shuqiang\n  Wang", "title": "3D Brain Reconstruction by Hierarchical Shape-Perception Network from a\n  Single Incomplete Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape reconstruction is essential in the navigation of minimally-invasive\nand auto robot-guided surgeries whose operating environments are indirect and\nnarrow, and there have been some works that focused on reconstructing the 3D\nshape of the surgical organ through limited 2D information available. However,\nthe lack and incompleteness of such information caused by intraoperative\nemergencies (such as bleeding) and risk control conditions have not been\nconsidered. In this paper, a novel hierarchical shape-perception network (HSPN)\nis proposed to reconstruct the 3D point clouds (PCs) of specific brains from\none single incomplete image with low latency. A tree-structured predictor and\nseveral hierarchical attention pipelines are constructed to generate point\nclouds that accurately describe the incomplete images and then complete these\npoint clouds with high quality. Meanwhile, attention gate blocks (AGBs) are\ndesigned to efficiently aggregate geometric local features of incomplete PCs\ntransmitted by hierarchical attention pipelines and internal features of\nreconstructing point clouds. With the proposed HSPN, 3D shape perception and\ncompletion can be achieved spontaneously. Comprehensive results measured by\nChamfer distance and PC-to-PC error demonstrate that the performance of the\nproposed HSPN outperforms other competitive methods in terms of qualitative\ndisplays, quantitative experiment, and classification evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 03:20:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Hu", "Bowen", ""], ["Lei", "Baiying", ""], ["Liu", "Yong", ""], ["Gan", "Min", ""], ["Wang", "Bingchuan", ""], ["Wang", "Shuqiang", ""]]}, {"id": "2107.11022", "submitter": "Kai Yao", "authors": "Kai Yao and Kaizhu Huang and Jie Sun and Curran Jude", "title": "AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned\n  Disentangling Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unsupervised cell nuclei segmentation in this paper. Exploiting\nthe recently-proposed unpaired image-to-image translation between cell nuclei\nimages and randomly synthetic masks, existing approaches, e.g., CycleGAN, have\nachieved encouraging results. However, these methods usually take a two-stage\npipeline and fail to learn end-to-end in cell nuclei images. More seriously,\nthey could lead to the lossy transformation problem, i.e., the content\ninconsistency between the original images and the corresponding segmentation\noutput. To address these limitations, we propose a novel end-to-end\nunsupervised framework called Aligned Disentangling Generative Adversarial\nNetwork (AD-GAN). Distinctively, AD-GAN introduces representation\ndisentanglement to separate content representation (the underling spatial\nstructure) from style representation (the rendering of the structure). With\nthis framework, spatial structure can be preserved explicitly, enabling a\nsignificant reduction of macro-level lossy transformation. We also propose a\nnovel training algorithm able to align the disentangled content in the latent\nspace to reduce micro-level lossy transformation. Evaluations on real-world 2D\nand 3D datasets show that AD-GAN substantially outperforms the other comparison\nmethods and the professional software both quantitatively and qualitatively.\nSpecifically, the proposed AD-GAN leads to significant improvement over the\ncurrent best unsupervised methods by an average 17.8% relatively (w.r.t. the\nmetric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN\neven performs competitive with the best supervised models, taking a further\nleap towards end-to-end unsupervised nuclei segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 04:08:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yao", "Kai", ""], ["Huang", "Kaizhu", ""], ["Sun", "Jie", ""], ["Jude", "Curran", ""]]}, {"id": "2107.11024", "submitter": "Ehsan Zobeidi", "authors": "Ehsan Zobeidi and Nikolay Atanasov", "title": "A Deep Signed Directional Distance Function for Object Shape\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks that map 3D coordinates to signed distance function (SDF) or\noccupancy values have enabled high-fidelity implicit representations of object\nshape. This paper develops a new shape model that allows synthesizing novel\ndistance views by optimizing a continuous signed directional distance function\n(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole\ncategories of shapes and complete or interpolate across shapes from partial\ninput data. Unlike an SDF, which measures distance to the nearest surface in\nany direction, an SDDF measures distance in a given direction. This allows\ntraining an SDDF model without 3D shape supervision, using only distance\nmeasurements, readily available from depth camera or Lidar sensors. Our model\nalso removes post-processing steps like surface extraction or rendering by\ndirectly predicting distance at arbitrary locations and viewing directions.\nUnlike deep view-synthesis techniques, such as Neural Radiance Fields, which\ntrain high-capacity black-box models, our model encodes by construction the\nproperty that SDDF values decrease linearly along the viewing direction. This\nstructure constraint not only results in dimensionality reduction but also\nprovides analytical confidence about the accuracy of SDDF predictions,\nregardless of the distance to the object surface.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 04:11:59 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Zobeidi", "Ehsan", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2107.11027", "submitter": "Yingchen Yu", "authors": "Yingchen Yu, Fangneng Zhan, Shijian Lu, Jianxiong Pan, Feiying Ma,\n  Xuansong Xie, Chunyan Miao", "title": "WaveFill: A Wavelet-based Generation Network for Image Inpainting", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting aims to complete the missing or corrupted regions of images\nwith realistic contents. The prevalent approaches adopt a hybrid objective of\nreconstruction and perceptual quality by using generative adversarial networks.\nHowever, the reconstruction loss and adversarial loss focus on synthesizing\ncontents of different frequencies and simply applying them together often leads\nto inter-frequency conflicts and compromised inpainting. This paper presents\nWaveFill, a wavelet-based inpainting network that decomposes images into\nmultiple frequency bands and fills the missing regions in each frequency band\nseparately and explicitly. WaveFill decomposes images by using discrete wavelet\ntransform (DWT) that preserves spatial information naturally. It applies L1\nreconstruction loss to the decomposed low-frequency bands and adversarial loss\nto high-frequency bands, hence effectively mitigate inter-frequency conflicts\nwhile completing images in spatial domain. To address the inpainting\ninconsistency in different frequency bands and fuse features with distinct\nstatistics, we design a novel normalization scheme that aligns and fuses the\nmulti-frequency features effectively. Extensive experiments over multiple\ndatasets show that WaveFill achieves superior image inpainting qualitatively\nand quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 04:44:40 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yu", "Yingchen", ""], ["Zhan", "Fangneng", ""], ["Lu", "Shijian", ""], ["Pan", "Jianxiong", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""], ["Miao", "Chunyan", ""]]}, {"id": "2107.11039", "submitter": "Ransalu Senanayake", "authors": "Ransalu Senanayake, Kyle Beltran Hatch, Jason Zheng and Mykel J.\n  Kochenderfer", "title": "3D Radar Velocity Maps for Uncertain Dynamic Environments", "comments": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future urban transportation concepts include a mixture of ground and air\nvehicles with varying degrees of autonomy in a congested environment. In such\ndynamic environments, occupancy maps alone are not sufficient for safe path\nplanning. Safe and efficient transportation requires reasoning about the 3D\nflow of traffic and properly modeling uncertainty. Several different approaches\ncan be taken for developing 3D velocity maps. This paper explores a Bayesian\napproach that captures our uncertainty in the map given training data. The\napproach involves projecting spatial coordinates into a high-dimensional\nfeature space and then applying Bayesian linear regression to make predictions\nand quantify uncertainty in our estimates. On a collection of air and ground\ndatasets, we demonstrate that this approach is effective and more scalable than\nseveral alternative approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 06:03:16 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Senanayake", "Ransalu", ""], ["Hatch", "Kyle Beltran", ""], ["Zheng", "Jason", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2107.11041", "submitter": "Yoonsik Kim", "authors": "Junyeop Lee, Yoonsik Kim, Seonghyeon Kim, Moonbin Yim, Seung Shin,\n  Gayoung Lee, Sungrae Park", "title": "RewriteNet: Realistic Scene Text Image Generation via Editing Text in\n  Real-world Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene text editing (STE), which converts a text in a scene image into the\ndesired text while preserving an original style, is a challenging task due to a\ncomplex intervention between text and style. To address this challenge, we\npropose a novel representational learning-based STE model, referred to as\nRewriteNet that employs textual information as well as visual information. We\nassume that the scene text image can be decomposed into content and style\nfeatures where the former represents the text information and style represents\nscene text characteristics such as font, alignment, and background. Under this\nassumption, we propose a method to separately encode content and style features\nof the input image by introducing the scene text recognizer that is trained by\ntext information. Then, a text-edited image is generated by combining the style\nfeature from the original image and the content feature from the target text.\nUnlike previous works that are only able to use synthetic images in the\ntraining phase, we also exploit real-world images by proposing a\nself-supervised training scheme, which bridges the domain gap between synthetic\nand real data. Our experiments demonstrate that RewriteNet achieves better\nquantitative and qualitative performance than other comparisons. Moreover, we\nvalidate that the use of text information and the self-supervised training\nscheme improves text switching performance. The implementation and dataset will\nbe publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 06:32:58 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lee", "Junyeop", ""], ["Kim", "Yoonsik", ""], ["Kim", "Seonghyeon", ""], ["Yim", "Moonbin", ""], ["Shin", "Seung", ""], ["Lee", "Gayoung", ""], ["Park", "Sungrae", ""]]}, {"id": "2107.11043", "submitter": "Manish Bhattarai", "authors": "Manish Bhattarai", "title": "Integrating Deep Learning and Augmented Reality to Enhance Situational\n  Awareness in Firefighting Environments", "comments": "PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a new four-pronged approach to build firefighter's situational\nawareness for the first time in the literature. We construct a series of deep\nlearning frameworks built on top of one another to enhance the safety,\nefficiency, and successful completion of rescue missions conducted by\nfirefighters in emergency first response settings. First, we used a deep\nConvolutional Neural Network (CNN) system to classify and identify objects of\ninterest from thermal imagery in real-time. Next, we extended this CNN\nframework for object detection, tracking, segmentation with a Mask RCNN\nframework, and scene description with a multimodal natural language\nprocessing(NLP) framework. Third, we built a deep Q-learning-based agent,\nimmune to stress-induced disorientation and anxiety, capable of making clear\nnavigation decisions based on the observed and stored facts in live-fire\nenvironments. Finally, we used a low computational unsupervised learning\ntechnique called tensor decomposition to perform meaningful feature extraction\nfor anomaly detection in real-time. With these ad-hoc deep learning structures,\nwe built the artificial intelligence system's backbone for firefighters'\nsituational awareness. To bring the designed system into usage by firefighters,\nwe designed a physical structure where the processed results are used as inputs\nin the creation of an augmented reality capable of advising firefighters of\ntheir location and key features around them, which are vital to the rescue\noperation at hand, as well as a path planning feature that acts as a virtual\nguide to assist disoriented first responders in getting back to safety. When\ncombined, these four approaches present a novel approach to information\nunderstanding, transfer, and synthesis that could dramatically improve\nfirefighter response and efficacy and reduce life loss.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 06:35:13 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Bhattarai", "Manish", ""]]}, {"id": "2107.11047", "submitter": "SangHun Kim", "authors": "Sanghun Kim and SeungKyu Lee", "title": "Unrealistic Feature Suppression for Generative Adversarial Networks", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the unstable nature of minimax game between generator and\ndiscriminator, improving the performance of GANs is a challenging task. Recent\nstudies have shown that selected high-quality samples in training improve the\nperformance of GANs. However, sampling approaches which discard samples show\nlimitations in some aspects such as the speed of training and optimality of the\nnetworks. In this paper we propose unrealistic feature suppression (UFS) module\nthat keeps high-quality features and suppresses unrealistic features. UFS\nmodule keeps the training stability of networks and improves the quality of\ngenerated images. We demonstrate the effectiveness of UFS module on various\nmodels such as WGAN-GP, SNGAN, and BigGAN. By using UFS module, we achieved\nbetter Frechet inception distance and inception score compared to various\nbaseline models. We also visualize how effectively our UFS module suppresses\nunrealistic features through class activation maps.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 06:43:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kim", "Sanghun", ""], ["Lee", "SeungKyu", ""]]}, {"id": "2107.11049", "submitter": "Jae Won Cho", "authors": "Jae Won Cho, Dong-Jin Kim, Yunjae Jung, In So Kweon", "title": "MCDAL: Maximum Classifier Discrepancy for Active Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent state-of-the-art active learning methods have mostly leveraged\nGenerative Adversarial Networks (GAN) for sample acquisition; however, GAN is\nusually known to suffer from instability and sensitivity to hyper-parameters.\nIn contrast to these methods, we propose in this paper a novel active learning\nframework that we call Maximum Classifier Discrepancy for Active Learning\n(MCDAL) which takes the prediction discrepancies between multiple classifiers.\nIn particular, we utilize two auxiliary classification layers that learn\ntighter decision boundaries by maximizing the discrepancies among them.\nIntuitively, the discrepancies in the auxiliary classification layers'\npredictions indicate the uncertainty in the prediction. In this regard, we\npropose a novel method to leverage the classifier discrepancies for the\nacquisition function for active learning. We also provide an interpretation of\nour idea in relation to existing GAN based active learning methods and domain\nadaptation frameworks. Moreover, we empirically demonstrate the utility of our\napproach where the performance of our approach exceeds the state-of-the-art\nmethods on several image classification and semantic segmentation datasets in\nactive learning setups.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 06:57:08 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Cho", "Jae Won", ""], ["Kim", "Dong-Jin", ""], ["Jung", "Yunjae", ""], ["Kweon", "In So", ""]]}, {"id": "2107.11052", "submitter": "Kwanyong Park", "authors": "Inkyu Shin, Kwanyong Park, Sanghyun Woo, In So Kweon", "title": "Unsupervised Domain Adaptation for Video Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised Domain Adaptation for semantic segmentation has gained immense\npopularity since it can transfer knowledge from simulation to real (Sim2Real)\nby largely cutting out the laborious per pixel labeling efforts at real. In\nthis work, we present a new video extension of this task, namely Unsupervised\nDomain Adaptation for Video Semantic Segmentation. As it became easy to obtain\nlarge-scale video labels through simulation, we believe attempting to maximize\nSim2Real knowledge transferability is one of the promising directions for\nresolving the fundamental data-hungry issue in the video. To tackle this new\nproblem, we present a novel two-phase adaptation scheme. In the first step, we\nexhaustively distill source domain knowledge using supervised loss functions.\nSimultaneously, video adversarial training (VAT) is employed to align the\nfeatures from source to target utilizing video context. In the second step, we\napply video self-training (VST), focusing only on the target data. To construct\nrobust pseudo labels, we exploit the temporal information in the video, which\nhas been rarely explored in the previous image-based self-training approaches.\nWe set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.\nWe show that our proposals significantly outperform previous image-based UDA\nmethods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 07:18:20 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Shin", "Inkyu", ""], ["Park", "Kwanyong", ""], ["Woo", "Sanghyun", ""], ["Kweon", "In So", ""]]}, {"id": "2107.11055", "submitter": "Zhongqi Yue", "authors": "Zhongqi Yue, Qianru Sun, Xian-Sheng Hua, Hanwang Zhang", "title": "Transporting Causal Mechanisms for Unsupervised Domain Adaptation", "comments": "ICCV 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate\nshift and conditional shift assumptions, which essentially encourage models to\nlearn common features across domains. However, due to the lack of supervision\nin the target domain, they suffer from the semantic loss: the feature will\ninevitably lose non-discriminative semantics in source domain, which is however\ndiscriminative in target domain. We use a causal view -- transportability\ntheory -- to identify that such loss is in fact a confounding effect, which can\nonly be removed by causal intervention. However, the theoretical solution\nprovided by transportability is far from practical for UDA, because it requires\nthe stratification and representation of the unobserved confounder that is the\ncause of the domain gap. To this end, we propose a practical solution:\nTransporting Causal Mechanisms (TCM), to identify the confounder stratum and\nrepresentations by using the domain-invariant disentangled causal mechanisms,\nwhich are discovered in an unsupervised fashion. Our TCM is both theoretically\nand empirically grounded. Extensive experiments show that TCM achieves\nstate-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA,\nOffice-Home, and VisDA-2017. Codes are available in Appendix.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 07:25:15 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 12:07:42 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Yue", "Zhongqi", ""], ["Sun", "Qianru", ""], ["Hua", "Xian-Sheng", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2107.11056", "submitter": "Tian Pinzhuo", "authors": "Pinzhuo Tian, Yao Gao", "title": "Improving the Generalization of Meta-learning on Unseen Domains via\n  Adversarial Shift", "comments": "v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning provides a promising way for learning to efficiently learn and\nachieves great success in many applications. However, most meta-learning\nliterature focuses on dealing with tasks from a same domain, making it brittle\nto generalize to tasks from the other unseen domains. In this work, we address\nthis problem by simulating tasks from the other unseen domains to improve the\ngeneralization and robustness of meta-learning method. Specifically, we propose\na model-agnostic shift layer to learn how to simulate the domain shift and\ngenerate pseudo tasks, and develop a new adversarial learning-to-learn\nmechanism to train it. Based on the pseudo tasks, the meta-learning model can\nlearn cross-domain meta-knowledge, which can generalize well on unseen domains.\nWe conduct extensive experiments under the domain generalization setting.\nExperimental results demonstrate that the proposed shift layer is applicable to\nvarious meta-learning frameworks. Moreover, our method also leads to\nstate-of-the-art performance on different cross-domain few-shot classification\nbenchmarks and produces good results on cross-domain few-shot regression.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 07:29:30 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tian", "Pinzhuo", ""], ["Gao", "Yao", ""]]}, {"id": "2107.11061", "submitter": "Shasha Mao", "authors": "Shasha Mao, Guanghui Shi, Licheng Jiao, Shuiping Gou, Yangyang Li, Lin\n  Xiong, Boxin Shi", "title": "Label Distribution Amendment with Emotional Semantic Correlations for\n  Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By utilizing label distribution learning, a probability distribution is\nassigned for a facial image to express a compound emotion, which effectively\nimproves the problem of label uncertainties and noises occurred in one-hot\nlabels. In practice, it is observed that correlations among emotions are\ninherently different, such as surprised and happy emotions are more possibly\nsynchronized than surprised and neutral. It indicates the correlation may be\ncrucial for obtaining a reliable label distribution. Based on this, we propose\na new method that amends the label distribution of each facial image by\nleveraging correlations among expressions in the semantic space. Inspired by\ninherently diverse correlations among word2vecs, the topological information\namong facial expressions is firstly explored in the semantic space, and each\nimage is embedded into the semantic space. Specially, a class-relation graph is\nconstructed to transfer the semantic correlation among expressions into the\ntask space. By comparing semantic and task class-relation graphs of each image,\nthe confidence of its label distribution is evaluated. Based on the confidence,\nthe label distribution is amended by enhancing samples with higher confidence\nand weakening samples with lower confidence. Experimental results demonstrate\nthe proposed method is more effective than compared state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 07:46:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Mao", "Shasha", ""], ["Shi", "Guanghui", ""], ["Jiao", "Licheng", ""], ["Gou", "Shuiping", ""], ["Li", "Yangyang", ""], ["Xiong", "Lin", ""], ["Shi", "Boxin", ""]]}, {"id": "2107.11077", "submitter": "Petia Koprinkova-Hristova", "authors": "Petia Koprinkova-Hristova", "title": "Reservoir Computing Approach for Gray Images Segmentation", "comments": "12 pages, 7 figures, submitted to conference ICANN 2021 but not\n  accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes a novel approach for gray scale images segmentation. It is\nbased on multiple features extraction from single feature per image pixel,\nnamely its intensity value, using Echo state network. The newly extracted\nfeatures -- reservoir equilibrium states -- reveal hidden image characteristics\nthat improve its segmentation via a clustering algorithm. Moreover, it was\ndemonstrated that the intrinsic plasticity tuning of reservoir fits its\nequilibrium states to the original image intensity distribution thus allowing\nfor its better segmentation. The proposed approach is tested on the benchmark\nimage Lena.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 08:37:24 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Koprinkova-Hristova", "Petia", ""]]}, {"id": "2107.11085", "submitter": "Patrik Puchert", "authors": "Patrik Puchert, Pedro Hermosilla, Tobias Ritschel, Timo Ropinski", "title": "Data-driven deep density estimation", "comments": "35 pages, 25 figures. Puplished in Neural Computing and Applications\n  (2021). The method described is available as python pip pachage\n  deep_density_estimation and on github https://github.com/trikpachu/ DDE", "journal-ref": null, "doi": "10.1007/s00521-021-06281-3", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Density estimation plays a crucial role in many data analysis tasks, as it\ninfers a continuous probability density function (PDF) from discrete samples.\nThus, it is used in tasks as diverse as analyzing population data, spatial\nlocations in 2D sensor readings, or reconstructing scenes from 3D scans. In\nthis paper, we introduce a learned, data-driven deep density estimation (DDE)\nto infer PDFs in an accurate and efficient manner, while being independent of\ndomain dimensionality or sample size. Furthermore, we do not require access to\nthe original PDF during estimation, neither in parametric form, nor as priors,\nor in the form of many samples. This is enabled by training an unstructured\nconvolutional neural network on an infinite stream of synthetic PDFs, as\nunbound amounts of synthetic training data generalize better across a deck of\nnatural PDFs than any natural finite training data will do. Thus, we hope that\nour publicly available DDE method will be beneficial in many areas of data\nanalysis, where continuous models are to be estimated from discrete\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 08:53:46 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Puchert", "Patrik", ""], ["Hermosilla", "Pedro", ""], ["Ritschel", "Tobias", ""], ["Ropinski", "Timo", ""]]}, {"id": "2107.11091", "submitter": "Mobarakol Islam", "authors": "Mengya Xu, Mobarakol Islam, Chwee Ming Lim, Hongliang Ren", "title": "Class-Incremental Domain Adaptation with Smoothing and Calibration for\n  Surgical Report Generation", "comments": "Accepted in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating surgical reports aimed at surgical scene understanding in\nrobot-assisted surgery can contribute to documenting entry tasks and\npost-operative analysis. Despite the impressive outcome, the deep learning\nmodel degrades the performance when applied to different domains encountering\ndomain shifts. In addition, there are new instruments and variations in\nsurgical tissues appeared in robotic surgery. In this work, we propose\nclass-incremental domain adaptation (CIDA) with a multi-layer transformer-based\nmodel to tackle the new classes and domain shift in the target domain to\ngenerate surgical reports during robotic surgery. To adapt incremental classes\nand extract domain invariant features, a class-incremental (CI) learning method\nwith supervised contrastive (SupCon) loss is incorporated with a feature\nextractor. To generate caption from the extracted feature, curriculum by\none-dimensional gaussian smoothing (CBS) is integrated with a multi-layer\ntransformer-based caption prediction model. CBS smoothes the features embedding\nusing anti-aliasing and helps the model to learn domain invariant features. We\nalso adopt label smoothing (LS) to calibrate prediction probability and obtain\nbetter feature representation with both feature extractor and captioning model.\nThe proposed techniques are empirically evaluated by using the datasets of two\nsurgical domains, such as nephrectomy operations and transoral robotic surgery.\nWe observe that domain invariant feature learning and the well-calibrated\nnetwork improves the surgical report generation performance in both source and\ntarget domain under domain shift and unseen classes in the manners of one-shot\nand few-shot learning. The code is publicly available at\nhttps://github.com/XuMengyaAmy/CIDACaptioning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 09:08:26 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Xu", "Mengya", ""], ["Islam", "Mobarakol", ""], ["Lim", "Chwee Ming", ""], ["Ren", "Hongliang", ""]]}, {"id": "2107.11099", "submitter": "Yu Jing", "authors": "Yu Jing, Yang Yang, Chonghang Wu, Wenbing Fu, Wei Hu, Xiaogang Li and\n  Hua Xu", "title": "RGB Image Classification with Quantum Convolutional Ansaetze", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of qubit numbers and coherence times in quantum\nhardware technology, implementing shallow neural networks on the so-called\nNoisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of\ninterest. Many quantum (convolutional) circuit ansaetze are proposed for\ngrayscale images classification tasks with promising empirical results.\nHowever, when applying these ansaetze on RGB images, the intra-channel\ninformation that is useful for vision tasks is not extracted effectively. In\nthis paper, we propose two types of quantum circuit ansaetze to simulate\nconvolution operations on RGB images, which differ in the way how inter-channel\nand intra-channel information are extracted. To the best of our knowledge, this\nis the first work of a quantum convolutional circuit to deal with RGB images\neffectively, with a higher test accuracy compared to the purely classical CNNs.\nWe also investigate the relationship between the size of quantum circuit ansatz\nand the learnability of the hybrid quantum-classical convolutional neural\nnetwork. Through experiments based on CIFAR-10 and MNIST datasets, we\ndemonstrate that a larger size of the quantum circuit ansatz improves\npredictive performance in multiclass classification tasks, providing useful\ninsights for near term quantum algorithm developments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 09:38:59 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Jing", "Yu", ""], ["Yang", "Yang", ""], ["Wu", "Chonghang", ""], ["Fu", "Wenbing", ""], ["Hu", "Wei", ""], ["Li", "Xiaogang", ""], ["Xu", "Hua", ""]]}, {"id": "2107.11119", "submitter": "Xinyang Wu", "authors": "Xinyang Wu", "title": "Cardiac CT segmentation based on distance regularized level set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Before analy z ing the CT image, it is very important to segment the heart\nimage, and the left ve ntricular (LV) inner and outer membrane segmentation is\none of the most important contents. However, manual segmentation is tedious and\ntime consuming. In order to facilitate doctors to focus on high tech tasks such\nas disease analysis and diagnosis, it is crucial to develop a fast and accurate\nsegmentation method [1]. In view of this phenomenon, this paper uses distance\nregularized level set (DRL SE) to explore the segmentation effect of epicardium\nand endocardium 2 ]], which includes a distance regula riz ed t erm and an\nexternal energy term. Finally, five CT images are used to verify the proposed\nmethod, and image quality evaluation indexes such as dice score and Hausdorff\ndistance are used to evaluate the segmentation effect. The results showed that\nthe me tho d could separate the inner and outer membrane very well (endocardium\ndice = 0.9253, Hausdorff = 7.8740; epicardium Hausdorff = 0.9687, Hausdorff = 6 .\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 10:13:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wu", "Xinyang", ""]]}, {"id": "2107.11159", "submitter": "Mohammed Hassanin", "authors": "Mohammed Hassanin, Ibrahim Radwan, Salman Khan, Murat Tahtali", "title": "Learning Discriminative Representations for Multi-Label Image\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-label recognition is a fundamental, and yet is a challenging task in\ncomputer vision. Recently, deep learning models have achieved great progress\ntowards learning discriminative features from input images. However,\nconventional approaches are unable to model the inter-class discrepancies among\nfeatures in multi-label images, since they are designed to work for image-level\nfeature discrimination. In this paper, we propose a unified deep network to\nlearn discriminative features for the multi-label task. Given a multi-label\nimage, the proposed method first disentangles features corresponding to\ndifferent classes. Then, it discriminates between these classes via increasing\nthe inter-class distance while decreasing the intra-class differences in the\noutput space. By regularizing the whole network with the proposed loss, the\nperformance of applying the wellknown ResNet-101 is improved significantly.\nExtensive experiments have been performed on COCO-2014, VOC2007 and VOC2012\ndatasets, which demonstrate that the proposed method outperforms\nstate-of-the-art approaches by a significant margin of 3:5% on large-scale COCO\ndataset. Moreover, analysis of the discriminative feature learning approach\nshows that it can be plugged into various types of multi-label methods as a\ngeneral module.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:10:46 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Hassanin", "Mohammed", ""], ["Radwan", "Ibrahim", ""], ["Khan", "Salman", ""], ["Tahtali", "Murat", ""]]}, {"id": "2107.11170", "submitter": "Lusine Abrahamyan", "authors": "Lusine Abrahamyan, Valentin Ziatchin, Yiming Chen and Nikos\n  Deligiannis", "title": "Bias Loss for Mobile Neural Networks", "comments": "Accepted at ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:37:56 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 14:41:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Abrahamyan", "Lusine", ""], ["Ziatchin", "Valentin", ""], ["Chen", "Yiming", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2107.11186", "submitter": "Yotam Nitzan", "authors": "Yotam Nitzan, Rinon Gal, Ofir Brenner, Daniel Cohen-Or", "title": "LARGE: Latent-Based Regression through GAN Semantics", "comments": "Code at https://github.com/YotamNitzan/LARGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for solving regression tasks using few-shot or weak\nsupervision. At the core of our method is the fundamental observation that GANs\nare incredibly successful at encoding semantic information within their latent\nspace, even in a completely unsupervised setting. For modern generative\nframeworks, this semantic encoding manifests as smooth, linear directions which\naffect image attributes in a disentangled manner. These directions have been\nwidely used in GAN-based image editing. We show that such directions are not\nonly linear, but that the magnitude of change induced on the respective\nattribute is approximately linear with respect to the distance traveled along\nthem. By leveraging this observation, our method turns a pre-trained GAN into a\nregression model, using as few as two labeled samples. This enables solving\nregression tasks on datasets and attributes which are difficult to produce\nquality supervision for. Additionally, we show that the same latent-distances\ncan be used to sort collections of images by the strength of given attributes,\neven in the absence of explicit supervision. Extensive experimental evaluations\ndemonstrate that our method can be applied across a wide range of domains,\nleverage multiple latent direction discovery frameworks, and achieve\nstate-of-the-art results in few-shot and low-supervision settings, even when\ncompared to methods designed to tackle a single task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:55:35 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nitzan", "Yotam", ""], ["Gal", "Rinon", ""], ["Brenner", "Ofir", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2107.11187", "submitter": "Hermann Baumgartl", "authors": "Hermann Baumgartl and Ricardo Buettner", "title": "Developing efficient transfer learning strategies for robust scene\n  recognition in mobile robotics using pre-trained convolutional neural\n  networks", "comments": "18 pages, 1 figures, 10 tables. Submitted to IEEE Transactions on\n  Robotics (T-RO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present four different robust transfer learning and data augmentation\nstrategies for robust mobile scene recognition. By training three mobile-ready\n(EfficientNetB0, MobileNetV2, MobileNetV3) and two large-scale baseline (VGG16,\nResNet50) convolutional neural network architectures on the widely available\nEvent8, Scene15, Stanford40, and MIT67 datasets, we show the generalization\nability of our transfer learning strategies. Furthermore, we tested the\nrobustness of our transfer learning strategies under viewpoint and lighting\nchanges using the KTH-Idol2 database. Also, the impact of inference\noptimization techniques on the general performance and the robustness under\ndifferent transfer learning strategies is evaluated. Experimental results show\nthat when employing transfer learning, Fine-Tuning in combination with\nextensive data augmentation improves the general accuracy and robustness in\nmobile scene recognition. We achieved state-of-the-art results using various\nbaseline convolutional neural networks and showed the robustness against\nlighting and viewpoint changes in challenging mobile robot place recognition.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:48:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Baumgartl", "Hermann", ""], ["Buettner", "Ricardo", ""]]}, {"id": "2107.11191", "submitter": "Margaret Duff", "authors": "Margaret Duff, Neill D. F. Campbell, Matthias J. Ehrhardt", "title": "Regularising Inverse Problems with Generative Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess models and guide future\nresearch. In our numerical experiments, we evaluate three common generative\nmodels, autoencoders, variational autoencoders and generative adversarial\nnetworks, against our desired criteria. We also test three different generative\nregularisers on the inverse problems of deblurring, deconvolution, and\ntomography. We show that the success of solutions restricted to lie exactly in\nthe range of the generator is highly dependent on the ability of the generative\nmodel but that allowing small deviations from the range of the generator\nproduces more consistent results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:47:36 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Duff", "Margaret", ""], ["Campbell", "Neill D. F.", ""], ["Ehrhardt", "Matthias J.", ""]]}, {"id": "2107.11196", "submitter": "Napat Wanchaitanawong", "authors": "Napat Wanchaitanawong, Masayuki Tanaka, Takashi Shibata, Masatoshi\n  Okutomi", "title": "Multi-Modal Pedestrian Detection with Large Misalignment Based on\n  Modal-Wise Regression and Multi-Modal IoU", "comments": "Accepted by MVA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combined use of multiple modalities enables accurate pedestrian detection\nunder poor lighting conditions by using the high visibility areas from these\nmodalities together. The vital assumption for the combination use is that there\nis no or only a weak misalignment between the two modalities. In general,\nhowever, this assumption often breaks in actual situations. Due to this\nassumption's breakdown, the position of the bounding boxes does not match\nbetween the two modalities, resulting in a significant decrease in detection\naccuracy, especially in regions where the amount of misalignment is large. In\nthis paper, we propose a multi-modal Faster-RCNN that is robust against large\nmisalignment. The keys are 1) modal-wise regression and 2) multi-modal IoU for\nmini-batch sampling. To deal with large misalignment, we perform bounding box\nregression for both the RPN and detection-head with both modalities. We also\npropose a new sampling strategy called \"multi-modal mini-batch sampling\" that\nintegrates the IoU for both modalities. We demonstrate that the proposed\nmethod's performance is much better than that of the state-of-the-art methods\nfor data with large misalignment through actual image experiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:58:41 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wanchaitanawong", "Napat", ""], ["Tanaka", "Masayuki", ""], ["Shibata", "Takashi", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2107.11214", "submitter": "Patrik Puchert", "authors": "Patrik Puchert, Timo Ropinski", "title": "Human Pose Estimation from Sparse Inertial Measurements through\n  Recurrent Graph Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose the adjacency adaptive graph convolutional long-short term memory\nnetwork (AAGC-LSTM) for human pose estimation from sparse inertial\nmeasurements, obtained from only 6 measurement units. The AAGC-LSTM combines\nboth spatial and temporal dependency in a single network operation. This is\nmade possible by equipping graph convolutions with adjacency adaptivity, which\nalso allows for learning unknown dependencies of the human body joints. To\nfurther boost accuracy, we propose longitudinal loss weighting to consider\nnatural movement patterns, as well as body-aware contralateral data\naugmentation. By combining these contributions, we are able to utilize the\ninherent graph nature of the human body, and can thus outperform the state of\nthe art for human pose estimation from sparse inertial measurements.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 13:23:10 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Puchert", "Patrik", ""], ["Ropinski", "Timo", ""]]}, {"id": "2107.11238", "submitter": "Th\\'eo Estienne", "authors": "Th\\'eo Estienne, Maria Vakalopoulou, Stergios Christodoulidis, Enzo\n  Battistella, Th\\'eophraste Henry, Marvin Lerousseau, Amaury Leroy, Guillaume\n  Chassagnon, Marie-Pierre Revel, Nikos Paragios and Eric Deutsch", "title": "Exploring Deep Registration Latent Spaces", "comments": "13 pages, 5 figures + 3 figures in supplementary materials Accepted\n  to DART 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Explainability of deep neural networks is one of the most challenging and\ninteresting problems in the field. In this study, we investigate the topic\nfocusing on the interpretability of deep learning-based registration methods.\nIn particular, with the appropriate model architecture and using a simple\nlinear projection, we decompose the encoding space, generating a new basis, and\nwe empirically show that this basis captures various decomposed anatomically\naware geometrical transformations. We perform experiments using two different\ndatasets focusing on lungs and hippocampus MRI. We show that such an approach\ncan decompose the highly convoluted latent spaces of registration pipelines in\nan orthogonal space with several interesting properties. We hope that this work\ncould shed some light on a better understanding of deep learning-based\nregistration methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 13:54:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Estienne", "Th\u00e9o", ""], ["Vakalopoulou", "Maria", ""], ["Christodoulidis", "Stergios", ""], ["Battistella", "Enzo", ""], ["Henry", "Th\u00e9ophraste", ""], ["Lerousseau", "Marvin", ""], ["Leroy", "Amaury", ""], ["Chassagnon", "Guillaume", ""], ["Revel", "Marie-Pierre", ""], ["Paragios", "Nikos", ""], ["Deutsch", "Eric", ""]]}, {"id": "2107.11252", "submitter": "Bingqian Lin", "authors": "Bingqian Lin, Yi Zhu, Yanxin Long, Xiaodan Liang, Qixiang Ye, Liang\n  Lin", "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language\n  Navigation", "comments": "Accepted by TPAMI 2021", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3097435", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:11:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lin", "Bingqian", ""], ["Zhu", "Yi", ""], ["Long", "Yanxin", ""], ["Liang", "Xiaodan", ""], ["Ye", "Qixiang", ""], ["Lin", "Liang", ""]]}, {"id": "2107.11262", "submitter": "Mohamed Abderrahmen Abid", "authors": "Mohamed Abderrahmen Abid, Ihsen Hedhli, Jean-Fran\\c{c}ois Lalonde,\n  Christian Gagne", "title": "Image-to-Image Translation with Low Resolution Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most image-to-image translation methods focus on learning mappings across\ndomains with the assumption that images share content (e.g., pose) but have\ntheir own domain-specific information known as style. When conditioned on a\ntarget image, such methods aim to extract the style of the target and combine\nit with the content of the source image. In this work, we consider the scenario\nwhere the target image has a very low resolution. More specifically, our\napproach aims at transferring fine details from a high resolution (HR) source\nimage to fit a coarse, low resolution (LR) image representation of the target.\nWe therefore generate HR images that share features from both HR and LR inputs.\nThis differs from previous methods that focus on translating a given image\nstyle into a target content, our translation approach being able to\nsimultaneously imitate the style and merge the structural information of the LR\ntarget. Our approach relies on training the generative model to produce HR\ntarget images that both 1) share distinctive information of the associated\nsource image; 2) correctly match the LR target image when downscaled. We\nvalidate our method on the CelebA-HQ and AFHQ datasets by demonstrating\nimprovements in terms of visual quality, diversity and coverage. Qualitative\nand quantitative results show that when dealing with intra-domain image\ntranslation, our method generates more realistic samples compared to\nstate-of-the-art methods such as Stargan-v2\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:22:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Abid", "Mohamed Abderrahmen", ""], ["Hedhli", "Ihsen", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["Gagne", "Christian", ""]]}, {"id": "2107.11264", "submitter": "Sanghun Jung", "authors": "Sanghun Jung, Jungsoo Lee, Daehoon Gwak, Sungha Choi, Jaegul Choo", "title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying\n  Unexpected Road Obstacles in Urban-Scene Segmentation", "comments": "Accepted to ICCV 2021 (Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying unexpected objects on roads in semantic segmentation (e.g.,\nidentifying dogs on roads) is crucial in safety-critical applications. Existing\napproaches use images of unexpected objects from external datasets or require\nadditional training (e.g., retraining segmentation networks or training an\nextra network), which necessitate a non-trivial amount of labor intensity or\nlengthy inference time. One possible alternative is to use prediction scores of\na pre-trained network such as the max logits (i.e., maximum values among\nclasses before the final softmax layer) for detecting such objects. However,\nthe distribution of max logits of each predicted class is significantly\ndifferent from each other, which degrades the performance of identifying\nunexpected objects in urban-scene segmentation. To address this issue, we\npropose a simple yet effective approach that standardizes the max logits in\norder to align the different distributions and reflect the relative meanings of\nmax logits within each predicted class. Moreover, we consider the local regions\nfrom two different perspectives based on the intuition that neighboring pixels\nshare similar semantic information. In contrast to previous approaches, our\nmethod does not utilize any external datasets or require additional training,\nwhich makes our method widely applicable to existing pre-trained segmentation\nmodels. Such a straightforward approach achieves a new state-of-the-art\nperformance on the publicly available Fishyscapes Lost & Found leaderboard with\na large margin.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:25:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Jung", "Sanghun", ""], ["Lee", "Jungsoo", ""], ["Gwak", "Daehoon", ""], ["Choi", "Sungha", ""], ["Choo", "Jaegul", ""]]}, {"id": "2107.11267", "submitter": "Jiacheng Wei", "authors": "Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Fayao Liu, Tzu-Yi Hung", "title": "Dense Supervision Propagation for Weakly Supervised Semantic\n  Segmentation on 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation on 3D point clouds is an important task for 3D scene\nunderstanding. While dense labeling on 3D data is expensive and time-consuming,\nonly a few works address weakly supervised semantic point cloud segmentation\nmethods to relieve the labeling cost by learning from simpler and cheaper\nlabels. Meanwhile, there are still huge performance gaps between existing\nweakly supervised methods and state-of-the-art fully supervised methods. In\nthis paper, we train a semantic point cloud segmentation network with only a\nsmall portion of points being labeled. We argue that we can better utilize the\nlimited supervision information as we densely propagate the supervision signal\nfrom the labeled points to other points within and across the input samples.\nSpecifically, we propose a cross-sample feature reallocating module to transfer\nsimilar features and therefore re-route the gradients across two samples with\ncommon classes and an intra-sample feature redistribution module to propagate\nsupervision signals on unlabeled points across and within point cloud samples.\nWe conduct extensive experiments on public datasets S3DIS and ScanNet. Our\nweakly supervised method with only 10\\% and 1\\% of labels can produce\ncompatible results with the fully supervised counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:34:57 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wei", "Jiacheng", ""], ["Lin", "Guosheng", ""], ["Yap", "Kim-Hui", ""], ["Liu", "Fayao", ""], ["Hung", "Tzu-Yi", ""]]}, {"id": "2107.11279", "submitter": "Ruifei He", "authors": "Ruifei He, Jihan Yang, Xiaojuan Qi", "title": "Re-distributing Biased Pseudo Labels for Semi-supervised Semantic\n  Segmentation: A Baseline Investigation", "comments": "ICCV 2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While self-training has advanced semi-supervised semantic segmentation, it\nseverely suffers from the long-tailed class distribution on real-world semantic\nsegmentation datasets that make the pseudo-labeled data bias toward majority\nclasses. In this paper, we present a simple and yet effective Distribution\nAlignment and Random Sampling (DARS) method to produce unbiased pseudo labels\nthat match the true class distribution estimated from the labeled data.\nBesides, we also contribute a progressive data augmentation and labeling\nstrategy to facilitate model training with pseudo-labeled data. Experiments on\nboth Cityscapes and PASCAL VOC 2012 datasets demonstrate the effectiveness of\nour approach. Albeit simple, our method performs favorably in comparison with\nstate-of-the-art approaches. Code will be available at\nhttps://github.com/CVMI-Lab/DARS.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:45:14 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 06:11:54 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["He", "Ruifei", ""], ["Yang", "Jihan", ""], ["Qi", "Xiaojuan", ""]]}, {"id": "2107.11291", "submitter": "Jiefeng Li", "authors": "Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu,\n  Cewu Lu", "title": "Human Pose Regression with Residual Log-likelihood Estimation", "comments": "ICCV 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap-based methods dominate in the field of human pose estimation by\nmodelling the output distribution through likelihood heatmaps. In contrast,\nregression-based methods are more efficient but suffer from inferior\nperformance. In this work, we explore maximum likelihood estimation (MLE) to\ndevelop an efficient and effective regression-based methods. From the\nperspective of MLE, adopting different regression losses is making different\nassumptions about the output density function. A density function closer to the\ntrue distribution leads to a better regression performance. In light of this,\nwe propose a novel regression paradigm with Residual Log-likelihood Estimation\n(RLE) to capture the underlying output distribution. Concretely, RLE learns the\nchange of the distribution instead of the unreferenced underlying distribution\nto facilitate the training process. With the proposed reparameterization\ndesign, our method is compatible with off-the-shelf flow models. The proposed\nmethod is effective, efficient and flexible. We show its potential in various\nhuman pose estimation tasks with comprehensive experiments. Compared to the\nconventional regression paradigm, regression with RLE bring 12.4 mAP\nimprovement on MSCOCO without any test-time overhead. Moreover, for the first\ntime, especially on multi-person pose estimation, our regression method is\nsuperior to the heatmap-based methods. Our code is available at\nhttps://github.com/Jeff-sjtu/res-loglikelihood-regression\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 15:06:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 03:10:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Jiefeng", ""], ["Bian", "Siyuan", ""], ["Zeng", "Ailing", ""], ["Wang", "Can", ""], ["Pang", "Bo", ""], ["Liu", "Wentao", ""], ["Lu", "Cewu", ""]]}, {"id": "2107.11298", "submitter": "Giuseppe Vecchio", "authors": "Giuseppe Vecchio, Simone Palazzo, Concetto Spampinato", "title": "SurfaceNet: Adversarial SVBRDF Estimation from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present SurfaceNet, an approach for estimating\nspatially-varying bidirectional reflectance distribution function (SVBRDF)\nmaterial properties from a single image. We pose the problem as an image\ntranslation task and propose a novel patch-based generative adversarial network\n(GAN) that is able to produce high-quality, high-resolution surface reflectance\nmaps. The employment of the GAN paradigm has a twofold objective: 1) allowing\nthe model to recover finer details than standard translation models; 2)\nreducing the domain shift between synthetic and real data distributions in an\nunsupervised way. An extensive evaluation, carried out on a public benchmark of\nsynthetic and real images under different illumination conditions, shows that\nSurfaceNet largely outperforms existing SVBRDF reconstruction methods, both\nquantitatively and qualitatively. Furthermore, SurfaceNet exhibits a remarkable\nability in generating high-quality maps from real samples without any\nsupervision at training time.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 15:18:54 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Vecchio", "Giuseppe", ""], ["Palazzo", "Simone", ""], ["Spampinato", "Concetto", ""]]}, {"id": "2107.11302", "submitter": "Sascha Saralajew", "authors": "Lukas Ewecker and Ebubekir Asan and Lars Ohnemus and Sascha Saralajew", "title": "Provident Vehicle Detection at Night for Advanced Driver Assistance\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, computer vision algorithms have become more and more\npowerful, which enabled technologies such as autonomous driving to evolve with\nrapid pace. However, current algorithms mainly share one limitation: They rely\non directly visible objects. This is a major drawback compared to human\nbehavior, where indirect visual cues caused by the actual object (e.g.,\nshadows) are already used intuitively to retrieve information or anticipate\noccurring objects. While driving at night, this performance deficit becomes\neven more obvious: Humans already process the light artifacts caused by\noncoming vehicles to assume their future appearance, whereas current object\ndetection systems rely on the oncoming vehicle's direct visibility. Based on\nprevious work in this subject, we present with this paper a complete system\ncapable of solving the task to providently detect oncoming vehicles at\nnighttime based on their caused light artifacts. For that, we outline the full\nalgorithm architecture ranging from the detection of light artifacts in the\nimage space, localizing the objects in the three-dimensional space, and\nverifying the objects over time. To demonstrate the applicability, we deploy\nthe system in a test vehicle and use the information of providently detected\nvehicles to control the glare-free high beam system proactively. Using this\nexperimental setting, we quantify the time benefit that the provident vehicle\ndetection system provides compared to an in-production computer vision system.\nAdditionally, the glare-free high beam use case provides a real-time and\nreal-world visualization interface of the detection results. With this\ncontribution, we want to put awareness on the unconventional sensing task of\nprovident object detection and further close the performance gap between human\nbehavior and computer vision algorithms in order to bring autonomous and\nautomated driving a step forward.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 15:27:17 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ewecker", "Lukas", ""], ["Asan", "Ebubekir", ""], ["Ohnemus", "Lars", ""], ["Saralajew", "Sascha", ""]]}, {"id": "2107.11317", "submitter": "Ben Saunders", "authors": "Ben Saunders, Necati Cihan Camgoz, Richard Bowden", "title": "Mixed SIGNals: Sign Language Production via a Mixture of Motion\n  Primitives", "comments": null, "journal-ref": "International Conference of Computer Vision (ICCV 2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice to represent spoken languages at their phonetic level.\nHowever, for sign languages, this implies breaking motion into its constituent\nmotion primitives. Avatar based Sign Language Production (SLP) has\ntraditionally done just this, building up animation from sequences of hand\nmotions, shapes and facial expressions. However, more recent deep learning\nbased solutions to SLP have tackled the problem using a single network that\nestimates the full skeletal structure.\n  We propose splitting the SLP task into two distinct jointly-trained\nsub-tasks. The first translation sub-task translates from spoken language to a\nlatent sign language representation, with gloss supervision. Subsequently, the\nanimation sub-task aims to produce expressive sign language sequences that\nclosely resemble the learnt spatio-temporal representation. Using a progressive\ntransformer for the translation sub-task, we propose a novel Mixture of Motion\nPrimitives (MoMP) architecture for sign language animation. A set of distinct\nmotion primitives are learnt during training, that can be temporally combined\nat inference to animate continuous sign language sequences.\n  We evaluate on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T)\ndataset, presenting extensive ablation studies and showing that MoMP\noutperforms baselines in user evaluations. We achieve state-of-the-art back\ntranslation performance with an 11% improvement over competing results.\nImportantly, and for the first time, we showcase stronger performance for a\nfull translation pipeline going from spoken language to sign, than from gloss\nto sign.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 15:53:11 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 09:13:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2107.11320", "submitter": "Gyri Reiersen", "authors": "Gyri Reiersen, David Dao, Bj\\\"orn L\\\"utjens, Konstantin Klemmer,\n  Xiaoxiang Zhu, and Ce Zhang", "title": "Tackling the Overestimation of Forest Carbon with Deep Learning and\n  Aerial Imagery", "comments": "Spotlight talk at the Tackling Climate Change with Machine Learning\n  workshop at the ICML 2021 https://www.climatechange.ai/papers/icml2021/79", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forest carbon offsets are increasingly popular and can play a significant\nrole in financing climate mitigation, forest conservation, and reforestation.\nMeasuring how much carbon is stored in forests is, however, still largely done\nvia expensive, time-consuming, and sometimes unaccountable field measurements.\nTo overcome these limitations, many verification bodies are leveraging machine\nlearning (ML) algorithms to estimate forest carbon from satellite or aerial\nimagery. Aerial imagery allows for tree species or family classification, which\nimproves the satellite imagery-based forest type classification. However,\naerial imagery is significantly more expensive to collect and it is unclear by\nhow much the higher resolution improves the forest carbon estimation. This\nproposal paper describes the first systematic comparison of forest carbon\nestimation from aerial imagery, satellite imagery, and ground-truth field\nmeasurements via deep learning-based algorithms for a tropical reforestation\nproject. Our initial results show that forest carbon estimates from satellite\nimagery can overestimate above-ground biomass by more than 10-times for\ntropical reforestation projects. The significant difference between aerial and\nsatellite-derived forest carbon measurements shows the potential for aerial\nimagery-based ML algorithms and raises the importance to extend this study to a\nglobal benchmark between options for carbon measurements.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 15:59:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Reiersen", "Gyri", ""], ["Dao", "David", ""], ["L\u00fctjens", "Bj\u00f6rn", ""], ["Klemmer", "Konstantin", ""], ["Zhu", "Xiaoxiang", ""], ["Zhang", "Ce", ""]]}, {"id": "2107.11355", "submitter": "Zhipeng Luo", "authors": "Zhipeng Luo, Zhongang Cai, Changqing Zhou, Gongjie Zhang, Haiyu Zhao,\n  Shuai Yi, Shijian Lu, Hongsheng Li, Shanghang Zhang, Ziwei Liu", "title": "Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based 3D object detection has achieved unprecedented success\nwith the advent of large-scale autonomous driving datasets. However, drastic\nperformance degradation remains a critical challenge for cross-domain\ndeployment. In addition, existing 3D domain adaptive detection methods often\nassume prior access to the target domain annotations, which is rarely feasible\nin the real world. To address this challenge, we study a more realistic\nsetting, unsupervised 3D domain adaptive detection, which only utilizes source\ndomain annotations. 1) We first comprehensively investigate the major\nunderlying factors of the domain gap in 3D detection. Our key insight is that\ngeometric mismatch is the key factor of domain shift. 2) Then, we propose a\nnovel and unified framework, Multi-Level Consistency Network (MLC-Net), which\nemploys a teacher-student paradigm to generate adaptive and reliable\npseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level\nconsistency to facilitate cross-domain transfer. Extensive experiments\ndemonstrate that MLC-Net outperforms existing state-of-the-art methods\n(including those using additional target domain information) on standard\nbenchmarks. Notably, our approach is detector-agnostic, which achieves\nconsistent gains on both single- and two-stage 3D detectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 17:19:23 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Luo", "Zhipeng", ""], ["Cai", "Zhongang", ""], ["Zhou", "Changqing", ""], ["Zhang", "Gongjie", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Lu", "Shijian", ""], ["Li", "Hongsheng", ""], ["Zhang", "Shanghang", ""], ["Liu", "Ziwei", ""]]}, {"id": "2107.11400", "submitter": "Ian Nielsen", "authors": "Ian E. Nielsen, Dimah Dera, Ghulam Rasool, Nidhal Bouaynaya, Ravi P.\n  Ramachandran", "title": "Robust Explainability: A Tutorial on Gradient-Based Attribution Methods\n  for Deep Neural Networks", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:06:29 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 17:18:31 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Nielsen", "Ian E.", ""], ["Dera", "Dimah", ""], ["Rasool", "Ghulam", ""], ["Bouaynaya", "Nidhal", ""], ["Ramachandran", "Ravi P.", ""]]}, {"id": "2107.11442", "submitter": "Lucas Liebenwein", "authors": "Lucas Liebenwein, Alaa Maalouf, Oren Gal, Dan Feldman, Daniela Rus", "title": "Compressing Neural Networks: Towards Determining the Optimal Layer-wise\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel global compression framework for deep neural networks that\nautomatically analyzes each layer to identify the optimal per-layer compression\nratio, while simultaneously achieving the desired overall compression. Our\nalgorithm hinges on the idea of compressing each convolutional (or\nfully-connected) layer by slicing its channels into multiple groups and\ndecomposing each group via low-rank decomposition. At the core of our algorithm\nis the derivation of layer-wise error bounds from the Eckart Young Mirsky\ntheorem. We then leverage these bounds to frame the compression problem as an\noptimization problem where we wish to minimize the maximum compression error\nacross layers and propose an efficient algorithm towards a solution. Our\nexperiments indicate that our method outperforms existing low-rank compression\napproaches across a wide range of networks and data sets. We believe that our\nresults open up new avenues for future research into the global\nperformance-size trade-offs of modern neural networks. Our code is available at\nhttps://github.com/lucaslie/torchprune.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 20:01:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liebenwein", "Lucas", ""], ["Maalouf", "Alaa", ""], ["Gal", "Oren", ""], ["Feldman", "Dan", ""], ["Rus", "Daniela", ""]]}, {"id": "2107.11443", "submitter": "Jiabo Huang", "authors": "Jiabo Huang, Yang Liu, Shaogang Gong and Hailin Jin", "title": "Cross-Sentence Temporal and Semantic Relations in Video Activity\n  Localisation", "comments": "International Conference on Computer Vision (ICCV'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video activity localisation has recently attained increasing attention due to\nits practical values in automatically localising the most salient visual\nsegments corresponding to their language descriptions (sentences) from\nuntrimmed and unstructured videos. For supervised model training, a temporal\nannotation of both the start and end time index of each video segment for a\nsentence (a video moment) must be given. This is not only very expensive but\nalso sensitive to ambiguity and subjective annotation bias, a much harder task\nthan image labelling. In this work, we develop a more accurate\nweakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM)\nin video moment proposal generation and matching when only a paragraph\ndescription of activities without per-sentence temporal annotation is\navailable. Specifically, we explore two cross-sentence relational constraints:\n(1) Temporal ordering and (2) semantic consistency among sentences in a\nparagraph description of video activities. Existing weakly-supervised\ntechniques only consider within-sentence video segment correlations in training\nwithout considering cross-sentence paragraph context. This can mislead due to\nambiguous expressions of individual sentences with visually indiscriminate\nvideo moment proposals in isolation. Experiments on two publicly available\nactivity localisation datasets show the advantages of our approach over the\nstate-of-the-art weakly supervised methods, especially so when the video\nactivity descriptions become more complex.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 20:04:01 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Huang", "Jiabo", ""], ["Liu", "Yang", ""], ["Gong", "Shaogang", ""], ["Jin", "Hailin", ""]]}, {"id": "2107.11447", "submitter": "Youssef Skandarani", "authors": "Youssef Skandarani, Pierre-Marc Jodoin and Alain Lalande", "title": "Deep Learning Based Cardiac MRI Segmentation: Do We Need Experts?", "comments": null, "journal-ref": "Algorithms 2021, 14(7), 212", "doi": "10.3390/a14070212", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods are the de-facto solutions to a multitude of medical\nimage analysis tasks. Cardiac MRI segmentation is one such application which,\nlike many others, requires a large number of annotated data so a trained\nnetwork can generalize well. Unfortunately, the process of having a large\nnumber of manually curated images by medical experts is both slow and utterly\nexpensive. In this paper, we set out to explore whether expert knowledge is a\nstrict requirement for the creation of annotated datasets that machine learning\ncan successfully train on. To do so, we gauged the performance of three\nsegmentation models, namely U-Net, Attention U-Net, and ENet, trained with\ndifferent loss functions on expert and non-expert groundtruth for cardiac\ncine-MRI segmentation. Evaluation was done with classic segmentation metrics\n(Dice index and Hausdorff distance) as well as clinical measurements, such as\nthe ventricular ejection fractions and the myocardial mass. Results reveal that\ngeneralization performances of a segmentation neural network trained on\nnon-expert groundtruth data is, to all practical purposes, as good as on expert\ngroundtruth data, in particular when the non-expert gets a decent level of\ntraining, highlighting an opportunity for the efficient and cheap creation of\nannotations for cardiac datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 20:10:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Skandarani", "Youssef", ""], ["Jodoin", "Pierre-Marc", ""], ["Lalande", "Alain", ""]]}, {"id": "2107.11468", "submitter": "Katy Blumer", "authors": "Katy Blumer, Subhashini Venugopalan, Michael P. Brenner, Jon Kleinberg", "title": "Using a Cross-Task Grid of Linear Probes to Interpret CNN Model\n  Predictions On Retinal Images", "comments": "Extended abstract at Interpretable Machine Learning in Healthcare\n  (IMLH) workshop at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze a dataset of retinal images using linear probes: linear regression\nmodels trained on some \"target\" task, using embeddings from a deep\nconvolutional (CNN) model trained on some \"source\" task as input. We use this\nmethod across all possible pairings of 93 tasks in the UK Biobank dataset of\nretinal images, leading to ~164k different models. We analyze the performance\nof these linear probes by source and target task and by layer depth. We observe\nthat representations from the middle layers of the network are more\ngeneralizable. We find that some target tasks are easily predicted irrespective\nof the source task, and that some other target tasks are more accurately\npredicted from correlated source tasks than from embeddings trained on the same\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 21:30:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Blumer", "Katy", ""], ["Venugopalan", "Subhashini", ""], ["Brenner", "Michael P.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2107.11470", "submitter": "Yunze Man", "authors": "Yunze Man, Xinshuo Weng, Prasanna Kumar Sivakuma, Matthew O'Toole,\n  Kris Kitani", "title": "Multi-Echo LiDAR for 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR sensors can be used to obtain a wide range of measurement signals other\nthan a simple 3D point cloud, and those signals can be leveraged to improve\nperception tasks like 3D object detection. A single laser pulse can be\npartially reflected by multiple objects along its path, resulting in multiple\nmeasurements called echoes. Multi-echo measurement can provide information\nabout object contours and semi-transparent surfaces which can be used to better\nidentify and locate objects. LiDAR can also measure surface reflectance\n(intensity of laser pulse return), as well as ambient light of the scene\n(sunlight reflected by objects). These signals are already available in\ncommercial LiDAR devices but have not been used in most LiDAR-based detection\nmodels. We present a 3D object detection model which leverages the full\nspectrum of measurement signals provided by LiDAR. First, we propose a\nmulti-signal fusion (MSF) module to combine (1) the reflectance and ambient\nfeatures extracted with a 2D CNN, and (2) point cloud features extracted using\na 3D graph neural network (GNN). Second, we propose a multi-echo aggregation\n(MEA) module to combine the information encoded in different set of echo\npoints. Compared with traditional single echo point cloud methods, our proposed\nMulti-Signal LiDAR Detector (MSLiD) extracts richer context information from a\nwider range of sensing measurements and achieves more accurate 3D object\ndetection. Experiments show that by incorporating the multi-modality of LiDAR,\nour method outperforms the state-of-the-art by up to 9.1%.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 21:43:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Man", "Yunze", ""], ["Weng", "Xinshuo", ""], ["Sivakuma", "Prasanna Kumar", ""], ["O'Toole", "Matthew", ""], ["Kitani", "Kris", ""]]}, {"id": "2107.11472", "submitter": "Yunhui Guo", "authors": "Yunhui Guo and Xudong Wang and Yubei Chen and Stella X. Yu", "title": "Free Hyperbolic Neural Networks with Limited Radii", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-Euclidean geometry with constant negative curvature, i.e., hyperbolic\nspace, has attracted sustained attention in the community of machine learning.\nHyperbolic space, owing to its ability to embed hierarchical structures\ncontinuously with low distortion, has been applied for learning data with\ntree-like structures. Hyperbolic Neural Networks (HNNs) that operate directly\nin hyperbolic space have also been proposed recently to further exploit the\npotential of hyperbolic representations. While HNNs have achieved better\nperformance than Euclidean neural networks (ENNs) on datasets with implicit\nhierarchical structure, they still perform poorly on standard classification\nbenchmarks such as CIFAR and ImageNet. The traditional wisdom is that it is\ncritical for the data to respect the hyperbolic geometry when applying HNNs. In\nthis paper, we first conduct an empirical study showing that the inferior\nperformance of HNNs on standard recognition datasets can be attributed to the\nnotorious vanishing gradient problem. We further discovered that this problem\nstems from the hybrid architecture of HNNs. Our analysis leads to a simple yet\neffective solution called Feature Clipping, which regularizes the hyperbolic\nembedding whenever its norm exceeding a given threshold. Our thorough\nexperiments show that the proposed method can successfully avoid the vanishing\ngradient problem when training HNNs with backpropagation. The improved HNNs are\nable to achieve comparable performance with ENNs on standard image recognition\ndatasets including MNIST, CIFAR10, CIFAR100 and ImageNet, while demonstrating\nmore adversarial robustness and stronger out-of-distribution detection\ncapability.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 22:10:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Guo", "Yunhui", ""], ["Wang", "Xudong", ""], ["Chen", "Yubei", ""], ["Yu", "Stella X.", ""]]}, {"id": "2107.11494", "submitter": "Aayush Rana", "authors": "Praveen Tirupattur, Aayush J Rana, Tushar Sangam, Shruti Vyas, Yogesh\n  S Rawat, Mubarak Shah", "title": "TinyAction Challenge: Recognizing Real-world Low-resolution Activities\n  in Videos", "comments": "8 pages. arXiv admin note: text overlap with arXiv:2007.07355", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper summarizes the TinyAction challenge which was organized in\nActivityNet workshop at CVPR 2021. This challenge focuses on recognizing\nreal-world low-resolution activities present in videos. Action recognition task\nis currently focused around classifying the actions from high-quality videos\nwhere the actors and the action is clearly visible. While various approaches\nhave been shown effective for recognition task in recent works, they often do\nnot deal with videos of lower resolution where the action is happening in a\ntiny region. However, many real world security videos often have the actual\naction captured in a small resolution, making action recognition in a tiny\nregion a challenging task. In this work, we propose a benchmark dataset,\nTinyVIRAT-v2, which is comprised of naturally occuring low-resolution actions.\nThis is an extension of the TinyVIRAT dataset and consists of actions with\nmultiple labels. The videos are extracted from security videos which makes them\nrealistic and more challenging. We use current state-of-the-art action\nrecognition methods on the dataset as a benchmark, and propose the TinyAction\nChallenge.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 00:41:19 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tirupattur", "Praveen", ""], ["Rana", "Aayush J", ""], ["Sangam", "Tushar", ""], ["Vyas", "Shruti", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "2107.11509", "submitter": "Youngjae Yu", "authors": "Jongseok Kim, Youngjae Yu, Seunghwan Lee, GunheeKim", "title": "Cycled Compositional Learning between Images and Text", "comments": "Fashion IQ 2020 challenge winner. Workshop tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach named the Cycled Composition Network that can measure\nthe semantic distance of the composition of image-text embedding. First, the\nComposition Network transit a reference image to target image in an embedding\nspace using relative caption. Second, the Correction Network calculates a\ndifference between reference and retrieved target images in the embedding space\nand match it with a relative caption. Our goal is to learn a Composition\nmapping with the Composition Network. Since this one-way mapping is highly\nunder-constrained, we couple it with an inverse relation learning with the\nCorrection Network and introduce a cycled relation for given Image We\nparticipate in Fashion IQ 2020 challenge and have won the first place with the\nensemble of our model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 01:59:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Kim", "Jongseok", ""], ["Yu", "Youngjae", ""], ["Lee", "Seunghwan", ""], ["GunheeKim", "", ""]]}, {"id": "2107.11517", "submitter": "Yinghuan Shi", "authors": "Qian Yu, Lei Qi, Luping Zhou, Lei Wang, Yilong Yin, Yinghuan Shi,\n  Wuzhang Wang, Yang Gao", "title": "Crosslink-Net: Double-branch Encoder Segmentation Network via Fusing\n  Vertical and Horizontal Convolutions", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate image segmentation plays a crucial role in medical image analysis,\nyet it faces great challenges of various shapes, diverse sizes, and blurry\nboundaries. To address these difficulties, square kernel-based encoder-decoder\narchitecture has been proposed and widely used, but its performance remains\nstill unsatisfactory. To further cope with these challenges, we present a novel\ndouble-branch encoder architecture. Our architecture is inspired by two\nobservations: 1) Since the discrimination of features learned via square\nconvolutional kernels needs to be further improved, we propose to utilize\nnon-square vertical and horizontal convolutional kernels in the double-branch\nencoder, so features learned by the two branches can be expected to complement\neach other. 2) Considering that spatial attention can help models to better\nfocus on the target region in a large-sized image, we develop an attention loss\nto further emphasize the segmentation on small-sized targets. Together, the\nabove two schemes give rise to a novel double-branch encoder segmentation\nframework for medical image segmentation, namely Crosslink-Net. The experiments\nvalidate the effectiveness of our model on four datasets. The code is released\nat https://github.com/Qianyu1226/Crosslink-Net.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 02:58:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yu", "Qian", ""], ["Qi", "Lei", ""], ["Zhou", "Luping", ""], ["Wang", "Lei", ""], ["Yin", "Yilong", ""], ["Shi", "Yinghuan", ""], ["Wang", "Wuzhang", ""], ["Gao", "Yang", ""]]}, {"id": "2107.11522", "submitter": "Xiujun Shu", "authors": "Xiujun Shu, Ge Li, Xiao Wang, Weijian Ruan, Qi Tian", "title": "Semantic-guided Pixel Sampling for Cloth-Changing Person\n  Re-identification", "comments": "This paper has been published on IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2021.3091924", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloth-changing person re-identification (re-ID) is a new rising research\ntopic that aims at retrieving pedestrians whose clothes are changed. This task\nis quite challenging and has not been fully studied to date. Current works\nmainly focus on body shape or contour sketch, but they are not robust enough\ndue to view and posture variations. The key to this task is to exploit\ncloth-irrelevant cues. This paper proposes a semantic-guided pixel sampling\napproach for the cloth-changing person re-ID task. We do not explicitly define\nwhich feature to extract but force the model to automatically learn\ncloth-irrelevant cues. Specifically, we first recognize the pedestrian's upper\nclothes and pants, then randomly change them by sampling pixels from other\npedestrians. The changed samples retain the identity labels but exchange the\npixels of clothes or pants among different pedestrians. Besides, we adopt a\nloss function to constrain the learned features to keep consistent before and\nafter changes. In this way, the model is forced to learn cues that are\nirrelevant to upper clothes and pants. We conduct extensive experiments on the\nlatest released PRCC dataset. Our method achieved 65.8% on Rank1 accuracy,\nwhich outperforms previous methods with a large margin. The code is available\nat https://github.com/shuxjweb/pixel_sampling.git.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 03:41:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shu", "Xiujun", ""], ["Li", "Ge", ""], ["Wang", "Xiao", ""], ["Ruan", "Weijian", ""], ["Tian", "Qi", ""]]}, {"id": "2107.11566", "submitter": "Olga Moskvyak", "authors": "Olga Moskvyak, Frederic Maire, Feras Dayoub, Mahsa Baktashmotlagh", "title": "Going Deeper into Semi-supervised Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person re-identification is the challenging task of identifying a person\nacross different camera views. Training a convolutional neural network (CNN)\nfor this task requires annotating a large dataset, and hence, it involves the\ntime-consuming manual matching of people across cameras. To reduce the need for\nlabeled data, we focus on a semi-supervised approach that requires only a\nsubset of the training data to be labeled. We conduct a comprehensive survey in\nthe area of person re-identification with limited labels. Existing works in\nthis realm are limited in the sense that they utilize features from multiple\nCNNs and require the number of identities in the unlabeled data to be known. To\novercome these limitations, we propose to employ part-based features from a\nsingle CNN without requiring the knowledge of the label space (i.e., the number\nof identities). This makes our approach more suitable for practical scenarios,\nand it significantly reduces the need for computational resources. We also\npropose a PartMixUp loss that improves the discriminative ability of learned\npart-based features for pseudo-labeling in semi-supervised settings. Our method\noutperforms the state-of-the-art results on three large-scale person re-id\ndatasets and achieves the same level of performance as fully supervised methods\nwith only one-third of labeled identities.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 09:28:13 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Moskvyak", "Olga", ""], ["Maire", "Frederic", ""], ["Dayoub", "Feras", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2107.11574", "submitter": "Jixiong Pu", "authors": "Xuetian Lai, Qiongyao Li, Ziyang Chen, Xiaopeng Shao, and Jixiong Pu", "title": "Reconstructing Images of Two Adjacent Objects through Scattering Medium\n  Using Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstruction of image by using convolutional neural networks (CNNs) has\nbeen vigorously studied in the last decade. Until now, there have being\ndeveloped several techniques for imaging of a single object through scattering\nmedium by using neural networks, however how to reconstruct images of more than\none object simultaneously seems hard to realize. In this paper, we demonstrate\nan approach by using generative adversarial network (GAN) to reconstruct images\nof two adjacent objects through scattering media. We construct an imaging\nsystem for imaging of two adjacent objects behind the scattering media. In\ngeneral, as the light field of two adjacent object images pass through the\nscattering slab, a speckle pattern is obtained. The designed adversarial\nnetwork, which is called as YGAN, is employed to reconstruct the images\nsimultaneously. It is shown that based on the trained YGAN, we can reconstruct\nimages of two adjacent objects from one speckle pattern with high fidelity. In\naddition, we study the influence of the object image types, and the distance\nbetween the two adjacent objects on the fidelity of the reconstructed images.\nMoreover even if another scattering medium is inserted between the two objects,\nwe can also reconstruct the images of two objects from a speckle with high\nquality. The technique presented in this work can be used for applications in\nareas of medical image analysis, such as medical image classification,\nsegmentation, and studies of multi-object scattering imaging etc.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 10:04:18 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lai", "Xuetian", ""], ["Li", "Qiongyao", ""], ["Chen", "Ziyang", ""], ["Shao", "Xiaopeng", ""], ["Pu", "Jixiong", ""]]}, {"id": "2107.11576", "submitter": "Jingjing Jiang", "authors": "Jingjing Jiang, Ziyi Liu, Yifan Liu, Zhixiong Nan, and Nanning Zheng", "title": "X-GGM: Graph Generative Modeling for Out-of-Distribution Generalization\n  in Visual Question Answering", "comments": "Accepted by ACM MM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Encouraging progress has been made towards Visual Question Answering (VQA) in\nrecent years, but it is still challenging to enable VQA models to adaptively\ngeneralize to out-of-distribution (OOD) samples. Intuitively, recompositions of\nexisting visual concepts (i.e., attributes and objects) can generate unseen\ncompositions in the training set, which will promote VQA models to generalize\nto OOD samples. In this paper, we formulate OOD generalization in VQA as a\ncompositional generalization problem and propose a graph generative\nmodeling-based training scheme (X-GGM) to handle the problem implicitly. X-GGM\nleverages graph generative modeling to iteratively generate a relation matrix\nand node representations for the predefined graph that utilizes\nattribute-object pairs as nodes. Furthermore, to alleviate the unstable\ntraining issue in graph generative modeling, we propose a gradient distribution\nconsistency loss to constrain the data distribution with adversarial\nperturbations and the generated distribution. The baseline VQA model (LXMERT)\ntrained with the X-GGM scheme achieves state-of-the-art OOD performance on two\nstandard VQA OOD benchmarks, i.e., VQA-CP v2 and GQA-OOD. Extensive ablation\nstudies demonstrate the effectiveness of X-GGM components.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 10:17:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jiang", "Jingjing", ""], ["Liu", "Ziyi", ""], ["Liu", "Yifan", ""], ["Nan", "Zhixiong", ""], ["Zheng", "Nanning", ""]]}, {"id": "2107.11585", "submitter": "Rupak Bose Mr.", "authors": "Rupak Bose, Shivam Pande, Biplab Banerjee", "title": "Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions", "comments": "Accepted in IEEE International conference on Image Processing (ICIP),\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the field of remote sensing is evolving, we witness the accumulation of\ninformation from several modalities, such as multispectral (MS), hyperspectral\n(HSI), LiDAR etc. Each of these modalities possess its own distinct\ncharacteristics and when combined synergistically, perform very well in the\nrecognition and classification tasks. However, fusing multiple modalities in\nremote sensing is cumbersome due to highly disparate domains. Furthermore, the\nexisting methods do not facilitate cross-modal interactions. To this end, we\npropose a novel transformer based fusion method for HSI and LiDAR modalities.\nThe model is composed of stacked auto encoders that harness the cross key-value\npairs for HSI and LiDAR, thus establishing a communication between the two\nmodalities, while simultaneously using the CNNs to extract the spectral and\nspatial information from HSI and LiDAR. We test our model on Houston (Data\nFusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 11:33:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bose", "Rupak", ""], ["Pande", "Shivam", ""], ["Banerjee", "Biplab", ""]]}, {"id": "2107.11617", "submitter": "Liang-Jian Deng", "authors": "Zi-Rong Jin and Liang-Jian Deng and Tai-Xiang Jiang and Tian-Jing\n  Zhang", "title": "LAConv: Local Adaptive Convolution for Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The convolution operation is a powerful tool for feature extraction and plays\na prominent role in the field of computer vision. However, when targeting the\npixel-wise tasks like image fusion, it would not fully perceive the\nparticularity of each pixel in the image if the uniform convolution kernel is\nused on different patches. In this paper, we propose a local adaptive\nconvolution (LAConv), which is dynamically adjusted to different spatial\nlocations. LAConv enables the network to pay attention to every specific local\narea in the learning process. Besides, the dynamic bias (DYB) is introduced to\nprovide more possibilities for the depiction of features and make the network\nmore flexible. We further design a residual structure network equipped with the\nproposed LAConv and DYB modules, and apply it to two image fusion tasks.\nExperiments for pansharpening and hyperspectral image super-resolution (HISR)\ndemonstrate the superiority of our method over other state-of-the-art methods.\nIt is worth mentioning that LAConv can also be competent for other\nsuper-resolution tasks with less computation effort.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 14:15:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jin", "Zi-Rong", ""], ["Deng", "Liang-Jian", ""], ["Jiang", "Tai-Xiang", ""], ["Zhang", "Tian-Jing", ""]]}, {"id": "2107.11626", "submitter": "Son Dao", "authors": "Son D.Dao, Ethan Zhao, Dinh Phung, Jianfei Cai", "title": "Multi-Label Image Classification with Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, as an effective way of learning latent representations, contrastive\nlearning has been increasingly popular and successful in various domains. The\nsuccess of constrastive learning in single-label classifications motivates us\nto leverage this learning framework to enhance distinctiveness for better\nperformance in multi-label image classification. In this paper, we show that a\ndirect application of contrastive learning can hardly improve in multi-label\ncases. Accordingly, we propose a novel framework for multi-label classification\nwith contrastive learning in a fully supervised setting, which learns multiple\nrepresentations of an image under the context of different labels. This\nfacilities a simple yet intuitive adaption of contrastive learning into our\nmodel to boost its performance in multi-label image classification. Extensive\nexperiments on two benchmark datasets show that the proposed framework achieves\nstate-of-the-art performance in the comparison with the advanced methods in\nmulti-label classification.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:00:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Dao", "Son D.", ""], ["Zhao", "Ethan", ""], ["Phung", "Dinh", ""], ["Cai", "Jianfei", ""]]}, {"id": "2107.11627", "submitter": "Zhiyuan Mao", "authors": "Zhiyuan Mao and Nicholas Chimitt and Stanley H. Chan", "title": "Accelerating Atmospheric Turbulence Simulation via Learned\n  Phase-to-Space Transform", "comments": "The paper will be published at the ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate simulation of imaging through atmospheric turbulence is\nessential for developing turbulence mitigation algorithms. Recognizing the\nlimitations of previous approaches, we introduce a new concept known as the\nphase-to-space (P2S) transform to significantly speed up the simulation. P2S is\nbuild upon three ideas: (1) reformulating the spatially varying convolution as\na set of invariant convolutions with basis functions, (2) learning the basis\nfunction via the known turbulence statistics models, (3) implementing the P2S\ntransform via a light-weight network that directly convert the phase\nrepresentation to spatial representation. The new simulator offers 300x --\n1000x speed up compared to the mainstream split-step simulators while\npreserving the essential turbulence statistics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:05:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mao", "Zhiyuan", ""], ["Chimitt", "Nicholas", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2107.11629", "submitter": "Ge-Peng Ji", "authors": "Yi Zhang, Fang-Yi Chao, Ge-Peng Ji, Deng-Ping Fan, Lu Zhang, Ling Shao", "title": "ASOD60K: Audio-Induced Salient Object Detection in Panoramic Videos", "comments": "22 pages, 17 figures, 7 tables (Project Page:\n  https://github.com/PanoAsh/ASOD60K)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploring to what humans pay attention in dynamic panoramic scenes is useful\nfor many fundamental applications, including augmented reality (AR) in retail,\nAR-powered recruitment, and visual language navigation. With this goal in mind,\nwe propose PV-SOD, a new task that aims to segment salient objects from\npanoramic videos. In contrast to existing fixation-level or object-level\nsaliency detection tasks, we focus on multi-modal salient object detection\n(SOD), which mimics human attention mechanism by segmenting salient objects\nwith the guidance of audio-visual cues. To support this task, we collect the\nfirst large-scale dataset, named ASOD60K, which contains 4K-resolution video\nframes annotated with a six-level hierarchy, thus distinguishing itself with\nrichness, diversity and quality. Specifically, each sequence is marked with\nboth its super-/sub-class, with objects of each sub-class being further\nannotated with human eye fixations, bounding boxes, object-/instance-level\nmasks, and associated attributes (e.g., geometrical distortion). These\ncoarse-to-fine annotations enable detailed analysis for PV-SOD modeling, e.g.,\ndetermining the major challenges for existing SOD models, and predicting\nscanpaths to study the long-term eye fixation behaviors of humans. We\nsystematically benchmark 11 representative approaches on ASOD60K and derive\nseveral interesting findings. We hope this study could serve as a good starting\npoint for advancing SOD research towards panoramic videos.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:14:20 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Yi", ""], ["Chao", "Fang-Yi", ""], ["Ji", "Ge-Peng", ""], ["Fan", "Deng-Ping", ""], ["Zhang", "Lu", ""], ["Shao", "Ling", ""]]}, {"id": "2107.11635", "submitter": "Kien Do", "authors": "Kien Do, Truyen Tran, Svetha Venkatesh", "title": "Clustering by Maximizing Mutual Information Across Views", "comments": "Accepted at ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel framework for image clustering that incorporates joint\nrepresentation learning and clustering. Our method consists of two heads that\nshare the same backbone network - a \"representation learning\" head and a\n\"clustering\" head. The \"representation learning\" head captures fine-grained\npatterns of objects at the instance level which serve as clues for the\n\"clustering\" head to extract coarse-grain information that separates objects\ninto clusters. The whole model is trained in an end-to-end manner by minimizing\nthe weighted sum of two sample-oriented contrastive losses applied to the\noutputs of the two heads. To ensure that the contrastive loss corresponding to\nthe \"clustering\" head is optimal, we introduce a novel critic function called\n\"log-of-dot-product\". Extensive experimental results demonstrate that our\nmethod significantly outperforms state-of-the-art single-stage clustering\nmethods across a variety of image datasets, improving over the best baseline by\nabout 5-7% in accuracy on CIFAR10/20, STL10, and ImageNet-Dogs. Further, the\n\"two-stage\" variant of our method also achieves better results than baselines\non three challenging ImageNet subsets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:36:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2107.11639", "submitter": "Yuan Tian", "authors": "Yuan Tian, Guo Lu, Xiongkuo Min, Zhaohui Che, Guangtao Zhai, Guodong\n  Guo, Zhiyong Gao", "title": "Self-Conditioned Probabilistic Learning of Video Rescaling", "comments": "accepted to ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bicubic downscaling is a prevalent technique used to reduce the video storage\nburden or to accelerate the downstream processing speed. However, the inverse\nupscaling step is non-trivial, and the downscaled video may also deteriorate\nthe performance of downstream tasks. In this paper, we propose a\nself-conditioned probabilistic framework for video rescaling to learn the\npaired downscaling and upscaling procedures simultaneously. During the\ntraining, we decrease the entropy of the information lost in the downscaling by\nmaximizing its probability conditioned on the strong spatial-temporal prior\ninformation within the downscaled video. After optimization, the downscaled\nvideo by our framework preserves more meaningful information, which is\nbeneficial for both the upscaling step and the downstream tasks, e.g., video\naction recognition task. We further extend the framework to a lossy video\ncompression system, in which a gradient estimator for non-differential\nindustrial lossy codecs is proposed for the end-to-end training of the whole\nsystem. Extensive experimental results demonstrate the superiority of our\napproach on video rescaling, video compression, and efficient action\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:57:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tian", "Yuan", ""], ["Lu", "Guo", ""], ["Min", "Xiongkuo", ""], ["Che", "Zhaohui", ""], ["Zhai", "Guangtao", ""], ["Guo", "Guodong", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2107.11640", "submitter": "Mohamed Shehata", "authors": "Mohamed Shehata, Mohamed Taha Abou-Kreisha, Hany Elnashar", "title": "Deep Machine Learning Based Egyptian Vehicle License Plate Recognition\n  Systems", "comments": "8 Pages, 20 Figures, 5 Tables, Published with Al-Azhar Engineering\n  Fifteenth International Conference (AEIC 2021)", "journal-ref": "Al-Azhar Engineering Fifteenth International Conference (March\n  2021): 69-76", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated Vehicle License Plate (VLP) detection and recognition have ended up\nbeing a significant research issue as of late. VLP localization and recognition\nare some of the most essential techniques for managing traffic using digital\ntechniques. In this paper, four smart systems are developed to recognize\nEgyptian vehicles license plates. Two systems are based on character\nrecognition, which are (System1, Characters Recognition with Classical Machine\nLearning) and (System2, Characters Recognition with Deep Machine Learning). The\nother two systems are based on the whole plate recognition which are (System3,\nWhole License Plate Recognition with Classical Machine Learning) and (System4,\nWhole License Plate Recognition with Deep Machine Learning). We use object\ndetection algorithms, and machine learning based object recognition algorithms.\nThe performance of the developed systems has been tested on real images, and\nthe experimental results demonstrate that the best detection accuracy rate for\nVLP is provided by using the deep learning method. Where the VLP detection\naccuracy rate is better than the classical system by 32%. However, the best\ndetection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is\nprovided by using the classical method. Where VLPAC detection accuracy rate is\nbetter than the deep learning-based system by 6%. Also, the results show that\ndeep learning is better than the classical technique used in VLP recognition\nprocesses. Where the recognition accuracy rate is better than the classical\nsystem by 8%. Finally, the paper output recommends a robust VLP recognition\nsystem based on both statistical and deep machine learning.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:58:01 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shehata", "Mohamed", ""], ["Abou-Kreisha", "Mohamed Taha", ""], ["Elnashar", "Hany", ""]]}, {"id": "2107.11643", "submitter": "Abbas Khosravi", "authors": "Maryam Habibpour, Hassan Gharoun, AmirReza Tajally, Afshar Shamsi,\n  Hamzeh Asgharnezhad, Abbas Khosravi, and Saeid Nahavandi", "title": "An Uncertainty-Aware Deep Learning Framework for Defect Detection in\n  Casting Products", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defects are unavoidable in casting production owing to the complexity of the\ncasting process. While conventional human-visual inspection of casting products\nis slow and unproductive in mass productions, an automatic and reliable defect\ndetection not just enhances the quality control process but positively improves\nproductivity. However, casting defect detection is a challenging task due to\ndiversity and variation in defects' appearance. Convolutional neural networks\n(CNNs) have been widely applied in both image classification and defect\ndetection tasks. Howbeit, CNNs with frequentist inference require a massive\namount of data to train on and still fall short in reporting beneficial\nestimates of their predictive uncertainty. Accordingly, leveraging the transfer\nlearning paradigm, we first apply four powerful CNN-based models (VGG16,\nResNet50, DenseNet121, and InceptionResNetV2) on a small dataset to extract\nmeaningful features. Extracted features are then processed by various machine\nlearning algorithms to perform the classification task. Simulation results\ndemonstrate that linear support vector machine (SVM) and multi-layer perceptron\n(MLP) show the finest performance in defect detection of casting images.\nSecondly, to achieve a reliable classification and to measure epistemic\nuncertainty, we employ an uncertainty quantification (UQ) technique (ensemble\nof MLP models) using features extracted from four pre-trained CNNs. UQ\nconfusion matrix and uncertainty accuracy metric are also utilized to evaluate\nthe predictive uncertainty estimates. Comprehensive comparisons reveal that UQ\nmethod based on VGG16 outperforms others to fetch uncertainty. We believe an\nuncertainty-aware automatic defect detection solution will reinforce casting\nproductions quality assurance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 16:17:20 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Habibpour", "Maryam", ""], ["Gharoun", "Hassan", ""], ["Tajally", "AmirReza", ""], ["Shamsi", "Afshar", ""], ["Asgharnezhad", "Hamzeh", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2107.11645", "submitter": "Yanwen Fang", "authors": "Wenming Cao, Philip L.H. Yu, Gilbert C.S. Lui, Keith W.H. Chiu,\n  Ho-Ming Cheng, Yanwen Fang, Man-Fung Yuen, Wai-Kay Seto", "title": "Dual-Attention Enhanced BDense-UNet for Liver Lesion Segmentation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a new segmentation network by integrating DenseUNet\nand bidirectional LSTM together with attention mechanism, termed as\nDA-BDense-UNet. DenseUNet allows learning enough diverse features and enhancing\nthe representative power of networks by regulating the information flow.\nBidirectional LSTM is responsible to explore the relationships between the\nencoded features and the up-sampled features in the encoding and decoding\npaths. Meanwhile, we introduce attention gates (AG) into DenseUNet to diminish\nresponses of unrelated background regions and magnify responses of salient\nregions progressively. Besides, the attention in bidirectional LSTM takes into\naccount the contribution differences of the encoded features and the up-sampled\nfeatures in segmentation improvement, which can in turn adjust proper weights\nfor these two kinds of features. We conduct experiments on liver CT image data\nsets collected from multiple hospitals by comparing them with state-of-the-art\nsegmentation models. Experimental results indicate that our proposed method\nDA-BDense-UNet has achieved comparative performance in terms of dice\ncoefficient, which demonstrates its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 16:28:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cao", "Wenming", ""], ["Yu", "Philip L. H.", ""], ["Lui", "Gilbert C. S.", ""], ["Chiu", "Keith W. H.", ""], ["Cheng", "Ho-Ming", ""], ["Fang", "Yanwen", ""], ["Yuen", "Man-Fung", ""], ["Seto", "Wai-Kay", ""]]}, {"id": "2107.11646", "submitter": "Xiong Zhang", "authors": "Xiong Zhang, Hongsheng Huang, Jianchao Tan, Hongmin Xu, Cheng Yang,\n  Guozhu Peng, Lei Wang, Ji Liu", "title": "Hand Image Understanding via Deep Multi-Task Learning", "comments": "Accepted By ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing and understanding hand information from multimedia materials like\nimages or videos is important for many real world applications and remains\nactive in research community. There are various works focusing on recovering\nhand information from single image, however, they usually solve a single task,\nfor example, hand mask segmentation, 2D/3D hand pose estimation, or hand mesh\nreconstruction and perform not well in challenging scenarios. To further\nimprove the performance of these tasks, we propose a novel Hand Image\nUnderstanding (HIU) framework to extract comprehensive information of the hand\nobject from a single RGB image, by jointly considering the relationships\nbetween these tasks. To achieve this goal, a cascaded multi-task learning (MTL)\nbackbone is designed to estimate the 2D heat maps, to learn the segmentation\nmask, and to generate the intermediate 3D information encoding, followed by a\ncoarse-to-fine learning paradigm and a self-supervised learning strategy.\nQualitative experiments demonstrate that our approach is capable of recovering\nreasonable mesh representations even in challenging situations. Quantitatively,\nour method significantly outperforms the state-of-the-art approaches on various\nwidely-used datasets, in terms of diverse evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 16:28:06 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 07:16:15 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Xiong", ""], ["Huang", "Hongsheng", ""], ["Tan", "Jianchao", ""], ["Xu", "Hongmin", ""], ["Yang", "Cheng", ""], ["Peng", "Guozhu", ""], ["Wang", "Lei", ""], ["Liu", "Ji", ""]]}, {"id": "2107.11669", "submitter": "Kemal Oksuz", "authors": "Kemal Oksuz and Baris Can Cam and Emre Akbas and Sinan Kalkan", "title": "Rank & Sort Loss for Object Detection and Instance Segmentation", "comments": "ICCV 2021, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Rank & Sort (RS) Loss, as a ranking-based loss function to train\ndeep object detection and instance segmentation methods (i.e. visual\ndetectors). RS Loss supervises the classifier, a sub-network of these methods,\nto rank each positive above all negatives as well as to sort positives among\nthemselves with respect to (wrt.) their continuous localisation qualities (e.g.\nIntersection-over-Union - IoU). To tackle the non-differentiable nature of\nranking and sorting, we reformulate the incorporation of error-driven update\nwith backpropagation as Identity Update, which enables us to model our novel\nsorting error among positives. With RS Loss, we significantly simplify\ntraining: (i) Thanks to our sorting objective, the positives are prioritized by\nthe classifier without an additional auxiliary head (e.g. for centerness, IoU,\nmask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class\nimbalance, and thus, no sampling heuristic is required, and (iii) we address\nthe multi-task nature of visual detectors using tuning-free task-balancing\ncoefficients. Using RS Loss, we train seven diverse visual detectors only by\ntuning the learning rate, and show that it consistently outperforms baselines:\ne.g. our RS Loss improves (i) Faster R-CNN by ~ 3 box AP and aLRP Loss\n(ranking-based baseline) by ~ 2 box AP on COCO dataset, (ii) Mask R-CNN with\nrepeat factor sampling (RFS) by 3.5 mask AP (~ 7 AP for rare classes) on LVIS\ndataset; and also outperforms all counterparts. Code available at\nhttps://github.com/kemaloksuz/RankSortLoss\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 18:44:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Oksuz", "Kemal", ""], ["Cam", "Baris Can", ""], ["Akbas", "Emre", ""], ["Kalkan", "Sinan", ""]]}, {"id": "2107.11671", "submitter": "Ali Rahmati", "authors": "Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Huaiyu Dai", "title": "Adversarial training may be a double-edged sword", "comments": "Presented as a RobustML workshop paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial training has been shown as an effective approach to improve the\nrobustness of image classifiers against white-box attacks. However, its\neffectiveness against black-box attacks is more nuanced. In this work, we\ndemonstrate that some geometric consequences of adversarial training on the\ndecision boundary of deep networks give an edge to certain types of black-box\nattacks. In particular, we define a metric called robustness gain to show that\nwhile adversarial training is an effective method to dramatically improve the\nrobustness in white-box scenarios, it may not provide such a good robustness\ngain against the more realistic decision-based black-box attacks. Moreover, we\nshow that even the minimal perturbation white-box attacks can converge faster\nagainst adversarially-trained neural networks compared to the regular ones.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 19:09:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Rahmati", "Ali", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Dai", "Huaiyu", ""]]}, {"id": "2107.11696", "submitter": "Saul Calderon Ramirez", "authors": "Saul Calderon-Ramirez, Diego Murillo-Hernandez, Kevin Rojas-Salazar,\n  David Elizondo, Shengxiang Yang, Miguel Molina-Cabello", "title": "A Real Use Case of Semi-Supervised Learning for Mammogram Classification\n  in a Local Clinic of Costa Rica", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The implementation of deep learning based computer aided diagnosis systems\nfor the classification of mammogram images can help in improving the accuracy,\nreliability, and cost of diagnosing patients. However, training a deep learning\nmodel requires a considerable amount of labeled images, which can be expensive\nto obtain as time and effort from clinical practitioners is required. A number\nof publicly available datasets have been built with data from different\nhospitals and clinics. However, using models trained on these datasets for\nlater work on images sampled from a different hospital or clinic might result\nin lower performance. This is due to the distribution mismatch of the datasets,\nwhich include different patient populations and image acquisition protocols.\nThe scarcity of labeled data can also bring a challenge towards the application\nof transfer learning with models trained using these source datasets. In this\nwork, a real world scenario is evaluated where a novel target dataset sampled\nfrom a private Costa Rican clinic is used, with few labels and heavily\nimbalanced data. The use of two popular and publicly available datasets\n(INbreast and CBIS-DDSM) as source data, to train and test the models on the\nnovel target dataset, is evaluated. The use of the semi-supervised deep\nlearning approach known as MixMatch, to leverage the usage of unlabeled data\nfrom the target dataset, is proposed and evaluated. In the tests, the\nperformance of models is extensively measured, using different metrics to\nassess the performance of a classifier under heavy data imbalance conditions.\nIt is shown that the use of semi-supervised deep learning combined with\nfine-tuning can provide a meaningful advantage when using scarce labeled\nobservations. We make available the novel dataset for the benefit of the\ncommunity.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 22:26:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Calderon-Ramirez", "Saul", ""], ["Murillo-Hernandez", "Diego", ""], ["Rojas-Salazar", "Kevin", ""], ["Elizondo", "David", ""], ["Yang", "Shengxiang", ""], ["Molina-Cabello", "Miguel", ""]]}, {"id": "2107.11707", "submitter": "Nasib Ullah", "authors": "Nasibullah, Partha Pratim Mohanta", "title": "Boosting Video Captioning with Dynamic Loss Network", "comments": "10 pages, 3 figures, Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video captioning is one of the challenging problems at the intersection of\nvision and language, having many real-life applications in video retrieval,\nvideo surveillance, assisting visually challenged people, Human-machine\ninterface, and many more. Recent deep learning-based methods have shown\npromising results but are still on the lower side than other vision tasks (such\nas image classification, object detection). A significant drawback with\nexisting video captioning methods is that they are optimized over cross-entropy\nloss function, which is uncorrelated to the de facto evaluation metrics (BLEU,\nMETEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate\nof the true loss function for video captioning. This paper addresses the\ndrawback by introducing a dynamic loss network (DLN), which provides an\nadditional feedback signal that directly reflects the evaluation metrics. Our\nresults on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to\nText (MSRVTT) datasets outperform previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 01:32:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Nasibullah", "", ""], ["Mohanta", "Partha Pratim", ""]]}, {"id": "2107.11711", "submitter": "Man Yao", "authors": "Man Yao, Huanhuan Gao, Guangshe Zhao, Dingheng Wang, Yihan Lin, Zhaoxu\n  Yang, Guoqi Li", "title": "Temporal-wise Attention Spiking Neural Networks for Event Streams\n  Classification", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to effectively and efficiently deal with spatio-temporal event streams,\nwhere the events are generally sparse and non-uniform and have the microsecond\ntemporal resolution, is of great value and has various real-life applications.\nSpiking neural network (SNN), as one of the brain-inspired event-triggered\ncomputing models, has the potential to extract effective spatio-temporal\nfeatures from the event streams. However, when aggregating individual events\ninto frames with a new higher temporal resolution, existing SNN models do not\nattach importance to that the serial frames have different signal-to-noise\nratios since event streams are sparse and non-uniform. This situation\ninterferes with the performance of existing SNNs. In this work, we propose a\ntemporal-wise attention SNN (TA-SNN) model to learn frame-based representation\nfor processing event streams. Concretely, we extend the attention concept to\ntemporal-wise input to judge the significance of frames for the final decision\nat the training stage, and discard the irrelevant frames at the inference\nstage. We demonstrate that TA-SNN models improve the accuracy of event streams\nclassification tasks. We also study the impact of multiple-scale temporal\nresolutions for frame-based representation. Our approach is tested on three\ndifferent classification tasks: gesture recognition, image classification, and\nspoken digit recognition. We report the state-of-the-art results on these\ntasks, and get the essential improvement of accuracy (almost 19\\%) for gesture\nrecognition with only 60 ms.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 02:28:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yao", "Man", ""], ["Gao", "Huanhuan", ""], ["Zhao", "Guangshe", ""], ["Wang", "Dingheng", ""], ["Lin", "Yihan", ""], ["Yang", "Zhaoxu", ""], ["Li", "Guoqi", ""]]}, {"id": "2107.11721", "submitter": "Qiang Meng", "authors": "Qiang Meng, Xiaqing Xu, Xiaobo Wang, Yang Qian, Yunxiao Qin, Zezheng\n  Wang, Chenxu Zhao, Feng Zhou, Zhen Lei", "title": "PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success achieved by deep learning methods in face\nrecognition, severe performance drops are observed for large pose variations in\nunconstrained environments (e.g., in cases of surveillance and photo-tagging).\nTo address it, current methods either deploy pose-specific models or frontalize\nfaces by additional modules. Still, they ignore the fact that identity\ninformation should be consistent across poses and are not realizing the data\nimbalance between frontal and profile face images during training. In this\npaper, we propose an efficient PoseFace framework which utilizes the facial\nlandmarks to disentangle the pose-invariant features and exploits a\npose-adaptive loss to handle the imbalance issue adaptively. Extensive\nexperimental results on the benchmarks of Multi-PIE, CFP, CPLFW and IJB have\ndemonstrated the superiority of our method over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 03:50:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Meng", "Qiang", ""], ["Xu", "Xiaqing", ""], ["Wang", "Xiaobo", ""], ["Qian", "Yang", ""], ["Qin", "Yunxiao", ""], ["Wang", "Zezheng", ""], ["Zhao", "Chenxu", ""], ["Zhou", "Feng", ""], ["Lei", "Zhen", ""]]}, {"id": "2107.11750", "submitter": "Yeli Feng", "authors": "Yeli Feng, Daniel Jun Xian Ng, Arvind Easwaran", "title": "Improving Variational Autoencoder based Out-of-Distribution Detection\n  for Embedded Real-time Applications", "comments": null, "journal-ref": null, "doi": "10.1145/3477026", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainties in machine learning are a significant roadblock for its\napplication in safety-critical cyber-physical systems (CPS). One source of\nuncertainty arises from distribution shifts in the input data between training\nand test scenarios. Detecting such distribution shifts in real-time is an\nemerging approach to address the challenge. The high dimensional input space in\nCPS applications involving imaging adds extra difficulty to the task.\nGenerative learning models are widely adopted for the task, namely\nout-of-distribution (OoD) detection. To improve the state-of-the-art, we\nstudied existing proposals from both machine learning and CPS fields. In the\nlatter, safety monitoring in real-time for autonomous driving agents has been a\nfocus. Exploiting the spatiotemporal correlation of motion in videos, we can\nrobustly detect hazardous motion around autonomous driving agents. Inspired by\nthe latest advances in the Variational Autoencoder (VAE) theory and practice,\nwe tapped into the prior knowledge in data to further boost OoD detection's\nrobustness. Comparison studies over nuScenes and Synthia data sets show our\nmethods significantly improve detection capabilities of OoD factors unique to\ndriving scenarios, 42% better than state-of-the-art approaches. Our model also\ngeneralized near-perfectly, 97% better than the state-of-the-art across the\nreal-world and simulation driving data sets experimented. Finally, we\ncustomized one proposed method into a twin-encoder model that can be deployed\nto resource limited embedded devices for real-time OoD detection. Its execution\ntime was reduced over four times in low-precision 8-bit integer inference,\nwhile detection capability is comparable to its corresponding floating-point\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 07:52:53 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Feng", "Yeli", ""], ["Ng", "Daniel Jun Xian", ""], ["Easwaran", "Arvind", ""]]}, {"id": "2107.11756", "submitter": "Yuqian Fu", "authors": "Yuqian Fu, Yanwei Fu, Yu-Gang Jiang", "title": "Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics\n  from Videos", "comments": "accpected by ICMR2021", "journal-ref": null, "doi": "10.1145/3460426.3463609", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video demonstration, can we imitate the action contained in this\nvideo? In this paper, we introduce a novel task, dubbed mesh-based action\nimitation. The goal of this task is to enable an arbitrary target human mesh to\nperform the same action shown on the video demonstration. To achieve this, a\nnovel Mesh-based Video Action Imitation (M-VAI) method is proposed by us. M-VAI\nfirst learns to reconstruct the meshes from the given source image frames, then\nthe initial recovered mesh sequence is fed into mesh2mesh, a mesh sequence\nsmooth module proposed by us, to improve the temporal consistency. Finally, we\nimitate the actions by transferring the pose from the constructed human body to\nour target identity mesh. High-quality and detailed human body meshes can be\ngenerated by using our M-VAI. Extensive experiments demonstrate the feasibility\nof our task and the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 08:42:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fu", "Yuqian", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2107.11758", "submitter": "Tianyang Zhang", "authors": "Tianyang Zhang, Xiangrong Zhang, Peng Zhu, Xu Tang, Chen Li, Licheng\n  Jiao, and Huiyu Zhou", "title": "Semantic Attention and Scale Complementary Network for Instance\n  Segmentation in Remote Sensing Images", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the challenging multicategory instance\nsegmentation problem in remote sensing images (RSIs), which aims at predicting\nthe categories of all instances and localizing them with pixel-level masks.\nAlthough many landmark frameworks have demonstrated promising performance in\ninstance segmentation, the complexity in the background and scale variability\ninstances still remain challenging for instance segmentation of RSIs. To\naddress the above problems, we propose an end-to-end multi-category instance\nsegmentation model, namely Semantic Attention and Scale Complementary Network,\nwhich mainly consists of a Semantic Attention (SEA) module and a Scale\nComplementary Mask Branch (SCMB). The SEA module contains a simple fully\nconvolutional semantic segmentation branch with extra supervision to strengthen\nthe activation of interest instances on the feature map and reduce the\nbackground noise's interference. To handle the under-segmentation of geospatial\ninstances with large varying scales, we design the SCMB that extends the\noriginal single mask branch to trident mask branches and introduces\ncomplementary mask supervision at different scales to sufficiently leverage the\nmulti-scale information. We conduct comprehensive experiments to evaluate the\neffectiveness of our proposed method on the iSAID dataset and the NWPU Instance\nSegmentation dataset and achieve promising performance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 08:53:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Tianyang", ""], ["Zhang", "Xiangrong", ""], ["Zhu", "Peng", ""], ["Tang", "Xu", ""], ["Li", "Chen", ""], ["Jiao", "Licheng", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2107.11769", "submitter": "Tsung-Han Wu", "authors": "Tsung-Han Wu, Yueh-Cheng Liu, Yu-Kai Huang, Hsin-Ying Lee, Hung-Ting\n  Su, Ping-Chia Huang, Winston H. Hsu", "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud\n  Semantic Segmentation", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 09:40:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Tsung-Han", ""], ["Liu", "Yueh-Cheng", ""], ["Huang", "Yu-Kai", ""], ["Lee", "Hsin-Ying", ""], ["Su", "Hung-Ting", ""], ["Huang", "Ping-Chia", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2107.11786", "submitter": "Faisal Mahmood", "authors": "Kutsev Bengisu Ozyoruk, Sermet Can, Guliz Irem Gokceler, Kayhan Basak,\n  Derya Demir, Gurdeniz Serin, Uguray Payam Hacisalihoglu, Berkan Darbaz, Ming\n  Y. Lu, Tiffany Y. Chen, Drew F. K. Williamson, Funda Yilmaz, Faisal Mahmood,\n  Mehmet Turan", "title": "Deep Learning-based Frozen Section to FFPE Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Frozen sectioning (FS) is the preparation method of choice for microscopic\nevaluation of tissues during surgical operations. The high speed of the\nprocedure allows pathologists to rapidly assess the key microscopic features,\nsuch as tumour margins and malignant status to guide surgical decision-making\nand minimise disruptions to the course of the operation. However, FS is prone\nto introducing many misleading artificial structures (histological artefacts),\nsuch as nuclear ice crystals, compression, and cutting artefacts, hindering\ntimely and accurate diagnostic judgement of the pathologist. Additional\ntraining and prolonged experience is often required to make highly effective\nand time-critical diagnosis on frozen sections. On the other hand, the gold\nstandard tissue preparation technique of formalin-fixation and\nparaffin-embedding (FFPE) provides significantly superior image quality, but is\na very time-consuming process (12-48 hours), making it unsuitable for\nintra-operative use. In this paper, we propose an artificial intelligence (AI)\nmethod that improves FS image quality by computationally transforming\nfrozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style\nimages in minutes. AI-FFPE rectifies FS artefacts with the guidance of an\nattention mechanism that puts a particular emphasis on artefacts while\nutilising a self-regularization mechanism established between FS input image\nand synthesized FFPE-style image that preserves clinically relevant features.\nAs a result, AI-FFPE method successfully generates FFPE-style images without\nsignificantly extending tissue processing time and consequently improves\ndiagnostic accuracy. We demonstrate the efficacy of AI-FFPE on lung and brain\nfrozen sections using a variety of different qualitative and quantitative\nmetrics including visual Turing tests from 20 board certified pathologists.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 11:32:57 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 20:03:26 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ozyoruk", "Kutsev Bengisu", ""], ["Can", "Sermet", ""], ["Gokceler", "Guliz Irem", ""], ["Basak", "Kayhan", ""], ["Demir", "Derya", ""], ["Serin", "Gurdeniz", ""], ["Hacisalihoglu", "Uguray Payam", ""], ["Darbaz", "Berkan", ""], ["Lu", "Ming Y.", ""], ["Chen", "Tiffany Y.", ""], ["Williamson", "Drew F. K.", ""], ["Yilmaz", "Funda", ""], ["Mahmood", "Faisal", ""], ["Turan", "Mehmet", ""]]}, {"id": "2107.11787", "submitter": "Lian Xu", "authors": "Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, Ferdous\n  Sohel, Dan Xu", "title": "Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised\n  Semantic Segmentation", "comments": "Accepted at ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a challenging task in the absence of densely\nlabelled data. Only relying on class activation maps (CAM) with image-level\nlabels provides deficient segmentation supervision. Prior works thus consider\npre-trained models to produce coarse saliency maps to guide the generation of\npseudo segmentation labels. However, the commonly used off-line heuristic\ngeneration process cannot fully exploit the benefits of these coarse saliency\nmaps. Motivated by the significant inter-task correlation, we propose a novel\nweakly supervised multi-task framework termed as AuxSegNet, to leverage\nsaliency detection and multi-label image classification as auxiliary tasks to\nimprove the primary task of semantic segmentation using only image-level\nground-truth labels. Inspired by their similar structured semantics, we also\npropose to learn a cross-task global pixel-level affinity map from the saliency\nand segmentation representations. The learned cross-task affinity can be used\nto refine saliency predictions and propagate CAM maps to provide improved\npseudo labels for both tasks. The mutual boost between pseudo label updating\nand cross-task affinity learning enables iterative improvements on segmentation\nperformance. Extensive experiments demonstrate the effectiveness of the\nproposed auxiliary learning network structure and the cross-task affinity\nlearning method. The proposed approach achieves state-of-the-art weakly\nsupervised segmentation performance on the challenging PASCAL VOC 2012 and MS\nCOCO benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 11:39:58 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 02:15:27 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xu", "Lian", ""], ["Ouyang", "Wanli", ""], ["Bennamoun", "Mohammed", ""], ["Boussaid", "Farid", ""], ["Sohel", "Ferdous", ""], ["Xu", "Dan", ""]]}, {"id": "2107.11795", "submitter": "Hrishikesh Viswanath", "authors": "P Preethi and Hrishikesh Viswanath", "title": "Character Spotting Using Machine Learning Techniques", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.24999.88485", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work presents a comparison of machine learning algorithms that are\nimplemented to segment the characters of text presented as an image. The\nalgorithms are designed to work on degraded documents with text that is not\naligned in an organized fashion. The paper investigates the use of Support\nVector Machines, K-Nearest Neighbor algorithm and an Encoder Network to perform\nthe operation of character spotting. Character Spotting involves extracting\npotential characters from a stream of text by selecting regions bound by white\nspace.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 12:36:57 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 07:15:15 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Preethi", "P", ""], ["Viswanath", "Hrishikesh", ""]]}, {"id": "2107.11800", "submitter": "Pengwen Dai", "authors": "Pengwen Dai, Xiaochun Cao", "title": "Comprehensive Studies for Arbitrary-shape Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous scene text detection methods have been proposed in recent years.\nMost of them declare they have achieved state-of-the-art performances. However,\nthe performance comparison is unfair, due to lots of inconsistent settings\n(e.g., training data, backbone network, multi-scale feature fusion, evaluation\nprotocols, etc.). These various settings would dissemble the pros and cons of\nthe proposed core techniques. In this paper, we carefully examine and analyze\nthe inconsistent settings, and propose a unified framework for the bottom-up\nbased scene text detection methods. Under the unified framework, we ensure the\nconsistent settings for non-core modules, and mainly investigate the\nrepresentations of describing arbitrary-shape scene texts, e.g., regressing\npoints on text contours, clustering pixels with predicted auxiliary\ninformation, grouping connected components with learned linkages, etc. With the\ncomprehensive investigations and elaborate analyses, it not only cleans up the\nobstacle of understanding the performance differences between existing methods\nbut also reveals the advantages and disadvantages of previous models under fair\ncomparisons.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 13:18:55 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Dai", "Pengwen", ""], ["Cao", "Xiaochun", ""]]}, {"id": "2107.11801", "submitter": "Hrishikesh Viswanath", "authors": "P Preethi and Hrishikesh Viswanath", "title": "Denoising and Segmentation of Epigraphical Scripts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper is a presentation of a new method for denoising images using\nHaralick features and further segmenting the characters using artificial neural\nnetworks. The image is divided into kernels, each of which is converted to a\nGLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation\nfunction is called, the result of which is an array with fourteen elements\ncorresponding to fourteen features The Haralick values and the corresponding\nnoise/text classification form a dictionary, which is then used to de-noise the\nimage through kernel comparison. Segmentation is the process of extracting\ncharacters from a document and can be used when letters are separated by white\nspace, which is an explicit boundary marker. Segmentation is the first step in\nmany Natural Language Processing problems. This paper explores the process of\nsegmentation using Neural Networks. While there have been numerous methods to\nsegment characters of a document, this paper is only concerned with the\naccuracy of doing so using neural networks. It is imperative that the\ncharacters be segmented correctly, for failing to do so will lead to incorrect\nrecognition by Natural language processing tools. Artificial Neural Networks\nwas used to attain accuracy of upto 89%. This method is suitable for languages\nwhere the characters are delimited by white space. However, this method will\nfail to provide acceptable results when the language heavily uses connected\nletters. An example would be the Devanagari script, which is predominantly used\nin northern India.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 13:25:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Preethi", "P", ""], ["Viswanath", "Hrishikesh", ""]]}, {"id": "2107.11810", "submitter": "Dror Aiger", "authors": "Dror Aiger, Simon Lynen, Jan Hosang, Bernhard Zeisl", "title": "Efficient Large Scale Inlier Voting for Geometric Vision Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier rejection and equivalently inlier set optimization is a key\ningredient in numerous applications in computer vision such as filtering\npoint-matches in camera pose estimation or plane and normal estimation in point\nclouds. Several approaches exist, yet at large scale we face a combinatorial\nexplosion of possible solutions and state-of-the-art methods like RANSAC, Hough\ntransform or Branch&Bound require a minimum inlier ratio or prior knowledge to\nremain practical. In fact, for problems such as camera posing in very large\nscenes these approaches become useless as they have exponential runtime growth\nif these conditions aren't met. To approach the problem we present a efficient\nand general algorithm for outlier rejection based on \"intersecting\"\n$k$-dimensional surfaces in $R^d$. We provide a recipe for casting a variety of\ngeometric problems as finding a point in $R^d$ which maximizes the number of\nnearby surfaces (and thus inliers). The resulting algorithm has linear\nworst-case complexity with a better runtime dependency in the approximation\nfactor than competing algorithms while not requiring domain specific bounds.\nThis is achieved by introducing a space decomposition scheme that bounds the\nnumber of computations by successively rounding and grouping samples. Our\nrecipe (and open-source code) enables anybody to derive such fast approaches to\nnew problems across a wide range of domains. We demonstrate the versatility of\nthe approach on several camera posing problems with a high number of matches at\nlow inlier ratio achieving state-of-the-art results at significantly lower\nprocessing times.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 14:13:07 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 10:15:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Aiger", "Dror", ""], ["Lynen", "Simon", ""], ["Hosang", "Jan", ""], ["Zeisl", "Bernhard", ""]]}, {"id": "2107.11813", "submitter": "Hanxi Lin", "authors": "Hanxi Lin, Xinxiao Wu, Jiebo Luo", "title": "Adaptive Recursive Circle Framework for Fine-grained Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to model fine-grained spatial-temporal dynamics in videos has been a\nchallenging problem for action recognition. It requires learning deep and rich\nfeatures with superior distinctiveness for the subtle and abstract motions.\nMost existing methods generate features of a layer in a pure feedforward\nmanner, where the information moves in one direction from inputs to outputs.\nAnd they rely on stacking more layers to obtain more powerful features,\nbringing extra non-negligible overheads. In this paper, we propose an Adaptive\nRecursive Circle (ARC) framework, a fine-grained decorator for pure feedforward\nlayers. It inherits the operators and parameters of the original layer but is\nslightly different in the use of those operators and parameters. Specifically,\nthe input of the layer is treated as an evolving state, and its update is\nalternated with the feature generation. At each recursive step, the input state\nis enriched by the previously generated features and the feature generation is\nmade with the newly updated input state. We hope the ARC framework can\nfacilitate fine-grained action recognition by introducing deeply refined\nfeatures and multi-scale receptive fields at a low cost. Significant\nimprovements over feedforward baselines are observed on several benchmarks. For\nexample, an ARC-equipped TSM-ResNet18 outperforms TSM-ResNet50 with 48% fewer\nFLOPs and 52% model parameters on Something-Something V1 and Diving48.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 14:24:29 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lin", "Hanxi", ""], ["Wu", "Xinxiao", ""], ["Luo", "Jiebo", ""]]}, {"id": "2107.11817", "submitter": "Fuzhao Xue", "authors": "Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, Yang You", "title": "Go Wider Instead of Deeper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transformer has recently achieved impressive results on various tasks. To\nfurther improve the effectiveness and efficiency of the transformer, there are\ntwo trains of thought among existing works: (1) going wider by scaling to more\ntrainable parameters; (2) going shallower by parameter sharing or model\ncompressing along with the depth. However, larger models usually do not scale\nwell when fewer tokens are available to train, and advanced parallelisms are\nrequired when the model is extremely large. Smaller models usually achieve\ninferior performance compared to the original transformer model due to the loss\nof representation power. In this paper, to achieve better performance with\nfewer trainable parameters, we propose a framework to deploy trainable\nparameters efficiently, by going wider instead of deeper. Specially, we scale\nalong model width by replacing feed-forward network (FFN) with\nmixture-of-experts (MoE). We then share the MoE layers across transformer\nblocks using individual layer normalization. Such deployment plays the role to\ntransform various semantic representations, which makes the model more\nparameter-efficient and effective. To evaluate our framework, we design WideNet\nand evaluate it on ImageNet-1K. Our best model outperforms Vision Transformer\n(ViT) by $1.46\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$\nand $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by\n$0.83\\%$ and $2.08\\%$, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 14:44:24 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 10:17:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Xue", "Fuzhao", ""], ["Shi", "Ziji", ""], ["Wei", "Futao", ""], ["Lou", "Yuxuan", ""], ["Liu", "Yong", ""], ["You", "Yang", ""]]}, {"id": "2107.11818", "submitter": "Thasin Abedin", "authors": "Thasin Abedin, Khondokar S. S. Prottoy, Ayana Moshruba and Safayat Bin\n  Hakim", "title": "Bangla sign language recognition using concatenated BdSL network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language is the only medium of communication for the hearing impaired\nand the deaf and dumb community. Communication with the general mass is thus\nalways a challenge for this minority group. Especially in Bangla sign language\n(BdSL), there are 38 alphabets with some having nearly identical symbols. As a\nresult, in BdSL recognition, the posture of hand is an important factor in\naddition to visual features extracted from traditional Convolutional Neural\nNetwork (CNN). In this paper, a novel architecture \"Concatenated BdSL Network\"\nis proposed which consists of a CNN based image network and a pose estimation\nnetwork. While the image network gets the visual features, the relative\npositions of hand keypoints are taken by the pose estimation network to obtain\nthe additional features to deal with the complexity of the BdSL symbols. A\nscore of 91.51% was achieved by this novel approach in test set and the\neffectiveness of the additional pose estimation network is suggested by the\nexperimental results.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 14:47:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Abedin", "Thasin", ""], ["Prottoy", "Khondokar S. S.", ""], ["Moshruba", "Ayana", ""], ["Hakim", "Safayat Bin", ""]]}, {"id": "2107.11822", "submitter": "Jay Nandy", "authors": "Jay Nandy and Wynne Hsu and Mong Li Lee", "title": "Distributional Shifts in Automated Diabetic Retinopathy Screening", "comments": "Accepted at IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based models are developed to automatically detect if a retina\nimage is `referable' in diabetic retinopathy (DR) screening. However, their\nclassification accuracy degrades as the input images distributionally shift\nfrom their training distribution. Further, even if the input is not a retina\nimage, a standard DR classifier produces a high confident prediction that the\nimage is `referable'. Our paper presents a Dirichlet Prior Network-based\nframework to address this issue. It utilizes an out-of-distribution (OOD)\ndetector model and a DR classification model to improve generalizability by\nidentifying OOD images. Experiments on real-world datasets indicate that the\nproposed framework can eliminate the unknown non-retina images and identify the\ndistributionally shifted retina images for human intervention.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 15:03:12 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Nandy", "Jay", ""], ["Hsu", "Wynne", ""], ["Lee", "Mong Li", ""]]}, {"id": "2107.11845", "submitter": "Debi Prasanna Mohanty Mr", "authors": "Anchal Pandey, Sukumar Moharana, Debi Prasanna Mohanty, Archit Panwar,\n  Dewang Agarwal, Siva Prasad Thota", "title": "On-Device Content Moderation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of internet, not safe for work(NSFW) content moderation is a\nmajor problem today. Since,smartphones are now part of daily life of billions\nof people,it becomes even more important to have a solution which coulddetect\nand suggest user about potential NSFW content present ontheir phone. In this\npaper we present a novel on-device solutionfor detecting NSFW images. In\naddition to conventional porno-graphic content moderation, we have also\nincluded semi-nudecontent moderation as it is still NSFW in a large\ndemography.We have curated a dataset comprising of three major\ncategories,namely nude, semi-nude and safe images. We have created anensemble\nof object detector and classifier for filtering of nudeand semi-nude contents.\nThe solution provides unsafe body partannotations along with identification of\nsemi-nude images. Weextensively tested our proposed solution on several public\ndatasetand also on our custom dataset. The model achieves F1 scoreof 0.91 with\n95% precision and 88% recall on our customNSFW16k dataset and 0.92 MAP on NPDI\ndataset. Moreover itachieves average 0.002 false positive rate on a collection\nof safeimage open datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 17:06:01 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pandey", "Anchal", ""], ["Moharana", "Sukumar", ""], ["Mohanty", "Debi Prasanna", ""], ["Panwar", "Archit", ""], ["Agarwal", "Dewang", ""], ["Thota", "Siva Prasad", ""]]}, {"id": "2107.11851", "submitter": "Yu Xiong", "authors": "Yu Xiong, Fabian Caba Heilbron, Dahua Lin", "title": "Transcript to Video: Efficient Clip Sequencing from Texts", "comments": "Tech Report; Demo and project page at\n  http://www.xiongyu.me/projects/transcript2video/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Among numerous videos shared on the web, well-edited ones always attract more\nattention. However, it is difficult for inexperienced users to make well-edited\nvideos because it requires professional expertise and immense manual labor. To\nmeet the demands for non-experts, we present Transcript-to-Video -- a\nweakly-supervised framework that uses texts as input to automatically create\nvideo sequences from an extensive collection of shots. Specifically, we propose\na Content Retrieval Module and a Temporal Coherent Module to learn\nvisual-language representations and model shot sequencing styles, respectively.\nFor fast inference, we introduce an efficient search strategy for real-time\nvideo clip sequencing. Quantitative results and user studies demonstrate\nempirically that the proposed learning framework can retrieve content-relevant\nshots while creating plausible video sequences in terms of style. Besides, the\nrun-time performance analysis shows that our framework can support real-world\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 17:24:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xiong", "Yu", ""], ["Heilbron", "Fabian Caba", ""], ["Lin", "Dahua", ""]]}, {"id": "2107.11853", "submitter": "Zilun Zhang", "authors": "Zilun Zhang, Shihao Ma, Yichun Zhang", "title": "Will Multi-modal Data Improves Few-shot Learning?", "comments": "Project Report", "journal-ref": null, "doi": null, "report-no": "01", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most few-shot learning models utilize only one modality of data. We would\nlike to investigate qualitatively and quantitatively how much will the model\nimprove if we add an extra modality (i.e. text description of the image), and\nhow it affects the learning procedure. To achieve this goal, we propose four\ntypes of fusion method to combine the image feature and text feature. To verify\nthe effectiveness of improvement, we test the fusion methods with two classical\nfew-shot learning models - ProtoNet and MAML, with image feature extractors\nsuch as ConvNet and ResNet12. The attention-based fusion method works best,\nwhich improves the classification accuracy by a large margin around 30%\ncomparing to the baseline result.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 17:34:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Zilun", ""], ["Ma", "Shihao", ""], ["Zhang", "Yichun", ""]]}, {"id": "2107.11857", "submitter": "Oscar Mendez", "authors": "Oscar Mendez, Matthew Vowels, Richard Bowden", "title": "Improving Robot Localisation by Ignoring Visual Distraction", "comments": "2021 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is an important component of modern deep learning. However, less\nemphasis has been put on its inverse: ignoring distraction. Our daily lives\nrequire us to explicitly avoid giving attention to salient visual features that\nconfound the task we are trying to accomplish. This visual prioritisation\nallows us to concentrate on important tasks while ignoring visual distractors.\n  In this work, we introduce Neural Blindness, which gives an agent the ability\nto completely ignore objects or classes that are deemed distractors. More\nexplicitly, we aim to render a neural network completely incapable of\nrepresenting specific chosen classes in its latent space. In a very real sense,\nthis makes the network \"blind\" to certain classes, allowing and agent to focus\non what is important for a given task, and demonstrates how this can be used to\nimprove localisation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 17:45:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mendez", "Oscar", ""], ["Vowels", "Matthew", ""], ["Bowden", "Richard", ""]]}, {"id": "2107.11878", "submitter": "Abhishek Aich", "authors": "Abhishek Aich, Meng Zheng, Srikrishna Karanam, Terrence Chen, Amit K.\n  Roy-Chowdhury, Ziyan Wu", "title": "Spatio-Temporal Representation Factorization for Video-based Person\n  Re-Identification", "comments": "Accepted at IEEE ICCV 2021, Includes Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite much recent progress in video-based person re-identification (re-ID),\nthe current state-of-the-art still suffers from common real-world challenges\nsuch as appearance similarity among various people, occlusions, and frame\nmisalignment. To alleviate these problems, we propose Spatio-Temporal\nRepresentation Factorization module (STRF), a flexible new computational unit\nthat can be used in conjunction with most existing 3D convolutional neural\nnetwork architectures for re-ID. The key innovations of STRF over prior work\ninclude explicit pathways for learning discriminative temporal and spatial\nfeatures, with each component further factorized to capture complementary\nperson-specific appearance and motion information. Specifically, temporal\nfactorization comprises two branches, one each for static features (e.g., the\ncolor of clothes) that do not change much over time, and dynamic features\n(e.g., walking patterns) that change over time. Further, spatial factorization\nalso comprises two branches to learn both global (coarse segments) as well as\nlocal (finer segments) appearance features, with the local features\nparticularly useful in cases of occlusion or spatial misalignment. These two\nfactorization operations taken together result in a modular architecture for\nour parameter-wise economic STRF unit that can be plugged in between any two 3D\nconvolutional layers, resulting in an end-to-end learning framework. We\nempirically show that STRF improves performance of various existing baseline\narchitectures while demonstrating new state-of-the-art results using standard\nperson re-identification evaluation protocols on three benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 19:29:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aich", "Abhishek", ""], ["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Wu", "Ziyan", ""]]}, {"id": "2107.11882", "submitter": "Riqiang Gao", "authors": "Riqiang Gao, Yucheng Tang, Kaiwen Xu, Ho Hin Lee, Steve Deppen, Kim\n  Sandler, Pierre Massion, Thomas A. Lasko, Yuankai Huo, Bennett A. Landman", "title": "Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing\n  Imputation Perspective", "comments": "Early Accepted by MICCAI 2021. Traveling Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data from multi-modality provide complementary information in clinical\nprediction, but missing data in clinical cohorts limits the number of subjects\nin multi-modal learning context. Multi-modal missing imputation is challenging\nwith existing methods when 1) the missing data span across heterogeneous\nmodalities (e.g., image vs. non-image); or 2) one modality is largely missing.\nIn this paper, we address imputation of missing data by modeling the joint\ndistribution of multi-modal data. Motivated by partial bidirectional generative\nadversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method\nthat imputes one modality combining the conditional knowledge from another\nmodality. Specifically, C-PBiGAN introduces a conditional latent space in a\nmissing imputation framework that jointly encodes the available multi-modal\ndata, along with a class regularization loss on imputed data to recover\ndiscriminative information. To our knowledge, it is the first generative\nadversarial model that addresses multi-modal missing imputation by modeling the\njoint distribution of image and non-image data. We validate our model with both\nthe national lung screening trial (NLST) dataset and an external clinical\nvalidation cohort. The proposed C-PBiGAN achieves significant improvements in\nlung cancer risk estimation compared with representative imputation methods\n(e.g., AUC values increase in both NLST (+2.9\\%) and in-house dataset (+4.3\\%)\ncompared with PBiGAN, p$<$0.05).\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 20:15:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gao", "Riqiang", ""], ["Tang", "Yucheng", ""], ["Xu", "Kaiwen", ""], ["Lee", "Ho Hin", ""], ["Deppen", "Steve", ""], ["Sandler", "Kim", ""], ["Massion", "Pierre", ""], ["Lasko", "Thomas A.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2107.11919", "submitter": "Zhanzhan Cheng", "authors": "Zhanzhan Cheng, Jing Lu, Baorui Zou, Shuigeng Zhou, and Fei Wu", "title": "ICDAR 2021 Competition on Scene Video Text Spotting", "comments": "SVTS Technique Report for ICDAR 2021 competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scene video text spotting (SVTS) is a very important research topic because\nof many real-life applications. However, only a little effort has put to\nspotting scene video text, in contrast to massive studies of scene text\nspotting in static images. Due to various environmental interferences like\nmotion blur, spotting scene video text becomes very challenging. To promote\nthis research area, this competition introduces a new challenge dataset\ncontaining 129 video clips from 21 natural scenarios in full annotations. The\ncompetition containts three tasks, that is, video text detection (Task 1),\nvideo text tracking (Task 2) and end-to-end video text spotting (Task3). During\nthe competition period (opened on 1st March, 2021 and closed on 11th April,\n2021), a total of 24 teams participated in the three proposed tasks with 46\nvalid submissions, respectively. This paper includes dataset descriptions, task\ndefinitions, evaluation protocols and results summaries of the ICDAR 2021 on\nSVTS competition. Thanks to the healthy number of teams as well as submissions,\nwe consider that the SVTS competition has been successfully held, drawing much\nattention from the community and promoting the field research and its\ndevelopment.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 01:25:57 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cheng", "Zhanzhan", ""], ["Lu", "Jing", ""], ["Zou", "Baorui", ""], ["Zhou", "Shuigeng", ""], ["Wu", "Fei", ""]]}, {"id": "2107.11920", "submitter": "Zhenhua Xu", "authors": "Zhenhua Xu, Yuxiang Sun, Lujia Wang, Ming Liu", "title": "CP-loss: Connectivity-preserving Loss for Road Curb Detection in\n  Autonomous Driving with Aerial Images", "comments": "Accepted by The IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road curb detection is important for autonomous driving. It can be used to\ndetermine road boundaries to constrain vehicles on roads, so that potential\naccidents could be avoided. Most of the current methods detect road curbs\nonline using vehicle-mounted sensors, such as cameras or 3-D Lidars. However,\nthese methods usually suffer from severe occlusion issues. Especially in\nhighly-dynamic traffic environments, most of the field of view is occupied by\ndynamic objects. To alleviate this issue, we detect road curbs offline using\nhigh-resolution aerial images in this paper. Moreover, the detected road curbs\ncan be used to create high-definition (HD) maps for autonomous vehicles.\nSpecifically, we first predict the pixel-wise segmentation map of road curbs,\nand then conduct a series of post-processing steps to extract the graph\nstructure of road curbs. To tackle the disconnectivity issue in the\nsegmentation maps, we propose an innovative connectivity-preserving loss\n(CP-loss) to improve the segmentation performance. The experimental results on\na public dataset demonstrate the effectiveness of our proposed loss function.\nThis paper is accompanied with a demonstration video and a supplementary\ndocument, which are available at\n\\texttt{\\url{https://sites.google.com/view/cp-loss}}.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 01:36:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Zhenhua", ""], ["Sun", "Yuxiang", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "2107.11943", "submitter": "Bing Su", "authors": "Bing Su, Ji-Rong Wen", "title": "Log-Polar Space Convolution for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks use regular quadrilateral convolution kernels\nto extract features. Since the number of parameters increases quadratically\nwith the size of the convolution kernel, many popular models use small\nconvolution kernels, resulting in small local receptive fields in lower layers.\nThis paper proposes a novel log-polar space convolution (LPSC) method, where\nthe convolution kernel is elliptical and adaptively divides its local receptive\nfield into different regions according to the relative directions and\nlogarithmic distances. The local receptive field grows exponentially with the\nnumber of distance levels. Therefore, the proposed LPSC not only naturally\nencodes local spatial structures, but also greatly increases the single-layer\nreceptive field while maintaining the number of parameters. We show that LPSC\ncan be implemented with conventional convolution via log-polar space pooling\nand can be applied in any network architecture to replace conventional\nconvolutions. Experiments on different tasks and datasets demonstrate the\neffectiveness of the proposed LPSC. Code is available at\nhttps://github.com/BingSu12/Log-Polar-Space-Convolution.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 03:41:40 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Su", "Bing", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2107.11945", "submitter": "Heran Yang", "authors": "Heran Yang, Jian Sun, Liwei Yang, and Zongben Xu", "title": "A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image\n  Translation", "comments": "11 pages, 4 figures, accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-contrast image translation is an important task for completing missing\ncontrasts in clinical diagnosis. However, most existing methods learn separate\ntranslator for each pair of contrasts, which is inefficient due to many\npossible contrast pairs in real scenarios. In this work, we propose a unified\nHyper-GAN model for effectively and efficiently translating between different\ncontrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder\nto first map from the source contrast to a common feature space, and then\nfurther map to the target contrast image. To facilitate the translation between\ndifferent contrast pairs, contrast-modulators are designed to tune the\nhyper-encoder and hyper-decoder adaptive to different contrasts. We also design\na common space loss to enforce that multi-contrast images of a subject share a\ncommon feature space, implicitly modeling the shared underlying anatomical\nstructures. Experiments on two datasets of IXI and BraTS 2019 show that our\nHyper-GAN achieves state-of-the-art results in both accuracy and efficiency,\ne.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less\nthan half the amount of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 03:49:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yang", "Heran", ""], ["Sun", "Jian", ""], ["Yang", "Liwei", ""], ["Xu", "Zongben", ""]]}, {"id": "2107.11960", "submitter": "Fei Pan", "authors": "Fei Pan, Chunlei Xu, Jie Guo, Yanwen Guo", "title": "Temporal Alignment Prediction for Few-Shot Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot video classification is to learn a classification model\nwith good generalization ability when trained with only a few labeled videos.\nHowever, it is difficult to learn discriminative feature representations for\nvideos in such a setting. In this paper, we propose Temporal Alignment\nPrediction (TAP) based on sequence similarity learning for few-shot video\nclassification. In order to obtain the similarity of a pair of videos, we\npredict the alignment scores between all pairs of temporal positions in the two\nvideos with the temporal alignment prediction function. Besides, the inputs to\nthis function are also equipped with the context information in the temporal\ndomain. We evaluate TAP on two video classification benchmarks including\nKinetics and Something-Something V2. The experimental results verify the\neffectiveness of TAP and show its superiority over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 05:12:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pan", "Fei", ""], ["Xu", "Chunlei", ""], ["Guo", "Jie", ""], ["Guo", "Yanwen", ""]]}, {"id": "2107.11970", "submitter": "Wentian Zhao", "authors": "Wentian Zhao, Yao Hu, Heda Wang, Xinxiao Wu, Jiebo Luo", "title": "Boosting Entity-aware Image Captioning with Multi-modal Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity-aware image captioning aims to describe named entities and events\nrelated to the image by utilizing the background knowledge in the associated\narticle. This task remains challenging as it is difficult to learn the\nassociation between named entities and visual cues due to the long-tail\ndistribution of named entities. Furthermore, the complexity of the article\nbrings difficulty in extracting fine-grained relationships between entities to\ngenerate informative event descriptions about the image. To tackle these\nchallenges, we propose a novel approach that constructs a multi-modal knowledge\ngraph to associate the visual objects with named entities and capture the\nrelationship between entities simultaneously with the help of external\nknowledge collected from the web. Specifically, we build a text sub-graph by\nextracting named entities and their relationships from the article, and build\nan image sub-graph by detecting the objects in the image. To connect these two\nsub-graphs, we propose a cross-modal entity matching module trained using a\nknowledge base that contains Wikipedia entries and the corresponding images.\nFinally, the multi-modal knowledge graph is integrated into the captioning\nmodel via a graph attention mechanism. Extensive experiments on both GoodNews\nand NYTimes800k datasets demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 05:50:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhao", "Wentian", ""], ["Hu", "Yao", ""], ["Wang", "Heda", ""], ["Wu", "Xinxiao", ""], ["Luo", "Jiebo", ""]]}, {"id": "2107.11975", "submitter": "Fei Pan", "authors": "Fei Pan, Chunlei Xu, Jie Guo, Yanwen Guo", "title": "Transductive Maximum Margin Classifier for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to train a classifier that can generalize well when\njust a small number of labeled samples per class are given. We introduce\nTransductive Maximum Margin Classifier (TMMC) for few-shot learning. The basic\nidea of the classical maximum margin classifier is to solve an optimal\nprediction function that the corresponding separating hyperplane can correctly\ndivide the training data and the resulting classifier has the largest geometric\nmargin. In few-shot learning scenarios, the training samples are scarce, not\nenough to find a separating hyperplane with good generalization ability on\nunseen data. TMMC is constructed using a mixture of the labeled support set and\nthe unlabeled query set in a given task. The unlabeled samples in the query set\ncan adjust the separating hyperplane so that the prediction function is optimal\non both the labeled and unlabeled samples. Furthermore, we leverage an\nefficient and effective quasi-Newton algorithm, the L-BFGS method to optimize\nTMMC. Experimental results on three standard few-shot learning benchmarks\nincluding miniImagenet, tieredImagenet and CUB suggest that our TMMC achieves\nstate-of-the-art accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:02:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pan", "Fei", ""], ["Xu", "Chunlei", ""], ["Guo", "Jie", ""], ["Guo", "Yanwen", ""]]}, {"id": "2107.11978", "submitter": "Yuqian Fu", "authors": "Yuqian Fu, Yanwei Fu, Yu-Gang Jiang", "title": "Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target\n  Data", "comments": "Accepted by ACM Multimedia 2021", "journal-ref": null, "doi": "10.1145/3474085.3475655", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent study finds that existing few-shot learning methods, trained on the\nsource domain, fail to generalize to the novel target domain when a domain gap\nis observed. This motivates the task of Cross-Domain Few-Shot Learning\n(CD-FSL). In this paper, we realize that the labeled target data in CD-FSL has\nnot been leveraged in any way to help the learning process. Thus, we advocate\nutilizing few labeled target data to guide the model learning. Technically, a\nnovel meta-FDMixup network is proposed. We tackle this problem mainly from two\naspects. Firstly, to utilize the source and the newly introduced target data of\ntwo different class sets, a mixup module is re-proposed and integrated into the\nmeta-learning mechanism. Secondly, a novel disentangle module together with a\ndomain classifier is proposed to extract the disentangled domain-irrelevant and\ndomain-specific features. These two modules together enable our model to narrow\nthe domain gap thus generalizing well to the target datasets. Additionally, a\ndetailed feasibility and pilot study is conducted to reflect the intuitive\nunderstanding of CD-FSL under our new setting. Experimental results show the\neffectiveness of our new setting and the proposed method. Codes and models are\navailable at https://github.com/lovelyqian/Meta-FDMixup.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:15:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fu", "Yuqian", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2107.11986", "submitter": "Xian Zhao", "authors": "Xian Zhao, Jiaming Zhang, Zhiyu Lin and Jitao Sang", "title": "Benign Adversarial Attack: Tricking Algorithm for Goodness", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the successful application in many fields, machine learning\nalgorithms today suffer from notorious problems like vulnerability to\nadversarial examples. Beyond falling into the cat-and-mouse game between\nadversarial attack and defense, this paper provides alternative perspective to\nconsider adversarial example and explore whether we can exploit it in benign\napplications. We first propose a novel taxonomy of visual information along\ntask-relevance and semantic-orientation. The emergence of adversarial example\nis attributed to algorithm's utilization of task-relevant non-semantic\ninformation. While largely ignored in classical machine learning mechanisms,\ntask-relevant non-semantic information enjoys three interesting characteristics\nas (1) exclusive to algorithm, (2) reflecting common weakness, and (3)\nutilizable as features. Inspired by this, we present brave new idea called\nbenign adversarial attack to exploit adversarial examples for goodness in three\ndirections: (1) adversarial Turing test, (2) rejecting malicious algorithm, and\n(3) adversarial data augmentation. Each direction is positioned with motivation\nelaboration, justification analysis and prototype applications to showcase its\npotential.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:46:19 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhao", "Xian", ""], ["Zhang", "Jiaming", ""], ["Lin", "Zhiyu", ""], ["Sang", "Jitao", ""]]}, {"id": "2107.11990", "submitter": "Yalong Bai", "authors": "Yalong Bai, Mohan Zhou, Yuxiang Chen, Wei Zhang, Bowen Zhou, Tao Mei", "title": "Augmentation Pathways Network for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is practically helpful for visual recognition, especially\nat the time of data scarcity. However, such success is only limited to quite a\nfew light augmentations (e.g., random crop, flip). Heavy augmentations (e.g.,\ngray, grid shuffle) are either unstable or show adverse effects during\ntraining, owing to the big gap between the original and augmented images. This\npaper introduces a novel network design, noted as Augmentation Pathways (AP),\nto systematically stabilize training on a much wider range of augmentation\npolicies. Notably, AP tames heavy data augmentations and stably boosts\nperformance without a careful selection among augmentation policies. Unlike\ntraditional single pathway, augmented images are processed in different neural\npaths. The main pathway handles light augmentations, while other pathways focus\non heavy augmentations. By interacting with multiple paths in a dependent\nmanner, the backbone network robustly learns from shared visual patterns among\naugmentations, and suppresses noisy patterns at the same time. Furthermore, we\nextend AP to a homogeneous version and a heterogeneous version for high-order\nscenarios, demonstrating its robustness and flexibility in practical usage.\nExperimental results on ImageNet benchmarks demonstrate the compatibility and\neffectiveness on a much wider range of augmentations (e.g., Crop, Gray, Grid\nShuffle, RandAugment), while consuming fewer parameters and lower computational\ncosts at inference time. Source code:https://github.com/ap-conv/ap-net.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:54:53 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bai", "Yalong", ""], ["Zhou", "Mohan", ""], ["Chen", "Yuxiang", ""], ["Zhang", "Wei", ""], ["Zhou", "Bowen", ""], ["Mei", "Tao", ""]]}, {"id": "2107.11991", "submitter": "Yue Jiao", "authors": "Yue Jiao, Jonathon Hare, Adam Pr\\\"ugel-Bennett", "title": "What Remains of Visual Semantic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero shot learning (ZSL) has seen a surge in interest over the decade for its\ntight links with the mechanism making young children recognize novel objects.\nAlthough different paradigms of visual semantic embedding models are designed\nto align visual features and distributed word representations, it is unclear to\nwhat extent current ZSL models encode semantic information from distributed\nword representations. In this work, we introduce the split of tiered-ImageNet\nto the ZSL task, in order to avoid the structural flaws in the standard\nImageNet benchmark. We build a unified framework for ZSL with contrastive\nlearning as pre-training, which guarantees no semantic information leakage and\nencourages linearly separable visual features. Our work makes it fair for\nevaluating visual semantic embedding models on a ZSL setting in which semantic\ninference is decisive. With this framework, we show that current ZSL models\nstruggle with encoding semantic relationships from word analogy and word\nhierarchy. Our analyses provide motivation for exploring the role of context\nlanguage representations in ZSL tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 06:55:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jiao", "Yue", ""], ["Hare", "Jonathon", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "2107.11992", "submitter": "Fan Lu", "authors": "Fan Lu, Guang Chen, Yinlong Liu, Lijun Zhang, Sanqing Qu, Shu Liu,\n  Rongqi Gu", "title": "HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point\n  Cloud Registration", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Point cloud registration is a fundamental problem in 3D computer vision.\nOutdoor LiDAR point clouds are typically large-scale and complexly distributed,\nwhich makes the registration challenging. In this paper, we propose an\nefficient hierarchical network named HRegNet for large-scale outdoor LiDAR\npoint cloud registration. Instead of using all points in the point clouds,\nHRegNet performs registration on hierarchically extracted keypoints and\ndescriptors. The overall framework combines the reliable features in deeper\nlayer and the precise position information in shallower layers to achieve\nrobust and precise registration. We present a correspondence network to\ngenerate correct and accurate keypoints correspondences. Moreover, bilateral\nconsensus and neighborhood consensus are introduced for keypoints matching and\nnovel similarity features are designed to incorporate them into the\ncorrespondence network, which significantly improves the registration\nperformance. Besides, the whole network is also highly efficient since only a\nsmall number of keypoints are used for registration. Extensive experiments are\nconducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate\nthe high accuracy and efficiency of the proposed HRegNet. The project website\nis https://ispc-group.github.io/hregnet.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 07:01:36 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lu", "Fan", ""], ["Chen", "Guang", ""], ["Liu", "Yinlong", ""], ["Zhang", "Lijun", ""], ["Qu", "Sanqing", ""], ["Liu", "Shu", ""], ["Gu", "Rongqi", ""]]}, {"id": "2107.12003", "submitter": "Seyun Um", "authors": "Se-Yun Um, Jihyun Kim, Jihyun Lee, Sangshin Oh, Kyungguen Byun, and\n  Hong-Goo Kang", "title": "Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent\n  Representations", "comments": "10 pages (including references), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective method to synthesize speaker-specific\nspeech waveforms by conditioning on videos of an individual's face. Using a\ngenerative adversarial network (GAN) with linguistic and speaker characteristic\nfeatures as auxiliary conditions, our method directly converts face images into\nspeech waveforms under an end-to-end training framework. The linguistic\nfeatures are extracted from lip movements using a lip-reading model, and the\nspeaker characteristic features are predicted from face images using\ncross-modal learning with a pre-trained acoustic model. Since these two\nfeatures are uncorrelated and controlled independently, we can flexibly\nsynthesize speech waveforms whose speaker characteristics vary depending on the\ninput face images. Therefore, our method can be regarded as a multi-speaker\nface-to-speech waveform model. We show the superiority of our proposed model\nover conventional methods in terms of both objective and subjective evaluation\nresults. Specifically, we evaluate the performances of the linguistic feature\nand the speaker characteristic generation modules by measuring the accuracy of\nautomatic speech recognition and automatic speaker/gender recognition tasks,\nrespectively. We also evaluate the naturalness of the synthesized speech\nwaveforms using a mean opinion score (MOS) test.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 07:36:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Um", "Se-Yun", ""], ["Kim", "Jihyun", ""], ["Lee", "Jihyun", ""], ["Oh", "Sangshin", ""], ["Byun", "Kyungguen", ""], ["Kang", "Hong-Goo", ""]]}, {"id": "2107.12009", "submitter": "Noa Cahan", "authors": "Noa Cahan, Edith M. Marom, Shelly Soffer, Yiftach Barash, Eli Konen,\n  Eyal Klang and Hayit Greenspan", "title": "Weakly Supervised Attention Model for RV StrainClassification from\n  volumetric CTPA Scans", "comments": "12 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood\nclots. PE accounts for approximately 100,000 deaths per year in the United\nStates alone. The clinical presentation of PE is often nonspecific, making the\ndiagnosis challenging. Thus, rapid and accurate risk stratification is of\nparamount importance. High-risk PE is caused by right ventricular (RV)\ndysfunction from acute pressure overload, which in return can help identify\nwhich patients require more aggressive therapy. Reconstructed four-chamber\nviews of the heart on chest CT can detect right ventricular enlargement. CT\npulmonary angiography (CTPA) is the golden standard in the diagnostic workup of\nsuspected PE. Therefore, it can link between diagnosis and risk stratification\nstrategies. We developed a weakly supervised deep learning algorithm, with an\nemphasis on a novel attention mechanism, to automatically classify RV strain on\nCTPA. Our method is a 3D DenseNet model with integrated 3D residual attention\nblocks. We evaluated our model on a dataset of CTPAs of emergency department\n(ED) PE patients. This model achieved an area under the receiver operating\ncharacteristic curve (AUC) of 0.88 for classifying RV strain. The model showed\na sensitivity of 87% and specificity of 83.7%. Our solution outperforms\nstate-of-the-art 3D CNN networks. The proposed design allows for a fully\nautomated network that can be trained easily in an end-to-end manner without\nrequiring computationally intensive and time-consuming preprocessing or\nstrenuous labeling of the data.We infer that unmarked CTPAs can be used for\neffective RV strain classification. This could be used as a second reader,\nalerting for high-risk PE patients. To the best of our knowledge, there are no\nprevious deep learning-based studies that attempted to solve this problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 07:57:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cahan", "Noa", ""], ["Marom", "Edith M.", ""], ["Soffer", "Shelly", ""], ["Barash", "Yiftach", ""], ["Konen", "Eli", ""], ["Klang", "Eyal", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2107.12014", "submitter": "Juan Tapia Dr.", "authors": "Jose Maureira, Juan Tapia, Claudia Arellano, Christoph Busch", "title": "Synthetic Periocular Iris PAI from a Small Set of Near-Infrared-Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Biometric has been increasing in relevance these days since it can be used\nfor several applications such as access control for instance. Unfortunately,\nwith the increased deployment of biometric applications, we observe an increase\nof attacks. Therefore, algorithms to detect such attacks (Presentation Attack\nDetection (PAD)) have been increasing in relevance. The LivDet-2020 competition\nwhich focuses on Presentation Attacks Detection (PAD) algorithms have shown\nstill open problems, specially for unknown attacks scenarios. In order to\nimprove the robustness of biometric systems, it is crucial to improve PAD\nmethods. This can be achieved by augmenting the number of presentation attack\ninstruments (PAI) and bona fide images that are used to train such algorithms.\nUnfortunately, the capture and creation of presentation attack instruments and\neven the capture of bona fide images is sometimes complex to achieve. This\npaper proposes a novel PAI synthetically created (SPI-PAI) using four\nstate-of-the-art GAN algorithms (cGAN, WGAN, WGAN-GP, and StyleGAN2) and a\nsmall set of periocular NIR images. A benchmark between GAN algorithms is\nperformed using the Frechet Inception Distance (FID) between the generated\nimages and the original images used for training. The best PAD algorithm\nreported by the LivDet-2020 competition was tested for us using the synthetic\nPAI which was obtained with the StyleGAN2 algorithm. Surprisingly, The PAD\nalgorithm was not able to detect the synthetic images as a Presentation Attack,\ncategorizing all of them as bona fide. Such results demonstrated the\nfeasibility of synthetic images to fool presentation attacks detection\nalgorithms and the need for such algorithms to be constantly updated and\ntrained with a larger number of images and PAI scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:07:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Maureira", "Jose", ""], ["Tapia", "Juan", ""], ["Arellano", "Claudia", ""], ["Busch", "Christoph", ""]]}, {"id": "2107.12021", "submitter": "Yue Jiao", "authors": "Yue Jiao, Jonathon Hare, Adam Pr\\\"ugel-Bennett", "title": "Language Models as Zero-shot Visual Semantic Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Semantic Embedding (VSE) models, which map images into a rich semantic\nembedding space, have been a milestone in object recognition and zero-shot\nlearning. Current approaches to VSE heavily rely on static word em-bedding\ntechniques. In this work, we propose a Visual Se-mantic Embedding Probe (VSEP)\ndesigned to probe the semantic information of contextualized word embeddings in\nvisual semantic understanding tasks. We show that the knowledge encoded in\ntransformer language models can be exploited for tasks requiring visual\nsemantic understanding.The VSEP with contextual representations can distinguish\nword-level object representations in complicated scenes as a compositional\nzero-shot learner. We further introduce a zero-shot setting with VSEPs to\nevaluate a model's ability to associate a novel word with a novel visual\ncategory. We find that contextual representations in language mod-els\noutperform static word embeddings, when the compositional chain of object is\nshort. We notice that current visual semantic embedding models lack a mutual\nexclusivity bias which limits their performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:22:55 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jiao", "Yue", ""], ["Hare", "Jonathon", ""], ["Pr\u00fcgel-Bennett", "Adam", ""]]}, {"id": "2107.12028", "submitter": "Cui Jiequan", "authors": "Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, Jiaya Jia", "title": "Parametric Contrastive Learning", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose Parametric Contrastive Learning (PaCo) to tackle\nlong-tailed recognition. Based on theoretical analysis, we observe supervised\ncontrastive loss tends to bias on high-frequency classes and thus increases the\ndifficulty of imbalance learning. We introduce a set of parametric class-wise\nlearnable centers to rebalance from an optimization perspective. Further, we\nanalyze our PaCo loss under a balanced setting. Our analysis demonstrates that\nPaCo can adaptively enhance the intensity of pushing samples of the same class\nclose as more samples are pulled together with their corresponding centers and\nbenefit hard example learning. Experiments on long-tailed CIFAR, ImageNet,\nPlaces, and iNaturalist 2018 manifest the new state-of-the-art for long-tailed\nrecognition. On full ImageNet, models trained with PaCo loss surpass supervised\ncontrastive learning across various ResNet backbones. Our code is available at\n\\url{https://github.com/jiequancui/Parametric-Contrastive-Learning}.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:37:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cui", "Jiequan", ""], ["Zhong", "Zhisheng", ""], ["Liu", "Shu", ""], ["Yu", "Bei", ""], ["Jia", "Jiaya", ""]]}, {"id": "2107.12038", "submitter": "Fabian Mentzer", "authors": "Fabian Mentzer, Eirikur Agustsson, Johannes Ball\\'e, David Minnen,\n  Nick Johnston, George Toderici", "title": "Towards Generative Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural video compression method based on generative adversarial\nnetworks (GANs) that outperforms previous neural video compression methods and\nis comparable to HEVC in a user study. We propose a technique to mitigate\ntemporal error accumulation caused by recursive frame compression that uses\nrandomized shifting and un-shifting, motivated by a spectral analysis. We\npresent in detail the network design choices, their relative importance, and\nelaborate on the challenges of evaluating video compression methods in user\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:53:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mentzer", "Fabian", ""], ["Agustsson", "Eirikur", ""], ["Ball\u00e9", "Johannes", ""], ["Minnen", "David", ""], ["Johnston", "Nick", ""], ["Toderici", "George", ""]]}, {"id": "2107.12046", "submitter": "Guang Yang", "authors": "Xi Guan, Guang Yang, Jianming Ye, Weiji Yang, Xiaomei Xu, Weiwei\n  Jiang, Xiaobo Lai", "title": "3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework", "comments": "34 pages, 12 figure, Accepted by BMC Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background: Glioma is the most common brain malignant tumor, with a high\nmorbidity rate and a mortality rate of more than three percent, which seriously\nendangers human health. The main method of acquiring brain tumors in the clinic\nis MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is\nhelpful for treatment inspection, post-diagnosis monitoring, and effect\nevaluation of patients. However, the common operation in clinical brain tumor\nsegmentation is still manual segmentation, lead to its time-consuming and large\nperformance difference between different operators, a consistent and accurate\nautomatic segmentation method is urgently needed. Methods: To meet the above\nchallenges, we propose an automatic brain tumor MRI data segmentation framework\nwhich is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is\nadded to each encoder, the Attention Guide Filter (AG) module is added to each\ndecoder, using the channel relationship to automatically enhance the useful\ninformation in the channel to suppress the useless information, and use the\nattention mechanism to guide the edge information and remove the influence of\nirrelevant information such as noise. Results: We used the BraTS2020 challenge\nonline verification tool to evaluate our approach. The focus of verification is\nthat the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced\ntumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI\nimages have different intensities, AGSE-VNet is not affected by the size of the\ntumor, and can more accurately extract the features of the three regions, it\nhas achieved impressive results and made outstanding contributions to the\nclinical diagnosis and treatment of brain tumor patients.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:04:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Guan", "Xi", ""], ["Yang", "Guang", ""], ["Ye", "Jianming", ""], ["Yang", "Weiji", ""], ["Xu", "Xiaomei", ""], ["Jiang", "Weiwei", ""], ["Lai", "Xiaobo", ""]]}, {"id": "2107.12052", "submitter": "Alfred Laugros", "authors": "Alfred Laugros and Alice Caplier and Matthieu Ospici", "title": "Using Synthetic Corruptions to Measure Robustness to Natural\n  Distribution Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic corruptions gathered into a benchmark are frequently used to\nmeasure neural network robustness to distribution shifts. However, robustness\nto synthetic corruption benchmarks is not always predictive of robustness to\ndistribution shifts encountered in real-world applications. In this paper, we\npropose a methodology to build synthetic corruption benchmarks that make\nrobustness estimations more correlated with robustness to real-world\ndistribution shifts. Using the overlapping criterion, we split synthetic\ncorruptions into categories that help to better understand neural network\nrobustness. Based on these categories, we identify three parameters that are\nrelevant to take into account when constructing a corruption benchmark: number\nof represented categories, balance among categories and size of benchmarks.\nApplying the proposed methodology, we build a new benchmark called\nImageNet-Syn2Nat to predict image classifier robustness.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:20:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Laugros", "Alfred", ""], ["Caplier", "Alice", ""], ["Ospici", "Matthieu", ""]]}, {"id": "2107.12059", "submitter": "Peng Wu", "authors": "Peng Wu, Xiangteng He, Mingqian Tang, Yiliang Lv, Jing Liu", "title": "HANet: Hierarchical Alignment Networks for Video-Text Retrieval", "comments": "This work has been accepted to ACM-MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-text retrieval is an important yet challenging task in vision-language\nunderstanding, which aims to learn a joint embedding space where related video\nand text instances are close to each other. Most current works simply measure\nthe video-text similarity based on video-level and text-level embeddings.\nHowever, the neglect of more fine-grained or local information causes the\nproblem of insufficient representation. Some works exploit the local details by\ndisentangling sentences, but overlook the corresponding videos, causing the\nasymmetry of video-text representation. To address the above limitations, we\npropose a Hierarchical Alignment Network (HANet) to align different level\nrepresentations for video-text matching. Specifically, we first decompose video\nand text into three semantic levels, namely event (video and text), action\n(motion and verb), and entity (appearance and noun). Based on these, we\nnaturally construct hierarchical representations in the individual-local-global\nmanner, where the individual level focuses on the alignment between frame and\nword, local level focuses on the alignment between video clip and textual\ncontext, and global level focuses on the alignment between the whole video and\ntext. Different level alignments capture fine-to-coarse correlations between\nvideo and text, as well as take the advantage of the complementary information\namong three semantic levels. Besides, our HANet is also richly interpretable by\nexplicitly learning key semantic concepts. Extensive experiments on two public\ndatasets, namely MSR-VTT and VATEX, show the proposed HANet outperforms other\nstate-of-the-art methods, which demonstrates the effectiveness of hierarchical\nrepresentation and alignment. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:28:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wu", "Peng", ""], ["He", "Xiangteng", ""], ["Tang", "Mingqian", ""], ["Lv", "Yiliang", ""], ["Liu", "Jing", ""]]}, {"id": "2107.12081", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song", "title": "Towards the Unseen: Iterative Text Recognition by Distilling from Errors", "comments": "IEEE International Conference on Computer Vision (ICCV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual text recognition is undoubtedly one of the most extensively researched\ntopics in computer vision. Great progress have been made to date, with the\nlatest models starting to focus on the more practical \"in-the-wild\" setting.\nHowever, a salient problem still hinders practical deployment -- prior arts\nmostly struggle with recognising unseen (or rarely seen) character sequences.\nIn this paper, we put forward a novel framework to specifically tackle this\n\"unseen\" problem. Our framework is iterative in nature, in that it utilises\npredicted knowledge of character sequences from a previous iteration, to\naugment the main network in improving the next prediction. Key to our success\nis a unique cross-modal variational autoencoder to act as a feedback module,\nwhich is trained with the presence of textual error distribution data. This\nmodule importantly translate a discrete predicted character space, to a\ncontinuous affine transformation parameter space used to condition the visual\nfeature map at next iteration. Experiments on common datasets have shown\ncompetitive performance over state-of-the-arts under the conventional setting.\nMost importantly, under the new disjoint setup where train-test labels are\nmutually exclusive, ours offers the best performance thus showcasing the\ncapability of generalising onto unseen words.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:06:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Chowdhury", "Pinaki Nath", ""], ["Sain", "Aneeshan", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2107.12085", "submitter": "Qing Guo", "authors": "Qing Guo and Ziyi Cheng and Felix Juefei-Xu and Lei Ma and Xiaofei Xie\n  and Yang Liu and Jianjun Zhao", "title": "Learning to Adversarially Blur Visual Object Tracking", "comments": "This work has been accepted to ICCV2021. 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur caused by the moving of the object or camera during the exposure\ncan be a key challenge for visual object tracking, affecting tracking accuracy\nsignificantly. In this work, we explore the robustness of visual object\ntrackers against motion blur from a new angle, i.e., adversarial blur attack\n(ABA). Our main objective is to online transfer input frames to their natural\nmotion-blurred counterparts while misleading the state-of-the-art trackers\nduring the tracking process. To this end, we first design the motion blur\nsynthesizing method for visual tracking based on the generation principle of\nmotion blur, considering the motion information and the light accumulation\nprocess. With this synthetic method, we propose \\textit{optimization-based ABA\n(OP-ABA)} by iteratively optimizing an adversarial objective function against\nthe tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is\nable to produce natural adversarial examples but the iteration can cause heavy\ntime cost, making it unsuitable for attacking real-time trackers. To alleviate\nthis issue, we further propose \\textit{one-step ABA (OS-ABA)} where we design\nand train a joint adversarial motion and accumulation predictive network\n(JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate\nthe adversarial motion and accumulation parameters in a one-step way. The\nexperiments on four popular datasets (\\eg, OTB100, VOT2018, UAV123, and LaSOT)\ndemonstrate that our methods are able to cause significant accuracy drops on\nfour state-of-the-art trackers with high transferability. Please find the\nsource code at https://github.com/tsingqguo/ABA\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:09:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Guo", "Qing", ""], ["Cheng", "Ziyi", ""], ["Juefei-Xu", "Felix", ""], ["Ma", "Lei", ""], ["Xie", "Xiaofei", ""], ["Liu", "Yang", ""], ["Zhao", "Jianjun", ""]]}, {"id": "2107.12087", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song", "title": "Text is Text, No Matter What: Unifying Text Recognition using Knowledge\n  Distillation", "comments": "IEEE International Conference on Computer Vision (ICCV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text recognition remains a fundamental and extensively researched topic in\ncomputer vision, largely owing to its wide array of commercial applications.\nThe challenging nature of the very problem however dictated a fragmentation of\nresearch efforts: Scene Text Recognition (STR) that deals with text in everyday\nscenes, and Handwriting Text Recognition (HTR) that tackles hand-written text.\nIn this paper, for the first time, we argue for their unification -- we aim for\na single model that can compete favourably with two separate state-of-the-art\nSTR and HTR models. We first show that cross-utilisation of STR and HTR models\ntrigger significant performance drops due to differences in their inherent\nchallenges. We then tackle their union by introducing a knowledge distillation\n(KD) based framework. This is however non-trivial, largely due to the\nvariable-length and sequential nature of text sequences, which renders\noff-the-shelf KD techniques that mostly works with global fixed-length data\ninadequate. For that, we propose three distillation losses all of which are\nspecifically designed to cope with the aforementioned unique characteristics of\ntext recognition. Empirical evidence suggests that our proposed unified model\nperforms on par with individual models, even surpassing them in certain cases.\nAblative studies demonstrate that naive baselines such as a two-stage\nframework, and domain adaption/generalisation alternatives do not work as well,\nfurther verifying the appropriateness of our design.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:10:34 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 23:06:56 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Sain", "Aneeshan", ""], ["Chowdhury", "Pinaki Nath", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2107.12090", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Aneeshan Sain, Amandeep Kumar, Shuvozit Ghose,\n  Pinaki Nath Chowdhury, Yi-Zhe Song", "title": "Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text\n  Recognition", "comments": "IEEE International Conference on Computer Vision (ICCV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although text recognition has significantly evolved over the years,\nstate-of-the-art (SOTA) models still struggle in the wild scenarios due to\ncomplex backgrounds, varying fonts, uncontrolled illuminations, distortions and\nother artefacts. This is because such models solely depend on visual\ninformation for text recognition, thus lacking semantic reasoning capabilities.\nIn this paper, we argue that semantic information offers a complementary role\nin addition to visual only. More specifically, we additionally utilize semantic\ninformation by proposing a multi-stage multi-scale attentional decoder that\nperforms joint visual-semantic reasoning. Our novelty lies in the intuition\nthat for text recognition, the prediction should be refined in a stage-wise\nmanner. Therefore our key contribution is in designing a stage-wise unrolling\nattentional decoder where non-differentiability, invoked by discretely\npredicted character labels, needs to be bypassed for end-to-end training. While\nthe first stage predicts using visual features, subsequent stages refine on top\nof it using joint visual-semantic information. Additionally, we introduce\nmulti-scale 2D attention along with dense and residual connections between\ndifferent stages to deal with varying scales of character sizes, for better\nperformance and faster convergence during training. Experimental results show\nour approach to outperform existing SOTA methods by a considerable margin.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:15:14 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 02:27:15 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Sain", "Aneeshan", ""], ["Kumar", "Amandeep", ""], ["Ghose", "Shuvozit", ""], ["Chowdhury", "Pinaki Nath", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2107.12093", "submitter": "Constantinos Loukas", "authors": "C. Loukas, A. Gazis, D. Schizas", "title": "A Multiple-Instance Learning Approach for the Assessment of Gallbladder\n  Vascularity from Laparoscopic Images", "comments": "6 pages, 5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An important task at the onset of a laparoscopic cholecystectomy (LC)\noperation is the inspection of gallbladder (GB) to evaluate the thickness of\nits wall, presence of inflammation and extent of fat. Difficulty in\nvisualization of the GB wall vessels may be due to the previous factors,\npotentially as a result of chronic inflammation or other diseases. In this\npaper we propose a multiple-instance learning (MIL) technique for assessment of\nthe GB wall vascularity via computer-vision analysis of images from LC\noperations. The bags correspond to a labeled (low vs. high) vascularity dataset\nof 181 GB images, from 53 operations. The instances correspond to unlabeled\npatches extracted from these images. Each patch is represented by a vector with\ncolor, texture and statistical features. We compare various state-of-the-art\nMIL and single-instance learning approaches, as well as a proposed MIL\ntechnique based on variational Bayesian inference. The methods were compared\nfor two experimental tasks: image-based and video-based (i.e. patient-based)\nclassification. The proposed approach presents the best performance with\naccuracy 92.1% and 90.3% for the first and second task, respectively. A\nsignificant advantage of the proposed technique is that it does not require the\ntime-consuming task of manual labelling the instances.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:22:16 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 07:23:29 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Loukas", "C.", ""], ["Gazis", "A.", ""], ["Schizas", "D.", ""]]}, {"id": "2107.12096", "submitter": "Yuedong Chen", "authors": "Yuedong Chen, Xu Yang, Tat-Jen Cham and Jianfei Cai", "title": "Towards Unbiased Visual Emotion Recognition via Causal Intervention", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although much progress has been made in visual emotion recognition,\nresearchers have realized that modern deep networks tend to exploit dataset\ncharacteristics to learn spurious statistical associations between the input\nand the target. Such dataset characteristics are usually treated as dataset\nbias, which damages the robustness and generalization performance of these\nrecognition systems. In this work, we scrutinize this problem from the\nperspective of causal inference, where such dataset characteristic is termed as\na confounder which misleads the system to learn the spurious correlation. To\nalleviate the negative effects brought by the dataset bias, we propose a novel\nInterventional Emotion Recognition Network (IERN) to achieve the backdoor\nadjustment, which is one fundamental deconfounding technique in causal\ninference. A series of designed tests validate the effectiveness of IERN, and\nexperiments on three emotion benchmarks demonstrate that IERN outperforms other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 10:40:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Yuedong", ""], ["Yang", "Xu", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""]]}, {"id": "2107.12137", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "AA3DNet: Attention Augmented Real Time 3D Object Detection", "comments": "12 pages, 8 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects using\npoint cloud data. We present anchor design along with custom loss functions\nused in this work. A combination of spatial and channel wise attention module\nis used in this work. We use the Kitti 3D Birds Eye View dataset for\nbenchmarking and validating our results. Our method surpasses previous state of\nthe art in this domain both in terms of average precision and speed running at\n> 30 FPS. Finally, we present the ablation study to demonstrate that the\nperformance of our network is generalizable. This makes it a feasible option to\nbe deployed in real time applications like self driving cars.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:18:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2107.12143", "submitter": "Alara Zindanc{\\i}o\\u{g}lu", "authors": "Alara Zindanc{\\i}o\\u{g}lu and T. Metin Sezgin", "title": "Perceptually Validated Precise Local Editing for Facial Action Units\n  with StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to edit facial expressions has a wide range of applications in\ncomputer graphics. The ideal facial expression editing algorithm needs to\nsatisfy two important criteria. First, it should allow precise and targeted\nediting of individual facial actions. Second, it should generate high fidelity\noutputs without artifacts. We build a solution based on StyleGAN, which has\nbeen used extensively for semantic manipulation of faces. As we do so, we add\nto our understanding of how various semantic attributes are encoded in\nStyleGAN. In particular, we show that a naive strategy to perform editing in\nthe latent space results in undesired coupling between certain action units,\neven if they are conceptually distinct. For example, although brow lowerer and\nlip tightener are distinct action units, they appear correlated in the training\ndata. Hence, StyleGAN has difficulty in disentangling them. We allow\ndisentangled editing of such action units by computing detached regions of\ninfluence for each action unit, and restrict editing to these regions. We\nvalidate the effectiveness of our local editing method through perception\nexperiments conducted with 23 subjects. The results show that our method\nprovides higher control over local editing and produces images with superior\nfidelity compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:21:37 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 09:05:22 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zindanc\u0131o\u011flu", "Alara", ""], ["Sezgin", "T. Metin", ""]]}, {"id": "2107.12147", "submitter": "Somali Chaterji", "authors": "Pranjal Jain, Shreyas Goenka, Saurabh Bagchi, Biplab Banerjee, Somali\n  Chaterji", "title": "Federated Action Recognition on Heterogeneous Embedded Devices", "comments": "13 pages, 12 figures", "journal-ref": "IEEE Transactions on Artificial Intelligence 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning allows a large number of devices to jointly learn a model\nwithout sharing data. In this work, we enable clients with limited computing\npower to perform action recognition, a computationally heavy task. We first\nperform model compression at the central server through knowledge distillation\non a large dataset. This allows the model to learn complex features and serves\nas an initialization for model fine-tuning. The fine-tuning is required because\nthe limited data present in smaller datasets is not adequate for action\nrecognition models to learn complex spatio-temporal features. Because the\nclients present are often heterogeneous in their computing resources, we use an\nasynchronous federated optimization and we further show a convergence bound. We\ncompare our approach to two baseline approaches: fine-tuning at the central\nserver (no clients) and fine-tuning using (heterogeneous) clients using\nsynchronous federated averaging. We empirically show on a testbed of\nheterogeneous embedded devices that we can perform action recognition with\ncomparable accuracy to the two baselines above, while our asynchronous learning\nstrategy reduces the training time by 40%, relative to synchronous learning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 02:33:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jain", "Pranjal", ""], ["Goenka", "Shreyas", ""], ["Bagchi", "Saurabh", ""], ["Banerjee", "Biplab", ""], ["Chaterji", "Somali", ""]]}, {"id": "2107.12167", "submitter": "Abdul Rafey Aftab", "authors": "Abdul Rafey Aftab, Michael von der Beeck, Steven Rohrhirsch, Benoit\n  Diotte, Michael Feld", "title": "Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of\n  Outside-Vehicle Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a growing interest in more intelligent natural user interaction with\nthe car. Hand gestures and speech are already being applied for driver-car\ninteraction. Moreover, multimodal approaches are also showing promise in the\nautomotive industry. In this paper, we utilize deep learning for a multimodal\nfusion network for referencing objects outside the vehicle. We use features\nfrom gaze, head pose and finger pointing simultaneously to precisely predict\nthe referenced objects in different car poses. We demonstrate the practical\nlimitations of each modality when used for a natural form of referencing,\nspecifically inside the car. As evident from our results, we overcome the\nmodality specific limitations, to a large extent, by the addition of other\nmodalities. This work highlights the importance of multimodal sensing,\nespecially when moving towards natural user interaction. Furthermore, our user\nbased analysis shows noteworthy differences in recognition of user behavior\ndepending upon the vehicle pose.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:37:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aftab", "Abdul Rafey", ""], ["von der Beeck", "Michael", ""], ["Rohrhirsch", "Steven", ""], ["Diotte", "Benoit", ""], ["Feld", "Michael", ""]]}, {"id": "2107.12189", "submitter": "Thanh Binh Nguyen", "authors": "Hieu T. Ung, Huy Q. Ung, Binh T. Nguyen", "title": "An Efficient Insect Pest Classification Using Multiple Convolutional\n  Neural Network Based Models", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate insect pest recognition is significant to protect the crop or take\nthe early treatment on the infected yield, and it helps reduce the loss for the\nagriculture economy. Design an automatic pest recognition system is necessary\nbecause manual recognition is slow, time-consuming, and expensive. The\nImage-based pest classifier using the traditional computer vision method is not\nefficient due to the complexity. Insect pest classification is a difficult task\nbecause of various kinds, scales, shapes, complex backgrounds in the field, and\nhigh appearance similarity among insect species. With the rapid development of\ndeep learning technology, the CNN-based method is the best way to develop a\nfast and accurate insect pest classifier. We present different convolutional\nneural network-based models in this work, including attention, feature pyramid,\nand fine-grained models. We evaluate our methods on two public datasets: the\nlarge-scale insect pest dataset, the IP102 benchmark dataset, and a smaller\ndataset, namely D0 in terms of the macro-average precision (MPre), the\nmacro-average recall (MRec), the macro-average F1- score (MF1), the accuracy\n(Acc), and the geometric mean (GM). The experimental results show that\ncombining these convolutional neural network-based models can better perform\nthan the state-of-the-art methods on these two datasets. For instance, the\nhighest accuracy we obtained on IP102 and D0 is $74.13\\%$ and $99.78\\%$,\nrespectively, bypassing the corresponding state-of-the-art accuracy: $67.1\\%$\n(IP102) and $98.8\\%$ (D0). We also publish our codes for contributing to the\ncurrent research related to the insect pest classification problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:53:28 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ung", "Hieu T.", ""], ["Ung", "Huy Q.", ""], ["Nguyen", "Binh T.", ""]]}, {"id": "2107.12192", "submitter": "Kai Xu", "authors": "Kai Xu and Angela Yao", "title": "Efficient Video Object Segmentation with Compressed Video", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient inference framework for semi-supervised video object\nsegmentation by exploiting the temporal redundancy of the video. Our method\nperforms inference on selected keyframes and makes predictions for other frames\nvia propagation based on motion vectors and residuals from the compressed video\nbitstream. Specifically, we propose a new motion vector-based warping method\nfor propagating segmentation masks from keyframes to other frames in a\nmulti-reference manner. Additionally, we propose a residual-based refinement\nmodule that can correct and add detail to the block-wise propagated\nsegmentation masks. Our approach is flexible and can be added on top of\nexisting video object segmentation algorithms. With STM with top-k filtering as\nour base model, we achieved highly competitive results on DAVIS16 and\nYouTube-VOS with substantial speedups of up to 4.9X with little loss in\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:57:04 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 02:55:16 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xu", "Kai", ""], ["Yao", "Angela", ""]]}, {"id": "2107.12205", "submitter": "Abhir Bhandary", "authors": "Abhir Bhandary, Ananth Prabhu G, Mustafa Basthikodi, Chaitra K M", "title": "Early Diagnosis of Lung Cancer Using Computer Aided Detection via Lung\n  Segmentation Approach", "comments": "9 pages, 10 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology\n  69.5(2021):85-93", "doi": "10.14445/22315381/IJETT-V69I5P213", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer begins in the lungs and leading to the reason of cancer demise\namid population in the creation. According to the American Cancer Society,\nwhich estimates about 27% of the deaths because of cancer. In the early phase\nof its evolution, lung cancer does not cause any symptoms usually. Many of the\npatients have been diagnosed in a developed phase where symptoms become more\nprominent, that results in poor curative treatment and high mortality rate.\nComputer Aided Detection systems are used to achieve greater accuracies for the\nlung cancer diagnosis. In this research exertion, we proposed a novel\nmethodology for lung Segmentation on the basis of Fuzzy C-Means Clustering,\nAdaptive Thresholding, and Segmentation of Active Contour Model. The\nexperimental results are analysed and presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 05:46:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bhandary", "Abhir", ""], ["G", "Ananth Prabhu", ""], ["Basthikodi", "Mustafa", ""], ["M", "Chaitra K", ""]]}, {"id": "2107.12207", "submitter": "Martin Marek", "authors": "Martin Marek", "title": "Image-Based Parking Space Occupancy Classification: Dataset and Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset for image-based parking space occupancy\nclassification: ACPDS. Unlike in prior datasets, each image is taken from a\nunique view, systematically annotated, and the parking lots in the train,\nvalidation, and test sets are unique. We use this dataset to propose a simple\nbaseline model for parking space occupancy classification, which achieves 98%\naccuracy on unseen parking lots, significantly outperforming existing models.\nWe share our dataset, code, and trained models under the MIT license.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:23:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Marek", "Martin", ""]]}, {"id": "2107.12213", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying Deng, Weiming Hu", "title": "Channel-wise Topology Refinement Graph Convolution for Skeleton-Based\n  Action Recognition", "comments": "Accepted to ICCV2021. Code is available at\n  https://github.com/Uason-Chen/CTR-GCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. In GCNs, graph\ntopology dominates feature aggregation and therefore is the key to extracting\nrepresentative features. In this work, we propose a novel Channel-wise Topology\nRefinement Graph Convolution (CTR-GC) to dynamically learn different topologies\nand effectively aggregate joint features in different channels for\nskeleton-based action recognition. The proposed CTR-GC models channel-wise\ntopologies through learning a shared topology as a generic prior for all\nchannels and refining it with channel-specific correlations for each channel.\nOur refinement method introduces few extra parameters and significantly reduces\nthe difficulty of modeling channel-wise topologies. Furthermore, via\nreformulating graph convolutions into a unified form, we find that CTR-GC\nrelaxes strict constraints of graph convolutions, leading to stronger\nrepresentation capability. Combining CTR-GC with temporal modeling modules, we\ndevelop a powerful graph convolutional network named CTR-GCN which notably\noutperforms state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and\nNW-UCLA datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:37:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Yuxin", ""], ["Zhang", "Ziqi", ""], ["Yuan", "Chunfeng", ""], ["Li", "Bing", ""], ["Deng", "Ying", ""], ["Hu", "Weiming", ""]]}, {"id": "2107.12220", "submitter": "Hendrik Schuff", "authors": "Hendrik Schuff, Heike Adel, Ngoc Thang Vu", "title": "Thought Flow Nets: From Single Predictions to Trains of Model Thought", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans solve complex problems, they rarely come up with a decision\nright-away. Instead, they start with an intuitive decision, reflect upon it,\nspot mistakes, resolve contradictions and jump between different hypotheses.\nThus, they create a sequence of ideas and follow a train of thought that\nultimately reaches a conclusive decision. Contrary to this, today's neural\nclassification models are mostly trained to map an input to one single and\nfixed output. In this paper, we investigate how we can give models the\nopportunity of a second, third and $k$-th thought. We take inspiration from\nHegel's dialectics and propose a method that turns an existing classifier's\nclass prediction (such as the image class forest) into a sequence of\npredictions (such as forest $\\rightarrow$ tree $\\rightarrow$ mushroom).\nConcretely, we propose a correction module that is trained to estimate the\nmodel's correctness as well as an iterative prediction update based on the\nprediction's gradient. Our approach results in a dynamic system over class\nprobability distributions $\\unicode{x2014}$ the thought flow. We evaluate our\nmethod on diverse datasets and tasks from computer vision and natural language\nprocessing. We observe surprisingly complex but intuitive behavior and\ndemonstrate that our method (i) can correct misclassifications, (ii)\nstrengthens model performance, (iii) is robust to high levels of adversarial\nattacks, (iv) can increase accuracy up to 4% in a label-distribution-shift\nsetting and (iv) provides a tool for model interpretability that uncovers model\nknowledge which otherwise remains invisible in a single distribution\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:56:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Schuff", "Hendrik", ""], ["Adel", "Heike", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2107.12270", "submitter": "Juncheng Li", "authors": "Juncheng Li, Siliang Tang, Linchao Zhu, Haochen Shi, Xuanwen Huang,\n  Fei Wu, Yi Yang, Yueting Zhuang", "title": "Adaptive Hierarchical Graph Reasoning with Semantic Coherence for\n  Video-and-Language Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-and-Language Inference is a recently proposed task for joint\nvideo-and-language understanding. This new task requires a model to draw\ninference on whether a natural language statement entails or contradicts a\ngiven video clip. In this paper, we study how to address three critical\nchallenges for this task: judging the global correctness of the statement\ninvolved multiple semantic meanings, joint reasoning over video and subtitles,\nand modeling long-range relationships and complex social interactions. First,\nwe propose an adaptive hierarchical graph network that achieves in-depth\nunderstanding of the video over complex interactions. Specifically, it performs\njoint reasoning over video and subtitles in three hierarchies, where the graph\nstructure is adaptively adjusted according to the semantic structures of the\nstatement. Secondly, we introduce semantic coherence learning to explicitly\nencourage the semantic coherence of the adaptive hierarchical graph network\nfrom three hierarchies. The semantic coherence learning can further improve the\nalignment between vision and linguistics, and the coherence across a sequence\nof video segments. Experimental results show that our method significantly\noutperforms the baseline by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:23:19 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Juncheng", ""], ["Tang", "Siliang", ""], ["Zhu", "Linchao", ""], ["Shi", "Haochen", ""], ["Huang", "Xuanwen", ""], ["Wu", "Fei", ""], ["Yang", "Yi", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2107.12283", "submitter": "John Quinn", "authors": "Wojciech Sirko, Sergii Kashubin, Marvin Ritter, Abigail Annkah, Yasser\n  Salah Edine Bouchareb, Yann Dauphin, Daniel Keysers, Maxim Neumann, Moustapha\n  Cisse, John Quinn", "title": "Continental-Scale Building Detection from High Resolution Satellite\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the locations and footprints of buildings is vital for many\npractical and scientific purposes. Such information can be particularly useful\nin developing regions where alternative data sources may be scarce. In this\nwork, we describe a model training pipeline for detecting buildings across the\nentire continent of Africa, using 50 cm satellite imagery. Starting with the\nU-Net model, widely used in satellite image analysis, we study variations in\narchitecture, loss functions, regularization, pre-training, self-training and\npost-processing that increase instance segmentation performance. Experiments\nwere carried out using a dataset of 100k satellite images across Africa\ncontaining 1.75M manually labelled building instances, and further datasets for\npre-training and self-training. We report novel methods for improving\nperformance of building detection with this type of model, including the use of\nmixup (mAP +0.12) and self-training with soft KL loss (mAP +0.06). The\nresulting pipeline obtains good results even on a wide variety of challenging\nrural and urban contexts, and was used to create the Open Buildings dataset of\n516M Africa-wide detected footprints.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:48:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sirko", "Wojciech", ""], ["Kashubin", "Sergii", ""], ["Ritter", "Marvin", ""], ["Annkah", "Abigail", ""], ["Bouchareb", "Yasser Salah Edine", ""], ["Dauphin", "Yann", ""], ["Keysers", "Daniel", ""], ["Neumann", "Maxim", ""], ["Cisse", "Moustapha", ""], ["Quinn", "John", ""]]}, {"id": "2107.12291", "submitter": "Hamideh Kerdegari Dr", "authors": "Hamideh Kerdegari, Phung Tran Huy Nhat, Angela McBride, Luigi Pisani,\n  Reza Razavi, Louise Thwaites, Sophie Yacoub, and Alberto Gomez", "title": "B-line Detection in Lung Ultrasound Videos: Cartesian vs Polar\n  Representation", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lung ultrasound (LUS) imaging is becoming popular in the intensive care units\n(ICU) for assessing lung abnormalities such as the appearance of B-line\nartefacts as a result of severe dengue. These artefacts appear in the LUS\nimages and disappear quickly, making their manual detection very challenging.\nThey also extend radially following the propagation of the sound waves. As a\nresult, we hypothesize that a polar representation may be more adequate for\nautomatic image analysis of these images. This paper presents an\nattention-based Convolutional+LSTM model to automatically detect B-lines in LUS\nvideos, comparing performance when image data is taken in Cartesian and polar\nrepresentations. Results indicate that the proposed framework with polar\nrepresentation achieves competitive performance compared to the Cartesian\nrepresentation for B-line classification and that attention mechanism can\nprovide better localization.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:59:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Kerdegari", "Hamideh", ""], ["Nhat", "Phung Tran Huy", ""], ["McBride", "Angela", ""], ["Pisani", "Luigi", ""], ["Razavi", "Reza", ""], ["Thwaites", "Louise", ""], ["Yacoub", "Sophie", ""], ["Gomez", "Alberto", ""]]}, {"id": "2107.12292", "submitter": "Ting Yao", "authors": "Yehao Li and Ting Yao and Yingwei Pan and Tao Mei", "title": "Contextual Transformer Networks for Visual Recognition", "comments": "Rank 1 in open-set image classification task of Open World Vision\n  Challenge @ CVPR 2021; The source code and models are publicly available at:\n  \\url{https://github.com/JDAI-CV/CoTNet}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer with self-attention has led to the revolutionizing of natural\nlanguage processing field, and recently inspires the emergence of\nTransformer-style architecture design with competitive results in numerous\ncomputer vision tasks. Nevertheless, most of existing designs directly employ\nself-attention over a 2D feature map to obtain the attention matrix based on\npairs of isolated queries and keys at each spatial location, but leave the rich\ncontexts among neighbor keys under-exploited. In this work, we design a novel\nTransformer-style module, i.e., Contextual Transformer (CoT) block, for visual\nrecognition. Such design fully capitalizes on the contextual information among\ninput keys to guide the learning of dynamic attention matrix and thus\nstrengthens the capacity of visual representation. Technically, CoT block first\ncontextually encodes input keys via a $3\\times3$ convolution, leading to a\nstatic contextual representation of inputs. We further concatenate the encoded\nkeys with input queries to learn the dynamic multi-head attention matrix\nthrough two consecutive $1\\times1$ convolutions. The learnt attention matrix is\nmultiplied by input values to achieve the dynamic contextual representation of\ninputs. The fusion of the static and dynamic contextual representations are\nfinally taken as outputs. Our CoT block is appealing in the view that it can\nreadily replace each $3\\times3$ convolution in ResNet architectures, yielding a\nTransformer-style backbone named as Contextual Transformer Networks (CoTNet).\nThrough extensive experiments over a wide range of applications (e.g., image\nrecognition, object detection and instance segmentation), we validate the\nsuperiority of CoTNet as a stronger backbone. Source code is available at\n\\url{https://github.com/JDAI-CV/CoTNet}.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:00:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Mei", "Tao", ""]]}, {"id": "2107.12304", "submitter": "Guy Oren", "authors": "Guy Oren and Lior Wolf", "title": "In Defense of the Learning Without Forgetting for Task Incremental\n  Learning", "comments": "12 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Catastrophic forgetting is one of the major challenges on the road for\ncontinual learning systems, which are presented with an on-line stream of\ntasks. The field has attracted considerable interest and a diverse set of\nmethods have been presented for overcoming this challenge. Learning without\nForgetting (LwF) is one of the earliest and most frequently cited methods. It\nhas the advantages of not requiring the storage of samples from the previous\ntasks, of implementation simplicity, and of being well-grounded by relying on\nknowledge distillation. However, the prevailing view is that while it shows a\nrelatively small amount of forgetting when only two tasks are introduced, it\nfails to scale to long sequences of tasks. This paper challenges this view, by\nshowing that using the right architecture along with a standard set of\naugmentations, the results obtained by LwF surpass the latest algorithms for\ntask incremental scenario. This improved performance is demonstrated by an\nextensive set of experiments over CIFAR-100 and Tiny-ImageNet, where it is also\nshown that other methods cannot benefit as much from similar improvements.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:23:13 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Oren", "Guy", ""], ["Wolf", "Lior", ""]]}, {"id": "2107.12308", "submitter": "Zixuan Ni", "authors": "Zixuan Ni and Haizhou Shi and Siliang Tang and Yueting Zhuang", "title": "Alleviate Representation Overlapping in Class Incremental Learning by\n  Contrastive Class Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of the Class Incremental Learning (CIL) lies in difficulty for\na learner to discern the old classes' data from the new while no previous data\nis preserved. Namely, the representation distribution of different phases\noverlaps with each other. In this paper, to alleviate the phenomenon of\nrepresentation overlapping for both memory-based and memory-free methods, we\npropose a new CIL framework, Contrastive Class Concentration for CIL (C4IL).\nOur framework leverages the class concentration effect of contrastive\nrepresentation learning, therefore yielding a representation distribution with\nbetter intra-class compactibility and inter-class separability. Quantitative\nexperiments showcase our framework that is effective in both memory-based and\nmemory-free cases: it outperforms the baseline methods of both cases by 5% in\nterms of the average and top-1 accuracy in 10-phase and 20-phase CIL.\nQualitative results also demonstrate that our method generates a more compact\nrepresentation distribution that alleviates the overlapping problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:27:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 04:23:55 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ni", "Zixuan", ""], ["Shi", "Haizhou", ""], ["Tang", "Siliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2107.12309", "submitter": "Yuren Cong", "authors": "Yuren Cong, Wentong Liao, Hanno Ackermann, Michael Ying Yang, Bodo\n  Rosenhahn", "title": "Spatial-Temporal Transformer for Dynamic Scene Graph Generation", "comments": "accepted by ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scene graph generation aims at generating a scene graph of the given\nvideo. Compared to the task of scene graph generation from images, it is more\nchallenging because of the dynamic relationships between objects and the\ntemporal dependencies between frames allowing for a richer semantic\ninterpretation. In this paper, we propose Spatial-temporal Transformer\n(STTran), a neural network that consists of two core modules: (1) a spatial\nencoder that takes an input frame to extract spatial context and reason about\nthe visual relationships within a frame, and (2) a temporal decoder which takes\nthe output of the spatial encoder as input in order to capture the temporal\ndependencies between frames and infer the dynamic relationships. Furthermore,\nSTTran is flexible to take varying lengths of videos as input without clipping,\nwhich is especially important for long videos. Our method is validated on the\nbenchmark dataset Action Genome (AG). The experimental results demonstrate the\nsuperior performance of our method in terms of dynamic scene graphs. Moreover,\na set of ablative studies is conducted and the effect of each proposed module\nis justified.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:30:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Cong", "Yuren", ""], ["Liao", "Wentong", ""], ["Ackermann", "Hanno", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2107.12321", "submitter": "Narinder Singh Punn", "authors": "Sachin Gupta, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "MAG-Net: Mutli-task attention guided network for brain tumor\n  segmentation and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain tumor is the most common and deadliest disease that can be found in all\nage groups. Generally, MRI modality is adopted for identifying and diagnosing\ntumors by the radiologists. The correct identification of tumor regions and its\ntype can aid to diagnose tumors with the followup treatment plans. However, for\nany radiologist analysing such scans is a complex and time-consuming task.\nMotivated by the deep learning based computer-aided-diagnosis systems, this\npaper proposes multi-task attention guided encoder-decoder network (MAG-Net) to\nclassify and segment the brain tumor regions using MRI images. The MAG-Net is\ntrained and evaluated on the Figshare dataset that includes coronal, axial, and\nsagittal views with 3 types of tumors meningioma, glioma, and pituitary tumor.\nWith exhaustive experimental trials the model achieved promising results as\ncompared to existing state-of-the-art models, while having least number of\ntraining parameters among other state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:51:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gupta", "Sachin", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2107.12351", "submitter": "Kai-En Lin", "authors": "Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, Ravi Ramamoorthi", "title": "NeLF: Neural Light-transport Field for Portrait View Synthesis and\n  Relighting", "comments": "Published at EGSR 2021. Project page with video and code:\n  http://cseweb.ucsd.edu/~viscomp/projects/EGSR21NeLF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human portraits exhibit various appearances when observed from different\nviews under different lighting conditions. We can easily imagine how the face\nwill look like in another setup, but computer algorithms still fail on this\nproblem given limited observations. To this end, we present a system for\nportrait view synthesis and relighting: given multiple portraits, we use a\nneural network to predict the light-transport field in 3D space, and from the\npredicted Neural Light-transport Field (NeLF) produce a portrait from a new\ncamera view under a new environmental lighting. Our system is trained on a\nlarge number of synthetic models, and can generalize to different synthetic and\nreal portraits under various lighting conditions. Our method achieves\nsimultaneous view synthesis and relighting given multi-view portraits as the\ninput, and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:44:52 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sun", "Tiancheng", ""], ["Lin", "Kai-En", ""], ["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2107.12357", "submitter": "Sophia J. Wagner", "authors": "Sophia J. Wagner, Nadieh Khalili, Raghav Sharma, Melanie Boxberg,\n  Carsten Marr, Walter de Back, Tingying Peng", "title": "Structure-Preserving Multi-Domain Stain Color Augmentation using\n  Style-Transfer with Disentangled Representations", "comments": "accepted at MICCAI 2021, code and model weights are available at\n  http://github.com/sophiajw/HistAuGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In digital pathology, different staining procedures and scanners cause\nsubstantial color variations in whole-slide images (WSIs), especially across\ndifferent laboratories. These color shifts result in a poor generalization of\ndeep learning-based methods from the training domain to external pathology\ndata. To increase test performance, stain normalization techniques are used to\nreduce the variance between training and test domain. Alternatively, color\naugmentation can be applied during training leading to a more robust model\nwithout the extra step of color normalization at test time. We propose a novel\ncolor augmentation technique, HistAuGAN, that can simulate a wide variety of\nrealistic histology stain colors, thus making neural networks stain-invariant\nwhen applied during training. Based on a generative adversarial network (GAN)\nfor image-to-image translation, our model disentangles the content of the\nimage, i.e., the morphological tissue structure, from the stain color\nattributes. It can be trained on multiple domains and, therefore, learns to\ncover different stain colors as well as other domain-specific variations\nintroduced in the slide preparation and imaging process. We demonstrate that\nHistAuGAN outperforms conventional color augmentation techniques on a\nclassification task on the publicly available dataset Camelyon17 and show that\nit is able to mitigate present batch effects.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:52:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wagner", "Sophia J.", ""], ["Khalili", "Nadieh", ""], ["Sharma", "Raghav", ""], ["Boxberg", "Melanie", ""], ["Marr", "Carsten", ""], ["de Back", "Walter", ""], ["Peng", "Tingying", ""]]}, {"id": "2107.12369", "submitter": "Dongdong Chen", "authors": "Suichan Li and Dongdong Chen and Yinpeng Chen and Lu Yuan and Lei\n  Zhang and Qi Chu and Bin Liu and Nenghai Yu", "title": "Improve Unsupervised Pretraining for Few-label Transfer", "comments": "ICCV 2021. arXiv admin note: substantial text overlap with\n  arXiv:2012.05899", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pretraining has achieved great success and many recent works\nhave shown unsupervised pretraining can achieve comparable or even slightly\nbetter transfer performance than supervised pretraining on downstream target\ndatasets. But in this paper, we find this conclusion may not hold when the\ntarget dataset has very few labeled samples for finetuning, \\ie, few-label\ntransfer. We analyze the possible reason from the clustering perspective: 1)\nThe clustering quality of target samples is of great importance to few-label\ntransfer; 2) Though contrastive learning is essential to learn how to cluster,\nits clustering quality is still inferior to supervised pretraining due to lack\nof label supervision. Based on the analysis, we interestingly discover that\nonly involving some unlabeled target domain into the unsupervised pretraining\ncan improve the clustering quality, subsequently reducing the transfer\nperformance gap with supervised pretraining. This finding also motivates us to\npropose a new progressive few-label transfer algorithm for real applications,\nwhich aims to maximize the transfer performance under a limited annotation\nbudget. To support our analysis and proposed method, we conduct extensive\nexperiments on nine different target datasets. Experimental results show our\nproposed method can significantly boost the few-label transfer performance of\nunsupervised pretraining.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:59:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Suichan", ""], ["Chen", "Dongdong", ""], ["Chen", "Yinpeng", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Chu", "Qi", ""], ["Liu", "Bin", ""], ["Yu", "Nenghai", ""]]}, {"id": "2107.12422", "submitter": "Miao Yin", "authors": "Miao Yin, Yang Sui, Siyu Liao and Bo Yuan", "title": "Towards Efficient Tensor Decomposition-Based DNN Model Compression with\n  Optimization Framework", "comments": "This paper was accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring\n(TR), has been widely studied for deep neural network (DNN) model compression,\nespecially for recurrent neural networks (RNNs). However, compressing\nconvolutional neural networks (CNNs) using TT/TR always suffers significant\naccuracy loss. In this paper, we propose a systematic framework for tensor\ndecomposition-based model compression using Alternating Direction Method of\nMultipliers (ADMM). By formulating TT decomposition-based model compression to\nan optimization problem with constraints on tensor ranks, we leverage ADMM\ntechnique to systemically solve this optimization problem in an iterative way.\nDuring this procedure, the entire DNN model is trained in the original\nstructure instead of TT format, but gradually enjoys the desired low tensor\nrank characteristics. We then decompose this uncompressed model to TT format\nand fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our\nframework is very general, and it works for both CNNs and RNNs, and can be\neasily modified to fit other tensor decomposition approaches. We evaluate our\nproposed framework on different DNN models for image classification and video\nrecognition tasks. Experimental results show that our ADMM-based TT-format\nmodels demonstrate very high compression performance with high accuracy.\nNotably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have\n1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and\nResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model\nachieves 2.47X FLOPs reduction without accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:31:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Yin", "Miao", ""], ["Sui", "Yang", ""], ["Liao", "Siyu", ""], ["Yuan", "Bo", ""]]}, {"id": "2107.12429", "submitter": "Pan Ji", "authors": "Pan Ji, Runze Li, Bir Bhanu, Yi Xu", "title": "MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth\n  Estimation for Indoor Environments", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-supervised depth estimation for indoor environments is more challenging\nthan its outdoor counterpart in at least the following two aspects: (i) the\ndepth range of indoor sequences varies a lot across different frames, making it\ndifficult for the depth network to induce consistent depth cues, whereas the\nmaximum distance in outdoor scenes mostly stays the same as the camera usually\nsees the sky; (ii) the indoor sequences contain much more rotational motions,\nwhich cause difficulties for the pose network, while the motions of outdoor\nsequences are pre-dominantly translational, especially for driving datasets\nsuch as KITTI. In this paper, special considerations are given to those\nchallenges and a set of good practices are consolidated for improving the\nperformance of self-supervised monocular depth estimation in indoor\nenvironments. The proposed method mainly consists of two novel modules, \\ie, a\ndepth factorization module and a residual pose estimation module, each of which\nis designed to respectively tackle the aforementioned challenges. The\neffectiveness of each module is shown through a carefully conducted ablation\nstudy and the demonstration of the state-of-the-art performance on three indoor\ndatasets, \\ie, EuRoC, NYUv2, and 7-scenes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:45:14 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 00:32:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ji", "Pan", ""], ["Li", "Runze", ""], ["Bhanu", "Bir", ""], ["Xu", "Yi", ""]]}, {"id": "2107.12435", "submitter": "Debesh Jha", "authors": "Debesh Jha, Pia H. Smedsrud, Dag Johansen, Thomas de Lange, H{\\aa}vard\n  D. Johansen, P{\\aa}l Halvorsen, and Michael A. Riegler", "title": "A Comprehensive Study on Colorectal Polyp Segmentation with ResUNet++,\n  Conditional Random Field and Test-Time Augmentation", "comments": "Accepted at IEEE Journal of BioMedical and Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colonoscopy is considered the gold standard for detection of colorectal\ncancer and its precursors. Existing examination methods are, however, hampered\nby high overall miss-rate, and many abnormalities are left undetected.\nComputer-Aided Diagnosis systems based on advanced machine learning algorithms\nare touted as a game-changer that can identify regions in the colon overlooked\nby the physicians during endoscopic examinations, and help detect and\ncharacterize lesions. In previous work, we have proposed the ResUNet++\narchitecture and demonstrated that it produces more efficient results compared\nwith its counterparts U-Net and ResUNet. In this paper, we demonstrate that\nfurther improvements to the overall prediction performance of the ResUNet++\narchitecture can be achieved by using conditional random field and test-time\naugmentation. We have performed extensive evaluations and validated the\nimprovements using six publicly available datasets: Kvasir-SEG, CVC-ClinicDB,\nCVC-ColonDB, ETIS-Larib Polyp DB, ASU-Mayo Clinic Colonoscopy Video Database,\nand CVC-VideoClinicDB. Moreover, we compare our proposed architecture and\nresulting model with other State-of-the-art methods. To explore the\ngeneralization capability of ResUNet++ on different publicly available polyp\ndatasets, so that it could be used in a real-world setting, we performed an\nextensive cross-dataset evaluation. The experimental results show that applying\nCRF and TTA improves the performance on various polyp segmentation datasets\nboth on the same dataset and cross-dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:55:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Jha", "Debesh", ""], ["Smedsrud", "Pia H.", ""], ["Johansen", "Dag", ""], ["de Lange", "Thomas", ""], ["Johansen", "H\u00e5vard D.", ""], ["Halvorsen", "P\u00e5l", ""], ["Riegler", "Michael A.", ""]]}, {"id": "2107.12461", "submitter": "A. Ben Hamza", "authors": "Hasib Zunair and A. Ben Hamza", "title": "Sharp U-Net: Depthwise Convolutional Network for Biomedical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The U-Net architecture, built upon the fully convolutional network, has\nproven to be effective in biomedical image segmentation. However, U-Net applies\nskip connections to merge semantically different low- and high-level\nconvolutional features, resulting in not only blurred feature maps, but also\nover- and under-segmented target regions. To address these limitations, we\npropose a simple, yet effective end-to-end depthwise encoder-decoder fully\nconvolutional network architecture, called Sharp U-Net, for binary and\nmulti-class biomedical image segmentation. The key rationale of Sharp U-Net is\nthat instead of applying a plain skip connection, a depthwise convolution of\nthe encoder feature map with a sharpening kernel filter is employed prior to\nmerging the encoder and decoder features, thereby producing a sharpened\nintermediate feature map of the same size as the encoder map. Using this\nsharpening filter layer, we are able to not only fuse semantically less\ndissimilar features, but also to smooth out artifacts throughout the network\nlayers during the early stages of training. Our extensive experiments on six\ndatasets show that the proposed Sharp U-Net model consistently outperforms or\nmatches the recent state-of-the-art baselines in both binary and multi-class\nsegmentation tasks, while adding no extra learnable parameters. Furthermore,\nSharp U-Net outperforms baselines that have more than three times the number of\nlearnable parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 20:27:25 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zunair", "Hasib", ""], ["Hamza", "A. Ben", ""]]}, {"id": "2107.12469", "submitter": "Michael Thoreau", "authors": "Michael Thoreau, Frazer Wilson", "title": "SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with\n  Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Access to high resolution satellite imagery has dramatically increased in\nrecent years as several new constellations have entered service. High revisit\nfrequencies as well as improved resolution has widened the use cases of\nsatellite imagery to areas such as humanitarian relief and even Search and\nRescue (SaR). We propose a novel remote sensing object detection dataset for\ndeep learning assisted SaR. This dataset contains only small objects that have\nbeen identified as potential targets as part of a live SaR response. We\nevaluate the application of popular object detection models to this dataset as\na baseline to inform further research. We also propose a novel object detection\nmetric, specifically designed to be used in a deep learning assisted SaR\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 20:52:36 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 16:54:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Thoreau", "Michael", ""], ["Wilson", "Frazer", ""]]}, {"id": "2107.12473", "submitter": "Aritra Chowdhury", "authors": "Alberto Santamaria-Pang, Jianwei Qiu, Aritra Chowdhury, James\n  Kubricht, Peter Tu, Iyer Naresh, Nurali Virani", "title": "Adversarial Attacks with Time-Scale Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel framework for real-time black-box universal attacks which\ndisrupts activations of early convolutional layers in deep learning models. Our\nhypothesis is that perturbations produced in the wavelet space disrupt early\nconvolutional layers more effectively than perturbations performed in the time\ndomain. The main challenge in adversarial attacks is to preserve low frequency\nimage content while minimally changing the most meaningful high frequency\ncontent. To address this, we formulate an optimization problem using time-scale\n(wavelet) representations as a dual space in three steps. First, we project\noriginal images into orthonormal sub-spaces for low and high scales via wavelet\ncoefficients. Second, we perturb wavelet coefficients for high scale projection\nusing a generator network. Third, we generate new adversarial images by\nprojecting back the original coefficients from the low scale and the perturbed\ncoefficients from the high scale sub-space. We provide a theoretical framework\nthat guarantees a dual mapping from time and time-scale domain representations.\nWe compare our results with state-of-the-art black-box attacks from\ngenerative-based and gradient-based models. We also verify efficacy against\nmultiple defense methods such as JPEG compression, Guided Denoiser and\nComdefend. Our results show that wavelet-based perturbations consistently\noutperform time-based attacks thus providing new insights into vulnerabilities\nof deep learning models and could potentially lead to robust architectures or\nnew defense and attack mechanisms by leveraging time-scale representations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 20:58:57 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Santamaria-Pang", "Alberto", ""], ["Qiu", "Jianwei", ""], ["Chowdhury", "Aritra", ""], ["Kubricht", "James", ""], ["Tu", "Peter", ""], ["Naresh", "Iyer", ""], ["Virani", "Nurali", ""]]}, {"id": "2107.12480", "submitter": "Bahar Azari", "authors": "Bahar Azari and Deniz Erdogmus", "title": "Circular-Symmetric Correlation Layer based on FFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the vast success of standard planar convolutional neural networks,\nthey are not the most efficient choice for analyzing signals that lie on an\narbitrarily curved manifold, such as a cylinder. The problem arises when one\nperforms a planar projection of these signals and inevitably causes them to be\ndistorted or broken where there is valuable information. We propose a\nCircular-symmetric Correlation Layer (CCL) based on the formalism of\nroto-translation equivariant correlation on the continuous group $S^1 \\times\n\\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier\nTransform (FFT) algorithm. We showcase the performance analysis of a general\nnetwork equipped with CCL on various recognition and classification tasks and\ndatasets. The PyTorch package implementation of CCL is provided online.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 21:06:20 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Azari", "Bahar", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "2107.12499", "submitter": "Praveen Ravirathinam", "authors": "Rahul Ghosh, Praveen Ravirathinam, Xiaowei Jia, Ankush Khandelwal,\n  David Mulla, Vipin Kumar", "title": "CalCROP21: A Georeferenced multi-spectral dataset of Satellite Imagery\n  and Crop Labels", "comments": "13 pages; 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mapping and monitoring crops is a key step towards sustainable\nintensification of agriculture and addressing global food security. A dataset\nlike ImageNet that revolutionized computer vision applications can accelerate\ndevelopment of novel crop mapping techniques. Currently, the United States\nDepartment of Agriculture (USDA) annually releases the Cropland Data Layer\n(CDL) which contains crop labels at 30m resolution for the entire United States\nof America. While CDL is state of the art and is widely used for a number of\nagricultural applications, it has a number of limitations (e.g., pixelated\nerrors, labels carried over from previous errors and absence of input imagery\nalong with class labels). In this work, we create a new semantic segmentation\nbenchmark dataset, which we call CalCROP21, for the diverse crops in the\nCentral Valley region of California at 10m spatial resolution using a Google\nEarth Engine based robust image processing pipeline and a novel attention based\nspatio-temporal semantic segmentation algorithm STATT. STATT uses re-sampled\n(interpolated) CDL labels for training, but is able to generate a better\nprediction than CDL by leveraging spatial and temporal patterns in Sentinel2\nmulti-spectral image series to effectively capture phenologic differences\namongst crops and uses attention to reduce the impact of clouds and other\natmospheric disturbances. We also present a comprehensive evaluation to show\nthat STATT has significantly better results when compared to the resampled CDL\nlabels. We have released the dataset and the processing pipeline code for\ngenerating the benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 22:20:16 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ghosh", "Rahul", ""], ["Ravirathinam", "Praveen", ""], ["Jia", "Xiaowei", ""], ["Khandelwal", "Ankush", ""], ["Mulla", "David", ""], ["Kumar", "Vipin", ""]]}, {"id": "2107.12507", "submitter": "Byeongjoon Noh", "authors": "Byeongjoon Noh, Hansaem Park, Hwasoo Yeo", "title": "Analyzing vehicle pedestrian interactions combining data cube structure\n  and predictive collision risk estimation model", "comments": "33 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic accidents are a threat to human lives, particularly pedestrians\ncausing premature deaths. Therefore, it is necessary to devise systems to\nprevent accidents in advance and respond proactively, using potential risky\nsituations as one of the surrogate safety measurements. This study introduces a\nnew concept of a pedestrian safety system that combines the field and the\ncentralized processes. The system can warn of upcoming risks immediately in the\nfield and improve the safety of risk frequent areas by assessing the safety\nlevels of roads without actual collisions. In particular, this study focuses on\nthe latter by introducing a new analytical framework for a crosswalk safety\nassessment with behaviors of vehicle/pedestrian and environmental features. We\nobtain these behavioral features from actual traffic video footage in the city\nwith complete automatic processing. The proposed framework mainly analyzes\nthese behaviors in multidimensional perspectives by constructing a data cube\nstructure, which combines the LSTM based predictive collision risk estimation\nmodel and the on line analytical processing operations. From the PCR estimation\nmodel, we categorize the severity of risks as four levels and apply the\nproposed framework to assess the crosswalk safety with behavioral features. Our\nanalytic experiments are based on two scenarios, and the various descriptive\nresults are harvested the movement patterns of vehicles and pedestrians by road\nenvironment and the relationships between risk levels and car speeds. Thus, the\nproposed framework can support decision makers by providing valuable\ninformation to improve pedestrian safety for future accidents, and it can help\nus better understand their behaviors near crosswalks proactively. In order to\nconfirm the feasibility and applicability of the proposed framework, we\nimplement and apply it to actual operating CCTVs in Osan City, Korea.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 23:00:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Noh", "Byeongjoon", ""], ["Park", "Hansaem", ""], ["Yeo", "Hwasoo", ""]]}, {"id": "2107.12512", "submitter": "Eduard Ramon", "authors": "Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime\n  Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer", "title": "H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning approaches that implicitly represent surface geometry using\ncoordinate-based neural representations have shown impressive results in the\nproblem of multi-view 3D reconstruction. The effectiveness of these techniques\nis, however, subject to the availability of a large number (several tens) of\ninput views of the scene, and computationally demanding optimizations. In this\npaper, we tackle these limitations for the specific problem of few-shot full 3D\nhead reconstruction, by endowing coordinate-based representations with a\nprobabilistic shape prior that enables faster convergence and better\ngeneralization when using few input images (down to three). First, we learn a\nshape model of 3D heads from thousands of incomplete raw scans using implicit\nrepresentations. At test time, we jointly overfit two coordinate-based neural\nnetworks to the scene, one modeling the geometry and another estimating the\nsurface radiance, using implicit differentiable rendering. We devise a\ntwo-stage optimization strategy in which the learned prior is used to\ninitialize and constrain the geometry during an initial optimization phase.\nThen, the prior is unfrozen and fine-tuned to the scene. By doing this, we\nachieve high-fidelity head reconstructions, including hair and shoulders, and\nwith a high level of detail that consistently outperforms both state-of-the-art\n3D Morphable Models methods in the few-shot scenario, and non-parametric\nmethods when large sets of views are available.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 23:04:18 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ramon", "Eduard", ""], ["Triginer", "Gil", ""], ["Escur", "Janna", ""], ["Pumarola", "Albert", ""], ["Garcia", "Jaime", ""], ["Giro-i-Nieto", "Xavier", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2107.12514", "submitter": "Jesse Thomason", "authors": "Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, Luke\n  Zettlemoyer", "title": "Language Grounding with 3D Objects", "comments": "https://github.com/snaredataset/snare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" When viewing\nmice on the shelf, the number of buttons or presence of a wire may not be\nvisible from certain angles or positions. Flat images of candidate mice may not\nprovide the discriminative information needed for \"wireless\". The world, and\nobjects in it, are not flat images but complex 3D shapes. If a human requests\nan object based on any of its basic properties, such as color, shape, or\ntexture, robots should perform the necessary exploration to accomplish the\ntask. In particular, while substantial effort and progress has been made on\nunderstanding explicitly visual attributes like color and category,\ncomparatively little progress has been made on understanding language about\nshapes and contours. In this work, we introduce a novel reasoning task that\ntargets both visual and non-visual language about 3D objects. Our new\nbenchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a\nmodel to choose which of two objects is being referenced by a natural language\ndescription. We introduce several CLIP-based models for distinguishing objects\nand demonstrate that while recent advances in jointly modeling vision and\nlanguage are useful for robotic language understanding, it is still the case\nthat these models are weaker at understanding the 3D nature of objects --\nproperties which play a key role in manipulation. In particular, we find that\nadding view estimation to language grounding models improves accuracy on both\nSNARE and when identifying objects referred to in language on a robot platform.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 23:35:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Thomason", "Jesse", ""], ["Shridhar", "Mohit", ""], ["Bisk", "Yonatan", ""], ["Paxton", "Chris", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2107.12518", "submitter": "Daniil Pakhomov", "authors": "Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir\n  Navab", "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with\n  Stylegan and CLIP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method that allows to automatically segment images into\nsemantically meaningful regions without human supervision. Derived regions are\nconsistent across different images and coincide with human-defined semantic\nclasses on some datasets. In cases where semantic regions might be hard for\nhuman to define and consistently label, our method is still able to find\nmeaningful and consistent semantic classes. In our work, we use pretrained\nStyleGAN2~\\cite{karras2020analyzing} generative model: clustering in the\nfeature space of the generative model allows to discover semantic classes. Once\nclasses are discovered, a synthetic dataset with generated images and\ncorresponding segmentation masks can be created. After that a segmentation\nmodel is trained on the synthetic dataset and is able to generalize to real\nimages. Additionally, by using CLIP~\\cite{radford2021learning} we are able to\nuse prompts defined in a natural language to discover some desired semantic\nclasses. We test our method on publicly available datasets and show\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 23:48:34 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Pakhomov", "Daniil", ""], ["Hira", "Sanchit", ""], ["Wagle", "Narayani", ""], ["Green", "Kemar E.", ""], ["Navab", "Nassir", ""]]}, {"id": "2107.12541", "submitter": "Runmin Cong", "authors": "Qi Tang, Runmin Cong, Ronghui Sheng, Lingzhi He, Dan Zhang, Yao Zhao,\n  and Sam Kwong", "title": "BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and\n  Monocular Depth Estimation", "comments": "10 pages, 7 figures, Accepted by ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Depth map super-resolution is a task with high practical application\nrequirements in the industry. Existing color-guided depth map super-resolution\nmethods usually necessitate an extra branch to extract high-frequency detail\ninformation from RGB image to guide the low-resolution depth map\nreconstruction. However, because there are still some differences between the\ntwo modalities, direct information transmission in the feature dimension or\nedge map dimension cannot achieve satisfactory result, and may even trigger\ntexture copying in areas where the structures of the RGB-D pair are\ninconsistent. Inspired by the multi-task learning, we propose a joint learning\nnetwork of depth map super-resolution (DSR) and monocular depth estimation\n(MDE) without introducing additional supervision labels. For the interaction of\ntwo subnetworks, we adopt a differentiated guidance strategy and design two\nbridges correspondingly. One is the high-frequency attention bridge (HABdg)\ndesigned for the feature encoding process, which learns the high-frequency\ninformation of the MDE task to guide the DSR task. The other is the content\nguidance bridge (CGBdg) designed for the depth map reconstruction process,\nwhich provides the content guidance learned from DSR task for MDE task. The\nentire network architecture is highly portable and can provide a paradigm for\nassociating the DSR and MDE tasks. Extensive experiments on benchmark datasets\ndemonstrate that our method achieves competitive performance. Our code and\nmodels are available at https://rmcong.github.io/proj_BridgeNet.html.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 01:28:23 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Tang", "Qi", ""], ["Cong", "Runmin", ""], ["Sheng", "Ronghui", ""], ["He", "Lingzhi", ""], ["Zhang", "Dan", ""], ["Zhao", "Yao", ""], ["Kwong", "Sam", ""]]}, {"id": "2107.12549", "submitter": "Yilin Wen", "authors": "Yilin Wen, Xiangyu Li, Hao Pan, Lei Yang, Zheng Wang, Taku Komura,\n  Wenping Wang", "title": "Disentangled Implicit Shape and Pose Learning for Scalable 6D Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D pose estimation of rigid objects from a single RGB image has seen\ntremendous improvements recently by using deep learning to combat complex\nreal-world variations, but a majority of methods build models on the per-object\nlevel, failing to scale to multiple objects simultaneously. In this paper, we\npresent a novel approach for scalable 6D pose estimation, by self-supervised\nlearning on synthetic data of multiple objects using a single autoencoder. To\nhandle multiple objects and generalize to unseen objects, we disentangle the\nlatent object shape and pose representations, so that the latent shape space\nmodels shape similarities, and the latent pose code is used for rotation\nretrieval by comparison with canonical rotations. To encourage shape space\nconstruction, we apply contrastive metric learning and enable the processing of\nunseen objects by referring to similar training objects. The different\nsymmetries across objects induce inconsistent latent pose spaces, which we\ncapture with a conditioned block producing shape-dependent pose codebooks by\nre-entangling shape and pose representations. We test our method on two\nmulti-object benchmarks with real data, T-LESS and NOCS REAL275, and show it\noutperforms existing RGB-based methods in terms of pose estimation accuracy and\ngeneralization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 01:55:30 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wen", "Yilin", ""], ["Li", "Xiangyu", ""], ["Pan", "Hao", ""], ["Yang", "Lei", ""], ["Wang", "Zheng", ""], ["Komura", "Taku", ""], ["Wang", "Wenping", ""]]}, {"id": "2107.12560", "submitter": "Jinchao Zhu", "authors": "Jinchao Zhu, Xiaoyu Zhang, Xian Fang, Junnan Liu", "title": "Perception-and-Regulation Network for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective fusion of different types of features is the key to salient object\ndetection. The majority of existing network structure design is based on the\nsubjective experience of scholars and the process of feature fusion does not\nconsider the relationship between the fused features and highest-level\nfeatures. In this paper, we focus on the feature relationship and propose a\nnovel global attention unit, which we term the \"perception- and-regulation\"\n(PR) block, that adaptively regulates the feature fusion process by explicitly\nmodeling interdependencies between features. The perception part uses the\nstructure of fully-connected layers in classification networks to learn the\nsize and shape of objects. The regulation part selectively strengthens and\nweakens the features to be fused. An imitating eye observation module (IEO) is\nfurther employed for improving the global perception ability of the network.\nThe imitation of foveal vision and peripheral vision enables IEO to scrutinize\nhighly detailed objects and to organize the broad spatial scene to better\nsegment objects. Sufficient experiments conducted on SOD datasets demonstrate\nthat the proposed method performs favorably against 22 state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 02:38:40 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhu", "Jinchao", ""], ["Zhang", "Xiaoyu", ""], ["Fang", "Xian", ""], ["Liu", "Junnan", ""]]}, {"id": "2107.12563", "submitter": "Yanzhao Wu", "authors": "Yanzhao Wu, Ling Liu, Ramana Kompella", "title": "Parallel Detection for Efficient Video Analytics at the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) trained object detectors are widely deployed in\nmany mission-critical systems for real time video analytics at the edge, such\nas autonomous driving and video surveillance. A common performance requirement\nin these mission-critical edge services is the near real-time latency of online\nobject detection on edge devices. However, even with well-trained DNN object\ndetectors, the online detection quality at edge may deteriorate for a number of\nreasons, such as limited capacity to run DNN object detection models on\nheterogeneous edge devices, and detection quality degradation due to random\nframe dropping when the detection processing rate is significantly slower than\nthe incoming video frame rate. This paper addresses these problems by\nexploiting multi-model multi-device detection parallelism for fast object\ndetection in edge systems with heterogeneous edge devices. First, we analyze\nthe performance bottleneck of running a well-trained DNN model at edge for real\ntime online object detection. We use the offline detection as a reference\nmodel, and examine the root cause by analyzing the mismatch among the incoming\nvideo streaming rate, video processing rate for object detection, and output\nrate for real time detection visualization of video streaming. Second, we study\nperformance optimizations by exploiting multi-model detection parallelism. We\nshow that the model-parallel detection approach can effectively speed up the\nFPS detection processing rate, minimizing the FPS disparity with the incoming\nvideo frame rate on heterogeneous edge devices. We evaluate the proposed\napproach using SSD300 and YOLOv3 on benchmark videos of different video stream\nrates. The results show that exploiting multi-model detection parallelism can\nspeed up the online object detection processing rate and deliver near real-time\nobject detection performance for efficient video analytics at edge.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 02:50:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wu", "Yanzhao", ""], ["Liu", "Ling", ""], ["Kompella", "Ramana", ""]]}, {"id": "2107.12569", "submitter": "Bo Miao", "authors": "Bo Miao, Mohammed Bennamoun, Yongsheng Gao, Ajmal Mian", "title": "Self-Supervised Video Object Segmentation by Motion-Aware Mask\n  Propagation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a self-supervised spatio-temporal matching method coined\nMotion-Aware Mask Propagation (MAMP) for semi-supervised video object\nsegmentation. During training, MAMP leverages the frame reconstruction task to\ntrain the model without the need for annotations. During inference, MAMP\nextracts high-resolution features from each frame to build a memory bank from\nthe features as well as the predicted masks of selected past frames. MAMP then\npropagates the masks from the memory bank to subsequent frames according to our\nmotion-aware spatio-temporal matching module, also proposed in this paper.\nEvaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves\nstate-of-the-art performance with stronger generalization ability compared to\nexisting self-supervised methods, i.e. 4.9\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on DAVIS-2017 and 4.85\\% higher mean\n$\\mathcal{J}\\&\\mathcal{F}$ on the unseen categories of YouTube-VOS than the\nnearest competitor. Moreover, MAMP performs on par with many supervised video\nobject segmentation methods. Our code is available at:\n\\url{https://github.com/bo-miao/MAMP}.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:07:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Miao", "Bo", ""], ["Bennamoun", "Mohammed", ""], ["Gao", "Yongsheng", ""], ["Mian", "Ajmal", ""]]}, {"id": "2107.12571", "submitter": "Denis Gudovskiy", "authors": "Denis Gudovskiy, Shun Ishizaka, Kazuki Kozuka", "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via\n  Conditional Normalizing Flows", "comments": "Accepted to WACV 2022. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised anomaly detection with localization has many practical\napplications when labeling is infeasible and, moreover, when anomaly examples\nare completely missing in the train data. While recently proposed models for\nsuch data setup achieve high accuracy metrics, their complexity is a limiting\nfactor for real-time processing. In this paper, we propose a real-time model\nand analytically derive its relationship to prior methods. Our CFLOW-AD model\nis based on a conditional normalizing flow framework adopted for anomaly\ndetection with localization. In particular, CFLOW-AD consists of a\ndiscriminatively pretrained encoder followed by a multi-scale generative\ndecoders where the latter explicitly estimate likelihood of the encoded\nfeatures. Our approach results in a computationally and memory-efficient model:\nCFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art\nwith the same input setting. Our experiments on the MVTec dataset show that\nCFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by\n1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source\nour code with fully reproducible experiments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:10:38 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gudovskiy", "Denis", ""], ["Ishizaka", "Shun", ""], ["Kozuka", "Kazuki", ""]]}, {"id": "2107.12579", "submitter": "Zhonghua Wu", "authors": "Xiangxi Shi, Zhonghua Wu, Guosheng Lin, Jianfei Cai and Shafiq Joty", "title": "Remember What You have drawn: Semantic Image Manipulation with Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image manipulation with natural language, which aims to manipulate images\nwith the guidance of language descriptions, has been a challenging problem in\nthe fields of computer vision and natural language processing (NLP). Currently,\na number of efforts have been made for this task, but their performances are\nstill distant away from generating realistic and text-conformed manipulated\nimages. Therefore, in this paper, we propose a memory-based Image Manipulation\nNetwork (MIM-Net), where a set of memories learned from images is introduced to\nsynthesize the texture information with the guidance of the textual\ndescription. We propose a two-stage network with an additional reconstruction\nstage to learn the latent memories efficiently. To avoid the unnecessary\nbackground changes, we propose a Target Localization Unit (TLU) to focus on the\nmanipulation of the region mentioned by the text. Moreover, to learn a robust\nmemory, we further propose a novel randomized memory training loss. Experiments\non the four popular datasets show the better performance of our method compared\nto the existing ones.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:41:59 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Shi", "Xiangxi", ""], ["Wu", "Zhonghua", ""], ["Lin", "Guosheng", ""], ["Cai", "Jianfei", ""], ["Joty", "Shafiq", ""]]}, {"id": "2107.12585", "submitter": "Song Tang", "authors": "Song Tang, Yan Yang, Zhiyuan Ma, Norman Hendrich, Fanyu Zeng, Shuzhi\n  Sam Ge, Changshui Zhang, Jianwei Zhang", "title": "Nearest Neighborhood-Based Deep Clustering for Source Data-absent\n  Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic setting of unsupervised domain adaptation (UDA), the labeled\nsource data are available in the training phase. However, in many real-world\nscenarios, owing to some reasons such as privacy protection and information\nsecurity, the source data is inaccessible, and only a model trained on the\nsource domain is available. This paper proposes a novel deep clustering method\nfor this challenging task. Aiming at the dynamical clustering at feature-level,\nwe introduce extra constraints hidden in the geometric structure between data\nto assist the process. Concretely, we propose a geometry-based constraint,\nnamed semantic consistency on the nearest neighborhood (SCNNH), and use it to\nencourage robust clustering. To reach this goal, we construct the nearest\nneighborhood for every target data and take it as the fundamental clustering\nunit by building our objective on the geometry. Also, we develop a more\nSCNNH-compliant structure with an additional semantic credibility constraint,\nnamed semantic hyper-nearest neighborhood (SHNNH). After that, we extend our\nmethod to this new geometry. Extensive experiments on three challenging UDA\ndatasets indicate that our method achieves state-of-the-art results. The\nproposed method has significant improvement on all datasets (as we adopt SHNNH,\nthe average accuracy increases by over 3.0\\% on the large-scaled dataset). Code\nis available at https://github.com/tntek/N2DCX.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:13:59 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Tang", "Song", ""], ["Yang", "Yan", ""], ["Ma", "Zhiyuan", ""], ["Hendrich", "Norman", ""], ["Zeng", "Fanyu", ""], ["Ge", "Shuzhi Sam", ""], ["Zhang", "Changshui", ""], ["Zhang", "Jianwei", ""]]}, {"id": "2107.12589", "submitter": "Fa-Ting Hong", "authors": "Fa-Ting Hong, Jia-Chang Feng, Dan Xu, Ying Shan, Wei-Shi Zheng", "title": "Cross-modal Consensus Network for Weakly Supervised Temporal Action\n  Localization", "comments": "ACM International Conference on Multimedia, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised temporal action localization (WS-TAL) is a challenging task\nthat aims to localize action instances in the given video with video-level\ncategorical supervision. Both appearance and motion features are used in\nprevious works, while they do not utilize them in a proper way but apply simple\nconcatenation or score-level fusion. In this work, we argue that the features\nextracted from the pretrained extractor, e.g., I3D, are not the\nWS-TALtask-specific features, thus the feature re-calibration is needed for\nreducing the task-irrelevant information redundancy. Therefore, we propose a\ncross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we\nmainly introduce two identical proposed cross-modal consensus modules (CCM)\nthat design a cross-modal attention mechanism to filter out the task-irrelevant\ninformation redundancy using the global information from the main modality and\nthe cross-modal local information of the auxiliary modality. Moreover, we treat\nthe attention weights derived from each CCMas the pseudo targets of the\nattention weights derived from another CCM to maintain the consistency between\nthe predictions derived from two CCMs, forming a mutual learning manner.\nFinally, we conduct extensive experiments on two common used temporal action\nlocalization datasets, THUMOS14 and ActivityNet1.2, to verify our method and\nachieve the state-of-the-art results. The experimental results show that our\nproposed cross-modal consensus module can produce more representative features\nfor temporal action localization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:21:01 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Hong", "Fa-Ting", ""], ["Feng", "Jia-Chang", ""], ["Xu", "Dan", ""], ["Shan", "Ying", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2107.12598", "submitter": "Daping Zhang", "authors": "Daping Zhang, Hongyu Yang, Jiayu Cao", "title": "Identify Apple Leaf Diseases Using Deep Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agriculture is an essential industry in the both society and economy of a\ncountry. However, the pests and diseases cause a great amount of reduction in\nagricultural production while there is not sufficient guidance for farmers to\navoid this disaster. To address this problem, we apply CNNs to plant disease\nrecognition by building a classification model. Within the dataset of 3,642\nimages of apple leaves, We use a pre-trained image classification model\nRestnet34 based on a Convolutional neural network (CNN) with the Fastai\nframework in order to save the training time. Overall, the accuracy of\nclassification is 93.765%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:55:16 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhang", "Daping", ""], ["Yang", "Hongyu", ""], ["Cao", "Jiayu", ""]]}, {"id": "2107.12600", "submitter": "Pan Xie", "authors": "Pan Xie and Mengyi Zhao and Xiaohui Hu", "title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware\n  Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the superiority of Transformer in learning long-term dependency, the\nsign language Transformer model achieves remarkable progress in Sign Language\nRecognition (SLR) and Translation (SLT). However, there are several issues with\nthe Transformer that prevent it from better sign language understanding. The\nfirst issue is that the self-attention mechanism learns sign video\nrepresentation in a frame-wise manner, neglecting the temporal semantic\nstructure of sign gestures. Secondly, the attention mechanism with absolute\nposition encoding is direction and distance unaware, thus limiting its ability.\nTo address these issues, we propose a new model architecture, namely PiSLTRc,\nwith two distinctive characteristics: (i) content-aware and position-aware\nconvolution layers. Specifically, we explicitly select relevant features using\na novel content-aware neighborhood gathering method. Then we aggregate these\nfeatures with position-informed temporal convolution layers, thus generating\nrobust neighborhood-enhanced sign representation. (ii) injecting the relative\nposition information to the attention mechanism in the encoder, decoder, and\neven encoder-decoder cross attention. Compared with the vanilla Transformer\nmodel, our model performs consistently better on three large-scale sign\nlanguage benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore,\nextensive experiments demonstrate that the proposed method achieves\nstate-of-the-art performance on translation quality with $+1.6$ BLEU\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 05:01:27 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xie", "Pan", ""], ["Zhao", "Mengyi", ""], ["Hu", "Xiaohui", ""]]}, {"id": "2107.12604", "submitter": "Xiaotian Han", "authors": "Xiaotian Han, Jianwei Yang, Houdong Hu, Lei Zhang, Jianfeng Gao,\n  Pengchuan Zhang", "title": "Image Scene Graph Generation (SGG) Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a surge of interest in image scene graph generation (object,\nattribute and relationship detection) due to the need of building fine-grained\nimage understanding models that go beyond object detection. Due to the lack of\na good benchmark, the reported results of different scene graph generation\nmodels are not directly comparable, impeding the research progress. We have\ndeveloped a much-needed scene graph generation benchmark based on the\nmaskrcnn-benchmark and several popular models. This paper presents main\nfeatures of our benchmark and a comprehensive ablation study of scene graph\ngeneration models using the Visual Genome and OpenImages Visual relationship\ndetection datasets. Our codebase is made publicly available at\nhttps://github.com/microsoft/scene_graph_benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 05:10:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Han", "Xiaotian", ""], ["Yang", "Jianwei", ""], ["Hu", "Houdong", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""], ["Zhang", "Pengchuan", ""]]}, {"id": "2107.12617", "submitter": "Rundong Ge", "authors": "Rundong Ge, Giuseppe Loianno", "title": "VIPose: Real-time Visual-Inertial 6D Object Pose Tracking", "comments": "Accepted by The IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the 6D pose of objects is beneficial for robotics tasks such as\ntransportation, autonomous navigation, manipulation as well as in scenarios\nbeyond robotics like virtual and augmented reality. With respect to single\nimage pose estimation, pose tracking takes into account the temporal\ninformation across multiple frames to overcome possible detection\ninconsistencies and to improve the pose estimation efficiency. In this work, we\nintroduce a novel Deep Neural Network (DNN) called VIPose, that combines\ninertial and camera data to address the object pose tracking problem in\nreal-time. The key contribution is the design of a novel DNN architecture which\nfuses visual and inertial features to predict the objects' relative 6D pose\nbetween consecutive image frames. The overall 6D pose is then estimated by\nconsecutively combining relative poses. Our approach shows remarkable pose\nestimation results for heavily occluded objects that are well known to be very\nchallenging to handle by existing state-of-the-art solutions. The effectiveness\nof the proposed approach is validated on a new dataset called VIYCB with RGB\nimage, IMU data, and accurate 6D pose annotations created by employing an\nautomated labeling technique. The approach presents accuracy performances\ncomparable to state-of-the-art techniques, but with additional benefit to be\nreal-time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 06:10:23 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ge", "Rundong", ""], ["Loianno", "Giuseppe", ""]]}, {"id": "2107.12618", "submitter": "Haisheng Su", "authors": "Haisheng Su, Peiqin Zhuang, Yukun Li, Dongliang Wang, Weihao Gan, Wei\n  Wu, Yu Qiao", "title": "Transferable Knowledge-Based Multi-Granularity Aggregation Network for\n  Temporal Action Localization: Submission to ActivityNet Challenge 2021", "comments": "Winner of HACS21 Challenge Weakly Supervised Learning Track with\n  extra data. arXiv admin note: text overlap with arXiv:2103.13141", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents an overview of our solution used in the\nsubmission to 2021 HACS Temporal Action Localization Challenge on both\nSupervised Learning Track and Weakly-Supervised Learning Track. Temporal Action\nLocalization (TAL) requires to not only precisely locate the temporal\nboundaries of action instances, but also accurately classify the untrimmed\nvideos into specific categories. However, Weakly-Supervised TAL indicates\nlocating the action instances using only video-level class labels. In this\npaper, to train a supervised temporal action localizer, we adopt Temporal\nContext Aggregation Network (TCANet) to generate high-quality action proposals\nthrough ``local and global\" temporal context aggregation and complementary as\nwell as progressive boundary refinement. As for the WSTAL, a novel framework is\nproposed to handle the poor quality of CAS generated by simple classification\nnetwork, which can only focus on local discriminative parts, rather than locate\nthe entire interval of target actions. Further inspired by the transfer\nlearning method, we also adopt an additional module to transfer the knowledge\nfrom trimmed videos (HACS Clips dataset) to untrimmed videos (HACS Segments\ndataset), aiming at promoting the classification performance on untrimmed\nvideos. Finally, we employ a boundary regression module embedded with\nOuter-Inner-Contrastive (OIC) loss to automatically predict the boundaries\nbased on the enhanced CAS. Our proposed scheme achieves 39.91 and 29.78 average\nmAP on the challenge testing set of supervised and weakly-supervised temporal\naction localization track respectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 06:18:21 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Su", "Haisheng", ""], ["Zhuang", "Peiqin", ""], ["Li", "Yukun", ""], ["Wang", "Dongliang", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Qiao", "Yu", ""]]}, {"id": "2107.12619", "submitter": "Changan Wang", "authors": "Changan Wang, Qingyu Song, Boshen Zhang, Yabiao Wang, Ying Tai, Xuyi\n  Hu, Chengjie Wang, Jilin Li, Jiayi Ma, Yang Wu", "title": "Uniformity in Heterogeneity:Diving Deep into Count Interval Partition\n  for Crowd Counting", "comments": "To be appear in ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 06:24:15 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wang", "Changan", ""], ["Song", "Qingyu", ""], ["Zhang", "Boshen", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Hu", "Xuyi", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Ma", "Jiayi", ""], ["Wu", "Yang", ""]]}, {"id": "2107.12628", "submitter": "Bo Li", "authors": "Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Ziwei Liu, Dongsheng Li", "title": "Energy-Based Open-World Uncertainty Modeling for Confidence Calibration", "comments": "ICCV 2021 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Confidence calibration is of great importance to the reliability of decisions\nmade by machine learning systems. However, discriminative classifiers based on\ndeep neural networks are often criticized for producing overconfident\npredictions that fail to reflect the true correctness likelihood of\nclassification accuracy. We argue that such an inability to model uncertainty\nis mainly caused by the closed-world nature in softmax: a model trained by the\ncross-entropy loss will be forced to classify input into one of $K$ pre-defined\ncategories with high probability. To address this problem, we for the first\ntime propose a novel $K$+1-way softmax formulation, which incorporates the\nmodeling of open-world uncertainty as the extra dimension. To unify the\nlearning of the original $K$-way classification task and the extra dimension\nthat models uncertainty, we propose a novel energy-based objective function,\nand moreover, theoretically prove that optimizing such an objective essentially\nforces the extra dimension to capture the marginal data distribution. Extensive\nexperiments show that our approach, Energy-based Open-World Softmax\n(EOW-Softmax), is superior to existing state-of-the-art methods in improving\nconfidence calibration.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 06:52:06 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 06:09:48 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wang", "Yezhen", ""], ["Li", "Bo", ""], ["Che", "Tong", ""], ["Zhou", "Kaiyang", ""], ["Liu", "Ziwei", ""], ["Li", "Dongsheng", ""]]}, {"id": "2107.12636", "submitter": "Wen Wang", "authors": "Wen Wang, Yang Cao, Jing Zhang, Fengxiang He, Zheng-Jun Zha, Yonggang\n  Wen, Dacheng Tao", "title": "Exploring Sequence Feature Alignment for Domain Adaptive Detection\n  Transformers", "comments": "Accepted by ACM MM2021. Source code is available at:\n  https://github.com/encounter1997/SFA", "journal-ref": null, "doi": "10.1145/3474085.3475317", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection transformers have recently shown promising object detection results\nand attracted increasing attention. However, how to develop effective domain\nadaptation techniques to improve its cross-domain performance remains\nunexplored and unclear. In this paper, we delve into this topic and empirically\nfind that direct feature distribution alignment on the CNN backbone only brings\nlimited improvements, as it does not guarantee domain-invariant sequence\nfeatures in the transformer for prediction. To address this issue, we propose a\nnovel Sequence Feature Alignment (SFA) method that is specially designed for\nthe adaptation of detection transformers. Technically, SFA consists of a domain\nquery-based feature alignment (DQFA) module and a token-wise feature alignment\n(TDA) module. In DQFA, a novel domain query is used to aggregate and align\nglobal context from the token sequence of both domains. DQFA reduces the domain\ndiscrepancy in global feature representations and object relations when\ndeploying in the transformer encoder and decoder, respectively. Meanwhile, TDA\naligns token features in the sequence from both domains, which reduces the\ndomain gaps in local and instance-level feature representations in the\ntransformer encoder and decoder, respectively. Besides, a novel bipartite\nmatching consistency loss is proposed to enhance the feature discriminability\nfor robust object detection. Experiments on three challenging benchmarks show\nthat SFA outperforms state-of-the-art domain adaptive object detection methods.\nCode has been made available at: https://github.com/encounter1997/SFA.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 07:17:12 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wang", "Wen", ""], ["Cao", "Yang", ""], ["Zhang", "Jing", ""], ["He", "Fengxiang", ""], ["Zha", "Zheng-Jun", ""], ["Wen", "Yonggang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2107.12642", "submitter": "Ning Huyan", "authors": "Ning Huyan, Dou Quan, Xiangrong Zhang, Xuefeng Liang, Jocelyn\n  Chanussot, Licheng Jiao", "title": "Unsupervised Outlier Detection using Memory and Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is one of the most important processes taken to create\ngood, reliable data in machine learning. The most methods of outlier detection\nleverage an auxiliary reconstruction task by assuming that outliers are more\ndifficult to be recovered than normal samples (inliers). However, it is not\nalways true, especially for auto-encoder (AE) based models. They may recover\ncertain outliers even outliers are not in the training data, because they do\nnot constrain the feature learning. Instead, we think outlier detection can be\ndone in the feature space by measuring the feature distance between outliers\nand inliers. We then propose a framework, MCOD, using a memory module and a\ncontrastive learning module. The memory module constrains the consistency of\nfeatures, which represent the normal data. The contrastive learning module\nlearns more discriminating features, which boosts the distinction between\noutliers and inliers. Extensive experiments on four benchmark datasets show\nthat our proposed MCOD achieves a considerable performance and outperforms nine\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 07:35:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Huyan", "Ning", ""], ["Quan", "Dou", ""], ["Zhang", "Xiangrong", ""], ["Liang", "Xuefeng", ""], ["Chanussot", "Jocelyn", ""], ["Jiao", "Licheng", ""]]}, {"id": "2107.12646", "submitter": "Ertug Olcay", "authors": "Erkin T\\\"urk\\\"oz, Ertug Olcay, Timo Oksanen", "title": "Computer Vision-Based Guidance Assistance Concept for Plowing Using\n  RGB-D Camera", "comments": "Accepted to be published in Proceedings of the 2021 IEEE\n  International Conference on Imaging Systems and Techniques, August 24-26 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a concept of computer vision-based guidance assistance\nfor agricultural vehicles to increase the accuracy in plowing and reduce\ndriver's cognitive burden in long-lasting tillage operations. Plowing is a\ncommon agricultural practice to prepare the soil for planting in many countries\nand it can take place both in the spring and the fall. Since plowing operation\nrequires high traction forces, it causes increased energy consumption.\nMoreover, longer operation time due to unnecessary maneuvers leads to higher\nfuel consumption. To provide necessary information for the driver and the\ncontrol unit of the tractor, a first concept of furrow detection system based\non an RGB-D camera was developed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 07:55:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["T\u00fcrk\u00f6z", "Erkin", ""], ["Olcay", "Ertug", ""], ["Oksanen", "Timo", ""]]}, {"id": "2107.12651", "submitter": "Xinzhe Han", "authors": "Xinzhe Han, Shuhui Wang, Chi Su, Qingming Huang, Qi Tian", "title": "Greedy Gradient Ensemble for Robust Visual Question Answering", "comments": "Accepted by ICCV 2021. Code: https://github.com/GeraldHan/GGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:02:49 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Han", "Xinzhe", ""], ["Wang", "Shuhui", ""], ["Su", "Chi", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2107.12654", "submitter": "Da-Wei Zhou", "authors": "Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan", "title": "Co-Transport for Class-Incremental Learning", "comments": "Accepted to ACM Multimedia 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional learning systems are trained in closed-world for a fixed number\nof classes, and need pre-collected datasets in advance. However, new classes\noften emerge in real-world applications and should be learned incrementally.\nFor example, in electronic commerce, new types of products appear daily, and in\na social media community, new topics emerge frequently. Under such\ncircumstances, incremental models should learn several new classes at a time\nwithout forgetting. We find a strong correlation between old and new classes in\nincremental learning, which can be applied to relate and facilitate different\nlearning stages mutually. As a result, we propose CO-transport for class\nIncremental Learning (COIL), which learns to relate across incremental tasks\nwith the class-wise semantic relationship. In detail, co-transport has two\naspects: prospective transport tries to augment the old classifier with optimal\ntransported knowledge as fast model adaptation. Retrospective transport aims to\ntransport new class classifiers backward as old ones to overcome forgetting.\nWith these transports, COIL efficiently adapts to new tasks, and stably resists\nforgetting. Experiments on benchmark and real-world multimedia datasets\nvalidate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:07:02 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhou", "Da-Wei", ""], ["Ye", "Han-Jia", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2107.12655", "submitter": "Sungmin Woo", "authors": "Sungmin Woo, Dogyoon Lee, Junhyeop Lee, Sangwon Hwang, Woojin Kim and\n  Sangyoun Lee", "title": "CKConv: Learning Feature Voxelization for Point Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable success of deep learning, optimal convolution\noperation on point cloud remains indefinite due to its irregular data\nstructure. In this paper, we present Cubic Kernel Convolution (CKConv) that\nlearns to voxelize the features of local points by exploiting both continuous\nand discrete convolutions. Our continuous convolution uniquely employs a 3D\ncubic form of kernel weight representation that splits a feature into voxels in\nembedding space. By consecutively applying discrete 3D convolutions on the\nvoxelized features in a spatial manner, preceding continuous convolution is\nforced to learn spatial feature mapping, i.e., feature voxelization. In this\nway, geometric information can be detailed by encoding with subdivided\nfeatures, and our 3D convolutions on these fixed structured data do not suffer\nfrom discretization artifacts thanks to voxelization in embedding space.\nFurthermore, we propose a spatial attention module, Local Set Attention (LSA),\nto provide comprehensive structure awareness within the local point set and\nhence produce representative features. By learning feature voxelization with\nLSA, CKConv can extract enriched features for effective point cloud analysis.\nWe show that CKConv has great applicability to point cloud processing tasks\nincluding object classification, object part segmentation, and scene semantic\nsegmentation with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:08:02 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Woo", "Sungmin", ""], ["Lee", "Dogyoon", ""], ["Lee", "Junhyeop", ""], ["Hwang", "Sangwon", ""], ["Kim", "Woojin", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2107.12657", "submitter": "Sohee Kim", "authors": "Sohee Kim, Seungkyu Lee", "title": "Continual Learning with Neuron Activation Importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continual learning is a concept of online learning with multiple sequential\ntasks. One of the critical barriers of continual learning is that a network\nshould learn a new task keeping the knowledge of old tasks without access to\nany data of the old tasks. In this paper, we propose a neuron activation\nimportance-based regularization method for stable continual learning regardless\nof the order of tasks. We conduct comprehensive experiments on existing\nbenchmark data sets to evaluate not just the stability and plasticity of our\nmethod with improved classification accuracy also the robustness of the\nperformance along the changes of task order.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:09:32 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kim", "Sohee", ""], ["Lee", "Seungkyu", ""]]}, {"id": "2107.12664", "submitter": "Shi-Xue Zhang", "authors": "Shi-Xue Zhang, Xiaobin Zhu, Chun Yang, Hongfa Wang, Xu-Cheng Yin", "title": "Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection", "comments": "10 pages, 8 figures, Accepted by ICCV2021", "journal-ref": "ICCV2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model. In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:25:24 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 09:19:10 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Shi-Xue", ""], ["Zhu", "Xiaobin", ""], ["Yang", "Chun", ""], ["Wang", "Hongfa", ""], ["Yin", "Xu-Cheng", ""]]}, {"id": "2107.12666", "submitter": "Changxing Ding", "authors": "Zefeng Ding, Changxing Ding, Zhiyin Shao, Dacheng Tao", "title": "Semantically Self-Aligned Network for Text-to-Image Part-aware Person\n  Re-identification", "comments": "A new database is provided. Code will be released", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image person re-identification (ReID) aims to search for images\ncontaining a person of interest using textual descriptions. However, due to the\nsignificant modality gap and the large intra-class variance in textual\ndescriptions, text-to-image ReID remains a challenging problem. Accordingly, in\nthis paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the\nabove problems. First, we propose a novel method that automatically extracts\nsemantically aligned part-level features from the two modalities. Second, we\ndesign a multi-view non-local network that captures the relationships between\nbody parts, thereby establishing better correspondences between body parts and\nnoun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use\nof textual descriptions for other images of the same identity to provide extra\nsupervision, thereby effectively reducing the intra-class variance in textual\nfeatures. Finally, to expedite future research in text-to-image ReID, we build\na new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN\noutperforms state-of-the-art approaches by significant margins. Both the new\nICFG-PEDES database and the SSAN code are available at\nhttps://github.com/zifyloo/SSAN.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:26:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ding", "Zefeng", ""], ["Ding", "Changxing", ""], ["Shao", "Zhiyin", ""], ["Tao", "Dacheng", ""]]}, {"id": "2107.12673", "submitter": "Paul Wimmer", "authors": "Paul Wimmer, Jens Mehnert, Alexandru Condurache", "title": "COPS: Controlled Pruning Before Training Starts", "comments": "Accepted by The International Joint Conference on Neural Network\n  (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural network (DNN) pruning techniques, applied\none-shot before training starts, evaluate sparse architectures with the help of\na single criterion -- called pruning score. Pruning weights based on a solitary\nscore works well for some architectures and pruning rates but may also fail for\nother ones. As a common baseline for pruning scores, we introduce the notion of\na generalized synaptic score (GSS). In this work we do not concentrate on a\nsingle pruning criterion, but provide a framework for combining arbitrary GSSs\nto create more powerful pruning strategies. These COmbined Pruning Scores\n(COPS) are obtained by solving a constrained optimization problem. Optimizing\nfor more than one score prevents the sparse network to overly specialize on an\nindividual task, thus COntrols Pruning before training Starts. The\ncombinatorial optimization problem given by COPS is relaxed on a linear program\n(LP). This LP is solved analytically and determines a solution for COPS.\nFurthermore, an algorithm to compute it for two scores numerically is proposed\nand evaluated. Solving COPS in such a way has lower complexity than the best\ngeneral LP solver. In our experiments we compared pruning with COPS against\nstate-of-the-art methods for different network architectures and image\nclassification tasks and obtained improved results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:48:01 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wimmer", "Paul", ""], ["Mehnert", "Jens", ""], ["Condurache", "Alexandru", ""]]}, {"id": "2107.12674", "submitter": "Eitan Kosman", "authors": "Eitan Kosman, Dotan Di Castro", "title": "Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time\n  Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:52:40 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kosman", "Eitan", ""], ["Di Castro", "Dotan", ""]]}, {"id": "2107.12675", "submitter": "Christian Rathgeb", "authors": "Pawel Drozdowski, Fabian Stockhardt, Christian Rathgeb, Dail\\'e\n  Osorio-Roig, Christoph Busch", "title": "Feature Fusion Methods for Indexing and Retrieval of Biometric Data:\n  Application to Face Recognition with Privacy Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally efficient, accurate, and privacy-preserving data storage and\nretrieval are among the key challenges faced by practical deployments of\nbiometric identification systems worldwide. In this work, a method of protected\nindexing of biometric data is presented. By utilising feature-level fusion of\nintelligently paired templates, a multi-stage search structure is created.\nDuring retrieval, the list of potential candidate identities is successively\npre-filtered, thereby reducing the number of template comparisons necessary for\na biometric identification transaction. Protection of the biometric probe\ntemplates, as well as the stored reference templates and the created index is\ncarried out using homomorphic encryption. The proposed method is extensively\nevaluated in closed-set and open-set identification scenarios on publicly\navailable databases using two state-of-the-art open-source face recognition\nsystems. With respect to a typical baseline algorithm utilising an exhaustive\nsearch-based retrieval algorithm, the proposed method enables a reduction of\nthe computational workload associated with a biometric identification\ntransaction by 90%, while simultaneously suffering no degradation of the\nbiometric performance. Furthermore, by facilitating a seamless integration of\ntemplate protection with open-source homomorphic encryption libraries, the\nproposed method guarantees unlinkability, irreversibility, and renewability of\nthe protected biometric data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:53:29 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Drozdowski", "Pawel", ""], ["Stockhardt", "Fabian", ""], ["Rathgeb", "Christian", ""], ["Osorio-Roig", "Dail\u00e9", ""], ["Busch", "Christoph", ""]]}, {"id": "2107.12689", "submitter": "Nick Byrne", "authors": "Nick Byrne, James R Clough, Isra Valverde, Giovanni Montana, Andrew P\n  King", "title": "A persistent homology-based topological loss for CNN-based multi-class\n  segmentation of CMR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class segmentation of cardiac magnetic resonance (CMR) images seeks a\nseparation of data into anatomical components with known structure and\nconfiguration. The most popular CNN-based methods are optimised using pixel\nwise loss functions, ignorant of the spatially extended features that\ncharacterise anatomy. Therefore, whilst sharing a high spatial overlap with the\nground truth, inferred CNN-based segmentations can lack coherence, including\nspurious connected components, holes and voids. Such results are implausible,\nviolating anticipated anatomical topology. In response, (single-class)\npersistent homology-based loss functions have been proposed to capture global\nanatomical features. Our work extends these approaches to the task of\nmulti-class segmentation. Building an enriched topological description of all\nclass labels and class label pairs, our loss functions make predictable and\nstatistically significant improvements in segmentation topology using a\nCNN-based post-processing framework. We also present (and make available) a\nhighly efficient implementation based on cubical complexes and parallel\nexecution, enabling practical application within high resolution 3D data for\nthe first time. We demonstrate our approach on 2D short axis and 3D whole heart\nCMR segmentation, advancing a detailed and faithful analysis of performance on\ntwo publicly available datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 09:21:38 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Byrne", "Nick", ""], ["Clough", "James R", ""], ["Valverde", "Isra", ""], ["Montana", "Giovanni", ""], ["King", "Andrew P", ""]]}, {"id": "2107.12692", "submitter": "Andr\\'es Eduardo G\\'omez Hernandez Mr", "authors": "Andr\\'es G\\'omez, Thomas Genevois, Jerome Lussereau and Christian\n  Laugier", "title": "Dynamic and Static Object Detection Considering Fusion Regions and\n  Point-wise Features", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection is a critical problem for the safe interaction between\nautonomous vehicles and road users. Deep-learning methodologies allowed the\ndevelopment of object detection approaches with better performance. However,\nthere is still the challenge to obtain more characteristics from the objects\ndetected in real-time. The main reason is that more information from the\nenvironment's objects can improve the autonomous vehicle capacity to face\ndifferent urban situations. This paper proposes a new approach to detect static\nand dynamic objects in front of an autonomous vehicle. Our approach can also\nget other characteristics from the objects detected, like their position,\nvelocity, and heading. We develop our proposal fusing results of the\nenvironment's interpretations achieved of YoloV3 and a Bayesian filter. To\ndemonstrate our proposal's performance, we asses it through a benchmark dataset\nand real-world data obtained from an autonomous platform. We compared the\nresults achieved with another approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 09:42:18 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["G\u00f3mez", "Andr\u00e9s", ""], ["Genevois", "Thomas", ""], ["Lussereau", "Jerome", ""], ["Laugier", "Christian", ""]]}, {"id": "2107.12706", "submitter": "Tanmoy Dam", "authors": "Tanmoy Dam, Sreenatha G. Anavatti, Hussein A. Abbass (Fellow,\n  IEEESchool of Engineering and Information Technology, University of New South\n  Wales Canberra, Australia)", "title": "Improving ClusterGAN Using Self-AugmentedInformation Maximization of\n  Disentangling LatentSpaces", "comments": "This paper is under review to IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Latent Space Clustering in Generative adversarial networks (ClusterGAN)\nmethod has been successful with high-dimensional data. However, the method\nassumes uniformlydistributed priors during the generation of modes, which isa\nrestrictive assumption in real-world data and cause loss ofdiversity in the\ngenerated modes. In this paper, we proposeself-augmentation information\nmaximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive\npriorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural\nnetworks: self-augmentation prior network,generator, discriminator and\nclustering inference autoencoder.The proposed method has been validated using\nseven bench-mark data sets and has shown improved performance overstate-of-the\nart methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on\nimbalanced dataset, wehave discussed two imbalanced conditions on MNIST\ndatasetswith one-class imbalance and three classes imbalanced cases.The results\nhighlight the advantages of SIMI-ClusterGAN.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:04:32 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Dam", "Tanmoy", "", "Fellow,\n  IEEESchool of Engineering and Information Technology, University of New South\n  Wales Canberra, Australia"], ["Anavatti", "Sreenatha G.", "", "Fellow,\n  IEEESchool of Engineering and Information Technology, University of New South\n  Wales Canberra, Australia"], ["Abbass", "Hussein A.", "", "Fellow,\n  IEEESchool of Engineering and Information Technology, University of New South\n  Wales Canberra, Australia"]]}, {"id": "2107.12707", "submitter": "Zhaoyu Su", "authors": "Zhaoyu Su, Pin Siang Tan, Yu-Hsing Wang", "title": "DV-Det: Efficient 3D Point Cloud Object Detection with Dynamic\n  Voxelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel two-stage framework for the efficient 3D\npoint cloud object detection. Instead of transforming point clouds into 2D bird\neye view projections, we parse the raw point cloud data directly in the 3D\nspace yet achieve impressive efficiency and accuracy. To achieve this goal, we\npropose dynamic voxelization, a method that voxellizes points at local scale\non-the-fly. By doing so, we preserve the point cloud geometry with 3D voxels,\nand therefore waive the dependence on expensive MLPs to learn from point\ncoordinates. On the other hand, we inherently still follow the same processing\npattern as point-wise methods (e.g., PointNet) and no longer suffer from the\nquantization issue like conventional convolutions. For further speed\noptimization, we propose the grid-based downsampling and voxelization method,\nand provide different CUDA implementations to accommodate to the discrepant\nrequirements during training and inference phases. We highlight our efficiency\non KITTI 3D object detection dataset with 75 FPS and on Waymo Open dataset with\n25 FPS inference speed with satisfactory accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:07:39 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Su", "Zhaoyu", ""], ["Tan", "Pin Siang", ""], ["Wang", "Yu-Hsing", ""]]}, {"id": "2107.12719", "submitter": "Alessio Xompero", "authors": "Alessio Xompero, Santiago Donaher, Vladimir Iashin, Francesca Palermo,\n  G\\\"okhan Solak, Claudio Coppola, Reina Ishikawa, Yuichi Nagao, Ryo Hachiuma,\n  Qi Liu, Fan Feng, Chuanlin Lan, Rosa H. M. Chan, Guilherme Christmann,\n  Jyun-Ting Song, Gonuguntla Neeharika, Chinnakotla Krishna Teja Reddy, Dinesh\n  Jain, Bakhtawar Ur Rehman, Andrea Cavallaro", "title": "Multi-modal estimation of the properties of containers and their\n  content: survey and evaluation", "comments": "13 pages, 9 tables, 5 figures, submitted to IEEE Transactions on\n  Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acoustic and visual sensing can support the contactless estimation of the\nweight of a container and the amount of its content when the container is\nmanipulated by a person. However, transparencies (both of the container and of\nthe content) and the variability of materials, shapes and sizes make this\nproblem challenging. In this paper, we present an open benchmarking framework\nand an in-depth comparative analysis of recent methods that estimate the\ncapacity of a container, as well as the type, mass, and amount of its content.\nThese methods use learned and handcrafted features, such as mel-frequency\ncepstrum coefficients, zero-crossing rate, spectrograms, with different types\nof classifiers to estimate the type and amount of the content with acoustic\ndata, and geometric approaches with visual data to determine the capacity of\nthe container. Results on a newly distributed dataset show that audio alone is\na strong modality and methods achieves a weighted average F1-score up to 81%\nand 97% for content type and level classification, respectively. Estimating the\ncontainer capacity with vision-only approaches and filling mass with\nmulti-modal, multi-stage algorithms reaches up to 65% weighted average capacity\nand mass scores.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:36:19 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xompero", "Alessio", ""], ["Donaher", "Santiago", ""], ["Iashin", "Vladimir", ""], ["Palermo", "Francesca", ""], ["Solak", "G\u00f6khan", ""], ["Coppola", "Claudio", ""], ["Ishikawa", "Reina", ""], ["Nagao", "Yuichi", ""], ["Hachiuma", "Ryo", ""], ["Liu", "Qi", ""], ["Feng", "Fan", ""], ["Lan", "Chuanlin", ""], ["Chan", "Rosa H. M.", ""], ["Christmann", "Guilherme", ""], ["Song", "Jyun-Ting", ""], ["Neeharika", "Gonuguntla", ""], ["Reddy", "Chinnakotla Krishna Teja", ""], ["Jain", "Dinesh", ""], ["Rehman", "Bakhtawar Ur", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2107.12734", "submitter": "Ralf Raumanns", "authors": "Ralf Raumanns, Gerard Schouten, Max Joosten, Josien P. W. Pluim and\n  Veronika Cheplygina", "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A\n  case study for skin lesion classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:23:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Raumanns", "Ralf", ""], ["Schouten", "Gerard", ""], ["Joosten", "Max", ""], ["Pluim", "Josien P. W.", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2107.12744", "submitter": "Sahar Darafsh", "authors": "Sahar Darafsh, Saeed Shiry Ghidary, Morteza Saheb Zamani", "title": "Real-Time Activity Recognition and Intention Recognition Using a\n  Vision-based Embedded System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in digital technologies, most fields of study include\nrecognition of human activity and intention recognition, which are important in\nsmart environments. In this research, we introduce a real-time activity\nrecognition to recognize people's intentions to pass or not pass a door. This\nsystem, if applied in elevators and automatic doors will save energy and\nincrease efficiency. For this study, data preparation is applied to combine the\nspatial and temporal features with the help of digital image processing\nprinciples. Nevertheless, unlike previous studies, only one AlexNet neural\nnetwork is used instead of two-stream convolutional neural networks. Our\nembedded system was implemented with an accuracy of 98.78% on our Intention\nRecognition dataset. We also examined our data representation approach on other\ndatasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of\n78.48%, 97.95%, and 100%, respectively. The image recognition and neural\nnetwork models were simulated and implemented using Xilinx simulators for\nZCU102 board. The operating frequency of this embedded system is 333 MHz, and\nit works in real-time with 120 frames per second (fps).\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:38:44 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Darafsh", "Sahar", ""], ["Ghidary", "Saeed Shiry", ""], ["Zamani", "Morteza Saheb", ""]]}, {"id": "2107.12746", "submitter": "Changan Wang", "authors": "Qingyu Song, Changan Wang, Zhengkai Jiang, Yabiao Wang, Ying Tai,\n  Chengjie Wang, Jilin Li, Feiyue Huang, Yang Wu", "title": "Rethinking Counting and Localization in Crowds:A Purely Point-Based\n  Framework", "comments": "To be appear in ICCV2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing individuals in crowds is more in accordance with the practical\ndemands of subsequent high-level crowd analysis tasks than simply counting.\nHowever, existing localization based methods relying on intermediate\nrepresentations (\\textit{i.e.}, density maps or pseudo boxes) serving as\nlearning targets are counter-intuitive and error-prone. In this paper, we\npropose a purely point-based framework for joint crowd counting and individual\nlocalization. For this framework, instead of merely reporting the absolute\ncounting error at image level, we propose a new metric, called density\nNormalized Average Precision (nAP), to provide more comprehensive and more\nprecise performance evaluation. Moreover, we design an intuitive solution under\nthis framework, which is called Point to Point Network (P2PNet). P2PNet\ndiscards superfluous steps and directly predicts a set of point proposals to\nrepresent heads in an image, being consistent with the human annotation\nresults. By thorough analysis, we reveal the key step towards implementing such\na novel idea is to assign optimal learning targets for these proposals.\nTherefore, we propose to conduct this crucial association in an one-to-one\nmatching manner using the Hungarian algorithm. The P2PNet not only\nsignificantly surpasses state-of-the-art methods on popular counting\nbenchmarks, but also achieves promising localization accuracy. The codes will\nbe available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:41:50 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Song", "Qingyu", ""], ["Wang", "Changan", ""], ["Jiang", "Zhengkai", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Wu", "Yang", ""]]}, {"id": "2107.12753", "submitter": "Xizhou Pan", "authors": "Xuan Xia, Xizhou Pan, Xing He, Jingfei Zhang, Ning Ding and Lin Ma", "title": "Discriminative-Generative Representation Learning for One-Class Anomaly\n  Detection", "comments": "e.g.:13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a kind of generative self-supervised learning methods, generative\nadversarial nets have been widely studied in the field of anomaly detection.\nHowever, the representation learning ability of the generator is limited since\nit pays too much attention to pixel-level details, and generator is difficult\nto learn abstract semantic representations from label prediction pretext tasks\nas effective as discriminator. In order to improve the representation learning\nability of generator, we propose a self-supervised learning framework combining\ngenerative methods and discriminative methods. The generator no longer learns\nrepresentation by reconstruction error, but the guidance of discriminator, and\ncould benefit from pretext tasks designed for discriminative methods. Our\ndiscriminative-generative representation learning method has performance close\nto discriminative methods and has a great advantage in speed. Our method used\nin one-class anomaly detection task significantly outperforms several\nstate-of-the-arts on multiple benchmark data sets, increases the performance of\nthe top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:46:15 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xia", "Xuan", ""], ["Pan", "Xizhou", ""], ["He", "Xing", ""], ["Zhang", "Jingfei", ""], ["Ding", "Ning", ""], ["Ma", "Lin", ""]]}, {"id": "2107.12762", "submitter": "Pan Xie", "authors": "Pan Xie, Zhi Cui, Yao Du, Mengyi Zhao, Jianwei Cui, Bin Wang, Xiaohui\n  Hu", "title": "Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign\n  Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continuous sign language recognition (cSLR) is a public significant task that\ntranscribes a sign language video into an ordered gloss sequence. It is\nimportant to capture the fine-grained gloss-level details, since there is no\nexplicit alignment between sign video frames and the corresponding glosses.\nAmong the past works, one promising way is to adopt a one-dimensional\nconvolutional network (1D-CNN) to temporally fuse the sequential frames.\nHowever, CNNs are agnostic to similarity or dissimilarity, and thus are unable\nto capture local consistent semantics within temporally neighboring frames. To\naddress the issue, we propose to adaptively fuse local features via temporal\nsimilarity for this task. Specifically, we devise a Multi-scale Local-Temporal\nSimilarity Fusion Network (mLTSF-Net) as follows: 1) In terms of a specific\nvideo frame, we firstly select its similar neighbours with multi-scale\nreceptive regions to accommodate different lengths of glosses. 2) To ensure\ntemporal consistency, we then use position-aware convolution to temporally\nconvolve each scale of selected frames. 3) To obtain a local-temporally\nenhanced frame-wise representation, we finally fuse the results of different\nscales using a content-dependent aggregator. We train our model in an\nend-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014\ndatasets (RWTH) demonstrate that our model achieves competitive performance\ncompared with several state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 12:06:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Xie", "Pan", ""], ["Cui", "Zhi", ""], ["Du", "Yao", ""], ["Zhao", "Mengyi", ""], ["Cui", "Jianwei", ""], ["Wang", "Bin", ""], ["Hu", "Xiaohui", ""]]}, {"id": "2107.12791", "submitter": "Mark Stamp", "authors": "Ruchira Gothankar, Fabio Di Troia, Mark Stamp", "title": "Clickbait Detection in YouTube Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  YouTube videos often include captivating descriptions and intriguing\nthumbnails designed to increase the number of views, and thereby increase the\nrevenue for the person who posted the video. This creates an incentive for\npeople to post clickbait videos, in which the content might deviate\nsignificantly from the title, description, or thumbnail. In effect, users are\ntricked into clicking on clickbait videos. In this research, we consider the\nchallenging problem of detecting clickbait YouTube videos. We experiment with\nmultiple state-of-the-art machine learning techniques using a variety of\ntextual features.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:39:32 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gothankar", "Ruchira", ""], ["Di Troia", "Fabio", ""], ["Stamp", "Mark", ""]]}, {"id": "2107.12800", "submitter": "Othmane Laousy", "authors": "Othmane Laousy, Guillaume Chassagnon, Edouard Oyallon, Nikos Paragios,\n  Marie-Pierre Revel, Maria Vakalopoulou", "title": "Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sarcopenia is a medical condition characterized by a reduction in muscle mass\nand function. A quantitative diagnosis technique consists of localizing the CT\nslice passing through the middle of the third lumbar area (L3) and segmenting\nmuscles at this level. In this paper, we propose a deep reinforcement learning\nmethod for accurate localization of the L3 CT slice. Our method trains a\nreinforcement learning agent by incentivizing it to discover the right\nposition. Specifically, a Deep Q-Network is trained to find the best policy to\nfollow for this problem. Visualizing the training process shows that the agent\nmimics the scrolling of an experienced radiologist. Extensive experiments\nagainst other state-of-the-art deep learning based methods for L3 localization\nprove the superiority of our technique which performs well even with limited\namount of data and annotations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 13:15:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Laousy", "Othmane", ""], ["Chassagnon", "Guillaume", ""], ["Oyallon", "Edouard", ""], ["Paragios", "Nikos", ""], ["Revel", "Marie-Pierre", ""], ["Vakalopoulou", "Maria", ""]]}, {"id": "2107.12815", "submitter": "Sreyas Mohan", "authors": "Sreyas Mohan, Joshua L. Vincent, Ramon Manzorro, Peter A. Crozier,\n  Eero P. Simoncelli, Carlos Fernandez-Granda", "title": "Adaptive Denoising via GainTuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) for image denoising are usually\ntrained on large datasets. These models achieve the current state of the art,\nbut they have difficulties generalizing when applied to data that deviate from\nthe training distribution. Recent work has shown that it is possible to train\ndenoisers on a single noisy image. These models adapt to the features of the\ntest image, but their performance is limited by the small amount of information\nused to train them. Here we propose \"GainTuning\", in which CNN models\npre-trained on large datasets are adaptively and selectively adjusted for\nindividual test images. To avoid overfitting, GainTuning optimizes a single\nmultiplicative scaling parameter (the \"Gain\") of each channel in the\nconvolutional layers of the CNN. We show that GainTuning improves\nstate-of-the-art CNNs on standard image-denoising benchmarks, boosting their\ndenoising performance on nearly every image in a held-out test set. These\nadaptive improvements are even more substantial for test images differing\nsystematically from the training data, either in noise level or image type. We\nillustrate the potential of adaptive denoising in a scientific application, in\nwhich a CNN is trained on synthetic data, and tested on real\ntransmission-electron-microscope images. In contrast to the existing\nmethodology, GainTuning is able to faithfully reconstruct the structure of\ncatalytic nanoparticles from these data at extremely low signal-to-noise\nratios.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 13:35:48 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Mohan", "Sreyas", ""], ["Vincent", "Joshua L.", ""], ["Manzorro", "Ramon", ""], ["Crozier", "Peter A.", ""], ["Simoncelli", "Eero P.", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "2107.12842", "submitter": "Riqiang Gao", "authors": "Riqiang Gao, Mirza S. Khan, Yucheng Tang, Kaiwen Xu, Steve Deppen,\n  Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Bennett A. Landman", "title": "Technical Report: Quality Assessment Tool for Machine Learning with\n  Clinical CT", "comments": "18 pages, 13 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image Quality Assessment (IQA) is important for scientific inquiry,\nespecially in medical imaging and machine learning. Potential data quality\nissues can be exacerbated when human-based workflows use limited views of the\ndata that may obscure digital artifacts. In practice, multiple factors such as\nnetwork issues, accelerated acquisitions, motion artifacts, and imaging\nprotocol design can impede the interpretation of image collections. The medical\nimage processing community has developed a wide variety of tools for the\ninspection and validation of imaging data. Yet, IQA of computed tomography (CT)\nremains an under-recognized challenge, and no user-friendly tool is commonly\navailable to address these potential issues. Here, we create and illustrate a\npipeline specifically designed to identify and resolve issues encountered with\nlarge-scale data mining of clinically acquired CT data. Using the widely\nstudied National Lung Screening Trial (NLST), we have identified approximately\n4% of image volumes with quality concerns out of 17,392 scans. To assess\nrobustness, we applied the proposed pipeline to our internal datasets where we\nfind our tool is generalizable to clinically acquired medical images. In\nconclusion, the tool has been useful and time-saving for research study of\nclinical data, and the code and tutorials are publicly available at\nhttps://github.com/MASILab/QA_tool.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:19:08 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gao", "Riqiang", ""], ["Khan", "Mirza S.", ""], ["Tang", "Yucheng", ""], ["Xu", "Kaiwen", ""], ["Deppen", "Steve", ""], ["Huo", "Yuankai", ""], ["Sandler", "Kim L.", ""], ["Massion", "Pierre P.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2107.12847", "submitter": "Srikrishna Karanam", "authors": "Runze Li and Srikrishna Karanam and Ren Li and Terrence Chen and Bir\n  Bhanu and Ziyan Wu", "title": "Learning Local Recurrent Models for Human Mesh Recovery", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating frame-level full human body meshes\ngiven a video of a person with natural motion dynamics. While much progress in\nthis field has been in single image-based mesh estimation, there has been a\nrecent uptick in efforts to infer mesh dynamics from video given its role in\nalleviating issues such as depth ambiguity and occlusions. However, a key\nlimitation of existing work is the assumption that all the observed motion\ndynamics can be modeled using one dynamical/recurrent model. While this may\nwork well in cases with relatively simplistic dynamics, inference with\nin-the-wild videos presents many challenges. In particular, it is typically the\ncase that different body parts of a person undergo different dynamics in the\nvideo, e.g., legs may move in a way that may be dynamically different from\nhands (e.g., a person dancing). To address these issues, we present a new\nmethod for video mesh recovery that divides the human mesh into several local\nparts following the standard skeletal model. We then model the dynamics of each\nlocal part with separate recurrent models, with each model conditioned\nappropriately based on the known kinematic structure of the human body. This\nresults in a structure-informed local recurrent learning architecture that can\nbe trained in an end-to-end fashion with available annotations. We conduct a\nvariety of experiments on standard video mesh recovery benchmark datasets such\nas Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design\nof modeling local dynamics as well as establishing state-of-the-art results\nbased on standard evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:30:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Runze", ""], ["Karanam", "Srikrishna", ""], ["Li", "Ren", ""], ["Chen", "Terrence", ""], ["Bhanu", "Bir", ""], ["Wu", "Ziyan", ""]]}, {"id": "2107.12852", "submitter": "Jie Li", "authors": "Jie Li, Sheng Zhang, Kai Han, Xia Yuan, Chunxia Zhao, Yu Liu", "title": "Real-time Keypoints Detection for Autonomous Recovery of the Unmanned\n  Ground Vehicle", "comments": "IET Image Processing, code: https://github.com/waterljwant/UGV-KPNet", "journal-ref": null, "doi": "10.1049/iet-ipr.2020.0864", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of a small unmanned ground vehicle (UGV) and a large unmanned\ncarrier vehicle allows more flexibility in real applications such as rescue in\ndangerous scenarios. The autonomous recovery system, which is used to guide the\nsmall UGV back to the carrier vehicle, is an essential component to achieve a\nseamless combination of the two vehicles. This paper proposes a novel\nautonomous recovery framework with a low-cost monocular vision system to\nprovide accurate positioning and attitude estimation of the UGV during\nnavigation. First, we introduce a light-weight convolutional neural network\ncalled UGV-KPNet to detect the keypoints of the small UGV from the images\ncaptured by a monocular camera. UGV-KPNet is computationally efficient with a\nsmall number of parameters and provides pixel-level accurate keypoints\ndetection results in real-time. Then, six degrees of freedom pose is estimated\nusing the detected keypoints to obtain positioning and attitude information of\nthe UGV. Besides, we are the first to create a large-scale real-world keypoints\ndataset of the UGV. The experimental results demonstrate that the proposed\nsystem achieves state-of-the-art performance in terms of both accuracy and\nspeed on UGV keypoint detection, and can further boost the 6-DoF pose\nestimation for the UGV.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:36:59 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Jie", ""], ["Zhang", "Sheng", ""], ["Han", "Kai", ""], ["Yuan", "Xia", ""], ["Zhao", "Chunxia", ""], ["Liu", "Yu", ""]]}, {"id": "2107.12858", "submitter": "Zhikang Zou", "authors": "Zhikang Zou, Xiaoye Qu, Pan Zhou, Shuangjie Xu, Xiaoqing Ye, Wenhao\n  Wu, Jin Ye", "title": "Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring\n  Network", "comments": "Accepted by ACMMM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent deep networks have convincingly demonstrated high capability in crowd\ncounting, which is a critical task attracting widespread attention due to its\nvarious industrial applications. Despite such progress, trained data-dependent\nmodels usually can not generalize well to unseen scenarios because of the\ninherent domain shift. To facilitate this issue, this paper proposes a novel\nadversarial scoring network (ASNet) to gradually bridge the gap across domains\nfrom coarse to fine granularity. In specific, at the coarse-grained stage, we\ndesign a dual-discriminator strategy to adapt source domain to be close to the\ntargets from the perspectives of both global and local feature space via\nadversarial learning. The distributions between two domains can thus be aligned\nroughly. At the fine-grained stage, we explore the transferability of source\ncharacteristics by scoring how similar the source samples are to target ones\nfrom multiple levels based on generative probability derived from coarse stage.\nGuided by these hierarchical scores, the transferable source features are\nproperly selected to enhance the knowledge transfer during the adaptation\nprocess. With the coarse-to-fine design, the generalization bottleneck induced\nfrom the domain discrepancy can be effectively alleviated. Three sets of\nmigration experiments show that the proposed methods achieve state-of-the-art\ncounting performance compared with major unsupervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:47:24 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zou", "Zhikang", ""], ["Qu", "Xiaoye", ""], ["Zhou", "Pan", ""], ["Xu", "Shuangjie", ""], ["Ye", "Xiaoqing", ""], ["Wu", "Wenhao", ""], ["Ye", "Jin", ""]]}, {"id": "2107.12859", "submitter": "Abhinav Narayan Harish", "authors": "Abhinav Narayan Harish, Rajendra Nagar and Shanmuganathan Raman", "title": "RGL-NET: A Recurrent Graph Learning framework for Progressive Part\n  Assembly", "comments": "Accepted to Winter Conference of Computer Vision (WACV 2022)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:47:43 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Harish", "Abhinav Narayan", ""], ["Nagar", "Rajendra", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2107.12889", "submitter": "Banafshe Felfeliyan", "authors": "Banafshe Felfeliyan, Abhilash Hareendranathan, Gregor Kuntze, Jacob L.\n  Jaremko, Janet L. Ronsky", "title": "Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance\n  segmentation platform (Data from the Osteoarthritis Initiative)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective assessment of Magnetic Resonance Imaging (MRI) scans of\nosteoarthritis (OA) can address the limitation of the current OA assessment.\nSegmentation of bone, cartilage, and joint fluid is necessary for the OA\nobjective assessment. Most of the proposed segmentation methods are not\nperforming instance segmentation and suffer from class imbalance problems. This\nstudy deployed Mask R-CNN instance segmentation and improved it (improved-Mask\nR-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for\nOA-associated tissues. Training and validation of the method were performed\nusing 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI\nscans of patients with symptomatic hip OA. Three modifications to Mask R-CNN\nyielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder\nlayer to the mask-header, and connecting them by a skip connection. The results\nwere assessed using Hausdorff distance, dice score, and coefficients of\nvariation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation\ncompared to Mask RCNN as indicated with the increase in dice score from 95% to\n98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and\n81% to 82% for tibial cartilage. For the effusion detection, dice improved with\niMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection\nbetween Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2\nand Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between\ntwo readers (0.21), indicating a high agreement between the human readers and\nboth Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and\nsimultaneously extract different scale articular tissues involved in OA,\nforming the foundation for automated assessment of OA. The iMaskRCNN results\nshow that the modification improved the network performance around the edges.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:41:31 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Felfeliyan", "Banafshe", ""], ["Hareendranathan", "Abhilash", ""], ["Kuntze", "Gregor", ""], ["Jaremko", "Jacob L.", ""], ["Ronsky", "Janet L.", ""]]}, {"id": "2107.12898", "submitter": "Yuda Song", "authors": "Yuda Song, Hui Qian, Xin Du", "title": "StarEnhancer: Learning Real-Time and Style-Aware Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image enhancement is a subjective process whose targets vary with user\npreferences. In this paper, we propose a deep learning-based image enhancement\nmethod covering multiple tonal styles using only a single model dubbed\nStarEnhancer. It can transform an image from one tonal style to another, even\nif that style is unseen. With a simple one-time setting, users can customize\nthe model to make the enhanced images more in line with their aesthetics. To\nmake the method more practical, we propose a well-designed enhancer that can\nprocess a 4K-resolution image over 200 FPS but surpasses the contemporaneous\nsingle style image enhancement methods in terms of PSNR, SSIM, and LPIPS.\nFinally, our proposed enhancement method has good interactability, which allows\nthe user to fine-tune the enhanced image using intuitive options.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:57:33 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 08:23:52 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Song", "Yuda", ""], ["Qian", "Hui", ""], ["Du", "Xin", ""]]}, {"id": "2107.12921", "submitter": "Menghan Hu", "authors": "Hang Liu, Menghan Hu, Yuzhen Chen, Qingli Li, Guangtao Zhai, Simon X.\n  Yang, Xiao-Ping Zhang, Xiaokang Yang", "title": "Angel's Girl for Blind Painters: an Efficient Painting Navigation System\n  Validated by Multimodal Evaluation Approach", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For people who ardently love painting but unfortunately have visual\nimpairments, holding a paintbrush to create a work is a very difficult task.\nPeople in this special group are eager to pick up the paintbrush, like Leonardo\nda Vinci, to create and make full use of their own talents. Therefore, to\nmaximally bridge this gap, we propose a painting navigation system to assist\nblind people in painting and artistic creation. The proposed system is composed\nof cognitive system and guidance system. The system adopts drawing board\npositioning based on QR code, brush navigation based on target detection and\nbush real-time positioning. Meanwhile, this paper uses human-computer\ninteraction on the basis of voice and a simple but efficient position\ninformation coding rule. In addition, we design a criterion to efficiently\njudge whether the brush reaches the target or not. According to the\nexperimental results, the thermal curves extracted from the faces of testers\nshow that it is relatively well accepted by blindfolded and even blind testers.\nWith the prompt frequency of 1s, the painting navigation system performs best\nwith the completion degree of 89% with SD of 8.37% and overflow degree of 347%\nwith SD of 162.14%. Meanwhile, the excellent and good types of brush tip\ntrajectory account for 74%, and the relative movement distance is 4.21 with SD\nof 2.51. This work demonstrates that it is practicable for the blind people to\nfeel the world through the brush in their hands. In the future, we plan to\ndeploy Angle's Eyes on the phone to make it more portable. The demo video of\nthe proposed painting navigation system is available at:\nhttps://doi.org/10.6084/m9.figshare.9760004.v1.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:23:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Liu", "Hang", ""], ["Hu", "Menghan", ""], ["Chen", "Yuzhen", ""], ["Li", "Qingli", ""], ["Zhai", "Guangtao", ""], ["Yang", "Simon X.", ""], ["Zhang", "Xiao-Ping", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2107.12932", "submitter": "Nachiket Deo", "authors": "Akshay Rangesh, Nachiket Deo, Ross Greer, Pujitha Gunaratne, Mohan M.\n  Trivedi", "title": "Predicting Take-over Time for Autonomous Driving with Real-World Data:\n  Robust Data Augmentation, Models, and Evaluation", "comments": "Journal version of arXiv:2104.11489", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding occupant-vehicle interactions by modeling control transitions\nis important to ensure safe approaches to passenger vehicle automation. Models\nwhich contain contextual, semantically meaningful representations of driver\nstates can be used to determine the appropriate timing and conditions for\ntransfer of control between driver and vehicle. However, such models rely on\nreal-world control take-over data from drivers engaged in distracting\nactivities, which is costly to collect. Here, we introduce a scheme for data\naugmentation for such a dataset. Using the augmented dataset, we develop and\ntrain take-over time (TOT) models that operate sequentially on mid and\nhigh-level features produced by computer vision algorithms operating on\ndifferent driver-facing camera views, showing models trained on the augmented\ndataset to outperform the initial dataset. The demonstrated model features\nencode different aspects of the driver state, pertaining to the face, hands,\nfoot and upper body of the driver. We perform ablative experiments on feature\ncombinations as well as model architectures, showing that a TOT model supported\nby augmented data can be used to produce continuous estimates of take-over\ntimes without delay, suitable for complex real-world scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:39:50 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Rangesh", "Akshay", ""], ["Deo", "Nachiket", ""], ["Greer", "Ross", ""], ["Gunaratne", "Pujitha", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "2107.12960", "submitter": "Zixin Zhu", "authors": "Zixin Zhu (Xi'an jiaotong University), Wei Tang (University of\n  Illinois at Chicago), Le Wang (Xi'an Jiaotong University), Nanning Zheng\n  (Xi'an Jiaotong University), Gang Hua (Wormpex AI Research)", "title": "Enriching Local and Global Contexts for Temporal Action Localization", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively tackling the problem of temporal action localization (TAL)\nnecessitates a visual representation that jointly pursues two confounding\ngoals, i.e., fine-grained discrimination for temporal localization and\nsufficient visual invariance for action classification. We address this\nchallenge by enriching both the local and global contexts in the popular\ntwo-stage temporal localization framework, where action proposals are first\ngenerated followed by action classification and temporal boundary regression.\nOur proposed model, dubbed ContextLoc, can be divided into three sub-networks:\nL-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained\nmodeling of snippet-level features, which is formulated as a\nquery-and-retrieval process. G-Net enriches the global context via higher-level\nmodeling of the video-level representation. In addition, we introduce a novel\ncontext adaptation module to adapt the global context to different proposals.\nP-Net further models the context-aware inter-proposal relations. We explore two\nexisting models to be the P-Net in our experiments. The efficacy of our\nproposed method is validated by experimental results on the THUMOS14 (54.3\\% at\nIoU@0.5) and ActivityNet v1.3 (51.24\\% at IoU@0.5) datasets, which outperforms\nrecent states of the art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:25:40 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhu", "Zixin", "", "Xi'an jiaotong University"], ["Tang", "Wei", "", "University of\n  Illinois at Chicago"], ["Wang", "Le", "", "Xi'an Jiaotong University"], ["Zheng", "Nanning", "", "Xi'an Jiaotong University"], ["Hua", "Gang", "", "Wormpex AI Research"]]}, {"id": "2107.12964", "submitter": "Alice Baird", "authors": "Alice Baird, Lukas Stappen, Lukas Christ, Lea Schumann, Eva-Maria\n  Me{\\ss}ner, Bj\\\"orn W. Schuller", "title": "A Physiologically-Adapted Gold Standard for Arousal during Stress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion is an inherently subjective psychophysiological human-state and to\nproduce an agreed-upon representation (gold standard) for continuous emotion\nrequires a time-consuming and costly training procedure of multiple human\nannotators. There is strong evidence in the literature that physiological\nsignals are sufficient objective markers for states of emotion, particularly\narousal. In this contribution, we utilise a dataset which includes continuous\nemotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal\nActivity (EDA), and Respiration-rate - captured during a stress inducing\nscenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,\nRecurrent Neural Network to explore the benefit of fusing these physiological\nsignals with arousal as the target, learning from various audio, video, and\ntextual based features. We utilise the state-of-the-art MuSe-Toolbox to\nconsider both annotation delay and inter-rater agreement weighting when fusing\nthe target signals. An improvement in Concordance Correlation Coefficient (CCC)\nis seen across features sets when fusing EDA with arousal, compared to the\narousal only gold standard results. Additionally, BERT-based textual features'\nresults improved for arousal plus all physiological signals, obtaining up to\n.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also\nimproves overall CCC with audio plus video features obtaining up to .6157 CCC\nto recognize arousal plus EDA and BPM.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:28:26 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 13:08:50 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Baird", "Alice", ""], ["Stappen", "Lukas", ""], ["Christ", "Lukas", ""], ["Schumann", "Lea", ""], ["Me\u00dfner", "Eva-Maria", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2107.12978", "submitter": "Brennan Nichyporuk", "authors": "Brennan Nichyporuk, Justin Szeto, Douglas L. Arnold, Tal Arbel", "title": "Optimizing Operating Points for High Performance Lesion Detection and\n  Segmentation Using Lesion Size Reweighting", "comments": "Accepted at MIDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:43:49 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Nichyporuk", "Brennan", ""], ["Szeto", "Justin", ""], ["Arnold", "Douglas L.", ""], ["Arbel", "Tal", ""]]}, {"id": "2107.13029", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda, Laura Sevilla-Lara, Kiyoon Kim, Frank Keller, and\n  Marcus Rohrbach", "title": "A New Split for Evaluating True Zero-Shot Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets (e.g.\nUCF101) and randomly split the classes into seen and unseen. However, most\nrecent work builds on representations pre-trained on the Kinetics dataset,\nwhere classes largely overlap with classes in the zero-shot evaluation\ndatasets. As a result, classes which are supposed to be unseen, are present\nduring supervised pre-training, invalidating the condition of the zero-shot\nsetting. A similar concern was previously noted several years ago for image\nbased zero-shot recognition, but has not been considered by the zero-shot\naction recognition community. In this paper, we propose a new split for true\nzero-shot action recognition with no overlap between unseen test classes and\ntraining or pre-training classes. We benchmark several recent approaches on the\nproposed True Zero-Shot (TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis we find that our\nTruZe splits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower, up\nto 9.4% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 14.1%. We publish our splits and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 18:22:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Sevilla-Lara", "Laura", ""], ["Kim", "Kiyoon", ""], ["Keller", "Frank", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "2107.13046", "submitter": "Fadi Boutros", "authors": "Fadi Boutros, Naser Damer, Meiling Fang, Florian Kirchbuchner and\n  Arjan Kuijper", "title": "MixFaceNets: Extremely Efficient Face Recognition Networks", "comments": "Accepted at International Join Conference on Biometrics (IJCB 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a set of extremely efficient and high throughput\nmodels for accurate face verification, MixFaceNets which are inspired by Mixed\nDepthwise Convolutional Kernels. Extensive experiment evaluations on Label Face\nin the Wild (LFW), Age-DB, MegaFace, and IARPA Janus Benchmarks IJB-B and IJB-C\ndatasets have shown the effectiveness of our MixFaceNets for applications\nrequiring extremely low computational complexity. Under the same level of\ncomputation complexity (< 500M FLOPs), our MixFaceNets outperform\nMobileFaceNets on all the evaluated datasets, achieving 99.60% accuracy on LFW,\n97.05% accuracy on AgeDB-30, 93.60 TAR (at FAR1e-6) on MegaFace, 90.94 TAR (at\nFAR1e-4) on IJB-B and 93.08 TAR (at FAR1e-4) on IJB-C. With computational\ncomplexity between 500M and 1G FLOPs, our MixFaceNets achieved results\ncomparable to the top-ranked models, while using significantly fewer FLOPs and\nless computation overhead, which proves the practical value of our proposed\nMixFaceNets. All training codes, pre-trained models, and training logs have\nbeen made available https://github.com/fdbtrs/mixfacenets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:10:27 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Boutros", "Fadi", ""], ["Damer", "Naser", ""], ["Fang", "Meiling", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2107.13048", "submitter": "Richard Chen J", "authors": "Richard J. Chen, Ming Y. Lu, Muhammad Shaban, Chengkuan Chen, Tiffany\n  Y. Chen, Drew F. K. Williamson, Faisal Mahmood", "title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival\n  Prediction using Patch-based Graph Convolutional Networks", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer prognostication is a challenging task in computational pathology that\nrequires context-aware representations of histology features to adequately\ninfer patient survival. Despite the advancements made in weakly-supervised deep\nlearning, many approaches are not context-aware and are unable to model\nimportant morphological feature interactions between cell identities and tissue\ntypes that are prognostic for patient survival. In this work, we present\nPatch-GCN, a context-aware, spatially-resolved patch-based graph convolutional\nnetwork that hierarchically aggregates instance-level histology features to\nmodel local- and global-level topological structures in the tumor\nmicroenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five\ndifferent cancer types from the Cancer Genome Atlas (TCGA), and demonstrate\nthat Patch-GCN outperforms all prior weakly-supervised approaches by\n3.58-9.46%. Our code and corresponding models are publicly available at\nhttps://github.com/mahmoodlab/Patch-GCN.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:17:37 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chen", "Richard J.", ""], ["Lu", "Ming Y.", ""], ["Shaban", "Muhammad", ""], ["Chen", "Chengkuan", ""], ["Chen", "Tiffany Y.", ""], ["Williamson", "Drew F. K.", ""], ["Mahmood", "Faisal", ""]]}, {"id": "2107.13054", "submitter": "Cameron R. Wolfe", "authors": "Cameron R. Wolfe and Keld T. Lundgaard", "title": "Exceeding the Limits of Visual-Linguistic Multi-Task Learning", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By leveraging large amounts of product data collected across hundreds of live\ne-commerce websites, we construct 1000 unique classification tasks that share\nsimilarly-structured input data, comprised of both text and images. These\nclassification tasks focus on learning the product hierarchy of different\ne-commerce websites, causing many of them to be correlated. Adopting a\nmulti-modal transformer model, we solve these tasks in unison using multi-task\nlearning (MTL). Extensive experiments are presented over an initial 100-task\ndataset to reveal best practices for \"large-scale MTL\" (i.e., MTL with more\nthan 100 tasks). From these experiments, a final, unified methodology is\nderived, which is composed of both best practices and new proposals such as\nDyPa, a simple heuristic for automatically allocating task-specific parameters\nto tasks that could benefit from extra capacity. Using our large-scale MTL\nmethodology, we successfully train a single model across all 1000 tasks in our\ndataset while using minimal task specific parameters, thereby showing that it\nis possible to extend several orders of magnitude beyond current efforts in\nMTL.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:42:14 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wolfe", "Cameron R.", ""], ["Lundgaard", "Keld T.", ""]]}, {"id": "2107.13083", "submitter": "Ying Jin", "authors": "Ying Jin, Yinpeng Chen, Lijuan Wang, Jianfeng Wang, Pei Yu, Zicheng\n  Liu, Jenq-Neng Hwang", "title": "Is Object Detection Necessary for Human-Object Interaction Recognition?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits human-object interaction (HOI) recognition at image level\nwithout using supervisions of object location and human pose. We name it\ndetection-free HOI recognition, in contrast to the existing\ndetection-supervised approaches which rely on object and keypoint detections to\nachieve state of the art. With our method, not only the detection supervision\nis evitable, but superior performance can be achieved by properly using\nimage-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign\n(LSE-Sign) loss function. Specifically, using text embeddings of class labels\nto initialize the linear classifier is essential for leveraging the CLIP\npre-trained image encoder. In addition, LSE-Sign loss facilitates learning from\nmultiple labels on an imbalanced dataset by normalizing gradients over all\nclasses in a softmax format. Surprisingly, our detection-free solution achieves\n60.5 mAP on the HICO dataset, outperforming the detection-supervised state of\nthe art by 13.4 mAP\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 21:15:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Jin", "Ying", ""], ["Chen", "Yinpeng", ""], ["Wang", "Lijuan", ""], ["Wang", "Jianfeng", ""], ["Yu", "Pei", ""], ["Liu", "Zicheng", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "2107.13087", "submitter": "Yanchao Yang", "authors": "Yanchao Yang, Yuefan Shen, Youyi Zheng, C. Karen Liu and Leonidas\n  Guibas", "title": "DCL: Differential Contrastive Learning for Geometry-Aware Depth\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for realistic depth synthesis that learns diverse\nvariations from the real depth scans and ensures geometric consistency for\neffective synthetic-to-real transfer. Unlike general image synthesis pipelines,\nwhere geometries are mostly ignored, we treat geometries carried by the depth\nbased on their own existence. We propose differential contrastive learning that\nexplicitly enforces the underlying geometric properties to be invariant\nregarding the real variations been learned. The resulting depth synthesis\nmethod is task-agnostic and can be used for training any task-specific networks\nwith synthetic labels. We demonstrate the effectiveness of the proposed method\nby extensive evaluations on downstream real-world geometric reasoning tasks. We\nshow our method achieves better synthetic-to-real transfer performance than the\nother state-of-the-art. When fine-tuned on a small number of real-world\nannotations, our method can even surpass the fully supervised baselines.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 21:25:07 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Yang", "Yanchao", ""], ["Shen", "Yuefan", ""], ["Zheng", "Youyi", ""], ["Liu", "C. Karen", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2107.13093", "submitter": "Reece Walsh", "authors": "Reece Walsh, Mohamed H. Abdelpakey, Mohamed S. Shehata, Mostafa\n  M.Mohamed", "title": "Automated Human Cell Classification in Sparse Datasets using Few-Shot\n  Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 22:26:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Walsh", "Reece", ""], ["Abdelpakey", "Mohamed H.", ""], ["Shehata", "Mohamed S.", ""], ["Mohamed", "Mostafa M.", ""]]}, {"id": "2107.13098", "submitter": "Daniel D'souza", "authors": "Daniel D'souza, Zach Nussbaum, Chirag Agarwal, Sara Hooker", "title": "A Tale Of Two Long Tails", "comments": "Preliminary results accepted to Workshop on Uncertainty and\n  Robustness in Deep Learning (UDL), ICML, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning models are increasingly employed to assist human\ndecision-makers, it becomes critical to communicate the uncertainty associated\nwith these model predictions. However, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking approaches - where the model\nassigns low probabilities or scores to uncertain examples. While this captures\nwhat examples are challenging for the model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek to identify examples the model\nis uncertain about and characterize the source of said uncertainty. We explore\nthe benefits of designing a targeted intervention - targeted data augmentation\nof the examples where the model is uncertain over the course of training. We\ninvestigate whether the rate of learning in the presence of additional\ninformation differs between atypical and noisy examples? Our results show that\nthis is indeed the case, suggesting that well-designed interventions over the\ncourse of training can be an effective way to characterize and distinguish\nbetween different sources of uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 22:49:59 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["D'souza", "Daniel", ""], ["Nussbaum", "Zach", ""], ["Agarwal", "Chirag", ""], ["Hooker", "Sara", ""]]}, {"id": "2107.13108", "submitter": "Nan Xue", "authors": "Bin Tan and Nan Xue and Song Bai and Tianfu Wu and Gui-Song Xia", "title": "PlaneTR: Structure-Guided Transformers for 3D Plane Recovery", "comments": "ICCV 2021; Code: https://git.io/PlaneTR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a neural network built upon Transformers, namely PlaneTR,\nto simultaneously detect and reconstruct planes from a single image. Different\nfrom previous methods, PlaneTR jointly leverages the context information and\nthe geometric structures in a sequence-to-sequence way to holistically detect\nplane instances in one forward pass. Specifically, we represent the geometric\nstructures as line segments and conduct the network with three main components:\n(i) context and line segments encoders, (ii) a structure-guided plane decoder,\n(iii) a pixel-wise plane embedding decoder. Given an image and its detected\nline segments, PlaneTR generates the context and line segment sequences via two\nspecially designed encoders and then feeds them into a Transformers-based\ndecoder to directly predict a sequence of plane instances by simultaneously\nconsidering the context and global structure cues. Finally, the pixel-wise\nembeddings are computed to assign each pixel to one predicted plane instance\nwhich is nearest to it in embedding space. Comprehensive experiments\ndemonstrate that PlaneTR achieves a state-of-the-art performance on the ScanNet\nand NYUv2 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 23:55:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Tan", "Bin", ""], ["Xue", "Nan", ""], ["Bai", "Song", ""], ["Wu", "Tianfu", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2107.13111", "submitter": "Ahmed Elhagry", "authors": "Ahmed Elhagry, Karima Kadaoui", "title": "Experimenting with Self-Supervision using Rotation Prediction for Image\n  Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning is a task in the field of Artificial Intelligence that\nmerges between computer vision and natural language processing. It is\nresponsible for generating legends that describe images, and has various\napplications like descriptions used by assistive technology or indexing images\n(for search engines for instance). This makes it a crucial topic in AI that is\nundergoing a lot of research. This task however, like many others, is trained\non large images labeled via human annotation, which can be very cumbersome: it\nneeds manual effort, both financial and temporal costs, it is error-prone and\npotentially difficult to execute in some cases (e.g. medical images). To\nmitigate the need for labels, we attempt to use self-supervised learning, a\ntype of learning where models use the data contained within the images\nthemselves as labels. It is challenging to accomplish though, since the task is\ntwo-fold: the images and captions come from two different modalities and\nusually handled by different types of networks. It is thus not obvious what a\ncompletely self-supervised solution would look like. How it would achieve\ncaptioning in a comparable way to how self-supervision is applied today on\nimage recognition tasks is still an ongoing research topic. In this project, we\nare using an encoder-decoder architecture where the encoder is a convolutional\nneural network (CNN) trained on OpenImages dataset and learns image features in\na self-supervised fashion using the rotation pretext task. The decoder is a\nLong Short-Term Memory (LSTM), and it is trained, along within the image\ncaptioning model, on MS COCO dataset and is responsible of generating captions.\nOur GitHub repository can be found:\nhttps://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 00:46:27 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Elhagry", "Ahmed", ""], ["Kadaoui", "Karima", ""]]}, {"id": "2107.13114", "submitter": "Ahmed Elhagry", "authors": "Ahmed Elhagry, Karima Kadaoui", "title": "A Thorough Review on Recent Deep Learning Methodologies for Image\n  Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Captioning is a task that combines computer vision and natural language\nprocessing, where it aims to generate descriptive legends for images. It is a\ntwo-fold process relying on accurate image understanding and correct language\nunderstanding both syntactically and semantically. It is becoming increasingly\ndifficult to keep up with the latest research and findings in the field of\nimage captioning due to the growing amount of knowledge available on the topic.\nThere is not, however, enough coverage of those findings in the available\nreview papers. We perform in this paper a run-through of the current\ntechniques, datasets, benchmarks and evaluation metrics used in image\ncaptioning. The current research on the field is mostly focused on deep\nlearning-based methods, where attention mechanisms along with deep\nreinforcement and adversarial learning appear to be in the forefront of this\nresearch topic. In this paper, we review recent methodologies such as UpDown,\nOSCAR, VIVO, Meta Learning and a model that uses conditional generative\nadversarial nets. Although the GAN-based model achieves the highest score,\nUpDown represents an important basis for image captioning and OSCAR and VIVO\nare more useful as they use novel object captioning. This review paper serves\nas a roadmap for researchers to keep up to date with the latest contributions\nmade in the field of image caption generation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 00:54:59 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Elhagry", "Ahmed", ""], ["Kadaoui", "Karima", ""]]}, {"id": "2107.13117", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi", "title": "Image color correction, enhancement, and editing", "comments": "PhD dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This thesis presents methods and approaches to image color correction, color\nenhancement, and color editing. To begin, we study the color correction problem\nfrom the standpoint of the camera's image signal processor (ISP). A camera's\nISP is hardware that applies a series of in-camera image processing and color\nmanipulation steps, many of which are nonlinear in nature, to render the\ninitial sensor image to its final photo-finished representation saved in the\n8-bit standard RGB (sRGB) color space. As white balance (WB) is one of the\nmajor procedures applied by the ISP for color correction, this thesis presents\ntwo different methods for ISP white balancing. Afterward, we discuss another\nscenario of correcting and editing image colors, where we present a set of\nmethods to correct and edit WB settings for images that have been improperly\nwhite-balanced by the ISP. Then, we explore another factor that has a\nsignificant impact on the quality of camera-rendered colors, in which we\noutline two different methods to correct exposure errors in camera-rendered\nimages. Lastly, we discuss post-capture auto color editing and manipulation. In\nparticular, we propose auto image recoloring methods to generate different\nrealistic versions of the same camera-rendered image with new colors. Through\nextensive evaluations, we demonstrate that our methods provide superior\nsolutions compared to existing alternatives targeting color correction, color\nenhancement, and color editing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 01:14:12 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Afifi", "Mahmoud", ""]]}, {"id": "2107.13118", "submitter": "Qiaoyong Zhong", "authors": "Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, Hong\n  Zhou", "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly\n  Detection", "comments": "accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction-based methods play an important role in unsupervised anomaly\ndetection in images. Ideally, we expect a perfect reconstruction for normal\nsamples and poor reconstruction for abnormal samples. Since the\ngeneralizability of deep neural networks is difficult to control, existing\nmodels such as autoencoder do not work well. In this work, we interpret the\nreconstruction of an image as a divide-and-assemble procedure. Surprisingly, by\nvarying the granularity of division on feature maps, we are able to modulate\nthe reconstruction capability of the model for both normal and abnormal\nsamples. That is, finer granularity leads to better reconstruction, while\ncoarser granularity leads to poorer reconstruction. With proper granularity,\nthe gap between the reconstruction error of normal and abnormal samples can be\nmaximized. The divide-and-assemble framework is implemented by embedding a\nnovel multi-scale block-wise memory module into an autoencoder network.\nBesides, we introduce adversarial learning and explore the semantic latent\nrepresentation of the discriminator, which improves the detection of subtle\nanomaly. We achieve state-of-the-art performance on the challenging MVTec AD\ndataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms\nof the AUROC score.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 01:14:32 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hou", "Jinlei", ""], ["Zhang", "Yingying", ""], ["Zhong", "Qiaoyong", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""], ["Zhou", "Hong", ""]]}, {"id": "2107.13122", "submitter": "Zhigao Fang", "authors": "Zhigao Fang and Jiaqi Zhang and Lu Yu and Yin Zhao", "title": "Subjective evaluation of traditional and learning-based image coding\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a subjective experiment to compare the performance of traditional\nimage coding methods and learning-based image coding methods. HEVC and VVC, the\nstate-of-the-art traditional coding methods, are used as the representative\ntraditional methods. The learning-based methods used contain not only CNN-based\nmethods, but also a GAN-based method, all of which are advanced or typical.\nSingle Stimuli (SS), which is also called Absolute Category Rating (ACR), is\nadopted as the methodology of the experiment to obtain perceptual quality of\nimages. Additionally, we utilize some typical and frequently used objective\nquality metrics to evaluate the coding methods in the experiment as comparison.\nThe experiment shows that CNN-based and GAN-based methods can perform better\nthan traditional methods in low bit-rates. In high bit-rates, however, it is\nhard to verify whether CNN-based methods are superior to traditional methods.\nBecause the GAN method does not provide models with high target bit-rates, we\ncannot exactly tell the performance of the GAN method in high bit-rates.\nFurthermore, some popular objective quality metrics have not shown the ability\nwell to measure quality of images generated by learning-based coding methods,\nespecially the GAN-based one.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 01:37:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Fang", "Zhigao", ""], ["Zhang", "Jiaqi", ""], ["Yu", "Lu", ""], ["Zhao", "Yin", ""]]}, {"id": "2107.13136", "submitter": "Ruihan Yang", "authors": "Ruihan Yang, Yibo Yang, Joseph Marino, Stephan Mandt", "title": "Insights from Generative Modeling for Neural Video Compression", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. arXiv admin note: text overlap with arXiv:2010.10258", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent machine learning research has revealed connections between deep\ngenerative models such as VAEs and rate-distortion losses used in learned\ncompression, most of this work has focused on images. In a similar spirit, we\nview recently proposed neural video coding algorithms through the lens of deep\nautoregressive and latent variable modeling. We present recent neural video\ncodecs as instances of a generalized stochastic temporal autoregressive\ntransform, and propose new avenues for further improvements inspired by\nnormalizing flows and structured priors. We propose several architectures that\nyield state-of-the-art video compression performance on full-resolution video\nand discuss their tradeoffs and ablations. In particular, we propose (i)\nimproved temporal autoregressive transforms, (ii) improved entropy models with\nstructured and temporal dependencies, and (iii) variable bitrate versions of\nour algorithms. Since our improvements are compatible with a large class of\nexisting models, we provide further evidence that the generative modeling\nviewpoint can advance the neural video coding field.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 02:19:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Yang", "Ruihan", ""], ["Yang", "Yibo", ""], ["Marino", "Joseph", ""], ["Mandt", "Stephan", ""]]}, {"id": "2107.13137", "submitter": "Yang Tang", "authors": "Chaoqiang Zhao, Yang Tang and Qiyu Sun", "title": "Unsupervised Monocular Depth Estimation in Highly Complex Environments", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous unsupervised monocular depth estimation methods mainly focus on the\nday-time scenario, and their frameworks are driven by warped photometric\nconsistency. While in some challenging environments, like night, rainy night or\nsnowy winter, the photometry of the same pixel on different frames is\ninconsistent because of the complex lighting and reflection, so that the\nday-time unsupervised frameworks cannot be directly applied to these complex\nscenarios. In this paper, we investigate the problem of unsupervised monocular\ndepth estimation in certain highly complex scenarios. We address this\nchallenging problem by using domain adaptation, and a unified image\ntransfer-based adaptation framework is proposed based on monocular videos in\nthis paper. The depth model trained on day-time scenarios is adapted to\ndifferent complex scenarios. Instead of adapting the whole depth network, we\njust consider the encoder network for lower computational complexity. The depth\nmodels adapted by the proposed framework to different scenarios share the same\ndecoder, which is practical. Constraints on both feature space and output space\npromote the framework to learn the key features for depth decoding, and the\nsmoothness loss is introduced into the adaptation framework for better depth\nestimation performance. Extensive experiments show the effectiveness of the\nproposed unsupervised framework in estimating the dense depth map from the\nnight-time, rainy night-time and snowy winter images.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 02:35:38 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhao", "Chaoqiang", ""], ["Tang", "Yang", ""], ["Sun", "Qiyu", ""]]}, {"id": "2107.13144", "submitter": "Min-Cheol Sagong", "authors": "Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Won Jung, and Sung-Jea Ko", "title": "Content-aware Directed Propagation Network with Pixel Adaptive Kernel\n  Attention", "comments": "submitted to IEEE transactions on Neural Networks and Learning System", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have been not only widespread but also\nachieved noticeable results on numerous applications including image\nclassification, restoration, and generation. Although the weight-sharing\nproperty of convolutions makes them widely adopted in various tasks, its\ncontent-agnostic characteristic can also be considered a major drawback. To\nsolve this problem, in this paper, we propose a novel operation, called pixel\nadaptive kernel attention (PAKA). PAKA provides directivity to the filter\nweights by multiplying spatially varying attention from learnable features. The\nproposed method infers pixel-adaptive attention maps along the channel and\nspatial directions separately to address the decomposed model with fewer\nparameters. Our method is trainable in an end-to-end manner and applicable to\nany CNN-based models. In addition, we propose an improved information\naggregation module with PAKA, called the hierarchical PAKA module (HPM). We\ndemonstrate the superiority of our HPM by presenting state-of-the-art\nperformance on semantic segmentation compared to the conventional information\naggregation modules. We validate the proposed method through additional\nablation studies and visualizing the effect of PAKA providing directivity to\nthe weights of convolutions. We also show the generalizability of the proposed\nmethod by applying it to multi-modal tasks especially color-guided depth map\nsuper-resolution.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 02:59:19 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sagong", "Min-Cheol", ""], ["Yeo", "Yoon-Jae", ""], ["Jung", "Seung-Won", ""], ["Ko", "Sung-Jea", ""]]}, {"id": "2107.13152", "submitter": "Wei Zhou", "authors": "Wei Zhou, Xin Cao, Xiaodan Zhang, Xingxing Hao, Dekui Wang, Ying He", "title": "Multi Point-Voxel Convolution (MPVConv) for Deep Learning on Point\n  Clouds", "comments": "arXiv admin note: substantial text overlap with arXiv:2104.14834", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The existing 3D deep learning methods adopt either individual point-based\nfeatures or local-neighboring voxel-based features, and demonstrate great\npotential for processing 3D data. However, the point based models are\ninefficient due to the unordered nature of point clouds and the voxel-based\nmodels suffer from large information loss. Motivated by the success of recent\npoint-voxel representation, such as PVCNN, we propose a new convolutional\nneural network, called Multi Point-Voxel Convolution (MPVConv), for deep\nlearning on point clouds. Integrating both the advantages of voxel and\npoint-based methods, MPVConv can effectively increase the neighboring\ncollection between point-based features and also promote independence among\nvoxel-based features. Moreover, most of the existing approaches aim at solving\none specific task, and only a few of them can handle a variety of tasks. Simply\nreplacing the corresponding convolution module with MPVConv, we show that\nMPVConv can fit in different backbones to solve a wide range of 3D tasks.\nExtensive experiments on benchmark datasets such as ShapeNet Part, S3DIS and\nKITTI for various tasks show that MPVConv improves the accuracy of the backbone\n(PointNet) by up to \\textbf{36\\%}, and achieves higher accuracy than the\nvoxel-based model with up to \\textbf{34}$\\times$ speedups. In addition, MPVConv\noutperforms the state-of-the-art point-based models with up to\n\\textbf{8}$\\times$ speedups. Notably, our MPVConv achieves better accuracy than\nthe newest point-voxel-based model PVCNN (a model more efficient than PointNet)\nwith lower latency.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 03:42:59 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhou", "Wei", ""], ["Cao", "Xin", ""], ["Zhang", "Xiaodan", ""], ["Hao", "Xingxing", ""], ["Wang", "Dekui", ""], ["He", "Ying", ""]]}, {"id": "2107.13154", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong,\n  Xiatian Zhu, Tao Xiang", "title": "Global Aggregation then Local Distribution for Scene Parsing", "comments": "Accepted by IEEE-TIP-2021. arXiv admin note: text overlap with\n  arXiv:1909.07229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling long-range contextual relationships is critical for pixel-wise\nprediction tasks such as semantic segmentation. However, convolutional neural\nnetworks (CNNs) are inherently limited to model such dependencies due to the\nnaive structure in its building modules (\\eg, local convolution kernel). While\nrecent global aggregation methods are beneficial for long-range structure\ninformation modelling, they would oversmooth and bring noise to the regions\ncontaining fine details (\\eg,~boundaries and small objects), which are very\nmuch cared for the semantic segmentation task. To alleviate this problem, we\npropose to explore the local context for making the aggregated long-range\nrelationship being distributed more accurately in local regions. In particular,\nwe design a novel local distribution module which models the affinity map\nbetween global and local relationship for each pixel adaptively. Integrating\nexisting global aggregation modules, we show that our approach can be\nmodularized as an end-to-end trainable block and easily plugged into existing\nsemantic segmentation networks, giving rise to the \\emph{GALD} networks.\nDespite its simplicity and versatility, our approach allows us to build new\nstate of the art on major semantic segmentation benchmarks including\nCityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained\nmodels are released at \\url{https://github.com/lxtGH/GALD-DGCNet} to foster\nfurther research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 03:46:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Xiangtai", ""], ["Zhang", "Li", ""], ["Cheng", "Guangliang", ""], ["Yang", "Kuiyuan", ""], ["Tong", "Yunhai", ""], ["Zhu", "Xiatian", ""], ["Xiang", "Tao", ""]]}, {"id": "2107.13155", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Hao He, Henghui Ding, Kuiyuan Yang, Guangliang Cheng,\n  Jianping Shi, Yunhai Tong", "title": "Improving Video Instance Segmentation via Temporal Pyramid Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Instance Segmentation (VIS) is a new and inherently multi-task problem,\nwhich aims to detect, segment and track each instance in a video sequence.\nExisting approaches are mainly based on single-frame features or single-scale\nfeatures of multiple frames, where temporal information or multi-scale\ninformation is ignored. To incorporate both temporal and scale information, we\npropose a Temporal Pyramid Routing (TPR) strategy to conditionally align and\nconduct pixel-level aggregation from a feature pyramid pair of two adjacent\nframes. Specifically, TPR contains two novel components, including Dynamic\nAligned Cell Routing (DACR) and Cross Pyramid Routing (CPR), where DACR is\ndesigned for aligning and gating pyramid features across temporal dimension,\nwhile CPR transfers temporally aggregated features across scale dimension.\nMoreover, our approach is a plug-and-play module and can be easily applied to\nexisting instance segmentation methods. Extensive experiments on YouTube-VIS\ndataset demonstrate the effectiveness and efficiency of the proposed approach\non several state-of-the-art instance segmentation methods. Codes and trained\nmodels will be publicly available to facilitate future\nresearch.(\\url{https://github.com/lxtGH/TemporalPyramidRouting}).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 03:57:12 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Xiangtai", ""], ["He", "Hao", ""], ["Ding", "Henghui", ""], ["Yang", "Kuiyuan", ""], ["Cheng", "Guangliang", ""], ["Shi", "Jianping", ""], ["Tong", "Yunhai", ""]]}, {"id": "2107.13156", "submitter": "Zunlei Feng", "authors": "Xin Gao (1), Zhenjiang Liu (1), Zunlei Feng (2), Chengji Shen (2),\n  Kairi Ou (1), Haihong Tang (1) and Mingli Song (2) ((1) Alibaba Group, (2)\n  Zhejiang University)", "title": "Shape Controllable Virtual Try-on for Underwear Models", "comments": "10 pages, 9 figures, conference", "journal-ref": null, "doi": "10.1145/3474085.3475210", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image virtual try-on task has abundant applications and has become a hot\nresearch topic recently. Existing 2D image-based virtual try-on methods aim to\ntransfer a target clothing image onto a reference person, which has two main\ndisadvantages: cannot control the size and length precisely; unable to\naccurately estimate the user's figure in the case of users wearing thick\nclothes, resulting in inaccurate dressing effect. In this paper, we put forward\nan akin task that aims to dress clothing for underwear models. %, which is also\nan urgent need in e-commerce scenarios. To solve the above drawbacks, we\npropose a Shape Controllable Virtual Try-On Network (SC-VTON), where a graph\nattention network integrates the information of model and clothing to generate\nthe warped clothing image. In addition, the control points are incorporated\ninto SC-VTON for the desired clothing shape. Furthermore, by adding a Splitting\nNetwork and a Synthesis Network, we can use clothing/model pair data to help\noptimize the deformation module and generalize the task to the typical virtual\ntry-on task. Extensive experiments show that the proposed method can achieve\naccurate shape control. Meanwhile, compared with other methods, our method can\ngenerate high-resolution results with detailed textures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 04:01:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gao", "Xin", ""], ["Liu", "Zhenjiang", ""], ["Feng", "Zunlei", ""], ["Shen", "Chengji", ""], ["Ou", "Kairi", ""], ["Tang", "Haihong", ""], ["Song", "Mingli", ""]]}, {"id": "2107.13157", "submitter": "Anusua Trivedi", "authors": "Anusua Trivedi, Jocelyn Desbiens, Ron Gross, Sunil Gupta, Rahul\n  Dodhia, Juan Lavista Ferres", "title": "Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular\n  Diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: To demonstrate that retinal microvasculature per se is a reliable\nbiomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular\ndiseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to\ncolor fundus images for semantic segmentation of the blood vessels and severity\nclassification on both vascular and full images. Vessel reconstruction through\nharmonic descriptors is also used as a smoothing and de-noising tool. The\nmathematical background of the theory is also outlined. Results: For diabetic\npatients, at least 93.8% of DR No-Refer vs. Refer classification can be related\nto vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening\ncase, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the\ndisease biomarkers are related topologically to the vasculature. Translational\nRelevance: Experiments conducted on eye blood vasculature reconstruction as a\nbiomarker shows a strong correlation between vasculature shape and later stages\nof DR.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 04:03:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Trivedi", "Anusua", ""], ["Desbiens", "Jocelyn", ""], ["Gross", "Ron", ""], ["Gupta", "Sunil", ""], ["Dodhia", "Rahul", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2107.13167", "submitter": "Wei Zhou", "authors": "Yao Hu, Guohua Geng, Kang Li, Wei Zhou, Xingxing Hao, Xin Cao", "title": "Unsupervised Segmentation for Terracotta Warrior with\n  Seed-Region-Growing CNN(SRG-Net)", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.00433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The repairing work of terracotta warriors in Emperor Qinshihuang Mausoleum\nSite Museum is handcrafted by experts, and the increasing amounts of unearthed\npieces of terracotta warriors make the archaeologists too challenging to\nconduct the restoration of terracotta warriors efficiently. We hope to segment\nthe 3D point cloud data of the terracotta warriors automatically and store the\nfragment data in the database to assist the archaeologists in matching the\nactual fragments with the ones in the database, which could result in higher\nrepairing efficiency of terracotta warriors. Moreover, the existing 3D neural\nnetwork research is mainly focusing on supervised classification, clustering,\nunsupervised representation, and reconstruction. There are few pieces of\nresearches concentrating on unsupervised point cloud part segmentation. In this\npaper, we present SRG-Net for 3D point clouds of terracotta warriors to address\nthese problems. Firstly, we adopt a customized seed-region-growing algorithm to\nsegment the point cloud coarsely. Then we present a supervised segmentation and\nunsupervised reconstruction networks to learn the characteristics of 3D point\nclouds. Finally, we combine the SRG algorithm with our improved CNN using a\nrefinement method. This pipeline is called SRG-Net, which aims at conducting\nsegmentation tasks on the terracotta warriors. Our proposed SRG-Net is\nevaluated on the terracotta warriors data and ShapeNet dataset by measuring the\naccuracy and the latency. The experimental results show that our SRG-Net\noutperforms the state-of-the-art methods. Our code is shown in Code File\n1~\\cite{Srgnet_2021}.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 04:50:27 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hu", "Yao", ""], ["Geng", "Guohua", ""], ["Li", "Kang", ""], ["Zhou", "Wei", ""], ["Hao", "Xingxing", ""], ["Cao", "Xin", ""]]}, {"id": "2107.13170", "submitter": "Xiaojie Gao", "authors": "Xiaojie Gao, Yueming Jin, Qi Dou, Chi-Wing Fu, and Pheng-Ann Heng", "title": "Accurate Grid Keypoint Learning for Efficient Video Prediction", "comments": "IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction methods generally consume substantial computing resources in\ntraining and deployment, among which keypoint-based approaches show promising\nimprovement in efficiency by simplifying dense image prediction to light\nkeypoint prediction. However, keypoint locations are often modeled only as\ncontinuous coordinates, so noise from semantically insignificant deviations in\nvideos easily disrupt learning stability, leading to inaccurate keypoint\nmodeling. In this paper, we design a new grid keypoint learning framework,\naiming at a robust and explainable intermediate keypoint representation for\nlong-term efficient video prediction. We have two major technical\ncontributions. First, we detect keypoints by jumping among candidate locations\nin our raised grid space and formulate a condensation loss to encourage\nmeaningful keypoints with strong representative capability. Second, we\nintroduce a 2D binary map to represent the detected grid keypoints and then\nsuggest propagating keypoint locations with stochasticity by selecting entries\nin the discrete grid space, thus preserving the spatial structure of keypoints\nin the longterm horizon for better future frame generation. Extensive\nexperiments verify that our method outperforms the state-ofthe-art stochastic\nvideo prediction methods while saves more than 98% of computing resources. We\nalso demonstrate our method on a robotic-assisted surgery dataset with\npromising results. Our code is available at\nhttps://github.com/xjgaocs/Grid-Keypoint-Learning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 05:04:30 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gao", "Xiaojie", ""], ["Jin", "Yueming", ""], ["Dou", "Qi", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2107.13180", "submitter": "Javier Naranjo-Alcazar", "authors": "Javier Naranjo-Alcazar, Sergi Perez-Castanos, Aaron Lopez-Garcia,\n  Pedro Zuccarello, Maximo Cobos, Francesc J. Ferri", "title": "Squeeze-Excitation Convolutional Recurrent Neural Networks for\n  Audio-Visual Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The use of multiple and semantically correlated sources can provide\ncomplementary information to each other that may not be evident when working\nwith individual modalities on their own. In this context, multi-modal models\ncan help producing more accurate and robust predictions in machine learning\ntasks where audio-visual data is available. This paper presents a multi-modal\nmodel for automatic scene classification that exploits simultaneously auditory\nand visual information. The proposed approach makes use of two separate\nnetworks which are respectively trained in isolation on audio and visual data,\nso that each network specializes in a given modality. The visual subnetwork is\na pre-trained VGG16 model followed by a bidiretional recurrent layer, while the\nresidual audio subnetwork is based on stacked squeeze-excitation convolutional\nblocks trained from scratch. After training each subnetwork, the fusion of\ninformation from the audio and visual streams is performed at two different\nstages. The early fusion stage combines features resulting from the last\nconvolutional block of the respective subnetworks at different time steps to\nfeed a bidirectional recurrent structure. The late fusion stage combines the\noutput of the early fusion stage with the independent predictions provided by\nthe two subnetworks, resulting in the final prediction. We evaluate the method\nusing the recently published TAU Audio-Visual Urban Scenes 2021, which contains\nsynchronized audio and video recordings from 12 European cities in 10 different\nscene classes. The proposed model has been shown to provide an excellent\ntrade-off between prediction performance (86.5%) and system complexity (15M\nparameters) in the evaluation results of the DCASE 2021 Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 06:10:10 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Naranjo-Alcazar", "Javier", ""], ["Perez-Castanos", "Sergi", ""], ["Lopez-Garcia", "Aaron", ""], ["Zuccarello", "Pedro", ""], ["Cobos", "Maximo", ""], ["Ferri", "Francesc J.", ""]]}, {"id": "2107.13193", "submitter": "Ze Yang", "authors": "Ze Yang, Haofei Wang, Feng Lu", "title": "Assessment of Deep Learning-based Heart Rate Estimation using Remote\n  Photoplethysmography under Different Illuminations", "comments": "3 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photoplethysmography (rPPG) monitors heart rate without requiring\nphysical contact, which allows for a wide variety of applications. Deep\nlearning-based rPPG have demonstrated superior performance over the traditional\napproaches in controlled context. However, the lighting situation in indoor\nspace is typically complex, with uneven light distribution and frequent\nvariations in illumination. It lacks a fair comparison of different methods\nunder different illuminations using the same dataset. In this paper, we present\na public dataset, namely the BH-rPPG dataset, which contains data from twelve\nsubjects under three illuminations: low, medium, and high illumination. We also\nprovide the ground truth heart rate measured by an oximeter. We evaluate the\nperformance of three deep learning-based methods to that of four traditional\nmethods using two public datasets: the UBFC-rPPG dataset and the BH-rPPG\ndataset. The experimental results demonstrate that traditional methods are\ngenerally more resistant to fluctuating illuminations. We found that the\nrPPGNet achieves lowest MAE among deep learning-based method under medium\nillumination, whereas the CHROM achieves 1.5 beats per minute (BPM),\noutperforming the rPPGNet by 60%. These findings suggest that while developing\ndeep learning-based heart rate estimation algorithms, illumination variation\nshould be taken into account. This work serves as a benchmark for rPPG\nperformance evaluation and it opens a pathway for future investigation into\ndeep learning-based rPPG under illumination variations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 06:50:52 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Yang", "Ze", ""], ["Wang", "Haofei", ""], ["Lu", "Feng", ""]]}, {"id": "2107.13200", "submitter": "Fan Zhang", "authors": "Fan Zhang, Bo Pan, Pengfei Shao, Peng Liu (Alzheimer's Disease\n  Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle\n  flagship study of ageing), Shuwei Shen, Peng Yao, Ronald X. Xu", "title": "An explainable two-dimensional single model deep learning approach for\n  Alzheimer's disease diagnosis and brain atrophy localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 07:19:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Fan", "", "Alzheimer's Disease\n  Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle\n  flagship study of ageing"], ["Pan", "Bo", "", "Alzheimer's Disease\n  Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle\n  flagship study of ageing"], ["Shao", "Pengfei", "", "Alzheimer's Disease\n  Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle\n  flagship study of ageing"], ["Liu", "Peng", "", "Alzheimer's Disease\n  Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle\n  flagship study of ageing"], ["Shen", "Shuwei", ""], ["Yao", "Peng", ""], ["Xu", "Ronald X.", ""]]}, {"id": "2107.13217", "submitter": "Geetika Arora", "authors": "Geetika Arora, Rohit K Bharadwaj, Kamlesh Tiwari", "title": "DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile\n  and Hand-held Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes teeth-photo, a new biometric modality for human\nauthentication on mobile and hand held devices. Biometrics samples are acquired\nusing the camera mounted on mobile device with the help of a mobile application\nhaving specific markers to register the teeth area. Region of interest (RoI) is\nthen extracted using the markers and the obtained sample is enhanced using\ncontrast limited adaptive histogram equalization (CLAHE) for better visual\nclarity. We propose a deep learning architecture and novel regularization\nscheme to obtain highly discriminative embedding for small size RoI. Proposed\ncustom loss function was able to achieve perfect classification for the tiny\nRoI of $75\\times 75$ size. The model is end-to-end and few-shot and therefore\nis very efficient in terms of time and energy requirements. The system can be\nused in many ways including device unlocking and secure authentication. To the\nbest of our understanding, this is the first work on teeth-photo based\nauthentication for mobile device. Experiments have been conducted on an\nin-house teeth-photo database collected using our application. The database is\nmade publicly available. Results have shown that the proposed system has\nperfect accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 08:00:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Arora", "Geetika", ""], ["Bharadwaj", "Rohit K", ""], ["Tiwari", "Kamlesh", ""]]}, {"id": "2107.13221", "submitter": "Jeesoo Kim", "authors": "Jeesoo Kim, Junsuk Choe, Sangdoo Yun, Nojun Kwak", "title": "Normalization Matters in Weakly Supervised Object Localization", "comments": "Accepted at ICCV 2021. 16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object localization (WSOL) enables finding an object using\na dataset without any localization information. By simply training a\nclassification model using only image-level annotations, the feature map of the\nmodel can be utilized as a score map for localization. In spite of many WSOL\nmethods proposing novel strategies, there has not been any de facto standard\nabout how to normalize the class activation map (CAM). Consequently, many WSOL\nmethods have failed to fully exploit their own capacity because of the misuse\nof a normalization method. In this paper, we review many existing normalization\nmethods and point out that they should be used according to the property of the\ngiven dataset. Additionally, we propose a new normalization method which\nsubstantially enhances the performance of any CAM-based WSOL methods. Using the\nproposed normalization method, we provide a comprehensive evaluation over three\ndatasets (CUB, ImageNet and OpenImages) on three different architectures and\nobserve significant performance gains over the conventional min-max\nnormalization method in all the evaluated cases.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 08:14:49 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kim", "Jeesoo", ""], ["Choe", "Junsuk", ""], ["Yun", "Sangdoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "2107.13233", "submitter": "Christos Kyrkou", "authors": "Christos Kyrkou", "title": "C^3Net: End-to-End deep learning for efficient real-time visual active\n  camera control", "comments": "Journal of Real-Time Image Processing , 2021. Real-time active\n  vision, Smart camera, Deep learning, End-to-end learning\n  https://www.youtube.com/watch?v=UuepDtWUpsg&ab_channel=ChristosKyrkou. arXiv\n  admin note: text overlap with arXiv:2012.06428", "journal-ref": null, "doi": "10.1007/s11554-021-01077-z", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The need for automated real-time visual systems in applications such as smart\ncamera surveillance, smart environments, and drones necessitates the\nimprovement of methods for visual active monitoring and control. Traditionally,\nthe active monitoring task has been handled through a pipeline of modules such\nas detection, filtering, and control. However, such methods are difficult to\njointly optimize and tune their various parameters for real-time processing in\nresource constraint systems. In this paper a deep Convolutional Camera\nController Neural Network is proposed to go directly from visual information to\ncamera movement to provide an efficient solution to the active vision problem.\nIt is trained end-to-end without bounding box annotations to control a camera\nand follow multiple targets from raw pixel values. Evaluation through both a\nsimulation framework and real experimental setup, indicate that the proposed\nsolution is robust to varying conditions and able to achieve better monitoring\nperformance than traditional approaches both in terms of number of targets\nmonitored as well as in effective monitoring time. The advantage of the\nproposed approach is that it is computationally less demanding and can run at\nover 10 FPS (~4x speedup) on an embedded smart camera providing a practical and\naffordable solution to real-time active monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 09:31:46 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kyrkou", "Christos", ""]]}, {"id": "2107.13237", "submitter": "Sidharth Pancholi", "authors": "Uddipan Mukherjee, Sidharth Pancholi", "title": "A Visual Domain Transfer Learning Approach for Heartbeat Sound\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heart disease is the most common reason for human mortality that causes\nalmost one-third of deaths throughout the world. Detecting the disease early\nincreases the chances of survival of the patient and there are several ways a\nsign of heart disease can be detected early. This research proposes to convert\ncleansed and normalized heart sound into visual mel scale spectrograms and then\nusing visual domain transfer learning approaches to automatically extract\nfeatures and categorize between heart sounds. Some of the previous studies\nfound that the spectrogram of various types of heart sounds is visually\ndistinguishable to human eyes, which motivated this study to experiment on\nvisual domain classification approaches for automated heart sound\nclassification. It will use convolution neural network-based architectures i.e.\nResNet, MobileNetV2, etc as the automated feature extractors from spectrograms.\nThese well-accepted models in the image domain showed to learn generalized\nfeature representations of cardiac sounds collected from different environments\nwith varying amplitude and noise levels. Model evaluation criteria used were\ncategorical accuracy, precision, recall, and AUROC as the chosen dataset is\nunbalanced. The proposed approach has been implemented on datasets A and B of\nthe PASCAL heart sound collection and resulted in ~ 90% categorical accuracy\nand AUROC of ~0.97 for both sets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 09:41:38 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Mukherjee", "Uddipan", ""], ["Pancholi", "Sidharth", ""]]}, {"id": "2107.13259", "submitter": "Xiao Gu", "authors": "Xiao Gu, Jianing Qiu, Yao Guo, Benny Lo, Guang-Zhong Yang", "title": "TransAction: ICL-SJTU Submission to EPIC-Kitchens Action Anticipation\n  Challenge 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, the technical details of our submission to the EPIC-Kitchens\nAction Anticipation Challenge 2021 are given. We developed a hierarchical\nattention model for action anticipation, which leverages Transformer-based\nattention mechanism to aggregate features across temporal dimension,\nmodalities, symbiotic branches respectively. In terms of Mean Top-5 Recall of\naction, our submission with team name ICL-SJTU achieved 13.39% for overall\ntesting set, 10.05% for unseen subsets and 11.88% for tailed subsets.\nAdditionally, it is noteworthy that our submission ranked 1st in terms of verb\nclass in all three (sub)sets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 10:42:47 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gu", "Xiao", ""], ["Qiu", "Jianing", ""], ["Guo", "Yao", ""], ["Lo", "Benny", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "2107.13261", "submitter": "Eugenio Lomurno", "authors": "Eugenio Lomurno, Andrea Romanoni, Matteo Matteucci", "title": "Improving Multi-View Stereo via Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today, Multi-View Stereo techniques are able to reconstruct robust and\ndetailed 3D models, especially when starting from high-resolution images.\nHowever, there are cases in which the resolution of input images is relatively\nlow, for instance, when dealing with old photos, or when hardware constrains\nthe amount of data that can be acquired. In this paper, we investigate if, how,\nand how much increasing the resolution of such input images through\nSuper-Resolution techniques reflects in quality improvements of the\nreconstructed 3D models, despite the artifacts that sometimes this may\ngenerate. We show that applying a Super-Resolution step before recovering the\ndepth maps in most cases leads to a better 3D model both in the case of\nPatchMatch-based and deep-learning-based algorithms. The use of\nSuper-Resolution improves especially the completeness of reconstructed models\nand turns out to be particularly effective in the case of textured scenes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 10:45:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lomurno", "Eugenio", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "2107.13263", "submitter": "Aji Resindra Widya", "authors": "Aji Resindra Widya, Yusuke Monno, Masatoshi Okutomi, Sho Suzuki,\n  Takuji Gotoda, Kenji Miki", "title": "Learning-Based Depth and Pose Estimation for Monocular Endoscope with\n  Loss Generalization", "comments": "Accepted for EMBC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastroendoscopy has been a clinical standard for diagnosing and treating\nconditions that affect a part of a patient's digestive system, such as the\nstomach. Despite the fact that gastroendoscopy has a lot of advantages for\npatients, there exist some challenges for practitioners, such as the lack of 3D\nperception, including the depth and the endoscope pose information. Such\nchallenges make navigating the endoscope and localizing any found lesion in a\ndigestive tract difficult. To tackle these problems, deep learning-based\napproaches have been proposed to provide monocular gastroendoscopy with\nadditional yet important depth and pose information. In this paper, we propose\na novel supervised approach to train depth and pose estimation networks using\nconsecutive endoscopy images to assist the endoscope navigation in the stomach.\nWe firstly generate real depth and pose training data using our previously\nproposed whole stomach 3D reconstruction pipeline to avoid poor generalization\nability between computer-generated (CG) models and real data for the stomach.\nIn addition, we propose a novel generalized photometric loss function to avoid\nthe complicated process of finding proper weights for balancing the depth and\nthe pose loss terms, which is required for existing direct depth and pose\nsupervision approaches. We then experimentally show that our proposed\ngeneralized loss performs better than existing direct supervision losses.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 10:51:06 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Widya", "Aji Resindra", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""], ["Suzuki", "Sho", ""], ["Gotoda", "Takuji", ""], ["Miki", "Kenji", ""]]}, {"id": "2107.13269", "submitter": "Chenhang He", "authors": "Chenhang He, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang", "title": "Aug3D-RPN: Improving Monocular 3D Object Detection by Synthetic Images\n  with Virtual Depth", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current geometry-based monocular 3D object detection models can efficiently\ndetect objects by leveraging perspective geometry, but their performance is\nlimited due to the absence of accurate depth information. Though this issue can\nbe alleviated in a depth-based model where a depth estimation module is plugged\nto predict depth information before 3D box reasoning, the introduction of such\nmodule dramatically reduces the detection speed. Instead of training a costly\ndepth estimator, we propose a rendering module to augment the training data by\nsynthesizing images with virtual-depths. The rendering module takes as input\nthe RGB image and its corresponding sparse depth image, outputs a variety of\nphoto-realistic synthetic images, from which the detection model can learn more\ndiscriminative features to adapt to the depth changes of the objects. Besides,\nwe introduce an auxiliary module to improve the detection model by jointly\noptimizing it through a depth estimation task. Both modules are working in the\ntraining time and no extra computation will be introduced to the detection\nmodel. Experiments show that by working with our proposed modules, a\ngeometry-based model can represent the leading accuracy on the KITTI 3D\ndetection benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:00:47 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["He", "Chenhang", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""], ["Zhang", "Lei", ""]]}, {"id": "2107.13271", "submitter": "Yanda Meng", "authors": "Yanda Meng, Hongrun Zhang, Yitian Zhao, Xiaoyun Yang, Xuesheng Qian,\n  Xiaowei Huang, Yalin Zheng", "title": "Spatial Uncertainty-Aware Semi-Supervised Crowd Counting", "comments": "Accepted by ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised approaches for crowd counting attract attention, as the fully\nsupervised paradigm is expensive and laborious due to its request for a large\nnumber of images of dense crowd scenarios and their annotations. This paper\nproposes a spatial uncertainty-aware semi-supervised approach via regularized\nsurrogate task (binary segmentation) for crowd counting problems. Different\nfrom existing semi-supervised learning-based crowd counting methods, to exploit\nthe unlabeled data, our proposed spatial uncertainty-aware teacher-student\nframework focuses on high confident regions' information while addressing the\nnoisy supervision from the unlabeled data in an end-to-end manner.\nSpecifically, we estimate the spatial uncertainty maps from the teacher model's\nsurrogate task to guide the feature learning of the main task (density\nregression) and the surrogate task of the student model at the same time.\nBesides, we introduce a simple yet effective differential transformation layer\nto enforce the inherent spatial consistency regularization between the main\ntask and the surrogate task in the student model, which helps the surrogate\ntask to yield more reliable predictions and generates high-quality uncertainty\nmaps. Thus, our model can also address the task-level perturbation problems\nthat occur spatial inconsistency between the primary and surrogate tasks in the\nstudent model. Experimental results on four challenging crowd counting datasets\ndemonstrate that our method achieves superior performance to the\nstate-of-the-art semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:06:52 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Meng", "Yanda", ""], ["Zhang", "Hongrun", ""], ["Zhao", "Yitian", ""], ["Yang", "Xiaoyun", ""], ["Qian", "Xuesheng", ""], ["Huang", "Xiaowei", ""], ["Zheng", "Yalin", ""]]}, {"id": "2107.13273", "submitter": "German Barquero", "authors": "Germ\\'an Barquero, Isabelle Hupont and Carles Fern\\'andez", "title": "Rank-based verification for long-term face tracking in crowded scenes", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.08675", "journal-ref": "IEEE Transactions on Biometrics, Behavior, and Identity Science,\n  2021", "doi": "10.1109/TBIOM.2021.3099568", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most current multi-object trackers focus on short-term tracking, and are\nbased on deep and complex systems that often cannot operate in real-time,\nmaking them impractical for video-surveillance. In this paper we present a\nlong-term, multi-face tracking architecture conceived for working in crowded\ncontexts where faces are often the only visible part of a person. Our system\nbenefits from advances in the fields of face detection and face recognition to\nachieve long-term tracking, and is particularly unconstrained to the motion and\nocclusions of people. It follows a tracking-by-detection approach, combining a\nfast short-term visual tracker with a novel online tracklet reconnection\nstrategy grounded on rank-based face verification. The proposed rank-based\nconstraint favours higher inter-class distance among tracklets, and reduces the\npropagation of errors due to wrong reconnections. Additionally, a correction\nmodule is included to correct past assignments with no extra computational\ncost. We present a series of experiments introducing novel specialized metrics\nfor the evaluation of long-term tracking capabilities, and publicly release a\nvideo dataset with 10 manually annotated videos and a total length of 8' 54\".\nOur findings validate the robustness of each of the proposed modules, and\ndemonstrate that, in these challenging contexts, our approach yields up to 50%\nlonger tracks than state-of-the-art deep learning trackers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:15:04 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Barquero", "Germ\u00e1n", ""], ["Hupont", "Isabelle", ""], ["Fern\u00e1ndez", "Carles", ""]]}, {"id": "2107.13277", "submitter": "Yue Shi", "authors": "Yue Shi, Liangxiu Han, Anthony Kleerekoper, Sheng Chang, Tongle Hu", "title": "A Novel CropdocNet for Automated Potato Late Blight Disease Detection\n  from the Unmanned Aerial Vehicle-based Hyperspectral Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Late blight disease is one of the most destructive diseases in potato crop,\nleading to serious yield losses globally. Accurate diagnosis of the disease at\nearly stage is critical for precision disease control and management. Current\nfarm practices in crop disease diagnosis are based on manual visual inspection,\nwhich is costly, time consuming, subject to individual bias. Recent advances in\nimaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote\nsensing and machine learning offer the opportunity to address this challenge.\nParticularly, hyperspectral imagery (HSI) combining with machine learning/deep\nlearning approaches is preferable for accurately identifying specific plant\ndiseases because the HSI consists of a wide range of high-quality reflectance\ninformation beyond human vision, capable of capturing both spectral-spatial\ninformation. The proposed method considers the potential disease specific\nreflectance radiation variance caused by the canopy structural diversity,\nintroduces the multiple capsule layers to model the hierarchical structure of\nthe spectral-spatial disease attributes with the encapsulated features to\nrepresent the various classes and the rotation invariance of the disease\nattributes in the feature space. We have evaluated the proposed method with the\nreal UAV-based HSI data under the controlled field conditions. The\neffectiveness of the hierarchical features has been quantitatively assessed and\ncompared with the existing representative machine learning/deep learning\nmethods. The experiment results show that the proposed model significantly\nimproves the accuracy performance when considering hierarchical-structure of\nspectral-spatial features, comparing to the existing methods only using\nspectral, or spatial or spectral-spatial features without consider\nhierarchical-structure of spectral-spatial features.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:18:48 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Shi", "Yue", ""], ["Han", "Liangxiu", ""], ["Kleerekoper", "Anthony", ""], ["Chang", "Sheng", ""], ["Hu", "Tongle", ""]]}, {"id": "2107.13279", "submitter": "Haokui Zhang", "authors": "Libo Sun, Haokui Zhang and Wei Yin", "title": "Pseudo-LiDAR Based Road Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road detection is a critically important task for self-driving cars. By\nemploying LiDAR data, recent works have significantly improved the accuracy of\nroad detection. Relying on LiDAR sensors limits the wide application of those\nmethods when only cameras are available. In this paper, we propose a novel road\ndetection approach with RGB being the only input during inference.\nSpecifically, we exploit pseudo-LiDAR using depth estimation, and propose a\nfeature fusion network where RGB and learned depth information are fused for\nimproved road detection. To further optimize the network structure and improve\nthe efficiency of the network. we search for the network structure of the\nfeature fusion module using NAS techniques. Finally, be aware of that\ngenerating pseudo-LiDAR from RGB via depth estimation introduces extra\ncomputational costs and relies on depth estimation networks, we design a\nmodality distillation strategy and leverage it to further free our network from\nthese extra computational cost and dependencies during inference. The proposed\nmethod achieves state-of-the-art performance on two challenging benchmarks,\nKITTI and R2D.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:21:42 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sun", "Libo", ""], ["Zhang", "Haokui", ""], ["Yin", "Wei", ""]]}, {"id": "2107.13335", "submitter": "Qiufu Li", "authors": "Qiufu Li, Linlin Shen, Sheng Guo, Zhihui Lai", "title": "WaveCNet: Wavelet Integrated CNNs to Suppress Aliasing Effect for\n  Noise-Robust Image Classification", "comments": "IEEE TIP accepted paper. arXiv admin note: substantial text overlap\n  with arXiv:2005.03337", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though widely used in image classification, convolutional neural networks\n(CNNs) are prone to noise interruptions, i.e. the CNN output can be drastically\nchanged by small image noise. To improve the noise robustness, we try to\nintegrate CNNs with wavelet by replacing the common down-sampling (max-pooling,\nstrided-convolution, and average pooling) with discrete wavelet transform\n(DWT). We firstly propose general DWT and inverse DWT (IDWT) layers applicable\nto various orthogonal and biorthogonal discrete wavelets like Haar, Daubechies,\nand Cohen, etc., and then design wavelet integrated CNNs (WaveCNets) by\nintegrating DWT into the commonly used CNNs (VGG, ResNets, and DenseNet).\nDuring the down-sampling, WaveCNets apply DWT to decompose the feature maps\ninto the low-frequency and high-frequency components. Containing the main\ninformation including the basic object structures, the low-frequency component\nis transmitted into the following layers to generate robust high-level\nfeatures. The high-frequency components are dropped to remove most of the data\nnoises. The experimental results show that %wavelet accelerates the CNN\ntraining, and WaveCNets achieve higher accuracy on ImageNet than various\nvanilla CNNs. We have also tested the performance of WaveCNets on the noisy\nversion of ImageNet, ImageNet-C and six adversarial attacks, the results\nsuggest that the proposed DWT/IDWT layers could provide better noise-robustness\nand adversarial robustness. When applying WaveCNets as backbones, the\nperformance of object detectors (i.e., faster R-CNN and RetinaNet) on COCO\ndetection dataset are consistently improved. We believe that suppression of\naliasing effect, i.e. separation of low frequency and high frequency\ninformation, is the main advantages of our approach. The code of our DWT/IDWT\nlayer and different WaveCNets are available at\nhttps://github.com/CVI-SZU/WaveCNet.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 12:59:15 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Qiufu", ""], ["Shen", "Linlin", ""], ["Guo", "Sheng", ""], ["Lai", "Zhihui", ""]]}, {"id": "2107.13355", "submitter": "Ashlesha Kumar Ms.", "authors": "Ashlesha Kumar, Kuldip Singh Sangwan and Dhiraj", "title": "A Computer Vision-Based Approach for Driver Distraction Recognition\n  using Deep Learning and Genetic Algorithm Based Ensemble", "comments": "12 pages, Presented in 20th International Conference on Artificial\n  Intelligence and Soft Computing (ICAISC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As the proportion of road accidents increases each year, driver distraction\ncontinues to be an important risk component in road traffic injuries and\ndeaths. The distractions caused by the increasing use of mobile phones and\nother wireless devices pose a potential risk to road safety. Our current study\naims to aid the already existing techniques in driver posture recognition by\nimproving the performance in the driver distraction classification problem. We\npresent an approach using a genetic algorithm-based ensemble of six independent\ndeep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla\nCNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two\ncomprehensive datasets, the AUC Distracted Driver Dataset, on which our\ntechnique achieves an accuracy of 96.37%, surpassing the previously obtained\n95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an\naccuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024\nseconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce\nGTX 1080.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 13:39:31 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kumar", "Ashlesha", ""], ["Sangwan", "Kuldip Singh", ""], ["Dhiraj", "", ""]]}, {"id": "2107.13362", "submitter": "Mariella Dimiccoli", "authors": "Mariella Dimiccoli, Llu\\'is Garrido, Guillem Rodriguez-Corominas,\n  Herwig Wendt", "title": "Graph Constrained Data Representation Learning for Human Motion\n  Segmentation", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, transfer subspace learning based approaches have shown to be a\nvalid alternative to unsupervised subspace clustering and temporal data\nclustering for human motion segmentation (HMS). These approaches leverage prior\nknowledge from a source domain to improve clustering performance on a target\ndomain, and currently they represent the state of the art in HMS. Bucking this\ntrend, in this paper, we propose a novel unsupervised model that learns a\nrepresentation of the data and digs clustering information from the data\nitself. Our model is reminiscent of temporal subspace clustering, but presents\ntwo critical differences. First, we learn an auxiliary data matrix that can\ndeviate from the initial data, hence confer more degrees of freedom to the\ncoding matrix. Second, we introduce a regularization term for this auxiliary\ndata matrix that preserves the local geometrical structure present in the\nhigh-dimensional space. The proposed model is efficiently optimized by using an\noriginal Alternating Direction Method of Multipliers (ADMM) formulation\nallowing to learn jointly the auxiliary data representation, a nonnegative\ndictionary and a coding matrix. Experimental results on four benchmark datasets\nfor HMS demonstrate that our approach achieves significantly better clustering\nperformance then state-of-the-art methods, including both unsupervised and more\nrecent semi-supervised transfer learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 13:49:16 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Garrido", "Llu\u00eds", ""], ["Rodriguez-Corominas", "Guillem", ""], ["Wendt", "Herwig", ""]]}, {"id": "2107.13379", "submitter": "Patrick Feeney", "authors": "Patrick Feeney and Michael C. Hughes", "title": "Evaluating the Use of Reconstruction Error for Novelty Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pixelwise reconstruction error of deep autoencoders is often utilized for\nimage novelty detection and localization under the assumption that pixels with\nhigh error indicate which parts of the input image are unfamiliar and therefore\nlikely to be novel. This assumed correlation between pixels with high\nreconstruction error and novel regions of input images has not been verified\nand may limit the accuracy of these methods. In this paper we utilize saliency\nmaps to evaluate whether this correlation exists. Saliency maps reveal directly\nhow much a change in each input pixel would affect reconstruction loss, while\neach pixel's reconstruction error may be attributed to many input pixels when\nlayers are fully connected. We compare saliency maps to reconstruction error\nmaps via qualitative visualizations as well as quantitative correspondence\nbetween the top K elements of the maps for both novel and normal images. Our\nresults indicate that reconstruction error maps do not closely correlate with\nthe importance of pixels in the input images, making them insufficient for\nnovelty localization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:10:55 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Feeney", "Patrick", ""], ["Hughes", "Michael C.", ""]]}, {"id": "2107.13389", "submitter": "Rindra Ramamonjison", "authors": "Rindra Ramamonjison, Amin Banitalebi-Dehkordi, Xinyu Kang, Xiaolong\n  Bai, Yong Zhang", "title": "SimROD: A Simple Adaptation Method for Robust Object Detection", "comments": "Accepted to ICCV 2021 conference for full oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:28:32 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ramamonjison", "Rindra", ""], ["Banitalebi-Dehkordi", "Amin", ""], ["Kang", "Xinyu", ""], ["Bai", "Xiaolong", ""], ["Zhang", "Yong", ""]]}, {"id": "2107.13407", "submitter": "Germ\\'an Mora-Mart\\'in", "authors": "Germ\\'an Mora-Mart\\'in, Alex Turpin, Alice Ruget, Abderrahim Halimi,\n  Robert Henderson, Jonathan Leach and Istvan Gyongy", "title": "High-speed object detection with a single-photon time-of-flight image\n  sensor", "comments": "13 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D time-of-flight (ToF) imaging is used in a variety of applications such as\naugmented reality (AR), computer interfaces, robotics and autonomous systems.\nSingle-photon avalanche diodes (SPADs) are one of the enabling technologies\nproviding accurate depth data even over long ranges. By developing SPADs in\narray format with integrated processing combined with pulsed, flood-type\nillumination, high-speed 3D capture is possible. However, array sizes tend to\nbe relatively small, limiting the lateral resolution of the resulting depth\nmaps, and, consequently, the information that can be extracted from the image\nfor applications such as object detection. In this paper, we demonstrate that\nthese limitations can be overcome through the use of convolutional neural\nnetworks (CNNs) for high-performance object detection. We present outdoor\nresults from a portable SPAD camera system that outputs 16-bin photon timing\nhistograms with 64x32 spatial resolution. The results, obtained with exposure\ntimes down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)\nratios as low as 0.05, point to the advantages of providing the CNN with full\nhistogram data rather than point clouds alone. Alternatively, a combination of\npoint cloud and active intensity data may be used as input, for a similar level\nof performance. In either case, the GPU-accelerated processing time is less\nthan 1 ms per frame, leading to an overall latency (image acquisition plus\nprocessing) in the millisecond range, making the results relevant for\nsafety-critical computer vision applications which would benefit from faster\nthan human reaction times.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:53:44 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Mora-Mart\u00edn", "Germ\u00e1n", ""], ["Turpin", "Alex", ""], ["Ruget", "Alice", ""], ["Halimi", "Abderrahim", ""], ["Henderson", "Robert", ""], ["Leach", "Jonathan", ""], ["Gyongy", "Istvan", ""]]}, {"id": "2107.13411", "submitter": "Antonino Furnari", "authors": "Ivan Rodin, Antonino Furnari, Dimitrios Mavroedis, Giovanni Maria\n  Farinella", "title": "Predicting the Future from First Person (Egocentric) Vision: A Survey", "comments": "Computer Vision and Image Understanding, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric videos can bring a lot of information about how humans perceive\nthe world and interact with the environment, which can be beneficial for the\nanalysis of human behaviour. The research in egocentric video analysis is\ndeveloping rapidly thanks to the increasing availability of wearable devices\nand the opportunities offered by new large-scale egocentric datasets. As\ncomputer vision techniques continue to develop at an increasing pace, the tasks\nrelated to the prediction of future are starting to evolve from the need of\nunderstanding the present. Predicting future human activities, trajectories and\ninteractions with objects is crucial in applications such as human-robot\ninteraction, assistive wearable technologies for both industrial and daily\nliving scenarios, entertainment and virtual or augmented reality. This survey\nsummarises the evolution of studies in the context of future prediction from\negocentric vision making an overview of applications, devices, existing\nproblems, commonly used datasets, models and input modalities. Our analysis\nhighlights that methods for future prediction from egocentric vision can have a\nsignificant impact in a range of applications and that further research efforts\nshould be devoted to the standardisation of tasks and the proposal of datasets\nconsidering real-world scenarios such as the ones with an industrial vocation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:58:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Rodin", "Ivan", ""], ["Furnari", "Antonino", ""], ["Mavroedis", "Dimitrios", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2107.13421", "submitter": "Yuan Liu", "authors": "Yuan Liu and Sida Peng and Lingjie Liu and Qianqian Wang and Peng Wang\n  and Christian Theobalt and Xiaowei Zhou and Wenping Wang", "title": "Neural Rays for Occlusion-aware Image-based Rendering", "comments": "16 pages and 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis (NVS) task with multi-view images as input. Existing\nneural scene representations for solving the NVS problem, such as NeRF, cannot\ngeneralize to new scenes and take an excessively long time on training on each\nnew scene from scratch. The other subsequent neural rendering methods based on\nstereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize\nto unseen scenes but suffer from view inconsistency in complex scenes with\nself-occlusions. To address these issues, our NeuRay method represents every\nscene by encoding the visibility of rays associated with the input views. This\nneural representation can efficiently be initialized from depths estimated by\nexternal MVS methods, which is able to generalize to new scenes and achieves\nsatisfactory rendering images without any training on the scene. Then, the\ninitialized NeuRay can be further optimized on every scene with little training\ntiming to enforce spatial coherence to ensure view consistency in the presence\nof severe self-occlusion. Experiments demonstrate that NeuRay can quickly\ngenerate high-quality novel view images of unseen scenes with little finetuning\nand can handle complex scenes with severe self-occlusions which previous\nmethods struggle with.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:09:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Yuan", ""], ["Peng", "Sida", ""], ["Liu", "Lingjie", ""], ["Wang", "Qianqian", ""], ["Wang", "Peng", ""], ["Theobalt", "Christian", ""], ["Zhou", "Xiaowei", ""], ["Wang", "Wenping", ""]]}, {"id": "2107.13429", "submitter": "Weixia Zhang", "authors": "Weixia Zhang and Kede Ma and Guangtao Zhai and Xiaokang Yang", "title": "Task-Specific Normalization for Continual Learning of Blind Image\n  Quality Models", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The computational vision community has recently paid attention to continual\nlearning for blind image quality assessment (BIQA). The primary challenge is to\ncombat catastrophic forgetting of previously-seen IQA datasets (i.e., tasks).\nIn this paper, we present a simple yet effective continual learning method for\nBIQA with improved quality prediction accuracy, plasticity-stability trade-off,\nand task-order/length robustness. The key step in our approach is to freeze all\nconvolution filters of a pre-trained deep neural network (DNN) for an explicit\npromise of stability, and learn task-specific normalization parameters for\nplasticity. We assign each new task a prediction head, and load the\ncorresponding normalization parameters to produce a quality score. The final\nquality estimate is computed by feature fusion and adaptive weighting using\nhierarchical representations, without leveraging the test-time oracle.\nExtensive experiments on six IQA datasets demonstrate the advantages of the\nproposed method in comparison to previous training techniques for BIQA.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:21:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Kede", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2107.13431", "submitter": "Shuang Ge", "authors": "Shuang Ge, Qiongyu Ye, Wenquan Xie, Desheng Sun, Huabin Zhang, Xiaobo\n  Zhou, Kehong Yuan", "title": "AI assisted method for efficiently generating breast ultrasound\n  screening reports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ultrasound is the preferred choice for early screening of dense breast\ncancer. Clinically, doctors have to manually write the screening report which\nis time-consuming and laborious, and it is easy to miss and miswrite.\nTherefore, this paper proposes a method for efficiently generating personalized\nbreast ultrasound screening preliminary reports by AI, especially for benign\nand normal cases which account for the majority. Doctors then make simple\nadjustments or corrections to quickly generate final reports. The proposed\napproach has been tested using a database of 1133 breast tumor instances.\nExperimental results indicate this pipeline improves doctors' work efficiency\nby up to 90%, which greatly reduces repetitive work.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:21:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ge", "Shuang", ""], ["Ye", "Qiongyu", ""], ["Xie", "Wenquan", ""], ["Sun", "Desheng", ""], ["Zhang", "Huabin", ""], ["Zhou", "Xiaobo", ""], ["Yuan", "Kehong", ""]]}, {"id": "2107.13452", "submitter": "Qing Guo", "authors": "Qing Guo and Zhijie Wang and Felix Juefei-Xu and Di Lin and Lei Ma and\n  Wei Feng and Yang Liu", "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion", "comments": "10 pages and 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:07:20 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Guo", "Qing", ""], ["Wang", "Zhijie", ""], ["Juefei-Xu", "Felix", ""], ["Lin", "Di", ""], ["Ma", "Lei", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""]]}, {"id": "2107.13459", "submitter": "Hanxiao Tan", "authors": "Hanxiao Tan, Helena Kotthaus", "title": "Surrogate Model-Based Explainability Methods for Point Cloud NNs", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of autonomous driving and robotics, point clouds are showing\ntheir excellent real-time performance as raw data from most of the mainstream\n3D sensors. Therefore, point cloud neural networks have become a popular\nresearch direction in recent years. So far, however, there has been little\ndiscussion about the explainability of deep neural networks for point clouds.\nIn this paper, we propose new explainability approaches for point cloud deep\nneural networks based on local surrogate model-based methods to show which\ncomponents make the main contribution to the classification. Moreover, we\npropose a quantitative validation method for explainability methods of point\nclouds which enhances the persuasive power of explainability by dropping the\nmost positive or negative contributing features and monitoring how the\nclassification scores of specific categories change. To enable an intuitive\nexplanation of misclassified instances, we display features with confounding\ncontributions. Our new explainability approach provides a fairly accurate, more\nintuitive and widely applicable explanation for point cloud classification\ntasks. Our code is available at https://github.com/Explain3D/Explainable3D\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:13:20 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Tan", "Hanxiao", ""], ["Kotthaus", "Helena", ""]]}, {"id": "2107.13463", "submitter": "Christoph Palm", "authors": "Maximilian Weiherer, Andreas Eigenberger, Vanessa Br\\'ebant, Lukas\n  Prantl, Christoph Palm", "title": "Learning the shape of female breasts: an open-access 3D statistical\n  shape model of the female breast built from 110 breast scans", "comments": "15 pages, 14 figures, for download of RBSM visit\n  https://rbsm.re-mic.de , submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Regensburg Breast Shape Model (RBSM) - a 3D statistical shape\nmodel of the female breast built from 110 breast scans, and the first ever\npublicly available. Together with the model, a fully automated, pairwise\nsurface registration pipeline used to establish correspondence among 3D breast\nscans is introduced. Our method is computationally efficient and requires only\nfour landmarks to guide the registration process. In order to weaken the strong\ncoupling between breast and thorax, we propose to minimize the variance outside\nthe breast region as much as possible. To achieve this goal, a novel concept\ncalled breast probability masks (BPMs) is introduced. A BPM assigns\nprobabilities to each point of a 3D breast scan, telling how likely it is that\na particular point belongs to the breast area. During registration, we use BPMs\nto align the template to the target as accurately as possible inside the breast\nregion and only roughly outside. This simple yet effective strategy\nsignificantly reduces the unwanted variance outside the breast region, leading\nto better statistical shape models in which breast shapes are quite well\ndecoupled from the thorax. The RBSM is thus able to produce a variety of\ndifferent breast shapes as independently as possible from the shape of the\nthorax. Our systematic experimental evaluation reveals a generalization ability\nof 0.17 mm and a specificity of 2.8 mm for the RBSM. Ultimately, our model is\nseen as a first step towards combining physically motivated deformable models\nof the breast and statistical approaches in order to enable more realistic\nsurgical outcome simulation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:14:49 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Weiherer", "Maximilian", ""], ["Eigenberger", "Andreas", ""], ["Br\u00e9bant", "Vanessa", ""], ["Prantl", "Lukas", ""], ["Palm", "Christoph", ""]]}, {"id": "2107.13465", "submitter": "Ti Bai", "authors": "Ti Bai, Anjali Balagopal, Michael Dohopolski, Howard E. Morgan, Rafe\n  McBeth, Jun Tan, Mu-Han Lin, David J. Sher, Dan Nguyen, and Steve Jiang", "title": "A Proof-of-Concept Study of Artificial Intelligence Assisted Contour\n  Revision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic segmentation of anatomical structures is critical for many medical\napplications. However, the results are not always clinically acceptable and\nrequire tedious manual revision. Here, we present a novel concept called\nartificial intelligence assisted contour revision (AIACR) and demonstrate its\nfeasibility. The proposed clinical workflow of AIACR is as follows given an\ninitial contour that requires a clinicians revision, the clinician indicates\nwhere a large revision is needed, and a trained deep learning (DL) model takes\nthis input to update the contour. This process repeats until a clinically\nacceptable contour is achieved. The DL model is designed to minimize the\nclinicians input at each iteration and to minimize the number of iterations\nneeded to reach acceptance. In this proof-of-concept study, we demonstrated the\nconcept on 2D axial images of three head-and-neck cancer datasets, with the\nclinicians input at each iteration being one mouse click on the desired\nlocation of the contour segment. The performance of the model is quantified\nwith Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff\nDistance (HD95). The average DSC/HD95 (mm) of the auto-generated initial\ncontours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were\nimproved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks,\nrespectively. Each DL-based contour update requires around 20 ms. We proposed a\nnovel AIACR concept that uses DL models to assist clinicians in revising\ncontours in an efficient and effective way, and we demonstrated its feasibility\nby using 2D axial CT images from three head-and-neck cancer datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:18:29 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bai", "Ti", ""], ["Balagopal", "Anjali", ""], ["Dohopolski", "Michael", ""], ["Morgan", "Howard E.", ""], ["McBeth", "Rafe", ""], ["Tan", "Jun", ""], ["Lin", "Mu-Han", ""], ["Sher", "David J.", ""], ["Nguyen", "Dan", ""], ["Jiang", "Steve", ""]]}, {"id": "2107.13467", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Site Li, Yubin Ge, Pengyi Ye, Jane You, Jun Lu", "title": "Recursively Conditional Gaussian for Ordinal Unsupervised Domain\n  Adaptation", "comments": "Accepted to ICCV 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a growing interest in unsupervised domain adaptation (UDA) to\nalleviate the data scalability issue, while the existing works usually focus on\nclassifying independently discrete labels. However, in many tasks (e.g.,\nmedical diagnosis), the labels are discrete and successively distributed. The\nUDA for ordinal classification requires inducing non-trivial ordinal\ndistribution prior to the latent space. Target for this, the partially ordered\nset (poset) is defined for constraining the latent vector. Instead of the\ntypically i.i.d. Gaussian latent prior, in this work, a recursively conditional\nGaussian (RCG) set is proposed for ordered constraint modeling, which admits a\ntractable joint distribution prior. Furthermore, we are able to control the\ndensity of content vectors that violate the poset constraint by a simple\n\"three-sigma rule\". We explicitly disentangle the cross-domain images into a\nshared ordinal prior induced ordinal content space and two separate\nsource/target ordinal-unrelated spaces, and the self-training is worked on the\nshared space exclusively for ordinal-aware domain alignment. Extensive\nexperiments on UDA medical diagnoses and facial age estimation demonstrate its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:26:46 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Li", "Site", ""], ["Ge", "Yubin", ""], ["Ye", "Pengyi", ""], ["You", "Jane", ""], ["Lu", "Jun", ""]]}, {"id": "2107.13469", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Zhenhua Guo, Site Li, Fangxu Xing, Jane You, C.-C. Jay\n  Kuo, Georges El Fakhri, Jonghye Woo", "title": "Adversarial Unsupervised Domain Adaptation with Conditional and Label\n  Shift: Infer, Align and Iterate", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an adversarial unsupervised domain adaptation (UDA)\napproach with the inherent conditional and label shifts, in which we aim to\nalign the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is\ninaccessible in the target domain, the conventional adversarial UDA assumes\n$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an\nalternative to the $p(x|y)$ alignment. To address this, we provide a thorough\ntheoretical and empirical analysis of the conventional adversarial UDA methods\nunder both conditional and label shifts, and propose a novel and practical\nalternative optimization scheme for adversarial UDA. Specifically, we infer the\nmarginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely\nalign the posterior $p(y|x)$ in testing. Our experimental results demonstrate\nits effectiveness on both classification and segmentation UDA, and partial UDA.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:28:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Guo", "Zhenhua", ""], ["Li", "Site", ""], ["Xing", "Fangxu", ""], ["You", "Jane", ""], ["Kuo", "C. -C. Jay", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2107.13484", "submitter": "Annika Hagemann", "authors": "Annika Hagemann, Moritz Knorr, Holger Janssen, Christoph Stiller", "title": "Inferring bias and uncertainty in camera calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate camera calibration is a precondition for many computer vision\napplications. Calibration errors, such as wrong model assumptions or imprecise\nparameter estimation, can deteriorate a system's overall performance, making\nthe reliable detection and quantification of these errors critical. In this\nwork, we introduce an evaluation scheme to capture the fundamental error\nsources in camera calibration: systematic errors (biases) and uncertainty\n(variance). The proposed bias detection method uncovers smallest systematic\nerrors and thereby reveals imperfections of the calibration setup and provides\nthe basis for camera model selection. A novel resampling-based uncertainty\nestimator enables uncertainty estimation under non-ideal conditions and thereby\nextends the classical covariance estimator. Furthermore, we derive a simple\nuncertainty metric that is independent of the camera model. In combination, the\nproposed methods can be used to assess the accuracy of individual calibrations,\nbut also to benchmark new calibration algorithms, camera models, or calibration\nsetups. We evaluate the proposed methods with simulations and real cameras.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:49:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hagemann", "Annika", ""], ["Knorr", "Moritz", ""], ["Janssen", "Holger", ""], ["Stiller", "Christoph", ""]]}, {"id": "2107.13516", "submitter": "Chengjiang Long", "authors": "Tao Hu, Chengjiang Long, Chunxia Xiao", "title": "CRD-CGAN: Category-Consistent and Relativistic Constraints for Diverse\n  Text-to-Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating photo-realistic images from a text description is a challenging\nproblem in computer vision. Previous works have shown promising performance to\ngenerate synthetic images conditional on text by Generative Adversarial\nNetworks (GANs). In this paper, we focus on the category-consistent and\nrelativistic diverse constraints to optimize the diversity of synthetic images.\nBased on those constraints, a category-consistent and relativistic diverse\nconditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images\nsimultaneously. We use the attention loss and diversity loss to improve the\nsensitivity of the GAN to word attention and noises. Then, we employ the\nrelativistic conditional loss to estimate the probability of relatively real or\nfake for synthetic images, which can improve the performance of basic\nconditional loss. Finally, we introduce a category-consistent loss to alleviate\nthe over-category issues between K synthetic images. We evaluate our approach\nusing the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the\nextensive experiments demonstrate superiority of the proposed method in\ncomparison with state-of-the-art methods in terms of photorealistic and\ndiversity of the generated synthetic images.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:38:33 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hu", "Tao", ""], ["Long", "Chengjiang", ""], ["Xiao", "Chunxia", ""]]}, {"id": "2107.13542", "submitter": "Madeleine Wyburd K", "authors": "Madeleine K. Wyburd, Nicola K. Dinsdale, Ana I.L. Namburete and Mark\n  Jenkinson", "title": "TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee\n  Topology Preservation in Segmentations", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate topology is key when performing meaningful anatomical segmentations,\nhowever, it is often overlooked in traditional deep learning methods. In this\nwork we propose TEDS-Net: a novel segmentation method that guarantees accurate\ntopology. Our method is built upon a continuous diffeomorphic framework, which\nenforces topology preservation. However, in practice, diffeomorphic fields are\nrepresented using a finite number of parameters and sampled using methods such\nas linear interpolation, violating the theoretical guarantees. We therefore\nintroduce additional modifications to more strictly enforce it. Our network\nlearns how to warp a binary prior, with the desired topological\ncharacteristics, to complete the segmentation task. We tested our method on\nmyocardium segmentation from an open-source 2D heart dataset. TEDS-Net\npreserved topology in 100% of the cases, compared to 90% from the U-Net,\nwithout sacrificing on Hausdorff Distance or Dice performance. Code will be\nmade available at: www.github.com/mwyburd/TEDS-Net\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:55:56 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wyburd", "Madeleine K.", ""], ["Dinsdale", "Nicola K.", ""], ["Namburete", "Ana I. L.", ""], ["Jenkinson", "Mark", ""]]}, {"id": "2107.13576", "submitter": "Chirag Raman", "authors": "Chirag Raman, Hayley Hung, Marco Loog", "title": "Social Processes: Self-Supervised Forecasting of Nonverbal Cues in\n  Social Conversations", "comments": "20 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The default paradigm for the forecasting of human behavior in social\nconversations is characterized by top-down approaches. These involve\nidentifying predictive relationships between low level nonverbal cues and\nfuture semantic events of interest (e.g. turn changes, group leaving). A common\nhurdle however, is the limited availability of labeled data for supervised\nlearning. In this work, we take the first step in the direction of a bottom-up\nself-supervised approach in the domain. We formulate the task of Social Cue\nForecasting to leverage the larger amount of unlabeled low-level behavior cues,\nand characterize the modeling challenges involved. To address these, we take a\nmeta-learning approach and propose the Social Process (SP) models--socially\naware sequence-to-sequence (Seq2Seq) models within the Neural Process (NP)\nfamily. SP models learn extractable representations of non-semantic future cues\nfor each participant, while capturing global uncertainty by jointly reasoning\nabout the future for all members of the group. Evaluation on synthesized and\nreal-world behavior data shows that our SP models achieve higher log-likelihood\nthan the NP baselines, and also highlights important considerations for\napplying such techniques within the domain of social human interactions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 18:01:08 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Raman", "Chirag", ""], ["Hung", "Hayley", ""], ["Loog", "Marco", ""]]}, {"id": "2107.13587", "submitter": "Faisal Mahmood", "authors": "Chengkuan Chen, Ming Y. Lu, Drew F. K. Williamson, Tiffany Y. Chen,\n  Andrew J. Schaumberg, Faisal Mahmood", "title": "Fast and Scalable Image Search For Histology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.TO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The expanding adoption of digital pathology has enabled the curation of large\nrepositories of histology whole slide images (WSIs), which contain a wealth of\ninformation. Similar pathology image search offers the opportunity to comb\nthrough large historical repositories of gigapixel WSIs to identify cases with\nsimilar morphological features and can be particularly useful for diagnosing\nrare diseases, identifying similar cases for predicting prognosis, treatment\noutcomes, and potential clinical trial success. A critical challenge in\ndeveloping a WSI search and retrieval system is scalability, which is uniquely\nchallenging given the need to search a growing number of slides that each can\nconsist of billions of pixels and are several gigabytes in size. Such systems\nare typically slow and retrieval speed often scales with the size of the\nrepository they search through, making their clinical adoption tedious and are\nnot feasible for repositories that are constantly growing. Here we present Fast\nImage Search for Histopathology (FISH), a histology image search pipeline that\nis infinitely scalable and achieves constant search speed that is independent\nof the image database size while being interpretable and without requiring\ndetailed annotations. FISH uses self-supervised deep learning to encode\nmeaningful representations from WSIs and a Van Emde Boas tree for fast search,\nfollowed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We\nevaluated FISH on multiple tasks and datasets with over 22,000 patient cases\nspanning 56 disease subtypes. We additionally demonstrate that FISH can be used\nto assist with the diagnosis of rare cancer types where sufficient cases may\nnot be available to train traditional supervised deep models. FISH is available\nas an easy-to-use, open-source software package\n(https://github.com/mahmoodlab/FISH).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 18:15:03 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Chengkuan", ""], ["Lu", "Ming Y.", ""], ["Williamson", "Drew F. K.", ""], ["Chen", "Tiffany Y.", ""], ["Schaumberg", "Andrew J.", ""], ["Mahmood", "Faisal", ""]]}, {"id": "2107.13625", "submitter": "William Paul", "authors": "William Paul, Philippe Burlina", "title": "Generalizing Fairness: Discovery and Mitigation of Unknown Sensitive\n  Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deploying artificial intelligence (AI) in the real world, being able to\ntrust the operation of the AI by characterizing how it performs is an\never-present and important topic. An important and still largely unexplored\ntask in this characterization is determining major factors within the real\nworld that affect the AI's behavior, such as weather conditions or lighting,\nand either a) being able to give justification for why it may have failed or b)\neliminating the influence the factor has. Determining these sensitive factors\nheavily relies on collected data that is diverse enough to cover numerous\ncombinations of these factors, which becomes more onerous when having many\npotential sensitive factors or operating in complex environments. This paper\ninvestigates methods that discover and separate out individual semantic\nsensitive factors from a given dataset to conduct this characterization as well\nas addressing mitigation of these factors' sensitivity. We also broaden\nremediation of fairness, which normally only addresses socially relevant\nfactors, and widen it to deal with the desensitization of AI with regard to all\npossible aspects of variation in the domain. The proposed methods which\ndiscover these major factors reduce the potentially onerous demands of\ncollecting a sufficiently diverse dataset. In experiments using the road sign\n(GTSRB) and facial imagery (CelebA) datasets, we show the promise of using this\nscheme to perform this characterization and remediation and demonstrate that\nour approach outperforms state of the art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:18:08 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Paul", "William", ""], ["Burlina", "Philippe", ""]]}, {"id": "2107.13627", "submitter": "Sindi Shkodrani", "authors": "Sindi Shkodrani, Yu Wang, Marco Manfredi, N\\'ora Baka", "title": "United We Learn Better: Harvesting Learning Improvements From Class\n  Hierarchies Across Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts of learning from hierarchical taxonomies in computer vision have\nbeen mostly focusing on image classification. Though ways of best harvesting\nlearning improvements from hierarchies in classification are far from being\nsolved, there is a need to target these problems in other vision tasks such as\nobject detection. As progress on the classification side is often dependent on\nhierarchical cross-entropy losses, novel detection architectures using sigmoid\nas an output function instead of softmax cannot easily apply these advances,\nrequiring novel methods in detection. In this work we establish a theoretical\nframework based on probability and set theory for extracting parent predictions\nand a hierarchical loss that can be used across tasks, showing results across\nclassification and detection benchmarks and opening up the possibility of\nhierarchical learning for sigmoid-based detection architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:25:37 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shkodrani", "Sindi", ""], ["Wang", "Yu", ""], ["Manfredi", "Marco", ""], ["Baka", "N\u00f3ra", ""]]}, {"id": "2107.13628", "submitter": "Tomasz {\\L}uczy\\'nski", "authors": "Tomasz Luczynski, Jonatan Scharff Willners, Elizabeth Vargas, Joshua\n  Roe, Shida Xu, Yu Cao, Yvan Petillot and Sen Wang", "title": "Underwater inspection and intervention dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a novel dataset for the development of visual navigation\nand simultaneous localisation and mapping (SLAM) algorithms as well as for\nunderwater intervention tasks. It differs from existing datasets as it contains\nground truth for the vehicle's position captured by an underwater motion\ntracking system. The dataset contains distortion-free and rectified stereo\nimages along with the calibration parameters of the stereo camera setup.\nFurthermore, the experiments were performed and recorded in a controlled\nenvironment, where current and waves could be generated allowing the dataset to\ncover a wide range of conditions - from calm water to waves and currents of\nsignificant strength.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:29:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Luczynski", "Tomasz", ""], ["Willners", "Jonatan Scharff", ""], ["Vargas", "Elizabeth", ""], ["Roe", "Joshua", ""], ["Xu", "Shida", ""], ["Cao", "Yu", ""], ["Petillot", "Yvan", ""], ["Wang", "Sen", ""]]}, {"id": "2107.13629", "submitter": "Chun-Han Yao", "authors": "Chun-Han Yao, Wei-Chih Hung, Varun Jampani, Ming-Hsuan Yang", "title": "Discovering 3D Parts from Image Collections", "comments": "Accepted by ICCV 2021. Project page: https://chhankyao.github.io/lpd/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning 3D shapes from 2D images is an essential yet challenging task,\nespecially when only single-view images are at our disposal. While an object\ncan have a complicated shape, individual parts are usually close to geometric\nprimitives and thus are easier to model. Furthermore, parts provide a mid-level\nrepresentation that is robust to appearance variations across objects in a\nparticular category. In this work, we tackle the problem of 3D part discovery\nfrom only 2D image collections. Instead of relying on manually annotated parts\nfor supervision, we propose a self-supervised approach, latent part discovery\n(LPD). Our key insight is to learn a novel part shape prior that allows each\npart to fit an object shape faithfully while constrained to have simple\ngeometry. Extensive experiments on the synthetic ShapeNet, PartNet, and\nreal-world Pascal 3D+ datasets show that our method discovers consistent object\nparts and achieves favorable reconstruction accuracy compared to the existing\nmethods with the same level of supervision.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:29:16 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yao", "Chun-Han", ""], ["Hung", "Wei-Chih", ""], ["Jampani", "Varun", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2107.13637", "submitter": "Peter Van Der Putten", "authors": "Manolis Fragkiadakis and Peter van der Putten", "title": "Sign and Search: Sign Search Functionality for Sign Language Lexica", "comments": "Accepted for the 1st International Workshop on Automatic Translation\n  for Signed and Spoken Languages (ATS4SSL), August 20, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language lexica are a useful resource for researchers and people\nlearning sign languages. Current implementations allow a user to search a sign\neither by its gloss or by selecting its primary features such as handshape and\nlocation. This study focuses on exploring a reverse search functionality where\na user can sign a query sign in front of a webcam and retrieve a set of\nmatching signs. By extracting different body joints combinations (upper body,\ndominant hand's arm and wrist) using the pose estimation framework OpenPose, we\ncompare four techniques (PCA, UMAP, DTW and Euclidean distance) as distance\nmetrics between 20 query signs, each performed by eight participants on a 1200\nsign lexicon. The results show that UMAP and DTW can predict a matching sign\nwith an 80\\% and 71\\% accuracy respectively at the top-20 retrieved signs using\nthe movement of the dominant hand arm. Using DTW and adding more sign instances\nfrom other participants in the lexicon, the accuracy can be raised to 90\\% at\nthe top-10 ranking. Our results suggest that our methodology can be used with\nno training in any sign language lexicon regardless of its size.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 20:48:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fragkiadakis", "Manolis", ""], ["van der Putten", "Peter", ""]]}, {"id": "2107.13643", "submitter": "Ahmed Elhagry", "authors": "Ahmed Elhagry, Mohamed Saeed, Musie Araia", "title": "Lighter Stacked Hourglass Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human pose estimation (HPE) is one of the most challenging tasks in computer\nvision as humans are deformable by nature and thus their pose has so much\nvariance. HPE aims to correctly identify the main joint locations of a single\nperson or multiple people in a given image or video. Locating joints of a\nperson in images or videos is an important task that can be applied in action\nrecognition and object tracking. As have many computer vision tasks, HPE has\nadvanced massively with the introduction of deep learning to the field. In this\npaper, we focus on one of the deep learning-based approaches of HPE proposed by\nNewell et al., which they named the stacked hourglass network. Their approach\nis widely used in many applications and is regarded as one of the best works in\nthis area. The main focus of their approach is to capture as much information\nas it can at all possible scales so that a coherent understanding of the local\nfeatures and full-body location is achieved. Their findings demonstrate that\nimportant cues such as orientation of a person, arrangement of limbs, and\nadjacent joints' relative location can be identified from multiple scales at\ndifferent resolutions. To do so, they makes use of a single pipeline to process\nimages in multiple resolutions, which comprises a skip layer to not lose\nspatial information at each resolution. The resolution of the images stretches\nas lower as 4x4 to make sure that a smaller spatial feature is included. In\nthis study, we study the effect of architectural modifications on the\ncomputational speed and accuracy of the network.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 21:05:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Elhagry", "Ahmed", ""], ["Saeed", "Mohamed", ""], ["Araia", "Musie", ""]]}, {"id": "2107.13647", "submitter": "Ahmed Elhagry", "authors": "Ahmed Elhagry, Rawan Gla", "title": "Egyptian Sign Language Recognition Using CNN and LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language is a set of gestures that deaf people use to communicate.\nUnfortunately, normal people don't understand it, which creates a communication\ngap that needs to be filled. Because of the variations in (Egyptian Sign\nLanguage) ESL from one region to another, ESL provides a challenging research\nproblem. In this work, we are providing applied research with its video-based\nEgyptian sign language recognition system that serves the local community of\ndeaf people in Egypt, with a moderate and reasonable accuracy. We present a\ncomputer vision system with two different neural networks architectures. The\nfirst is a Convolutional Neural Network (CNN) for extracting spatial features.\nThe CNN model was retrained on the inception mod. The second architecture is a\nCNN followed by a Long Short-Term Memory (LSTM) for extracting both spatial and\ntemporal features. The two models achieved an accuracy of 90% and 72%,\nrespectively. We examined the power of these two architectures to distinguish\nbetween 9 common words (with similar signs) among some deaf people community in\nEgypt.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 21:33:35 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Elhagry", "Ahmed", ""], ["Gla", "Rawan", ""]]}, {"id": "2107.13648", "submitter": "Peter Van Der Putten", "authors": "Michail Tsiaousis, Gertjan Burghouts, Fieke Hillerstr\\\"om and Peter\n  van der Putten", "title": "Spot What Matters: Learning Context Using Graph Convolutional Networks\n  for Weakly-Supervised Action Detection", "comments": "Paper presented at the International Workshop on Deep Learning for\n  Human-Centric Activity Understanding (DL-HAU2020), January 11, 2021", "journal-ref": "International Workshop on Deep Learning for Human-Centric Activity\n  Understanding (DL-HAU2020), January 11, 2021", "doi": "10.1007/978-3-030-68799-1_9", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dominant paradigm in spatiotemporal action detection is to classify\nactions using spatiotemporal features learned by 2D or 3D Convolutional\nNetworks. We argue that several actions are characterized by their context,\nsuch as relevant objects and actors present in the video. To this end, we\nintroduce an architecture based on self-attention and Graph Convolutional\nNetworks in order to model contextual cues, such as actor-actor and\nactor-object interactions, to improve human action detection in video. We are\ninterested in achieving this in a weakly-supervised setting, i.e. using as less\nannotations as possible in terms of action bounding boxes. Our model aids\nexplainability by visualizing the learned context as an attention map, even for\nactions and objects unseen during training. We evaluate how well our model\nhighlights the relevant context by introducing a quantitative metric based on\nrecall of objects retrieved by attention maps. Our model relies on a 3D\nconvolutional RGB stream, and does not require expensive optical flow\ncomputation. We evaluate our models on the DALY dataset, which consists of\nhuman-object interaction actions. Experimental results show that our\ncontextualized approach outperforms a baseline action detection approach by\nmore than 2 points in Video-mAP. Code is available at\n\\url{https://github.com/micts/acgcn}\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 21:37:18 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Tsiaousis", "Michail", ""], ["Burghouts", "Gertjan", ""], ["Hillerstr\u00f6m", "Fieke", ""], ["van der Putten", "Peter", ""]]}, {"id": "2107.13651", "submitter": "Marcin Iwanowski", "authors": "Marcin Iwanowski and Marcin Grzabka", "title": "Similarity and symmetry measures based on fuzzy descriptors of image\n  objects` composition", "comments": "10 pages", "journal-ref": "WSCG 2021 29. International Conference in Central Europe on\n  Computer Graphics, Visualization and Computer Vision", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The paper describes a method for measuring the similarity and symmetry of an\nimage annotated with bounding boxes indicating image objects. The latter\nrepresentation became popular recently due to the rapid development of fast and\nefficient deep-learning-based object-detection methods. The proposed approach\nallows for comparing sets of bounding boxes to estimate the degree of\nsimilarity of their underlying images. It is based on the fuzzy approach that\nuses the fuzzy mutual position (FMP) matrix to describe spatial composition and\nrelations between bounding boxes within an image. A method of computing the\nsimilarity of two images described by their FMP matrices is proposed and the\nalgorithm of its computation. It outputs the single scalar value describing the\ndegree of content-based image similarity. By modifying the method`s parameters,\ninstead of similarity, the reflectional symmetry of object composition may also\nbe measured. The proposed approach allows for measuring differences in objects`\ncomposition of various intensities. It is also invariant to translation and\nscaling and - in case of symmetry detection - position and orientation of the\nsymmetry axis. A couple of examples illustrate the method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 21:42:19 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Iwanowski", "Marcin", ""], ["Grzabka", "Marcin", ""]]}, {"id": "2107.13682", "submitter": "John Willes", "authors": "John Willes, James Harrison, Ali Harakeh, Chelsea Finn, Marco Pavone,\n  Steven Waslander", "title": "Bayesian Embeddings for Few-Shot Open World Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous decision-making agents move from narrow operating environments\nto unstructured worlds, learning systems must move from a closed-world\nformulation to an open-world and few-shot setting in which agents continuously\nlearn new classes from small amounts of information. This stands in stark\ncontrast to modern machine learning systems that are typically designed with a\nknown set of classes and a large number of examples for each class. In this\nwork we extend embedding-based few-shot learning algorithms to the open-world\nrecognition setting. We combine Bayesian non-parametric class priors with an\nembedding-based pre-training scheme to yield a highly flexible framework which\nwe refer to as few-shot learning for open world recognition (FLOWR). We\nbenchmark our framework on open-world extensions of the common MiniImageNet and\nTieredImageNet few-shot learning datasets. Our results show, compared to prior\nmethods, strong classification accuracy performance and up to a 12% improvement\nin H-measure (a measure of novel class detection) from our non-parametric\nopen-world few-shot learning scheme.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 00:38:47 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Willes", "John", ""], ["Harrison", "James", ""], ["Harakeh", "Ali", ""], ["Finn", "Chelsea", ""], ["Pavone", "Marco", ""], ["Waslander", "Steven", ""]]}, {"id": "2107.13693", "submitter": "Yaohai Zhou", "authors": "Zhiyuan Ren, Yaohai Zhou, Yizhe Chen, Ruisong Zhou, Yayu Gao", "title": "Efficient Human Pose Estimation by Maximizing Fusion and High-Level\n  Spatial Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient human pose estimation network -- SFM\n(slender fusion model) by fusing multi-level features and adding lightweight\nattention blocks -- HSA (High-Level Spatial Attention). Many existing methods\non efficient network have already taken feature fusion into consideration,\nwhich largely boosts the performance. However, its performance is far inferior\nto large network such as ResNet and HRNet due to its limited fusion operation\nin the network. Specifically, we expand the number of fusion operation by\nbuilding bridges between two pyramid frameworks without adding layers.\nMeanwhile, to capture long-range dependency, we propose a lightweight attention\nblock -- HSA, which computes second-order attention map. In summary, SFM\nmaximizes the number of feature fusion in a limited number of layers. HSA\nlearns high precise spatial information by computing the attention of spatial\nattention map. With the help of SFM and HSA, our network is able to generate\nmulti-level feature and extract precise global spatial information with little\ncomputing resource. Thus, our method achieve comparable or even better accuracy\nwith less parameters and computational cost. Our SFM achieve 89.0 in PCKh@0.5,\n42.0 in PCKh@0.1 on MPII validation set and 71.7 in AP, 90.7 in AP@0.5 on COCO\nvalidation with only 1.7G FLOPs and 1.5M parameters. The source code will be\npublic soon.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 00:55:17 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ren", "Zhiyuan", ""], ["Zhou", "Yaohai", ""], ["Chen", "Yizhe", ""], ["Zhou", "Ruisong", ""], ["Gao", "Yayu", ""]]}, {"id": "2107.13703", "submitter": "Mehdi Afshari", "authors": "Mehdi Afshari, H.R. Tizhoosh", "title": "A Similarity Measure of Histopathology Images by Deep Embeddings", "comments": "4 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Histopathology digital scans are large-size images that contain valuable\ninformation at the pixel level. Content-based comparison of these images is a\nchallenging task. This study proposes a content-based similarity measure for\nhigh-resolution gigapixel histopathology images. The proposed similarity\nmeasure is an expansion of cosine vector similarity to a matrix. Each image is\ndivided into same-size patches with a meaningful amount of information (i.e.,\ncontained enough tissue). The similarity is measured by the extraction of\npatch-level deep embeddings of the last pooling layer of a pre-trained deep\nmodel at four different magnification levels, namely, 1x, 2.5x, 5x, and 10x\nmagnifications. In addition, for faster measurement, embedding reduction is\ninvestigated. Finally, to assess the proposed method, an image search method is\nimplemented. Results show that the similarity measure represents the slide\nlabels with a maximum accuracy of 93.18\\% for top-5 search at 5x magnification.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 01:45:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Afshari", "Mehdi", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2107.13706", "submitter": "Luchuan Song", "authors": "Luchuan Song, Bin Liu, Huihui Zhu, Qi Chu, Nenghai Yu", "title": "Abnormal Behavior Detection Based on Target Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abnormal behavior detection in surveillance video is a pivotal part of the\nintelligent city. Most existing methods only consider how to detect anomalies,\nwith less considering to explain the reason of the anomalies. We investigate an\northogonal perspective based on the reason of these abnormal behaviors. To this\nend, we propose a multivariate fusion method that analyzes each target through\nthree branches: object, action and motion. The object branch focuses on the\nappearance information, the motion branch focuses on the distribution of the\nmotion features, and the action branch focuses on the action category of the\ntarget. The information that these branches focus on is different, and they can\ncomplement each other and jointly detect abnormal behavior. The final abnormal\nscore can then be obtained by combining the abnormal scores of the three\nbranches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 02:03:47 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Song", "Luchuan", ""], ["Liu", "Bin", ""], ["Zhu", "Huihui", ""], ["Chu", "Qi", ""], ["Yu", "Nenghai", ""]]}, {"id": "2107.13715", "submitter": "Chuanguang Yang", "authors": "Chuanguang Yang, Zhulin An, Linhang Cai, Yongjun Xu", "title": "Hierarchical Self-supervised Augmented Knowledge Distillation", "comments": "7 pages, IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation often involves how to define and transfer knowledge\nfrom teacher to student effectively. Although recent self-supervised\ncontrastive knowledge achieves the best performance, forcing the network to\nlearn such knowledge may damage the representation learning of the original\nclass recognition task. We therefore adopt an alternative self-supervised\naugmented task to guide the network to learn the joint distribution of the\noriginal recognition task and self-supervised auxiliary task. It is\ndemonstrated as a richer knowledge to improve the representation power without\nlosing the normal classification capability. Moreover, it is incomplete that\nprevious methods only transfer the probabilistic knowledge between the final\nlayers. We propose to append several auxiliary classifiers to hierarchical\nintermediate feature maps to generate diverse self-supervised knowledge and\nperform the one-to-one transfer to teach the student network thoroughly. Our\nmethod significantly surpasses the previous SOTA SSKD with an average\nimprovement of 2.56\\% on CIFAR-100 and an improvement of 0.77\\% on ImageNet\nacross widely used network pairs. Codes are available at\nhttps://github.com/winycg/HSAKD.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 02:57:21 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yang", "Chuanguang", ""], ["An", "Zhulin", ""], ["Cai", "Linhang", ""], ["Xu", "Yongjun", ""]]}, {"id": "2107.13718", "submitter": "Luchuan Song", "authors": "Kun Zhao, Luchuan Song, Bin Liu, Qi Chu, Nenghai Yu", "title": "Cascaded Residual Density Network for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting is a challenging task due to the issues such as scale\nvariation and perspective variation in real crowd scenes. In this paper, we\npropose a novel Cascaded Residual Density Network (CRDNet) in a coarse-to-fine\napproach to generate the high-quality density map for crowd counting more\naccurately. (1) We estimate the residual density maps by multi-scale pyramidal\nfeatures through cascaded residual density modules. It can improve the quality\nof density map layer by layer effectively. (2) A novel additional local count\nloss is presented to refine the accuracy of crowd counting, which reduces the\nerrors of pixel-wise Euclidean loss by restricting the number of people in the\nlocal crowd areas. Experiments on two public benchmark datasets show that the\nproposed method achieves effective improvement compared with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 03:07:11 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhao", "Kun", ""], ["Song", "Luchuan", ""], ["Liu", "Bin", ""], ["Chu", "Qi", ""], ["Yu", "Nenghai", ""]]}, {"id": "2107.13720", "submitter": "Xinyang Feng", "authors": "Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao\n  Ni, Haifeng Chen", "title": "Convolutional Transformer based Dual Discriminator Generative\n  Adversarial Networks for Video Anomaly Detection", "comments": "Accepted for publication in the 29th ACM International Conference on\n  Multimedia (ACMMM '21)", "journal-ref": null, "doi": "10.1145/3474085.3475693", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 03:07:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Feng", "Xinyang", ""], ["Song", "Dongjin", ""], ["Chen", "Yuncong", ""], ["Chen", "Zhengzhang", ""], ["Ni", "Jingchao", ""], ["Chen", "Haifeng", ""]]}, {"id": "2107.13731", "submitter": "Xiaoxue Zang", "authors": "Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav\n  Rastogi, Jindong Chen, Blaise Aguera y Arcas", "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding", "comments": "8 pages, IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 03:51:36 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Bai", "Chongyang", ""], ["Zang", "Xiaoxue", ""], ["Xu", "Ying", ""], ["Sunkara", "Srinivas", ""], ["Rastogi", "Abhinav", ""], ["Chen", "Jindong", ""], ["Arcas", "Blaise Aguera y", ""]]}, {"id": "2107.13741", "submitter": "Jizong Peng", "authors": "Jizong Peng, Ping Wang, Chrisitian Desrosiers, Marco Pedersoli", "title": "Self-Paced Contrastive Learning for Semi-supervisedMedical Image\n  Segmentation with Meta-labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-training a recognition model with contrastive learning on a large dataset\nof unlabeled data has shown great potential to boost the performance of a\ndownstream task, e.g., image classification. However, in domains such as\nmedical imaging, collecting unlabeled data can be challenging and expensive. In\nthis work, we propose to adapt contrastive learning to work with meta-label\nannotations, for improving the model's performance in medical image\nsegmentation even when no additional unlabeled data is available. Meta-labels\nsuch as the location of a 2D slice in a 3D MRI scan or the type of device used,\noften come for free during the acquisition process. We use the meta-labels for\npre-training the image encoder as well as to regularize a semi-supervised\ntraining, in which a reduced set of annotated data is used for training.\nFinally, to fully exploit the weak annotations, a self-paced learning approach\nis used to help the learning and discriminate useful labels from noise. Results\non three different medical image segmentation datasets show that our approach:\ni) highly boosts the performance of a model trained on a few scans, ii)\noutperforms previous contrastive and semi-supervised approaches, and iii)\nreaches close to the performance of a model trained on the full data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 04:30:46 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Peng", "Jizong", ""], ["Wang", "Ping", ""], ["Desrosiers", "Chrisitian", ""], ["Pedersoli", "Marco", ""]]}, {"id": "2107.13742", "submitter": "Veeru Talreja", "authors": "Fariborz Taherkhani, Veeru Talreja, Jeremy Dawson, Matthew C. Valenti,\n  and Nasser M. Nasrabadi", "title": "Profile to Frontal Face Recognition in the Wild Using Coupled\n  Conditional GAN", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.02166", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, with the advent of deep-learning, face recognition has\nachieved exceptional success. However, many of these deep face recognition\nmodels perform much better in handling frontal faces compared to profile faces.\nThe major reason for poor performance in handling of profile faces is that it\nis inherently difficult to learn pose-invariant deep representations that are\nuseful for profile face recognition. In this paper, we hypothesize that the\nprofile face domain possesses a latent connection with the frontal face domain\nin a latent feature subspace. We look to exploit this latent connection by\nprojecting the profile faces and frontal faces into a common latent subspace\nand perform verification or retrieval in the latent domain. We leverage a\ncoupled conditional generative adversarial network (cpGAN) structure to find\nthe hidden relationship between the profile and frontal images in a latent\ncommon embedding subspace. Specifically, the cpGAN framework consists of two\nconditional GAN-based sub-networks, one dedicated to the frontal domain and the\nother dedicated to the profile domain. Each sub-network tends to find a\nprojection that maximizes the pair-wise correlation between the two feature\ndomains in a common embedding feature subspace. The efficacy of our approach\ncompared with the state-of-the-art is demonstrated using the CFP, CMU\nMulti-PIE, IJB-A, and IJB-C datasets. Additionally, we have also implemented a\ncoupled convolutional neural network (cpCNN) and an adversarial discriminative\ndomain adaptation network (ADDA) for profile to frontal face recognition. We\nhave evaluated the performance of cpCNN and ADDA and compared it with the\nproposed cpGAN. Finally, we have also evaluated our cpGAN for reconstruction of\nfrontal faces from input profile faces contained in the VGGFace2 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 04:33:43 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Talreja", "Veeru", ""], ["Dawson", "Jeremy", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2107.13757", "submitter": "Jiali Duan", "authors": "Jiali Duan, C.-C. Jay Kuo", "title": "Bridging Gap between Image Pixels and Semantics via Supervision: A\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fact that there exists a gap between low-level features and semantic\nmeanings of images, called the semantic gap, is known for decades. Resolution\nof the semantic gap is a long standing problem. The semantic gap problem is\nreviewed and a survey on recent efforts in bridging the gap is made in this\nwork. Most importantly, we claim that the semantic gap is primarily bridged\nthrough supervised learning today. Experiences are drawn from two application\ndomains to illustrate this point: 1) object detection and 2) metric learning\nfor content-based image retrieval (CBIR). To begin with, this paper offers a\nhistorical retrospective on supervision, makes a gradual transition to the\nmodern data-driven methodology and introduces commonly used datasets. Then, it\nsummarizes various supervision methods to bridge the semantic gap in the\ncontext of object detection and metric learning.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 05:55:40 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Duan", "Jiali", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2107.13760", "submitter": "Yu Cheng Hsu Mr", "authors": "Yu Cheng Hsu, Qingpeng Zhang, Efstratios Tsougenis, Kwok-Leung Tsui", "title": "Viewpoint-Invariant Exercise Repetition Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counting the repetition of human exercise and physical rehabilitation is a\ncommon task in rehabilitation and exercise training. The existing vision-based\nrepetition counting methods less emphasize the concurrent motions in the same\nvideo. This work presents a vision-based human motion repetition counting\napplicable to counting concurrent motions through the skeleton location\nextracted from various pose estimation methods. The presented method was\nvalidated on the University of Idaho Physical Rehabilitation Movements Data Set\n(UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit\nwas 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset\nwas 0.06 with OBOA 0.95. We have also tested the performance in a variety of\ncamera locations and concurrent motions with conveniently collected video with\noverall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and\nmotion agnostic concurrent motion counting. This method can potentially use in\nlarge-scale remote rehabilitation and exercise training with only one camera.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 06:00:52 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Hsu", "Yu Cheng", ""], ["Zhang", "Qingpeng", ""], ["Tsougenis", "Efstratios", ""], ["Tsui", "Kwok-Leung", ""]]}, {"id": "2107.13766", "submitter": "Amir Mazaheri", "authors": "Amir Mazaheri, Mubarak Shah", "title": "Video Generation from Text Employing Latent Path Construction for\n  Temporal Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video generation is one of the most challenging tasks in Machine Learning and\nComputer Vision fields of study. In this paper, we tackle the text to video\ngeneration problem, which is a conditional form of video generation. Humans can\nlisten/read natural language sentences, and can imagine or visualize what is\nbeing described; therefore, we believe that video generation from natural\nlanguage sentences will have an important impact on Artificial Intelligence.\nVideo generation is relatively a new field of study in Computer Vision, which\nis far from being solved. The majority of recent works deal with synthetic\ndatasets or real datasets with very limited types of objects, scenes, and\nemotions. To the best of our knowledge, this is the very first work on the text\n(free-form sentences) to video generation on more realistic video datasets like\nActor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of\nvideo generation by regressing the latent representations of the first and last\nframes and employing a context-aware interpolation method to build the latent\nrepresentations of in-between frames. We propose a stacking ``upPooling'' block\nto sequentially generate RGB frames out of each latent representation and\nprogressively increase the resolution. Moreover, our proposed Discriminator\nencodes videos based on single and multiple frames. We provide quantitative and\nqualitative results to support our arguments and show the superiority of our\nmethod over well-known baselines like Recurrent Neural Network (RNN) and\nDeconvolution (as known as Convolutional Transpose) based video generation\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 06:28:20 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Mazaheri", "Amir", ""], ["Shah", "Mubarak", ""]]}, {"id": "2107.13774", "submitter": "Yan Lu", "authors": "Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie\n  Yan and Wanli Ouyang", "title": "Geometry Uncertainty Projection Network for Monocular 3D Object\n  Detection", "comments": "To appear at ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometry Projection is a powerful depth estimation method in monocular 3D\nobject detection. It estimates depth dependent on heights, which introduces\nmathematical priors into the deep model. But projection process also introduces\nthe error amplification problem, in which the error of the estimated height\nwill be amplified and reflected greatly at the output depth. This property\nleads to uncontrollable depth inferences and also damages the training\nefficiency. In this paper, we propose a Geometry Uncertainty Projection Network\n(GUP Net) to tackle the error amplification problem at both inference and\ntraining stages. Specifically, a GUP module is proposed to obtains the\ngeometry-guided uncertainty of the inferred depth, which not only provides high\nreliable confidence for each depth but also benefits depth learning.\nFurthermore, at the training stage, we propose a Hierarchical Task Learning\nstrategy to reduce the instability caused by error amplification. This learning\nalgorithm monitors the learning situation of each task by a proposed indicator\nand adaptively assigns the proper loss weights for different tasks according to\ntheir pre-tasks situation. Based on that, each task starts learning only when\nits pre-tasks are learned well, which can significantly improve the stability\nand efficiency of the training process. Extensive experiments demonstrate the\neffectiveness of the proposed method. The overall model can infer more reliable\nobject depth than existing methods and outperforms the state-of-the-art\nimage-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and\npedestrian categories on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 06:59:07 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Lu", "Yan", ""], ["Ma", "Xinzhu", ""], ["Yang", "Lei", ""], ["Zhang", "Tianzhu", ""], ["Liu", "Yating", ""], ["Chu", "Qi", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2107.13780", "submitter": "Yunfei Liu", "authors": "Yunfei Liu, Ruicong Liu, Haofei Wang, Feng Lu", "title": "Generalizing Gaze Estimation with Outlier-guided Collaborative\n  Adaptation", "comments": "This paper is accepted by ICCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have significantly improved appearance-based gaze\nestimation accuracy. However, it still suffers from unsatisfactory performance\nwhen generalizing the trained model to new domains, e.g., unseen environments\nor persons. In this paper, we propose a plug-and-play gaze adaptation framework\n(PnP-GA), which is an ensemble of networks that learn collaboratively with the\nguidance of outliers. Since our proposed framework does not require\nground-truth labels in the target domain, the existing gaze estimation networks\ncan be directly plugged into PnP-GA and generalize the algorithms to new\ndomains. We test PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII,\nETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental\nresults demonstrate that the PnP-GA framework achieves considerable performance\nimprovements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The\nproposed framework also outperforms the state-of-the-art domain adaptation\napproaches on gaze domain adaptation tasks. Code has been released at\nhttps://github.com/DreamtaleCore/PnP-GA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 07:23:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liu", "Yunfei", ""], ["Liu", "Ruicong", ""], ["Wang", "Haofei", ""], ["Lu", "Feng", ""]]}, {"id": "2107.13788", "submitter": "Tom Wehrbein", "authors": "Tom Wehrbein, Marco Rudolph, Bodo Rosenhahn, Bastian Wandt", "title": "Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D human pose estimation from monocular images is a highly ill-posed problem\ndue to depth ambiguities and occlusions. Nonetheless, most existing works\nignore these ambiguities and only estimate a single solution. In contrast, we\ngenerate a diverse set of hypotheses that represents the full posterior\ndistribution of feasible 3D poses. To this end, we propose a normalizing flow\nbased method that exploits the deterministic 3D-to-2D mapping to solve the\nambiguous inverse 2D-to-3D problem. Additionally, uncertain detections and\nocclusions are effectively modeled by incorporating uncertainty information of\nthe 2D detector as condition. Further keys to success are a learned 3D pose\nprior and a generalization of the best-of-M loss. We evaluate our approach on\nthe two benchmark datasets Human3.6M and MPI-INF-3DHP, outperforming all\ncomparable methods in most metrics. The implementation is available on GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 07:33:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wehrbein", "Tom", ""], ["Rudolph", "Marco", ""], ["Rosenhahn", "Bodo", ""], ["Wandt", "Bastian", ""]]}, {"id": "2107.13800", "submitter": "Tianxiao Gao", "authors": "Tianxiao Gao, Wu Wei, Zhongbin Cai, Zhun Fan, Shane Xie, Xinmei Wang,\n  Qiuda Yu", "title": "CI-Net: Contextual Information for Joint Semantic Segmentation and Depth\n  Estimation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monocular depth estimation and semantic segmentation are two fundamental\ngoals of scene understanding. Due to the advantages of task interaction, many\nworks study the joint task learning algorithm. However, most existing methods\nfail to fully leverage the semantic labels, ignoring the provided context\nstructures and only using them to supervise the prediction of segmentation\nsplit. In this paper, we propose a network injected with contextual information\n(CI-Net) to solve the problem. Specifically, we introduce self-attention block\nin the encoder to generate attention map. With supervision from the ground\ntruth created by semantic labels, the network is embedded with contextual\ninformation so that it could understand the scene better, utilizing dependent\nfeatures to make accurate prediction. Besides, a feature sharing module is\nconstructed to make the task-specific features deeply fused and a consistency\nloss is devised to make the features mutually guided. We evaluate the proposed\nCI-Net on the NYU-Depth-v2 and SUN-RGBD datasets. The experimental results\nvalidate that our proposed CI-Net is competitive with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 07:58:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Gao", "Tianxiao", ""], ["Wei", "Wu", ""], ["Cai", "Zhongbin", ""], ["Fan", "Zhun", ""], ["Xie", "Shane", ""], ["Wang", "Xinmei", ""], ["Yu", "Qiuda", ""]]}, {"id": "2107.13802", "submitter": "Zq Yan", "authors": "Zhiqiang Yan and Kun Wang and Xiang Li and Zhenyu Zhang and Baobei Xu\n  and Jun Li and Jian Yang", "title": "RigNet: Repetitive Image Guided Network for Depth Completion", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion deals with the problem of recovering dense depth maps from\nsparse ones, where color images are often used to facilitate this completion.\nRecent approaches mainly focus on image guided learning to predict dense\nresults. However, blurry image guidance and object structures in depth still\nimpede the performance of image guided frameworks. To tackle these problems, we\nexplore a repetitive design in our image guided network to sufficiently and\ngradually recover depth values. Specifically, the repetition is embodied in a\ncolor image guidance branch and a depth generation branch. In the former\nbranch, we design a repetitive hourglass network to extract higher-level image\nfeatures of complex environments, which can provide powerful context guidance\nfor depth prediction. In the latter branch, we design a repetitive guidance\nmodule based on dynamic convolution where the convolution factorization is\napplied to simultaneously reduce its complexity and progressively model\nhigh-frequency structures, e.g., boundaries. Further, in this module, we\npropose an adaptive fusion mechanism to effectively aggregate multi-step depth\nfeatures. Extensive experiments show that our method achieves state-of-the-art\nresult on the NYUv2 dataset and ranks 1st on the KITTI benchmark at the time of\nsubmission.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:00:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yan", "Zhiqiang", ""], ["Wang", "Kun", ""], ["Li", "Xiang", ""], ["Zhang", "Zhenyu", ""], ["Xu", "Baobei", ""], ["Li", "Jun", ""], ["Yang", "Jian", ""]]}, {"id": "2107.13807", "submitter": "Shiming Chen", "authors": "Shiming Chen, Wenjie Wang, Beihao Xia, Qinmu Peng, Xinge You, Feng\n  Zheng, Ling Shao", "title": "FREE: Feature Refinement for Generalized Zero-Shot Learning", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) has achieved significant progress, with\nmany efforts dedicated to overcoming the problems of visual-semantic domain gap\nand seen-unseen bias. However, most existing methods directly use feature\nextraction models trained on ImageNet alone, ignoring the cross-dataset bias\nbetween ImageNet and GZSL benchmarks. Such a bias inevitably results in\npoor-quality visual features for GZSL tasks, which potentially limits the\nrecognition performance on both seen and unseen classes. In this paper, we\npropose a simple yet effective GZSL method, termed feature refinement for\ngeneralized zero-shot learning (FREE), to tackle the above problem. FREE\nemploys a feature refinement (FR) module that incorporates\n\\textit{semantic$\\rightarrow$visual} mapping into a unified generative model to\nrefine the visual features of seen and unseen class samples. Furthermore, we\npropose a self-adaptive margin center loss (SAMC-loss) that cooperates with a\nsemantic cycle-consistency loss to guide FR to learn class- and\nsemantically-relevant representations, and concatenate the features in FR to\nextract the fully refined features. Extensive experiments on five benchmark\ndatasets demonstrate the significant performance gain of FREE over its baseline\nand current state-of-the-art methods. Our codes are available at\nhttps://github.com/shiming-chen/FREE .\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:11:01 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Shiming", ""], ["Wang", "Wenjie", ""], ["Xia", "Beihao", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""], ["Zheng", "Feng", ""], ["Shao", "Ling", ""]]}, {"id": "2107.13812", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Yong Du, Wenpeng Xiao, Xuemiao Xu and Shengfeng He", "title": "From Continuity to Editability: Inverting GANs with Consecutive Images", "comments": "Paper has been accepted by ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing GAN inversion methods are stuck in a paradox that the inverted codes\ncan either achieve high-fidelity reconstruction, or retain the editing\ncapability. Having only one of them clearly cannot realize real image editing.\nIn this paper, we resolve this paradox by introducing consecutive images (\\eg,\nvideo frames or the same person with different poses) into the inversion\nprocess. The rationale behind our solution is that the continuity of\nconsecutive images leads to inherent editable directions. This inborn property\nis used for two unique purposes: 1) regularizing the joint inversion process,\nsuch that each of the inverted code is semantically accessible from one of the\nother and fastened in a editable domain; 2) enforcing inter-image coherence,\nsuch that the fidelity of each inverted code can be maximized with the\ncomplement of other images. Extensive experiments demonstrate that our\nalternative significantly outperforms state-of-the-art methods in terms of\nreconstruction fidelity and editability on both the real image dataset and\nsynthesis dataset. Furthermore, our method provides the first support of\nvideo-based GAN inversion, and an interesting application of unsupervised\nsemantic transfer from consecutive images. Source code can be found at:\n\\url{https://github.com/Qingyang-Xu/InvertingGANs_with_ConsecutiveImgs}.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:19:58 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Xu", "Yangyang", ""], ["Du", "Yong", ""], ["Xiao", "Wenpeng", ""], ["Xu", "Xuemiao", ""], ["He", "Shengfeng", ""]]}, {"id": "2107.13820", "submitter": "Shao-Hua Wu", "authors": "Ching, Kai Lin, Shao, Hua Wu, Jerry Chang, Yun, Chien Cheng", "title": "The interpretation of endobronchial ultrasound image using 3D\n  convolutional neural network for differentiating malignant and benign\n  mediastinal lesions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The purpose of this study is to differentiate malignant and benign\nmediastinal lesions by using the three-dimensional convolutional neural network\nthrough the endobronchial ultrasound (EBUS) image. Compared with previous\nstudy, our proposed model is robust to noise and able to fuse various imaging\nfeatures and spatiotemporal features of EBUS videos. Endobronchial\nultrasound-guided transbronchial needle aspiration (EBUS-TBNA) is a diagnostic\ntool for intrathoracic lymph nodes. Physician can observe the characteristics\nof the lesion using grayscale mode, doppler mode, and elastography during the\nprocedure. To process the EBUS data in the form of a video and appropriately\nintegrate the features of multiple imaging modes, we used a time-series\nthree-dimensional convolutional neural network (3D CNN) to learn the\nspatiotemporal features and design a variety of architectures to fuse each\nimaging mode. Our model (Res3D_UDE) took grayscale mode, Doppler mode, and\nelastography as training data and achieved an accuracy of 82.00% and area under\nthe curve (AUC) of 0.83 on the validation set. Compared with previous study, we\ndirectly used videos recorded during procedure as training and validation data,\nwithout additional manual selection, which might be easier for clinical\napplication. In addition, model designed with 3D CNN can also effectively learn\nspatiotemporal features and improve accuracy. In the future, our model may be\nused to guide physicians to quickly and correctly find the target lesions for\nslice sampling during the inspection process, reduce the number of slices of\nbenign lesions, and shorten the inspection time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:38:17 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ching", "", ""], ["Lin", "Kai", ""], ["Shao", "", ""], ["Wu", "Hua", ""], ["Chang", "Jerry", ""], ["Yun", "", ""], ["Cheng", "Chien", ""]]}, {"id": "2107.13824", "submitter": "Zeyu Hu", "authors": "Zeyu Hu, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Jiayu Dong, Xin\n  Wang, Guangyuan Sun, Hongbo Fu, Chiew-Lan Tai", "title": "VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation", "comments": "ICCV2021(Oral), supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sparse voxel-based methods have become the state-of-the-arts\nfor 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.\nNevertheless, being oblivious to the underlying geometry, voxel-based methods\nsuffer from ambiguous features on spatially close objects and struggle with\nhandling complex and irregular geometries due to the lack of geodesic\ninformation. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D\ndeep architecture that operates on the voxel and mesh representations\nleveraging both the Euclidean and geodesic information. Intuitively, the\nEuclidean information extracted from voxels can offer contextual cues\nrepresenting interactions between nearby objects, while the geodesic\ninformation extracted from meshes can help separate objects that are spatially\nclose but have disconnected surfaces. To incorporate such information from the\ntwo domains, we design an intra-domain attentive module for effective feature\naggregation and an inter-domain attentive module for adaptive feature fusion.\nExperimental results validate the effectiveness of VMNet: specifically, on the\nchallenging ScanNet dataset for large-scale segmentation of indoor scenes, it\noutperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%\nand 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M\nparameters). Code release: https://github.com/hzykent/VMNet\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:41:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Hu", "Zeyu", ""], ["Bai", "Xuyang", ""], ["Shang", "Jiaxiang", ""], ["Zhang", "Runze", ""], ["Dong", "Jiayu", ""], ["Wang", "Xin", ""], ["Sun", "Guangyuan", ""], ["Fu", "Hongbo", ""], ["Tai", "Chiew-Lan", ""]]}, {"id": "2107.13833", "submitter": "Frieda Van Den Noort", "authors": "Frieda van den Noort, Beril Sirmacek, Cornelis H. Slump", "title": "Recurrent U-net for automatic pelvic floor muscle segmentation on 3D\n  ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The prevalance of pelvic floor problems is high within the female population.\nTransperineal ultrasound (TPUS) is the main imaging modality used to\ninvestigate these problems. Automating the analysis of TPUS data will help in\ngrowing our understanding of pelvic floor related problems. In this study we\npresent a U-net like neural network with some convolutional long short term\nmemory (CLSTM) layers to automate the 3D segmentation of the levator ani muscle\n(LAM) in TPUS volumes. The CLSTM layers are added to preserve the inter-slice\n3D information. We reach human level performance on this segmentation task.\nTherefore, we conclude that we successfully automated the segmentation of the\nLAM on 3D TPUS data. This paves the way towards automatic in-vivo analysis of\nthe LAM mechanics in the context of large study populations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:53:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Noort", "Frieda van den", ""], ["Sirmacek", "Beril", ""], ["Slump", "Cornelis H.", ""]]}, {"id": "2107.13904", "submitter": "Wenhang Ge", "authors": "Wenhang Ge, Chunyan Pan, Ancong Wu, Hongwei Zheng, Wei-Shi Zheng", "title": "Cross-Camera Feature Prediction for Intra-Camera Supervised Person\n  Re-identification across Distant Scenes", "comments": "10 pages, 6 figures, accepted by ACM International Conference on\n  Multimedia", "journal-ref": null, "doi": "10.1145/3474085.3475382", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims to match person images across\nnon-overlapping camera views. The majority of Re-ID methods focus on\nsmall-scale surveillance systems in which each pedestrian is captured in\ndifferent camera views of adjacent scenes. However, in large-scale surveillance\nsystems that cover larger areas, it is required to track a pedestrian of\ninterest across distant scenes (e.g., a criminal suspect escapes from one city\nto another). Since most pedestrians appear in limited local areas, it is\ndifficult to collect training data with cross-camera pairs of the same person.\nIn this work, we study intra-camera supervised person re-identification across\ndistant scenes (ICS-DS Re-ID), which uses cross-camera unpaired data with\nintra-camera identity labels for training. It is challenging as cross-camera\npaired data plays a crucial role for learning camera-invariant features in most\nexisting Re-ID methods. To learn camera-invariant representation from\ncross-camera unpaired training data, we propose a cross-camera feature\nprediction method to mine cross-camera self supervision information from\ncamera-specific feature distribution by transforming fake cross-camera positive\nfeature pairs and minimize the distances of the fake pairs. Furthermore, we\nautomatically localize and extract local-level feature by a transformer. Joint\nlearning of global-level and local-level features forms a global-local\ncross-camera feature prediction scheme for mining fine-grained cross-camera\nself supervision information. Finally, cross-camera self supervision and\nintra-camera supervision are aggregated in a framework. The experiments are\nconducted in the ICS-DS setting on Market-SCT, Duke-SCT and MSMT17-SCT\ndatasets. The evaluation results demonstrate the superiority of our method,\nwhich gains significant improvements of 15.4 Rank-1 and 22.3 mAP on Market-SCT\nas compared to the second best method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 11:27:50 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ge", "Wenhang", ""], ["Pan", "Chunyan", ""], ["Wu", "Ancong", ""], ["Zheng", "Hongwei", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2107.13931", "submitter": "Yinmin Zhang", "authors": "Yinmin Zhang, Xinzhu Ma, Shuai Yi, Jun Hou, Zhihui Wang, Wanli Ouyang,\n  Dan Xu", "title": "Learning Geometry-Guided Depth via Projective Modeling for Monocular 3D\n  Object Detection", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial task of autonomous driving, 3D object detection has made great\nprogress in recent years. However, monocular 3D object detection remains a\nchallenging problem due to the unsatisfactory performance in depth estimation.\nMost existing monocular methods typically directly regress the scene depth\nwhile ignoring important relationships between the depth and various geometric\nelements (e.g. bounding box sizes, 3D object dimensions, and object poses). In\nthis paper, we propose to learn geometry-guided depth estimation with\nprojective modeling to advance monocular 3D object detection. Specifically, a\nprincipled geometry formula with projective modeling of 2D and 3D depth\npredictions in the monocular 3D object detection network is devised. We further\nimplement and embed the proposed formula to enable geometry-aware deep\nrepresentation learning, allowing effective 2D and 3D interactions for boosting\nthe depth estimation. Moreover, we provide a strong baseline through addressing\nsubstantial misalignment between 2D annotation and projected boxes to ensure\nrobust learning with the proposed geometric formula. Experiments on the KITTI\ndataset show that our method remarkably improves the detection performance of\nthe state-of-the-art monocular-based method without extra data by 2.80% on the\nmoderate test setting. The model and code will be released at\nhttps://github.com/YinminZhang/MonoGeo.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 12:30:39 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhang", "Yinmin", ""], ["Ma", "Xinzhu", ""], ["Yi", "Shuai", ""], ["Hou", "Jun", ""], ["Wang", "Zhihui", ""], ["Ouyang", "Wanli", ""], ["Xu", "Dan", ""]]}, {"id": "2107.13938", "submitter": "Vladimir Loginov", "authors": "Vladimir Loginov", "title": "Why You Should Try the Real Data for the Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works in the text recognition area have pushed forward the recognition\nresults to the new horizons. But for a long time a lack of large human-labeled\nnatural text recognition datasets has been forcing researchers to use synthetic\ndata for training text recognition models. Even though synthetic datasets are\nvery large (MJSynth and SynthTest, two most famous synthetic datasets, have\nseveral million images each), their diversity could be insufficient, compared\nto natural datasets like ICDAR and others. Fortunately, the recently released\ntext-recognition annotation for OpenImages V5 dataset has comparable with\nsynthetic dataset number of instances and more diverse examples. We have used\nthis annotation with a Text Recognition head architecture from the Yet Another\nMask Text Spotter and got comparable to the SOTA results. On some datasets we\nhave even outperformed previous SOTA models. In this paper we also introduce a\ntext recognition model. The model's code is available.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 12:58:57 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Loginov", "Vladimir", ""]]}, {"id": "2107.13967", "submitter": "Yu Fu", "authors": "Yu Fu, TianYang Xu, XiaoJun Wu, Josef Kittler", "title": "PPT Fusion: Pyramid Patch Transformerfor a Case Study in Image Fusion", "comments": "We have added more experiments and are improving the article, and\n  will submit it to the journal. It will be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The Transformer architecture has achieved rapiddevelopment in recent years,\noutperforming the CNN archi-tectures in many computer vision tasks, such as the\nVisionTransformers (ViT) for image classification. However, existingvisual\ntransformer models aim to extract semantic informationfor high-level tasks such\nas classification and detection, distortingthe spatial resolution of the input\nimage, thus sacrificing thecapacity in reconstructing the input or generating\nhigh-resolutionimages. In this paper, therefore, we propose a Patch\nPyramidTransformer(PPT) to effectively address the above issues. Specif-ically,\nwe first design a Patch Transformer to transform theimage into a sequence of\npatches, where transformer encodingis performed for each patch to extract local\nrepresentations.In addition, we construct a Pyramid Transformer to\neffectivelyextract the non-local information from the entire image.\nAfterobtaining a set of multi-scale, multi-dimensional, and multi-anglefeatures\nof the original image, we design the image reconstructionnetwork to ensure that\nthe features can be reconstructed intothe original input. To validate the\neffectiveness, we apply theproposed Patch Pyramid Transformer to the image\nfusion task.The experimental results demonstrate its superior\nperformanceagainst the state-of-the-art fusion approaches, achieving the\nbestresults on several evaluation indicators. The underlying capacityof the PPT\nnetwork is reflected by its universal power in featureextraction and image\nreconstruction, which can be directlyapplied to different image fusion tasks\nwithout redesigning orretraining the network.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 13:57:45 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fu", "Yu", ""], ["Xu", "TianYang", ""], ["Wu", "XiaoJun", ""], ["Kittler", "Josef", ""]]}, {"id": "2107.13973", "submitter": "Rushali Grandhe", "authors": "Farha Al Breiki, Muhammad Ridzuan, Rushali Grandhe", "title": "Self-Supervised Learning for Fine-Grained Image Classification", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification involves identifying different\nsubcategories of a class which possess very subtle discriminatory features.\nFine-grained datasets usually provide bounding box annotations along with class\nlabels to aid the process of classification. However, building large scale\ndatasets with such annotations is a mammoth task. Moreover, this extensive\nannotation is time-consuming and often requires expertise, which is a huge\nbottleneck in building large datasets. On the other hand, self-supervised\nlearning (SSL) exploits the freely available data to generate supervisory\nsignals which act as labels. The features learnt by performing some pretext\ntasks on huge unlabelled data proves to be very helpful for multiple downstream\ntasks.\n  Our idea is to leverage self-supervision such that the model learns useful\nrepresentations of fine-grained image classes. We experimented with 3 kinds of\nmodels: Jigsaw solving as pretext task, adversarial learning (SRGAN) and\ncontrastive learning based (SimCLR) model. The learned features are used for\ndownstream tasks such as fine-grained image classification. Our code is\navailable at\nhttp://github.com/rush2406/Self-Supervised-Learning-for-Fine-grained-Image-Classification\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:01:31 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Breiki", "Farha Al", ""], ["Ridzuan", "Muhammad", ""], ["Grandhe", "Rushali", ""]]}, {"id": "2107.13978", "submitter": "Yu Zhang", "authors": "Yu Zhang and Chang-Bin Zhang and Peng-Tao Jiang and Feng Mao and\n  Ming-Ming Cheng", "title": "Personalized Image Semantic Segmentation", "comments": "ICCV2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 04:03:11 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhang", "Yu", ""], ["Zhang", "Chang-Bin", ""], ["Jiang", "Peng-Tao", ""], ["Mao", "Feng", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2107.13994", "submitter": "Wenkang Shan", "authors": "Wenkang Shan, Haopeng Lu, Shanshe Wang, Xinfeng Zhang, Wen Gao", "title": "Improving Robustness and Accuracy via Relative Information Encoding in\n  3D Human Pose Estimation", "comments": "In Proceedings of the 29th ACM International Conference on Multimedia\n  (MM '21)", "journal-ref": null, "doi": "10.1145/3474085.3475504", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing 3D human pose estimation approaches mainly focus on\npredicting 3D positional relationships between the root joint and other human\njoints (local motion) instead of the overall trajectory of the human body\n(global motion). Despite the great progress achieved by these approaches, they\nare not robust to global motion, and lack the ability to accurately predict\nlocal motion with a small movement range. To alleviate these two problems, we\npropose a relative information encoding method that yields positional and\ntemporal enhanced representations. Firstly, we encode positional information by\nutilizing relative coordinates of 2D poses to enhance the consistency between\nthe input and output distribution. The same posture with different absolute 2D\npositions can be mapped to a common representation. It is beneficial to resist\nthe interference of global motion on the prediction results. Second, we encode\ntemporal information by establishing the connection between the current pose\nand other poses of the same person within a period of time. More attention will\nbe paid to the movement changes before and after the current pose, resulting in\nbetter prediction performance on local motion with a small movement range. The\nablation studies validate the effectiveness of the proposed relative\ninformation encoding method. Besides, we introduce a multi-stage optimization\nmethod to the whole framework to further exploit the positional and temporal\nenhanced representations. Our method outperforms state-of-the-art methods on\ntwo public datasets. Code is available at\nhttps://github.com/paTRICK-swk/Pose3D-RIE.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:12:19 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shan", "Wenkang", ""], ["Lu", "Haopeng", ""], ["Wang", "Shanshe", ""], ["Zhang", "Xinfeng", ""], ["Gao", "Wen", ""]]}, {"id": "2107.13998", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "\"Excavating AI\" Re-excavated: Debunking a Fallacious Account of the\n  JAFFE Dataset", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I\ndesigned and photographed JAFFE, a set of facial expression images intended for\nuse in a study of face perception. In 2019, without seeking permission or\ninforming us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely\npublicized art shows. In addition, they published a nonfactual account of the\nimages in the essay \"Excavating AI: The Politics of Images in Machine Learning\nTraining Sets.\" The present article recounts the creation of the JAFFE dataset\nand unravels each of Crawford and Paglen's fallacious statements. I also\ndiscuss JAFFE more broadly in connection with research on facial expression,\naffective computing, and human-computer interaction.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 01:31:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "2107.14048", "submitter": "Laurent Kloeker", "authors": "Laurent Kloeker, Amarin Kloeker, Fabian Thomsen, Armin Erraji, Lutz\n  Eckstein, Serge Lamberty, Adrian Fazekas, Eszter Kall\\'o, Markus Oeser,\n  Charlotte Fl\\'echon, Jochen Lohmiller, Pascal Pfeiffer, Martin Sommer, Helen\n  Winter", "title": "Corridor for new mobility Aachen-D\\\"usseldorf: Methods and concepts of\n  the research project ACCorD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the Corridor for New Mobility Aachen - D\\\"usseldorf, an integrated\ndevelopment environment is created, incorporating existing test capabilities,\nto systematically test and validate automated vehicles in interaction with\nconnected Intelligent Transport Systems Stations (ITS-Ss). This is achieved\nthrough a time- and cost-efficient toolchain and methodology, in which\nsimulation, closed test sites as well as test fields in public transport are\nlinked in the best possible way. By implementing a digital twin, the recorded\ntraffic events can be visualized in real-time and driving functions can be\ntested in the simulation based on real data. In order to represent diverse\ntraffic scenarios, the corridor contains a highway section, a rural area, and\nurban areas. First, this paper outlines the project goals before describing the\nindividual project contents in more detail. These include the concepts of\ntraffic detection, driving function development, digital twin development, and\npublic involvement.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:09:51 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kloeker", "Laurent", ""], ["Kloeker", "Amarin", ""], ["Thomsen", "Fabian", ""], ["Erraji", "Armin", ""], ["Eckstein", "Lutz", ""], ["Lamberty", "Serge", ""], ["Fazekas", "Adrian", ""], ["Kall\u00f3", "Eszter", ""], ["Oeser", "Markus", ""], ["Fl\u00e9chon", "Charlotte", ""], ["Lohmiller", "Jochen", ""], ["Pfeiffer", "Pascal", ""], ["Sommer", "Martin", ""], ["Winter", "Helen", ""]]}, {"id": "2107.14051", "submitter": "Xinyu Gao", "authors": "Xinyu Gao, Yi Li, Yanqing Qiu, Bangning Mao, Miaogen Chen, Yanlong\n  Meng, Chunliu Zhao, Juan Kang, Yong Guo, and Changyu Shen", "title": "Improvement of image classification by multiple optical scattering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple optical scattering occurs when light propagates in a non-uniform\nmedium. During the multiple scattering, images were distorted and the spatial\ninformation they carried became scrambled. However, the image information is\nnot lost but presents in the form of speckle patterns (SPs). In this study, we\nbuilt up an optical random scattering system based on an LCD and an RGB laser\nsource. We found that the image classification can be improved by the help of\nrandom scattering which is considered as a feedforward neural network to\nextracts features from image. Along with the ridge classification deployed on\ncomputer, we achieved excellent classification accuracy higher than 94%, for a\nvariety of data sets covering medical, agricultural, environmental protection\nand other fields. In addition, the proposed optical scattering system has the\nadvantages of high speed, low power consumption, and miniaturization, which is\nsuitable for deploying in edge computing applications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 04:12:41 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Gao", "Xinyu", ""], ["Li", "Yi", ""], ["Qiu", "Yanqing", ""], ["Mao", "Bangning", ""], ["Chen", "Miaogen", ""], ["Meng", "Yanlong", ""], ["Zhao", "Chunliu", ""], ["Kang", "Juan", ""], ["Guo", "Yong", ""], ["Shen", "Changyu", ""]]}, {"id": "2107.14053", "submitter": "Eugene Lee", "authors": "Eugene Lee, Cheng-Han Huang, Chen-Yi Lee", "title": "Few-Shot and Continual Learning with Attentive Independent Mechanisms", "comments": "20 pages, 44 figures, accepted by International Conference of\n  Computer Vision 2021 (ICCV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) are known to perform well when deployed to test\ndistributions that shares high similarity with the training distribution.\nFeeding DNNs with new data sequentially that were unseen in the training\ndistribution has two major challenges -- fast adaptation to new tasks and\ncatastrophic forgetting of old tasks. Such difficulties paved way for the\non-going research on few-shot learning and continual learning. To tackle these\nproblems, we introduce Attentive Independent Mechanisms (AIM). We incorporate\nthe idea of learning using fast and slow weights in conjunction with the\ndecoupling of the feature extraction and higher-order conceptual learning of a\nDNN. AIM is designed for higher-order conceptual learning, modeled by a mixture\nof experts that compete to learn independent concepts to solve a new task. AIM\nis a modular component that can be inserted into existing deep learning\nframeworks. We demonstrate its capability for few-shot learning by adding it to\nSIB and trained on MiniImageNet and CIFAR-FS, showing significant improvement.\nAIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and\nMiniImageNet to demonstrate its capability in continual learning. Code made\npublicly available at https://github.com/huang50213/AIM-Fewshot-Continual.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:43:24 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Lee", "Eugene", ""], ["Huang", "Cheng-Han", ""], ["Lee", "Chen-Yi", ""]]}, {"id": "2107.14061", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul", "title": "The Need and Status of Sea Turtle Conservation and Survey of Associated\n  Computer Vision Advances", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For over hundreds of millions of years, sea turtles and their ancestors have\nswum in the vast expanses of the ocean. They have undergone a number of\nevolutionary changes, leading to speciation and sub-speciation. However, in the\npast few decades, some of the most notable forces driving the genetic variance\nand population decline have been global warming and anthropogenic impact\nranging from large-scale poaching, collecting turtle eggs for food, besides\ndumping trash including plastic waste into the ocean. This leads to severe\ndetrimental effects in the sea turtle population, driving them to extinction.\nThis research focusses on the forces causing the decline in sea turtle\npopulation, the necessity for the global conservation efforts along with its\nsuccesses and failures, followed by an in-depth analysis of the modern advances\nin detection and recognition of sea turtles, involving Machine Learning and\nComputer Vision systems, aiding the conservation efforts.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:53:47 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Paul", "Aditya Jyoti", ""]]}, {"id": "2107.14062", "submitter": "Leonardo Scabini", "authors": "Leonardo F. S. Scabini and Odemir M. Bruno", "title": "Structure and Performance of Fully Connected Neural Networks: Emerging\n  Complex Network Properties", "comments": "18 pages, 7 figures, and 2 tables. Submitted to a peer-review journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV physics.app-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding the behavior of Artificial Neural Networks is one of the main\ntopics in the field recently, as black-box approaches have become usual since\nthe widespread of deep learning. Such high-dimensional models may manifest\ninstabilities and weird properties that resemble complex systems. Therefore, we\npropose Complex Network (CN) techniques to analyze the structure and\nperformance of fully connected neural networks. For that, we build a dataset\nwith 4 thousand models and their respective CN properties. They are employed in\na supervised classification setup considering four vision benchmarks. Each\nneural network is approached as a weighted and undirected graph of neurons and\nsynapses, and centrality measures are computed after training. Results show\nthat these measures are highly related to the network classification\nperformance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based\napproach for finding topological signatures linking similar neurons. Results\nsuggest that six neuronal types emerge in such networks, independently of the\ntarget domain, and are distributed differently according to classification\naccuracy. We also tackle specific CN properties related to performance, such as\nhigher subgraph centrality on lower-performing models. Our findings suggest\nthat CN properties play a critical role in the performance of fully connected\nneural networks, with topological patterns emerging independently on a wide\nrange of models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:53:52 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Scabini", "Leonardo F. S.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "2107.14070", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul, Smaranjit Ghose, Kanishka Aggarwal, Niketha\n  Nethaji, Shivam Pal, Arnab Dutta Purkayastha", "title": "Machine Learning Advances aiding Recognition and Classification of\n  Indian Monuments and Landmarks", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tourism in India plays a quintessential role in the country's economy with an\nestimated 9.2% GDP share for the year 2018. With a yearly growth rate of 6.2%,\nthe industry holds a huge potential for being the primary driver of the economy\nas observed in the nations of the Middle East like the United Arab Emirates.\nThe historical and cultural diversity exhibited throughout the geography of the\nnation is a unique spectacle for people around the world and therefore serves\nto attract tourists in tens of millions in number every year. Traditionally,\ntour guides or academic professionals who study these heritage monuments were\nresponsible for providing information to the visitors regarding their\narchitectural and historical significance. However, unfortunately this system\nhas several caveats when considered on a large scale such as unavailability of\nsufficient trained people, lack of accurate information, failure to convey the\nrichness of details in an attractive format etc. Recently, machine learning\napproaches revolving around the usage of monument pictures have been shown to\nbe useful for rudimentary analysis of heritage sights. This paper serves as a\nsurvey of the research endeavors undertaken in this direction which would\neventually provide insights for building an automated decision system that\ncould be utilized to make the experience of tourism in India more modernized\nfor visitors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:01:02 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Paul", "Aditya Jyoti", ""], ["Ghose", "Smaranjit", ""], ["Aggarwal", "Kanishka", ""], ["Nethaji", "Niketha", ""], ["Pal", "Shivam", ""], ["Purkayastha", "Arnab Dutta", ""]]}, {"id": "2107.14072", "submitter": "David LeBauer", "authors": "David LeBauer, Max Burnette, Noah Fahlgren, Rob Kooper, Kenton\n  McHenry, Abby Stylianou", "title": "What Does TERRA-REF's High Resolution, Multi Sensor Plant Sensing Public\n  Domain Data Offer the Computer Vision Community?", "comments": "7 pages, 4 figures, submitted to CVPR 2021 Agriculture-Vision\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A core objective of the TERRA-REF project was to generate an open-access\nreference dataset for the study of evaluation of sensing technology to study\nplants under field conditions. The TERRA-REF program deployed a suite of high\nresolution, cutting edge technology sensors on a gantry system with the aim of\nscanning 1 hectare (~$10^4$ m) at around $1 mm^2$ spatial resolution multiple\ntimes per week. The system contains co-located sensors including a stereo-pair\nRGB camera, a thermal imager, a laser scanner to capture 3D structure, and two\nhyperspectral cameras covering wavelengths of 300-2500nm. This sensor data is\nprovided alongside over sixty types of traditional plant measurements that can\nbe used to train new machine learning models. Associated weather and\nenvironmental measurements, information about agronomic management and\nexperimental design, and the genomic sequences of hundreds of plant varieties\nhave been collected and are available alongside the sensor and plant trait\n(phenotype) data.\n  Over the course of four years and ten growing seasons, the TERRA-REF system\ngenerated over 1 PB of sensor data and almost 45 million files. The subset that\nhas been released to the public domain accounts for two seasons and about half\nof the total data volume. This provides an unprecedented opportunity for\ninvestigations far beyond the core biological scope of the project.\n  This focus of this paper is to provide the Computer Vision and Machine\nLearning communities an overview of the available data and some potential\napplications of this one of a kind data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:01:29 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["LeBauer", "David", ""], ["Burnette", "Max", ""], ["Fahlgren", "Noah", ""], ["Kooper", "Rob", ""], ["McHenry", "Kenton", ""], ["Stylianou", "Abby", ""]]}, {"id": "2107.14091", "submitter": "Nikhil Woodruff", "authors": "Nikhil Woodruff, Amir Enshaei, Bashar Awwad Shiekh Hasan", "title": "Fully-Automatic Pipeline for Document Signature Analysis to Detect Money\n  Laundering Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Signatures present on corporate documents are often used in investigations of\nrelationships between persons of interest, and prior research into the task of\noffline signature verification has evaluated a wide range of methods on\nstandard signature datasets. However, such tasks often benefit from prior human\nsupervision in the collection, adjustment and labelling of isolated signature\nimages from which all real-world context has been removed. Signatures found in\nonline document repositories such as the United Kingdom Companies House\nregularly contain high variation in location, size, quality and degrees of\nobfuscation under stamps. We propose an integrated pipeline of signature\nextraction and curation, with no human assistance from the obtaining of company\ndocuments to the clustering of individual signatures. We use a sequence of\nheuristic methods, convolutional neural networks, generative adversarial\nnetworks and convolutional Siamese networks for signature extraction,\nfiltering, cleaning and embedding respectively. We evaluate both the\neffectiveness of the pipeline at matching obscured same-author signature pairs\nand the effectiveness of the entire pipeline against a human baseline for\ndocument signature analysis, as well as presenting uses for such a pipeline in\nthe field of real-world anti-money laundering investigation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:17:28 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Woodruff", "Nikhil", ""], ["Enshaei", "Amir", ""], ["Hasan", "Bashar Awwad Shiekh", ""]]}, {"id": "2107.14110", "submitter": "Juan C. P\\'erez", "authors": "Juan C. P\\'erez, Motasem Alfarra, Guillaume Jeanneret, Laura Rueda,\n  Ali Thabet, Bernard Ghanem, Pablo Arbel\\'aez", "title": "Enhancing Adversarial Robustness via Test-time Transformation Ensembling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models are prone to being fooled by imperceptible perturbations\nknown as adversarial attacks. In this work, we study how equipping models with\nTest-time Transformation Ensembling (TTE) can work as a reliable defense\nagainst such attacks. While transforming the input data, both at train and test\ntimes, is known to enhance model performance, its effects on adversarial\nrobustness have not been studied. Here, we present a comprehensive empirical\nstudy of the impact of TTE, in the form of widely-used image transforms, on\nadversarial robustness. We show that TTE consistently improves model robustness\nagainst a variety of powerful attacks without any need for re-training, and\nthat this improvement comes at virtually no trade-off with accuracy on clean\nsamples. Finally, we show that the benefits of TTE transfer even to the\ncertified robustness domain, in which TTE provides sizable and consistent\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:32:35 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["P\u00e9rez", "Juan C.", ""], ["Alfarra", "Motasem", ""], ["Jeanneret", "Guillaume", ""], ["Rueda", "Laura", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""], ["Arbel\u00e1ez", "Pablo", ""]]}, {"id": "2107.14123", "submitter": "Benjamin Kellenberger", "authors": "Benjamin Kellenberger and John E. Vargas-Mu\\~noz and Devis Tuia and\n  Rodrigo C. Daudt and Konrad Schindler and Thao T-T Whelan and Brenda Ayo and\n  Ferda Ofli and Muhammad Imran", "title": "Mapping Vulnerable Populations with AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humanitarian actions require accurate information to efficiently delegate\nsupport operations. Such information can be maps of building footprints,\nbuilding functions, and population densities. While the access to this\ninformation is comparably easy in industrialized countries thanks to reliable\ncensus data and national geo-data infrastructures, this is not the case for\ndeveloping countries, where that data is often incomplete or outdated. Building\nmaps derived from remote sensing images may partially remedy this challenge in\nsuch countries, but are not always accurate due to different landscape\nconfigurations and lack of validation data. Even when they exist, building\nfootprint layers usually do not reveal more fine-grained building properties,\nsuch as the number of stories or the building's function (e.g., office,\nresidential, school, etc.). In this project we aim to automate building\nfootprint and function mapping using heterogeneous data sources. In a first\nstep, we intend to delineate buildings from satellite data, using deep learning\nmodels for semantic image segmentation. Building functions shall be retrieved\nby parsing social media data like for instance tweets, as well as ground-based\nimagery, to automatically identify different buildings functions and retrieve\nfurther information such as the number of building stories. Building maps\naugmented with those additional attributes make it possible to derive more\naccurate population density maps, needed to support the targeted provision of\nhumanitarian aid.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:52:11 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kellenberger", "Benjamin", ""], ["Vargas-Mu\u00f1oz", "John E.", ""], ["Tuia", "Devis", ""], ["Daudt", "Rodrigo C.", ""], ["Schindler", "Konrad", ""], ["Whelan", "Thao T-T", ""], ["Ayo", "Brenda", ""], ["Ofli", "Ferda", ""], ["Imran", "Muhammad", ""]]}, {"id": "2107.14153", "submitter": "Siyu Huang", "authors": "Siyu Huang, Tianyang Wang, Haoyi Xiong, Jun Huan, Dejing Dou", "title": "Semi-Supervised Active Learning with Temporal Output Discrepancy", "comments": "ICCV 2021. Code is available at https://github.com/siyuhuang/TOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning succeeds in a wide range of tasks, it highly depends on\nthe massive collection of annotated data which is expensive and time-consuming.\nTo lower the cost of data annotation, active learning has been proposed to\ninteractively query an oracle to annotate a small proportion of informative\nsamples in an unlabeled dataset. Inspired by the fact that the samples with\nhigher loss are usually more informative to the model than the samples with\nlower loss, in this paper we present a novel deep active learning approach that\nqueries the oracle for data annotation when the unlabeled sample is believed to\nincorporate high loss. The core of our approach is a measurement Temporal\nOutput Discrepancy (TOD) that estimates the sample loss by evaluating the\ndiscrepancy of outputs given by models at different optimization steps. Our\ntheoretical investigation shows that TOD lower-bounds the accumulated sample\nloss thus it can be used to select informative unlabeled samples. On basis of\nTOD, we further develop an effective unlabeled data sampling strategy as well\nas an unsupervised learning criterion that enhances model performance by\nincorporating the unlabeled data. Due to the simplicity of TOD, our active\nlearning approach is efficient, flexible, and task-agnostic. Extensive\nexperimental results demonstrate that our approach achieves superior\nperformances than the state-of-the-art active learning methods on image\nclassification and semantic segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:25:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Huang", "Siyu", ""], ["Wang", "Tianyang", ""], ["Xiong", "Haoyi", ""], ["Huan", "Jun", ""], ["Dou", "Dejing", ""]]}, {"id": "2107.14160", "submitter": "Tai Wang", "authors": "Tai Wang, Xinge Zhu, Jiangmiao Pang, Dahua Lin", "title": "Probabilistic and Geometric Depth: Detecting Objects in Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D object detection is an important capability needed in various practical\napplications such as driver assistance systems. Monocular 3D detection, as an\neconomical solution compared to conventional settings relying on binocular\nvision or LiDAR, has drawn increasing attention recently but still yields\nunsatisfactory results. This paper first presents a systematic study on this\nproblem and observes that the current monocular 3D detection problem can be\nsimplified as an instance depth estimation problem: The inaccurate instance\ndepth blocks all the other 3D attribute predictions from improving the overall\ndetection performance. However, recent methods directly estimate the depth\nbased on isolated instances or pixels while ignoring the geometric relations\nacross different objects, which can be valuable constraints as the key\ninformation about depth is not directly manifest in the monocular image.\nTherefore, we construct geometric relation graphs across predicted objects and\nuse the graph to facilitate depth estimation. As the preliminary depth\nestimation of each instance is usually inaccurate in this ill-posed setting, we\nincorporate a probabilistic representation to capture the uncertainty. It\nprovides an important indicator to identify confident predictions and further\nguide the depth propagation. Despite the simplicity of the basic idea, our\nmethod obtains significant improvements on KITTI and nuScenes benchmarks,\nachieving the 1st place out of all monocular vision-only methods while still\nmaintaining real-time efficiency. Code and models will be released at\nhttps://github.com/open-mmlab/mmdetection3d.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:30:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wang", "Tai", ""], ["Zhu", "Xinge", ""], ["Pang", "Jiangmiao", ""], ["Lin", "Dahua", ""]]}, {"id": "2107.14175", "submitter": "Nicolas Basty", "authors": "Nicolas Basty, Marjola Thanaj, Madeleine Cule, Elena P. Sorokin, Yi\n  Liu, Jimmy D. Bell, E. Louise Thomas, and Brandon Whitcher", "title": "Swap-Free Fat-Water Separation in Dixon MRI using Conditional Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dixon MRI is widely used for body composition studies. Current processing\nmethods associated with large whole-body volumes are time intensive and prone\nto artifacts during fat-water separation performed on the scanner, making the\ndata difficult to analyse. The most common artifact are fat-water swaps, where\nthe labels are inverted at the voxel level. It is common for researchers to\ndiscard swapped data (generally around 10%), which can be wasteful and lead to\nunintended biases. The UK Biobank is acquiring Dixon MRI for over 100,000\nparticipants, and thousands of swaps will occur. If those go undetected, errors\nwill propagate into processes such as abdominal organ segmentation and dilute\nthe results in population-based analyses. There is a clear need for a fast and\nrobust method to accurately separate fat and water channels. In this work we\npropose such a method based on style transfer using a conditional generative\nadversarial network. We also introduce a new Dixon loss function for the\ngenerator model. Using data from the UK Biobank Dixon MRI, our model is able to\npredict highly accurate fat and water channels that are free from artifacts. We\nshow that the model separates fat and water channels using either single input\n(in-phase) or dual input (in-phase and opposed-phase), with the latter\nproducing improved results. Our proposed method enables faster and more\naccurate downstream analysis of body composition from Dixon MRI in population\nstudies by eliminating the need for visual inspection or discarding data due to\nfat-water swaps.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:56:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Basty", "Nicolas", ""], ["Thanaj", "Marjola", ""], ["Cule", "Madeleine", ""], ["Sorokin", "Elena P.", ""], ["Liu", "Yi", ""], ["Bell", "Jimmy D.", ""], ["Thomas", "E. Louise", ""], ["Whitcher", "Brandon", ""]]}, {"id": "2107.14178", "submitter": "Xuewen Yang", "authors": "Xuewen Yang, Yingru Liu, Xin Wang", "title": "ReFormer: The Relational Transformer for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:03:36 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yang", "Xuewen", ""], ["Liu", "Yingru", ""], ["Wang", "Xin", ""]]}, {"id": "2107.14185", "submitter": "Hengchang Guo", "authors": "Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan Qin, Kui Ren", "title": "Feature Importance-aware Transferable Adversarial Attacks", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferability of adversarial examples is of central importance for\nattacking an unknown model, which facilitates adversarial attacks in more\npractical scenarios, e.g., blackbox attacks. Existing transferable attacks tend\nto craft adversarial examples by indiscriminately distorting features to\ndegrade prediction accuracy in a source model without aware of intrinsic\nfeatures of objects in the images. We argue that such brute-force degradation\nwould introduce model-specific local optimum into adversarial examples, thus\nlimiting the transferability. By contrast, we propose the Feature\nImportance-aware Attack (FIA), which disrupts important object-aware features\nthat dominate model decisions consistently. More specifically, we obtain\nfeature importance by introducing the aggregate gradient, which averages the\ngradients with respect to feature maps of the source model, computed on a batch\nof random transforms of the original clean image. The gradients will be highly\ncorrelated to objects of interest, and such correlation presents invariance\nacross different models. Besides, the random transforms will preserve intrinsic\nfeatures of objects and suppress model-specific information. Finally, the\nfeature importance guides to search for adversarial examples towards disrupting\ncritical features, achieving stronger transferability. Extensive experimental\nevaluation demonstrates the effectiveness and superior performance of the\nproposed FIA, i.e., improving the success rate by 8.4% against normally trained\nmodels and 11.7% against defense models as compared to the state-of-the-art\ntransferable attacks. Code is available at: https://github.com/hcguoO0/FIA\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:13:29 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wang", "Zhibo", ""], ["Guo", "Hengchang", ""], ["Zhang", "Zhifei", ""], ["Liu", "Wenxin", ""], ["Qin", "Zhan", ""], ["Ren", "Kui", ""]]}, {"id": "2107.14202", "submitter": "Junlong Li", "authors": "Guangyi Chen, Junlong Li, Jiwen Lu, Jie Zhou", "title": "Human Trajectory Prediction via Counterfactual Analysis", "comments": "Accepted to ICCV 2021. Code: https://github.com/CHENGY12/CausalHTP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting human trajectories in complex dynamic environments plays a\ncritical role in autonomous vehicles and intelligent robots. Most existing\nmethods learn to predict future trajectories by behavior clues from history\ntrajectories and interaction clues from environments. However, the inherent\nbias between training and deployment environments is ignored. Hence, we propose\na counterfactual analysis method for human trajectory prediction to investigate\nthe causality between the predicted trajectories and input clues and alleviate\nthe negative effects brought by environment bias. We first build a causal graph\nfor trajectory forecasting with history trajectory, future trajectory, and the\nenvironment interactions. Then, we cut off the inference from environment to\ntrajectory by constructing the counterfactual intervention on the trajectory\nitself. Finally, we compare the factual and counterfactual trajectory clues to\nalleviate the effects of environment bias and highlight the trajectory clues.\nOur counterfactual analysis is a plug-and-play module that can be applied to\nany baseline prediction methods including RNN- and CNN-based ones. We show that\nour method achieves consistent improvement for different baselines and obtains\nthe state-of-the-art results on public pedestrian trajectory forecasting\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:41:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Guangyi", ""], ["Li", "Junlong", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2107.14204", "submitter": "Junlong Li", "authors": "Guangyi Chen, Junlong Li, Nuoxing Zhou, Liangliang Ren, Jiwen Lu", "title": "Personalized Trajectory Prediction via Distribution Discrimination", "comments": "Accepted to ICCV 2021. Code: https://github.com/CHENGY12/DisDis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is confronted with the dilemma to capture the\nmulti-modal nature of future dynamics with both diversity and accuracy. In this\npaper, we present a distribution discrimination (DisDis) method to predict\npersonalized motion patterns by distinguishing the potential distributions.\nMotivated by that the motion pattern of each person is personalized due to\nhis/her habit, our DisDis learns the latent distribution to represent different\nmotion patterns and optimize it by the contrastive discrimination. This\ndistribution discrimination encourages latent distributions to be more\ndiscriminative. Our method can be integrated with existing multi-modal\nstochastic predictive models as a plug-and-play module to learn the more\ndiscriminative latent distribution. To evaluate the latent distribution, we\nfurther propose a new metric, probability cumulative minimum distance (PCMD)\ncurve, which cumulatively calculates the minimum distance on the sorted\nprobabilities. Experimental results on the ETH and UCY datasets show the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:42:12 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Guangyi", ""], ["Li", "Junlong", ""], ["Zhou", "Nuoxing", ""], ["Ren", "Liangliang", ""], ["Lu", "Jiwen", ""]]}, {"id": "2107.14206", "submitter": "Santosh Thoduka", "authors": "Santosh Thoduka and Juergen Gall and Paul G. Pl\\\"oger", "title": "Using Visual Anomaly Detection for Task Execution Monitoring", "comments": "Accepted for publication at the 2021 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Execution monitoring is essential for robots to detect and respond to\nfailures. Since it is impossible to enumerate all failures for a given task, we\nlearn from successful executions of the task to detect visual anomalies during\nruntime. Our method learns to predict the motions that occur during the nominal\nexecution of a task, including camera and robot body motion. A probabilistic\nU-Net architecture is used to learn to predict optical flow, and the robot's\nkinematics and 3D model are used to model camera and body motion. The errors\nbetween the observed and predicted motion are used to calculate an anomaly\nscore. We evaluate our method on a dataset of a robot placing a book on a\nshelf, which includes anomalies such as falling books, camera occlusions, and\nrobot disturbances. We find that modeling camera and body motion, in addition\nto the learning-based optical flow prediction, results in an improvement of the\narea under the receiver operating characteristic curve from 0.752 to 0.804, and\nthe area under the precision-recall curve from 0.467 to 0.549.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:46:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Thoduka", "Santosh", ""], ["Gall", "Juergen", ""], ["Pl\u00f6ger", "Paul G.", ""]]}, {"id": "2107.14209", "submitter": "Fangrui Zhu", "authors": "Fangrui Zhu, Yi Zhu, Li Zhang, Chongruo Wu, Yanwei Fu, Mu Li", "title": "A Unified Efficient Pyramid Transformer for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a challenging problem due to difficulties in\nmodeling context in complex scenes and class confusions along boundaries. Most\nliterature either focuses on context modeling or boundary refinement, which is\nless generalizable in open-world scenarios. In this work, we advocate a unified\nframework(UN-EPT) to segment objects by considering both context information\nand boundary artifacts. We first adapt a sparse sampling strategy to\nincorporate the transformer-based attention mechanism for efficient context\nmodeling. In addition, a separate spatial branch is introduced to capture image\ndetails for boundary refinement. The whole model can be trained in an\nend-to-end manner. We demonstrate promising performance on three popular\nbenchmarks for semantic segmentation with low memory footprint. Code will be\nreleased soon.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:47:32 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhu", "Fangrui", ""], ["Zhu", "Yi", ""], ["Zhang", "Li", ""], ["Wu", "Chongruo", ""], ["Fu", "Yanwei", ""], ["Li", "Mu", ""]]}, {"id": "2107.14222", "submitter": "Houwen Peng", "authors": "Kan Wu and Houwen Peng and Minghao Chen and Jianlong Fu and Hongyang\n  Chao", "title": "Rethinking and Improving Relative Position Encoding for Vision\n  Transformer", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relative position encoding (RPE) is important for transformer to capture\nsequence ordering of input tokens. General efficacy has been proven in natural\nlanguage processing. However, in computer vision, its efficacy is not well\nstudied and even remains controversial, e.g., whether relative position\nencoding can work equally well as absolute position? In order to clarify this,\nwe first review existing relative position encoding methods and analyze their\npros and cons when applied in vision transformers. We then propose new relative\nposition encoding methods dedicated to 2D images, called image RPE (iRPE). Our\nmethods consider directional relative distance modeling as well as the\ninteractions between queries and relative position embeddings in self-attention\nmechanism. The proposed iRPE methods are simple and lightweight. They can be\neasily plugged into transformer blocks. Experiments demonstrate that solely due\nto the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc)\nand 1.3% (mAP) stable improvements over their original versions on ImageNet and\nCOCO respectively, without tuning any extra hyperparameters such as learning\nrate and weight decay. Our ablation and analysis also yield interesting\nfindings, some of which run counter to previous understanding. Code and models\nare open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:55:10 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wu", "Kan", ""], ["Peng", "Houwen", ""], ["Chen", "Minghao", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""]]}, {"id": "2107.14228", "submitter": "Jason Kuen", "authors": "Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang Zhao, Zhe Lin,\n  Philip Torr, Jiaya Jia", "title": "Open-World Entity Segmentation", "comments": "Project page: http://luqi.info/Entity_Web", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new image segmentation task, termed Entity Segmentation (ES)\nwith the aim to segment all visual entities in an image without considering\nsemantic category labels. It has many practical applications in image\nmanipulation/editing where the segmentation mask quality is typically crucial\nbut category labels are less important. In this setting, all\nsemantically-meaningful segments are equally treated as categoryless entities\nand there is no thing-stuff distinction. Based on our unified entity\nrepresentation, we propose a center-based entity segmentation framework with\ntwo novel modules to improve mask quality. Experimentally, both our new task\nand framework demonstrate superior advantages as against existing work. In\nparticular, ES enables the following: (1) merging multiple datasets to form a\nlarge training set without the need to resolve label conflicts; (2) any model\ntrained on one dataset can generalize exceptionally well to other datasets with\nunseen domains. Our code is made publicly available at\nhttps://github.com/dvlab-research/Entity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:59:05 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Qi", "Lu", ""], ["Kuen", "Jason", ""], ["Wang", "Yi", ""], ["Gu", "Jiuxiang", ""], ["Zhao", "Hengshuang", ""], ["Lin", "Zhe", ""], ["Torr", "Philip", ""], ["Jia", "Jiaya", ""]]}, {"id": "2107.14229", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati, Pietro Cerri, Raoul de Charette", "title": "Guided Disentanglement in Generative Networks", "comments": "Journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), thus lowering the translation quality and variability. In this\npaper, we present a comprehensive method for disentangling physics-based traits\nin the translation, guiding the learning process with neural or physical\nmodels. For the latter, we integrate adversarial estimation and genetic\nalgorithms to correctly achieve disentanglement. The results show our approach\ndramatically increase performances in many challenging scenarios for image\ntranslation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:59:31 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Pizzati", "Fabio", ""], ["Cerri", "Pietro", ""], ["de Charette", "Raoul", ""]]}, {"id": "2107.14230", "submitter": "Dongdong Chen", "authors": "Shuquan Ye and Dongdong Chen and Songfang Han and Jing Liao", "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation", "comments": "ICCV 2021 Oral, Relabeled ScanNetV2 and code are available at\n  https://shuquanye.com/PNAL_website/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the test set of a popular but noisy real-world\nscene dataset ScanNetV2 to make it clean, for rigorous experiment and future\nresearch. Our code and data will be available at\n\\url{https://shuquanye.com/PNAL_website/}.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:59:54 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ye", "Shuquan", ""], ["Chen", "Dongdong", ""], ["Han", "Songfang", ""], ["Liao", "Jing", ""]]}]