[{"id": "1208.0378", "submitter": "Charless Fowlkes", "authors": "Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes", "title": "Fast Planar Correlation Clustering for Image Segmentation", "comments": "This is the extended version of a paper to appear at the 12th\n  European Conference on Computer Vision (ECCV 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new optimization scheme for finding high-quality correlation\nclusterings in planar graphs that uses weighted perfect matching as a\nsubroutine. Our method provides lower-bounds on the energy of the optimal\ncorrelation clustering that are typically fast to compute and tight in\npractice. We demonstrate our algorithm on the problem of image segmentation\nwhere this approach outperforms existing global optimization techniques in\nminimizing the objective and is competitive with the state of the art in\nproducing high-quality segmentations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 00:54:02 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Yarkony", "Julian", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1208.0385", "submitter": "Ramakrishna Kakarala", "authors": "Ramakrishna Kakarala and Philip Ogunbona", "title": "A phase-sensitive method for filtering on the sphere", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2012.2213083", "report-no": null, "categories": "math.RT cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines filtering on a sphere, by first examining the roles of\nspherical harmonic magnitude and phase. We show that phase is more important\nthan magnitude in determining the structure of a spherical function. We examine\nthe properties of linear phase shifts in the spherical harmonic domain, which\nsuggest a mechanism for constructing finite-impulse-response (FIR) filters. We\nshow that those filters have desirable properties, such as being associative,\nmapping spherical functions to spherical functions, allowing directional\nfiltering, and being defined by relatively simple equations. We provide\nexamples of the filters for both spherical and manifold data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 02:31:35 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2012 07:58:36 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Kakarala", "Ramakrishna", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1208.0432", "submitter": "Ju Sun", "authors": "Ju Sun and Yuqian Zhang and John Wright", "title": "Efficient Point-to-Subspace Query in $\\ell^1$ with Application to Robust\n  Object Instance Recognition", "comments": "Revised based on reviewers' feedback; one new experiment on\n  synthesized data added; one section discussing the speed up added", "journal-ref": "SIAM Journal on Imaging Sciences, 7(4):2105 - 2138, 2014", "doi": "10.1137/130936166", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space, and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. In contrast to the naive exhaustive search which entails large-scale\nlinear programs, we show that the computational burden can be cut down\nsignificantly by a simple two-stage algorithm: (1) projecting the query and\ndata-base subspaces into lower-dimensional space by random Cauchy matrix, and\nsolving small-scale distance evaluations (linear programs) in the projection\nspace to locate candidate nearest; (2) with few candidates upon independent\nrepetition of (1), getting back to the high-dimensional space and performing\nexhaustive search. To preserve the identity of the nearest subspace with\nnontrivial probability, the projection dimension typically is low-order\npolynomial of the subspace dimension multiplied by logarithm of number of the\nsubspaces (Theorem 2.1). The reduced dimensionality and hence complexity\nrenders the proposed algorithm particularly relevant to vision application such\nas robust face and object instance recognition that we investigate empirically.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 08:43:45 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2013 19:59:11 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 06:12:11 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Sun", "Ju", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""]]}, {"id": "1208.0803", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Anamitra Bardhan Roy, Sayantan Dey", "title": "A Novel Approach of Color Image Hiding using RGB Color planes and DWT", "comments": "6 pages, 14 figures, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 36(5):19-24,\n  December 2011", "doi": "10.5120/4487-6316", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a wavelet based Steganographic technique for the color\nimage. The true color cover image and the true color secret image both are\ndecomposed into three separate color planes namely R, G and B. Each plane of\nthe images is decomposed into four sub bands using DWT. Each color plane of the\nsecret image is hidden by alpha blending technique in the corresponding sub\nbands of the respective color planes of the original image. During embedding,\nsecret image is dispersed within the original image depending upon the alpha\nvalue. Extraction of the secret image varies according to the alpha value. In\nthis approach the stego image generated is of acceptable level of\nimperceptibility and distortion compared to the cover image and the overall\nsecurity is high.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 17:41:55 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Dey", "Nilanjan", ""], ["Roy", "Anamitra Bardhan", ""], ["Dey", "Sayantan", ""]]}, {"id": "1208.0959", "submitter": "Misha Denil", "authors": "Misha Denil and Nando de Freitas", "title": "Recklessly Approximate Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been observed that certain extremely simple feature encoding\ntechniques are able to achieve state of the art performance on several standard\nimage classification benchmarks including deep belief networks, convolutional\nnets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders and\nseveral others. Moreover, these \"triangle\" or \"soft threshold\" encodings are\nex- tremely efficient to compute. Several intuitive arguments have been put\nforward to explain this remarkable performance, yet no mathematical\njustification has been offered.\n  The main result of this report is to show that these features are realized as\nan approximate solution to the a non-negative sparse coding problem. Using this\nconnection we describe several variants of the soft threshold features and\ndemonstrate their effectiveness on two image classification benchmark tasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2012 21:48:52 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2013 19:00:48 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Denil", "Misha", ""], ["de Freitas", "Nando", ""]]}, {"id": "1208.0967", "submitter": "Hema Swetha Koppula", "authors": "Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena", "title": "Human Activity Learning using Object Affordances from RGB-D Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities comprise several sub-activities performed in a sequence and\ninvolve interactions with various objects. This makes reasoning about the\nobject affordances a central task for activity recognition. In this work, we\nconsider the problem of jointly labeling the object affordances and human\nactivities from RGB-D videos. We frame the problem as a Markov Random Field\nwhere the nodes represent objects and sub-activities, and the edges represent\nthe relationships between object affordances, their relations with\nsub-activities, and their evolution over time. We formulate the learning\nproblem using a structural SVM approach, where labeling over various alternate\ntemporal segmentations are considered as latent variables. We tested our method\non a dataset comprising 120 activity videos collected from four subjects, and\nobtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the\nactivities.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2012 23:44:07 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Koppula", "Hema Swetha", ""], ["Gupta", "Rudhir", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1208.1670", "submitter": "Josphineleela Ramakrishnan", "authors": "Josphineleela Ramakrishnan, Ramakrishnan Malaisamy", "title": "Performance Measurement and Method Analysis (PMMA) for Fingerprint\n  Reconstruction", "comments": "4pages,1 figure,1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint reconstruction is one of the most well-known and publicized\nbiometrics. Because of their uniqueness and consistency over time, fingerprints\nhave been used for identification over a century, more recently becoming\nautomated due to advancements in computed capabilities. Fingerprint\nreconstruction is popular because of the inherent ease of acquisition, the\nnumerous sources (e.g. ten fingers) available for collection, and their\nestablished use and collections by law enforcement and immigration.\nFingerprints have always been the most practical and positive means of\nidentification. Offenders, being well aware of this, have been coming up with\nways to escape identification by that means. Erasing left over fingerprints,\nusing gloves, fingerprint forgery; are certain examples of methods tried by\nthem, over the years. Failing to prevent themselves, they moved to an extent of\nmutilating their finger skin pattern, to remain unidentified. This article is\nbased upon obliteration of finger ridge patterns and discusses some known cases\nin relation to the same, in chronological order; highlighting the reasons why\noffenders go to an extent of performing such act. The paper gives an overview\nof different methods and performance measurement of the fingerprint\nreconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 14:15:38 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Ramakrishnan", "Josphineleela", ""], ["Malaisamy", "Ramakrishnan", ""]]}, {"id": "1208.1672", "submitter": "Josphineleela Ramakrishnan", "authors": "Josphineleela Ramakrishnan, M. Ramakrishnan", "title": "An Efficient Automatic Attendance System Using Fingerprint\n  Reconstruction Technique", "comments": "6pages,5figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric time and attendance system is one of the most successful\napplications of biometric technology. One of the main advantage of a biometric\ntime and attendance system is it avoids \"buddy-punching\". Buddy punching was a\nmajor loophole which will be exploiting in the traditional time attendance\nsystems. Fingerprint recognition is an established field today, but still\nidentifying individual from a set of enrolled fingerprints is a time taking\nprocess. Most fingerprint-based biometric systems store the minutiae template\nof a user in the database. It has been traditionally assumed that the minutiae\ntemplate of a user does not reveal any information about the original\nfingerprint. This belief has now been shown to be false; several algorithms\nhave been proposed that can reconstruct fingerprint images from minutiae\ntemplates. In this paper, a novel fingerprint reconstruction algorithm is\nproposed to reconstruct the phase image, which is then converted into the\ngrayscale image. The proposed reconstruction algorithm reconstructs the phase\nimage from minutiae. The proposed reconstruction algorithm is used to automate\nthe whole process of taking attendance, manually which is a laborious and\ntroublesome work and waste a lot of time, with its managing and maintaining the\nrecords for a period of time is also a burdensome task. The proposed\nreconstruction algorithm has been evaluated with respect to the success rates\nof type-I attack (match the reconstructed fingerprint against the original\nfingerprint) and type-II attack (match the reconstructed fingerprint against\ndifferent impressions of the original fingerprint) using a commercial\nfingerprint recognition system. Given the reconstructed image from our\nalgorithm, we show that both types of attacks can be effectively launched\nagainst a fingerprint recognition system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 14:23:50 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Ramakrishnan", "Josphineleela", ""], ["Ramakrishnan", "M.", ""]]}, {"id": "1208.1679", "submitter": "Ou Wu", "authors": "Ou Wu", "title": "Color Assessment and Transfer for Web Pages", "comments": "10pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colors play a particularly important role in both designing and accessing Web\npages. A well-designed color scheme improves Web pages' visual aesthetic and\nfacilitates user interactions. As far as we know, existing color assessment\nstudies focus on images; studies on color assessment and editing for Web pages\nare rare. This paper investigates color assessment for Web pages based on\nexisting online color theme-rating data sets and applies this assessment to Web\ncolor edit. This study consists of three parts. First, we study the extraction\nof a Web page's color theme. Second, we construct color assessment models that\nscore the color compatibility of a Web page by leveraging machine learning\ntechniques. Third, we incorporate the learned color assessment model into a new\napplication, namely, color transfer for Web pages. Our study combines\ntechniques from computer graphics, Web mining, computer vision, and machine\nlearning. Experimental results suggest that our constructed color assessment\nmodels are effective, and useful in the color transfer for Web pages, which has\nreceived little attention in both Web mining and computer graphics communities.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2012 02:24:36 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Wu", "Ou", ""]]}, {"id": "1208.1880", "submitter": "Supreeth Rao", "authors": "Supreeth K. Rao (1), Arpitha Prasad B. (1), Anushree R. Shetty (1),\n  Chinmai (1), R. Bhakthavathsalam (2) and Rajeshwari Hegde (1) ((1) BMS\n  College of Engineering, Bangalore, India, (2) Indian Institute of Science,\n  Bangalore, India)", "title": "Stereo Acoustic Perception based on Real Time Video Acquisition for\n  Navigational Assistance", "comments": "12 pages, 8 figures, 1 table, SIPM-2012, pp. 97-108, 2012;\n  http://airccj.org/CSCP/vol2/csit2311.pdf", "journal-ref": null, "doi": "10.5121/csit.2012.2311", "report-no": null, "categories": "cs.CV cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A smart navigation system (an Electronic Travel Aid) based on an object\ndetection mechanism has been designed to detect the presence of obstacles that\nimmediately impede the path, by means of real time video processing. The\nalgorithm can be used for any general purpose navigational aid. This paper is\ndiscussed, keeping in mind the navigation of the visually impaired, and is not\nlimited to the same. A video camera feeds images of the surroundings to a Da-\nVinci Digital Media Processor, DM642, which works on the video, frame by frame.\nThe processor carries out image processing techniques whose result contains\ninformation about the object in terms of image pixels. The algorithm aims to\nselect the object which, among all others, poses maximum threat to the\nnavigation. A database containing a total of three sounds is constructed.\nHence, each image translates to a beep, where every beep informs the navigator\nof the obstacles directly in front of him. This paper implements an algorithm\nthat is more efficient as compared to its predecessors.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 11:46:10 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Rao", "Supreeth K.", ""], ["B.", "Arpitha Prasad", ""], ["Shetty", "Anushree R.", ""], ["Chinmai", "", ""], ["Bhakthavathsalam", "R.", ""], ["Hegde", "Rajeshwari", ""]]}, {"id": "1208.2092", "submitter": "Rajalakshmi  M", "authors": "M. Rajalakshmi, P. Subashini", "title": "A study on non-destructive method for detecting Toxin in pepper using\n  Neural networks", "comments": "11 pages,1 figure; International Journal of Artificial Intelligence &\n  Applications (IJAIA), Vol.3, No.4, July 2012", "journal-ref": null, "doi": "10.5121/ijaia.2012.3414", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mycotoxin contamination in certain agricultural systems have been a serious\nconcern for human and animal health. Mycotoxins are toxic substances produced\nmostly as secondary metabolites by fungi that grow on seeds and feed in the\nfield, or in storage. The food-borne Mycotoxins likely to be of greatest\nsignificance for human health in tropical developing countries are Aflatoxins\nand Fumonisins. Chili pepper is also prone to Aflatoxin contamination during\nharvesting, production and storage periods.Various methods used for detection\nof Mycotoxins give accurate results, but they are slow, expensive and\ndestructive. Destructive method is testing a material that degrades the sample\nunder investigation. Whereas, non-destructive testing will, after testing,\nallow the part to be used for its intended purpose. Ultrasonic methods,\nMultispectral image processing methods, Terahertz methods, X-ray and\nThermography have been very popular in nondestructive testing and\ncharacterization of materials and health monitoring. Image processing methods\nare used to improve the visual quality of the pictures and to extract useful\ninformation from them. In this proposed work, the chili pepper samples will be\ncollected, and the X-ray, multispectral images of the samples will be processed\nusing image processing methods. The term \"Computational Intelligence\" referred\nas simulation of human intelligence on computers. It is also called as\n\"Artificial Intelligence\" (AI) approach. The techniques used in AI approach are\nNeural network, Fuzzy logic and evolutionary computation. Finally, the\ncomputational intelligence method will be used in addition to image processing\nto provide best, high performance and accurate results for detecting the\nMycotoxin level in the samples collected.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 05:50:04 GMT"}], "update_date": "2012-08-13", "authors_parsed": [["Rajalakshmi", "M.", ""], ["Subashini", "P.", ""]]}, {"id": "1208.2128", "submitter": "V.p.gladis Pushparathi", "authors": "V. P. Gladis Pushpa Rathi and S. Palani", "title": "Brain tumor MRI image classification with feature selection and\n  extraction using linear discriminant analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a method of capturing visual content of an image. The\nfeature extraction is the process to represent raw image in its reduced form to\nfacilitate decision making such as pattern classification. We have tried to\naddress the problem of classification MRI brain images by creating a robust and\nmore accurate classifier which can act as an expert assistant to medical\npractitioners. The objective of this paper is to present a novel method of\nfeature selection and extraction. This approach combines the Intensity,\nTexture, shape based features and classifies the tumor as white matter, Gray\nmatter, CSF, abnormal and normal area. The experiment is performed on 140 tumor\ncontained brain MR images from the Internet Brain Segmentation Repository. The\nproposed technique has been carried out over a larger database as compare to\nany previous work and is more robust and effective. PCA and Linear Discriminant\nAnalysis (LDA) were applied on the training sets. The Support Vector Machine\n(SVM) classifier served as a comparison of nonlinear techniques Vs linear ones.\nPCA and LDA methods are used to reduce the number of features used. The feature\nselection using the proposed technique is more beneficial as it analyses the\ndata according to grouping class variable and gives reduced feature set with\nhigh classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 09:33:37 GMT"}], "update_date": "2012-08-13", "authors_parsed": [["Rathi", "V. P. Gladis Pushpa", ""], ["Palani", "S.", ""]]}, {"id": "1208.2651", "submitter": "Manuel J. A. Eugster", "authors": "Anne-Laure Boulesteix and Manuel J. A. Eugster", "title": "A Plea for Neutral Comparison Studies in Computational Sciences", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0061562", "report-no": null, "categories": "stat.CO cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context where most published articles are devoted to the development of\n\"new methods\", comparison studies are generally appreciated by readers but\nsurprisingly given poor consideration by many scientific journals. In\nconnection with recent articles on over-optimism and epistemology published in\nBioinformatics, this letter stresses the importance of neutral comparison\nstudies for the objective evaluation of existing methods and the establishment\nof standards by drawing parallels with clinical research.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:01:17 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Boulesteix", "Anne-Laure", ""], ["Eugster", "Manuel J. A.", ""]]}, {"id": "1208.2655", "submitter": "Mikhail Kharinov", "authors": "M. Kharinov", "title": "Stable Segmentation of Digital Image", "comments": "6 pages, 10 formulas, 2 figures, in russian", "journal-ref": "Proc. of the 22th Int. Conf. on Comp. Graphics and Vision\n  (Graphicon 2012), Moscow: MSU, 01-05 Oct. 2012, pp. 208-213. (in Russian)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper the optimal image segmentation by means of piecewise constant\napproximations is considered. The optimality is defined by a minimum value of\nthe total squared error or by equivalent value of standard deviation of the\napproximation from the image. The optimal approximations are defined\nindependently on the method of their obtaining and might be generated in\ndifferent algorithms. We investigate the computation of the optimal\napproximation on the grounds of stability with respect to a given set of\nmodifications. To obtain the optimal approximation the Mumford-Shuh model is\ngeneralized and developed, which in the computational part is combined with the\nOtsu method in multi-thresholding version. The proposed solution is proved\nanalytically and experimentally on the example of the standard image.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:21:05 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Kharinov", "M.", ""]]}, {"id": "1208.3133", "submitter": "Wajeb Gharibi", "authors": "Walaa M. Abd-Elhafiez, Wajeb Gharibi", "title": "Color Image Compression Algorithm Based on the DCT Blocks", "comments": "5 pages. arXiv admin note: text overlap with standard references on\n  JPEG without attribution", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 4, No 3, July 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the performance of different blockbased discrete cosine\ntransform (DCT) algorithms for compressing color image. In this RGB component\nof color image are converted to YCbCr before DCT transform is applied. Y is\nluminance component;Cb and Cr are chrominance components of the image. The\nmodification of the image data is done based on the classification of image\nblocks to edge blocks and non-edge blocks, then the edge block of the image is\ncompressed with low compression and the nonedge blocks is compressed with high\ncompression. The analysis results have indicated that the performance of the\nsuggested method is much better, where the constructed images are less\ndistorted and compressed with higher factor.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 14:51:45 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Abd-Elhafiez", "Walaa M.", ""], ["Gharibi", "Wajeb", ""]]}, {"id": "1208.3512", "submitter": "Toshiro Kubota", "authors": "Toshiro Kubota", "title": "Contour Completion Around a Fixation Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents two edge grouping algorithms for finding a closed contour\nstarting from a particular edge point and enclosing a fixation point. Both\nalgorithms search a shortest simple cycle in \\textit{an angularly ordered\ngraph} derived from an edge image where a vertex is an end point of a contour\nfragment and an undirected arc is drawn between a pair of end-points whose\nvisual angle from the fixation point is less than a threshold value, which is\nset to $\\pi/2$ in our experiments. The first algorithm restricts the search\nspace by disregarding arcs that cross the line extending from the fixation\npoint to the starting point. The second algorithm improves the solution of the\nfirst algorithm in a greedy manner. The algorithms were tested with a large\nnumber of natural images with manually placed fixation and starting points. The\nresults are promising.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 23:22:50 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Kubota", "Toshiro", ""]]}, {"id": "1208.3665", "submitter": "Christian Riess", "authors": "Vincent Christlein, Christian Riess, Johannes Jordan, Corinna Riess\n  and Elli Angelopoulou", "title": "An Evaluation of Popular Copy-Move Forgery Detection Approaches", "comments": "Main paper: 14 pages, supplemental material: 12 pages, main paper\n  appeared in IEEE Transaction on Information Forensics and Security", "journal-ref": "IEEE Transactions on Information Forensics and Security, volume 7,\n  number 6, 2012, pp. 1841-1854", "doi": "10.1109/TIFS.2012.2218597", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A copy-move forgery is created by copying and pasting content within the same\nimage, and potentially post-processing it. In recent years, the detection of\ncopy-move forgeries has become one of the most actively researched topics in\nblind image forensics. A considerable number of different algorithms have been\nproposed focusing on different types of postprocessed copies. In this paper, we\naim to answer which copy-move forgery detection algorithms and processing steps\n(e.g., matching, filtering, outlier detection, affine transformation\nestimation) perform best in various postprocessing scenarios. The focus of our\nanalysis is to evaluate the performance of previously proposed feature sets. We\nachieve this by casting existing algorithms in a common pipeline. In this\npaper, we examined the 15 most prominent feature sets. We analyzed the\ndetection performance on a per-image basis and on a per-pixel basis. We created\na challenging real-world copy-move dataset, and a software framework for\nsystematic image manipulation. Experiments show, that the keypoint-based\nfeatures SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA and\nZernike features perform very well. These feature sets exhibit the best\nrobustness against various noise sources and downsampling, while reliably\nidentifying the copied regions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 19:41:23 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 20:53:51 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Christlein", "Vincent", ""], ["Riess", "Christian", ""], ["Jordan", "Johannes", ""], ["Riess", "Corinna", ""], ["Angelopoulou", "Elli", ""]]}, {"id": "1208.3670", "submitter": "Qiong Liu ms.", "authors": "Qiong Liu", "title": "A Survey of Recent View-based 3D Model Retrieval Methods", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1207.7244 by\n  other author without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive research efforts have been dedicated to 3D model retrieval in\nrecent decades. Recently, view-based methods have attracted much research\nattention due to the high discriminative property of multi-views for 3D object\nrepresentation. In this report, we summarize the view-based 3D model methods\nand provide the further research trends. This paper focuses on the scheme for\nmatching between multiple views of 3D models and the application of\nbag-of-visual-words method in 3D model retrieval. For matching between multiple\nviews, the many-to-many matching, probabilistic matching and semisupervised\nlearning methods are introduced. For bag-of-visual-words application in 3D\nmodel retrieval, we first briefly review the bag-of-visual-words works on\nmultimedia and computer vision tasks, where the visual dictionary has been\ndetailed introduced. Then a series of 3D model retrieval methods by using\nbag-of-visual-words description are surveyed in this paper. At last, we\nsummarize the further research content in view-based 3D model retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 04:45:42 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Liu", "Qiong", ""]]}, {"id": "1208.3687", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Vishal M. Patel, Rama Chellappa", "title": "Information-theoretic Dictionary Learning for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a two-stage approach for learning dictionaries for object\nclassification tasks based on the principle of information maximization. The\nproposed method seeks a dictionary that is compact, discriminative, and\ngenerative. In the first stage, dictionary atoms are selected from an initial\ndictionary by maximizing the mutual information measure on dictionary\ncompactness, discrimination and reconstruction. In the second stage, the\nselected dictionary atoms are updated for improved reconstructive and\ndiscriminative power using a simple gradient ascent algorithm on mutual\ninformation. Experiments using real datasets demonstrate the effectiveness of\nour approach for image classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 20:38:56 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1208.3716", "submitter": "Jian Zhang", "authors": "Jian Zhang, Shaohui Liu, Debin Zhao, Ruiqin Xiong, Siwei Ma", "title": "Improved Total Variation based Image Compressive Sensing Recovery by\n  Nonlocal Regularization", "comments": "4 Pages, 1 figures, 3 tables, to be published at IEEE Int. Symposium\n  of Circuits and Systems (ISCAS) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, total variation (TV) based minimization algorithms have achieved\ngreat success in compressive sensing (CS) recovery for natural images due to\nits virtue of preserving edges. However, the use of TV is not able to recover\nthe fine details and textures, and often suffers from undesirable staircase\nartifact. To reduce these effects, this letter presents an improved TV based\nimage CS recovery algorithm by introducing a new nonlocal regularization\nconstraint into CS optimization problem. The nonlocal regularization is built\non the well known nonlocal means (NLM) filtering and takes advantage of\nself-similarity in images, which helps to suppress the staircase effect and\nrestore the fine details. Furthermore, an efficient augmented Lagrangian based\nalgorithm is developed to solve the above combined TV and nonlocal\nregularization constrained problem. Experimental results demonstrate that the\nproposed algorithm achieves significant performance improvements over the\nstate-of-the-art TV based algorithm in both PSNR and visual perception.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 01:48:05 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2012 00:47:04 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Zhang", "Jian", ""], ["Liu", "Shaohui", ""], ["Zhao", "Debin", ""], ["Xiong", "Ruiqin", ""], ["Ma", "Siwei", ""]]}, {"id": "1208.3723", "submitter": "Jian Zhang", "authors": "Jian Zhang, Chen Zhao, Ruiqin Xiong, Siwei Ma, and Debin Zhao", "title": "Image Super-Resolution via Dual-Dictionary Learning And Sparse\n  Representation", "comments": "4 pages, 4 figures, 1 table, to be published at IEEE Int. Symposium\n  of Circuits and Systems (ISCAS) 2012", "journal-ref": null, "doi": "10.1109/ISCAS.2012.6271583", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based image super-resolution aims to reconstruct high-frequency (HF)\ndetails from the prior model trained by a set of high- and low-resolution image\npatches. In this paper, HF to be estimated is considered as a combination of\ntwo components: main high-frequency (MHF) and residual high-frequency (RHF),\nand we propose a novel image super-resolution method via dual-dictionary\nlearning and sparse representation, which consists of the main dictionary\nlearning and the residual dictionary learning, to recover MHF and RHF\nrespectively. Extensive experimental results on test images validate that by\nemploying the proposed two-layer progressive scheme, more image details can be\nrecovered and much better results can be achieved than the state-of-the-art\nalgorithms in terms of both PSNR and visual perception.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 03:00:03 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Jian", ""], ["Zhao", "Chen", ""], ["Xiong", "Ruiqin", ""], ["Ma", "Siwei", ""], ["Zhao", "Debin", ""]]}, {"id": "1208.3822", "submitter": "Jim Jing-Yan Wang", "authors": "Jingyan Wang", "title": "Joint-ViVo: Selecting and Weighting Visual Words Jointly for\n  Bag-of-Features based Tissue Classification in Medical Images", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically classifying the tissues types of Region of Interest (ROI) in\nmedical imaging has been an important application in Computer-Aided Diagnosis\n(CAD), such as classification of breast parenchymal tissue in the mammogram,\nclassify lung disease patterns in High-Resolution Computed Tomography (HRCT)\netc. Recently, bag-of-features method has shown its power in this field,\ntreating each ROI as a set of local features. In this paper, we investigate\nusing the bag-of-features strategy to classify the tissue types in medical\nimaging applications. Two important issues are considered here: the visual\nvocabulary learning and weighting. Although there are already plenty of\nalgorithms to deal with them, all of them treat them independently, namely, the\nvocabulary learned first and then the histogram weighted. Inspired by\nAuto-Context who learns the features and classifier jointly, we try to develop\na novel algorithm that learns the vocabulary and weights jointly. The new\nalgorithm, called Joint-ViVo, works in an iterative way. In each iteration, we\nfirst learn the weights for each visual word by maximizing the margin of ROI\ntriplets, and then select the most discriminate visual words based on the\nlearned weights for the next iteration. We test our algorithm on three tissue\nclassification tasks: identifying brain tissue type in magnetic resonance\nimaging (MRI), classifying lung tissue in HRCT images, and classifying breast\ntissue density in mammograms. The results show that Joint-ViVo can perform\neffectively for classifying tissues.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 11:12:44 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2013 07:30:44 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Wang", "Jingyan", ""]]}, {"id": "1208.3839", "submitter": "Jing-Yan Wang", "authors": "Jing-Yan Wang", "title": "Discriminative Sparse Coding on Multi-Manifold for Data Representation\n  and Classification", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding has been popularly used as an effective data representation\nmethod in various applications, such as computer vision, medical imaging and\nbioinformatics, etc. However, the conventional sparse coding algorithms and its\nmanifold regularized variants (graph sparse coding and Laplacian sparse\ncoding), learn the codebook and codes in a unsupervised manner and neglect the\nclass information available in the training set. To address this problem, in\nthis paper we propose a novel discriminative sparse coding method based on\nmulti-manifold, by learning discriminative class-conditional codebooks and\nsparse codes from both data feature space and class labels. First, the entire\ntraining set is partitioned into multiple manifolds according to the class\nlabels. Then, we formulate the sparse coding as a manifold-manifold matching\nproblem and learn class-conditional codebooks and codes to maximize the\nmanifold margins of different classes. Lastly, we present a data point-manifold\nmatching error based strategy to classify the unlabeled data point.\nExperimental results on somatic mutations identification and breast tumors\nclassification in ultrasonic images tasks demonstrate the efficacy of the\nproposed data representation-classification approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 14:49:27 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 14:21:40 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wang", "Jing-Yan", ""]]}, {"id": "1208.3845", "submitter": "Jing-Yan Wang", "authors": "Jing-Yan Wang and Mustafa AbdulJabbar", "title": "Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix\n  Factorization", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) has been continuously evolving in\nseveral areas like pattern recognition and information retrieval methods. It\nfactorizes a matrix into a product of 2 low-rank non-negative matrices that\nwill define parts-based, and linear representation of nonnegative data.\nRecently, Graph regularized NMF (GrNMF) is proposed to find a compact\nrepresentation,which uncovers the hidden semantics and simultaneously respects\nthe intrinsic geometric structure. In GNMF, an affinity graph is constructed\nfrom the original data space to encode the geometrical information. In this\npaper, we propose a novel idea which engages a Multiple Kernel Learning\napproach into refining the graph structure that reflects the factorization of\nthe matrix and the new data space. The GrNMF is improved by utilizing the graph\nrefined by the kernel learning, and then a novel kernel learning method is\nintroduced under the GrNMF framework. Our approach shows encouraging results of\nthe proposed algorithm in comparison to the state-of-the-art clustering\nalgorithms like NMF, GrNMF, SVD etc.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 15:21:09 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2012 07:24:22 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2013 14:21:50 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wang", "Jing-Yan", ""], ["AbdulJabbar", "Mustafa", ""]]}, {"id": "1208.3901", "submitter": "Igor  G. Olaizola", "authors": "Igor G. Olaizola and Marco Quartulli and Julian Florez and Basilio\n  Sierra", "title": "Trace transform based method for color image domain identification", "comments": "This paper has been momentaneously withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context categorization is a fundamental pre-requisite for multi-domain\nmultimedia content analysis applications in order to manage contextual\ninformation in an efficient manner. In this paper, we introduce a new color\nimage context categorization method (DITEC) based on the trace transform. The\nproblem of dimensionality reduction of the obtained trace transform signal is\naddressed through statistical descriptors that keep the underlying information.\nThese extracted features offer a highly discriminant behavior for content\ncategorization. The theoretical properties of the method are analyzed and\nvalidated experimentally through two different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 22:21:19 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 16:49:30 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 12:48:59 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Olaizola", "Igor G.", ""], ["Quartulli", "Marco", ""], ["Florez", "Julian", ""], ["Sierra", "Basilio", ""]]}, {"id": "1208.4316", "submitter": "Sreeraj M", "authors": "Sreeraj.M and Sumam Mary Idicula", "title": "An Online Character Recognition System to Convert Grantha Script to\n  Malayalam", "comments": "6 pages, 6 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA, Volume 3 Issue 7, 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to recognize Grantha, an ancient script\nin South India and converting it to Malayalam, a prevalent language in South\nIndia using online character recognition mechanism. The motivation behind this\nwork owes its credit to (i) developing a mechanism to recognize Grantha script\nin this modern world and (ii) affirming the strong connection among Grantha and\nMalayalam. A framework for the recognition of Grantha script using online\ncharacter recognition is designed and implemented. The features extracted from\nthe Grantha script comprises mainly of time-domain features based on writing\ndirection and curvature. The recognized characters are mapped to corresponding\nMalayalam characters. The framework was tested on a bed of medium length\nmanuscripts containing 9-12 sample lines and printed pages of a book titled\nSoundarya Lahari writtenin Grantha by Sri Adi Shankara to recognize the words\nand sentences. The manuscript recognition rates with the system are for Grantha\nas 92.11%, Old Malayalam 90.82% and for new Malayalam script 89.56%. The\nrecognition rates of pages of the printed book are for Grantha as 96.16%, Old\nMalayalam script 95.22% and new Malayalam script as 92.32% respectively. These\nresults show the efficiency of the developed system.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 17:40:15 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["M", "Sreeraj.", ""], ["Idicula", "Sumam Mary", ""]]}, {"id": "1208.4384", "submitter": "Joshua Chang", "authors": "Joshua C. Chang and Tom Chou", "title": "Iterative graph cuts for image segmentation with a nonlinear statistical\n  shape prior", "comments": "Revision submitted to JMIV (02/24/13)", "journal-ref": null, "doi": "10.1007/s10851-013-0440-9", "report-no": null, "categories": "cs.CV math.OC physics.data-an q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape-based regularization has proven to be a useful method for delineating\nobjects within noisy images where one has prior knowledge of the shape of the\ntargeted object. When a collection of possible shapes is available, the\nspecification of a shape prior using kernel density estimation is a natural\ntechnique. Unfortunately, energy functionals arising from kernel density\nestimation are of a form that makes them impossible to directly minimize using\nefficient optimization algorithms such as graph cuts. Our main contribution is\nto show how one may recast the energy functional into a form that is\nminimizable iteratively and efficiently using graph cuts.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 20:50:40 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 13:13:49 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Chang", "Joshua C.", ""], ["Chou", "Tom", ""]]}, {"id": "1208.4391", "submitter": "Ganesh Sundaramoorthi", "authors": "Yanchao Yang and Ganesh Sundaramoorthi", "title": "Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev\n  Descent", "comments": "Extension of ICCV paper, added coarse-to-fine optimization based on\n  new Riemannian manifold of parameterized regions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to track the precise shape of an object in video based on\nnew modeling and optimization on a new Riemannian manifold of parameterized\nregions.\n  Joint dynamic shape and appearance models, in which a template of the object\nis propagated to match the object shape and radiance in the next frame, are\nadvantageous over methods employing global image statistics in cases of complex\nobject radiance and cluttered background. In cases of 3D object motion and\nviewpoint change, self-occlusions and dis-occlusions of the object are\nprominent, and current methods employing joint shape and appearance models are\nunable to adapt to new shape and appearance information, leading to inaccurate\nshape detection. In this work, we model self-occlusions and dis-occlusions in a\njoint shape and appearance tracking framework.\n  Self-occlusions and the warp to propagate the template are coupled, thus a\njoint problem is formulated. We derive a coarse-to-fine optimization scheme,\nadvantageous in object tracking, that initially perturbs the template by coarse\nperturbations before transitioning to finer-scale perturbations, traversing all\nscales, seamlessly and automatically. The scheme is a gradient descent on a\nnovel infinite-dimensional Riemannian manifold that we introduce. The manifold\nconsists of planar parameterized regions, and the metric that we introduce is a\nnovel Sobolev-type metric defined on infinitesimal vector fields on regions.\nThe metric has the property of resulting in a gradient descent that\nautomatically favors coarse-scale deformations (when they reduce the energy)\nbefore moving to finer-scale deformations.\n  Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and\nbackground show that occlusion/dis-occlusion modeling leads to superior shape\naccuracy compared to recent methods employing joint shape/appearance models or\nemploying global statistics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 21:37:15 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 16:48:57 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Yang", "Yanchao", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "1208.4398", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Rama Chellappa", "title": "A Unified Approach for Modeling and Recognition of Individual Actions\n  and Group Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing group activities is challenging due to the difficulties in\nisolating individual entities, finding the respective roles played by the\nindividuals and representing the complex interactions among the participants.\nIndividual actions and group activities in videos can be represented in a\ncommon framework as they share the following common feature: both are composed\nof a set of low-level features describing motions, e.g., optical flow for each\npixel or a trajectory for each feature point, according to a set of composition\nconstraints in both temporal and spatial dimensions. In this paper, we present\na unified model to assess the similarity between two given individual or group\nactivities. Our approach avoids explicit extraction of individual actors,\nidentifying and representing the inter-person interactions. With the proposed\napproach, retrieval from a video database can be performed through\nQuery-by-Example; and activities can be recognized by querying videos\ncontaining known activities. The suggested video matching process can be\nperformed in an unsupervised manner. We demonstrate the performance of our\napproach by recognizing a set of human actions and football plays.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 22:40:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Chellappa", "Rama", ""]]}, {"id": "1208.4662", "submitter": "Dandan Hu", "authors": "Dandan Hu, Pinaki Sarder, Peter Ronhovde, Sandra Orthaus, Samuel\n  Achilefu, Zohar Nussinov", "title": "Automatic Segmentation of Fluorescence Lifetime Microscopy Images of\n  Cells Using Multi-Resolution Community Detection", "comments": "21 pages, 6 figures", "journal-ref": "Journal of Microscopy Volume 253, Issue 1, pages 54 - 64, 2014", "doi": "10.1111/jmi.12097", "report-no": null, "categories": "physics.med-ph cond-mat.stat-mech cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an automatic method for segmenting fluorescence lifetime\n(FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolution\ncommunity detection (MCD) based network segmentation method. The image\nprocessing problem is framed as identifying segments with respective average\nFLTs against a background in FLIM images. The proposed method segments a FLIM\nimage for a given resolution of the network composed using image pixels as the\nnodes and similarity between the pixels as the edges. In the resulting\nsegmentation, low network resolution leads to larger segments and high network\nresolution leads to smaller segments. Further, the mean-square error (MSE) in\nestimating the FLT segments in a FLIM image using the proposed method was found\nto be consistently decreasing with increasing resolution of the corresponding\nnetwork. The proposed MCD method outperformed a popular spectral clustering\nbased method in performing FLIM image segmentation. The spectral segmentation\nmethod introduced noisy segments in its output at high resolution. It was\nunable to offer a consistent decrease in MSE with increasing resolution.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2012 02:56:32 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 01:40:58 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Hu", "Dandan", ""], ["Sarder", "Pinaki", ""], ["Ronhovde", "Peter", ""], ["Orthaus", "Sandra", ""], ["Achilefu", "Samuel", ""], ["Nussinov", "Zohar", ""]]}, {"id": "1208.4842", "submitter": "Firouz AL-Wassai", "authors": "Firouz Abdullah Al-Wassai, N. V. Kalyankar", "title": "The Segmentation Fusion Method On10 Multi-Sensors", "comments": "http://www.ijltemas.in/new-icae-2012-?df=1&t=1345561079771", "journal-ref": "International Journal of Latest Technology in\n  Engineering,Management & Applied Science (IJLTEMAS),Vol. I, Issue V, 2012,\n  124-138", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most significant problem may be undesirable effects for the spectral\nsignatures of fused images as well as the benefits of using fused images mostly\ncompared to their source images were acquired at the same time by one sensor.\nThey may or may not be suitable for the fusion of other images. It becomes\ntherefore increasingly important to investigate techniques that allow\nmulti-sensor, multi-date image fusion to make final conclusions can be drawn on\nthe most suitable method of fusion. So, In this study we present a new method\nSegmentation Fusion method (SF) for remotely sensed images is presented by\nconsidering the physical characteristics of sensors, which uses a feature level\nprocessing paradigm. In a particularly, attempts to test the proposed method\nperformance on 10 multi-sensor images and comparing it with different fusion\ntechniques for estimating the quality and degree of information improvement\nquantitatively by using various spatial and spectral metrics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2012 14:55:30 GMT"}], "update_date": "2012-08-27", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""]]}, {"id": "1208.5016", "submitter": "Ender Konukoglu", "authors": "Ender Konukoglu, Ben Glocker, Antonio Criminisi and Kilian M. Pohl", "title": "WESD - Weighted Spectral Distance for Measuring Shape Dissimilarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new distance for measuring shape dissimilarity\nbetween objects. Recent publications introduced the use of eigenvalues of the\nLaplace operator as compact shape descriptors. Here, we revisit the eigenvalues\nto define a proper distance, called Weighted Spectral Distance (WESD), for\nquantifying shape dissimilarity. The definition of WESD is derived through\nanalysing the heat-trace. This analysis provides the proposed distance an\nintuitive meaning and mathematically links it to the intrinsic geometry of\nobjects. We analyse the resulting distance definition, present and prove its\nimportant theoretical properties. Some of these properties include: i) WESD is\ndefined over the entire sequence of eigenvalues yet it is guaranteed to\nconverge, ii) it is a pseudometric, iii) it is accurately approximated with a\nfinite number of eigenvalues, and iv) it can be mapped to the [0,1) interval.\nLastly, experiments conducted on synthetic and real objects are presented.\nThese experiments highlight the practical benefits of WESD for applications in\nvision and medical image analysis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2012 17:38:46 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Konukoglu", "Ender", ""], ["Glocker", "Ben", ""], ["Criminisi", "Antonio", ""], ["Pohl", "Kilian M.", ""]]}, {"id": "1208.5092", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xiaogang Wang, Deli Zhao and Xiaoou Tang", "title": "Graph Degree Linkage: Agglomerative Clustering on a Directed Graph", "comments": "Proceedings of European Conference on Computer Vision (ECCV), 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple but effective graph-based agglomerative\nalgorithm, for clustering high-dimensional data. We explore the different roles\nof two fundamental concepts in graph theory, indegree and outdegree, in the\ncontext of clustering. The average indegree reflects the density near a sample,\nand the average outdegree characterizes the local geometry around a sample.\nBased on such insights, we define the affinity measure of clusters via the\nproduct of average indegree and average outdegree. The product-based affinity\nmakes our algorithm robust to noise. The algorithm has three main advantages:\ngood performance, easy implementation, and high computational efficiency. We\ntest the algorithm on two fundamental computer vision problems: image\nclustering and object matching. Extensive experiments demonstrate that it\noutperforms the state-of-the-arts in both applications.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2012 02:51:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Zhang", "Wei", ""], ["Wang", "Xiaogang", ""], ["Zhao", "Deli", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1208.5365", "submitter": "Salah A. Aly", "authors": "Salah A. Aly", "title": "A Missing and Found Recognition System for Hajj and Umrah", "comments": "website available via http://www.mfhajj.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2012 11:20:49 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Aly", "Salah A.", ""]]}, {"id": "1208.5451", "submitter": "Zhongwei  TANG", "authors": "Zhongwei Tang, Alexey Castrodad, Mariano Tepper and Guillermo Sapiro", "title": "Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity\n  Analysis from a Single Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for unsupervised group activity analysis from a single video is\nhere presented. Our working hypothesis is that human actions lie on a union of\nlow-dimensional subspaces, and thus can be efficiently modeled as sparse linear\ncombinations of atoms from a learned dictionary representing the action's\nprimitives. Contrary to prior art, and with the primary goal of spatio-temporal\naction grouping, in this work only one single video segment is available for\nboth unsupervised learning and analysis without any prior training information.\nAfter extracting simple features at a single spatio-temporal scale, we learn a\ndictionary for each individual in the video during each short time lapse. These\ndictionaries allow us to compare the individuals' actions by producing an\naffinity matrix which contains sufficient discriminative information about the\nactions in the scene leading to grouping with simple and efficient tools. With\ndiverse publicly available real videos, we demonstrate the effectiveness of the\nproposed framework and its robustness to cluttered backgrounds, changes of\nhuman appearance, and action variability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2012 17:21:39 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Tang", "Zhongwei", ""], ["Castrodad", "Alexey", ""], ["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1208.5842", "submitter": "Kieran Larkin", "authors": "Kieran G. Larkin, Peter A. Fletcher and Stephen J. Hardy", "title": "Tenacious tagging of images via Mellin monomials", "comments": "Previous MS rejected as the journal no longer accepts purely image\n  processing articles. Reworked with emphasis on the embedded patterns: Mellin\n  monomials, which underpin optical rotation and scale invariant pattern\n  recognition. Noted visual imperceptibility. Kept animation sequence showing\n  how correlation detection survives image distortions. Submitted to an \"optics\n  and image science\" journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for attaching persistent metadata to an image. The\nmethod can be interpreted as a template-based blind watermarking scheme, robust\nto common editing operations, namely: cropping, rotation, scaling, stretching,\nshearing, compression, printing, scanning, noise, and color removal. Robustness\nis achieved through the reciprocity of the embedding and detection invariants.\nThe embedded patterns are real onedimensional Mellin monomial patterns\ndistributed over two-dimensions. The embedded patterns are scale invariant and\ncan be directly embedded in an image by simple pixel addition. Detection\nachieves rotation and general affine invariance by signal projection using\nimplicit Radon transformation. Embedded signals contract to one-dimension in\nthe two-dimensional Fourier polar domain. The real signals are detected by\ncorrelation with complex Mellin monomial templates. Using a unique template of\n4 chirp patterns we detect the affine signature with exquisite sensitivity and\nmoderate security. The practical implementation achieves efficiencies through\nfast Fourier transform (FFT) correspondences such as the projection-slice\ntheorem, the FFT correlation relation, and fast resampling via the chirp-z\ntransform. The overall method utilizes orthodox spread spectrum patterns for\nthe payload and performs well in terms of the classic\nrobustness-capacity-visibility performance triangle. Tags are entirely\nimperceptible with a mean SSIM greater than 0.988 in all cases tested.\nWatermarked images survive almost all Stirmark attacks. The method is ideal for\nattaching metadata robustly to both digital and analogue images.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2012 06:02:55 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 01:12:55 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2012 23:32:15 GMT"}, {"version": "v4", "created": "Fri, 6 Jun 2014 01:32:12 GMT"}, {"version": "v5", "created": "Mon, 16 Jun 2014 03:26:05 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Larkin", "Kieran G.", ""], ["Fletcher", "Peter A.", ""], ["Hardy", "Stephen J.", ""]]}, {"id": "1208.6137", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, M N Anil Prasad and A G Ramakrishnan", "title": "Benchmarking recognition results on word image datasets", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have benchmarked the maximum obtainable recognition accuracy on various\nword image datasets using manual segmentation and a currently available\ncommercial OCR. We have developed a Matlab program, with graphical user\ninterface, for semi-automated pixel level segmentation of word images. We\ndiscuss the advantages of pixel level annotation. We have covered five\ndatabases adding up to over 3600 word images. These word images have been\ncropped from camera captured scene, born-digital and street view images. We\nrecognize the segmented word image using the trial version of Nuance Omnipage\nOCR. We also discuss, how the degradations introduced during acquisition or\ninaccuracies introduced during creation of word images affect the recognition\nof the word present in the image. Word images for different kinds of\ndegradations and correction for slant and curvy nature of words are also\ndiscussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,\nStreet view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,\n88.5% and 86.7% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 11:24:44 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Kumar", "Deepak", ""], ["Prasad", "M N Anil", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1208.6335", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Sushmit Mallik and Ravdeep Johar", "title": "Comparative Study and Optimization of Feature-Extraction Techniques for\n  Content based Image Retrieval", "comments": "8 pages, 16 figures, 11 tables", "journal-ref": "International Journal of Computer Applications 52(20):35-42, 2012", "doi": "10.5120/8320-1959", "report-no": "Volume 52, Number 20, 2012", "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query\nby Image Content (QBIC), is to help users to retrieve relevant images based on\ntheir contents. CBIR technologies provide a method to find images in large\ndatabases by using unique descriptors from a trained image. The image\ndescriptors include texture, color, intensity and shape of the object inside an\nimage. Several feature-extraction techniques viz., Average RGB, Color Moments,\nCo-occurrence, Local Color Histogram, Global Color Histogram and Geometric\nMoment have been critically compared in this paper. However, individually these\ntechniques result in poor performance. So, combinations of these techniques\nhave also been evaluated and results for the most efficient combination of\ntechniques have been presented and optimized for each class of image query. We\nalso propose an improvement in image retrieval performance by introducing the\nidea of Query modification through image cropping. It enables the user to\nidentify a region of interest and modify the initial query to refine and\npersonalize the image retrieval results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 23:50:06 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 01:34:05 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chadha", "Aman", ""], ["Mallik", "Sushmit", ""], ["Johar", "Ravdeep", ""]]}, {"id": "1208.6516", "submitter": "Joseph  Salmon", "authors": "Joseph Salmon and Rebecca Willett and Ery Arias-Castro", "title": "A two-stage denoising filter: the preprocessed Yaroslavsky filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple image noise removal method which combines a\npreprocessing step with the Yaroslavsky filter for strong numerical, visual,\nand theoretical performance on a broad class of images. The framework developed\nis a two-stage approach. In the first stage the image is filtered with a\nclassical denoising method (e.g., wavelet or curvelet thresholding). In the\nsecond stage a modification of the Yaroslavsky filter is performed on the\noriginal noisy image, where the weights of the filters are governed by pixel\nsimilarities in the denoised image from the first stage. Similar prefiltering\nideas have proved effective previously in the literature, and this paper\nprovides theoretical guarantees and important insight into why prefiltering can\nbe effective. Empirically, this simple approach achieves very good performance\nfor cartoon images, and can be computed much more quickly than current\npatch-based denoising algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 15:08:22 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Salmon", "Joseph", ""], ["Willett", "Rebecca", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "1208.6523", "submitter": "David Guenther", "authors": "Jan Reininghaus (1) and David G\\\"unther (2) and Ingrid Hotz (3) and\n  Tino Weinkauf (2) and Hans Peter Seidel (2) ((1) Institute for Science and\n  Technology Austria, (2) MPI for Informatics Germany, (3) Zuse-Insitute Berlin\n  Germany)", "title": "Combinatorial Gradient Fields for 2D Images with Empirically Convergent\n  Separatrices", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient probabilistic method that computes\ncombinatorial gradient fields for two dimensional image data. In contrast to\nexisting algorithms, this approach yields a geometric Morse-Smale complex that\nconverges almost surely to its continuous counterpart when the image resolution\nis increased. This approach is motivated using basic ideas from probability\ntheory and builds upon an algorithm from discrete Morse theory with a strong\nmathematical foundation. While a formal proof is only hinted at, we do provide\na thorough numerical evaluation of our method and compare it to established\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 15:19:09 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Reininghaus", "Jan", ""], ["G\u00fcnther", "David", ""], ["Hotz", "Ingrid", ""], ["Weinkauf", "Tino", ""], ["Seidel", "Hans Peter", ""]]}]