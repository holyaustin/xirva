[{"id": "1005.0069", "submitter": "Yair Censor", "authors": "Y. Censor, R. Davidi and G.T. Herman", "title": "Perturbation Resilience and Superiorization of Iterative Algorithms", "comments": "Accepted for publication in Inverse Problems, 2010", "journal-ref": "Inverse Problems, 26 (2010) 065008 (12pp)", "doi": "10.1088/0266-5611/26/6/065008", "report-no": null, "categories": "math.OC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative algorithms aimed at solving some problems are discussed. For\ncertain problems, such as finding a common point in the intersection of a\nfinite number of convex sets, there often exist iterative algorithms that\nimpose very little demand on computer resources. For other problems, such as\nfinding that point in the intersection at which the value of a given function\nis optimal, algorithms tend to need more computer memory and longer execution\ntime. A methodology is presented whose aim is to produce automatically for an\niterative algorithm of the first kind a \"superiorized version\" of it that\nretains its computational efficiency but nevertheless goes a long way towards\nsolving an optimization problem. This is possible to do if the original\nalgorithm is \"perturbation resilient,\" which is shown to be the case for\nvarious projection algorithms for solving the consistent convex feasibility\nproblem. The superiorized versions of such algorithms use perturbations that\ndrive the process in the direction of the optimizer of the given function.\nAfter presenting these intuitive ideas in a precise mathematical form, they are\nillustrated in image reconstruction from projections for two different\nprojection algorithms superiorized for the function whose value is the total\nvariation of the image.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 13:14:44 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Censor", "Y.", ""], ["Davidi", "R.", ""], ["Herman", "G. T.", ""]]}, {"id": "1005.0527", "submitter": "Kostadin Koroutchev", "authors": "Kostadin Koroutchev and Elka Korutcheva", "title": "Detecting the Most Unusual Part of Two and Three-dimensional Digital\n  Images", "comments": "16 pages", "journal-ref": "Pattern Recognition 42(8): 1684-1692 (2009)", "doi": null, "report-no": null, "categories": "physics.data-an cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce an algorithm that can detect the\nmost unusual part of a digital image in probabilistic setting. The most unusual\npart of a given shape is defined as a part of the image that has the maximal\ndistance to all non intersecting shapes with the same form. The method is\ntested on two and three-dimensional images and has shown very good results\nwithout any predefined model. A version of the method independent of the\ncontrast of the image is considered and is found to be useful for finding the\nmost unusual part (and the most similar part) of the image conditioned on given\nimage. The results can be used to scan large image databases, as for example\nmedical databases.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 19:56:59 GMT"}, {"version": "v2", "created": "Wed, 5 May 2010 05:49:40 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Koroutchev", "Kostadin", ""], ["Korutcheva", "Elka", ""]]}, {"id": "1005.0858", "submitter": "Gilad Lerman Dr", "authors": "Teng Zhang, Arthur Szlam, Yi Wang and Gilad Lerman", "title": "Randomized hybrid linear modeling by local best-fit flats", "comments": "To appear in the proceedings of CVPR 2010", "journal-ref": "2010 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) (13-18 June 2010), pp. 1927-1934", "doi": "10.1109/CVPR.2010.5539866", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hybrid linear modeling problem is to identify a set of d-dimensional\naffine sets in a D-dimensional Euclidean space. It arises, for example, in\nobject tracking and structure from motion. The hybrid linear model can be\nconsidered as the second simplest (behind linear) manifold model of data. In\nthis paper we will present a very simple geometric method for hybrid linear\nmodeling based on selecting a set of local best fit flats that minimize a\nglobal l1 error measure. The size of the local neighborhoods is determined\nautomatically by the Jones' l2 beta numbers; it is proven under certain\ngeometric conditions that good local neighborhoods exist and are found by our\nmethod. We also demonstrate how to use this algorithm for fast determination of\nthe number of affine subspaces. We give extensive experimental evidence\ndemonstrating the state of the art accuracy and speed of the algorithm on\nsynthetic and real hybrid linear data.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 21:46:13 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Zhang", "Teng", ""], ["Szlam", "Arthur", ""], ["Wang", "Yi", ""], ["Lerman", "Gilad", ""]]}, {"id": "1005.0907", "submitter": "Rdv Ijcsis", "authors": "Yasser M. Alginaih, Abdul Ahad Siddiqi", "title": "Multistage Hybrid Arabic/Indian Numeral OCR System", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The use of OCR in postal services is not yet universal and there are still\nmany countries that process mail sorting manually. Automated Arabic/Indian\nnumeral Optical Character Recognition (OCR) systems for Postal services are\nbeing used in some countries, but still there are errors during the mail\nsorting process, thus causing a reduction in efficiency. The need to\ninvestigate fast and efficient recognition algorithms/systems is important so\nas to correctly read the postal codes from mail addresses and to eliminate any\nerrors during the mail sorting stage. The objective of this study is to\nrecognize printed numerical postal codes from mail addresses. The proposed\nsystem is a multistage hybrid system which consists of three different feature\nextraction methods, i.e., binary, zoning, and fuzzy features, and three\ndifferent classifiers, i.e., Hamming Nets, Euclidean Distance, and Fuzzy Neural\nNetwork Classifiers. The proposed system, systematically compares the\nperformance of each of these methods, and ensures that the numerals are\nrecognized correctly. Comprehensive results provide a very high recognition\nrate, outperforming the other known developed methods in literature.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 07:25:23 GMT"}], "update_date": "2010-05-07", "authors_parsed": [["Alginaih", "Yasser M.", ""], ["Siddiqi", "Abdul Ahad", ""]]}, {"id": "1005.0945", "submitter": "Rdv Ijcsis", "authors": "Mohit Soni, Sandesh Gupta, M.S. Rao, Phalguni Gupta", "title": "An Efficient Vein Pattern-based Recognition System", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": "10.1109/SECURWARE.2010.45", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents an efficient human recognition system based on vein\npattern from the palma dorsa. A new absorption based technique has been\nproposed to collect good quality images with the help of a low cost camera and\nlight source. The system automatically detects the region of interest from the\nimage and does the necessary preprocessing to extract features. A Euclidean\nDistance based matching technique has been used for making the decision. It has\nbeen tested on a data set of 1750 image samples collected from 341 individuals.\nThe accuracy of the verification system is found to be 99.26% with false\nrejection rate (FRR) of 0.03%.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 09:34:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Soni", "Mohit", ""], ["Gupta", "Sandesh", ""], ["Rao", "M. S.", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1005.1471", "submitter": "Karin Schnass", "authors": "Karin Schnass and Pierre Vandergheynst", "title": "Classification via Incoherent Subspaces", "comments": "22 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new classification framework that can extract\nindividual features per class. The scheme is based on a model of incoherent\nsubspaces, each one associated to one class, and a model on how the elements in\na class are represented in this subspace. After the theoretical analysis an\nalternate projection algorithm to find such a collection is developed. The\nclassification performance and speed of the proposed method is tested on the AR\nand YaleB databases and compared to that of Fisher's LDA and a recent approach\nbased on on $\\ell_1$ minimisation. Finally connections of the presented scheme\nto already existing work are discussed and possible ways of extensions are\npointed out.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 08:49:56 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Schnass", "Karin", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1005.2638", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "Hierarchical Clustering for Finding Symmetries and Other Patterns in\n  Massive, High Dimensional Datasets", "comments": "41 pages, 13 figures, 6 tables. 81 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. \"Structure\" can be understood as\nsymmetry and a range of symmetries are expressed by hierarchy. Such symmetries\ndirectly point to invariants, that pinpoint intrinsic properties of the data\nand of the background empirical domain of interest. We review many aspects of\nhierarchy here, including ultrametric topology, generalized ultrametric,\nlinkages with lattices and other discrete algebraic structures and with p-adic\nnumber representations. By focusing on symmetries in data we have a powerful\nmeans of structuring and analyzing massive, high dimensional data stores. We\nillustrate the powerfulness of hierarchical clustering in case studies in\nchemistry and finance, and we provide pointers to other published case studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2010 23:12:03 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1005.2715", "submitter": "Stefanos Zafeiriou", "authors": "Georgios Tzimiropoulos and Stefanos Zafeiriou", "title": "On the Subspace of Image Gradient Orientations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of Principal Component Analysis (PCA) of image\ngradient orientations. As image data is typically noisy, but noise is\nsubstantially different from Gaussian, traditional PCA of pixel intensities\nvery often fails to estimate reliably the low-dimensional subspace of a given\ndata population. We show that replacing intensities with gradient orientations\nand the $\\ell_2$ norm with a cosine-based distance measure offers, to some\nextend, a remedy to this problem. Our scheme requires the eigen-decomposition\nof a covariance matrix and is as computationally efficient as standard $\\ell_2$\nPCA. We demonstrate some of its favorable properties on robust subspace\nestimation.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2010 00:31:19 GMT"}], "update_date": "2010-05-18", "authors_parsed": [["Tzimiropoulos", "Georgios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1005.4020", "submitter": "William Jackson", "authors": "Salem Saleh Al-amri, N.V. Kalyankar and Khamitkar S.D.", "title": "Image Segmentation by Using Threshold Techniques", "comments": "http://www.journalofcomputing.org", "journal-ref": "Journal of Computing, Volume 2, Issue 5, May 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to undertake the study of segmentation image techniques\nby using five threshold methods as Mean method, P-tile method, Histogram\nDependent Technique (HDT), Edge Maximization Technique (EMT) and visual\nTechnique and they are compared with one another so as to choose the best\ntechnique for threshold segmentation techniques image. These techniques applied\non three satellite images to choose base guesses for threshold segmentation\nimage.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 17:30:08 GMT"}], "update_date": "2010-05-24", "authors_parsed": [["Al-amri", "Salem Saleh", ""], ["Kalyankar", "N. V.", ""], ["D.", "Khamitkar S.", ""]]}, {"id": "1005.4032", "submitter": "Debotosh Bhattacharjee", "authors": "Sandhya Arora, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar\n  Basu, and Mahantapas Kundu", "title": "Combining Multiple Feature Extraction Techniques for Handwritten\n  Devnagari Character Recognition", "comments": "6 pages, 8-10 December 2008", "journal-ref": "ICIIS 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an OCR for Handwritten Devnagari Characters. Basic\nsymbols are recognized by neural classifier. We have used four feature\nextraction techniques namely, intersection, shadow feature, chain code\nhistogram and straight line fitting features. Shadow features are computed\nglobally for character image while intersection features, chain code histogram\nfeatures and line fitting features are computed by dividing the character image\ninto different segments. Weighted majority voting technique is used for\ncombining the classification decision obtained from four Multi Layer\nPerceptron(MLP) based classifier. On experimentation with a dataset of 4900\nsamples the overall recognition rate observed is 92.80% as we considered top\nfive choices results. This method is compared with other recent methods for\nHandwritten Devnagari Character Recognition and it has been observed that this\napproach has better success rate than other methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 17:57:50 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Arora", "Sandhya", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1005.4034", "submitter": "Debotosh Bhattacharjee", "authors": "Santanu Halder, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar\n  Basu and Mahantapas Kundu", "title": "Face Synthesis (FASY) System for Generation of a Face Image from Human\n  Description", "comments": null, "journal-ref": "ICIIS 2008", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at generating a new face based on the human like description\nusing a new concept. The FASY (FAce SYnthesis) System is a Face Database\nRetrieval and new Face generation System that is under development. One of its\nmain features is the generation of the requested face when it is not found in\nthe existing database, which allows a continuous growing of the database also.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 18:03:44 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Halder", "Santanu", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1005.4035", "submitter": "Debotosh Bhattacharjee", "authors": "Mrinal Kanti Bhowmik, Debotosh Bhattacharjee, Mita Nasipuri, Dipak\n  Kumar Basu and Mahantapas Kundu", "title": "Classification of Polar-Thermal Eigenfaces using Multilayer Perceptron\n  for Human Face Recognition", "comments": null, "journal-ref": "ICIIS 2008", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to handle the challenges of face\nrecognition. In this work thermal face images are considered, which minimizes\nthe affect of illumination changes and occlusion due to moustache, beards,\nadornments etc. The proposed approach registers the training and testing\nthermal face images in polar coordinate, which is capable to handle\ncomplicacies introduced by scaling and rotation. Polar images are projected\ninto eigenspace and finally classified using a multi-layer perceptron. In the\nexperiments we have used Object Tracking and Classification Beyond Visible\nSpectrum (OTCBVS) database benchmark thermal face images. Experimental results\nshow that the proposed approach significantly improves the verification and\nidentification performance and the success rate is 97.05%.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 18:07:42 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Bhowmik", "Mrinal Kanti", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1005.4044", "submitter": "Debotosh Bhattacharjee", "authors": "Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri and M. Kundu", "title": "Reduction of Feature Vectors Using Rough Set Theory for Human Face\n  Recognition", "comments": null, "journal-ref": "ICCS 2005", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a procedure to reduce the size of the input feature\nvector. A complex pattern recognition problem like face recognition involves\nhuge dimension of input feature vector. To reduce that dimension here we have\nused eigenspace projection (also called as Principal Component Analysis), which\nis basically transformation of space. To reduce further we have applied feature\nselection method to select indispensable features, which will remain in the\nfinal feature vectors. Features those are not selected are removed from the\nfinal feature vector considering them as redundant or superfluous. For\nselection of features we have used the concept of reduct and core from rough\nset theory. This method has shown very good performance. It is worth to mention\nthat in some cases the recognition rate increases with the decrease in the\nfeature vector dimension.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2010 19:13:39 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "M.", ""]]}, {"id": "1005.4103", "submitter": "Chunhua Shen", "authors": "Chunhua Shen and Peng Wang and Hanxi Li", "title": "LACBoost and FisherBoost: Optimally Building Cascade Classifiers", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is one of the key tasks in computer vision. The cascade\nframework of Viola and Jones has become the de facto standard. A classifier in\neach node of the cascade is required to achieve extremely high detection rates,\ninstead of low overall classification error. Although there are a few reported\nmethods addressing this requirement in the context of object detection, there\nis no a principled feature selection method that explicitly takes into account\nthis asymmetric node learning objective. We provide such a boosting algorithm\nin this work. It is inspired by the linear asymmetric classifier (LAC) of Wu et\nal. in that our boosting algorithm optimizes a similar cost function. The new\ntotally-corrective boosting algorithm is implemented by the column generation\ntechnique in convex optimization. Experimental results on face detection\nsuggest that our proposed boosting algorithms can improve the state-of-the-art\nmethods in detection performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2010 04:22:57 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Shen", "Chunhua", ""], ["Wang", "Peng", ""], ["Li", "Hanxi", ""]]}, {"id": "1005.4118", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai and Chunhua Shen and Jian Zhang", "title": "Incremental Training of a Detector Using Online Sparse\n  Eigen-decomposition", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TIP.2010.2053548", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to efficiently and accurately detect objects plays a very crucial\nrole for many computer vision tasks. Recently, offline object detectors have\nshown a tremendous success. However, one major drawback of offline techniques\nis that a complete set of training data has to be collected beforehand. In\naddition, once learned, an offline detector can not make use of newly arriving\ndata. To alleviate these drawbacks, online learning has been adopted with the\nfollowing objectives: (1) the technique should be computationally and storage\nefficient; (2) the updated classifier must maintain its high classification\naccuracy. In this paper, we propose an effective and efficient framework for\nlearning an adaptive online greedy sparse linear discriminant analysis (GSLDA)\nmodel. Unlike many existing online boosting detectors, which usually apply\nexponential or logistic loss, our online algorithm makes use of LDA's learning\ncriterion that not only aims to maximize the class-separation criterion but\nalso incorporates the asymmetrical property of training data distributions. We\nprovide a better alternative for online boosting algorithms in the context of\ntraining a visual object detector. We demonstrate the robustness and efficiency\nof our methods on handwriting digit and face data sets. Our results confirm\nthat object detection tasks benefit significantly when trained in an online\nmanner.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2010 11:05:58 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Zhang", "Jian", ""]]}, {"id": "1005.4216", "submitter": "William Jackson", "authors": "Y.Babykalpana and K.ThanushKodi", "title": "Classification of LULC Change Detection using Remotely Sensed Data for\n  Coimbatore City, Tamilnadu, India", "comments": "http://www.journalofcomputing.org", "journal-ref": "Journal of Computing, Volume 2, Issue 5, May 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maps are used to describe far-off places . It is an aid for navigation and\nmilitary strategies. Mapping of the lands are important and the mapping work is\nbased on (i). Natural resource management & development (ii). Information\ntechnology ,(iii). Environmental development ,(iv). Facility management and\n(v). e-governance. The Landuse / Landcover system espoused by almost all\nOrganisations and scientists, engineers and remote sensing community who are\ninvolved in mapping of earth surface features, is a system which is derived\nfrom the united States Geological Survey (USGS) LULC classification system. The\napplication of RS and GIS involves influential of homogeneous zones, drift\nanalysis of land use integration of new area changes or change detection\netc.,National Remote Sensing Agency(NRSA) Govt. of India has devised a\ngeneralized LULC classification system respect to the Indian conditions based\non the various categories of Earth surface features , resolution of available\nsatellite data, capabilities of sensors and present and future applications.\nThe profusion information of the earth surface offered by the high resolution\nsatellite images for remote sensing applications. Using change detection\nmethodologies to extract the target changes in the areas from high resolution\nimages and rapidly updates geodatabase information processing.Traditionally,\nclassification approaches have focused on per-pixel technologies. Pixels within\nareas assumed to be automatically homogeneous are analyzed independently.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2010 18:16:49 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Babykalpana", "Y.", ""], ["ThanushKodi", "K.", ""]]}, {"id": "1005.4292", "submitter": "Chriss Romy", "authors": "Mrigank Rajya, Sonal Rewri, Swati Sheoran", "title": "Application Of Fuzzy System In Segmentation Of MRI Brain Tumor", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Segmentation of images holds an important position in the area of image\nprocessing. It becomes more important whi le typically dealing with medical\nimages where presurgery and post surgery decisions are required for the purpose\nof initiating and speeding up the recovery process. Segmentation of 3-D tumor\nstructures from magnetic resonance images (MRI) is a very challenging problem\ndue to the variability of tumor geometry and intensity patterns. Level set\nevolution combining global smoothness with the flexibility of topology changes\noffers significant advantages over the conventional statistical classification\nfollowed by mathematical morphology. Level set evolution with constant\npropagation needs to be initialized either completely inside or outside the\ntumor and can leak through weak or missing boundary parts. Replacing the\nconstant propagation term by a statistical force overcomes these limitations\nand results in a convergence to a stable solution. Using MR images presenting\ntumors, probabilities for background and tumor regions are calculated from a\npre- and post-contrast difference image and mixture modeling fit of the\nhistogram. The whole image is used for initialization of the level set\nevolution to segment the tumor boundaries.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 09:59:08 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Rajya", "Mrigank", ""], ["Rewri", "Sonal", ""], ["Sheoran", "Swati", ""]]}, {"id": "1005.5181", "submitter": "Daniel Burfoot", "authors": "Daniel Burfoot", "title": "Compression Rate Method for Empirical Science and Application to\n  Computer Vision", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This philosophical paper proposes a modified version of the scientific\nmethod, in which large databases are used instead of experimental observations\nas the necessary empirical ingredient. This change in the source of the\nempirical data allows the scientific method to be applied to several aspects of\nphysical reality that previously resisted systematic interrogation. Under the\nnew method, scientific theories are compared by instantiating them as\ncompression programs, and examining the codelengths they achieve on a database\nof measurements related to a phenomenon of interest. Because of the\nimpossibility of compressing random data, \"real world\" data can only be\ncompressed by discovering and exploiting the empirical structure it exhibits.\nThe method also provides a new way of thinking about two longstanding issues in\nthe philosophy of science: the problem of induction and the problem of\ndemarcation.\n  The second part of the paper proposes to reformulate computer vision as an\nempirical science of visual reality, by applying the new method to large\ndatabases of natural images. The immediate goal of the proposed reformulation\nis to repair the chronic difficulties in evaluation experienced by the field of\ncomputer vision. The reformulation should bring a wide range of benefits,\nincluding a substantially increased degree of methodological rigor, the ability\nto justify complex theories without overfitting, a scalable evaluation\nparadigm, and the potential to make systematic progress. A crucial argument is\nthat the change is not especially drastic, because most computer vision tasks\ncan be reformulated as specialized image compression techniques. Finally, a\nconcrete proposal is discussed in which a database is produced by recording\nfrom a roadside video camera, and compression is achieved by developing a\ncomputational understanding of the appearance of moving cars.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 21:27:43 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Burfoot", "Daniel", ""]]}, {"id": "1005.5437", "submitter": "Secretary Aircc Journal", "authors": "Ch.Srinivasa Rao(1), S.Srinivas Kumar(2), B.Chandra Mohan(3), ((1)Sri\n  Sai Aditya Institute of Science & Technology, India, (2)JNTUK, India,\n  (3)Bapatla Engineering College, India)", "title": "Content Based Image Retrieval Using Exact Legendre Moments and Support\n  Vector Machine", "comments": "11 Pages, IJMA", "journal-ref": "International journal of Multimedia & Its Applications 2.2 (2010)\n  69-79", "doi": "10.5121/ijma.2010.2206", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Content Based Image Retrieval (CBIR) systems based on shape using invariant\nimage moments, viz., Moment Invariants (MI) and Zernike Moments (ZM) are\navailable in the literature. MI and ZM are good at representing the shape\nfeatures of an image. However, non-orthogonality of MI and poor reconstruction\nof ZM restrict their application in CBIR. Therefore, an efficient and\northogonal moment based CBIR system is needed. Legendre Moments (LM) are\northogonal, computationally faster, and can represent image shape features\ncompactly. CBIR system using Exact Legendre Moments (ELM) for gray scale images\nis proposed in this work. Superiority of the proposed CBIR system is observed\nover other moment based methods, viz., MI and ZM in terms of retrieval\nefficiency and retrieval time. Further, the classification efficiency is\nimproved by employing Support Vector Machine (SVM) classifier. Improved\nretrieval results are obtained over existing CBIR algorithm based on Stacked\nEuler Vector (SERVE) combined with Modified Moment Invariants (MMI).\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 08:12:16 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Rao", "Ch. Srinivasa", ""], ["Kumar", "S. Srinivas", ""], ["Mohan", "B. Chandra", ""]]}, {"id": "1005.5439", "submitter": "Secretary Aircc Journal", "authors": "Amer A. Al-Rahayfeh, Abdelshakour A. Abuzneid", "title": "Detection of Bleeding in Wireless Capsule Endoscopy Images Using Range\n  Ratio Color", "comments": "10 Pages, IJMA", "journal-ref": "International journal of Multimedia & Its Applications 2.2 (2010)\n  1-10", "doi": "10.5121/ijma.2010.2201", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Wireless Capsule Endoscopy (WCE) is device to detect abnormalities in\ncolon,esophagus,small intestinal and stomach, to distinguish bleeding in WCE\nimages from non bleeding is a hard job by human reviewing and very time\nconsuming. Consequently, automation for classifying bleeding frames not only\nwill expedite the process but will reduce the burden on the doctors. Using the\npurity of the red color we can detect the Bleeding areas in WCE images. But, we\ncould find various intensity of red color values in different parts of the\nsmall intestinal,so it is not enough to depend on the red color feature alone.\nWe select RGB(Red,Green,Blue) because it takes raw level values and it is easy\nto use. In this paper we will put range ratio color for each of R,G,and B.\nTherefore, we divide each image into multiple pixels and apply the range ratio\ncolor condition for each pixel. Then we count the number of the pixels that\nachieved our condition. If the number of pixels grater than zero, then the\nframe is classified as a bleeding type. Otherwise, it is a non-bleeding. Our\nexperimental results show that this method could achieve a very high accuracy\nin detecting bleeding images for the different parts of the small intestinal\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 08:25:50 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Al-Rahayfeh", "Amer A.", ""], ["Abuzneid", "Abdelshakour A.", ""]]}]