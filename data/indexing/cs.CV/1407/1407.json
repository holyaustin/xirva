[{"id": "1407.0010", "submitter": "Liangqiong Qu", "authors": "Liangqiong Qu, Jiandong Tian, Zhi Han, and Yandong Tang", "title": "Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and\n  Shadow-free Image", "comments": "This paper has been published in Optics Express, Vol. 23, Issue 3,\n  pp. 2220-2239. The final version is available on\n  http://dx.doi.org/10.1364/OE.23.002220. Please refer to that version when\n  citing this paper", "journal-ref": "Optics Express, Vol. 23, Issue 3, pp. 2220-2239 (2015)", "doi": "10.1364/OE.23.002220", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel, effective and fast method to obtain a\ncolor illumination invariant and shadow-free image from a single outdoor image.\nDifferent from state-of-the-art methods for shadow-free image that either need\nshadow detection or statistical learning, we set up a linear equation set for\neach pixel value vector based on physically-based shadow invariants, deduce a\npixel-wise orthogonal decomposition for its solutions, and then get an\nillumination invariant vector for each pixel value vector on an image. The\nillumination invariant vector is the unique particular solution of the linear\nequation set, which is orthogonal to its free solutions. With this illumination\ninvariant vector and Lab color space, we propose an algorithm to generate a\nshadow-free image which well preserves the texture and color information of the\noriginal image. A series of experiments on a diverse set of outdoor images and\nthe comparisons with the state-of-the-art methods validate our method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 07:55:27 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 03:41:52 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Qu", "Liangqiong", ""], ["Tian", "Jiandong", ""], ["Han", "Zhi", ""], ["Tang", "Yandong", ""]]}, {"id": "1407.0221", "submitter": "Dirk Lorenz", "authors": "Jan Lellmann, Dirk A. Lorenz, Carola Sch\\\"onlieb, Tuomo Valkonen", "title": "Imaging with Kantorovich-Rubinstein discrepancy", "comments": null, "journal-ref": null, "doi": "10.1137/140975528", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of the Kantorovich-Rubinstein norm from optimal transport\nin imaging problems. In particular, we discuss a variational regularisation\nmodel endowed with a Kantorovich-Rubinstein discrepancy term and total\nvariation regularization in the context of image denoising and cartoon-texture\ndecomposition. We point out connections of this approach to several other\nrecently proposed methods such as total generalized variation and norms\ncapturing oscillating patterns. We also show that the respective optimization\nproblem can be turned into a convex-concave saddle point problem with simple\nconstraints and hence, can be solved by standard tools. Numerical examples\nexhibit interesting features and favourable performance for denoising and\ncartoon-texture decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 13:06:00 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Lellmann", "Jan", ""], ["Lorenz", "Dirk A.", ""], ["Sch\u00f6nlieb", "Carola", ""], ["Valkonen", "Tuomo", ""]]}, {"id": "1407.0342", "submitter": "Jinwei Xu", "authors": "Jinwei Xu, Jiankun Hu, Xiuping Jia", "title": "A New Path to Construct Parametric Orientation Field: Sparse FOMFE Model\n  and Compressed Sparse FOMFE Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orientation field, representing the fingerprint ridge structure direction,\nplays a crucial role in fingerprint-related image processing tasks. Orientation\nfield is able to be constructed by either non-parametric or parametric methods.\nIn this paper, the advantages and disadvantages regarding to the existing\nnon-parametric and parametric approaches are briefly summarized. With the\nfurther investigation for constructing the orientation field by parametric\ntechnique, two new models - sparse FOMFE model and compressed sparse FOMFE\nmodel are introduced, based on the rapidly developing signal sparse\nrepresentation and compressed sensing theories. The experiments on high-quality\nfingerprint image dataset (plain and rolled print) and poor-quality fingerprint\nimage dataset (latent print) demonstrate their feasibilities to construct the\norientation field in a sparse or even compressed sparse mode. The comparisons\namong the state-of-art orientation field modeling approaches show that the\nproposed two models have the potential availability in big data-oriented\nfingerprint indexing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 18:18:39 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Xu", "Jinwei", ""], ["Hu", "Jiankun", ""], ["Jia", "Xiuping", ""]]}, {"id": "1407.0439", "submitter": "Haixia Liu", "authors": "Haixia Liu, Raymond H. Chan, and Yuan Yao", "title": "Geometric Tight Frame based Stylometry for Art Authentication of van\n  Gogh Paintings", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about authenticating genuine van Gogh paintings from forgeries.\nThe authentication process depends on two key steps: feature extraction and\noutlier detection. In this paper, a geometric tight frame and some simple\nstatistics of the tight frame coefficients are used to extract features from\nthe paintings. Then a forward stage-wise rank boosting is used to select a\nsmall set of features for more accurate classification so that van Gogh\npaintings are highly concentrated towards some center point while forgeries are\nspread out as outliers. Numerical results show that our method can achieve\n86.08% classification accuracy under the leave-one-out cross-validation\nprocedure. Our method also identifies five features that are much more\npredominant than other features. Using just these five features for\nclassification, our method can give 88.61% classification accuracy which is the\nhighest so far reported in literature. Evaluation of the five features is also\nperformed on two hundred datasets generated by bootstrap sampling with\nreplacement. The median and the mean are 88.61% and 87.77% respectively. Our\nresults show that a small set of statistics of the tight frame coefficients\nalong certain orientations can serve as discriminative features for van Gogh\npaintings. It is more important to look at the tail distributions of such\ndirectional coefficients than mean values and standard deviations. It reflects\na highly consistent style in van Gogh's brushstroke movements, where many\nforgeries demonstrate a more diverse spread in these features.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 01:55:37 GMT"}, {"version": "v2", "created": "Sat, 13 Sep 2014 00:53:16 GMT"}, {"version": "v3", "created": "Tue, 13 Jan 2015 07:20:12 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Liu", "Haixia", ""], ["Chan", "Raymond H.", ""], ["Yao", "Yuan", ""]]}, {"id": "1407.0623", "submitter": "Lamberto Ballan", "authors": "Lamberto Ballan, Marco Bertini, Giuseppe Serra, Alberto Del Bimbo", "title": "A Data-Driven Approach for Tag Refinement and Localization in Web Videos", "comments": "Preprint submitted to Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2015.05.009", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tagging of visual content is becoming more and more widespread as web-based\nservices and social networks have popularized tagging functionalities among\ntheir users. These user-generated tags are used to ease browsing and\nexploration of media collections, e.g. using tag clouds, or to retrieve\nmultimedia content. However, not all media are equally tagged by users. Using\nthe current systems is easy to tag a single photo, and even tagging a part of a\nphoto, like a face, has become common in sites like Flickr and Facebook. On the\nother hand, tagging a video sequence is more complicated and time consuming, so\nthat users just tag the overall content of a video. In this paper we present a\nmethod for automatic video annotation that increases the number of tags\noriginally provided by users, and localizes them temporally, associating tags\nto keyframes. Our approach exploits collective knowledge embedded in\nuser-generated tags and web sources, and visual similarity of keyframes and\nimages uploaded to social sites like YouTube and Flickr, as well as web sources\nlike Google and Bing. Given a keyframe, our method is able to select on the fly\nfrom these visual sources the training exemplars that should be the most\nrelevant for this test sample, and proceeds to transfer labels across similar\nimages. Compared to existing video tagging approaches that require training\nclassifiers for each tag, our system has few parameters, is easy to implement\nand can deal with an open vocabulary scenario. We demonstrate the approach on\ntag refinement and localization on DUT-WEBV, a large dataset of web videos, and\nshow state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 15:48:37 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 18:12:36 GMT"}, {"version": "v3", "created": "Thu, 28 May 2015 17:12:54 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Ballan", "Lamberto", ""], ["Bertini", "Marco", ""], ["Serra", "Giuseppe", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1407.0717", "submitter": "Lubomir Bourdev", "authors": "Lubomir Bourdev, Fei Yang, Rob Fergus", "title": "Deep Poselets for Human Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of detecting people in natural scenes using a part\napproach based on poselets. We propose a bootstrapping method that allows us to\ncollect millions of weakly labeled examples for each poselet type. We use these\nexamples to train a Convolutional Neural Net to discriminate different poselet\ntypes and separate them from the background class. We then use the trained CNN\nas a way to represent poselet patches with a Pose Discriminative Feature (PDF)\nvector -- a compact 256-dimensional feature vector that is effective at\ndiscriminating pose from appearance. We train the poselet model on top of PDF\nfeatures and combine them with object-level CNNs for detection and bounding box\nprediction. The resulting model leads to state-of-the-art performance for human\ndetection on the PASCAL datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 20:28:22 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Bourdev", "Lubomir", ""], ["Yang", "Fei", ""], ["Fergus", "Rob", ""]]}, {"id": "1407.0733", "submitter": "Davide Barbieri", "authors": "Giacomo Cocci, Davide Barbieri, Giovanna Citti, Alessandro Sarti", "title": "Cortical spatio-temporal dimensionality reduction for visual grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual systems of many mammals, including humans, is able to integrate\nthe geometric information of visual stimuli and to perform cognitive tasks\nalready at the first stages of the cortical processing. This is thought to be\nthe result of a combination of mechanisms, which include feature extraction at\nsingle cell level and geometric processing by means of cells connectivity. We\npresent a geometric model of such connectivities in the space of detected\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\nused to obtain low-level object segmentation. The main idea is that of defining\na spectral clustering procedure with anisotropic affinities over datasets\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\nNeural plausibility of the proposed arguments will be discussed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 22:07:06 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 16:46:41 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Cocci", "Giacomo", ""], ["Barbieri", "Davide", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1407.0765", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson,\n  Bartlomiej Rajwa", "title": "BiofilmQuant: A Computer-Assisted Tool for Dental Biofilm Quantification", "comments": "4 pages, 4 figures, 36th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC 2014)", "journal-ref": null, "doi": "10.1109/EMBC.2014.6944561", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Dental biofilm is the deposition of microbial material over a tooth\nsubstratum. Several methods have recently been reported in the literature for\nbiofilm quantification; however, at best they provide a barely automated\nsolution requiring significant input needed from the human expert. On the\ncontrary, state-of-the-art automatic biofilm methods fail to make their way\ninto clinical practice because of the lack of effective mechanism to\nincorporate human input to handle praxis or misclassified regions. Manual\ndelineation, the current gold standard, is time consuming and subject to expert\nbias. In this paper, we introduce a new semi-automated software tool,\nBiofilmQuant, for dental biofilm quantification in quantitative light-induced\nfluorescence (QLF) images. The software uses a robust statistical modeling\napproach to automatically segment the QLF image into three classes (background,\nbiofilm, and tooth substratum) based on the training data. This initial\nsegmentation has shown a high degree of consistency and precision on more than\n200 test QLF dental scans. Further, the proposed software provides the\nclinicians full control to fix any misclassified areas using a single click. In\naddition, BiofilmQuant also provides a complete solution for the longitudinal\nquantitative analysis of biofilm of the full set of teeth, providing greater\nease of usability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 02:40:13 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Mansoor", "Awais", ""], ["Patsekin", "Valery", ""], ["Scherl", "Dale", ""], ["Robinson", "J. Paul", ""], ["Rajwa", "Bartlomiej", ""]]}, {"id": "1407.0786", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "Strengthening the Effectiveness of Pedestrian Detection with Spatially\n  Pooled Features", "comments": "16 pages. Appearing in Proc. European Conf. Computer Vision (ECCV)\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective approach to the problem of pedestrian\ndetection which outperforms the current state-of-the-art. Our new features are\nbuilt on the basis of low-level visual features and spatial pooling.\nIncorporating spatial pooling improves the translational invariance and thus\nthe robustness of the detection process. We then directly optimise the partial\narea under the ROC curve (\\pAUC) measure, which concentrates detection\nperformance in the range of most practical importance. The combination of these\nfactors leads to a pedestrian detector which outperforms all competitors on all\nof the standard benchmark datasets. We advance state-of-the-art results by\nlowering the average miss rate from $13\\%$ to $11\\%$ on the INRIA benchmark,\n$41\\%$ to $37\\%$ on the ETH benchmark, $51\\%$ to $42\\%$ on the TUD-Brussels\nbenchmark and $36\\%$ to $29\\%$ on the Caltech-USA benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 05:39:30 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1407.0921", "submitter": "Frank Lenzen", "authors": "Frank Lenzen, Jan Lellmann, Florian Becker, Christoph Schn\\\"orr", "title": "Solving QVIs for Image Restoration with Adaptive Constraint Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of quasi-variational inequalities (QVIs) for adaptive\nimage restoration, where the adaptivity is described via solution-dependent\nconstraint sets. In previous work we studied both theoretical and numerical\nissues. While we were able to show the existence of solutions for a relatively\nbroad class of problems, we encountered problems concerning uniqueness of the\nsolution as well as convergence of existing algorithms for solving QVIs. In\nparticular, it seemed that with increasing image size the growing condition\nnumber of the involved differential operator poses severe problems. In the\npresent paper we prove uniqueness for a larger class of problems and in\nparticular independent of the image size. Moreover, we provide a numerical\nalgorithm with proved convergence. Experimental results support our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 13:44:34 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Lenzen", "Frank", ""], ["Lellmann", "Jan", ""], ["Becker", "Florian", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1407.0935", "submitter": "Gopalkrishna  MT", "authors": "M. T Gopalakrishna, M. Ravishankar and D. R Rameshbabu", "title": "Multiple Moving Object Recognitions in video based on Log Gabor-PCA\n  Approach", "comments": "8,26,conference", "journal-ref": null, "doi": "10.1007/978-3-319-01778-5_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in the video sequence or images is one of the sub-field of\ncomputer vision. Moving object recognition from a video sequence is an\nappealing topic with applications in various areas such as airport safety,\nintrusion surveillance, video monitoring, intelligent highway, etc. Moving\nobject recognition is the most challenging task in intelligent video\nsurveillance system. In this regard, many techniques have been proposed based\non different methods. Despite of its importance, moving object recognition in\ncomplex environments is still far from being completely solved for low\nresolution videos, foggy videos, and also dim video sequences. All in all,\nthese make it necessary to develop exceedingly robust techniques. This paper\nintroduces multiple moving object recognition in the video sequence based on\nLoG Gabor-PCA approach and Angle based distance Similarity measures techniques\nused to recognize the object as a human, vehicle etc. Number of experiments are\nconducted for indoor and outdoor video sequences of standard datasets and also\nour own collection of video sequences comprising of partial night vision video\nsequences. Experimental results show that our proposed approach achieves an\nexcellent recognition rate. Results obtained are satisfactory and competent.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 14:52:56 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Gopalakrishna", "M. T", ""], ["Ravishankar", "M.", ""], ["Rameshbabu", "D. R", ""]]}, {"id": "1407.1120", "submitter": "Mehrtash Harandi", "authors": "Mehrtash T. Harandi and Mathieu Salzmann and Richard Hartley", "title": "From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for\n  SPD Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing images and videos with Symmetric Positive Definite (SPD)\nmatrices and considering the Riemannian geometry of the resulting space has\nproven beneficial for many recognition tasks. Unfortunately, computation on the\nRiemannian manifold of SPD matrices --especially of high-dimensional ones--\ncomes at a high cost that limits the applicability of existing techniques. In\nthis paper we introduce an approach that lets us handle high-dimensional SPD\nmatrices by constructing a lower-dimensional, more discriminative SPD manifold.\nTo this end, we model the mapping from the high-dimensional SPD manifold to the\nlow-dimensional one with an orthonormal projection. In particular, we search\nfor a projection that yields a low-dimensional manifold with maximum\ndiscriminative power encoded via an affinity-weighted similarity measure based\non metrics on the manifold. Learning can then be expressed as an optimization\nproblem on a Grassmann manifold. Our evaluation on several classification tasks\nshows that our approach leads to a significant accuracy gain over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 05:10:43 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 00:01:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Salzmann", "Mathieu", ""], ["Hartley", "Richard", ""]]}, {"id": "1407.1123", "submitter": "Mehrtash Harandi", "authors": "Mehrtash T. Harandi and Mathieu Salzmann and Sadeep Jayasumana and\n  Richard Hartley and Hongdong Li", "title": "Expanding the Family of Grassmannian Kernels: An Embedding Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling videos and image-sets as linear subspaces has proven beneficial for\nmany visual recognition tasks. However, it also incurs challenges arising from\nthe fact that linear subspaces do not obey Euclidean geometry, but lie on a\nspecial type of Riemannian manifolds known as Grassmannian. To leverage the\ntechniques developed for Euclidean spaces (e.g, support vector machines) with\nsubspaces, several recent studies have proposed to embed the Grassmannian into\na Hilbert space by making use of a positive definite kernel. Unfortunately,\nonly two Grassmannian kernels are known, none of which -as we will show- is\nuniversal, which limits their ability to approximate a target function\narbitrarily well. Here, we introduce several positive definite Grassmannian\nkernels, including universal ones, and demonstrate their superiority over\npreviously-known kernels in various tasks, such as classification, clustering,\nsparse coding and hashing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 05:34:38 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Salzmann", "Mathieu", ""], ["Jayasumana", "Sadeep", ""], ["Hartley", "Richard", ""], ["Li", "Hongdong", ""]]}, {"id": "1407.1151", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Jianxin Wu", "title": "Optimizing Ranking Measures for Compact Binary Code Learning", "comments": "Appearing in Proc. European Conference on Computer Vision 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has proven a valuable tool for large-scale information retrieval.\nDespite much success, existing hashing methods optimize over simple objectives\nsuch as the reconstruction error or graph Laplacian related loss functions,\ninstead of the performance evaluation criteria of interest---multivariate\nperformance measures such as the AUC and NDCG. Here we present a general\nframework (termed StructHash) that allows one to directly optimize multivariate\nperformance measures. The resulting optimization problem can involve\nexponentially or infinitely many variables and constraints, which is more\nchallenging than standard structured output learning. To solve the StructHash\noptimization problem, we use a combination of column generation and\ncutting-plane techniques. We demonstrate the generality of StructHash by\napplying it to ranking prediction and image retrieval, and show that it\noutperforms a few state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 08:18:45 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Wu", "Jianxin", ""]]}, {"id": "1407.1165", "submitter": "Prashant  Borde", "authors": "Prashant Bordea, Amarsinh Varpeb, Ramesh Manzac, Pravin Yannawara", "title": "Recognition of Isolated Words using Zernike and MFCC features for Audio\n  Visual Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) by machine is an attractive research topic\nin signal processing domain and has attracted many researchers to contribute in\nthis area. In recent year, there have been many advances in automatic speech\nreading system with the inclusion of audio and visual speech features to\nrecognize words under noisy conditions. The objective of audio-visual speech\nrecognition system is to improve recognition accuracy. In this paper we\ncomputed visual features using Zernike moments and audio feature using Mel\nFrequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of\nIndependent Standard Words) dataset which contains collection of isolated set\nof city names of 10 speakers. The visual features were normalized and dimension\nof features set was reduced by Principal Component Analysis (PCA) in order to\nrecognize the isolated word utterance on PCA space.The performance of\nrecognition of isolated words based on visual only and audio only features\nresults in 63.88 and 100 respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 09:32:10 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Bordea", "Prashant", ""], ["Varpeb", "Amarsinh", ""], ["Manzac", "Ramesh", ""], ["Yannawara", "Pravin", ""]]}, {"id": "1407.1208", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski, R\\'emi Lajugie, Francis Bach, Ivan Laptev, Jean\n  Ponce, Cordelia Schmid, Josef Sivic", "title": "Weakly Supervised Action Labeling in Videos Under Ordering Constraints", "comments": "17 pages, completed version of a ECCV2014 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a set of video clips, each one annotated with an {\\em ordered}\nlist of actions, such as \"walk\" then \"sit\" then \"answer phone\" extracted from,\nfor example, the associated text script. We seek to temporally localize the\nindividual actions in each clip as well as to learn a discriminative classifier\nfor each action. We formulate the problem as a weakly supervised temporal\nassignment with ordering constraints. Each video clip is divided into small\ntime intervals and each time interval of each video clip is assigned one action\nlabel, while respecting the order in which the action labels appear in the\ngiven annotations. We show that the action label assignment can be determined\ntogether with learning a classifier for each action in a discriminative manner.\nWe evaluate the proposed model on a new and challenging dataset of 937 video\nclips with a total of 787720 frames containing sequences of 16 different\nactions from 69 Hollywood movies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 12:53:15 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Lajugie", "R\u00e9mi", ""], ["Bach", "Francis", ""], ["Laptev", "Ivan", ""], ["Ponce", "Jean", ""], ["Schmid", "Cordelia", ""], ["Sivic", "Josef", ""]]}, {"id": "1407.1267", "submitter": "Qiang Fu", "authors": "Qiang Fu, Quan Quan, Kai-Yuan Cai", "title": "Calibration of Multiple Fish-Eye Cameras Using a Wand", "comments": "23 pages, 9 figures, submitted to IET Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fish-eye cameras are becoming increasingly popular in computer vision, but\ntheir use for 3D measurement is limited partly due to the lack of an accurate,\nefficient and user-friendly calibration procedure. For such a purpose, we\npropose a method to calibrate the intrinsic and extrinsic parameters (including\nradial distortion parameters) of two/multiple fish-eye cameras simultaneously\nby using a wand under general motions. Thanks to the generic camera model used,\nthe proposed calibration method is also suitable for two/multiple conventional\ncameras and mixed cameras (e.g. two conventional cameras and a fish-eye\ncamera). Simulation and real experiments demonstrate the effectiveness of the\nproposed method. Moreover, we develop the camera calibration toolbox, which is\navailable online.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 15:58:23 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Fu", "Qiang", ""], ["Quan", "Quan", ""], ["Cai", "Kai-Yuan", ""]]}, {"id": "1407.1339", "submitter": "Tejas Kulkarni", "authors": "Tejas D. Kulkarni and Vikash K. Mansinghka and Pushmeet Kohli and\n  Joshua B. Tenenbaum", "title": "Inverse Graphics with Probabilistic CAD Models", "comments": "For correspondence, contact tejask@mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recently, multiple formulations of vision problems as probabilistic\ninversions of generative models based on computer graphics have been proposed.\nHowever, applications to 3D perception from natural images have focused on\nlow-dimensional latent scenes, due to challenges in both modeling and\ninference. Accounting for the enormous variability in 3D object shape and 2D\nappearance via realistic generative models seems intractable, as does inverting\neven simple versions of the many-to-many computations that link 3D scenes to 2D\nimages. This paper proposes and evaluates an approach that addresses key\naspects of both these challenges. We show that it is possible to solve\nchallenging, real-world 3D vision problems by approximate inference in\ngenerative models for images based on rendering the outputs of probabilistic\nCAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D\nmeshes corresponding to plausible objects and apply affine transformations to\nplace them in a scene. Image likelihoods are based on similarity in a feature\nspace based on standard mid-level image representations from the vision\nliterature. Our inference algorithm integrates single-site and locally blocked\nMetropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative\ndata-driven proposals learned from training data generated from our models. We\napply this approach to 3D human pose estimation and object shape reconstruction\nfrom single images, achieving quantitative and qualitative performance\nimprovements over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 23:03:25 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Mansinghka", "Vikash K.", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1407.1352", "submitter": "Deli Zhao", "authors": "Deli Zhao and Xiaoou Tang", "title": "Homophilic Clustering by Locally Asymmetric Geometry", "comments": "10 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is indispensable for data analysis in many scientific disciplines.\nDetecting clusters from heavy noise remains challenging, particularly for\nhigh-dimensional sparse data. Based on graph-theoretic framework, the present\npaper proposes a novel algorithm to address this issue. The locally asymmetric\ngeometries of neighborhoods between data points result in a directed similarity\ngraph to model the structural connectivity of data points. Performing\nsimilarity propagation on this directed graph simply by its adjacency matrix\npowers leads to an interesting discovery, in the sense that if the in-degrees\nare ordered by the corresponding sorted out-degrees, they will be\nself-organized to be homophilic layers according to the different distributions\nof cluster densities, which is dubbed the Homophilic In-degree figure (the HI\nfigure). With the HI figure, we can easily single out all cores of clusters,\nidentify the boundary between cluster and noise, and visualize the intrinsic\nstructures of clusters. Based on the in-degree homophily, we also develop a\nsimple efficient algorithm of linear space complexity to cluster noisy data.\nExtensive experiments on toy and real-world scientific data validate the\neffectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 01:58:13 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Zhao", "Deli", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1407.1490", "submitter": "Jianguo Li", "authors": "Jianguo Li and Yurong Chen", "title": "Large-scale Supervised Hierarchical Feature Learning for Face\n  Recognition", "comments": "8 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes a novel face recognition algorithm based on large-scale\nsupervised hierarchical feature learning. The approach consists of two parts:\nhierarchical feature learning and large-scale model learning. The hierarchical\nfeature learning searches feature in three levels of granularity in a\nsupervised way. First, face images are modeled by receptive field theory, and\nthe representation is an image with many channels of Gaussian receptive maps.\nWe activate a few most distinguish channels by supervised learning. Second, the\nface image is further represented by patches of picked channels, and we search\nfrom the over-complete patch pool to activate only those most discriminant\npatches. Third, the feature descriptor of each patch is further projected to\nlower dimension subspace with discriminant subspace analysis.\n  Learned feature of activated patches are concatenated to get a full face\nrepresentation.A linear classifier is learned to separate face pairs from same\nsubjects and different subjects. As the number of face pairs are extremely\nlarge, we introduce ADMM (alternative direction method of multipliers) to train\nthe linear classifier on a computing cluster. Experiments show that more\ntraining samples will bring notable accuracy improvement.\n  We conduct experiments on FRGC and LFW. Results show that the proposed\napproach outperforms existing algorithms under the same protocol notably.\nBesides, the proposed approach is small in memory footprint, and low in\ncomputing cost, which makes it suitable for embedded applications.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 12:45:23 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Li", "Jianguo", ""], ["Chen", "Yurong", ""]]}, {"id": "1407.1531", "submitter": "Tuomo Valkonen", "authors": "Tuomo Valkonen", "title": "The jump set under geometric regularisation. Part 1: Basic technique and\n  first-order denoising", "comments": null, "journal-ref": null, "doi": "10.1137/140976248", "report-no": null, "categories": "math.FA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $u \\in \\mbox{BV}(\\Omega)$ solve the total variation denoising problem\nwith $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model.\nSimul. 6 (2008), 879--894] have shown the containment $\\mathcal{H}^{m-1}(J_u\n\\setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proof\nunfortunately depends heavily on the co-area formula, as do many results in\nthis area, and as such is not directly extensible to higher-order,\ncurvature-based, and other advanced geometric regularisers, such as total\ngeneralised variation (TGV) and Euler's elastica. These have received increased\nattention in recent times due to their better practical regularisation\nproperties compared to conventional total variation or wavelets. We prove\nanalogous jump set containment properties for a general class of regularisers.\nWe do this with novel Lipschitz transformation techniques, and do not require\nthe co-area formula. In the present Part 1 we demonstrate the general technique\non first-order regularisers, while in Part 2 we will extend it to higher-order\nregularisers. In particular, we concentrate in this part on TV and, as a\nnovelty, Huber-regularised TV. We also demonstrate that the technique would\napply to non-convex TV models as well as the Perona-Malik anisotropic\ndiffusion, if these approaches were well-posed to begin with.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 19:02:45 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 09:52:51 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Valkonen", "Tuomo", ""]]}, {"id": "1407.1610", "submitter": "Pulkit Agrawal", "authors": "Pulkit Agrawal, Ross Girshick, Jitendra Malik", "title": "Analyzing the Performance of Multilayer Neural Networks for Object\n  Recognition", "comments": "Published in European Conference on Computer Vision 2014 (ECCV-2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two years, convolutional neural networks (CNNs) have achieved an\nimpressive suite of results on standard recognition datasets and tasks.\nCNN-based features seem poised to quickly replace engineered representations,\nsuch as SIFT and HOG. However, compared to SIFT and HOG, we understand much\nless about the nature of the features learned by large CNNs. In this paper, we\nexperimentally probe several aspects of CNN feature learning in an attempt to\nhelp practitioners gain useful, evidence-backed intuitions about how to apply\nCNNs to computer vision problems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 08:00:57 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 17:49:01 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Agrawal", "Pulkit", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1407.1723", "submitter": "Michael Moeller", "authors": "Thomas M\\\"ollenhoff and Evgeny Strekalovskiy and Michael Moeller and\n  Daniel Cremers", "title": "The Primal-Dual Hybrid Gradient Method for Semiconvex Splittings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the analysis of a recent reformulation of the\nprimal-dual hybrid gradient method [Zhu and Chan 2008, Pock, Cremers, Bischof\nand Chambolle 2009, Esser, Zhang and Chan 2010, Chambolle and Pock 2011], which\nallows to apply it to nonconvex regularizers as first proposed for truncated\nquadratic penalization in [Strekalovskiy and Cremers 2014]. Particularly, it\ninvestigates variational problems for which the energy to be minimized can be\nwritten as $G(u) + F(Ku)$, where $G$ is convex, $F$ semiconvex, and $K$ is a\nlinear operator. We study the method and prove convergence in the case where\nthe nonconvexity of $F$ is compensated by the strong convexity of the $G$. The\nconvergence proof yields an interesting requirement for the choice of algorithm\nparameters, which we show to not only be sufficient, but necessary.\nAdditionally, we show boundedness of the iterates under much weaker conditions.\nFinally, we demonstrate effectiveness and convergence of the algorithm beyond\nthe theoretical guarantees in several numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 14:12:25 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Strekalovskiy", "Evgeny", ""], ["Moeller", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "1407.1785", "submitter": "Zemin Zhang", "authors": "Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer", "title": "Novel methods for multilinear data completion and de-noising based on\n  tensor-SVD", "comments": "8 pages, 8 figures. It is accepted as CVPR 2014 oral presentation.\n  arXiv admin note: substantial text overlap with arXiv:1307.0805", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose novel methods for completion (from limited samples)\nand de-noising of multilinear (tensor) data and as an application consider 3-D\nand 4- D (color) video data completion and de-noising. We exploit the recently\nproposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, the\nnotion of multilinear rank and a related tensor nuclear norm was proposed in\n[11] to characterize informational and structural complexity of multilinear\ndata. We first show that videos with linear camera motion can be represented\nmore efficiently using t-SVD compared to the approaches based on vectorizing or\nflattening of the tensors. Since efficiency in representation implies\nefficiency in recovery, we outline a tensor nuclear norm penalized algorithm\nfor video completion from missing entries. Application of the proposed\nalgorithm for video recovery from missing entries is shown to yield a superior\nperformance over existing methods. We also consider the problem of tensor\nrobust Principal Component Analysis (PCA) for de-noising 3-D video data from\nsparse random corruptions. We show superior performance of our method compared\nto the matrix robust PCA adapted to this setting as proposed in [4].\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 17:47:54 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 18:34:47 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Zhang", "Zemin", ""], ["Ely", "Gregory", ""], ["Aeron", "Shuchin", ""], ["Hao", "Ning", ""], ["Kilmer", "Misha", ""]]}, {"id": "1407.1808", "submitter": "Bharath Hariharan", "authors": "Bharath Hariharan and Pablo Arbel\\'aez and Ross Girshick and Jitendra\n  Malik", "title": "Simultaneous Detection and Segmentation", "comments": "To appear in the European Conference on Computer Vision (ECCV), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to detect all instances of a category in an image and, for each\ninstance, mark the pixels that belong to it. We call this task Simultaneous\nDetection and Segmentation (SDS). Unlike classical bounding box detection, SDS\nrequires a segmentation and not just a box. Unlike classical semantic\nsegmentation, we require individual object instances. We build on recent work\nthat uses convolutional neural networks to classify category-independent region\nproposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We\nthen use category-specific, top- down figure-ground predictions to refine our\nbottom-up proposals. We show a 7 point boost (16% relative) over our baselines\non SDS, a 5 point boost (10% relative) over state-of-the-art on semantic\nsegmentation, and state-of-the-art performance in object detection. Finally, we\nprovide diagnostic tools that unpack performance and provide directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 18:59:11 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Hariharan", "Bharath", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1407.1885", "submitter": "Antony Schutz", "authors": "Antony Schutz and Andr\\'e Ferrari and David Mary and F\\'err\\'eol\n  Soulez and \\'Eric Thi\\'ebaut and Martin Vannier", "title": "PAINTER: a spatio-spectral image reconstruction algorithm for optical\n  interferometry", "comments": "12 pages, 10 figures,\n  http://www.opticsinfobase.org/submit/review/copyright_permissions.cfm", "journal-ref": null, "doi": "10.1364/JOSAA.31.002334", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomical optical interferometers sample the Fourier transform of the\nintensity distribution of a source at the observation wavelength. Because of\nrapid perturbations caused by atmospheric turbulence, the phases of the complex\nFourier samples (visibilities) cannot be directly exploited. Consequently,\nspecific image reconstruction methods have been devised in the last few\ndecades. Modern polychromatic optical interferometric instruments are now\npaving the way to multiwavelength imaging. This paper is devoted to the\nderivation of a spatio-spectral (3D) image reconstruction algorithm, coined\nPAINTER (Polychromatic opticAl INTErferometric Reconstruction software). The\nalgorithm relies on an iterative process, which alternates estimation of\npolychromatic images and of complex visibilities. The complex visibilities are\nnot only estimated from squared moduli and closure phases, but also\ndifferential phases, which helps to better constrain the polychromatic\nreconstruction. Simulations on synthetic data illustrate the efficiency of the\nalgorithm and in particular the relevance of injecting a differential phases\nmodel in the reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 13:34:14 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 18:01:36 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Schutz", "Antony", ""], ["Ferrari", "Andr\u00e9", ""], ["Mary", "David", ""], ["Soulez", "F\u00e9rr\u00e9ol", ""], ["Thi\u00e9baut", "\u00c9ric", ""], ["Vannier", "Martin", ""]]}, {"id": "1407.1957", "submitter": "Hilton Bristow", "authors": "Hilton Bristow and Simon Lucey", "title": "Regression-Based Image Alignment for General Object Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-descent methods have exhibited fast and reliable performance for\nimage alignment in the facial domain, but have largely been ignored by the\nbroader vision community. They require the image function be smooth and\n(numerically) differentiable -- properties that hold for pixel-based\nrepresentations obeying natural image statistics, but not for more general\nclasses of non-linear feature transforms. We show that transforms such as Dense\nSIFT can be incorporated into a Lucas Kanade alignment framework by predicting\ndescent directions via regression. This enables robust matching of instances\nfrom general object categories whilst maintaining desirable properties of Lucas\nKanade such as the capacity to handle high-dimensional warp parametrizations\nand a fast rate of convergence. We present alignment results on a number of\nobjects from ImageNet, and an extension of the method to unsupervised joint\nalignment of objects from a corpus of images.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 05:43:47 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Bristow", "Hilton", ""], ["Lucey", "Simon", ""]]}, {"id": "1407.1974", "submitter": "Jianjia Zhang", "authors": "Jianjia Zhang, Lei Wang, Luping Zhou, Wanqing Li", "title": "Learning Discriminative Stein Kernel for SPD Matrices and Its\n  Applications", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein kernel has recently shown promising performance on classifying images\nrepresented by symmetric positive definite (SPD) matrices. It evaluates the\nsimilarity between two SPD matrices through their eigenvalues. In this paper,\nwe argue that directly using the original eigenvalues may be problematic\nbecause: i) Eigenvalue estimation becomes biased when the number of samples is\ninadequate, which may lead to unreliable kernel evaluation; ii) More\nimportantly, eigenvalues only reflect the property of an individual SPD matrix.\nThey are not necessarily optimal for computing Stein kernel when the goal is to\ndiscriminate different sets of SPD matrices. To address the two issues in one\nshot, we propose a discriminative Stein kernel, in which an extra parameter\nvector is defined to adjust the eigenvalues of the input SPD matrices. The\noptimal parameter values are sought by optimizing a proxy of classification\nperformance. To show the generality of the proposed method, three different\nkernel learning criteria that are commonly used in the literature are employed\nrespectively as a proxy. A comprehensive experimental study is conducted on a\nvariety of image classification tasks to compare our proposed discriminative\nStein kernel with the original Stein kernel and other commonly used methods for\nevaluating the similarity between SPD matrices. The experimental results\ndemonstrate that, the discriminative Stein kernel can attain greater\ndiscrimination and better align with classification tasks by altering the\neigenvalues. This makes it produce higher classification performance than the\noriginal Stein kernel and other commonly used methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 07:07:12 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 00:49:06 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 11:49:32 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Zhang", "Jianjia", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Li", "Wanqing", ""]]}, {"id": "1407.2044", "submitter": "Mohamed Hedi Dridi", "authors": "Mohamed H. Dridi", "title": "Tracking Individual Targets in High Density Crowd Scenes Analysis of a\n  Video Recording in Hajj 2009", "comments": "20 pages, 17 figures, correction of some references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a number of methods (manual, semi-automatic and\nautomatic) for tracking individual targets in high density crowd scenes where\nthousand of people are gathered. The necessary data about the motion of\nindividuals and a lot of other physical information can be extracted from\nconsecutive image sequences in different ways, including optical flow and block\nmotion estimation. One of the famous methods for tracking moving objects is the\nblock matching method. This way to estimate subject motion requires the\nspecification of a comparison window which determines the scale of the\nestimate. In this work we present a real-time method for pedestrian recognition\nand tracking in sequences of high resolution images obtained by a stationary\n(high definition) camera located in different places on the Haram mosque in\nMecca. The objective is to estimate pedestrian velocities as a function of the\nlocal density.The resulting data of tracking moving pedestrians based on video\nsequences are presented in the following section. Through the evaluated system\nthe spatio-temporal coordinates of each pedestrian during the Tawaf ritual are\nestablished. The pilgrim velocities as function of the local densities in the\nMataf area (Haram Mosque Mecca) are illustrated and very precisely documented.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 11:41:27 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 14:53:06 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Dridi", "Mohamed H.", ""]]}, {"id": "1407.2170", "submitter": "Giorgos Tolias", "authors": "Giorgos Tolias (INRIA), Teddy Furon (INRIA), Herv\\'e J\\'egou (INRIA)", "title": "Orientation covariant aggregation of local descriptors with embeddings", "comments": "European Conference on Computer Vision (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image search systems based on local descriptors typically achieve orientation\ninvariance by aligning the patches on their dominant orientations. Albeit\nsuccessful, this choice introduces too much invariance because it does not\nguarantee that the patches are rotated consistently. This paper introduces an\naggregation strategy of local descriptors that achieves this covariance\nproperty by jointly encoding the angle in the aggregation stage in a continuous\nmanner. It is combined with an efficient monomial embedding to provide a\ncodebook-free method to aggregate local descriptors into a single vector\nrepresentation. Our strategy is also compatible and employed with several\npopular encoding methods, in particular bag-of-words, VLAD and the Fisher\nvector. Our geometric-aware aggregation strategy is effective for image search,\nas shown by experiments performed on standard benchmarks for image and\nparticular object retrieval, namely Holidays and Oxford buildings.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 16:55:36 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 11:43:20 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Tolias", "Giorgos", "", "INRIA"], ["Furon", "Teddy", "", "INRIA"], ["J\u00e9gou", "Herv\u00e9", "", "INRIA"]]}, {"id": "1407.2334", "submitter": "Tuomo Valkonen", "authors": "Tuomo Valkonen", "title": "The jump set under geometric regularisation. Part 2: Higher-order\n  approaches", "comments": "Part 1: arXiv 1407.1531 [math.FA]", "journal-ref": null, "doi": "10.1016/j.jmaa.2017.04.037", "report-no": null, "categories": "math.FA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part 1, we developed a new technique based on Lipschitz pushforwards for\nproving the jump set containment property $\\mathcal{H}^{m-1}(J_u \\setminus\nJ_f)=0$ of solutions $u$ to total variation denoising. We demonstrated that the\ntechnique also applies to Huber-regularised TV. Now, in this Part 2, we extend\nthe technique to higher-order regularisers. We are not quite able to prove the\nproperty for total generalised variation (TGV) based on the symmetrised\ngradient for the second-order term. We show that the property holds under three\nconditions: First, the solution $u$ is locally bounded. Second, the\nsecond-order variable is of locally bounded variation, $w \\in\n\\mbox{BV}_\\mbox{loc}(\\Omega; \\mathbb{R}^m)$, instead of just bounded\ndeformation, $w \\in \\mbox{BD}(\\Omega)$. Third, $w$ does not jump on $J_u$\nparallel to it. The second condition can be achieved for non-symmetric TGV.\nBoth the second and third condition can be achieved if we change the Radon (or\n$L^1$) norm of the symmetrised gradient $Ew$ into an $L^p$ norm, $p>1$, in\nwhich case Korn's inequality holds. We also consider the application of the\ntechnique to infimal convolution TV, and study the limiting behaviour of the\nsingular part of $D u$, as the second parameter of $\\mbox{TGV}^2$ goes to zero.\nUnsurprisingly, it vanishes, but in numerical discretisations the situation\nlooks quite different. Finally, our work additionally includes a result on\nTGV-strict approximation in $\\mbox{BV}(\\Omega)$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 01:55:28 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Valkonen", "Tuomo", ""]]}, {"id": "1407.2343", "submitter": "Kunal Narayan Chaudhury", "authors": "S. Ghosh and K. N. Chaudhury", "title": "Fast Separable Non-Local Means", "comments": null, "journal-ref": "SPIE Journal of Electronic Imaging, vol. 25, no. 2, pp. 023026\n  (2016)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and fast algorithm called PatchLift for computing\ndistances between patches (contiguous block of samples) extracted from a given\none-dimensional signal. PatchLift is based on the observation that the patch\ndistances can be efficiently computed from a matrix that is derived from the\none-dimensional signal using lifting; importantly, the number of operations\nrequired to compute the patch distances using this approach does not scale with\nthe patch length. We next demonstrate how PatchLift can be used for patch-based\ndenoising of images corrupted with Gaussian noise. In particular, we propose a\nseparable formulation of the classical Non-Local Means (NLM) algorithm that can\nbe implemented using PatchLift. We demonstrate that the PatchLift-based\nimplementation of separable NLM is few orders faster than standard NLM, and is\ncompetitive with existing fast implementations of NLM. Moreover, its denoising\nperformance is shown to be consistently superior to that of NLM and some of its\nvariants, both in terms of PSNR/SSIM and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 03:15:37 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 06:35:50 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Ghosh", "S.", ""], ["Chaudhury", "K. N.", ""]]}, {"id": "1407.2390", "submitter": "Deepjoy Das", "authors": "SRM Prasanna, Rituparna Devi, Deepjoy Das, Subhankar Ghosh, Krishna\n  Naik", "title": "Online Stroke and Akshara Recognition GUI in Assamese Language Using\n  Hidden Markov Model", "comments": "6 pages, 9 figures, International Journal of Scientific and Research\n  Publications, Volume 4, Issue 1, January 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work describes the development of Online Assamese Stroke & Akshara\nRecognizer based on a set of language rules. In handwriting literature strokes\nare composed of two coordinate trace in between pen down and pen up labels. The\nAssamese aksharas are combination of a number of strokes, the maximum number of\nstrokes taken to make a combination being eight. Based on these combinations\neight language rule models have been made which are used to test if a set of\nstrokes form a valid akshara. A Hidden Markov Model is used to train 181\ndifferent stroke patterns which generates a model used during stroke level\ntesting. Akshara level testing is performed by integrating a GUI (provided by\nCDAC-Pune) with the Binaries of HTK toolkit classifier, HMM train model and the\nlanguage rules using a dynamic linked library (dll). We have got a stroke level\nperformance of 94.14% and akshara level performance of 84.2%.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 08:48:41 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Prasanna", "SRM", ""], ["Devi", "Rituparna", ""], ["Das", "Deepjoy", ""], ["Ghosh", "Subhankar", ""], ["Naik", "Krishna", ""]]}, {"id": "1407.2572", "submitter": "Reza Azad", "authors": "Reza Azad, Babak Azad, Iraj Mogharreb, Shahram Jamali", "title": "Classifiers fusion method to recognize handwritten persian numerals", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 5 and 6, and some mistake in Table 1 information. please\n  let me for changing this information and updating this paper", "journal-ref": "International Journal on Cybernetics & Informatics (IJCI) Vol. 3,\n  No. 3, June 2014", "doi": "10.5121/ijci.2014.3301", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of Persian handwritten characters has been considered as a\nsignificant field of research for the last few years under pattern analysing\ntechnique. In this paper, a new approach for robust handwritten Persian\nnumerals recognition using strong feature set and a classifier fusion method is\nscrutinized to increase the recognition percentage. For implementing the\nclassifier fusion technique, we have considered k nearest neighbour (KNN),\nlinear classifier (LC) and support vector machine (SVM) classifiers. The\ninnovation of this tactic is to attain better precision with few features using\nclassifier fusion method. For evaluation of the proposed method we considered a\nPersian numerals database with 20,000 handwritten samples. Spending 15,000\nsamples for training stage, we verified our technique on other 5,000 samples,\nand the correct recognition ratio achieved approximately 99.90%. Additional, we\ngot 99.97% exactness using four-fold cross validation procedure on 20,000\ndatabases.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 17:49:11 GMT"}, {"version": "v2", "created": "Fri, 15 Aug 2014 04:27:36 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Azad", "Reza", ""], ["Azad", "Babak", ""], ["Mogharreb", "Iraj", ""], ["Jamali", "Shahram", ""]]}, {"id": "1407.2602", "submitter": "Lior Weizman", "authors": "Lior Weizman, Yonina C. Eldar, Dafna Ben Bashat", "title": "Compressed sensing for longitudinal MRI: An adaptive-weighted approach", "comments": null, "journal-ref": null, "doi": "10.1118/1.4928148", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Repeated brain MRI scans are performed in many clinical scenarios,\nsuch as follow up of patients with tumors and therapy response assessment. In\nthis paper, the authors show an approach to utilize former scans of the patient\nfor the acceleration of repeated MRI scans.\n  Methods: The proposed approach utilizes the possible similarity of the\nrepeated scans in longitudinal MRI studies. Since similarity is not guaranteed,\nsampling and reconstruction are adjusted during acquisition to match the actual\nsimilarity between the scans. The baseline MR scan is utilized both in the\nsampling stage, via adaptive sampling, and in the reconstruction stage, with\nweighted reconstruction. In adaptive sampling, k-space sampling locations are\noptimized during acquisition. Weighted reconstruction uses the locations of the\nnonzero coefficients in the sparse domains as a prior in the recovery process.\nThe approach was tested on 2D and 3D MRI scans of patients with brain tumors.\n  Results: The longitudinal adaptive CS MRI (LACS-MRI) scheme provides\nreconstruction quality which outperforms other CS-based approaches for rapid\nMRI. Examples are shown on patients with brain tumors and demonstrate improved\nspatial resolution. Compared with data sampled at Nyquist rate, LACS-MRI\nexhibits Signal-to-Error Ratio (SER) of 24.8dB with undersampling factor of\n16.6 in 3D MRI.\n  Conclusions: The authors have presented a novel method for image\nreconstruction utilizing similarity of scans in longitudinal MRI studies, where\npossible. The proposed approach can play a major part and significantly reduce\nscanning time in many applications that consist of disease follow-up and\nmonitoring of longitudinal changes in brain MRI.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 08:05:11 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 10:25:47 GMT"}, {"version": "v3", "created": "Tue, 6 Jan 2015 12:35:52 GMT"}, {"version": "v4", "created": "Fri, 25 Aug 2017 09:17:49 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Weizman", "Lior", ""], ["Eldar", "Yonina C.", ""], ["Bashat", "Dafna Ben", ""]]}, {"id": "1407.2630", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Valery Patsekin, Dale Scherl, J. Paul Robinson,\n  Bartlomiej Rajwa", "title": "A Statistical Modeling Approach to Computer-Aided Quantification of\n  Dental Biofilm", "comments": "10 pages, 7 figures, Journal of Biomedical and Health Informatics\n  2014. keywords: {Biomedical imaging;Calibration;Dentistry;Estimation;Image\n  segmentation;Manuals;Teeth},\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6758338&isnumber=6363502", "journal-ref": null, "doi": "10.1109/JBHI.2014.2310204", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biofilm is a formation of microbial material on tooth substrata. Several\nmethods to quantify dental biofilm coverage have recently been reported in the\nliterature, but at best they provide a semi-automated approach to\nquantification with significant input from a human grader that comes with the\ngraders bias of what are foreground, background, biofilm, and tooth.\nAdditionally, human assessment indices limit the resolution of the\nquantification scale; most commercial scales use five levels of quantification\nfor biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, current\nstate-of-the-art techniques in automatic plaque quantification fail to make\ntheir way into practical applications owing to their inability to incorporate\nhuman input to handle misclassifications. This paper proposes a new interactive\nmethod for biofilm quantification in Quantitative light-induced fluorescence\n(QLF) images of canine teeth that is independent of the perceptual bias of the\ngrader. The method partitions a QLF image into segments of uniform texture and\nintensity called superpixels; every superpixel is statistically modeled as a\nrealization of a single 2D Gaussian Markov random field (GMRF) whose parameters\nare estimated; the superpixel is then assigned to one of three classes\n(background, biofilm, tooth substratum) based on the training set of data. The\nquantification results show a high degree of consistency and precision. At the\nsame time, the proposed method gives pathologists full control to post-process\nthe automatic quantification by flipping misclassified superpixels to a\ndifferent state (background, tooth, biofilm) with a single click, providing\ngreater usability than simply marking the boundaries of biofilm and tooth as\ndone by current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 20:32:32 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Mansoor", "Awais", ""], ["Patsekin", "Valery", ""], ["Scherl", "Dale", ""], ["Robinson", "J. Paul", ""], ["Rajwa", "Bartlomiej", ""]]}, {"id": "1407.2649", "submitter": "Alican Bozkurt", "authors": "Alican Bozkurt, Pinar Duygulu, A. Enis Cetin", "title": "Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recognizing fonts has become an important task in document analysis, due to\nthe increasing number of available digital documents in different fonts and\nemphases. A generic font-recognition system independent of language, script and\ncontent is desirable for processing various types of documents. At the same\ntime, categorizing calligraphy styles in handwritten manuscripts is important\nfor palaeographic analysis, but has not been studied sufficiently in the\nliterature. We address the font-recognition problem as analysis and\ncategorization of textures. We extract features using complex wavelet transform\nand use support vector machines for classification. Extensive experimental\nevaluations on different datasets in four languages and comparisons with\nstate-of-the-art studies show that our proposed method achieves higher\nrecognition accuracy while being computationally simpler. Furthermore, on a new\ndataset generated from Ottoman manuscripts, we show that the proposed method\ncan also be used for categorizing Ottoman calligraphy with high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 22:25:32 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Bozkurt", "Alican", ""], ["Duygulu", "Pinar", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1407.2700", "submitter": "Anwar Ebrahim alawady", "authors": "Ghazali Sulong, Anwar Yahy Ebrahim and Muhammad Jehanzeb", "title": "Offline handwritten signature identification using adaptive window\n  positioning techniques", "comments": "13 pages, 9 figures, 2 tables, Offline Handwritten Signature, GPDS\n  dataset, Verification, Identification, Adaptive window positioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents to address this challenge, we have proposed the use of\nAdaptive Window Positioning technique which focuses on not just the meaning of\nthe handwritten signature but also on the individuality of the writer. This\ninnovative technique divides the handwritten signature into 13 small windows of\nsize nxn(13x13).This size should be large enough to contain ample information\nabout the style of the author and small enough to ensure a good identification\nperformance.The process was tested with a GPDS data set containing 4870\nsignature samples from 90 different writers by comparing the robust features of\nthe test signature with that of the user signature using an appropriate\nclassifier. Experimental results reveal that adaptive window positioning\ntechnique proved to be the efficient and reliable method for accurate signature\nfeature extraction for the identification of offline handwritten signatures.The\ncontribution of this technique can be used to detect signatures signed under\nemotional duress.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 06:03:00 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Sulong", "Ghazali", ""], ["Ebrahim", "Anwar Yahy", ""], ["Jehanzeb", "Muhammad", ""]]}, {"id": "1407.2721", "submitter": "Erik Rodner", "authors": "Bj\\\"orn Barz and Erik Rodner and Joachim Denzler", "title": "ARTOS -- Adaptive Real-Time Object Detection System", "comments": "http://cvjena.github.io/artos/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ARTOS is all about creating, tuning, and applying object detection models\nwith just a few clicks. In particular, ARTOS facilitates learning of models for\nvisual object detection by eliminating the burden of having to collect and\nannotate a large set of positive and negative samples manually and in addition\nit implements a fast learning technique to reduce the time needed for the\nlearning step.\n  A clean and friendly GUI guides the user through the process of model\ncreation, adaptation of learned models to different domains using in-situ\nimages, and object detection on both offline images and images from a video\nstream. A library written in C++ provides the main functionality of ARTOS with\na C-style procedural interface, so that it can be easily integrated with any\nother project.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 08:02:23 GMT"}, {"version": "v2", "created": "Mon, 25 Aug 2014 12:55:37 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1407.2776", "submitter": "Seyed-Mahdi Khaligh-Razavi", "authors": "Seyed-Mahdi Khaligh-Razavi", "title": "What you need to know about the state-of-the-art computational models of\n  object-vision: A tour through the models", "comments": "36 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of object vision have been of great interest in computer vision and\nvisual neuroscience. During the last decades, several models have been\ndeveloped to extract visual features from images for object recognition tasks.\nSome of these were inspired by the hierarchical structure of primate visual\nsystem, and some others were engineered models. The models are varied in\nseveral aspects: models that are trained by supervision, models trained without\nsupervision, and models (e.g. feature extractors) that are fully hard-wired and\ndo not need training. Some of the models come with a deep hierarchical\nstructure consisting of several layers, and some others are shallow and come\nwith only one or two layers of processing. More recently, new models have been\ndeveloped that are not hand-tuned but trained using millions of images, through\nwhich they learn how to extract informative task-related features. Here I will\nsurvey all these different models and provide the reader with an intuitive, as\nwell as a more detailed, understanding of the underlying computations in each\nof the models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 13:15:18 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Khaligh-Razavi", "Seyed-Mahdi", ""]]}, {"id": "1407.2904", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "An eigenanalysis of data centering in machine learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many pattern recognition methods rely on statistical information from\ncentered data, with the eigenanalysis of an empirical central moment, such as\nthe covariance matrix in principal component analysis (PCA), as well as partial\nleast squares regression, canonical-correlation analysis and Fisher\ndiscriminant analysis. Recently, many researchers advocate working on\nnon-centered data. This is the case for instance with the singular value\ndecomposition approach, with the (kernel) entropy component analysis, with the\ninformation-theoretic learning framework, and even with nonnegative matrix\nfactorization. Moreover, one can also consider a non-centered PCA by using the\nsecond-order non-central moment.\n  The main purpose of this paper is to bridge the gap between these two\nviewpoints in designing machine learning methods. To provide a study at the\ncornerstone of kernel-based machines, we conduct an eigenanalysis of the inner\nproduct matrices from centered and non-centered data. We derive several results\nconnecting their eigenvalues and their eigenvectors. Furthermore, we explore\nthe outer product matrices, by providing several results connecting the largest\neigenvectors of the covariance matrix and its non-centered counterpart. These\nresults lay the groundwork to several extensions beyond conventional centering,\nwith the weighted mean shift, the rank-one update, and the multidimensional\nscaling. Experiments conducted on simulated and real data illustrate the\nrelevance of this work.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:04:49 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1407.2961", "submitter": "Youness Aliyari Ghassabeh", "authors": "Youness Aliyari Ghassabeh", "title": "On the Convergence of the Mean Shift Algorithm in the One-Dimensional\n  Space", "comments": "13 pages, 10 figures, Published in Pattern Recognition Letters", "journal-ref": "Pattern Recognition Letters, 2013, vol. 34(12)", "doi": "10.1016/j.patrec.2013.05.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean shift algorithm is a non-parametric and iterative technique that has\nbeen used for finding modes of an estimated probability density function. It\nhas been successfully employed in many applications in specific areas of\nmachine vision, pattern recognition, and image processing. Although the mean\nshift algorithm has been used in many applications, a rigorous proof of its\nconvergence is still missing in the literature. In this paper we address the\nconvergence of the mean shift algorithm in the one-dimensional space and prove\nthat the sequence generated by the mean shift algorithm is a monotone and\nconvergent sequence.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 20:55:25 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Ghassabeh", "Youness Aliyari", ""]]}, {"id": "1407.2987", "submitter": "Eren Golge", "authors": "Eren Golge and Pinar Duygulu", "title": "FAME: Face Association through Model Evolution", "comments": "Draft version of the study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We attack the problem of learning face models for public faces from\nweakly-labelled images collected from web through querying a name. The data is\nvery noisy even after face detection, with several irrelevant faces\ncorresponding to other people. We propose a novel method, Face Association\nthrough Model Evolution (FAME), that is able to prune the data in an iterative\nway, for the face models associated to a name to evolve. The idea is based on\ncapturing discriminativeness and representativeness of each instance and\neliminating the outliers. The final models are used to classify faces on novel\ndatasets with possibly different characteristics. On benchmark datasets, our\nresults are comparable to or better than state-of-the-art studies for the task\nof face identification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 23:52:44 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Golge", "Eren", ""], ["Duygulu", "Pinar", ""]]}, {"id": "1407.3026", "submitter": "Hima Patel", "authors": "Ramasubramanian Sundararajan, Hima Patel, Dattesh Shanbhag, Vivek\n  Vaidya", "title": "An SVM Based Approach for Cardiac View Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the problem of automatically prescribing oblique planes (short\naxis, 4 chamber and 2 chamber views) in Cardiac Magnetic Resonance Imaging\n(MRI). A concern with technologist-driven acquisitions of these planes is the\nquality and time taken for the total examination. We propose an automated\nsolution incorporating anatomical features external to the cardiac region. The\nsolution uses support vector machine regression models wherein complexity and\nfeature selection are optimized using multi-objective genetic algorithms.\nAdditionally, we examine the robustness of our approach by training our models\non images with additive Rician-Gaussian mixtures at varying Signal to Noise\n(SNR) levels. Our approach has shown promising results, with an angular\ndeviation of less than 15 degrees on 90% cases across oblique planes, measured\nin terms of average 6-fold cross validation performance -- this is generally\nwithin acceptable bounds of variation as specified by clinicians.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 04:56:49 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Sundararajan", "Ramasubramanian", ""], ["Patel", "Hima", ""], ["Shanbhag", "Dattesh", ""], ["Vaidya", "Vivek", ""]]}, {"id": "1407.3068", "submitter": "Marijn Stollenga", "authors": "Marijn Stollenga, Jonathan Masci, Faustino Gomez, Juergen Schmidhuber", "title": "Deep Networks with Internal Selective Attention through Feedback\n  Connections", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional convolutional neural networks (CNN) are stationary and\nfeedforward. They neither change their parameters during evaluation nor use\nfeedback from higher to lower layers. Real brains, however, do. So does our\nDeep Attention Selective Network (dasNet) architecture. DasNets feedback\nstructure can dynamically alter its convolutional filter sensitivities during\nclassification. It harnesses the power of sequential processing to improve\nclassification performance, by allowing the network to iteratively focus its\ninternal attention on some of its convolutional filters. Feedback is trained\nthrough direct policy search in a huge million-dimensional parameter space,\nthrough scalable natural evolution strategies (SNES). On the CIFAR-10 and\nCIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 08:56:54 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 08:22:50 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Stollenga", "Marijn", ""], ["Masci", "Jonathan", ""], ["Gomez", "Faustino", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1407.3176", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Ulas Bagci, Brent Foster, Ziyue Xu, Deborah Douglas,\n  Jeffrey M. Solomon, Jayaram K. Udupa, Daniel J. Mollura", "title": "CIDI-Lung-Seg: A Single-Click Annotation Tool for Automatic Delineation\n  of Lungs from CT Scans", "comments": "4 pages, 6 figures; to appear in the proceedings of 36th Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society (EMBC 2014)", "journal-ref": null, "doi": "10.1109/EMBC.2014.6943783", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Accurate and fast extraction of lung volumes from computed tomography (CT)\nscans remains in a great demand in the clinical environment because the\navailable methods fail to provide a generic solution due to wide anatomical\nvariations of lungs and existence of pathologies. Manual annotation, current\ngold standard, is time consuming and often subject to human bias. On the other\nhand, current state-of-the-art fully automated lung segmentation methods fail\nto make their way into the clinical practice due to their inability to\nefficiently incorporate human input for handling misclassifications and praxis.\nThis paper presents a lung annotation tool for CT images that is interactive,\nefficient, and robust. The proposed annotation tool produces an \"as accurate as\npossible\" initial annotation based on the fuzzy-connectedness image\nsegmentation, followed by efficient manual fixation of the initial extraction\nif deemed necessary by the practitioner. To provide maximum flexibility to the\nusers, our annotation tool is supported in three major operating systems\n(Windows, Linux, and the Mac OS X). The quantitative results comparing our free\nsoftware with commercially available lung segmentation tools show higher degree\nof consistency and precision of our software with a considerable potential to\nenhance the performance of routine clinical tasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 14:41:27 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Mansoor", "Awais", ""], ["Bagci", "Ulas", ""], ["Foster", "Brent", ""], ["Xu", "Ziyue", ""], ["Douglas", "Deborah", ""], ["Solomon", "Jeffrey M.", ""], ["Udupa", "Jayaram K.", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1407.3179", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Ulas Bagci, and Daniel J. Mollura", "title": "Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation", "comments": "4 pages, 5 figures; to appear in the proceedings of 36th Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society (EMBC 2014)", "journal-ref": null, "doi": "10.1109/EMBC.2014.6945004", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Accurate delineation of pathological lungs from computed tomography (CT)\nimages remains mostly unsolved because available methods fail to provide a\nreliable generic solution due to high variability of abnormality appearance.\nLocal descriptor-based classification methods have shown to work well in\nannotating pathologies; however, these methods are usually computationally\nintensive which restricts their widespread use in real-time or near-real-time\nclinical applications. In this paper, we present a novel approach for fast,\naccurate, reliable segmentation of pathological lungs from CT scans by\ncombining region-based segmentation method with local descriptor classification\nthat is performed on an optimized sampling grid. Our method works in two\nstages; during stage one, we adapted the fuzzy connectedness (FC) image\nsegmentation algorithm to perform initial lung parenchyma extraction. In the\nsecond stage, texture-based local descriptors are utilized to segment abnormal\nimaging patterns using a near optimal keypoint analysis by employing centroid\nof supervoxel as grid points. The quantitative results show that our\npathological lung segmentation method is fast, robust, and improves on current\nstandards and has potential to enhance the performance of routine clinical\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 14:53:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Mansoor", "Awais", ""], ["Bagci", "Ulas", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1407.3193", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Ulas Bagci, Daniel J. Mollura", "title": "Optimally Stabilized PET Image Denoising Using Trilateral Filtering", "comments": "8 pages, 3 figures; to appear in the Lecture Notes in Computer\n  Science (MICCAI 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Low-resolution and signal-dependent noise distribution in positron emission\ntomography (PET) images makes denoising process an inevitable step prior to\nqualitative and quantitative image analysis tasks. Conventional PET denoising\nmethods either over-smooth small-sized structures due to resolution limitation\nor make incorrect assumptions about the noise characteristics. Therefore,\nclinically important quantitative information may be corrupted. To address\nthese challenges, we introduced a novel approach to remove signal-dependent\nnoise in the PET images where the noise distribution was considered as\nPoisson-Gaussian mixed. Meanwhile, the generalized Anscombe's transformation\n(GAT) was used to stabilize varying nature of the PET noise. Other than noise\nstabilization, it is also desirable for the noise removal filter to preserve\nthe boundaries of the structures while smoothing the noisy regions. Indeed, it\nis important to avoid significant loss of quantitative information such as\nstandard uptake value (SUV)-based metrics as well as metabolic lesion volume.\nTo satisfy all these properties, we extended bilateral filtering method into\ntrilateral filtering through multiscaling and optimal Gaussianization process.\nThe proposed method was tested on more than 50 PET-CT images from various\npatients having different cancers and achieved the superior performance\ncompared to the widely used denoising techniques in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 15:08:18 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Mansoor", "Awais", ""], ["Bagci", "Ulas", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1407.3234", "submitter": "Bin Han", "authors": "Yi Shen, Bin Han, and Elena Braverman", "title": "Image Inpainting Using Directional Tensor Product Complex Tight\n  Framelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are particularly interested in the image inpainting problem\nusing directional complex tight wavelet frames. Under the assumption that frame\ncoefficients of images are sparse, several iterative thresholding algorithms\nfor the image inpainting problem have been proposed in the literature. The\noutputs of such iterative algorithms are closely linked to solutions of several\nconvex minimization models using the balanced approach which simultaneously\ncombines the $l_1$-regularization for sparsity of frame coefficients and the\n$l_2$-regularization for smoothness of the solution. Due to the redundancy of a\ntight frame, elements of a tight frame could be highly correlated and\ntherefore, their corresponding frame coefficients of an image are expected to\nclose to each other. This is called the grouping effect in statistics. In this\npaper, we establish the grouping effect property for frame-based convex\nminimization models using the balanced approach. This result on grouping effect\npartially explains the effectiveness of models using the balanced approach for\nseveral image restoration problems. Inspired by recent development on\ndirectional tensor product complex tight framelets (TP-CTFs) and their\nimpressive performance for the image denoising problem, in this paper we\npropose an iterative thresholding algorithm using a single tight frame derived\nfrom TP-CTFs for the image inpainting problem. Experimental results show that\nour proposed algorithm can handle well both cartoons and textures\nsimultaneously and performs comparably and often better than several well-known\nframe-based iterative thresholding algorithms for the image inpainting problem\nwithout noise. For the image inpainting problem with additive zero-mean i.i.d.\nGaussian noise, our proposed algorithm using TP-CTFs performs superior than\nother known state-of-the-art frame-based image inpainting algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 17:49:00 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Shen", "Yi", ""], ["Han", "Bin", ""], ["Braverman", "Elena", ""]]}, {"id": "1407.3399", "submitter": "Xianjie Chen", "authors": "Xianjie Chen, Alan Yuille", "title": "Articulated Pose Estimation by a Graphical Model with Image Dependent\n  Pairwise Relations", "comments": "NIPS 2014 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating articulated human pose from a single\nstatic image based on a graphical model with novel pairwise relations that make\nadaptive use of local image measurements. More precisely, we specify a\ngraphical model for human pose which exploits the fact the local image\nmeasurements can be used both to detect parts (or joints) and also to predict\nthe spatial relationships between them (Image Dependent Pairwise Relations).\nThese spatial relationships are represented by a mixture model. We use Deep\nConvolutional Neural Networks (DCNNs) to learn conditional probabilities for\nthe presence of parts and their spatial relationships within image patches.\nHence our model combines the representational flexibility of graphical models\nwith the efficiency and statistical power of DCNNs. Our method significantly\noutperforms the state of the art methods on the LSP and FLIC datasets and also\nperforms very well on the Buffy dataset without any training.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 17:04:21 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 17:28:15 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Chen", "Xianjie", ""], ["Yuille", "Alan", ""]]}, {"id": "1407.3535", "submitter": "Arif Mahmood", "authors": "Arif Mahmood, Ajmal Mian and Robyn Owens", "title": "Optimizing Auto-correlation for Fast Target Search in Large Search Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing image-blurring is induced by many sources such as\natmospheric scatter, optical aberration, spatial and temporal sensor\nintegration. The natural blurring can be exploited to speed up target search by\nfast template matching. In this paper, we synthetically induce additional\nnon-uniform blurring to further increase the speed of the matching process. To\navoid loss of accuracy, the amount of synthetic blurring is varied spatially\nover the image according to the underlying content. We extend transitive\nalgorithm for fast template matching by incorporating controlled image blur. To\nthis end we propose an Efficient Group Size (EGS) algorithm which minimizes the\nnumber of similarity computations for a particular search image. A larger\nefficient group size guarantees less computations and more speedup. EGS\nalgorithm is used as a component in our proposed Optimizing auto-correlation\n(OptA) algorithm. In OptA a search image is iteratively non-uniformly blurred\nwhile ensuring no accuracy degradation at any image location. In each iteration\nefficient group size and overall computations are estimated by using the\nproposed EGS algorithm. The OptA algorithm stops when the number of\ncomputations cannot be further decreased without accuracy degradation. The\nproposed algorithm is compared with six existing state of the art exhaustive\naccuracy techniques using correlation coefficient as the similarity measure.\nExperiments on satellite and aerial image datasets demonstrate the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 03:57:57 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 00:47:47 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Mahmood", "Arif", ""], ["Mian", "Ajmal", ""], ["Owens", "Robyn", ""]]}, {"id": "1407.3540", "submitter": "Tarek El-Gaaly", "authors": "Tarek El-Gaaly, Joshua Gluckman", "title": "Measuring Atmospheric Scattering from Digital Images of Urban Scenery\n  using Temporal Polarization-Based Vision", "comments": "Masters in Computer Science Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particulate Matter (PM) is a form of air pollution that visually degrades\nurban scenery and is hazardous to human health and the environment. Current\nmonitoring devices are limited in measuring average PM over large areas.\nQuantifying the visual effects of haze in digital images of urban scenery and\ncorrelating these effects to PM levels is a vital step in more practically\nmonitoring our environment. Current image haze extraction algorithms remove\nhaze from the scene for the sole purpose of enhancing vision. We present two\nalgorithms which bridge the gap between image haze extraction and environmental\nmonitoring. We provide a means of measuring atmospheric scattering from images\nof urban scenery by incorporating temporal knowledge. In doing so, we also\npresent a method of recovering an accurate depthmap of the scene and recovering\nthe scene without the visual effects of haze. We compare our algorithm to three\nknown haze removal methods. The algorithms are composed of an optimization over\na model of haze formation in images and an optimization using a constraint of\nconstant depth over a sequence of images taken over time. These algorithms not\nonly measure atmospheric scattering, but also recover a more accurate depthmap\nand dehazed image. The measurements of atmospheric scattering this research\nproduces, can be directly correlated to PM levels and therefore pave the way to\nmonitoring the health of the environment by visual means. Accurate atmospheric\nsensing from digital images is a challenging and under-researched problem. This\nwork provides an important step towards a more practical and accurate visual\nmeans of measuring PM from digital images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 04:36:31 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["El-Gaaly", "Tarek", ""], ["Gluckman", "Joshua", ""]]}, {"id": "1407.3664", "submitter": "Mohammed Abdelsamea", "authors": "Mohammed M. Abdelsamea", "title": "An Enhancement Neighborhood connected Segmentation for 2D-Cellular Image", "comments": "International Journal of International Journal of Bioscience,\n  Biochemistry and Bioinformatics, 2011. arXiv admin note: text overlap with\n  arXiv:1406.0074 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good segmentation result depends on a set of \"correct\" choice for the\nseeds. When the input images are noisy, the seeds may fall on atypical pixels\nthat are not representative of the region statistics. This can lead to\nerroneous segmentation results. In this paper, an automatic seeded region\ngrowing algorithm is proposed for cellular image segmentation. First, the\nregions of interest (ROIs) extracted from the preprocessed image. Second, the\ninitial seeds are automatically selected based on ROIs extracted from the\nimage. Third, the most reprehensive seeds are selected using a machine learning\nalgorithm. Finally, the cellular image is segmented into regions where each\nregion corresponds to a seed. The aim of the proposed is to automatically\nextract the Region of Interests (ROI) from in the cellular images in terms of\novercoming the explosion, under segmentation and over segmentation problems.\nExperimental results show that the proposed algorithm can improve the segmented\nimage and the segmented results are less noisy as compared to some existing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 14:21:38 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Abdelsamea", "Mohammed M.", ""]]}, {"id": "1407.3673", "submitter": "arXiv Admin", "authors": "Isha Tyagi, Ashish Nautiyal, Vishwanath Bijalwan, Meenu Balodhi", "title": "Enhanced EZW Technique for Compression of Image by Setting Detail\n  Retaining Pass Number", "comments": "This submission has been withdrawn by arXiv administrators because it\n  contains excessive and unattributed reuse of content from other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This submission has been withdrawn by arXiv administrators because it\ncontains excessive and unattributed reuse of content from other authors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 10:55:52 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 16:07:15 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Tyagi", "Isha", ""], ["Nautiyal", "Ashish", ""], ["Bijalwan", "Vishwanath", ""], ["Balodhi", "Meenu", ""]]}, {"id": "1407.3675", "submitter": "Archana Vijayan", "authors": "Archana Vijayan, Vincy Salam", "title": "A New Approach for Super resolution by Using Web Images and FFT Based\n  Image Registration", "comments": "7 pages,4 figures,Published with International Journal of Engineering\n  Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT), V12(9),473-479 June 2014", "doi": "10.14445/22315381/IJETT-V12P289", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving accuracy is a challenging issue in super resolution images. In\nthis paper, we propose a new FFT based image registration algorithm and a\nsparse based super resolution algorithm to improve the accuracy of super\nresolution image. Given a low resolution image, our approach initially extracts\nthe local descriptors from the input and then the local descriptors from the\nwhole correlated images using the SIFT algorithm. Once this is completed, it\nwill compare the local descriptors on the basis of a threshold value. The\nretrieved images could be having different focal length, illumination,\ninclination and size. To overcome the above differences of the retrieved\nimages, we propose a new FFT based image registration algorithm. After the\nregistration stage, we apply a sparse based super resolution on the images for\nrecreating images with better resolution compared to the input. Based on the\nPSSNR calculation and SSIM comparison, we can see that the new methodology\ncreates a better image than the traditional methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 13:10:01 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Vijayan", "Archana", ""], ["Salam", "Vincy", ""]]}, {"id": "1407.3686", "submitter": "Alejandro Gonz\\'alez Alzate", "authors": "Alejandro Gonz\\'alez and Sebastian Ramos and David V\\'azquez and\n  Antonio M. L\\'opez and Jaume Amores", "title": "Spatiotemporal Stacked Sequential Learning for Pedestrian Detection", "comments": "8 pages, 5 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian classifiers decide which image windows contain a pedestrian. In\npractice, such classifiers provide a relatively high response at neighbor\nwindows overlapping a pedestrian, while the responses around potential false\npositives are expected to be lower. An analogous reasoning applies for image\nsequences. If there is a pedestrian located within a frame, the same pedestrian\nis expected to appear close to the same location in neighbor frames. Therefore,\nsuch a location has chances of receiving high classification scores during\nseveral frames, while false positives are expected to be more spurious. In this\npaper we propose to exploit such correlations for improving the accuracy of\nbase pedestrian classifiers. In particular, we propose to use two-stage\nclassifiers which not only rely on the image descriptors required by the base\nclassifiers but also on the response of such base classifiers in a given\nspatiotemporal neighborhood. More specifically, we train pedestrian classifiers\nusing a stacked sequential learning (SSL) paradigm. We use a new pedestrian\ndataset we have acquired from a car to evaluate our proposal at different frame\nrates. We also test on a well known dataset: Caltech. The obtained results show\nthat our SSL proposal boosts detection accuracy significantly with a minimal\nimpact on the computational cost. Interestingly, SSL improves more the accuracy\nat the most dangerous situations, i.e. when a pedestrian is close to the\ncamera.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 15:03:01 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Gonz\u00e1lez", "Alejandro", ""], ["Ramos", "Sebastian", ""], ["V\u00e1zquez", "David", ""], ["L\u00f3pez", "Antonio M.", ""], ["Amores", "Jaume", ""]]}, {"id": "1407.3695", "submitter": "Isidora Stankovic", "authors": "Isidora Stankovi\\'c", "title": "Recovery of Images with Missing Pixels using a Gradient Compressive\n  Sensing Algorithm", "comments": "7 pages, 12 figures, A part of the Final year project at the\n  University of Westminster, London, United Kingdom, supervised by Dragana\n  Barjamovic (submitted 28. April 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the possibility of reconstruction of images\nconsidering that they are sparse in the DCT transformation domain. Two\napproaches are considered. One when the image is pre-processed in the DCT\ndomain, using 8x8 blocks. The image is made sparse by setting the smallest DCT\ncoefficients to zero. In the other case the original image is considered\nwithout pre-processing, assuming the sparsity as intrinsic property of the\nanalyzed image. A gradient based algorithm is used to recover a large number of\nmissing pixels in the image. The case of a salt-and-paper noise affecting a\nlarge number of pixels is easily reduced to the case of missing pixels and\nconsidered within the same framework. The reconstruction of images affected\nwith salt-and-paper impulsive is compared with the images filtered using a\nmedian filter. The same algorithm can be used considering transformation of the\nwhole image. Reconstructions of black and white and colour images are\nconsidered.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 20:18:32 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Stankovi\u0107", "Isidora", ""]]}, {"id": "1407.3840", "submitter": "Lee-Kang Liu", "authors": "Lee-Kang Liu, Stanley H. Chan, and Truong Q. Nguyen", "title": "Depth Reconstruction from Sparse Samples: Representation, Algorithm, and\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of 3D technology and computer vision applications have\nmotivated a thrust of methodologies for depth acquisition and estimation.\nHowever, most existing hardware and software methods have limited performance\ndue to poor depth precision, low resolution and high computational cost. In\nthis paper, we present a computationally efficient method to recover dense\ndepth maps from sparse measurements. We make three contributions. First, we\nprovide empirical evidence that depth maps can be encoded much more sparsely\nthan natural images by using common dictionaries such as wavelets and\ncontourlets. We also show that a combined wavelet-contourlet dictionary\nachieves better performance than using either dictionary alone. Second, we\npropose an alternating direction method of multipliers (ADMM) to achieve fast\nreconstruction. A multi-scale warm start procedure is proposed to speed up the\nconvergence. Third, we propose a two-stage randomized sampling scheme to\noptimally choose the sampling locations, thus maximizing the reconstruction\nperformance for any given sampling budget. Experimental results show that the\nproposed method produces high quality dense depth estimates, and is robust to\nnoisy measurements. Applications to real data in stereo matching are\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 22:52:05 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 21:58:20 GMT"}, {"version": "v3", "created": "Wed, 12 Nov 2014 04:54:48 GMT"}, {"version": "v4", "created": "Thu, 12 Feb 2015 01:39:27 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Liu", "Lee-Kang", ""], ["Chan", "Stanley H.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1407.3867", "submitter": "Ning Zhang", "authors": "Ning Zhang, Jeff Donahue, Ross Girshick, Trevor Darrell", "title": "Part-based R-CNNs for Fine-grained Category Detection", "comments": "16 pages. To appear at European Conference on Computer Vision (ECCV),\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic part localization can facilitate fine-grained categorization by\nexplicitly isolating subtle appearance differences associated with specific\nobject parts. Methods for pose-normalized representations have been proposed,\nbut generally presume bounding box annotations at test time due to the\ndifficulty of object detection. We propose a model for fine-grained\ncategorization that overcomes these limitations by leveraging deep\nconvolutional features computed on bottom-up region proposals. Our method\nlearns whole-object and part detectors, enforces learned geometric constraints\nbetween them, and predicts a fine-grained category from a pose-normalized\nrepresentation. Experiments on the Caltech-UCSD bird dataset confirm that our\nmethod outperforms state-of-the-art fine-grained categorization methods in an\nend-to-end evaluation without requiring a bounding box at test time.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 02:32:16 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Zhang", "Ning", ""], ["Donahue", "Jeff", ""], ["Girshick", "Ross", ""], ["Darrell", "Trevor", ""]]}, {"id": "1407.3956", "submitter": "Bernhard Schmitzer", "authors": "Bernhard Schmitzer and Christoph Schn\\\"orr", "title": "Globally Optimal Joint Image Segmentation and Shape Matching Based on\n  Wasserstein Modes", "comments": "31 pages, 16 figures. Accepted by Journal of Mathematical Imaging and\n  Vision, published online. Printed publication pending", "journal-ref": null, "doi": "10.1007/s10851-014-0546-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A functional for joint variational object segmentation and shape matching is\ndeveloped. The formulation is based on optimal transport w.r.t. geometric\ndistance and local feature similarity. Geometric invariance and modelling of\nobject-typical statistical variations is achieved by introducing degrees of\nfreedom that describe transformations and deformations of the shape template.\nThe shape model is mathematically equivalent to contour-based approaches but\ninference can be performed without conversion between the contour and region\nrepresentations, allowing combination with other convex segmentation approaches\nand simplifying optimization. While the overall functional is non-convex,\nnon-convexity is confined to a low-dimensional variable. We propose a locally\noptimal alternating optimization scheme and a globally optimal branch and bound\nscheme, based on adaptive convex relaxation. Combining both methods allows to\neliminate the delicate initialization problem inherent to many contour based\napproaches while remaining computationally practical. The properties of the\nfunctional, its ability to adapt to a wide range of input data structures and\nthe different optimization schemes are illustrated and compared by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 12:12:54 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 10:22:19 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Schmitzer", "Bernhard", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1407.3969", "submitter": "Anna Maria Massone", "authors": "Giorgio Ricca, Mauro C. Beltrametti, Anna Maria Massone", "title": "An iterative approach to Hough transform without re-voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many bone shapes in the human skeleton are characterized by profiles that can\nbe associated to equations of algebraic curves. Fixing the parameters in the\ncurve equation, by means of a classical pattern recognition procedure like the\nHough transform technique, it is then possible to associate an equation to a\nspecific bone profile. However, most skeleton districts are more accurately\ndescribed by piecewise defined curves. This paper utilizes an iterative\napproach of the Hough transform without re-voting, to provide an efficient\nprocedure for describing the profile of a bone in the human skeleton as a\ncollection of different but continuously attached curves.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 12:56:35 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Ricca", "Giorgio", ""], ["Beltrametti", "Mauro C.", ""], ["Massone", "Anna Maria", ""]]}, {"id": "1407.3986", "submitter": "Haritha Raveendran", "authors": "Haritha Raveendran, Deepa Thomas", "title": "Image Fusion Using LEP Filtering and Bilinear Interpolation", "comments": "5 pages. 4 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT), V12(9),427-431 June 2014", "doi": "10.14445/22315381/IJETT-V12P282", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Image Fusion is the process in which core information from a set of component\nimages is merged to form a single image, which is more informative and complete\nthan the component input images in quality and appearance. This paper presents\na fast and effective image fusion method for creating high quality fused images\nby merging component images. In the proposed method, the input image is broken\ndown to a two-scale image representation with a base layer having large scale\nvariations in intensity, and a detail layer containing small scale details.\nHere fusion of the base and detail layers is implemented by means of a Local\nEdge preserving filtering based technique. The proposed method is an efficient\nimage fusion technique in which the noise component is very low and quality of\nthe resultant image is high so that it can be used for applications like\nmedical image processing, requiring very accurate edge preserved images.\nPerformance is tested by calculating PSNR and SSIM of images. The benefit of\nthe proposed method is that it removes noise without altering the underlying\nstructures of the image. This paper also presents an image zooming technique\nusing bilinear interpolation in which a portion of the input image is cropped\nand bilinear interpolation is applied. Experimental results showed that the\nwhen PSNR value is calculated, the noise is found to be very low for the\nresultant image portion.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 13:12:39 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Raveendran", "Haritha", ""], ["Thomas", "Deepa", ""]]}, {"id": "1407.4023", "submitter": "Bin Yang", "authors": "Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li", "title": "Aggregate channel features for multi-view face detection", "comments": "8 pages, 6 figures. Submitted to International Joint Conference on\n  Biometrics, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has drawn much attention in recent decades since the seminal\nwork by Viola and Jones. While many subsequences have improved the work with\nmore powerful learning algorithms, the feature representation used for face\ndetection still can't meet the demand for effectively and efficiently handling\nfaces with large appearance variance in the wild. To solve this bottleneck, we\nborrow the concept of channel features to the face detection domain, which\nextends the image channel to diverse types like gradient magnitude and oriented\ngradient histograms and therefore encodes rich information in a simple form. We\nadopt a novel variant called aggregate channel features, make a full\nexploration of feature design, and discover a multi-scale version of features\nwith better performance. To deal with poses of faces in the wild, we propose a\nmulti-view detection approach featuring score re-ranking and detection\nadjustment. Following the learning pipelines in Viola-Jones framework, the\nmulti-view face detector using aggregate channel features shows competitive\nperformance against state-of-the-art algorithms on AFW and FDDB testsets, while\nruns at 42 FPS on VGA images.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 15:31:39 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 10:27:23 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Yang", "Bin", ""], ["Yan", "Junjie", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1407.4118", "submitter": "Lise du Buisson", "authors": "L. du Buisson, N. Sivanandam, B.A. Bassett and M. Smith", "title": "Machine Learning Classification of SDSS Transient Survey Images", "comments": "14 pages, 8 figures. In this version extremely minor adjustments to\n  the paper were made - e.g. Figure 5 is now easier to view in greyscale", "journal-ref": "L. du Buisson; N. Sivanandam; Bruce A. Bassett; M. Smith; Monthly\n  Notices of the Royal Astronomical Society, 2015, 454 (2): 2026-2038", "doi": "10.1093/mnras/stv2041", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that multiple machine learning algorithms can match human performance\nin classifying transient imaging data from the Sloan Digital Sky Survey (SDSS)\nsupernova survey into real objects and artefacts. This is a first step in any\ntransient science pipeline and is currently still done by humans, but future\nsurveys such as the Large Synoptic Survey Telescope (LSST) will necessitate\nfully machine-enabled solutions. Using features trained from eigenimage\nanalysis (principal component analysis, PCA) of single-epoch g, r and\ni-difference images, we can reach a completeness (recall) of 96 per cent, while\nonly incorrectly classifying at most 18 per cent of artefacts as real objects,\ncorresponding to a precision (purity) of 84 per cent. In general, random\nforests performed best, followed by the k-nearest neighbour and the SkyNet\nartificial neural net algorithms, compared to other methods such as na\\\"ive\nBayes and kernel support vector machine. Our results show that PCA-based\nmachine learning can match human success levels and can naturally be extended\nby including multiple epochs of data, transient colours and host galaxy\ninformation which should allow for significant further improvements, especially\nat low signal-to-noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 20:00:02 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 20:00:15 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 18:21:46 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Buisson", "L. du", ""], ["Sivanandam", "N.", ""], ["Bassett", "B. A.", ""], ["Smith", "M.", ""]]}, {"id": "1407.4206", "submitter": "Yichao Xu", "authors": "Yichao Xu, Kazuki Maeno, Hajime Nagahara, Rin-ichiro Taniguchi", "title": "Mobile Camera Array Calibration for Light Field Acquisition", "comments": "11th International Conference on Quality Control by Artificial Vision\n  (QCAV2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light field camera is useful for computer graphics and vision\napplications. Calibration is an essential step for these applications. After\ncalibration, we can rectify the captured image by using the calibrated camera\nparameters. However, the large camera array calibration method, which assumes\nthat all cameras are on the same plane, ignores the orientation and intrinsic\nparameters. The multi-camera calibration technique usually assumes that the\nworking volume and viewpoints are fixed. In this paper, we describe a\ncalibration algorithm suitable for a mobile camera array based light field\nacquisition system. The algorithm performs in Zhang's style by moving a\ncheckerboard, and computes the initial parameters in closed form. Global\noptimization is then applied to refine all the parameters simultaneously. Our\nimplementation is rather flexible in that users can assign the number of\nviewpoints and refinement of intrinsic parameters is optional. Experiments on\nboth simulated data and real data acquired by a commercial product show that\nour method yields good results. Digital refocusing application shows the\ncalibrated light field can well focus to the target object we desired.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 06:38:24 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Xu", "Yichao", ""], ["Maeno", "Kazuki", ""], ["Nagahara", "Hajime", ""], ["Taniguchi", "Rin-ichiro", ""]]}, {"id": "1407.4420", "submitter": "Paul Honeine", "authors": "Fei Zhu, Paul Honeine, Maya Kallas", "title": "Kernel Nonnegative Matrix Factorization Without the Curse of the\n  Pre-image - Application to Unmixing Hyperspectral Images", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonnegative matrix factorization (NMF) is widely used in signal and image\nprocessing, including bio-informatics, blind source separation and\nhyperspectral image analysis in remote sensing. A great challenge arises when\ndealing with a nonlinear formulation of the NMF. Within the framework of kernel\nmachines, the models suggested in the literature do not allow the\nrepresentation of the factorization matrices, which is a fallout of the curse\nof the pre-image. In this paper, we propose a novel kernel-based model for the\nNMF that does not suffer from the pre-image problem, by investigating the\nestimation of the factorization matrices directly in the input space. For\ndifferent kernel functions, we describe two schemes for iterative algorithms:\nan additive update rule based on a gradient descent scheme and a multiplicative\nupdate rule in the same spirit as in the Lee and Seung algorithm. Within the\nproposed framework, we develop several extensions to incorporate constraints,\nincluding sparseness, smoothness, and spatial regularization with a\ntotal-variation-like penalty. The effectiveness of the proposed method is\ndemonstrated with the problem of unmixing hyperspectral images, using\nwell-known real images and results with state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:46:41 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2016 20:44:42 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Zhu", "Fei", ""], ["Honeine", "Paul", ""], ["Kallas", "Maya", ""]]}, {"id": "1407.4739", "submitter": "T Sarath", "authors": "T.Sarath, G.Nagalakshmi", "title": "An landcover fuzzy logic classification by maximumlikelihood", "comments": "5 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In present days remote sensing is most used application in many sectors. This\nremote sensing uses different images like multispectral, hyper spectral or\nultra spectral. The remote sensing image classification is one of the\nsignificant method to classify image. In this state we classify the maximum\nlikelihood classification with fuzzy logic. In this we experimenting fuzzy\nlogic like spatial, spectral texture methods in that different sub methods to\nbe used for image classification.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 17:10:06 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Sarath", "T.", ""], ["Nagalakshmi", "G.", ""]]}, {"id": "1407.4764", "submitter": "Ken Chatfield", "authors": "Ken Chatfield, Karen Simonyan and Andrew Zisserman", "title": "Efficient On-the-fly Category Retrieval using ConvNets and GPUs", "comments": "Published in proceedings of ACCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the gains in precision and speed, that can be obtained by\nusing Convolutional Networks (ConvNets) for on-the-fly retrieval - where\nclassifiers are learnt at run time for a textual query from downloaded images,\nand used to rank large image or video datasets.\n  We make three contributions: (i) we present an evaluation of state-of-the-art\nimage representations for object category retrieval over standard benchmark\ndatasets containing 1M+ images; (ii) we show that ConvNets can be used to\nobtain features which are incredibly performant, and yet much lower dimensional\nthan previous state-of-the-art image representations, and that their\ndimensionality can be reduced further without loss in performance by\ncompression using product quantization or binarization. Consequently, features\nwith the state-of-the-art performance on large-scale datasets of millions of\nimages can fit in the memory of even a commodity GPU card; (iii) we show that\nan SVM classifier can be learnt within a ConvNet framework on a GPU in parallel\nwith downloading the new training images, allowing for a continuous refinement\nof the model as more images become available, and simultaneous training and\nranking. The outcome is an on-the-fly system that significantly outperforms its\npredecessors in terms of: precision of retrieval, memory requirements, and\nspeed, facilitating accurate on-the-fly learning and ranking in under a second\non a single GPU.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 18:29:38 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 08:27:23 GMT"}, {"version": "v3", "created": "Mon, 17 Nov 2014 12:10:23 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Chatfield", "Ken", ""], ["Simonyan", "Karen", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1407.4867", "submitter": "Pushkar Dixit", "authors": "Jay Prakash Gupta, Pushkar Dixit, Nishant Singh, Vijay Bhaskar Semwal", "title": "Analysis of Gait Pattern to Recognize the Human Activities", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition based on the computer vision is the process of\nlabelling image sequences with action labels. Accurate systems for this problem\nare applied in areas such as visual surveillance, human computer interaction\nand video retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 01:45:17 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 12:10:04 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Gupta", "Jay Prakash", ""], ["Dixit", "Pushkar", ""], ["Singh", "Nishant", ""], ["Semwal", "Vijay Bhaskar", ""]]}, {"id": "1407.4874", "submitter": "Bin Fan", "authors": "Zhenhua Wang, Bin Fan, and Fuchao Wu", "title": "Affine Subspace Representation for Feature Description", "comments": "To Appear in the 2014 European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Affine Subspace Representation (ASR) descriptor\nto deal with affine distortions induced by viewpoint changes. Unlike the\ntraditional local descriptors such as SIFT, ASR inherently encodes local\ninformation of multi-view patches, making it robust to affine distortions while\nmaintaining a high discriminative ability. To this end, PCA is used to\nrepresent affine-warped patches as PCA-patch vectors for its compactness and\nefficiency. Then according to the subspace assumption, which implies that the\nPCA-patch vectors of various affine-warped patches of the same keypoint can be\nrepresented by a low-dimensional linear subspace, the ASR descriptor is\nobtained by using a simple subspace-to-point mapping. Such a linear subspace\nrepresentation could accurately capture the underlying information of a\nkeypoint (local structure) under multiple views without sacrificing its\ndistinctiveness. To accelerate the computation of ASR descriptor, a fast\napproximate algorithm is proposed by moving the most computational part (ie,\nwarp patch under various affine transformations) to an offline training stage.\nExperimental results show that ASR is not only better than the state-of-the-art\ndescriptors under various image transformations, but also performs well without\na dedicated affine invariant detector when dealing with viewpoint changes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 02:33:56 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Wang", "Zhenhua", ""], ["Fan", "Bin", ""], ["Wu", "Fuchao", ""]]}, {"id": "1407.4898", "submitter": "Ghassem Tofighi", "authors": "Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N.\n  Venetsanopoulos", "title": "Hand Pointing Detection Using Live Histogram Template of Forehead Skin", "comments": "Accepted for oral presentation in DSP2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pointing detection has multiple applications in many fields such as\nvirtual reality and control devices in smart homes. In this paper, we proposed\na novel approach to detect pointing vector in 2D space of a room. After\nbackground subtraction, face and forehead is detected. In the second step,\nforehead skin H-S plane histograms in HSV space is calculated. By using these\nhistogram templates of users skin, and back projection method, skin areas are\ndetected. The contours of hand are extracted using Freeman chain code\nalgorithm. Next step is finding fingertips. Points in hand contour which are\ncandidates for the fingertip can be found in convex defects of convex hull and\ncontour. We introduced a novel method for finding the fingertip based on the\nspecial points on the contour and their relationships. Our approach detects\nhand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 07:10:03 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Tofighi", "Ghassem", ""], ["Afarin", "Nasser Ali", ""], ["Raahemifar", "Kamraan", ""], ["Venetsanopoulos", "Anastasios N.", ""]]}, {"id": "1407.4979", "submitter": "Dong Yi", "authors": "Dong Yi and Zhen Lei and Stan Z. Li", "title": "Deep Metric Learning for Practical Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various hand-crafted features and metric learning methods prevail in the\nfield of person re-identification. Compared to these methods, this paper\nproposes a more general way that can learn a similarity metric from image\npixels directly. By using a \"siamese\" deep neural network, the proposed method\ncan jointly learn the color feature, texture feature and metric in a unified\nframework. The network has a symmetry structure with two sub-networks which are\nconnected by Cosine function. To deal with the big variations of person images,\nbinomial deviance is used to evaluate the cost between similarities and labels,\nwhich is proved to be robust to outliers.\n  Compared to existing researches, a more practical setting is studied in the\nexperiments that is training and test on different datasets (cross dataset\nperson re-identification). Both in \"intra dataset\" and \"cross dataset\"\nsettings, the superiorities of the proposed method are illustrated on VIPeR and\nPRID.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 13:07:16 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Yi", "Dong", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1407.5035", "submitter": "Judy Hoffman", "authors": "Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff\n  Donahue, Ross Girshick, Trevor Darrell, and Kate Saenko", "title": "LSDA: Large Scale Detection Through Adaptation", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS) 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in scaling object detection is the difficulty of obtaining\nlabeled images for large numbers of categories. Recently, deep convolutional\nneural networks (CNNs) have emerged as clear winners on object classification\nbenchmarks, in part due to training with 1.2M+ labeled classification images.\nUnfortunately, only a small fraction of those labels are available for the\ndetection task. It is much cheaper and easier to collect large quantities of\nimage-level labels from search engines than it is to collect detection data and\nlabel it with precise bounding boxes. In this paper, we propose Large Scale\nDetection through Adaptation (LSDA), an algorithm which learns the difference\nbetween the two tasks and transfers this knowledge to classifiers for\ncategories without bounding box annotated data, turning them into detectors.\nOur method has the potential to enable detection for the tens of thousands of\ncategories that lack bounding box annotations, yet have plenty of\nclassification data. Evaluation on the ImageNet LSVRC-2013 detection challenge\ndemonstrates the efficacy of our approach. This algorithm enables us to produce\na >7.6K detector by using available classification data from leaf nodes in the\nImageNet tree. We additionally demonstrate how to modify our architecture to\nproduce a fast detector (running at 2fps for the 7.6K detector). Models and\nsoftware are available at\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 17:08:02 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 00:38:38 GMT"}, {"version": "v3", "created": "Sat, 1 Nov 2014 01:48:26 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Hoffman", "Judy", ""], ["Guadarrama", "Sergio", ""], ["Tzeng", "Eric", ""], ["Hu", "Ronghang", ""], ["Donahue", "Jeff", ""], ["Girshick", "Ross", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1407.5055", "submitter": "Enming Luo", "authors": "Enming Luo, Stanley H. Chan, Truong Q. Nguyen", "title": "Adaptive Image Denoising by Targeted Databases", "comments": "15 pages, 13 figures, 2 tables, journal", "journal-ref": null, "doi": "10.1109/TIP.2015.2414873", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-dependent denoising procedure to restore noisy images.\nDifferent from existing denoising algorithms which search for patches from\neither the noisy image or a generic database, the new algorithm finds patches\nfrom a database that contains only relevant patches. We formulate the denoising\nproblem as an optimal filter design problem and make two contributions. First,\nwe determine the basis function of the denoising filter by solving a group\nsparsity minimization problem. The optimization formulation generalizes\nexisting denoising algorithms and offers systematic analysis of the\nperformance. Improvement methods are proposed to enhance the patch search\nprocess. Second, we determine the spectral coefficients of the denoising filter\nby considering a localized Bayesian prior. The localized prior leverages the\nsimilarity of the targeted database, alleviates the intensive Bayesian\ncomputation, and links the new method to the classical linear minimum mean\nsquared error estimation. We demonstrate applications of the proposed method in\na variety of scenarios, including text images, multiview images and face\nimages. Experimental results show the superiority of the new algorithm over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 22:39:56 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 03:58:16 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 05:27:58 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Luo", "Enming", ""], ["Chan", "Stanley H.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1407.5104", "submitter": "Pulkit Agrawal", "authors": "Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant", "title": "Pixels to Voxels: Modeling Visual Representation in the Human Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is adept at solving difficult high-level visual processing\nproblems such as image interpretation and object recognition in natural scenes.\nOver the past few years neuroscientists have made remarkable progress in\nunderstanding how the human brain represents categories of objects and actions\nin natural scenes. However, all current models of high-level human vision\noperate on hand annotated images in which the objects and actions have been\nassigned semantic tags by a human operator. No current models can account for\nhigh-level visual function directly in terms of low-level visual input (i.e.,\npixels). To overcome this fundamental limitation we sought to develop a new\nclass of models that can predict human brain activity directly from low-level\nvisual input (i.e., pixels). We explored two classes of models drawn from\ncomputer vision and machine learning. The first class of models was based on\nFisher Vectors (FV) and the second was based on Convolutional Neural Networks\n(ConvNets). We find that both classes of models accurately predict brain\nactivity in high-level visual areas, directly from pixels and without the need\nfor any semantic tags or hand annotation of images. This is the first time that\nsuch a mapping has been obtained. The fit models provide a new platform for\nexploring the functional principles of human vision, and they show that modern\nmethods of computer vision and machine learning provide important tools for\ncharacterizing brain function.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 20:10:06 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Agrawal", "Pulkit", ""], ["Stansbury", "Dustin", ""], ["Malik", "Jitendra", ""], ["Gallant", "Jack L.", ""]]}, {"id": "1407.5242", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Philip H.S. Torr", "title": "Object Proposal Generation using Two-Stage Cascade SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposal algorithms have shown great promise as a first step for\nobject recognition and detection. Good object proposal generation algorithms\nrequire high object recall rate as well as low computational cost, because\ngenerating object proposals is usually utilized as a preprocessing step. The\nproblem of how to accelerate the object proposal generation and evaluation\nprocess without decreasing recall is thus of great interest. In this paper, we\npropose a new object proposal generation method using two-stage cascade SVMs,\nwhere in the first stage linear filters are learned for predefined quantized\nscales/aspect-ratios independently, and in the second stage a global linear\nclassifier is learned across all the quantized scales/aspect-ratios for\ncalibration, so that all the proposals can be compared properly. The proposals\nwith highest scores are our final output. Specifically, we explain our\nscale/aspect-ratio quantization scheme, and investigate the effects of\ncombinations of $\\ell_1$ and $\\ell_2$ regularizers in cascade SVMs with/without\nranking constraints in learning. Comprehensive experiments on VOC2007 dataset\nare conducted, and our results achieve the state-of-the-art performance with\nhigh object recall rate and high computational efficiency. Besides, our method\nhas been demonstrated to be suitable for not only class-specific but also\ngeneric object proposal generation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 03:53:21 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Zhang", "Ziming", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1407.5245", "submitter": "Liantao Wang", "authors": "Ji Zhao, Liantao Wang, Ricardo Cabral, Fernando De la Torre", "title": "Feature and Region Selection for Visual Learning", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2016, vol. 25, pp.\n  1084-1094", "doi": "10.1109/TIP.2016.2514503", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual learning problems such as object classification and action recognition\nare typically approached using extensions of the popular bag-of-words (BoW)\nmodel. Despite its great success, it is unclear what visual features the BoW\nmodel is learning: Which regions in the image or video are used to discriminate\namong classes? Which are the most discriminative visual words? Answering these\nquestions is fundamental for understanding existing BoW models and inspiring\nbetter models for visual recognition.\n  To answer these questions, this paper presents a method for feature selection\nand region selection in the visual BoW model. This allows for an intermediate\nvisualization of the features and regions that are important for visual\nlearning. The main idea is to assign latent weights to the features or regions,\nand jointly optimize these latent variables with the parameters of a classifier\n(e.g., support vector machine). There are four main benefits of our approach:\n(1) Our approach accommodates non-linear additive kernels such as the popular\n$\\chi^2$ and intersection kernel; (2) our approach is able to handle both\nregions in images and spatio-temporal regions in videos in a unified way; (3)\nthe feature selection problem is convex, and both problems can be solved using\na scalable reduced gradient method; (4) we point out strong connections with\nmultiple kernel learning and multiple instance learning approaches.\nExperimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTube\nillustrate the benefits of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 04:42:50 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 03:27:59 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Zhao", "Ji", ""], ["Wang", "Liantao", ""], ["Cabral", "Ricardo", ""], ["De la Torre", "Fernando", ""]]}, {"id": "1407.5324", "submitter": "Reza Azad", "authors": "Reza Azad, Babak Azad, Iman Tavakoli Kazerooni", "title": "Optimized Method for Iranian Road Signs Detection and recognition system", "comments": null, "journal-ref": "International Journal of Research in Computer Science, 4 (1): pp.\n  19-26, January 2014", "doi": "10.7815/ijorcs.41.2014.077", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road sign recognition is one of the core technologies in Intelligent\nTransport Systems. In the current study, a robust and real-time method is\npresented to identify and detect the roads speed signs in road image in\ndifferent situations. In our proposed method, first, the connected components\nare created in the main image using the edge detection and mathematical\nmorphology and the location of the road signs extracted by the geometric and\ncolor data; then the letters are segmented and recognized by Multiclass Support\nVector Machine (SVMs) classifiers. Regarding that the geometric and color\nfeatures ate properly used in detection the location of the road signs, so it\nis not sensitive to the distance and noise and has higher speed and efficiency.\nIn the result part, the proposed approach is applied on Iranian road speed sign\ndatabase and the detection and recognition accuracy rate achieved 98.66% and\n100% respectively.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 18:53:20 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Azad", "Reza", ""], ["Azad", "Babak", ""], ["Kazerooni", "Iman Tavakoli", ""]]}, {"id": "1407.5367", "submitter": "Sameer Agarwal", "authors": "Sameer Agarwal, Hon-leung Lee, Bernd Sturmfels and Rekha R. Thomas", "title": "Certifying the Existence of Epipolar Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of point correspondences in two images, the existence of a\nfundamental matrix is a necessary condition for the points to be the images of\na 3-dimensional scene imaged with two pinhole cameras. If the camera\ncalibration is known then one requires the existence of an essential matrix.\n  We present an efficient algorithm, using exact linear algebra, for testing\nthe existence of a fundamental matrix. The input is any number of point\ncorrespondences. For essential matrices, we characterize the solvability of the\nDemazure polynomials. In both scenarios, we determine which linear subspaces\nintersect a fixed set defined by non-linear polynomials. The conditions we\nderive are polynomials stated purely in terms of image coordinates. They\nrepresent a new class of two-view invariants, free of fundamental\n(resp.~essential)~matrices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 03:56:13 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Agarwal", "Sameer", ""], ["Lee", "Hon-leung", ""], ["Sturmfels", "Bernd", ""], ["Thomas", "Rekha R.", ""]]}, {"id": "1407.5536", "submitter": "Kamlesh Pawar", "authors": "Kamlesh Pawar, Gary F. Egan and Jingxin Zhang", "title": "Multichannel Compressive Sensing MRI Using Noiselet Encoding", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0126386", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incoherence between measurement and sparsifying transform matrices and\nthe restricted isometry property (RIP) of measurement matrix are two of the key\nfactors in determining the performance of compressive sensing (CS). In CS-MRI,\nthe randomly under-sampled Fourier matrix is used as the measurement matrix and\nthe wavelet transform is usually used as sparsifying transform matrix. However,\nthe incoherence between the randomly under-sampled Fourier matrix and the\nwavelet matrix is not optimal, which can deteriorate the performance of CS-MRI.\nUsing the mathematical result that noiselets are maximally incoherent with\nwavelets, this paper introduces the noiselet unitary bases as the measurement\nmatrix to improve the incoherence and RIP in CS-MRI, and presents a method to\ndesign the pulse sequence for the noiselet encoding. This novel encoding scheme\nis combined with the multichannel compressive sensing (MCS) framework to take\nthe advantage of multichannel data acquisition used in MRI scanners. An\nempirical RIP analysis is presented to compare the multichannel noiselet and\nmultichannel Fourier measurement matrices in MCS. Simulations are presented in\nthe MCS framework to compare the performance of noiselet encoding\nreconstructions and Fourier encoding reconstructions at different acceleration\nfactors. The comparisons indicate that multichannel noiselet measurement matrix\nhas better RIP than that of its Fourier counterpart, and that noiselet encoded\nMCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution and\ncan achieve higher acceleration factors. To demonstrate the feasibility of the\nproposed noiselet encoding scheme, two pulse sequences with tailored spatially\nselective RF excitation pulses was designed and implemented on a 3T scanner to\nacquire the data in the noiselet domain from a phantom and a human brain.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 17:14:18 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 02:56:38 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Pawar", "Kamlesh", ""], ["Egan", "Gary F.", ""], ["Zhang", "Jingxin", ""]]}, {"id": "1407.5736", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Ross Girshick, Pablo Arbel\\'aez, Jitendra Malik", "title": "Learning Rich Features from RGB-D Images for Object Detection and\n  Segmentation", "comments": "To appear in the European Conference on Computer Vision (ECCV), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of object detection for RGB-D images using\nsemantically rich image and depth features. We propose a new geocentric\nembedding for depth images that encodes height above ground and angle with\ngravity for each pixel in addition to the horizontal disparity. We demonstrate\nthat this geocentric embedding works better than using raw depth images for\nlearning feature representations with convolutional neural networks. Our final\nobject detection system achieves an average precision of 37.3%, which is a 56%\nrelative improvement over existing methods. We then focus on the task of\ninstance segmentation where we label pixels belonging to object instances found\nby our detector. For this task, we propose a decision forest approach that\nclassifies pixels in the detection window as foreground or background using a\nfamily of unary and binary tests that query shape and geocentric pose features.\nFinally, we use the output from our object detectors in an existing superpixel\nclassification framework for semantic scene segmentation and achieve a 24%\nrelative improvement over current state-of-the-art for the object categories\nthat we study. We believe advances such as those represented in this paper will\nfacilitate the use of perception in fields like robotics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 05:31:32 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Gupta", "Saurabh", ""], ["Girshick", "Ross", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Malik", "Jitendra", ""]]}, {"id": "1407.5754", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Tree-based iterated local search for Markov random fields with\n  applications in image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{maximum a posteriori} (MAP) assignment for general structure Markov\nrandom fields (MRFs) is computationally intractable. In this paper, we exploit\ntree-based methods to efficiently address this problem. Our novel method, named\nTree-based Iterated Local Search (T-ILS) takes advantage of the tractability of\ntree-structures embedded within MRFs to derive strong local search in an ILS\nframework. The method efficiently explores exponentially large neighborhood and\ndoes so with limited memory without any requirement on the cost functions. We\nevaluate the T-ILS in a simulation of Ising model and two real-world problems\nin computer vision: stereo matching, image denoising. Experimental results\ndemonstrate that our methods are competitive against state-of-the-art rivals\nwith a significant computational gain.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 06:43:41 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.5759", "submitter": "Denis Fortun", "authors": "Denis Fortun (INRIA), Patrick Bouthemy (INRIA), Charles Kervrann\n  (INRIA)", "title": "Aggregation of local parametric candidates with exemplar-based occlusion\n  handling for optical flow", "comments": "Submission,IEEE Transactions on Image Processing (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling all together large displacements, motion details and occlusions\nremains an open issue for reliable computation of optical flow in a video\nsequence. We propose a two-step aggregation paradigm to address this problem.\nThe idea is to supply local motion candidates at every pixel in a first step,\nand then to combine them to determine the global optical flow field in a second\nstep. We exploit local parametric estimations combined with patch\ncorrespondences and we experimentally demonstrate that they are sufficient to\nproduce highly accurate motion candidates. The aggregation step is designed as\nthe discrete optimization of a global regularized energy. The occlusion map is\nestimated jointly with the flow field throughout the two steps. We propose a\ngeneric exemplar-based approach for occlusion filling with motion vectors. We\nachieve state-of-the-art results in computer vision benchmarks, with\nparticularly significant improvements in the case of large displacements and\nocclusions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 06:50:40 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Fortun", "Denis", "", "INRIA"], ["Bouthemy", "Patrick", "", "INRIA"], ["Kervrann", "Charles", "", "INRIA"]]}, {"id": "1407.5976", "submitter": "Holger Roth", "authors": "Holger R. Roth and Jianhua Yao and Le Lu and James Stieger and Joseph\n  E. Burns and Ronald M. Summers", "title": "Detection of Sclerotic Spine Metastases via Random Aggregation of Deep\n  Convolutional Neural Network Classifications", "comments": "This paper will be presented at \"Computational Methods and Clinical\n  Applications for Spine Imaging\" workshop held in conjunction with MICCAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automated detection of sclerotic metastases (bone lesions) in Computed\nTomography (CT) images has potential to be an important tool in clinical\npractice and research. State-of-the-art methods show performance of 79%\nsensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume.\nWe design a two-tiered coarse-to-fine cascade framework to first operate a\nhighly sensitive candidate generation system at a maximum sensitivity of ~92%\nbut with high FP level (~50 per patient). Regions of interest (ROI) for lesion\ncandidates are generated in this step and function as input for the second\ntier. In the second tier we generate N 2D views, via scale, random\ntranslations, and rotations with respect to each ROI centroid coordinates.\nThese random views are used to train a deep Convolutional Neural Network (CNN)\nclassifier. In testing, the CNN is employed to assign individual probabilities\nfor a new set of N random views that are averaged at each ROI to compute a\nfinal per-candidate classification probability. This second tier behaves as a\nhighly selective process to reject difficult false positives while preserving\nhigh sensitivities. We validate the approach on CT images of 59 patients (49\nwith sclerotic metastases and 10 normal controls). The proposed method reduces\nthe number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing a\nsensitivity rates of 60%, 70%, and 80% respectively in testing. The\nArea-Under-the-Curve (AUC) is 0.834. The results show marked improvement upon\nprevious work.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 19:06:50 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Roth", "Holger R.", ""], ["Yao", "Jianhua", ""], ["Lu", "Le", ""], ["Stieger", "James", ""], ["Burns", "Joseph E.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1407.6067", "submitter": "Marcelo S. Reis", "authors": "Marcelo S. Reis, Carlos E. Ferreira, and Junior Barrera", "title": "The U-curve optimization problem: improvements on the original algorithm\n  and time complexity analysis", "comments": "Original results from the Ph.D. thesis of Marcelo S. Reis. This\n  thesis can be accessed through the following link:\n  http://www.teses.usp.br/teses/disponiveis/45/45134/tde-05022013-123757/en.php", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The U-curve optimization problem is characterized by a decomposable in\nU-shaped curves cost function over the chains of a Boolean lattice. This\nproblem can be applied to model the classical feature selection problem in\nMachine Learning. Recently, the U-Curve algorithm was proposed to give optimal\nsolutions to the U-curve problem. In this article, we point out that the\nU-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS)\nalgorithm, which is actually optimal. We also present the results of optimal\nand suboptimal experiments, in which UCS is compared with the UBB optimal\nbranch-and-bound algorithm and the SFFS heuristic, respectively. We show that,\nin both experiments, $\\proc{UCS}$ had a better performance than its competitor.\nFinally, we analyze the obtained results and point out improvements on UCS that\nmight enhance the performance of this algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 23:18:08 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Reis", "Marcelo S.", ""], ["Ferreira", "Carlos E.", ""], ["Barrera", "Junior", ""]]}, {"id": "1407.6082", "submitter": "Igor Milevskiy", "authors": "Igor Milevskiy, Yuri Boykov", "title": "Joint Energy-based Detection and Classificationon of Multilingual Text\n  Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new hierarchical MDL-based model for a joint detection\nand classification of multilingual text lines in im- ages taken by hand-held\ncameras. The majority of related text detec- tion methods assume alphabet-based\nwriting in a single language, e.g. in Latin. They use simple clustering\nheuristics specific to such texts: prox- imity between letters within one line,\nlarger distance between separate lines, etc. We are interested in a\nsignificantly more ambiguous problem where images combine alphabet and\nlogographic characters from multiple languages and typographic rules vary a lot\n(e.g. English, Korean, and Chinese). Complexity of detecting and classifying\ntext lines in multiple languages calls for a more principled approach based on\ninformation- theoretic principles. Our new MDL model includes data costs\ncombining geometric errors with classification likelihoods and a hierarchical\nsparsity term based on label costs. This energy model can be efficiently\nminimized by fusion moves. We demonstrate robustness of the proposed algorithm\non a large new database of multilingual text images collected in the pub- lic\ntransit system of Seoul.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 01:14:01 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Milevskiy", "Igor", ""], ["Boykov", "Yuri", ""]]}, {"id": "1407.6174", "submitter": "Fatih Cakir", "authors": "Fatih Cakir and Stan Sclaroff", "title": "Visual Word Selection without Re-Coding and Re-Pooling", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bag-of-Words (BoW) representation is widely used in computer vision. The\nsize of the codebook impacts the time and space complexity of the applications\nthat use BoW. Thus, given a training set for a particular computer vision task,\na key problem is pruning a large codebook to select only a subset of visual\nwords. Evaluating possible selections of words to be included in the pruned\ncodebook can be computationally prohibitive; in a brute-force scheme,\nevaluating each pruned codebook requires re-coding of all features extracted\nfrom training images to words in the candidate codebook and then re-pooling the\nwords to obtain a representation of each image, e.g., histogram of visual word\nfrequencies. In this paper, a method is proposed that selects and evaluates a\nsubset of words from an initially large codebook, without the need for\nre-coding or re-pooling. Formulations are proposed for two commonly-used\nschemes: hard and soft (kernel) coding of visual words with average-pooling.\nThe effectiveness of these formulations is evaluated on the 15 Scenes and\nCaltech 10 benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 11:10:39 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Cakir", "Fatih", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1407.6245", "submitter": "Fran\\c{c}ois Boulogne", "authors": "Stefan van der Walt, Johannes L. Sch\\\"onberger, Juan Nunez-Iglesias,\n  Fran\\c{c}ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart,\n  Tony Yu, the scikit-image contributors", "title": "scikit-image: Image processing in Python", "comments": "Distributed under Creative Commons CC-BY 4.0. Published in PeerJ", "journal-ref": null, "doi": "10.7717/peerj.453", "report-no": null, "categories": "cs.MS cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  scikit-image is an image processing library that implements algorithms and\nutilities for use in research, education and industry applications. It is\nreleased under the liberal \"Modified BSD\" open source license, provides a\nwell-documented API in the Python programming language, and is developed by an\nactive, international team of collaborators. In this paper we highlight the\nadvantages of open source to achieve the goals of the scikit-image library, and\nwe showcase several real-world image processing applications that use\nscikit-image.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 14:55:21 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["van der Walt", "Stefan", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Nunez-Iglesias", "Juan", ""], ["Boulogne", "Fran\u00e7ois", ""], ["Warner", "Joshua D.", ""], ["Yager", "Neil", ""], ["Gouillart", "Emmanuelle", ""], ["Yu", "Tony", ""], ["contributors", "the scikit-image", ""]]}, {"id": "1407.6251", "submitter": "Philip Lenz", "authors": "Philip Lenz, Andreas Geiger, Raquel Urtasun", "title": "FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory\n  and Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most popular approaches to multi-target tracking is\ntracking-by-detection. Current min-cost flow algorithms which solve the data\nassociation problem optimally have three main drawbacks: they are\ncomputationally expensive, they assume that the whole video is given as a\nbatch, and they scale badly in memory and computation with the length of the\nvideo sequence. In this paper, we address each of these issues, resulting in a\ncomputationally and memory-bounded solution. First, we introduce a dynamic\nversion of the successive shortest-path algorithm which solves the data\nassociation problem optimally while reusing computation, resulting in\nsignificantly faster inference than standard solvers. Second, we address the\noptimal solution to the data association problem when dealing with an incoming\nstream of data (i.e., online setting). Finally, we present our main\ncontribution which is an approximate online solution with bounded memory and\ncomputation which is capable of handling videos of arbitrarily length while\nperforming tracking in real time. We demonstrate the effectiveness of our\nalgorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art\nperformance, while being significantly faster than existing solvers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 15:07:12 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 14:15:01 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Lenz", "Philip", ""], ["Geiger", "Andreas", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1407.6318", "submitter": "Reza Azad", "authors": "Reza Azad, Fatemeh Davami", "title": "A robust and adaptable method for face detection based on Color\n  Probabilistic Estimation Technique", "comments": null, "journal-ref": "International Journal of Research in Computer Science A Unit of\n  White Globe Publications, Volume 3, Issue 6, 2013", "doi": "10.7815/ijorcs.36.2013.072", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face perception is currently an active research area in the computer\nvision community. Skin detection is one of the most important and primary\nstages for this purpose. So far, many approaches are proposed to done this\ncase. Near all of these methods have tried to find best match intensity\ndistribution with skin pixels based on popular color spaces such as RGB, HSI or\nYCBCR. Results show that these methods cannot provide an accurate approach for\nevery kind of skin. In this paper, an approach is proposed to solve this\nproblem using a color probabilistic estimation technique. This approach is\nincluding two stages. In the first one, the skin intensity distribution is\nestimated using some train photos of pure skin, and at the second stage, the\nskin pixels are detected using Gaussian model and optimal threshold tuning.\nThen from the skin region facial features have been extracted to get the face\nfrom the skin region. In the results section, the proposed approach is applied\non FEI database and the accuracy rate reached 99.25%. The proposed approach can\nbe used for all kinds of skin using train stage which is the main advantage\namong the other advantages, such as Low noise sensitivity and low computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 18:12:08 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Azad", "Reza", ""], ["Davami", "Fatemeh", ""]]}, {"id": "1407.6321", "submitter": "Reza Azad", "authors": "Reza Azad, Majid Nazari", "title": "Novel and Automatic Parking Inventory System Based on Pattern\n  Recognition and Directional Chain Code", "comments": null, "journal-ref": "2013 First International Conference on computer, Information\n  Technology and Digital Media", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to design an efficient vehicle license plate\nrecognition System and to implement it for automatic parking inventory system.\nThe system detects the vehicle first and then captures the image of the front\nview of the vehicle. Vehicle license plate is localized and characters are\nsegmented. For finding the place of plate, a novel and real time method is\nexpressed. A new and robust technique based on directional chain code is used\nfor character recognition. The resulting vehicle number is then compared with\nthe available database of all the vehicles so as to come up with information\nabout the vehicle type and to charge entrance cost accordingly. The system is\nthen allowed to open parking barrier for the vehicle and generate entrance cost\nreceipt. The vehicle information (such as entrance time, date, and cost amount)\nis also stored in the database to maintain the record. The hardware and\nsoftware integrated system is implemented and a working prototype model is\ndeveloped. Under the available database, the average accuracy of locating\nvehicle license plate obtained 100%. Using 70% samples of character for\ntraining, we tested our scheme on whole samples and obtained 100% correct\nrecognition rate. Further we tested our character recognition stage on Persian\nvehicle data set and we achieved 99% correct recognition.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 18:20:56 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Azad", "Reza", ""], ["Nazari", "Majid", ""]]}, {"id": "1407.6423", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Longyu Jiang, Xu Han, Lotfi Senhadji, Huazhong Shu", "title": "Performance evaluation of wavelet scattering network in image texture\n  classification in various color spaces", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Texture plays an important role in many image analysis applications. In this\npaper, we give a performance evaluation of color texture classification by\nperforming wavelet scattering network in various color spaces. Experimental\nresults on the KTH_TIPS_COL database show that opponent RGB based wavelet\nscattering network outperforms other color spaces. Therefore, when dealing with\nthe problem of color texture classification, opponent RGB based wavelet\nscattering network is recommended.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 01:39:33 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Wu", "Jiasong", ""], ["Jiang", "Longyu", ""], ["Han", "Xu", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1407.6432", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning Structured Outputs from Partial Labels using Forest Ensemble", "comments": "Conference version appeared in Truyen et al, AdaBoost.MRF: Boosted\n  Markov random forests and application to multilevel activity recognition.\n  CVPR'06", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structured outputs with general structures is computationally\nchallenging, except for tree-structured models. Thus we propose an efficient\nboosting-based algorithm AdaBoost.MRF for this task. The idea is based on the\nrealization that a graph is a superimposition of trees. Different from most\nexisting work, our algorithm can handle partial labelling, and thus is\nparticularly attractive in practice where reliable labels are often sparsely\nobserved. In addition, our method works exclusively on trees and thus is\nguaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor video\nsurveillance scenario, where activities are modelled at multiple levels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 02:53:52 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6492", "submitter": "Reza Azad", "authors": "Reza Azad, Fatemeh Davami, Hamid Reza Shayegh", "title": "Recognition of Handwritten Persian/Arabic Numerals Based on Robust\n  Feature Set and K-NN Classifier", "comments": "This paper has been withdrawn by the main author due to the Table 1\n  and equation 2 errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author due to a crucial sign error in\nequation 2 and some mistake in Table 1 information. please let me for changing\nthis information and updating this paper.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 08:53:00 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 11:28:48 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Azad", "Reza", ""], ["Davami", "Fatemeh", ""], ["Shayegh", "Hamid Reza", ""]]}, {"id": "1407.6496", "submitter": "Reza A", "authors": "Reza Azad, Mohammad Baghdadi", "title": "Novel and Fast Algorithm for Extracting License Plate Location Based on\n  Edge Analysis", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 1, and some mistake in Table 1 information. please remove\n  this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays in developing or developed countries, the Intelligent Transportation\nSystem (ITS) technology has attracted so much attention to itself. License\nPlate Recognition (LPR) systems have many applications in ITSs, such as the\npayment of parking fee, controlling the traffic volume, traffic data\ncollection, etc. This paper presents a new and fast method for license plate\nextraction based on edge analysis. our proposed method consist of four stage,\nwhich are edge detection, non-useable edge and noise removing, edge analysis\nand morphology-based license plate extraction. In the result part, the proposed\nalgorithm is applied on vehicle database and the accuracy rate reached 98%.\nFrom the experimental results it is shown that the proposed method gives fairly\nacceptable level of accuracy for practical license plate recognition system.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 09:01:36 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 14:45:53 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Azad", "Reza", ""], ["Baghdadi", "Mohammad", ""]]}, {"id": "1407.6498", "submitter": "Reza Azad", "authors": "Reza Azad, Babak Azad, Hamid Reza Shayegh", "title": "Real-Time and Efficient Method for Accuracy Enhancement of Edge Based\n  License Plate Recognition System", "comments": "2013 First International Conference on computer, Information\n  Technology and Digital Media. arXiv admin note: substantial text overlap with\n  arXiv:1407.6321", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License Plate Recognition plays an important role on the traffic monitoring\nand parking management. Administration and restriction of those transportation\ntools for their better service becomes very essential. In this paper, a fast\nand real time method has an appropriate application to find plates that the\nplat has tilt and the picture quality is poor. In the proposed method, at the\nbeginning, the image is converted into binary mode with use of adaptive\nthreshold. And with use of edge detection and morphology operation, plate\nnumber location has been specified and if the plat has tilt; its tilt is\nremoved away. Then its characters are distinguished using image processing\ntechniques. Finally, K Nearest Neighbour (KNN) classifier was used for\ncharacter recognition. This method has been tested on available data set that\nhas different images of the background, considering distance, and angel of view\nso that the correct extraction rate of plate reached at 98% and character\nrecognition rate achieved at 99.12%. Further we tested our character\nrecognition stage on Persian vehicle data set and we achieved 99% correct\nrecognition rate.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 09:02:16 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Azad", "Reza", ""], ["Azad", "Babak", ""], ["Shayegh", "Hamid Reza", ""]]}, {"id": "1407.6506", "submitter": "Reza Azad", "authors": "Reza Azad, Hamid Reza Shayegh", "title": "Novel and Tuneable Method for Skin Detection Based on Hybrid Color Space\n  and Color Statistical Features", "comments": "2013 First International Conference on computer, Information\n  Technology and Digital Media. arXiv admin note: substantial text overlap with\n  arXiv:1407.6318", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin detection is one of the most important and primary stages in some of\nimage processing applications such as face detection and human tracking. So\nfar, many approaches are proposed to done this case. Near all of these methods\nhave tried to find best match intensity distribution with skin pixels based on\npopular color spaces such as RGB, CMYK or YCbCr. Results show these methods\ncannot provide an accurate approach for every kinds of skin. In this paper, an\napproach is proposed to solve this problem using statistical features\ntechnique. This approach is including two stages. In the first one, from pure\nskin statistical features were extracted and at the second stage, the skin\npixels are detected using HSV and YCbCr color spaces. In the result part, the\nproposed approach is applied on FEI database and the accuracy rate reached\n99.25 + 0.2. Further proposed method is applied on complex background database\nand accuracy rate obtained 95.40+0.31%. The proposed approach can be used for\nall kinds of skin using train stage which is the main advantages of it. Low\nnoise sensitivity and low computational complexity are some of other\nadvantages.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 09:31:13 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Azad", "Reza", ""], ["Shayegh", "Hamid Reza", ""]]}, {"id": "1407.6510", "submitter": "Reza Azad", "authors": "Reza Azad, Hamid Reza Shayegh", "title": "New Method for Optimization of License Plate Recognition system with Use\n  of Edge Detection and Connected Component", "comments": "3rd IEEE International Conference on Computer and Knowledge\n  Engineering (ICCKE 2013), October 31 & November 1, 2013, Ferdowsi Universit\n  Mashhad", "journal-ref": null, "doi": "10.1109/ICCKE.2013.6682800", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License Plate recognition plays an important role on the traffic monitoring\nand parking management systems. In this paper, a fast and real time method has\nbeen proposed which has an appropriate application to find tilt and poor\nquality plates. In the proposed method, at the beginning, the image is\nconverted into binary mode using adaptive threshold. Then, by using some edge\ndetection and morphology operations, plate number location has been specified.\nFinally, if the plat has tilt, its tilt is removed away. This method has been\ntested on another paper data set that has different images of the background,\nconsidering distance, and angel of view so that the correct extraction rate of\nplate reached at 98.66%.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 09:52:47 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Azad", "Reza", ""], ["Shayegh", "Hamid Reza", ""]]}, {"id": "1407.6705", "submitter": "Reza Azad", "authors": "Reza Azad, Hamid Reza Shayegh, Hamed Amiri", "title": "A Robust and Efficient Method for Improving Accuracy of License Plate\n  Characters Recognition", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 1 and some mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License Plate Recognition (LPR) plays an important role on the traffic\nmonitoring and parking management. A robust and efficient method for enhancing\naccuracy of license plate characters recognition based on K Nearest Neighbours\n(K-NN) classifier is presented in this paper. The system first prepares a\ncontour form of the extracted character, then the angle and distance feature\ninformation about the character is extracted and finally K-NN classifier is\nused to character recognition. Angle and distance features of a character have\nbeen computed based on distribution of points on the bitmap image of character.\nIn K-NN method, the Euclidean distance between testing point and reference\npoints is calculated in order to find the k-nearest neighbours. We evaluated\nour method on the available dataset that contain 1200 sample. Using 70% samples\nfor training, we tested our method on whole samples and obtained 99% correct\nrecognition rate.Further, we achieved average 99.41% accuracy using\nthree/strategy validation technique on 1200 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 09:26:01 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 07:28:21 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Azad", "Reza", ""], ["Shayegh", "Hamid Reza", ""], ["Amiri", "Hamed", ""]]}, {"id": "1407.6748", "submitter": "Ayodeji Makinde", "authors": "Ayodeji S. Makinde, Yaw Nkansah-Gyekye, Loserian S. Laizer", "title": "Enhancing the Accuracy of Biometric Feature Extraction Fusion Using\n  Gabor Filter and Mahalanobis Distance Algorithm", "comments": "Focused on extraction of feature from two different modalities (face\n  and fingerprint) using Gabor filter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Biometric recognition systems have advanced significantly in the last decade\nand their use in specific applications will increase in the near future. The\nability to conduct meaningful comparisons and assessments will be crucial to\nsuccessful deployment and increasing biometric adoption. The best modality used\nas unimodal biometric systems are unable to fully address the problem of higher\nrecognition rate. Multimodal biometric systems are able to mitigate some of the\nlimitations encountered in unimodal biometric systems, such as\nnon-universality, distinctiveness, non-acceptability, noisy sensor data, spoof\nattacks, and performance. More reliable recognition accuracy and performance\nare achievable as different modalities were being combined together and\ndifferent algorithms or techniques were being used. The work presented in this\npaper focuses on a bimodal biometric system using face and fingerprint. An\nimage enhancement technique (histogram equalization) is used to enhance the\nface and fingerprint images. Salient features of the face and fingerprint were\nextracted using the Gabor filter technique. A dimensionality reduction\ntechnique was carried out on both images extracted features using a principal\ncomponent analysis technique. A feature level fusion algorithm (Mahalanobis\ndistance technique) is used to combine each unimodal feature together. The\nperformance of the proposed approach is validated and is effective.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 22:24:01 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Makinde", "Ayodeji S.", ""], ["Nkansah-Gyekye", "Yaw", ""], ["Laizer", "Loserian S.", ""]]}, {"id": "1407.7091", "submitter": "Andrew Barry", "authors": "Andrew J. Barry and Russ Tedrake", "title": "Pushbroom Stereo for High-Speed Navigation in Cluttered Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel stereo vision algorithm that is capable of obstacle\ndetection on a mobile-CPU processor at 120 frames per second. Our system\nperforms a subset of standard block-matching stereo processing, searching only\nfor obstacles at a single depth. By using an onboard IMU and state-estimator,\nwe can recover the position of obstacles at all other depths, building and\nupdating a full depth-map at framerate.\n  Here, we describe both the algorithm and our implementation on a high-speed,\nsmall UAV, flying at over 20 MPH (9 m/s) close to obstacles. The system\nrequires no external sensing or computation and is, to the best of our\nknowledge, the first high-framerate stereo detection system running onboard a\nsmall UAV.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 03:49:22 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Barry", "Andrew J.", ""], ["Tedrake", "Russ", ""]]}, {"id": "1407.7317", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague", "title": "A unified framework for thermal face recognition", "comments": "International Conference on Neural Information Processing, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reduction of the cost of infrared (IR) cameras in recent years has made\nIR imaging a highly viable modality for face recognition in practice. A\nparticularly attractive advantage of IR-based over conventional, visible\nspectrum-based face recognition stems from its invariance to visible\nillumination. In this paper we argue that the main limitation of previous work\non face recognition using IR lies in its ad hoc approach to treating different\nnuisance factors which affect appearance, prohibiting a unified approach that\nis capable of handling concurrent changes in multiple (or indeed all) major\nextrinsic sources of variability, which is needed in practice. We describe the\nfirst approach that attempts to achieve this - the framework we propose\nachieves outstanding recognition performance in the presence of variable (i)\npose, (ii) facial expression, (iii) physiological state, (iv) partial occlusion\ndue to eye-wear, and (v) quasi-occlusion due to facial hair growth.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 04:20:24 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Ghiass", "Reza Shoja", ""], ["Arandjelovic", "Ognjen", ""], ["Bendada", "Hakim", ""], ["Maldague", "Xavier", ""]]}, {"id": "1407.7330", "submitter": "Arnold Wiliem", "authors": "Arnold Wiliem, Peter Hobson, Brian C. Lovell", "title": "Discovering Discriminative Cell Attributes for HEp-2 Specimen Image\n  Classification", "comments": "WACV 2014: IEEE Winter Conference on Applications of Computer Vision", "journal-ref": null, "doi": "10.1109/WACV.2014.6836071", "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in developing Computer Aided\nDiagnostic (CAD) systems for improving the reliability and consistency of\npathology test results. This paper describes a novel CAD system for the\nAnti-Nuclear Antibody (ANA) test via Indirect Immunofluorescence protocol on\nHuman Epithelial Type 2 (HEp-2) cells. While prior works have primarily focused\non classifying cell images extracted from ANA specimen images, this work takes\na further step by focussing on the specimen image classification problem\nitself. Our system is able to efficiently classify specimen images as well as\nproducing meaningful descriptions of ANA pattern class which helps physicians\nto understand the differences between various ANA patterns. We achieve this\ngoal by designing a specimen-level image descriptor that: (1) is highly\ndiscriminative; (2) has small descriptor length and (3) is semantically\nmeaningful at the cell level. In our work, a specimen image descriptor is\nrepresented by its overall cell attribute descriptors. As such, we propose two\nmax-margin based learning schemes to discover cell attributes whilst still\nmaintaining the discrimination of the specimen image descriptor. Our learning\nschemes differ from the existing discriminative attribute learning approaches\nas they primarily focus on discovering image-level attributes. Comparative\nevaluations were undertaken to contrast the proposed approach to various\nstate-of-the-art approaches on a novel HEp-2 cell dataset which was\nspecifically proposed for the specimen-level classification. Finally, we\nshowcase the ability of the proposed approach to provide textual descriptions\nto explain ANA patterns.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 06:03:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wiliem", "Arnold", ""], ["Hobson", "Peter", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1407.7390", "submitter": "Jos\\'e Ram\\'on Padilla-L\\'opez", "authors": "Jos\\'e Ram\\'on Padilla-L\\'opez and Alexandros Andr\\'e Chaaraoui and\n  Francisco Fl\\'orez-Revuelta", "title": "A discussion on the validation tests employed to compare human action\n  recognition methods using the MSR Action3D dataset", "comments": "16 pages and 7 tables", "journal-ref": null, "doi": null, "report-no": "hdl:10045/39889", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to determine which is the best human action recognition\nmethod based on features extracted from RGB-D devices, such as the Microsoft\nKinect. A review of all the papers that make reference to MSR Action3D, the\nmost used dataset that includes depth information acquired from a RGB-D device,\nhas been performed. We found that the validation method used by each work\ndiffers from the others. So, a direct comparison among works cannot be made.\nHowever, almost all the works present their results comparing them without\ntaking into account this issue. Therefore, we present different rankings\naccording to the methodology used for the validation in orden to clarify the\nexisting confusion.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 11:59:30 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 11:30:40 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2015 19:57:45 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Padilla-L\u00f3pez", "Jos\u00e9 Ram\u00f3n", ""], ["Chaaraoui", "Alexandros Andr\u00e9", ""], ["Fl\u00f3rez-Revuelta", "Francisco", ""]]}, {"id": "1407.7504", "submitter": "Lluis Gomez", "authors": "Lluis Gomez and Dimosthenis Karatzas", "title": "A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene\n  Text Extraction", "comments": "Manuscript Preprint. 11 pages. This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typography and layout lead to the hierarchical organisation of text in words,\ntext lines, paragraphs. This inherent structure is a key property of text in\nany script and language, which has nonetheless been minimally leveraged by\nexisting text detection methods. This paper addresses the problem of text\nsegmentation in natural scenes from a hierarchical perspective. Contrary to\nexisting methods, we make explicit use of text structure, aiming directly to\nthe detection of region groupings corresponding to text within a hierarchy\nproduced by an agglomerative similarity clustering process over individual\nregions. We propose an optimal way to construct such an hierarchy introducing a\nfeature space designed to produce text group hypotheses with high recall and a\nnovel stopping rule combining a discriminative classifier and a probabilistic\nmeasure of group meaningfulness based in perceptual organization. Results\nobtained over four standard datasets, covering text in variable orientations\nand different languages, demonstrate that our algorithm, while being trained in\na single mixed dataset, outperforms state of the art methods in unconstrained\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 19:21:53 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1407.7556", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Alireza Sadeghian, Witold Pedrycz", "title": "Entropic one-class classifiers", "comments": "To appear in IEEE-TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2418332", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-class classification problem is a well-known research endeavor in\npattern recognition. The problem is also known under different names, such as\noutlier and novelty/anomaly detection. The core of the problem consists in\nmodeling and recognizing patterns belonging only to a so-called target class.\nAll other patterns are termed non-target, and therefore they should be\nrecognized as such. In this paper, we propose a novel one-class classification\nsystem that is based on an interplay of different techniques. Primarily, we\nfollow a dissimilarity representation based approach; we embed the input data\ninto the dissimilarity space by means of an appropriate parametric\ndissimilarity measure. This step allows us to process virtually any type of\ndata. The dissimilarity vectors are then represented through a weighted\nEuclidean graphs, which we use to (i) determine the entropy of the data\ndistribution in the dissimilarity space, and at the same time (ii) derive\neffective decision regions that are modeled as clusters of vertices. Since the\ndissimilarity measure for the input data is parametric, we optimize its\nparameters by means of a global optimization scheme, which considers both\nmesoscopic and structural characteristics of the data represented through the\ngraphs. The proposed one-class classifier is designed to provide both hard\n(Boolean) and soft decisions about the recognition of test patterns, allowing\nan accurate description of the classification process. We evaluate the\nperformance of the system on different benchmarking datasets, containing either\nfeature-based or structured patterns. Experimental results demonstrate the\neffectiveness of the proposed technique.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 20:26:24 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 20:46:21 GMT"}, {"version": "v3", "created": "Sun, 11 Jan 2015 16:27:23 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Livi", "Lorenzo", ""], ["Sadeghian", "Alireza", ""], ["Pedrycz", "Witold", ""]]}, {"id": "1407.7626", "submitter": "Deepak Nayak Ranjan", "authors": "Deepak Ranjan Nayak, Prashanta Kumar Patra, Amitav Mahapatra", "title": "A Survey on Two Dimensional Cellular Automata and Its Application in\n  Image Processing", "comments": "10 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel algorithms for solving any image processing task is a highly\ndemanded approach in the modern world. Cellular Automata (CA) are the most\ncommon and simple models of parallel computation. So, CA has been successfully\nused in the domain of image processing for the last couple of years. This paper\nprovides a survey of available literatures of some methodologies employed by\ndifferent researchers to utilize the cellular automata for solving some\nimportant problems of image processing. The survey includes some important\nimage processing tasks such as rotation, zooming, translation, segmentation,\nedge detection, compression and noise reduction of images. Finally, the\nexperimental results of some methodologies are presented.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 04:09:09 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Nayak", "Deepak Ranjan", ""], ["Patra", "Prashanta Kumar", ""], ["Mahapatra", "Amitav", ""]]}, {"id": "1407.7686", "submitter": "Zohaib Khan", "authors": "Zohaib Khan", "title": "Hyperspectral Imaging and Analysis for Sparse Reconstruction and\n  Recognition", "comments": "PhD Thesis, School of Computer Science and Software Engineering, The\n  University of Western Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis proposes spatio-spectral techniques for hyperspectral image\nanalysis. Adaptive spatio-spectral support and variable exposure hyperspectral\nimaging is demonstrated to improve spectral reflectance recovery from\nhyperspectral images. Novel spectral dimensionality reduction techniques have\nbeen proposed from the perspective of spectral only and spatio-spectral\ninformation preservation. It was found that the joint sparse and joint group\nsparse hyperspectral image models achieve lower reconstruction error and higher\nrecognition accuracy using only a small subset of bands. Hyperspectral image\ndatabases have been developed and made publicly available for further research\nin compressed hyperspectral imaging, forensic document analysis and spectral\nreflectance recovery.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 10:29:28 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Khan", "Zohaib", ""]]}, {"id": "1407.8121", "submitter": "Dibya Jyoti Bora", "authors": "Dibya Jyoti Bora, Anil Kumar Gupta", "title": "Clustering Approach Towards Image Segmentation: An Analytical Study", "comments": "10 pages, 3 figures", "journal-ref": "International Journal of Research in Computer Applications and\n  Robotics, ISSN 2320-7345, Vol.2, Issue.7, Pg.: 115-124 July 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image processing is an important research area in computer vision. Image\nsegmentation plays the vital rule in image processing research. There exist so\nmany methods for image segmentation. Clustering is an unsupervised study.\nClustering can also be used for image segmentation. In this paper, an in-depth\nstudy is done on different clustering techniques that can be used for image\nsegmentation with their pros and cons. An experiment for color image\nsegmentation based on clustering with K-Means algorithm is performed to observe\nthe accuracy of clustering technique for the segmentation purpose.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 16:45:00 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Bora", "Dibya Jyoti", ""], ["Gupta", "Anil Kumar", ""]]}, {"id": "1407.8123", "submitter": "T.R. Gopalakrishnan Nair", "authors": "T.R. Gopalakrishnan Nair, Richa Sharma", "title": "Merging and Shifting of Images with Prominence Coefficient for\n  Predictive Analysis using Combined Image", "comments": "7 pages,4 figures,Emerging Research in Computing, Information,\n  Communication and Application (ERCICA13), International Conference on, NMIT,\n  Bangalore, India, pp. 205,211, 2-3 Aug.2013 ISBN: 9789351071020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shifting of objects in an image and merging many images after appropriate\nshifting is being used in several engineering and scientific applications which\nrequire complex perception development. A method has been presented here which\ncould be used in precision engineering and biological applications where more\nprecise prediction is required of a combined phenomenon with varying prominence\nof each phenomenon. Accurate merging of intended pixels can be achieved in high\nquality using frequency domain techniques even though initial properties of the\noriginal pixels are lost in this process. This paper introduces a technique to\nshift and merge various images with varying prominence of each image. A\ncoefficient named prominence coefficient has been introduced which is capable\nof making some of the images transparent and highlighting the rest as per\nrequirement of merging process which can be used as a simple but effective\ntechnique for overlapped view of a set of images.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 16:54:54 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Nair", "T. R. Gopalakrishnan", ""], ["Sharma", "Richa", ""]]}, {"id": "1407.8176", "submitter": "T.R. Gopalakrishnan Nair", "authors": "T.R. Gopalakrishnan Nair, Richa Sharma", "title": "Accurate merging of images for predictive analysis using combined image", "comments": "5 pages, 4 figures,Signal Processing Image Processing & Pattern\n  Recognition (ICSIPR), 2013 International Conference on, Karunya University,\n  Coimbatore, India, pp.169,173, 7-8 Feb. 2013. arXiv admin note: substantial\n  text overlap with arXiv:1407.8123", "journal-ref": null, "doi": "10.1109/ICSIPR.2013.6497980", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several Scientific and engineering applications require merging of sampled\nimages for complex perception development. In most cases, for such\nrequirements, images are merged at intensity level. Even though it gives fairly\ngood perception of combined scenario of objects and scenes, it is found that\nthey are not sufficient enough to analyze certain engineering cases. The main\nproblem is incoherent modulation of intensity arising out of phase properties\nbeing lost. In order to compensate these losses, combined phase and amplitude\nmerge is demanded. We present here a method which could be used in precision\nengineering and biological applications where more precise prediction is\nrequired of a combined phenomenon. When pixels are added, its original property\nis lost but accurate merging of intended pixels can be achieved in high quality\nusing frequency domain properties of an image. This paper introduces a\ntechnique to merge various images which can be used as a simple but effective\ntechnique for overlapped view of a set of images and producing reduced dataset\nfor review purposes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 07:08:31 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Nair", "T. R. Gopalakrishnan", ""], ["Sharma", "Richa", ""]]}, {"id": "1407.8337", "submitter": "Pavan Kumar C", "authors": "G. Vishnu Murthy, Pavan Kumar C., Vakulabharanam Vijaya Kumar", "title": "A New Model of Array Grammar for generating Connected Patterns on an\n  Image Neighborhood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of patterns on images is recognized as an important step in\ncharacterization and classification of image. The ability to efficiently\nanalyze and describe image patterns is thus of fundamental importance. The\nstudy of syntactic methods of describing pictures has been of interest for\nresearchers. Array Grammars can be used to represent and recognize connected\npatterns. In any image the patterns are recognized using connected patterns. It\nis very difficult to represent all connected patterns (CP) even on a small 3 x\n3 neighborhood in a pictorial way. The present paper proposes the model of\narray grammar capable of generating any kind of simple or complex pattern and\nderivation of connected pattern in an image neighborhood using the proposed\ngrammar is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 10:04:19 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Murthy", "G. Vishnu", ""], ["C.", "Pavan Kumar", ""], ["Kumar", "Vakulabharanam Vijaya", ""]]}, {"id": "1407.8497", "submitter": "Amal Farag", "authors": "Amal Farag, Le Lu, Evrim Turkbey, Jiamin Liu and Ronald M. Summers", "title": "A Bottom-Up Approach for Automatic Pancreas Segmentation in Abdominal CT\n  Scans", "comments": "Abdominal Workshop in Conjunction with MICCAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Organ segmentation is a prerequisite for a computer-aided diagnosis (CAD)\nsystem to detect pathologies and perform quantitative analysis. For\nanatomically high-variability abdominal organs such as the pancreas, previous\nsegmentation works report low accuracies when comparing to organs like the\nheart or liver. In this paper, a fully-automated bottom-up method is presented\nfor pancreas segmentation, using abdominal computed tomography (CT) scans. The\nmethod is based on a hierarchical two-tiered information propagation by\nclassifying image patches. It labels superpixels as pancreas or not via pooling\npatch-level confidences on 2D CT slices over-segmented by the Simple Linear\nIterative Clustering approach. A supervised random forest (RF) classifier is\ntrained on the patch level and a two-level cascade of RFs is applied at the\nsuperpixel level, coupled with multi-channel feature extraction, respectively.\nOn six-fold cross-validation using 80 patient CT volumes, we achieved 68.8%\nDice coefficient and 57.2% Jaccard Index, comparable to or slightly better than\npublished state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 17:51:03 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Farag", "Amal", ""], ["Lu", "Le", ""], ["Turkbey", "Evrim", ""], ["Liu", "Jiamin", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1407.8518", "submitter": "Roberto Rigamonti", "authors": "Roberto Rigamonti, Vincent Lepetit, Pascal Fua", "title": "Beyond KernelBoost", "comments": null, "journal-ref": null, "doi": null, "report-no": "EPFL-REPORT-200378", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Technical Report we propose a set of improvements with respect to the\nKernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start with\na scheme inspired by Auto-Context, but that is suitable in situations where the\nlack of large training sets poses a potential problem of overfitting. The aim\nis to capture the interactions between neighboring image pixels to better\nregularize the boundaries of segmented regions. As in Auto-Context [Tu et al.,\nPAMI 2009] the segmentation process is iterative and, at each iteration, the\nsegmentation results for the previous iterations are taken into account in\nconjunction with the image itself. However, unlike in [Tu et al., PAMI 2009],\nwe organize our recursion so that the classifiers can progressively focus on\ndifficult-to-classify locations. This lets us exploit the power of the\ndecision-tree paradigm while avoiding over-fitting. In the context of this\narchitecture, KernelBoost represents a powerful building block due to its\nability to learn on the score maps coming from previous iterations. We first\nintroduce two important mechanisms to empower the KernelBoost classifier,\nnamely pooling and the clustering of positive samples based on the appearance\nof the corresponding ground-truth. These operations significantly contribute to\nincrease the effectiveness of the system on biomedical images, where texture\nplays a major role in the recognition of the different image components. We\nthen present some other techniques that can be easily integrated in the\nKernelBoost framework to further improve the accuracy of the final\nsegmentation. We show extensive results on different medical image datasets,\nincluding some multi-label tasks, on which our method is shown to outperform\nstate-of-the-art approaches. The resulting segmentations display high accuracy,\nneat contours, and reduced noise.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 09:07:03 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Rigamonti", "Roberto", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}]