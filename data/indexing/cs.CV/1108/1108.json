[{"id": "1108.0007", "submitter": "Sheng Yi", "authors": "Sheng Yi, Hamid Krim, Larry K. Norris", "title": "A Invertible Dimension Reduction of Curves on a Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we propose a novel lower dimensional representation of a shape\nsequence. The proposed dimension reduction is invertible and computationally\nmore efficient in comparison to other related works. Theoretically, the\ndifferential geometry tools such as moving frame and parallel transportation\nare successfully adapted into the dimension reduction problem of high\ndimensional curves. Intuitively, instead of searching for a global flat\nsubspace for curve embedding, we deployed a sequence of local flat subspaces\nadaptive to the geometry of both of the curve and the manifold it lies on. In\npractice, the experimental results of the dimension reduction and\nreconstruction algorithms well illustrate the advantages of the proposed\ntheoretical innovation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 20:06:59 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Yi", "Sheng", ""], ["Krim", "Hamid", ""], ["Norris", "Larry K.", ""]]}, {"id": "1108.0502", "submitter": "Ankit Chaudhary", "authors": "Jagdish Lal Raheja, Karen Das, Ankit Chaudhary", "title": "An Efficient Real Time Method of Fingertip Detection", "comments": "This paper was published in the 7th International Conference on\n  Trends in Industrial Measurements and Automation (TIMA 2011), CSIR Complex,\n  Chennai, India, 6-8 Jan, 2011, pp. 447-450", "journal-ref": "7th International Conference on Trends in Industrial Measurements\n  and Automation (TIMA 2011), CSIR Complex, Chennai, India, 6-8 Jan, 2011, pp.\n  447-450", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingertips detection has been used in many applications, and it is very\npopular and commonly used in the area of Human Computer Interaction these days.\nThis paper presents a novel time efficient method that will lead to fingertip\ndetection after cropping the irrelevant parts of input image. Binary silhouette\nof the input image is generated using HSV color space based skin filter and\nhand cropping done based on histogram of the hand image. The cropped image will\nbe used to figure out the fingertips.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 07:46:16 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Raheja", "Jagdish Lal", ""], ["Das", "Karen", ""], ["Chaudhary", "Ankit", ""]]}, {"id": "1108.1122", "submitter": "Yaniv Taigman", "authors": "Yaniv Taigman and Lior Wolf", "title": "Leveraging Billions of Faces to Overcome Performance Barriers in\n  Unconstrained Face Recognition", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ the face recognition technology developed in house at face.com to a\nwell accepted benchmark and show that without any tuning we are able to\nconsiderably surpass state of the art results. Much of the improvement is\nconcentrated in the high-valued performance point of zero false positive\nmatches, where the obtained recall rate almost doubles the best reported result\nto date. We discuss the various components and innovations of our system that\nenable this significant performance gap. These components include extensive\nutilization of an accurate 3D reconstructed shape model dealing with challenges\narising from pose and illumination. In addition, discriminative models based on\nbillions of faces are used in order to overcome aging and facial expression as\nwell as low light and overexposure. Finally, we identify a challenging set of\nidentification queries that might provide useful focus for future research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 15:51:19 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Taigman", "Yaniv", ""], ["Wolf", "Lior", ""]]}, {"id": "1108.1169", "submitter": "Karol Gregor", "authors": "Karol Gregor and Yann LeCun", "title": "Learning Representations by Maximizing Compression", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm that learns a representation of data through\ncompression. The algorithm 1) predicts bits sequentially from those previously\nseen and 2) has a structure and a number of computations similar to an\nautoencoder. The likelihood under the model can be calculated exactly, and\narithmetic coding can be used directly for compression. When training on digits\nthe algorithm learns filters similar to those of restricted boltzman machines\nand denoising autoencoders. Independent samples can be drawn from the model by\na single sweep through the pixels. The algorithm has a good compression\nperformance when compared to other methods that work under random ordering of\npixels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 19:00:14 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["Gregor", "Karol", ""], ["LeCun", "Yann", ""]]}, {"id": "1108.1353", "submitter": "Susheel  Kumar k", "authors": "K.Susheel Kumar, Vijay Bhaskar Semwal, R C Tripathi", "title": "Real time face recognition using adaboost improved fast PCA algorithm", "comments": "14 pages; ISSN : 0975-900X (Online), 0976-2191 (Print)", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol.2, No.3, July 2011, 45-58", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated system for human face recognition in a real\ntime background world for a large homemade dataset of persons face. The task is\nvery difficult as the real time background subtraction in an image is still a\nchallenge. Addition to this there is a huge variation in human face image in\nterms of size, pose and expression. The system proposed collapses most of this\nvariance. To detect real time human face AdaBoost with Haar cascade is used and\na simple fast PCA and LDA is used to recognize the faces detected. The matched\nface is then used to mark attendance in the laboratory, in our case. This\nbiometric system is a real time attendance system based on the human face\nrecognition with a simple and fast algorithms and gaining a high accuracy\nrate..\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2011 15:41:31 GMT"}], "update_date": "2011-08-08", "authors_parsed": [["Kumar", "K. Susheel", ""], ["Semwal", "Vijay Bhaskar", ""], ["Tripathi", "R C", ""]]}, {"id": "1108.1500", "submitter": "Sahar Yousefi ms", "authors": "Sahar Yousefi, Morteza Zahedi", "title": "Gender Recognition Based on Sift Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a robust approach for face detection and gender\nclassification in color images. Previous researches about gender recognition\nsuppose an expensive computational and time-consuming pre-processing step in\norder to alignment in which face images are aligned so that facial landmarks\nlike eyes, nose, lips, chin are placed in uniform locations in image. In this\npaper, a novel technique based on mathematical analysis is represented in three\nstages that eliminates alignment step. First, a new color based face detection\nmethod is represented with a better result and more robustness in complex\nbackgrounds. Next, the features which are invariant to affine transformations\nare extracted from each face using scale invariant feature transform (SIFT)\nmethod. To evaluate the performance of the proposed algorithm, experiments have\nbeen conducted by employing a SVM classifier on a database of face images which\ncontains 500 images from distinct people with equal ratio of male and female.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2011 17:52:00 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Yousefi", "Sahar", ""], ["Zahedi", "Morteza", ""]]}, {"id": "1108.1636", "submitter": "Peng Zhang", "authors": "Peng Zhang, Yuanyuan Ren, and Bo Zhang", "title": "A new embedding quality assessment method for manifold learning", "comments": "16 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning is a hot research topic in the field of computer science. A\ncrucial issue with current manifold learning methods is that they lack a\nnatural quantitative measure to assess the quality of learned embeddings, which\ngreatly limits their applications to real-world problems. In this paper, a new\nembedding quality assessment method for manifold learning, named as\nNormalization Independent Embedding Quality Assessment (NIEQA), is proposed.\nCompared with current assessment methods which are limited to isometric\nembeddings, the NIEQA method has a much larger application range due to two\nfeatures. First, it is based on a new measure which can effectively evaluate\nhow well local neighborhood geometry is preserved under normalization, hence it\ncan be applied to both isometric and normalized embeddings. Second, it can\nprovide both local and global evaluations to output an overall assessment.\nTherefore, NIEQA can serve as a natural tool in model selection and evaluation\ntasks for manifold learning. Experimental results on benchmark data sets\nvalidate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2011 08:59:05 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Zhang", "Peng", ""], ["Ren", "Yuanyuan", ""], ["Zhang", "Bo", ""]]}, {"id": "1108.2475", "submitter": "Asha V", "authors": "V. Asha", "title": "Undithering using linear filtering and non-linear diffusion techniques", "comments": "14 pages, 15 figures. International Journal of Artificial\n  Intelligence, Spring 2009, Volume 2, Number S09", "journal-ref": "International Journal of Artificial Intelligence, Spring 2009,\n  Volume 2, Number S09", "doi": null, "report-no": "ISSN: 0974-0635", "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression is a method of improving the efficiency of transmission and\nstorage of images. Dithering, as a method of data compression, can be used to\nconvert an 8-bit gray level image into a 1-bit / binary image. Undithering is\nthe process of reconstruction of gray image from binary image obtained from\ndithering of gray image. In the present paper, I propose a method of\nundithering using linear filtering followed by anisotropic diffusion which\nbrings the advantage of smoothing and edge enhancement. First-order statistical\nparameters, second-order statistical parameters, mean-squared error (MSE)\nbetween reconstructed image and the original image before dithering, and peak\nsignal to noise ratio (PSNR) are evaluated at each step of diffusion. Results\nof the experiments show that the reconstructed image is not as sharp as the\nimage before dithering but a large number of gray values are reproduced with\nreference to those of the original image prior to dithering.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 15:09:01 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Asha", "V.", ""]]}, {"id": "1108.2632", "submitter": "Philip Schniter", "authors": "Subhojit Som and Philip Schniter", "title": "Compressive Imaging using Approximate Message Passing and a Markov-Tree\n  Prior", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2012.2191780", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for compressive imaging that exploits both the\nsparsity and persistence across scales found in the 2D wavelet transform\ncoefficients of natural images. Like other recent works, we model wavelet\nstructure using a hidden Markov tree (HMT) but, unlike other works, ours is\nbased on loopy belief propagation (LBP). For LBP, we adopt a recently proposed\n\"turbo\" message passing schedule that alternates between exploitation of HMT\nstructure and exploitation of compressive-measurement structure. For the\nlatter, we leverage Donoho, Maleki, and Montanari's recently proposed\napproximate message passing (AMP) algorithm. Experiments with a large image\ndatabase suggest that, relative to existing schemes, our turbo LBP approach\nyields state-of-the-art reconstruction performance with substantial reduction\nin complexity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2011 14:41:49 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Som", "Subhojit", ""], ["Schniter", "Philip", ""]]}, {"id": "1108.3250", "submitter": "Firouz Wassai", "authors": "Firouz Abdullah Al-Wassai, N.V. Kalyankar, Ali A. Al-Zaky", "title": "The Statistical methods of Pixel-Based Image Fusion Techniques", "comments": "Keywords: Data Fusion, Resolution Enhancement, Statistical fusion,\n  Correlation Modeling, Matching, pixel based fusion", "journal-ref": "International Journal of Artificial Intelligence and Knowledge\n  Discovery Vol.1, Issue 3, July, 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many image fusion methods that can be used to produce\nhigh-resolution mutlispectral images from a high-resolution panchromatic (PAN)\nimage and low-resolution multispectral (MS) of remote sensed images. This paper\nattempts to undertake the study of image fusion techniques with different\nStatistical techniques for image fusion as Local Mean Matching (LMM), Local\nMean and Variance Matching (LMVM), Regression variable substitution (RVS),\nLocal Correlation Modeling (LCM) and they are compared with one another so as\nto choose the best technique, that can be applied on multi-resolution satellite\nimages. This paper also devotes to concentrate on the analytical techniques for\nevaluating the quality of image fusion (F) by using various methods including\nStandard Deviation (SD), Entropy(En), Correlation Coefficient (CC), Signal-to\nNoise Ratio (SNR), Normalization Root Mean Square Error (NRMSE) and Deviation\nIndex (DI) to estimate the quality and degree of information improvement of a\nfused image quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2011 16:51:21 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""], ["Al-Zaky", "Ali A.", ""]]}, {"id": "1108.3251", "submitter": "Artem Migukin", "authors": "Artem Migukin, Vladimir Katkovnik and Jaakko Astola", "title": "Advanced phase retrieval: maximum likelihood technique with sparse\n  regularization of phase and amplitude", "comments": "Submitted to the 10th IMEKO Symposium LMPMI (Laser Metrology for\n  Precision Measurement and Inspection in Industry) on May 31, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse modeling is one of the efficient techniques for imaging that allows\nrecovering lost information. In this paper, we present a novel iterative\nphase-retrieval algorithm using a sparse representation of the object amplitude\nand phase. The algorithm is derived in terms of a constrained maximum\nlikelihood, where the wave field reconstruction is performed using a number of\nnoisy intensity-only observations with a zero-mean additive Gaussian noise. The\ndeveloped algorithm enables the optimal solution for the object wave field\nreconstruction. Our goal is an improvement of the reconstruction quality with\nrespect to the conventional algorithms. Sparse regularization results in\nadvanced reconstruction accuracy, and numerical simulations demonstrate\nsignificant enhancement of imaging.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 09:37:15 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Migukin", "Artem", ""], ["Katkovnik", "Vladimir", ""], ["Astola", "Jaakko", ""]]}, {"id": "1108.3298", "submitter": "Nando de Freitas", "authors": "Byron Knoll, Nando de Freitas", "title": "A Machine Learning Perspective on Predictive Coding with PAQ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAQ8 is an open source lossless data compression algorithm that currently\nachieves the best compression rates on many benchmarks. This report presents a\ndetailed description of PAQ8 from a statistical machine learning perspective.\nIt shows that it is possible to understand some of the modules of PAQ8 and use\nthis understanding to improve the method. However, intuitive statistical\nexplanations of the behavior of other modules remain elusive. We hope the\ndescription in this report will be a starting point for discussions that will\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\nrecurrent neural networks and stochastic memoizers. Finally, the report\npresents a broad range of new applications of PAQ to machine learning tasks\nincluding language modeling and adaptive text prediction, adaptive game\nplaying, classification, and compression using features from the field of deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 18:06:29 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Knoll", "Byron", ""], ["de Freitas", "Nando", ""]]}, {"id": "1108.3525", "submitter": "Jason Corso", "authors": "Yingjie Miao and Jason J. Corso", "title": "Hamiltonian Streamline Guided Feature Extraction with Applications to\n  Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new feature extraction method based on two dynamical systems\ninduced by intensity landscape: the negative gradient system and the\nHamiltonian system. We build features based on the Hamiltonian streamlines.\nThese features contain nice global topological information about the intensity\nlandscape, and can be used for object detection. We show that for training\nimages of same size, our feature space is much smaller than that generated by\nHaar-like features. The training time is extremely short, and detection speed\nand accuracy is similar to Haar-like feature based classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 17:06:41 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Miao", "Yingjie", ""], ["Corso", "Jason J.", ""]]}, {"id": "1108.3605", "submitter": "Adrian Barbu", "authors": "Adrian Barbu", "title": "Hierarchical Object Parsing from Structured Noisy Point Clouds", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2012.262", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object parsing and segmentation from point clouds are challenging tasks\nbecause the relevant data is available only as thin structures along object\nboundaries or other features, and is corrupted by large amounts of noise. To\nhandle this kind of data, flexible shape models are desired that can accurately\nfollow the object boundaries. Popular models such as Active Shape and Active\nAppearance models lack the necessary flexibility for this task, while recent\napproaches such as the Recursive Compositional Models make model\nsimplifications in order to obtain computational guarantees. This paper\ninvestigates a hierarchical Bayesian model of shape and appearance in a\ngenerative setting. The input data is explained by an object parsing layer,\nwhich is a deformation of a hidden PCA shape model with Gaussian prior. The\npaper also introduces a novel efficient inference algorithm that uses informed\ndata-driven proposals to initialize local searches for the hidden variables.\nApplied to the problem of object parsing from structured point clouds such as\nedge detection images, the proposed approach obtains state of the art parsing\nerrors on two standard datasets without using any intensity information.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 02:11:34 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2012 14:24:08 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Barbu", "Adrian", ""]]}, {"id": "1108.4079", "submitter": "Jason J Corso", "authors": "Jason J. Corso", "title": "Toward Parts-Based Scene Understanding with Pixel-Support Parts-Sparse\n  Pictorial Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding remains a significant challenge in the computer vision\ncommunity. The visual psychophysics literature has demonstrated the importance\nof interdependence among parts of the scene. Yet, the majority of methods in\ncomputer vision remain local. Pictorial structures have arisen as a fundamental\nparts-based model for some vision problems, such as articulated object\ndetection. However, the form of classical pictorial structures limits their\napplicability for global problems, such as semantic pixel labeling. In this\npaper, we propose an extension of the pictorial structures approach, called\npixel-support parts-sparse pictorial structures, or PS3, to overcome this\nlimitation. Our model extends the classical form in two ways: first, it defines\nparts directly based on pixel-support rather than in a parametric form, and\nsecond, it specifies a space of plausible parts-based scene models and permits\none to be used for inference on any given image. PS3 makes strides toward\nunifying object-level and pixel-level modeling of scene elements. In this\nreport, we implement the first half of our model and rely upon external\nknowledge to provide an initial graph structure for a given image. Our\nexperimental results on benchmark datasets demonstrate the capability of this\nnew parts-based view of scene modeling.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 02:08:45 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Corso", "Jason J.", ""]]}, {"id": "1108.4098", "submitter": "Firouz Wassai", "authors": "Firouz Abdullah Al-Wassai, N.V. Kalyankar, Ali A. Al-Zaky", "title": "Multisensor Images Fusion Based on Feature-Level", "comments": "Keywords: Image fusion, Feature, Edge Fusion, Segment Fusion, IHS,\n  PCA", "journal-ref": "International Journal of Latest Technology in\n  Engineering,Management & Applied Science (IJLTEMAS),Vol. I, Issue V, 2012,\n  124-138", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until now, of highest relevance for remote sensing data processing and\nanalysis have been techniques for pixel level image fusion. So, This paper\nattempts to undertake the study of Feature-Level based image fusion. For this\npurpose, feature based fusion techniques, which are usually based on empirical\nor heuristic rules, are employed. Hence, in this paper we consider feature\nextraction (FE) for fusion. It aims at finding a transformation of the original\nspace that would produce such new features, which preserve or improve as much\nas possible. This study introduces three different types of Image fusion\ntechniques including Principal Component Analysis based Feature Fusion (PCA),\nSegment Fusion (SF) and Edge fusion (EF). This paper also devotes to\nconcentrate on the analytical techniques for evaluating the quality of image\nfusion (F) by using various methods including (SD), (En), (CC), (SNR), (NRMSE)\nand (DI) to estimate the quality and degree of information improvement of a\nfused image quantitatively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 07:43:46 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Al-Wassai", "Firouz Abdullah", ""], ["Kalyankar", "N. V.", ""], ["Al-Zaky", "Ali A.", ""]]}, {"id": "1108.4315", "submitter": "Won Yeol Lee", "authors": "Won Yeol Lee, Young Woo Kim, Se Yun Kim, Jae Young Lim, Dong Hoon Lim", "title": "Edge detection based on morphological amoebas", "comments": "To appear in The Imaging Science Journal", "journal-ref": null, "doi": "10.1179/1743131X11Y.0000000013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the edges of objects within images is critical for quality image\nprocessing. We present an edge-detecting technique that uses morphological\namoebas that adjust their shape based on variation in image contours. We\nevaluate the method both quantitatively and qualitatively for edge detection of\nimages, and compare it to classic morphological methods. Our amoeba-based\nedge-detection system performed better than the classic edge detectors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2011 13:49:57 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Lee", "Won Yeol", ""], ["Kim", "Young Woo", ""], ["Kim", "Se Yun", ""], ["Lim", "Jae Young", ""], ["Lim", "Dong Hoon", ""]]}, {"id": "1108.4973", "submitter": "Alexandre Levada", "authors": "Alexandre L. M. Levada", "title": "Learning from Complex Systems: On the Roles of Entropy and Fisher\n  Information in Pairwise Isotropic Gaussian Markov Random Fields", "comments": "46 pages, 16 Figures", "journal-ref": "Entropy, v. 16, n. 2, Special Issue on Information Geometry, 2014", "doi": "10.3390/e16021002", "report-no": null, "categories": "cs.IT cs.AI cs.CV math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Random Field models are powerful tools for the study of complex\nsystems. However, little is known about how the interactions between the\nelements of such systems are encoded, especially from an information-theoretic\nperspective. In this paper, our goal is to enlight the connection between\nFisher information, Shannon entropy, information geometry and the behavior of\ncomplex systems modeled by isotropic pairwise Gaussian Markov random fields. We\npropose analytical expressions to compute local and global versions of these\nmeasures using Besag's pseudo-likelihood function, characterizing the system's\nbehavior through its \\emph{Fisher curve}, a parametric trajectory accross the\ninformation space that provides a geometric representation for the study of\ncomplex systems. Computational experiments show how the proposed tools can be\nuseful in extrating relevant information from complex patterns. The obtained\nresults quantify and support our main conclusion, which is: in terms of\ninformation, moving towards higher entropy states (A --> B) is different from\nmoving towards lower entropy states (B --> A), since the \\emph{Fisher curves}\nare not the same given a natural orientation (the direction of time).\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 00:50:42 GMT"}, {"version": "v10", "created": "Mon, 27 May 2013 19:24:52 GMT"}, {"version": "v11", "created": "Tue, 11 Jun 2013 17:15:42 GMT"}, {"version": "v12", "created": "Wed, 16 Oct 2013 20:35:39 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2011 14:36:19 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2011 17:04:07 GMT"}, {"version": "v4", "created": "Fri, 25 Nov 2011 01:50:30 GMT"}, {"version": "v5", "created": "Thu, 24 May 2012 18:59:01 GMT"}, {"version": "v6", "created": "Mon, 26 Nov 2012 23:37:41 GMT"}, {"version": "v7", "created": "Sun, 2 Dec 2012 01:26:00 GMT"}, {"version": "v8", "created": "Sat, 8 Dec 2012 12:40:00 GMT"}, {"version": "v9", "created": "Thu, 23 May 2013 23:45:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Levada", "Alexandre L. M.", ""]]}, {"id": "1108.5359", "submitter": "Risheng Liu", "authors": "Risheng Liu and Zhouchen Lin and Siming Wei and Zhixun Su", "title": "Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, exactly recovering the intrinsic data structure from\ncorrupted observations, which is known as robust principal component analysis\n(RPCA), has attracted tremendous interests and found many applications in\ncomputer vision. Recently, this problem has been formulated as recovering a\nlow-rank component and a sparse component from the observed data matrix. It is\nproved that under some suitable conditions, this problem can be exactly solved\nby principal component pursuit (PCP), i.e., minimizing a combination of nuclear\nnorm and $l_1$ norm. Most of the existing methods for solving PCP require\nsingular value decompositions (SVD) of the data matrix, resulting in a high\ncomputational complexity, hence preventing the applications of RPCA to very\nlarge scale computer vision problems. In this paper, we propose a novel\nalgorithm, called $l_1$ filtering, for \\emph{exactly} solving PCP with an\n$O(r^2(m+n))$ complexity, where $m\\times n$ is the size of data matrix and $r$\nis the rank of the matrix to recover, which is supposed to be much smaller than\n$m$ and $n$. Moreover, $l_1$ filtering is \\emph{highly parallelizable}. It is\nthe first algorithm that can \\emph{exactly} solve a nuclear norm minimization\nproblem in \\emph{linear time} (with respect to the data size). Experiments on\nboth synthetic data and real applications testify to the great advantage of\n$l_1$ filtering in speed over state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 17:40:30 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2011 01:32:34 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2012 08:39:27 GMT"}, {"version": "v4", "created": "Sun, 6 May 2012 06:16:32 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Liu", "Risheng", ""], ["Lin", "Zhouchen", ""], ["Wei", "Siming", ""], ["Su", "Zhixun", ""]]}, {"id": "1108.5395", "submitter": "Laurent Duval", "authors": "Caroline Chaux and Jean-Christophe Pesquet and Laurent Duval", "title": "Noise Covariance Properties in Dual-Tree Wavelet Decompositions", "comments": null, "journal-ref": "IEEE Transactions on Information Theory, December 2007, Volume 53,\n  Issue 12, p. 2397 - 2412", "doi": "10.1109/TIT.2007.909104", "report-no": null, "categories": "math.ST cs.CV stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-tree wavelet decompositions have recently gained much popularity, mainly\ndue to their ability to provide an accurate directional analysis of images\ncombined with a reduced redundancy. When the decomposition of a random process\nis performed -- which occurs in particular when an additive noise is corrupting\nthe signal to be analyzed -- it is useful to characterize the statistical\nproperties of the dual-tree wavelet coefficients of this process. As dual-tree\ndecompositions constitute overcomplete frame expansions, correlation structures\nare introduced among the coefficients, even when a white noise is analyzed. In\nthis paper, we show that it is possible to provide an accurate description of\nthe covariance properties of the dual-tree coefficients of a wide-sense\nstationary process. The expressions of the (cross-)covariance sequences of the\ncoefficients are derived in the one and two-dimensional cases. Asymptotic\nresults are also provided, allowing to predict the behaviour of the\nsecond-order moments for large lag values or at coarse resolution. In addition,\nthe cross-correlations between the primal and dual wavelets, which play a\nprimary role in our theoretical analysis, are calculated for a number of\nclassical wavelet families. Simulation results are finally provided to validate\nthese results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 21:06:56 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Chaux", "Caroline", ""], ["Pesquet", "Jean-Christophe", ""], ["Duval", "Laurent", ""]]}, {"id": "1108.5710", "submitter": "Mark Schmidt", "authors": "Mark Schmidt (INRIA Paris - Rocquencourt), Karteek Alahari (INRIA\n  Paris - Rocquencourt)", "title": "Generalized Fast Approximate Energy Minimization via Graph Cuts:\n  Alpha-Expansion Beta-Shrink Moves", "comments": "Conference on Uncertainty in Artificial Intelligence (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present alpha-expansion beta-shrink moves, a simple generalization of the\nwidely-used alpha-beta swap and alpha-expansion algorithms for approximate\nenergy minimization. We show that in a certain sense, these moves dominate both\nalpha-beta-swap and alpha-expansion moves, but unlike previous generalizations\nthe new moves require no additional assumptions and are still solvable in\npolynomial-time. We show promising experimental results with the new moves,\nwhich we believe could be used in any context where alpha-expansions are\ncurrently employed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 19:06:30 GMT"}], "update_date": "2012-02-19", "authors_parsed": [["Schmidt", "Mark", "", "INRIA Paris - Rocquencourt"], ["Alahari", "Karteek", "", "INRIA\n  Paris - Rocquencourt"]]}, {"id": "1108.5720", "submitter": "Michael Noelle", "authors": "Michael N\\\"olle and Martin Suda", "title": "Conjugate Variables as a Resource in Signal and Image Processing", "comments": "22 pages, 2 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new technique to model joint distributions of\nsignals. Our technique is based on quantum mechanical conjugate variables. We\nshow that the transition probability of quantum states leads to a distance\nfunction on the signals. This distance function obeys the triangle inequality\non all quantum states and becomes a metric on pure quantum states. Treating\nsignals as conjugate variables allows us to create a new approach to segment\nthem.\n  Keywords: Quantum information, transition probability, Euclidean distance,\nFubini-study metric, Bhattacharyya coefficients, conjugate variable,\nsignal/sensor fusion, signal and image segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 19:28:13 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["N\u00f6lle", "Michael", ""], ["Suda", "Martin", ""]]}, {"id": "1108.6294", "submitter": "Sudha Livingston", "authors": "L.R Sudha, Dr.R Bhavani", "title": "Biometric Authorization System using Gait Biometry", "comments": "12 pages, 4 figures, 6 tables; International Journal of Computer\n  Science, Engineering and Applications (IJCSEA) Vol.1, No.4, August 2011", "journal-ref": null, "doi": "10.5121/ijcsea.2011.1401", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Human gait, which is a new biometric aimed to recognize individuals by the\nway they walk have come to play an increasingly important role in visual\nsurveillance applications. In this paper a novel hybrid holistic approach is\nproposed to show how behavioural walking characteristics can be used to\nrecognize unauthorized and suspicious persons when they enter a surveillance\narea. Initially background is modelled from the input video captured from\ncameras deployed for security and the foreground moving object in the\nindividual frames are segmented using the background subtraction algorithm.\nThen gait representing spatial, temporal and wavelet components are extracted\nand fused for training and testing multi class support vector machine models\n(SVM). The proposed system is evaluated using side view videos of NLPR\ndatabase. The experimental results demonstrate that the proposed system\nachieves a pleasing recognition rate and also the results indicate that the\nclassification ability of SVM with Radial Basis Function (RBF) is better than\nwith other kernel functions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 17:22:51 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Sudha", "L. R", ""], ["Bhavani", "Dr. R", ""]]}, {"id": "1108.6304", "submitter": "Eraldo Marinho", "authors": "Eraldo Pereira Marinho and Carmen Maria Andreazza", "title": "Anisotropic k-Nearest Neighbor Search Using Covariance Quadtree", "comments": "Work presented at the Minisymposia of Computational Geometry in the\n  joint events IX Argentinian Congress on Computational Mechanics, XXXI\n  Iberian-Latin-American Congress on Computational Methods in Engineering, II\n  South American Congress on Computational Mechanics, held in Buenos Aires in\n  15-18 November 2010; Mec\\'anica Computacional (Computational Mechanics) Vol.\n  XXIX, 2010, ISSN 1666-6070", "journal-ref": "2010, Mec\\'anica Computacional, Volume XXIX. Number 60.\n  Computational Geometry (A), pp 6045-6064", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a variant of the hyper-quadtree that divides a multidimensional\nspace according to the hyperplanes associated to the principal components of\nthe data in each hyperquadrant. Each of the $2^\\lambda$ hyper-quadrants is a\ndata partition in a $\\lambda$-dimension subspace, whose intrinsic\ndimensionality $\\lambda\\leq d$ is reduced from the root dimensionality $d$ by\nthe principal components analysis, which discards the irrelevant eigenvalues of\nthe local covariance matrix. In the present method a component is irrelevant if\nits length is smaller than, or comparable to, the local inter-data spacing.\nThus, the covariance hyper-quadtree is fully adaptive to the local\ndimensionality. The proposed data-structure is used to compute the anisotropic\nK nearest neighbors (kNN), supported by the Mahalanobis metric. As an\napplication, we used the present k nearest neighbors method to perform density\nestimation over a noisy data distribution. Such estimation method can be\nfurther incorporated to the smoothed particle hydrodynamics, allowing computer\nsimulations of anisotropic fluid flows.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 17:57:27 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Marinho", "Eraldo Pereira", ""], ["Andreazza", "Carmen Maria", ""]]}]