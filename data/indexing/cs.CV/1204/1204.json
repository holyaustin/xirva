[{"id": "1204.0171", "submitter": "Mete Ozay", "authors": "Mete Ozay, Fatos T. Yarman Vural", "title": "A New Fuzzy Stacked Generalization Technique and Analysis of its\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a new Stacked Generalization technique called Fuzzy Stacked\nGeneralization (FSG) is proposed to minimize the difference between N -sample\nand large-sample classification error of the Nearest Neighbor classifier. The\nproposed FSG employs a new hierarchical distance learning strategy to minimize\nthe error difference. For this purpose, we first construct an ensemble of\nbase-layer fuzzy k- Nearest Neighbor (k-NN) classifiers, each of which receives\na different feature set extracted from the same sample set. The fuzzy\nmembership values computed at the decision space of each fuzzy k-NN classifier\nare concatenated to form the feature vectors of a fusion space. Finally, the\nfeature vectors are fed to a meta-layer classifier to learn the degree of\naccuracy of the decisions of the base-layer classifiers for meta-layer\nclassification. Rather than the power of the individual base layer-classifiers,\ndiversity and cooperation of the classifiers become an important issue to\nimprove the overall performance of the proposed FSG. A weak base-layer\nclassifier may boost the overall performance more than a strong classifier, if\nit is capable of recognizing the samples, which are not recognized by the rest\nof the classifiers, in its own feature space. The experiments explore the type\nof the collaboration among the individual classifiers required for an improved\nperformance of the suggested architecture. Experiments on multiple feature\nreal-world datasets show that the proposed FSG performs better than the state\nof the art ensemble learning algorithms such as Adaboost, Random Subspace and\nRotation Forest. On the other hand, compatible performances are observed in the\nexperiments on single feature multi-attribute datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 07:16:47 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2012 19:32:21 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2012 06:39:31 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2012 14:53:55 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2013 21:13:37 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Ozay", "Mete", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1204.0357", "submitter": "Stefan Bauer", "authors": "Stefan Bauer (1), Lutz-P. Nolte (1), Mauricio Reyes (1) ((1) Institute\n  for Surgical Technology and Biomechanics, University of Bern, Switzerland)", "title": "Skull-stripping for Tumor-bearing Brain Images", "comments": "Swiss Society of Biomedical Engineering, Annual Meeting 2011, Bern,\n  Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skull-stripping separates the skull region of the head from the soft brain\ntissues. In many cases of brain image analysis, this is an essential\npreprocessing step in order to improve the final result. This is true for both\nregistration and segmentation tasks. In fact, skull-stripping of magnetic\nresonance images (MRI) is a well-studied problem with numerous publications in\nrecent years. Many different algorithms have been proposed, a summary and\ncomparison of which can be found in [Fennema-Notestine, 2006]. Despite the\nabundance of approaches, we discovered that the algorithms which had been\nsuggested so far, perform poorly when dealing with tumor-bearing brain images.\nThis is mostly due to additional difficulties in separating the brain from the\nskull in this case, especially when the lesion is located very close to the\nskull border. Additionally, images acquired according to standard clinical\nprotocols, often exhibit anisotropic resolution and only partial coverage,\nwhich further complicates the task. Therefore, we developed a method which is\ndedicated to skull-stripping for clinically acquired tumor-bearing brain\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 09:48:12 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bauer", "Stefan", ""], ["Nolte", "Lutz-P.", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1204.0684", "submitter": "Matthias Scholz", "authors": "Matthias Scholz", "title": "Validation of nonlinear PCA", "comments": "12 pages, 5 figures", "journal-ref": "Neural Processing Letters, 2012", "doi": "10.1007/s11063-012-9220-6", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear principal component analysis (PCA) can be extended to a nonlinear PCA\nby using artificial neural networks. But the benefit of curved components\nrequires a careful control of the model complexity. Moreover, standard\ntechniques for model selection, including cross-validation and more generally\nthe use of an independent test set, fail when applied to nonlinear PCA because\nof its inherent unsupervised characteristics. This paper presents a new\napproach for validating the complexity of nonlinear PCA models by using the\nerror in missing data estimation as a criterion for model selection. It is\nmotivated by the idea that only the model of optimal complexity is able to\npredict missing values with the highest accuracy. While standard test set\nvalidation usually favours over-fitted nonlinear PCA models, the proposed model\nvalidation approach correctly selects the optimal model complexity.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 13:22:07 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Scholz", "Matthias", ""]]}, {"id": "1204.0767", "submitter": "Vini Katyal", "authors": "Vini Katyal, Deepesh Srivastava", "title": "Efficient Fruit Defect Detection and Glare removal Algorithm by\n  anisotropic diffusion and 2D Gabor filter", "comments": "Errors in material", "journal-ref": "International Journal of Engineering Science & Advanced Technology\n  (IJESAT), Volume-2, Issue-2, 352 - 357 Mar-Apr 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on fruit defect detection and glare removal using\nmorphological operations, Glare removal can be considered as an important\npreprocessing step as uneven lighting may introduce it in images, which hamper\nthe results produced through segmentation by Gabor filters .The problem of\nglare in images is very pronounced sometimes due to the unusual reflectance\nfrom the camera sensor or stray light entering, this method counteracts this\nproblem and makes the defect detection much more pronounced. Anisotropic\ndiffusion is used for further smoothening of the images and removing the high\nenergy regions in an image for better defect detection and makes the defects\nmore retrievable. Our algorithm is robust and scalable the employability of a\nparticular mask for glare removal has been checked and proved useful for\ncounteracting.this problem, anisotropic diffusion further enhances the defects\nwith its use further Optimal Gabor filter at various orientations is used for\ndefect detection.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 19:02:54 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 13:37:41 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Katyal", "Vini", ""], ["Srivastava", "Deepesh", ""]]}, {"id": "1204.1177", "submitter": "Aamir Khan", "authors": "Aamir Khan, Hasan Farooq", "title": "Principal Component Analysis-Linear Discriminant Analysis Feature\n  Extractor for Pattern Recognition", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 6, No 2, November 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of embedded biometric systems is of prime importance with the\nemergence of fourth generation communication devices and advancement in\nsecurity systems This paper presents the realization of such technologies which\ndemands reliable and error-free biometric identity verification systems. High\ndimensional patterns are not permitted due to eigen-decomposition in high\ndimensional image space and degeneration of scattering matrices in small size\nsample. Generalization, dimensionality reduction and maximizing the margins are\ncontrolled by minimizing weight vectors. Results show good pattern by\nmultimodal biometric system proposed in this paper. This paper is aimed at\ninvestigating a biometric identity system using Principal Component Analysis\nand Lindear Discriminant Analysis with K-Nearest Neighbor and implementing such\nsystem in real-time using SignalWAVE.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 10:48:09 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Khan", "Aamir", ""], ["Farooq", "Hasan", ""]]}, {"id": "1204.1198", "submitter": "Md. Abu Naser Bikas", "authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel and Md. Abu Naser Bikas", "title": "A Complete Workflow for Development of Bangla OCR", "comments": null, "journal-ref": "International Journal of Computer Applications, Volume 21, No.9,\n  May 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 12:28:11 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Omee", "Farjana Yeasmin", ""], ["Himel", "Shiam Shabbir", ""], ["Bikas", "Md. Abu Naser", ""]]}, {"id": "1204.1277", "submitter": "Vikram Kumar", "authors": "Vikram Kumar, Kamran Niyazi, Swapnil Mahe, Swapnil Vyawahare", "title": "Mouse Simulation Using Two Coloured Tapes", "comments": "5 pages", "journal-ref": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.2, No.2, March 2012", "doi": "10.5121/ijist.2012.2206", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for Human Computer Interaction\n(HCI) where, we control cursor movement using a real-time camera. Current\nmethods involve changing mouse parts such as adding more buttons or changing\nthe position of the tracking ball. Instead, our method is to use a camera and\ncomputer vision technology, such as image segmentation and gesture recognition,\nto control mouse tasks (left and right clicking, double-clicking, and\nscrolling) and we show how it can perform everything as current mouse devices\ncan. The software will be developed in JAVA language. Recognition and pose\nestimation in this system are user independent and robust as we will be using\ncolour tapes on our finger to perform actions. The software can be used as an\nintuitive input interface to applications that require multi-dimensional\ncontrol e.g. computer games etc.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 17:00:47 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Kumar", "Vikram", ""], ["Niyazi", "Kamran", ""], ["Mahe", "Swapnil", ""], ["Vyawahare", "Swapnil", ""]]}, {"id": "1204.1393", "submitter": "Raquel Urtasun", "authors": "Koichiro Yamaguchi and Tamir Hazan and David McAllester and Raquel\n  Urtasun", "title": "Continuous Markov Random Fields for Robust Stereo Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel slanted-plane MRF model which reasons\njointly about occlusion boundaries as well as depth. We formulate the problem\nas the one of inference in a hybrid MRF composed of both continuous (i.e.,\nslanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.\nThis allows us to define potentials encoding the ownership of the pixels that\ncompose the boundary between segments, as well as potentials encoding which\njunctions are physically possible. Our approach outperforms the\nstate-of-the-art on Middlebury high resolution imagery as well as in the more\nchallenging KITTI dataset, while being more efficient than existing slanted\nplane MRF-based methods, taking on average 2 minutes to perform inference on\nhigh resolution imagery.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 01:40:21 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Yamaguchi", "Koichiro", ""], ["Hazan", "Tamir", ""], ["McAllester", "David", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1204.1611", "submitter": "Choon Boon Ng", "authors": "Choon Boon Ng, Yong Haur Tay, Bok Min Goi", "title": "Vision-based Human Gender Recognition: A Survey", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender is an important demographic attribute of people. This paper provides a\nsurvey of human gender recognition in computer vision. A review of approaches\nexploiting information from face and whole body (either from a still image or\ngait sequence) is presented. We highlight the challenges faced and survey the\nrepresentative methods of these approaches. Based on the results, good\nperformance have been achieved for datasets captured under controlled\nenvironments, but there is still much work that can be done to improve the\nrobustness of gender recognition under real-life environments.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 08:17:40 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Ng", "Choon Boon", ""], ["Tay", "Yong Haur", ""], ["Goi", "Bok Min", ""]]}, {"id": "1204.1615", "submitter": "Sofiene Haboubi", "authors": "Sofiene Haboubi and Samia Maddouri and Hamid Amiri", "title": "Discrimination between Arabic and Latin from bilingual documents", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/CCCA.2011.6031496", "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  2011 International Conference on Communications, Computing and Control\nApplications (CCCA)\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 09:28:19 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Haboubi", "Sofiene", ""], ["Maddouri", "Samia", ""], ["Amiri", "Hamid", ""]]}, {"id": "1204.1629", "submitter": "Mohamed Ali Mahjoub", "authors": "Mohamed Ali Mahjoub, karim kalti", "title": "Image segmentation by adaptive distance based on EM algorithm", "comments": "6 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Special Issue on Image Processing and Analysis, May 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Bayesian image segmentation algorithm based on finite\nmixtures. An EM algorithm is developed to estimate parameters of the Gaussian\nmixtures. The finite mixture is a flexible and powerful probabilistic modeling\ntool. It can be used to provide a model-based clustering in the field of\npattern recognition. However, the application of finite mixtures to image\nsegmentation presents some difficulties; especially it's sensible to noise. In\nthis paper we propose a variant of this method which aims to resolve this\nproblem. Our approach proceeds by the characterization of pixels by two\nfeatures: the first one describes the intrinsic properties of the pixel and the\nsecond characterizes the neighborhood of pixel. Then the classification is made\non the base on adaptive distance which privileges the one or the other features\naccording to the spatial position of the pixel in the image. The obtained\nresults have shown a significant improvement of our approach compared to the\nstandard version of EM algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 13:04:24 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Mahjoub", "Mohamed Ali", ""], ["kalti", "karim", ""]]}, {"id": "1204.1631", "submitter": "Mohamed Ali Mahjoub", "authors": "Khlifia jayech, mohamed ali mahjoub", "title": "New approach using Bayesian Network to improve content based image\n  classification systems", "comments": "10 pages, IJCSI International Journal of Computer Science Issues,\n  Vol. 7, Issue 6, November 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach based on augmented naive Bayes for image\nclassification. Initially, each image is cutting in a whole of blocks. For each\nblock, we compute a vector of descriptors. Then, we propose to carry out a\nclassification of the vectors of descriptors to build a vector of labels for\neach image. Finally, we propose three variants of Bayesian Networks such as\nNaive Bayesian Network (NB), Tree Augmented Naive Bayes (TAN) and Forest\nAugmented Naive Bayes (FAN) to classify the image using the vector of labels.\nThe results showed a marked improvement over the FAN, NB and TAN.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 13:29:17 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["jayech", "Khlifia", ""], ["mahjoub", "mohamed ali", ""]]}, {"id": "1204.1634", "submitter": "Mohamed Ali Mahjoub", "authors": "Oussema zayane, besma jouini, Mohamed Ali Mahjoub", "title": "Automatic liver segmentation method in CT images", "comments": "4 pages", "journal-ref": "Canadian Journal on Image Processing & Computer Vision Vol. 2, No.\n  8, 1923-1717 December 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to develop a method for automatic segmentation of the\nliver based on a priori knowledge of the image, such as location and shape of\nthe liver.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 13:46:24 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["zayane", "Oussema", ""], ["jouini", "besma", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1204.1678", "submitter": "Moncef Charfi", "authors": "Moncef Charfi, Monji Kherallah, Abdelkarim El Baati, Adel M. Alimi", "title": "A New Approach for Arabic Handwritten Postal Addresses Recognition", "comments": "7 pages, 7 figures; (IJACSA) International Journal of Advanced\n  Computer Science and Applications, Vol. 3, No. 3, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an automatic analysis system for the Arabic\nhandwriting postal addresses recognition, by using the beta elliptical model.\nOur system is divided into different steps: analysis, pre-processing and\nclassification. The first operation is the filtering of image. In the second,\nwe remove the border print, stamps and graphics. After locating the address on\nthe envelope, the address segmentation allows the extraction of postal code and\ncity name separately. The pre-processing system and the modeling approach are\nbased on two basic steps. The first step is the extraction of the temporal\norder in the image of the handwritten trajectory. The second step is based on\nthe use of Beta-Elliptical model for the representation of handwritten script.\nThe recognition system is based on Graph-matching algorithm. Our modeling and\nrecognition approaches were validated by using the postal code and city names\nextracted from the Tunisian postal envelopes data. The recognition rate\nobtained is about 98%.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 20:45:06 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Charfi", "Moncef", ""], ["Kherallah", "Monji", ""], ["Baati", "Abdelkarim El", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1204.1679", "submitter": "Mohamed Ali Mahjoub", "authors": "Khlifia Jayech, Mohamed Ali Mahjoub", "title": "Clustering and Bayesian network for image of faces classification", "comments": "12 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Special Issue on Image processing and Analysis, pp 35-44 May\n  2011", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a content based image classification system, target images are sorted by\nfeature similarities with respect to the query (CBIR). In this paper, we\npropose to use new approach combining distance tangent, k-means algorithm and\nBayesian network for image classification. First, we use the technique of\ntangent distance to calculate several tangent spaces representing the same\nimage. The objective is to reduce the error in the classification phase.\nSecond, we cut the image in a whole of blocks. For each block, we compute a\nvector of descriptors. Then, we use K-means to cluster the low-level features\nincluding color and texture information to build a vector of labels for each\nimage. Finally, we apply five variants of Bayesian networks classifiers\n(Na\\\"ive Bayes, Global Tree Augmented Na\\\"ive Bayes (GTAN), Global Forest\nAugmented Na\\\"ive Bayes (GFAN), Tree Augmented Na\\\"ive Bayes for each class\n(TAN), and Forest Augmented Na\\\"ive Bayes for each class (FAN) to classify the\nimage of faces using the vector of labels. In order to validate the feasibility\nand effectively, we compare the results of GFAN to FAN and to the others\nclassifiers (NB, GTAN, TAN). The results demonstrate FAN outperforms than GFAN,\nNB, GTAN and TAN in the overall classification accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 20:52:10 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Jayech", "Khlifia", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1204.1704", "submitter": "Vimala Alagumalai", "authors": "K.Somasundaram and S.Vimala", "title": "Multi-Level Coding Efficiency with Improved Quality for Image\n  Compression based on AMBTC", "comments": "10 Pages, 3 Figures, 2 Tables", "journal-ref": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.2, No.2, March 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed an extended version of Absolute Moment Block\nTruncation Coding (AMBTC) to compress images. Generally the elements of a\nbitplane used in the variants of Block Truncation Coding (BTC) are of size 1\nbit. But it has been extended to two bits in the proposed method. Number of\nstatistical moments preserved to reconstruct the compressed has also been\nraised from 2 to 4. Hence, the quality of the reconstructed images has been\nimproved significantly from 33.62 to 38.12 with the increase in bpp by 1. The\nincreased bpp (3) is further reduced to 1.75in multiple levels: in one level,\nby dropping 4 elements of the bitplane in such a away that the pixel values of\nthe dropped elements can easily be interpolated with out much of loss in the\nquality, in level two, eight elements are dropped and reconstructed later and\nin level three, the size of the statistical moments is reduced. The experiments\nwere carried over standard images of varying intensities. In all the cases, the\nproposed method outperforms the existing AMBTC technique in terms of both PSNR\nand bpp.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2012 03:44:13 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Somasundaram", "K.", ""], ["Vimala", "S.", ""]]}, {"id": "1204.1811", "submitter": "Rehan Khan", "authors": "Rehanullah Khan, Asad Maqsood, Zeeshan Khan, Muhammad Ishaq, Arsalan\n  Arif", "title": "Skin-color based videos categorization", "comments": "International Journal of Computer Science Issues (IJCSI), Volume 9,\n  Issue 1, No 3, January 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On dedicated websites, people can upload videos and share it with the rest of\nthe world. Currently these videos are cat- egorized manually by the help of the\nuser community. In this paper, we propose a combination of color spaces with\nthe Bayesian network approach for robust detection of skin color followed by an\nautomated video categorization. Exper- imental results show that our method can\nachieve satisfactory performance for categorizing videos based on skin color.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 06:44:42 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Khan", "Rehanullah", ""], ["Maqsood", "Asad", ""], ["Khan", "Zeeshan", ""], ["Ishaq", "Muhammad", ""], ["Arif", "Arsalan", ""]]}, {"id": "1204.2062", "submitter": "Babasaheb Patil", "authors": "Babasaheb G. Patil, Shaila Subbaraman", "title": "SVD-EBP Algorithm for Iris Pattern Recognition", "comments": "Dec2011-volume2.Issue 12 (IJACSA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper proposes a neural network approach based on Error Back Propagation\n(EBP) for classification of different eye images. To reduce the complexity of\nlayered neural network the dimensions of input vectors are optimized using\nSingular Value Decomposition (SVD). The main of this work is to provide for\nbest method for feature extraction and classification. The details of this\ncombined system named as SVD-EBP system, and results thereof are presented in\nthis paper.\n  Keywords- Singular value decomposition(SVD), Error back Propagation(EBP).\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:10:06 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Patil", "Babasaheb G.", ""], ["Subbaraman", "Shaila", ""]]}, {"id": "1204.2073", "submitter": "Sunanda Khandait", "authors": "S.P. Khandait, R.C. Thool and P.D. Khandait", "title": "Automatic facial feature extraction and expression recognition based on\n  neural network", "comments": "6 pages,pp. 113-118, (IJACSA) International Journal of Advanced\n  Computer Science and Applications, Vol. 2, No.1, January 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an approach to the problem of automatic facial feature\nextraction from a still frontal posed image and classification and recognition\nof facial expression and hence emotion and mood of a person is presented. Feed\nforward back propagation neural network is used as a classifier for classifying\nthe expressions of supplied face into seven basic categories like surprise,\nneutral, sad, disgust, fear, happy and angry. For face portion segmentation and\nlocalization, morphological image processing operations are used. Permanent\nfacial features like eyebrows, eyes, mouth and nose are extracted using SUSAN\nedge detection operator, facial geometry, edge projection analysis. Experiments\nare carried out on JAFFE facial expression database and gives better\nperformance in terms of 100% accuracy for training set and 95.26% accuracy for\ntest set.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:57:53 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Khandait", "S. P.", ""], ["Thool", "R. C.", ""], ["Khandait", "P. D.", ""]]}, {"id": "1204.2114", "submitter": "Yong Haur Tay", "authors": "Jun Yee Ng and Yong Haur Tay", "title": "Image-based Vehicle Classification System", "comments": "The 11th Asia-Pacific ITS Forum and Exhibition (ITS-AP 2011),\n  Kaoshiung, Taiwan. June 8-11, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic toll collection (ETC) system has been a common trend used for toll\ncollection on toll road nowadays. The implementation of electronic toll\ncollection allows vehicles to travel at low or full speed during the toll\npayment, which help to avoid the traffic delay at toll road. One of the major\ncomponents of an electronic toll collection is the automatic vehicle detection\nand classification (AVDC) system which is important to classify the vehicle so\nthat the toll is charged according to the vehicle classes. Vision-based vehicle\nclassification system is one type of vehicle classification system which adopt\ncamera as the input sensing device for the system. This type of system has\nadvantage over the rest for it is cost efficient as low cost camera is used.\nThe implementation of vision-based vehicle classification system requires lower\ninitial investment cost and very suitable for the toll collection trend\nmigration in Malaysia from single ETC system to full-scale multi-lane free flow\n(MLFF). This project includes the development of an image-based vehicle\nclassification system as an effort to seek for a robust vision-based vehicle\nclassification system. The techniques used in the system include\nscale-invariant feature transform (SIFT) technique, Canny's edge detector,\nK-means clustering as well as Euclidean distance matching. In this project, a\nunique way to image description as matching medium is proposed. This\ndistinctiveness of method is analogous to the human DNA concept which is highly\nunique. The system is evaluated on open datasets and return promising results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 11:59:10 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Ng", "Jun Yee", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1204.2134", "submitter": "Fernand  Meyer", "authors": "Fernand Meyer", "title": "The steepest watershed: from graphs to images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The watershed is a powerful tool for segmenting objects whose contours appear\nas crest lines on a gradient image. The watershed transform associates to a\ntopographic surface a partition into catchment basins, defined as attraction\nzones of a drop of water falling on the relief and following a line of steepest\ndescent. Unfortunately, catchment basins may overlap and do not form a\npartition. Moreover, current watershed algorithms, being shortsighted, do not\ncorrectly estimate the steepness of the downwards trajectories and overestimate\nthe overlapping zones of catchment basins. An arbitrary division of these zones\nbetween adjacent catchment basin results in a poor localization of the\ncontours. We propose an algorithm without myopia, which considers the total\nlength of a trajectory for estimating its steepness. We first consider\ntopographic surfaces defined on node weighted graphs. The graphs are pruned in\norder to eliminate all downwards trajectories which are not the steepest. An\niterative algorithm with simple neighborhood operations performs the pruning\nand constructs the catchment basins. The algorithm is then adapted to gray tone\nimages. The graph structure itself is encoded as an image thanks to the fixed\nneighborhood structure of grids. A pair of adaptative erosions and dilations\nprune the graph and extend the catchment basins. As a result one obtains a\nprecise detection of the catchment basins and a graph of the steepest\ntrajectories. A last iterative algorithm allows to follow selected downwards\ntrajectories in order to detect particular structures such as rivers or thalweg\nlines of the topographic surface.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 13:08:34 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Meyer", "Fernand", ""]]}, {"id": "1204.2294", "submitter": "Wan Mohd Yaakob Wan Bejuri B.Sc. Dip", "authors": "Wan Mohd Yaakob Wan Bejuri, Mohd Murtadha Mohamad, Maimunah Sapri and\n  Mohd Adly Rosly", "title": "Ubiquitous WLAN/Camera Positioning using Inverse Intensity Chromaticity\n  Space-based Feature Detection and Matching: A Preliminary Result", "comments": "International Conference on Man-Machine Systems 2012 (ICOMMS 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper present our new intensity chromaticity space-based feature\ndetection and matching algorithm. This approach utilizes hybridization of\nwireless local area network and camera internal sensor which to receive signal\nstrength from a access point and the same time retrieve interest point\ninformation from hallways. This information is combined by model fitting\napproach in order to find the absolute of user target position. No conventional\nsearching algorithm is required, thus it is expected reducing the computational\ncomplexity. Finally we present pre-experimental results to illustrate the\nperformance of the localization system for an indoor environment set-up.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 22:05:34 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Bejuri", "Wan Mohd Yaakob Wan", ""], ["Mohamad", "Mohd Murtadha", ""], ["Sapri", "Maimunah", ""], ["Rosly", "Mohd Adly", ""]]}, {"id": "1204.2311", "submitter": "Bin Shen", "authors": "Bin Shen, Luo Si, Rongrong Ji, Baodi Liu", "title": "Robust Nonnegative Matrix Factorization via $L_1$ Norm Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) is a widely used technique in many\napplications such as face recognition, motion segmentation, etc. It\napproximates the nonnegative data in an original high dimensional space with a\nlinear representation in a low dimensional space by using the product of two\nnonnegative matrices. In many applications data are often partially corrupted\nwith large additive noise. When the positions of noise are known, some existing\nvariants of NMF can be applied by treating these corrupted entries as missing\nvalues. However, the positions are often unknown in many real world\napplications, which prevents the usage of traditional NMF or other existing\nvariants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization\n(RobustNMF) algorithm that explicitly models the partial corruption as large\nadditive noise without requiring the information of positions of noise. In\npractice, large additive noise can be used to model outliers. In particular,\nthe proposed method jointly approximates the clean data matrix with the product\nof two nonnegative matrices and estimates the positions and values of\noutliers/noise. An efficient iterative optimization algorithm with a solid\ntheoretical justification has been proposed to learn the desired matrix\nfactorization. Experimental results demonstrate the advantages of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 01:03:03 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Shen", "Bin", ""], ["Si", "Luo", ""], ["Ji", "Rongrong", ""], ["Liu", "Baodi", ""]]}, {"id": "1204.2336", "submitter": "Venkata Ramana Chary Ramagiri", "authors": "R. Venkata Ramana Chary, D. Rajya Lakshmi, K. V. N. Sunitha", "title": "Feature Extraction Methods for Color Image Similarity", "comments": "11 pages, Advanced Computing: An International Journal (ACIJ), Vol.3,\n  No.2, March 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many User interactive systems are proposed all methods are trying to\nimplement as a user friendly and various approaches proposed but most of the\nsystems not reached to the use specifications like user friendly systems with\nuser interest, all proposed method implemented basic techniques some are\nimproved methods also propose but not reaching to the user specifications. In\nthis proposed paper we concentrated on image retrieval system with in early\ndays many user interactive systems performed with basic concepts but such\nsystems are not reaching to the user specifications and not attracted to the\nuser so a lot of research interest in recent years with new specifications,\nrecent approaches have user is interested in friendly interacted methods are\nexpecting, many are concentrated for improvement in all methods. In this\nproposed system we focus on the retrieval of images within a large image\ncollection based on color projections and different mathematical approaches are\nintroduced and applied for retrieval of images. before Appling proposed methods\nimages are sub grouping using threshold values, in this paper R G B color\ncombinations considered for retrieval of images, in proposed methods are\nimplemented and results are included, through results it is observed that we\nobtaining efficient results comparatively previous and existing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 04:45:51 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Chary", "R. Venkata Ramana", ""], ["Lakshmi", "D. Rajya", ""], ["Sunitha", "K. V. N.", ""]]}, {"id": "1204.2358", "submitter": "Meng Yang", "authors": "Lei Zhang, Meng Yang, Xiangchu Feng, Yi Ma, and David Zhang", "title": "Collaborative Representation based Classification for Face Recognition", "comments": "It is a substantial revision of a previous conference paper (L.\n  Zhang, M. Yang, et al. \"Sparse Representation or Collaborative\n  Representation: Which Helps Face Recognition?\" in ICCV 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  By coding a query sample as a sparse linear combination of all training\nsamples and then classifying it by evaluating which class leads to the minimal\ncoding residual, sparse representation based classification (SRC) leads to\ninteresting results for robust face recognition. It is widely believed that the\nl1- norm sparsity constraint on coding coefficients plays a key role in the\nsuccess of SRC, while its use of all training samples to collaboratively\nrepresent the query sample is rather ignored. In this paper we discuss how SRC\nworks, and show that the collaborative representation mechanism used in SRC is\nmuch more crucial to its success of face classification. The SRC is a special\ncase of collaborative representation based classification (CRC), which has\nvarious instantiations by applying different norms to the coding residual and\ncoding coefficient. More specifically, the l1 or l2 norm characterization of\ncoding residual is related to the robustness of CRC to outlier facial pixels,\nwhile the l1 or l2 norm characterization of coding coefficient is related to\nthe degree of discrimination of facial features. Extensive experiments were\nconducted to verify the face recognition accuracy and efficiency of CRC with\ndifferent instantiations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 07:13:20 GMT"}, {"version": "v2", "created": "Mon, 10 Mar 2014 09:42:43 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Zhang", "Lei", ""], ["Yang", "Meng", ""], ["Feng", "Xiangchu", ""], ["Ma", "Yi", ""], ["Zhang", "David", ""]]}, {"id": "1204.2741", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, Aaron Michaux, Siddharth Narayanaswamy, and Jeffrey Mark\n  Siskind", "title": "Simultaneous Object Detection, Tracking, and Event Recognition", "comments": null, "journal-ref": "Advances in Cognitive Systems, Vol. 2, pp. 203-220, 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common internal structure and algorithmic organization of object\ndetection, detection-based tracking, and event recognition facilitates a\ngeneral approach to integrating these three components. This supports\nmultidirectional information flow between these components allowing object\ndetection to influence tracking and event recognition and event recognition to\ninfluence tracking and object detection. The performance of the combination can\nexceed the performance of the components in isolation. This can be done with\nlinear asymptotic complexity.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 14:47:41 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Barbu", "Andrei", ""], ["Michaux", "Aaron", ""], ["Narayanaswamy", "Siddharth", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1204.2742", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven\n  Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy,\n  Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell\n  Waggoner, Song Wang, Jinlian Wei, Yifan Yin, and Zhiqi Zhang", "title": "Video In Sentences Out", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that produces sentential descriptions of video: who did\nwhat to whom, and where and how they did it. Action class is rendered as a\nverb, participant objects as noun phrases, properties of those objects as\nadjectival modifiers in those noun phrases,spatial relations between those\nparticipants as prepositional phrases, and characteristics of the event as\nprepositional-phrase adjuncts and adverbial modifiers. Extracting the\ninformation needed to render these linguistic entities requires an approach to\nevent recognition that recovers object tracks, the track-to-role assignments,\nand changing body posture.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 14:47:44 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Barbu", "Andrei", ""], ["Bridge", "Alexander", ""], ["Burchill", "Zachary", ""], ["Coroian", "Dan", ""], ["Dickinson", "Sven", ""], ["Fidler", "Sanja", ""], ["Michaux", "Aaron", ""], ["Mussman", "Sam", ""], ["Narayanaswamy", "Siddharth", ""], ["Salvi", "Dhaval", ""], ["Schmidt", "Lara", ""], ["Shangguan", "Jiangnan", ""], ["Siskind", "Jeffrey Mark", ""], ["Waggoner", "Jarrell", ""], ["Wang", "Song", ""], ["Wei", "Jinlian", ""], ["Yin", "Yifan", ""], ["Zhang", "Zhiqi", ""]]}, {"id": "1204.2801", "submitter": "Andrei Barbu", "authors": "Siddharth Narayanaswamy, Andrei Barbu, and Jeffrey Mark Siskind", "title": "Seeing Unseeability to See the Unseeable", "comments": null, "journal-ref": "Advances in Cognitive Systems, Vol. 2, pp. 77-94, 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that allows an observer to determine occluded portions\nof a structure by finding the maximum-likelihood estimate of those occluded\nportions consistent with visible image evidence and a consistency model. Doing\nthis requires determining which portions of the structure are occluded in the\nfirst place. Since each process relies on the other, we determine a solution to\nboth problems in tandem. We extend our framework to determine confidence of\none's assessment of which portions of an observed structure are occluded, and\nthe estimate of that occluded structure, by determining the sensitivity of\none's assessment to potential new observations. We further extend our framework\nto determine a robotic action whose execution would allow a new observation\nthat would maximally increase one's confidence.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 18:18:35 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Narayanaswamy", "Siddharth", ""], ["Barbu", "Andrei", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1204.2837", "submitter": "Fernand  Meyer", "authors": "Fernand Meyer", "title": "Watersheds, waterfalls, on edge or node weighted graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algebraic approach to the watershed adapted to edge or node\nweighted graphs. Starting with the flooding adjunction, we introduce the\nflooding graphs, for which node and edge weights may be deduced one from the\nother. Each node weighted or edge weighted graph may be transformed in a\nflooding graph, showing that there is no superiority in using one or the other,\nboth being equivalent. We then introduce pruning operators extract subgraphs of\nincreasing steepness. For an increasing steepness, the number of never\nascending paths becomes smaller and smaller. This reduces the watershed zone,\nwhere catchment basins overlap. A last pruning operator called scissor\nassociates to each node outside the regional minima one and only one edge. The\ncatchment basins of this new graph do not overlap and form a watershed\npartition. Again, with an increasing steepness, the number of distinct\nwatershed partitions contained in a graph becomes smaller and smaller.\nUltimately, for natural image, an infinite steepness leads to a unique\nsolution, as it is not likely that two absolutely identical non ascending paths\nof infinite steepness connect a node with two distinct minima. It happens that\nnon ascending paths of a given steepness are the geodesics of lexicographic\ndistance functions of a given depth. This permits to extract the watershed\npartitions as skeletons by zone of influence of the minima for such\nlexicographic distances. The waterfall hierarchy is obtained by a sequence of\noperations. The first constructs the minimum spanning forest which spans an\ninitial watershed partition. The contraction of the trees into one node\nproduces a reduced graph which may be submitted to the same treatment. The\nprocess is iterated until only one region remains. The union of the edges of\nall forests produced constitutes a minimum spanning tree of the initial graph.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 20:18:53 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Meyer", "Fernand", ""]]}, {"id": "1204.2912", "submitter": "Chunhua Shen", "authors": "Xi Li, Chunhua Shen, Qinfeng Shi, Anthony Dick, Anton van den Hengel", "title": "Non-sparse Linear Representations for Visual Tracking with Online\n  Reservoir Metric Learning", "comments": "Appearing in IEEE Conf. Computer Vision and Pattern Recognition, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most sparse linear representation-based trackers need to solve a\ncomputationally expensive L1-regularized optimization problem. To address this\nproblem, we propose a visual tracker based on non-sparse linear\nrepresentations, which admit an efficient closed-form solution without\nsacrificing accuracy. Moreover, in order to capture the correlation information\nbetween different feature dimensions, we learn a Mahalanobis distance metric in\nan online fashion and incorporate the learned metric into the optimization\nproblem for obtaining the linear representation. We show that online metric\nlearning using proximity comparison significantly improves the robustness of\nthe tracking, especially on those sequences exhibiting drastic appearance\nchanges. Furthermore, in order to prevent the unbounded growth in the number of\ntraining samples for the metric learning, we design a time-weighted reservoir\nsampling method to maintain and update limited-sized foreground and background\nsample buffers for balancing sample diversity and adaptability. Experimental\nresults on challenging videos demonstrate the effectiveness and robustness of\nthe proposed tracker.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 08:16:41 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Li", "Xi", ""], ["Shen", "Chunhua", ""], ["Shi", "Qinfeng", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1204.2994", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti and Todd Zickler", "title": "Image Restoration with Signal-dependent Camera Noise", "comments": "6 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a fast iterative algorithm for image denoising and\ndeconvolution with signal-dependent observation noise. We use an optimization\nstrategy based on variable splitting that adapts traditional Gaussian\nnoise-based restoration algorithms to account for the observed image being\ncorrupted by mixed Poisson-Gaussian noise and quantization errors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 13:48:27 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Zickler", "Todd", ""]]}, {"id": "1204.3616", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, Alexander Bridge, Dan Coroian, Sven Dickinson, Sam\n  Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan\n  Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei,\n  Yifan Yin, and Zhiqi Zhang", "title": "Large-Scale Automatic Labeling of Video Events with Verbs Based on\n  Event-Participant Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to labeling short video clips with English verbs as\nevent descriptions. A key distinguishing aspect of this work is that it labels\nvideos with verbs that describe the spatiotemporal interaction between event\nparticipants, humans and objects interacting with each other, abstracting away\nall object-class information and fine-grained image characteristics, and\nrelying solely on the coarse-grained motion of the event participants. We apply\nour approach to a large set of 22 distinct verb classes and a corpus of 2,584\nvideos, yielding two surprising outcomes. First, a classification accuracy of\ngreater than 70% on a 1-out-of-22 labeling task and greater than 85% on a\nvariety of 1-out-of-10 subsets of this labeling task is independent of the\nchoice of which of two different time-series classifiers we employ. Second, we\nachieve this level of accuracy using a highly impoverished intermediate\nrepresentation consisting solely of the bounding boxes of one or two event\nparticipants as a function of time. This indicates that successful event\nrecognition depends more on the choice of appropriate features that\ncharacterize the linguistic invariants of the event classes than on the\nparticular classifier algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 19:59:15 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Barbu", "Andrei", ""], ["Bridge", "Alexander", ""], ["Coroian", "Dan", ""], ["Dickinson", "Sven", ""], ["Mussman", "Sam", ""], ["Narayanaswamy", "Siddharth", ""], ["Salvi", "Dhaval", ""], ["Schmidt", "Lara", ""], ["Shangguan", "Jiangnan", ""], ["Siskind", "Jeffrey Mark", ""], ["Waggoner", "Jarrell", ""], ["Wang", "Song", ""], ["Wei", "Jinlian", ""], ["Yin", "Yifan", ""], ["Zhang", "Zhiqi", ""]]}, {"id": "1204.3618", "submitter": "Mohammad Tofighi", "authors": "Mohammad Tofighi, Ali Ayremlou, Farokh Marvasti", "title": "Compensating Interpolation Distortion by Using New Optimized Modular\n  Method", "comments": "7 pages. Journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modular method was suggested before to recover a band limited signal from\nthe sample and hold and linearly interpolated (or, in general, an\nnth-order-hold) version of the regular samples. In this paper a novel approach\nfor compensating the distortion of any interpolation based on modular method\nhas been proposed. In this method the performance of the modular method is\noptimized by adding only some simply calculated coefficients. This approach\ncauses drastic improvement in terms of signal-to-noise ratios with fewer\nmodules compared to the classical modular method. Simulation results clearly\nconfirm the improvement of the proposed method and also its superior robustness\nagainst additive noise.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 20:16:11 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Tofighi", "Mohammad", ""], ["Ayremlou", "Ali", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1204.3748", "submitter": "Klaus Frick", "authors": "Klaus Frick, Philipp Marnitz, Axel Munk", "title": "Statistical Multiresolution Estimation for Variational Imaging: With an\n  Application in Poisson-Biophotonics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a spatially-adaptive method for image reconstruction\nthat is based on the concept of statistical multiresolution estimation as\nintroduced in [Frick K, Marnitz P, and Munk A. \"Statistical multiresolution\nDantzig estimation in imaging: Fundamental concepts and algorithmic framework\".\nElectron. J. Stat., 6:231-268, 2012]. It constitutes a variational\nregularization technique that uses an supremum-type distance measure as\ndata-fidelity combined with a convex cost functional. The resulting convex\noptimization problem is approached by a combination of an inexact alternating\ndirection method of multipliers and Dykstra's projection algorithm. We describe\na novel method for balancing data-fit and regularity that is fully automatic\nand allows for a sound statistical interpretation. The performance of our\nestimation approach is studied for various problems in imaging. Among others,\nthis includes deconvolution problems that arise in Poisson nanoscale\nfluorescence microscopy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 09:54:32 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Frick", "Klaus", ""], ["Marnitz", "Philipp", ""], ["Munk", "Axel", ""]]}, {"id": "1204.3968", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Soumith Chintala, Yann LeCun", "title": "Convolutional Neural Networks Applied to House Numbers Digit\n  Classification", "comments": "4 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify digits of real-world house numbers using convolutional neural\nnetworks (ConvNets). ConvNets are hierarchical feature learning neural networks\nwhose structure is biologically inspired. Unlike many popular vision approaches\nthat are hand-designed, ConvNets can automatically learn a unique set of\nfeatures optimized for a given task. We augmented the traditional ConvNet\narchitecture by learning multi-stage features and by using Lp pooling and\nestablish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2%\nerror improvement). Furthermore, we analyze the benefits of different pooling\nmethods and multi-stage features in ConvNets. The source code and a tutorial\nare available at eblearn.sf.net.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 03:48:38 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Sermanet", "Pierre", ""], ["Chintala", "Soumith", ""], ["LeCun", "Yann", ""]]}, {"id": "1204.4257", "submitter": "Aamir Khan", "authors": "Aamir Khan, Muhammad Farhan, Asar Ali", "title": "Speech Recognition: Increasing Efficiency of Support Vector Machines", "comments": "5 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1201.3720 and arXiv:1204.1177", "journal-ref": "International Journal of Computer Applications 35(7):17-21,\n  December 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of communication and security technologies, it has\nbecome crucial to have robustness of embedded biometric systems. This paper\npresents the realization of such technologies which demands reliable and\nerror-free biometric identity verification systems. High dimensional patterns\nare not permitted due to eigen-decomposition in high dimensional feature space\nand degeneration of scattering matrices in small size sample. Generalization,\ndimensionality reduction and maximizing the margins are controlled by\nminimizing weight vectors. Results show good pattern by multimodal biometric\nsystem proposed in this paper. This paper is aimed at investigating a biometric\nidentity system using Support Vector Machines(SVMs) and Lindear Discriminant\nAnalysis(LDA) with MFCCs and implementing such system in real-time using\nSignalWAVE.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 06:10:02 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Khan", "Aamir", ""], ["Farhan", "Muhammad", ""], ["Ali", "Asar", ""]]}, {"id": "1204.4294", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and Klaus Obermayer", "title": "Learning in Riemannian Orbifolds", "comments": "arXiv admin note: substantial text overlap with arXiv:1001.0921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in Riemannian orbifolds is motivated by existing machine learning\nalgorithms that directly operate on finite combinatorial structures such as\npoint patterns, trees, and graphs. These methods, however, lack statistical\njustification. This contribution derives consistency results for learning\nproblems in structured domains and thereby generalizes learning in vector\nspaces and manifolds.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 09:29:10 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1204.4476", "submitter": "Rizwan Chaudhry", "authors": "Rizwan Chaudhry and Gregory Hager and Rene Vidal", "title": "Dynamic Template Tracking and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of tracking non-rigid objects whose\nlocal appearance and motion changes as a function of time. This class of\nobjects includes dynamic textures such as steam, fire, smoke, water, etc., as\nwell as articulated objects such as humans performing various actions. We model\nthe temporal evolution of the object's appearance/motion using a Linear\nDynamical System (LDS). We learn such models from sample videos and use them as\ndynamic templates for tracking objects in novel videos. We pose the problem of\ntracking a dynamic non-rigid object in the current frame as a maximum\na-posteriori estimate of the location of the object and the latent state of the\ndynamical system, given the current image features and the best estimate of the\nstate in the previous frame. The advantage of our approach is that we can\nspecify a-priori the type of texture to be tracked in the scene by using\npreviously trained models for the dynamics of these textures. Our framework\nnaturally generalizes common tracking methods such as SSD and kernel-based\ntracking from static templates to dynamic templates. We test our algorithm on\nsynthetic as well as real examples of dynamic textures and show that our simple\ndynamics-based trackers perform at par if not better than the state-of-the-art.\nSince our approach is general and applicable to any image feature, we also\napply it to the problem of human action tracking and build action-specific\noptical flow trackers that perform better than the state-of-the-art when\ntracking a human performing a particular action. Finally, since our approach is\ngenerative, we can use a-priori trained trackers for different texture or\naction classes to simultaneously track and recognize the texture or action in\nthe video.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 21:17:08 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Chaudhry", "Rizwan", ""], ["Hager", "Gregory", ""], ["Vidal", "Rene", ""]]}, {"id": "1204.4521", "submitter": "Ayan Acharya", "authors": "Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh", "title": "A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster\n  Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a privacy-aware Bayesian approach that combines\nensembles of classifiers and clusterers to perform semi-supervised and\ntransductive learning. We consider scenarios where instances and their\nclassification/clustering results are distributed across different data sites\nand have sharing restrictions. As a special case, the privacy aware computation\nof the model when instances of the target data are distributed across different\ndata sites, is also discussed. Experimental results show that the proposed\napproach can provide good classification accuracies while adhering to the\ndata/model sharing constraints.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 03:01:56 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Acharya", "Ayan", ""], ["Hruschka", "Eduardo R.", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1204.4758", "submitter": "Yongchao  Xu", "authors": "Yongchao Xu, Thierry G\\'eraud and Laurent Najman", "title": "Morphological Filtering in Shape Spaces: Applications using Tree-Based\n  Image Representations", "comments": "4 pages, will appear in 21st International Conference on Pattern\n  Recognition (ICPR 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected operators are filtering tools that act by merging elementary\nregions of an image. A popular strategy is based on tree-based image\nrepresentations: for example, one can compute an attribute on each node of the\ntree and keep only the nodes for which the attribute is sufficiently strong.\nThis operation can be seen as a thresholding of the tree, seen as a graph whose\nnodes are weighted by the attribute. Rather than being satisfied with a mere\nthresholding, we propose to expand on this idea, and to apply connected filters\non this latest graph. Consequently, the filtering is done not in the space of\nthe image, but on the space of shapes build from the image. Such a processing\nis a generalization of the existing tree-based connected operators. Indeed, the\nframework includes classical existing connected operators by attributes. It\nalso allows us to propose a class of novel connected operators from the\nleveling family, based on shape attributes. Finally, we also propose a novel\nclass of self-dual connected operators that we call morphological shapings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 22:23:11 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2012 14:13:22 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Xu", "Yongchao", ""], ["G\u00e9raud", "Thierry", ""], ["Najman", "Laurent", ""]]}, {"id": "1204.4867", "submitter": "Shai Bagon", "authors": "Shai Bagon and Meirav Galun", "title": "A Unified Multiscale Framework for Discrete Energy Minimization", "comments": "11 pages, 8 figures, 6 tables, submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete energy minimization is a ubiquitous task in computer vision, yet is\nNP-hard in most cases. In this work we propose a multiscale framework for\ncoping with the NP-hardness of discrete optimization. Our approach utilizes\nalgebraic multiscale principles to efficiently explore the discrete solution\nspace, yielding improved results on challenging, non-submodular energies for\nwhich current methods provide unsatisfactory approximations. In contrast to\npopular multiscale methods in computer vision, that builds an image pyramid,\nour framework acts directly on the energy to construct an energy pyramid.\nDeriving a multiscale scheme from the energy itself makes our framework\napplication independent and widely applicable. Our framework gives rise to two\ncomplementary energy coarsening strategies: one in which coarser scales involve\nfewer variables, and a more revolutionary one in which the coarser scales\ninvolve fewer discrete labels. We empirically evaluated our unified framework\non a variety of both non-submodular and submodular energies, including energies\nfrom Middlebury benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2012 07:43:46 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Bagon", "Shai", ""], ["Galun", "Meirav", ""]]}, {"id": "1204.5309", "submitter": "Simon Hawe", "authors": "Simon Hawe, Martin Kleinsteuber, and Klaus Diepold", "title": "Analysis Operator Learning and Its Application to Image Reconstruction", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TIP.2013.2246175", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting a priori known structural information lies at the core of many\nimage reconstruction methods that can be stated as inverse problems. The\nsynthesis model, which assumes that images can be decomposed into a linear\ncombination of very few atoms of some dictionary, is now a well established\ntool for the design of image reconstruction algorithms. An interesting\nalternative is the analysis model, where the signal is multiplied by an\nanalysis operator and the outcome is assumed to be the sparse. This approach\nhas only recently gained increasing interest. The quality of reconstruction\nmethods based on an analysis model severely depends on the right choice of the\nsuitable operator.\n  In this work, we present an algorithm for learning an analysis operator from\ntraining images. Our method is based on an $\\ell_p$-norm minimization on the\nset of full rank matrices with normalized columns. We carefully introduce the\nemployed conjugate gradient method on manifolds, and explain the underlying\ngeometry of the constraints. Moreover, we compare our approach to\nstate-of-the-art methods for image denoising, inpainting, and single image\nsuper-resolution. Our numerical results show competitive performance of our\ngeneral approach in all presented applications compared to the specialized\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 08:56:42 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 19:34:35 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 11:51:49 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hawe", "Simon", ""], ["Kleinsteuber", "Martin", ""], ["Diepold", "Klaus", ""]]}, {"id": "1204.5416", "submitter": "Pradeep  Singla", "authors": "Manoj Kumar, Vikas Kaushik, Pradeep Singla", "title": "A New Approach of Improving CFA Image for Digital Camera's", "comments": "4 Pages, 6 Figures", "journal-ref": "Proceedings of ETEIC-2012:pp 256-259, April-2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper work directly towards the improving the quality of the image for\nthe digital cameras and other visual capturing products. In this Paper, the\nauthors clearly defines the problems occurs in the CFA image. A different\nmethodology for removing the noise is discuses in the paper for color\ncorrection and color balancing of the image. At the same time, the authors also\nproposed a new methodology of providing denoisiing process before the\ndemosaickingfor the improving the image quality of CFA which is much efficient\nthen the other previous defined. The demosaicking process for producing the\ncolors in the image in a best way is also discuss.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 15:45:37 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Kumar", "Manoj", ""], ["Kaushik", "Vikas", ""], ["Singla", "Pradeep", ""]]}, {"id": "1204.5431", "submitter": "Mohammad Tofighi", "authors": "Mohammad Tofighi and Hashem Kalbkhani and Mahrokh G. Shayesteh and\n  Mehdi Ghasemzadeh", "title": "Robust Head Pose Estimation Using Contourlet Transform", "comments": "5 pages, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating pose of the head is an important preprocessing step in many\npattern recognition and computer vision systems such as face recognition. Since\nthe performance of the face recognition systems is greatly affected by the\nposes of the face, how to estimate the accurate pose of the face in human face\nimage is still a challenging problem. In this paper, we represent a novel\nmethod for head pose estimation. To enhance the efficiency of the estimation we\nuse contourlet transform for feature extraction. Contourlet transform is\nmulti-resolution, multi-direction transform. In order to reduce the feature\nspace dimension and obtain appropriate features we use LDA (Linear Discriminant\nAnalysis) and PCA (Principal Component Analysis) to remove ineffcient features.\nThen, we apply different classifiers such as k-nearest neighborhood (knn) and\nminimum distance. We use the public available FERET database to evaluate the\nperformance of proposed method. Simulation results indicate the superior\nrobustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 17:08:04 GMT"}, {"version": "v2", "created": "Sat, 12 May 2012 13:56:32 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Tofighi", "Mohammad", ""], ["Kalbkhani", "Hashem", ""], ["Shayesteh", "Mahrokh G.", ""], ["Ghasemzadeh", "Mehdi", ""]]}, {"id": "1204.6326", "submitter": "Jean-Philippe Jodoin", "authors": "Jean-Philippe Jodoin and Guillaume-Alexandre Bilodeau and Nicolas\n  Saunier", "title": "Background subtraction based on Local Shape", "comments": "4 pages, 5 figures, 3 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to background subtraction that is based on the\nlocal shape of small image regions. In our approach, an image region centered\non a pixel is mod-eled using the local self-similarity descriptor. We aim at\nobtaining a reliable change detection based on local shape change in an image\nwhen foreground objects are moving. The method first builds a background model\nand compares the local self-similarities between the background model and the\nsubsequent frames to distinguish background and foreground objects.\nPost-processing is then used to refine the boundaries of moving objects.\nResults show that this approach is promising as the foregrounds obtained are\ncom-plete, although they often include shadows.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 20:26:34 GMT"}, {"version": "v2", "created": "Thu, 17 May 2012 13:14:40 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Jodoin", "Jean-Philippe", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "1204.6385", "submitter": "Yankui Sun", "authors": "Yankui Sun, Tian Zhang", "title": "A 3D Segmentation Method for Retinal Optical Coherence Tomography Volume\n  Data", "comments": "4 pages, 9 figures", "journal-ref": "China Patent Application (201110247341.5), 2011", "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of spectral-domain optical coherence tomography (OCT),\nmuch larger image datasets are routinely acquired compared to what was possible\nusing the previous generation of time-domain OCT. Thus, the need for 3-D\nsegmentation methods for processing such data is becoming increasingly\nimportant. We present a new 3D segmentation method for retinal OCT volume data,\nwhich generates an enhanced volume data by using pixel intensity, boundary\nposition information, intensity changes on both sides of the border\nsimultaneously, and preliminary discrete boundary points are found from all\nA-Scans and then the smoothed boundary surface can be obtained after removing a\nsmall quantity of error points. Our experiments show that this method is\nefficient, accurate and robust.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 09:05:56 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Sun", "Yankui", ""], ["Zhang", "Tian", ""]]}, {"id": "1204.6458", "submitter": "Junyan Wang", "authors": "Junyan Wang and Kap Luk Chan", "title": "Active Contour with A Tangential Component", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Conventional edge-based active contours often require the normal component of\nan edge indicator function on the optimal contours to approximate zero, while\nthe tangential component can still be significant. In real images, the full\ngradients of the edge indicator function along the object boundaries are often\nsmall. Hence, the curve evolution of edge-based active contours can terminate\nearly before converging to the object boundaries with a careless contour\ninitialization. We propose a novel Geodesic Snakes (GeoSnakes) active contour\nthat requires the full gradients of the edge indicator to vanish at the optimal\nsolution. Besides, the conventional curve evolution approach for minimizing\nactive contour energy cannot fully solve the Euler-Lagrange (EL) equation of\nour GeoSnakes active contour, causing a Pseudo Stationary Phenomenon (PSP). To\naddress the PSP problem, we propose an auxiliary curve evolution equation,\nnamed the equilibrium flow (EF) equation. Based on the EF and the conventional\ncurve evolution, we obtain a solution to the full EL equation of GeoSnakes\nactive contour. Experimental results validate the proposed geometrical\ninterpretation of the early termination problem, and they also show that the\nproposed method overcomes the problem.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 07:17:28 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Wang", "Junyan", ""], ["Chan", "Kap Luk", ""]]}, {"id": "1204.6563", "submitter": "Prabhu Kaliamoorthi Mr", "authors": "Prabhu Kaliamoorthi and Ramakrishna Kakarala", "title": "Parametric annealing: a stochastic search method for human pose tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model based methods to marker-free motion capture have a very high\ncomputational overhead that make them unattractive. In this paper we describe a\nmethod that improves on existing global optimization techniques to tracking\narticulated objects. Our method improves on the state-of-the-art Annealed\nParticle Filter (APF) by reusing samples across annealing layers and by using\nan adaptive parametric density for diffusion. We compare the proposed method\nwith APF on a scalable problem and study how the two methods scale with the\ndimensionality, multi-modality and the range of search. Then we perform\nsensitivity analysis on the parameters of our algorithm and show that it\ntolerates a wide range of parameter settings. We also show results on tracking\nhuman pose from the widely-used Human Eva I dataset. Our results show that the\nproposed method reduces the tracking error despite using less than 50% of the\ncomputational resources as APF. The tracked output also shows a significant\nqualitative improvement over APF as demonstrated through image and video\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 07:04:08 GMT"}, {"version": "v2", "created": "Wed, 2 May 2012 04:37:03 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Kaliamoorthi", "Prabhu", ""], ["Kakarala", "Ramakrishna", ""]]}, {"id": "1204.6653", "submitter": "Vini Katyal", "authors": "Vini Katyal, Aviral, Deepesh Srivastava", "title": "Elimination of Glass Artifacts and Object Segmentation", "comments": null, "journal-ref": null, "doi": "10.5120/6215-8919", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many images nowadays are captured from behind the glasses and may have\ncertain stains discrepancy because of glass and must be processed to make\ndifferentiation between the glass and objects behind it. This research paper\nproposes an algorithm to remove the damaged or corrupted part of the image and\nmake it consistent with other part of the image and to segment objects behind\nthe glass. The damaged part is removed using total variation inpainting method\nand segmentation is done using kmeans clustering, anisotropic diffusion and\nwatershed transformation. The final output is obtained by interpolation. This\nalgorithm can be useful to applications in which some part of the images are\ncorrupted due to data transmission or needs to segment objects from an image\nfor further processing.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 14:47:45 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Katyal", "Vini", ""], ["Aviral", "", ""], ["Srivastava", "Deepesh", ""]]}, {"id": "1204.6725", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov and Yankui Sun", "title": "OCT Segmentation Survey and Summary Reviews and a Novel 3D Segmentation\n  Algorithm and a Proof of Concept Implementation", "comments": "51 pages, 22 figures, TOC, index, code excerpts; v2 refreshes\n  references and fixes some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview the existing OCT work, especially the practical aspects of it. We\ncreate a novel algorithm for 3D OCT segmentation with the goals of speed and/or\naccuracy while remaining flexible in the design and implementation for future\nextensions and improvements. The document at this point is a running draft\nbeing iteratively \"developed\" as a progress report as the work and survey\nadvance. It contains the review and summarization of select OCT works, the\ndesign and implementation of the OCTMARF experimentation application and some\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 18:48:25 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2012 02:12:22 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Mokhov", "Serguei A.", ""], ["Sun", "Yankui", ""]]}]