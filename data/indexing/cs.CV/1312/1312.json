[{"id": "1312.0072", "submitter": "Ngoc Son Vu", "authors": "Ngoc-Son Vu, Thanh Phuong Nguyen, Christophe Garcia", "title": "Improving Texture Categorization with Biologically Inspired Filtering", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the domain of texture classification, a lot of effort has been spent\non local descriptors, leading to many powerful algorithms. However,\npreprocessing techniques have received much less attention despite their\nimportant potential for improving the overall classification performance. We\naddress this question by proposing a novel, simple, yet very powerful\nbiologically-inspired filtering (BF) which simulates the performance of human\nretina. In the proposed approach, given a texture image, after applying a DoG\nfilter to detect the \"edges\", we first split the filtered image into two \"maps\"\nalongside the sides of its edges. The feature extraction step is then carried\nout on the two \"maps\" instead of the input image. Our algorithm has several\nadvantages such as simplicity, robustness to illumination and noise, and\ndiscriminative power. Experimental results on three large texture databases\nshow that with an extremely low computational cost, the proposed method\nimproves significantly the performance of many texture classification systems,\nnotably in noisy environments. The source codes of the proposed algorithm can\nbe downloaded from https://sites.google.com/site/nsonvu/code.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 07:09:17 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Vu", "Ngoc-Son", ""], ["Nguyen", "Thanh Phuong", ""], ["Garcia", "Christophe", ""]]}, {"id": "1312.0760", "submitter": "Adithya Kumar Pediredla Mr.", "authors": "Jayanth Krishna Mogali, Adithya Kumar Pediredla, and Chandra Sekhar\n  Seelamantula", "title": "Template-Based Active Contours", "comments": "Active Contours, Snakes, Affine matching, Contrast function, Shape\n  constraint, Image segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generalized active contour formalism for image segmentation\nbased on shape templates. The shape template is subjected to a restricted\naffine transformation (RAT) in order to segment the object of interest. RAT\nallows for translation, rotation, and scaling, which give a total of five\ndegrees of freedom. The proposed active contour comprises an inner and outer\ncontour pair, which are closed and concentric. The active contour energy is a\ncontrast function defined based on the intensities of pixels that lie inside\nthe inner contour and those that lie in the annulus between the inner and outer\ncontours. We show that the contrast energy functional is optimal under certain\nconditions. The optimal RAT parameters are computed by maximizing the contrast\nfunction using a gradient descent optimizer. We show that the calculations are\nmade efficient through use of Green's theorem. The proposed formalism is\ncapable of handling a variety of shapes because for a chosen template,\noptimization is carried with respect to the RAT parameters only. The proposed\nformalism is validated on multiple images to show robustness to Gaussian and\nPoisson noise, to initialization, and to partial loss of structure in the\nobject to be segmented.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 10:27:59 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Mogali", "Jayanth Krishna", ""], ["Pediredla", "Adithya Kumar", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1312.0788", "submitter": "Guillermo Gallego Bonet", "authors": "Guillermo Gallego and Anthony Yezzi", "title": "A compact formula for the derivative of a 3-D rotation in exponential\n  coordinates", "comments": "6 pages", "journal-ref": "Journal of Mathematical Imaging and Vision, vol. 51, no. 3, pp\n  378-384, Mar. 2015", "doi": "10.1007/s10851-014-0528-x", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a compact formula for the derivative of a 3-D rotation matrix with\nrespect to its exponential coordinates. A geometric interpretation of the\nresulting expression is provided, as well as its agreement with other\nless-compact but better-known formulas. To the best of our knowledge, this\nsimpler formula does not appear anywhere in the literature. We hope by\nproviding this more compact expression to alleviate the common pressure to\nreluctantly resort to alternative representations in various computational\napplications simply as a means to avoid the complexity of differential analysis\nin exponential coordinates.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 12:09:01 GMT"}, {"version": "v2", "created": "Fri, 8 Aug 2014 08:00:26 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Gallego", "Guillermo", ""], ["Yezzi", "Anthony", ""]]}, {"id": "1312.0809", "submitter": "Suranjan Ganguly", "authors": "Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri and Dipak Kumar\n  Basu", "title": "Automatic White Blood Cell Measuring Aid for Medical Diagnosis", "comments": "6 pages, International Conference", "journal-ref": "International Conference on Process Automation, Control and\n  Computing- PACC 2011 ISBN: 978-1-61284-762-7", "doi": "10.1109/PACC.2011.5978895", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Blood related invasive pathological investigations play a major role in\ndiagnosis of diseases. But in India and other third world countries there are\nno enough pathological infrastructures for medical diagnosis. Moreover, most of\nthe remote places of those countries have neither pathologists nor physicians.\nTelemedicine partially solves the lack of physicians. But the pathological\ninvestigation infrastructure can not be integrated with the telemedicine\ntechnology. The objective of this work is to automate the blood related\npathological investigation process. Detection of different white blood cells\nhas been automated in this work. This system can be deployed in the remote area\nas a supporting aid for telemedicine technology and only high school education\nis sufficient to operate it. The proposed system achieved 97.33 percent\naccuracy for the samples collected to test this system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 13:06:30 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Ghosh", "Pramit", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1312.0852", "submitter": "Saptarshi Bhattacharjee", "authors": "Samir Kumar Bandyopadhyay, S Arunkumar, Saptarshi Bhattacharjee", "title": "Feature Extraction of Human Lip Prints", "comments": "8 pages, 18 figures", "journal-ref": "Journal of Current Computer Science and Technology Vol. 2 Issue 1\n  [2012] 01-08", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods have been used for identification of human by recognizing lip prints.\nHuman lips have a number of elevation and depressions features called lip\nprints and examination of lip prints is referred to as cheiloscopy. Lip prints\nof each human being are unique in nature like many others features of human. In\nthis paper lip print is first smoothened using a Gaussian Filter. Next Sobel\nEdge Detector and Canny Edge Detector are used to detect the vertical and\nhorizontal groove pattern in the lip. This method of identification will be\nuseful both in criminal forensics and personal identification. It is our\nassumption that study of lip prints and their types are well connected to play\na song in a better way that are well accepted to people who loves to hear\nsongs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 15:25:07 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Bandyopadhyay", "Samir Kumar", ""], ["Arunkumar", "S", ""], ["Bhattacharjee", "Saptarshi", ""]]}, {"id": "1312.0940", "submitter": "Suranjan Ganguly", "authors": "Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri and Dipak Kumar\n  Basu", "title": "Medical Aid for Automatic Detection of Malaria", "comments": "8 pages, International Conference on Computer Information Systems and\n  Industrial Management Applications 2011. arXiv admin note: text overlap with\n  arXiv:1312.0809", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The analysis and counting of blood cells in a microscope image can provide\nuseful information concerning to the health of a person. In particular,\nmorphological analysis of red blood cells deformations can effectively detect\nimportant disease like malaria. Blood images, obtained by the microscope, which\nis coupled with a digital camera, are analyzed by the computer for diagnosis or\ncan be transmitted easily to clinical centers than liquid blood samples.\nAutomatic analysis system for the presence of Plasmodium in microscopic image\nof blood can greatly help pathologists and doctors that typically inspect blood\nfilms manually. Unfortunately, the analysis made by human experts is not rapid\nand not yet standardized due to the operators capabilities and tiredness. The\npaper shows how effectively and accurately it is possible to identify the\nPlasmodium in the blood film. In particular, the paper presents how to enhance\nthe microscopic image and filter out the unnecessary segments followed by the\nthreshold based segmentation and recognize the presence of Plasmodium. The\nproposed system can be deployed in the remote area as a supporting aid for\ntelemedicine technology and only basic training is sufficient to operate it.\nThis system achieved more than 98 percentage accuracy for the samples collected\nto test this system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 13:15:57 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Ghosh", "Pramit", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1312.1461", "submitter": "Suranjan Ganguly", "authors": "Sourav Pramanik and Debotosh Bhattacharjee", "title": "Multi-Sensor Image Fusion Based on Moment Calculation", "comments": "5 pages, International Conference", "journal-ref": "IEEE International Conference on Parallel, Distributed and Grid\n  Computing-PDGC, 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  An image fusion method based on salient features is proposed in this paper.\nIn this work, we have concentrated on salient features of the image for fusion\nin order to preserve all relevant information contained in the input images and\ntried to enhance the contrast in fused image and also suppressed noise to a\nmaximum extent. In our system, first we have applied a mask on two input images\nin order to conserve the high frequency information along with some low\nfrequency information and stifle noise to a maximum extent. Thereafter, for\nidentification of salience features from sources images, a local moment is\ncomputed in the neighborhood of a coefficient. Finally, a decision map is\ngenerated based on local moment in order to get the fused image. To verify our\nproposed algorithm, we have tested it on 120 sensor image pairs collected from\nManchester University UK database. The experimental results show that the\nproposed method can provide superior fused image in terms of several\nquantitative fusion evaluation index.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 07:56:50 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Pramanik", "Sourav", ""], ["Bhattacharjee", "Debotosh", ""]]}, {"id": "1312.1462", "submitter": "Suranjan Ganguly", "authors": "Sourav Pramanik and Debotosh Bhattacharjee", "title": "Geometric Feature Based Face-Sketch Recognition", "comments": "7 pages, International Conference", "journal-ref": "IEEE International Conf on Pattern Recognition, Informatics and\n  Medical Engineering,PRIME-2012,March 21-23", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper presents a novel facial sketch image or face-sketch recognition\napproach based on facial feature extraction. To recognize a face-sketch, we\nhave concentrated on a set of geometric face features like eyes, nose,\neyebrows, lips, etc and their length and width ratio because it is difficult to\nmatch photos and sketches because they belong to two different modalities. In\nthis system, first the facial features/components from training images are\nextracted, then ratios of length, width, and area etc. are calculated and those\nare stored as feature vectors for individual images. After that the mean\nfeature vectors are computed and subtracted from each feature vector for\ncentering of the feature vectors. In the next phase, feature vector for the\nincoming probe face-sketch is also computed in similar fashion. Here, K-NN\nclassifier is used to recognize probe face-sketch. It is experimentally\nverified that the proposed method is robust against faces are in a frontal\npose, with normal lighting and neutral expression and have no occlusions. The\nexperiment has been conducted with 80 male and female face images from\ndifferent face databases. It has useful applications for both law enforcement\nand digital entertainment.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 08:02:08 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Pramanik", "Sourav", ""], ["Bhattacharjee", "Debotosh", ""]]}, {"id": "1312.1492", "submitter": "Vitaliy Kurlin", "authors": "Vitaliy Kurlin", "title": "A fast and robust algorithm to count topologically persistent holes in\n  noisy clouds", "comments": "Full version of the paper that has appeared in Proceedings of IEEE\n  conference CVPR 2014: Computer Vision and Pattern Recognition, Columbus,\n  Ohio, USA (10 pages, 20 figures, 3 appendices, more examples will be at\n  http://kurlin.org)", "journal-ref": null, "doi": "10.1109/CVPR.2014.189", "report-no": null, "categories": "cs.CG cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preprocessing a 2D image often produces a noisy cloud of interest points. We\nstudy the problem of counting holes in unorganized clouds in the plane. The\nholes in a given cloud are quantified by the topological persistence of their\nboundary contours when the cloud is analyzed at all possible scales. We design\nthe algorithm to count holes that are most persistent in the filtration of\noffsets (neighborhoods) around given points. The input is a cloud of $n$ points\nin the plane without any user-defined parameters. The algorithm has $O(n\\log\nn)$ time and $O(n)$ space. The output is the array (number of holes, relative\npersistence in the filtration). We prove theoretical guarantees when the\nalgorithm finds the correct number of holes (components in the complement) of\nan unknown shape approximated by a cloud.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 10:24:59 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 15:40:29 GMT"}, {"version": "v3", "created": "Sat, 19 Jul 2014 20:38:54 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kurlin", "Vitaliy", ""]]}, {"id": "1312.1494", "submitter": "Vitaliy Kurlin", "authors": "Vitaliy Kurlin", "title": "Approximating persistent homology for a cloud of $n$ points in a\n  subquadratic time", "comments": "The paper turned out to closely follow a previously known approach\n  that the author didn't know at the time of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vietoris-Rips filtration for an $n$-point metric space is a sequence of\nlarge simplicial complexes adding a topological structure to the otherwise\ndisconnected space. The persistent homology is a key tool in topological data\nanalysis and studies topological features of data that persist over many\nscales. The fastest algorithm for computing persistent homology of a filtration\nhas time $O(M(u)+u^2\\log^2 u)$, where $u$ is the number of updates (additions\nor deletions of simplices), $M(u)=O(u^{2.376})$ is the time for multiplication\nof $u\\times u$ matrices. For a space of $n$ points given by their pairwise\ndistances, we approximate the Vietoris-Rips filtration by a zigzag filtration\nconsisting of $u=o(n)$ updates, which is sublinear in $n$. The constant depends\non a given error of approximation and on the doubling dimension of the metric\nspace. Then the persistent homology of this sublinear-size filtration can be\ncomputed in time $o(n^2)$, which is subquadratic in $n$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 10:39:40 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 10:03:46 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kurlin", "Vitaliy", ""]]}, {"id": "1312.1512", "submitter": "Suranjan Ganguly", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "An adaptive block based integrated LDP,GLCM,and Morphological features\n  for Face Recognition", "comments": "7 pages, Science Academy Publisher, United Kingdom", "journal-ref": "International Journal of Research and Reviews in Computer\n  Science-IJRRCS,2011 ISSN: 2079-2557, Vol. 2, No. 5, October 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a technique for automatic face recognition using\nintegrated multiple feature sets extracted from the significant blocks of a\ngradient image. We discuss about the use of novel morphological, local\ndirectional pattern (LDP) and gray-level co-occurrence matrix GLCM based\nfeature extraction technique to recognize human faces. Firstly, the new\nmorphological features i.e., features based on number of runs of pixels in four\ndirections (N,NE,E,NW) are extracted, together with the GLCM based statistical\nfeatures and LDP features that are less sensitive to the noise and\nnon-monotonic illumination changes, are extracted from the significant blocks\nof the gradient image. Then these features are concatenated together. We\nintegrate the above mentioned methods to take full advantage of the three\napproaches. Extraction of the significant blocks from the absolute gradient\nimage and hence from the original image to extract pertinent information with\nthe idea of dimension reduction forms the basis of the work. The efficiency of\nour method is demonstrated by the experiment on 1100 images from the FRAV2D\nface database, 2200 images from the FERET database, where the images vary in\npose, expression, illumination and scale and 400 images from the ORL face\ndatabase, where the images slightly vary in pose. Our method has shown 90.3%,\n93% and 98.75% recognition accuracy for the FRAV2D, FERET and the ORL database\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 11:59:31 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1312.1517", "submitter": "Suranjan Ganguly", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "A Gabor block based Kernel Discriminative Common Vector (KDCV) approach\n  using cosine kernels for Human Face Recognition", "comments": "9 pages,Hindawi Publishing Corporation, Received 14 March 2012;\n  Revised 16 July 2012; Accepted 13 August 2012. International Journal of\n  Computational Intelligence and Neuroscience,2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper a nonlinear Gabor Wavelet Transform (GWT) discriminant feature\nextraction approach for enhanced face recognition is proposed. Firstly, the\nlow-energized blocks from Gabor wavelet transformed images are extracted.\nSecondly, the nonlinear discriminating features are analyzed and extracted from\nthe selected low-energized blocks by the generalized Kernel Discriminative\nCommon Vector (KDCV) method. The KDCV method is extended to include cosine\nkernel function in the discriminating method. The KDCV with the cosine kernels\nis then applied on the extracted low energized discriminating feature vectors\nto obtain the real component of a complex quantity for face recognition. In\norder to derive positive kernel discriminative vectors; we apply only those\nkernel discriminative eigenvectors that are associated with non-zero\neigenvalues. The feasibility of the low energized Gabor block based generalized\nKDCV method with cosine kernel function models has been successfully tested for\nimage classification using the L1, L2 distance measures; and the cosine\nsimilarity measure on both frontal and pose-angled face recognition.\nExperimental results on the FRAV2D and the FERET database demonstrate the\neffectiveness of this new approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 12:20:10 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1312.1520", "submitter": "Suranjan Ganguly", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "A Face Recognition approach based on entropy estimate of the nonlinear\n  DCT features in the Logarithm Domain together with Kernel Entropy Component\n  Analysis", "comments": "9 pages,Published Online August 2013 in MECS. International Journal\n  of Information Technology and Computer Science, 2013. arXiv admin note: text\n  overlap with arXiv:1112.3712 by other authors", "journal-ref": null, "doi": "10.5815/ijitcs.2013.09.03", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exploits the feature extraction capabilities of the discrete\ncosine transform (DCT) together with an illumination normalization approach in\nthe logarithm domain that increase its robustness to variations in facial\ngeometry and illumination. Secondly in the same domain the entropy measures are\napplied on the DCT coefficients so that maximum entropy preserving pixels can\nbe extracted as the feature vector. Thus the informative features of a face can\nbe extracted in a low dimensional space. Finally, the kernel entropy component\nanalysis (KECA) with an extension of arc cosine kernels is applied on the\nextracted DCT coefficients that contribute most to the entropy estimate to\nobtain only those real kernel ECA eigenvectors that are associated with\neigenvalues having high positive entropy contribution. The resulting system was\nsuccessfully tested on real image sequences and is robust to significant\npartial occlusion and illumination changes, validated with the experiments on\nthe FERET, AR, FRAV2D and ORL face databases. Experimental comparison is\ndemonstrated to prove the superiority of the proposed approach in respect to\nrecognition accuracy. Using specificity and sensitivity we find that the best\nis achieved when Renyi entropy is applied on the DCT coefficients. Extensive\nexperimental comparison is demonstrated to prove the superiority of the\nproposed approach in respect to recognition accuracy. Moreover, the proposed\napproach is very simple, computationally fast and can be implemented in any\nreal-time face recognition system.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 12:32:36 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1312.1681", "submitter": "Suranjan Ganguly", "authors": "Sourav Pramanik, Dr. Debotosh Bhattacharjee", "title": "An Approach: Modality Reduction and Face-Sketch Recognition", "comments": "7 pages. arXiv admin note: substantial text overlap with\n  arXiv:1312.1462", "journal-ref": "International Journal of Computational Intelligence and\n  Informatics, Vol.1: No. 2, July-September 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  To recognize face sketch through face photo database is a challenging task\nfor todays researchers. Because face photo images in training set and face\nsketch images in testing set have different modality. Difference between two\nface photos of difference person is smaller than the difference between same\nperson in a face photo and face sketched. In this paper, for reduction of the\nmodality between face photo and face sketch we first bring face photo and face\nsketch images in a new dimension using 2D Discrete Haar wavelet transform with\nscale 3 followed by a negative approach. After that, extract features from\ntransformed images using Principal Component Analysis (PCA). Thereafter, we use\nSVM classifier and K-NN classifier for better classification. Our proposed\nmethod is experimentally verified by its robustness against faces that are\ncaptured in a good lighting condition and in a frontal pose. The experiment has\nbeen conducted with 100 male and female face images as training set and 100\nmale and female face sketch images as testing set collected from CUHK training\nand testing cropped photos and CUHK training and testing cropped sketches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 06:43:59 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Pramanik", "Sourav", ""], ["Bhattacharjee", "Dr. Debotosh", ""]]}, {"id": "1312.1683", "submitter": "Suranjan Ganguly", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "Face Recognition using Hough Peaks extracted from the significant blocks\n  of the Gradient Image", "comments": "6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1312.1512", "journal-ref": "International Journal of Advanced Research in Computer Science and\n  Software Engineering, ISSN: 2277 128X, Volume 2, Issue 1, January 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a new technique for automatic face recognition using\nintegrated peaks of the Hough transformed significant blocks of the binary\ngradient image. In this approach firstly the gradient of an image is calculated\nand a threshold is set to obtain a binary gradient image, which is less\nsensitive to noise and illumination changes. Secondly, significant blocks are\nextracted from the absolute gradient image, to extract pertinent information\nwith the idea of dimension reduction. Finally the best fitted Hough peaks are\nextracted from the Hough transformed significant blocks for efficient face\nrecognition. Then these Hough peaks are concatenated together, which are used\nas feature in classification process. The efficiency of the proposed method is\ndemonstrated by the experiment on 1100 images from the FRAV2D face database,\n2200 images from the FERET database, where the images vary in pose, expression,\nillumination and scale and 400 images from the ORL face database, where the\nimages slightly vary in pose. Our method has shown 93.3%, 88.5% and 99%\nrecognition accuracy for the FRAV2D, FERET and the ORL database respectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 12:09:48 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1312.1684", "submitter": "Suranjan Ganguly", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "High Performance Human Face Recognition using Gabor based Pseudo Hidden\n  Markov Model", "comments": "9 pages. arXiv admin note: substantial text overlap with\n  arXiv:1312.1517", "journal-ref": "International Journal of Applied Evolutionary Computation, 2013\n  4(1), 81-102, January-March 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper introduces a novel methodology that combines the multi-resolution\nfeature of the Gabor wavelet transformation (GWT) with the local interactions\nof the facial structures expressed through the Pseudo Hidden Markov model\n(PHMM). Unlike the traditional zigzag scanning method for feature extraction a\ncontinuous scanning method from top-left corner to right then top-down and\nright to left and so on until right-bottom of the image i.e. a spiral scanning\ntechnique has been proposed for better feature selection. Unlike traditional\nHMMs, the proposed PHMM does not perform the state conditional independence of\nthe visible observation sequence assumption. This is achieved via the concept\nof local structures introduced by the PHMM used to extract facial bands and\nautomatically select the most informative features of a face image. Thus, the\nlong-range dependency problem inherent to traditional HMMs has been drastically\nreduced. Again with the use of most informative pixels rather than the whole\nimage makes the proposed method reasonably faster for face recognition. This\nmethod has been successfully tested on frontal face images from the ORL, FRAV2D\nand FERET face databases where the images vary in pose, illumination,\nexpression, and scale. The FERET data set contains 2200 frontal face images of\n200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjects\nand the full ORL database is considered. The results reported in this\napplication are far better than the recent and most referred systems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 12:27:06 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1312.1685", "submitter": "Suranjan Ganguly", "authors": "Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri,\n  Mahantapas Kundu", "title": "Human Face Recognition using Gabor based Kernel Entropy Component\n  Analysis", "comments": "October, 2012. International Journal of Computer Vision and Image\n  Processing : IGI Global(USA), 2012. arXiv admin note: substantial text\n  overlap with arXiv:1312.1517, arXiv:1312.1520", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we present a novel Gabor wavelet based Kernel Entropy\nComponent Analysis (KECA) method by integrating the Gabor wavelet\ntransformation (GWT) of facial images with the KECA method for enhanced face\nrecognition performance. Firstly, from the Gabor wavelet transformed images the\nmost important discriminative desirable facial features characterized by\nspatial frequency, spatial locality and orientation selectivity to cope with\nthe variations due to illumination and facial expression changes were derived.\nAfter that KECA, relating to the Renyi entropy is extended to include cosine\nkernel function. The KECA with the cosine kernels is then applied on the\nextracted most important discriminating feature vectors of facial images to\nobtain only those real kernel ECA eigenvectors that are associated with\neigenvalues having positive entropy contribution. Finally, these real KECA\nfeatures are used for image classification using the L1, L2 distance measures;\nthe Mahalanobis distance measure and the cosine similarity measure. The\nfeasibility of the Gabor based KECA method with the cosine kernel has been\nsuccessfully tested on both frontal and pose-angled face recognition, using\ndatasets from the ORL, FRAV2D and the FERET database.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 12:36:11 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Kar", "Arindam", ""], ["Bhattacharjee", "Debotosh", ""], ["Basu", "Dipak Kumar", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1312.1725", "submitter": "Vitaliy Kurlin", "authors": "Vitaliy Kurlin", "title": "Book embeddings of Reeb graphs", "comments": "12 pages, 5 figures, more examples will be at http://kurlin.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a simplicial complex with a piecewise linear function\n$f:X\\to\\mathbb{R}$. The Reeb graph $Reeb(f,X)$ is the quotient of $X$, where we\ncollapse each connected component of $f^{-1}(t)$ to a single point. Let the\nnodes of $Reeb(f,X)$ be all homologically critical points where any homology of\nthe corresponding component of the level set $f^{-1}(t)$ changes. Then we can\nlabel every arc of $Reeb(f,X)$ with the Betti numbers\n$(\\beta_1,\\beta_2,\\dots,\\beta_d)$ of the corresponding $d$-dimensional\ncomponent of a level set. The homology labels give more information about the\noriginal complex $X$ than the classical Reeb graph. We describe a canonical\nembedding of a Reeb graph into a multi-page book (a star cross a line) and give\na unique linear code of this book embedding.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 22:47:42 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Kurlin", "Vitaliy", ""]]}, {"id": "1312.1743", "submitter": "Deva Ramanan", "authors": "Deva Ramanan", "title": "Dual coordinate solvers for large-scale structural SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript describes a method for training linear SVMs (including binary\nSVMs, SVM regression, and structural SVMs) from large, out-of-core training\ndatasets. Current strategies for large-scale learning fall into one of two\ncamps; batch algorithms which solve the learning problem given a finite\ndatasets, and online algorithms which can process out-of-core datasets. The\nformer typically requires datasets small enough to fit in memory. The latter is\noften phrased as a stochastic optimization problem; such algorithms enjoy\nstrong theoretical properties but often require manual tuned annealing\nschedules, and may converge slowly for problems with large output spaces (e.g.,\nstructural SVMs). We discuss an algorithm for an \"intermediate\" regime in which\nthe data is too large to fit in memory, but the active constraints (support\nvectors) are small enough to remain in memory. In this case, one can design\nrather efficient learning algorithms that are as stable as batch algorithms,\nbut capable of processing out-of-core datasets. We have developed such a\nMATLAB-based solver and used it to train a collection of recognition systems\nfor articulated pose estimation, facial analysis, 3D object recognition, and\naction classification, all with publicly-available code. This writeup describes\nthe solver in detail.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 00:55:51 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 04:10:06 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Ramanan", "Deva", ""]]}, {"id": "1312.1909", "submitter": "Qi Wang", "authors": "Qi Wang and Joseph JaJa", "title": "From Maxout to Channel-Out: Encoding Information on Sparse Pathways", "comments": "10 pages including the appendix, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an important insight from neural science, we propose a new\nframework for understanding the success of the recently proposed \"maxout\"\nnetworks. The framework is based on encoding information on sparse pathways and\nrecognizing the correct pathway at inference time. Elaborating further on this\ninsight, we propose a novel deep network architecture, called \"channel-out\"\nnetwork, which takes a much better advantage of sparse pathway encoding. In\nchannel-out networks, pathways are not only formed a posteriori, but they are\nalso actively selected according to the inference outputs from the lower\nlayers. From a mathematical perspective, channel-out networks can represent a\nwider class of piece-wise continuous functions, thereby endowing the network\nwith more expressive power than that of maxout networks. We test our\nchannel-out networks on several well-known image classification benchmarks,\nsetting new state-of-the-art performance on CIFAR-100 and STL-10, which\nrepresent some of the \"harder\" image classification benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 17:56:11 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Wang", "Qi", ""], ["JaJa", "Joseph", ""]]}, {"id": "1312.1931", "submitter": "Liheng Bian", "authors": "Liheng Bian, Jinli Suo, Feng Chen and Qionghai Dai", "title": "Multi-frame denoising of high speed optical coherence tomography data\n  using inter-frame and intra-frame priors", "comments": null, "journal-ref": null, "doi": "10.1117/1.JBO.20.3.036006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is an important interferometric diagnostic\ntechnique which provides cross-sectional views of the subsurface microstructure\nof biological tissues. However, the imaging quality of high-speed OCT is\nlimited due to the large speckle noise. To address this problem, this paper\nproposes a multi-frame algorithmic method to denoise OCT volume.\nMathematically, we build an optimization model which forces the temporally\nregistered frames to be low rank, and the gradient in each frame to be sparse,\nunder logarithmic image formation and noise variance constraints. Besides, a\nconvex optimization algorithm based on the augmented Lagrangian method is\nderived to solve the above model. The results reveal that our approach\noutperforms the other methods in terms of both speckle noise suppression and\ncrucial detail preservation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 17:17:00 GMT"}, {"version": "v2", "created": "Sat, 29 Nov 2014 11:18:40 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bian", "Liheng", ""], ["Suo", "Jinli", ""], ["Chen", "Feng", ""], ["Dai", "Qionghai", ""]]}, {"id": "1312.2061", "submitter": "Krishna A N", "authors": "Krishna A N, B G Prasad", "title": "Region and Location Based Indexing and Retrieval of MR-T2 Brain Tumor\n  Images", "comments": "10 pages", "journal-ref": "International Journal of Information Processing, 7(3):16-25, 2013", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, region based and location based retrieval systems have been\nimplemented for retrieval of MR-T2 axial 2-D brain images. This is done by\nextracting and characterizing the tumor portion of 2-D brain slices by use of a\nsuitable threshold computed over the entire image. Indexing and retrieval is\nthen performed by computing texture features based on gray-tone\nspatial-dependence matrix of segmented regions. A Hash structure is used to\nindex all images. A combined index is adopted to point to all similar images in\nterms of the texture features. At query time, only those images that are in the\nsame hash bucket as those of the queried image are compared for similarity,\nthus reducing the search space and time.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 05:40:59 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["N", "Krishna A", ""], ["Prasad", "B G", ""]]}, {"id": "1312.2249", "submitter": "Dumitru Erhan", "authors": "Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov", "title": "Scalable Object Detection using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have recently achieved state-of-the-art\nperformance on a number of image recognition benchmarks, including the ImageNet\nLarge-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on\nthe localization sub-task was a network that predicts a single bounding box and\na confidence score for each object category in the image. Such a model captures\nthe whole-image context around the objects but cannot handle multiple instances\nof the same object in the image without naively replicating the number of\noutputs for each instance. In this work, we propose a saliency-inspired neural\nnetwork model for detection, which predicts a set of class-agnostic bounding\nboxes along with a single score for each box, corresponding to its likelihood\nof containing any object of interest. The model naturally handles a variable\nnumber of instances for each class and allows for cross-class generalization at\nthe highest levels of the network. We are able to obtain competitive\nrecognition performance on VOC2007 and ILSVRC2012, while using only the top few\npredicted locations in each image and a small number of neural network\nevaluations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 19:40:51 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Erhan", "Dumitru", ""], ["Szegedy", "Christian", ""], ["Toshev", "Alexander", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "1312.2383", "submitter": "Griffith Klogo S", "authors": "Griffith S. Klogo, Akpeko Gasonoo and Isaac K. E. Ampomah", "title": "On the Performance of Filters for Reduction of Speckle Noise in SAR\n  Images off the Coast of the Gulf of Guinea", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Radar (SAR) imagery to monitor oil spills are some methods\nthat have been proposed for the West African sub-region. With the increase in\nthe number of oil exploration companies in Ghana (and her neighbors) and the\nrise in the coastal activities in the sub-region, there is the need for proper\nmonitoring of the environmental impact of these socio-economic activities on\nthe environment. Detection and near real-time information about oil spills are\nfundamental in reducing oil spill environmental impact. SAR images are prone to\nsome noise, which is predominantly speckle noise around the coastal areas. This\npaper evaluates the performance of the mean and median filters used in the\npreprocessing filtering to reduce speckle noise in SAR images for most image\nprocessing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 11:14:21 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Klogo", "Griffith S.", ""], ["Gasonoo", "Akpeko", ""], ["Ampomah", "Isaac K. E.", ""]]}, {"id": "1312.2877", "submitter": "Mohammad H. Alomari", "authors": "Mohammad H. Alomari, Aya Samaha, Khaled AlKamha", "title": "Automated Classification of L/R Hand Movement EEG Signals using Advanced\n  Feature Extraction and Machine Learning", "comments": "6 pages, 4 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (ijacsa) 07/2013; 4(6):207-212", "doi": "10.14569/IJACSA.2013.040628", "report-no": null, "categories": "cs.NE cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an automated computer platform for the purpose of\nclassifying Electroencephalography (EEG) signals associated with left and right\nhand movements using a hybrid system that uses advanced feature extraction\ntechniques and machine learning algorithms. It is known that EEG represents the\nbrain activity by the electrical voltage fluctuations along the scalp, and\nBrain-Computer Interface (BCI) is a device that enables the use of the brain\nneural activity to communicate with others or to control machines, artificial\nlimbs, or robots without direct physical movements. In our research work, we\naspired to find the best feature extraction method that enables the\ndifferentiation between left and right executed fist movements through various\nclassification algorithms. The EEG dataset used in this research was created\nand contributed to PhysioNet by the developers of the BCI2000 instrumentation\nsystem. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts\nremoval was done using AAR. Data was epoched on the basis of Event-Related (De)\nSynchronization (ERD/ERS) and movement-related cortical potentials (MRCP)\nfeatures. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta\nrhythms were isolated for the MRCP analysis. The Independent Component Analysis\n(ICA) spatial filter was applied on related channels for noise reduction and\nisolation of both artifactually and neutrally generated EEG sources. The final\nfeature vector included the ERD, ERS, and MRCP features in addition to the\nmean, power and energy of the activations of the resulting independent\ncomponents of the epoched feature datasets. The datasets were inputted into two\nmachine-learning algorithms: Neural Networks (NNs) and Support Vector Machines\n(SVMs). Intensive experiments were carried out and optimum classification\nperformances of 89.8 and 97.1 were obtained using NN and SVM, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 17:04:18 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Alomari", "Mohammad H.", ""], ["Samaha", "Aya", ""], ["AlKamha", "Khaled", ""]]}, {"id": "1312.3035", "submitter": "Michael Bronstein", "authors": "Michael M. Bronstein, Klaus Glashoff", "title": "Heat kernel coupling for multiple graph analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce heat kernel coupling (HKC) as a method of\nconstructing multimodal spectral geometry on weighted graphs of different size\nwithout vertex-wise bijective correspondence. We show that Laplacian averaging\ncan be derived as a limit case of HKC, and demonstrate its applications on\nseveral problems from the manifold learning and pattern recognition domain.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 04:59:49 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Glashoff", "Klaus", ""]]}, {"id": "1312.3061", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Jing Wang, Qifa Ke, Gang Zeng, Shipeng Li", "title": "Fast Approximate $K$-Means via Cluster Closures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $K$-means, a simple and effective clustering algorithm, is one of the most\nwidely used algorithms in multimedia and computer vision community. Traditional\n$k$-means is an iterative algorithm---in each iteration new cluster centers are\ncomputed and each data point is re-assigned to its nearest center. The cluster\nre-assignment step becomes prohibitively expensive when the number of data\npoints and cluster centers are large.\n  In this paper, we propose a novel approximate $k$-means algorithm to greatly\nreduce the computational complexity in the assignment step. Our approach is\nmotivated by the observation that most active points changing their cluster\nassignments at each iteration are located on or near cluster boundaries. The\nidea is to efficiently identify those active points by pre-assembling the data\ninto groups of neighboring points using multiple random spatial partition\ntrees, and to use the neighborhood information to construct a closure for each\ncluster, in such a way only a small number of cluster candidates need to be\nconsidered when assigning a data point to its nearest cluster. Using complexity\nanalysis, image data clustering, and applications to image retrieval, we show\nthat our approach out-performs state-of-the-art approximate $k$-means\nalgorithms in terms of clustering quality and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 08:02:09 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Wang", "Jingdong", ""], ["Wang", "Jing", ""], ["Ke", "Qifa", ""], ["Zeng", "Gang", ""], ["Li", "Shipeng", ""]]}, {"id": "1312.3062", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Jing Wang, Gang Zeng, Rui Gan, Shipeng Li, Baining Guo", "title": "Fast Neighborhood Graph Search using Cartesian Concatenation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new data structure for approximate nearest\nneighbor search. This structure augments the neighborhood graph with a bridge\ngraph. We propose to exploit Cartesian concatenation to produce a large set of\nvectors, called bridge vectors, from several small sets of subvectors. Each\nbridge vector is connected with a few reference vectors near to it, forming a\nbridge graph. Our approach finds nearest neighbors by simultaneously traversing\nthe neighborhood graph and the bridge graph in the best-first strategy. The\nsuccess of our approach stems from two factors: the exact nearest neighbor\nsearch over a large number of bridge vectors can be done quickly, and the\nreference vectors connected to a bridge (reference) vector near the query are\nalso likely to be near the query. Experimental results on searching over large\nscale datasets (SIFT, GIST and HOG) show that our approach outperforms\nstate-of-the-art ANN search algorithms in terms of efficiency and accuracy. The\ncombination of our approach with the IVFADC system also shows superior\nperformance over the BIGANN dataset of $1$ billion SIFT features compared with\nthe best previously published result.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 08:02:29 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Wang", "Jingdong", ""], ["Wang", "Jing", ""], ["Zeng", "Gang", ""], ["Gan", "Rui", ""], ["Li", "Shipeng", ""], ["Guo", "Baining", ""]]}, {"id": "1312.3199", "submitter": "Rahele Kafieh", "authors": "Raheleh Kafieh, Hossein Rabbani, Fedra Hajizadeh, Michael D. Abramoff,\n  Milan Sonka", "title": "Thickness Mapping of Eleven Retinal Layers in Normal Eyes Using Spectral\n  Domain Optical Coherence Tomography", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. This study was conducted to determine the thickness map of eleven\nretinal layers in normal subjects by spectral domain optical coherence\ntomography (SD-OCT) and evaluate their association with sex and age. Methods.\nMean regional retinal thickness of 11 retinal layers were obtained by automatic\nthree-dimensional diffusion-map-based method in 112 normal eyes of 76 Iranian\nsubjects. Results. The thickness map of central foveal area in layer 1, 3, and\n4 displayed the minimum thickness (P<0.005 for all). Maximum thickness was\nobserved in nasal to the fovea of layer 1 (P<0.001) and in a circular pattern\nin the parafoveal retinal area of layers 2, 3 and 4 and in central foveal area\nof layer 6 (P<0.001). Temporal and inferior quadrants of the total retinal\nthickness and most of other quadrants of layer 1 were significantly greater in\nthe men than in the women. Surrounding eight sectors of total retinal thickness\nand a limited number of sectors in layer 1 and 4 significantly correlated with\nage. Conclusion. SD-OCT demonstrated the three-dimensional thickness\ndistribution of retinal layers in normal eyes. Thickness of layers varied with\nsex and age and in different sectors. These variables should be considered\nwhile evaluating macular thickness.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 15:00:53 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Kafieh", "Raheleh", ""], ["Rabbani", "Hossein", ""], ["Hajizadeh", "Fedra", ""], ["Abramoff", "Michael D.", ""], ["Sonka", "Milan", ""]]}, {"id": "1312.3240", "submitter": "Alexander Vezhnevets", "authors": "Alexander Vezhnevets and Vittorio Ferrari", "title": "Associative embeddings for large-scale knowledge transfer with\n  self-assessment", "comments": "A final CVPR version with a correction in (1). IEEE Computer Vision\n  and Pattern Recognition, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for knowledge transfer between semantically related\nclasses in ImageNet. By transferring knowledge from the images that have\nbounding-box annotations to the others, our method is capable of automatically\npopulating ImageNet with many more bounding-boxes and even pixel-level\nsegmentations. The underlying assumption that objects from semantically related\nclasses look alike is formalized in our novel Associative Embedding (AE)\nrepresentation. AE recovers the latent low-dimensional space of appearance\nvariations among image windows. The dimensions of AE space tend to correspond\nto aspects of window appearance (e.g. side view, close up, background). We\nmodel the overlap of a window with an object using Gaussian Processes (GP)\nregression, which spreads annotation smoothly through AE space. The\nprobabilistic nature of GP allows our method to perform self-assessment, i.e.\nassigning a quality estimate to its own output. It enables trading off the\namount of returned annotations for their quality. A large scale experiment on\n219 classes and 0.5 million images demonstrates that our method outperforms\nstate-of-the-art methods and baselines for both object localization and\nsegmentation. Using self-assessment we can automatically return bounding-box\nannotations for 30% of all images with high localization accuracy (i.e.~73%\naverage overlap with ground-truth).\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 16:59:21 GMT"}, {"version": "v2", "created": "Wed, 30 Jul 2014 13:21:36 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Vezhnevets", "Alexander", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1312.3429", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic", "title": "Unsupervised learning of depth and motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for the joint estimation of disparity and motion. The\nmodel is based on learning about the interrelations between images from\nmultiple cameras, multiple frames in a video, or the combination of both. We\nshow that learning depth and motion cues, as well as their combinations, from\ndata is possible within a single type of architecture and a single type of\nlearning algorithm, by using biologically inspired \"complex cell\" like units,\nwhich encode correlations between the pixels across image pairs. Our\nexperimental results show that the learning of depth and motion makes it\npossible to achieve state-of-the-art performance in 3-D activity analysis, and\nto outperform existing hand-engineered 3-D motion features by a very large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 10:03:47 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 16:11:52 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""]]}, {"id": "1312.3522", "submitter": "Weizhi  Lu", "authors": "Weizhi Lu and Weiyu Li and Kidiyo Kpalma and Joseph Ronsin", "title": "Sparse Matrix-based Random Projection for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As a typical dimensionality reduction technique, random projection can be\nsimply implemented with linear projection, while maintaining the pairwise\ndistances of high-dimensional data with high probability. Considering this\ntechnique is mainly exploited for the task of classification, this paper is\ndeveloped to study the construction of random matrix from the viewpoint of\nfeature selection, rather than of traditional distance preservation. This\nyields a somewhat surprising theoretical result, that is, the sparse random\nmatrix with exactly one nonzero element per column, can present better feature\nselection performance than other more dense matrices, if the projection\ndimension is sufficiently large (namely, not much smaller than the number of\nfeature elements); otherwise, it will perform comparably to others. For random\nprojection, this theoretical result implies considerable improvement on both\ncomplexity and performance, which is widely confirmed with the classification\nexperiments on both synthetic data and real data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 15:26:57 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 13:56:04 GMT"}, {"version": "v3", "created": "Sun, 12 Oct 2014 22:10:13 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Lu", "Weizhi", ""], ["Li", "Weiyu", ""], ["Kpalma", "Kidiyo", ""], ["Ronsin", "Joseph", ""]]}, {"id": "1312.3724", "submitter": "Pierluigi Gallo", "authors": "Pierluigi Gallo, Ilenia Tinnirello, Laura Giarr\\'e, Domenico Garlisi,\n  Daniele Croce and Adriano Fagiolini", "title": "ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented\n  perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ARIANNA stands for pAth Recognition for Indoor Assisted Navigation with\nAugmented perception. It is a flexible and low cost navigation system for vi-\nsually impaired people. Arianna permits to navigate colored paths painted or\nsticked on the floor revealing their directions through vibrational feedback on\ncommercial smartphones.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 07:54:22 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Gallo", "Pierluigi", ""], ["Tinnirello", "Ilenia", ""], ["Giarr\u00e9", "Laura", ""], ["Garlisi", "Domenico", ""], ["Croce", "Daniele", ""], ["Fagiolini", "Adriano", ""]]}, {"id": "1312.3787", "submitter": "Kunal Ghosh", "authors": "Dharini S., Guru Prasad M., Hari haran. V., Kiran Tej J. L., Kunal\n  Ghosh", "title": "Analysis and Understanding of Various Models for Efficient\n  Representation and Accurate Recognition of Human Faces", "comments": "Proceedings of National Conference on \"Emerging Trends in IT\" -\n  eit10, March 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we have tried to compare the various face recognition models\nagainst their classical problems. We look at the methods followed by these\napproaches and evaluate to what extent they are able to solve the problems. All\nmethods proposed have some drawbacks under certain conditions. To overcome\nthese drawbacks we propose a multi-model approach\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 12:25:09 GMT"}, {"version": "v2", "created": "Sat, 14 Feb 2015 17:32:32 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["S.", "Dharini", ""], ["M.", "Guru Prasad", ""], ["V.", "Hari haran.", ""], ["L.", "Kiran Tej J.", ""], ["Ghosh", "Kunal", ""]]}, {"id": "1312.3989", "submitter": "Nima Hatami", "authors": "Nima Hatami and Camelia Chira", "title": "Classifiers With a Reject Option for Early Time-Series Classification", "comments": null, "journal-ref": "Computational Intelligence and Ensemble Learning (CIEL), IEEE\n  Symposium on, 9-16, 2013", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early classification of time-series data in a dynamic environment is a\nchallenging problem of great importance in signal processing. This paper\nproposes a classifier architecture with a reject option capable of online\ndecision making without the need to wait for the entire time series signal to\nbe present. The main idea is to classify an odor/gas signal with an acceptable\naccuracy as early as possible. Instead of using posterior probability of a\nclassifier, the proposed method uses the \"agreement\" of an ensemble to decide\nwhether to accept or reject the candidate label. The introduced algorithm is\napplied to the bio-chemistry problem of odor classification to build a novel\nElectronic-Nose called Forefront-Nose. Experimental results on wind tunnel\ntest-bed facility confirms the robustness of the forefront-nose compared to the\nstandard classifiers from both earliness and recognition perspectives.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 00:28:32 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Hatami", "Nima", ""], ["Chira", "Camelia", ""]]}, {"id": "1312.3990", "submitter": "Nima Hatami", "authors": "Nima Hatami, Reza Ebrahimpour, Reza Ghaderi", "title": "ECOC-Based Training of Neural Networks for Face Recognition", "comments": null, "journal-ref": "Cybernetics and Intelligent Systems, IEEE Conference on, 450-454,\n  2008", "doi": "10.1109/ICCIS.2008.4670763", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error Correcting Output Codes, ECOC, is an output representation method\ncapable of discovering some of the errors produced in classification tasks.\nThis paper describes the application of ECOC to the training of feed forward\nneural networks, FFNN, for improving the overall accuracy of classification\nsystems. Indeed, to improve the generalization of FFNN classifiers, this paper\nproposes an ECOC-Based training method for Neural Networks that use ECOC as the\noutput representation, and adopts the traditional Back-Propagation algorithm,\nBP, to adjust weights of the network. Experimental results for face recognition\nproblem on Yale database demonstrate the effectiveness of our method. With a\nrejection scheme defined by a simple robustness rate, high reliability is\nachieved in this application.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 00:29:36 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Hatami", "Nima", ""], ["Ebrahimpour", "Reza", ""], ["Ghaderi", "Reza", ""]]}, {"id": "1312.4074", "submitter": "Srinjoy Ganguly Mr.", "authors": "Srinjoy Ganguly, Digbalay Bose and Amit Konar", "title": "Clustering using Vector Membership: An Extension of the Fuzzy C-Means\n  Algorithm", "comments": "6 pages, 8 figures and 1 table (Conference Paper)", "journal-ref": "Proceedings of the IEEE International Conference on Advanced\n  Computing (ICoAC)-2013, pp.XX-XX,Chennai, India, 18 - 20 December (2013)", "doi": "10.1109/ICoAC.2013.6921922", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an important facet of explorative data mining and finds\nextensive use in several fields. In this paper, we propose an extension of the\nclassical Fuzzy C-Means clustering algorithm. The proposed algorithm,\nabbreviated as VFC, adopts a multi-dimensional membership vector for each data\npoint instead of the traditional, scalar membership value defined in the\noriginal algorithm. The membership vector for each point is obtained by\nconsidering each feature of that point separately and obtaining individual\nmembership values for the same. We also propose an algorithm to efficiently\nallocate the initial cluster centers close to the actual centers, so as to\nfacilitate rapid convergence. Further, we propose a scheme to achieve crisp\nclustering using the VFC algorithm. The proposed, novel clustering scheme has\nbeen tested on two standard data sets in order to analyze its performance. We\nalso examine the efficacy of the proposed scheme by analyzing its performance\non image segmentation examples and comparing it with the classical Fuzzy\nC-means clustering algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 18:11:33 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ganguly", "Srinjoy", ""], ["Bose", "Digbalay", ""], ["Konar", "Amit", ""]]}, {"id": "1312.4124", "submitter": "Maryam Khalili", "authors": "Maryam Soltanali Khalili and Hamed Sadjedi", "title": "A robust Iris recognition method on adverse conditions", "comments": null, "journal-ref": "International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol.3,No.5,October 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As a stable biometric system, iris has recently attracted great attention\namong the researchers. However, research is still needed to provide appropriate\nsolutions to ensure the resistance of the system against error factors. The\npresent study has tried to apply a mask to the image so that the unexpected\nfactors affecting the location of the iris can be removed. So, pupil\nlocalization will be faster and robust. Then to locate the exact location of\nthe iris, a simple stage of boundary displacement due to the Canny edge\ndetector has been applied. Then, with searching left and right IRIS edge point,\nouter radios of IRIS will be detect. Through the process of extracting the iris\nfeatures, it has been sought to obtain the distinctive iris texture features by\nusing a discrete stationary wavelets transform 2-D (DSWT2). Using DSWT2 tool\nand symlet 4 wavelet, distinctive features are extracted. To reduce the\ncomputational cost, the features obtained from the application of the wavelet\nhave been investigated and a feature selection procedure, using similarity\ncriteria, has been implemented. Finally, the iris matching has been performed\nusing a semi-correlation criterion. The accuracy of the proposed method for\nlocalization on CASIA-v1, CASIA-v3 is 99.73%, 98.24% and 97.04%, respectively.\nThe accuracy of the feature extraction proposed method for CASIA3 iris images\ndatabase is 97.82%, which confirms the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 08:54:45 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Khalili", "Maryam Soltanali", ""], ["Sadjedi", "Hamed", ""]]}, {"id": "1312.4190", "submitter": "Jakub Konecny", "authors": "Jakub Kone\\v{c}n\\'y and Michal Hagara", "title": "One-Shot-Learning Gesture Recognition using HOG-HOF Features", "comments": "20 pages, 10 figures, 2 tables To appear in Journal of Machine\n  Learning Research subject to minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to describe one-shot-learning gesture\nrecognition systems developed on the \\textit{ChaLearn Gesture Dataset}. We use\nRGB and depth images and combine appearance (Histograms of Oriented Gradients)\nand motion descriptors (Histogram of Optical Flow) for parallel temporal\nsegmentation and recognition. The Quadratic-Chi distance family is used to\nmeasure differences between histograms to capture cross-bin relationships. We\nalso propose a new algorithm for trimming videos --- to remove all the\nunimportant frames from videos. We present two methods that use combination of\nHOG-HOF descriptors together with variants of Dynamic Time Warping technique.\nBoth methods outperform other published methods and help narrow down the gap\nbetween human performance and algorithms on this task. The code has been made\npublicly available in the MLOSS repository.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 20:58:21 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2014 17:47:11 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Hagara", "Michal", ""]]}, {"id": "1312.4346", "submitter": "Noritaka Sato", "authors": "Noritaka Sato, Masataka Ito, Yoshifumi Morita and Fumitoshi Matsuno", "title": "Teleoperation System Using Past Image Records Considering Narrow\n  Communication Band", "comments": "ROSIN2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teleoperation is necessary when the robot is applied to real missions, for\nexample surveillance, search and rescue. We proposed teleoperation system using\npast image records (SPIR). SPIR virtually generates the bird's-eye view image\nby overlaying the CG model of the robot at the corresponding current position\non the background image which is captured from the camera mounted on the robot\nat a past time. The problem for SPIR is that the communication bandwidth is\noften narrow in some teleoperation tasks. In this case, the candidates of\nbackground image of SPIR are few and the position of the robot is often\ndelayed. In this study, we propose zoom function for insufficiency of\ncandidates of the background image and additional interpolation lines for the\ndelay of the position data of the robot. To evaluate proposed system, an\noutdoor experiments are carried out. The outdoor experiment is conducted on a\ntraining course of a driving school.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 13:18:40 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Sato", "Noritaka", ""], ["Ito", "Masataka", ""], ["Morita", "Yoshifumi", ""], ["Matsuno", "Fumitoshi", ""]]}, {"id": "1312.4354", "submitter": "Lukas F. Lang", "authors": "Clemens Kirisits, Lukas F. Lang, Otmar Scherzer", "title": "Decomposition of Optical Flow on the Sphere", "comments": "The final publication is available at link.springer.com", "journal-ref": null, "doi": "10.1007/s13137-013-0055-8", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a number of variational regularisation methods for the estimation\nand decomposition of motion fields on the $2$-sphere. While motion estimation\nis based on the optical flow equation, the presented decomposition models are\nmotivated by recent trends in image analysis. In particular we treat $u+v$\ndecomposition as well as hierarchical decomposition. Helmholtz decomposition of\nmotion fields is obtained as a natural by-product of the chosen numerical\nmethod based on vector spherical harmonics. All models are tested on time-lapse\nmicroscopy data depicting fluorescently labelled endodermal cells of a\nzebrafish embryo.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 13:38:27 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 15:19:43 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Kirisits", "Clemens", ""], ["Lang", "Lukas F.", ""], ["Scherzer", "Otmar", ""]]}, {"id": "1312.4384", "submitter": "Eren Golge", "authors": "Eren Golge and Pinar Duygulu", "title": "Rectifying Self Organizing Maps for Automatic Concept Learning from Web\n  Images", "comments": "present CVPR2014 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attack the problem of learning concepts automatically from noisy web image\nsearch results. Going beyond low level attributes, such as colour and texture,\nwe explore weakly-labelled datasets for the learning of higher level concepts,\nsuch as scene categories. The idea is based on discovering common\ncharacteristics shared among subsets of images by posing a method that is able\nto organise the data while eliminating irrelevant instances. We propose a novel\nclustering and outlier detection method, namely Rectifying Self Organizing Maps\n(RSOM). Given an image collection returned for a concept query, RSOM provides\nclusters pruned from outliers. Each cluster is used to train a model\nrepresenting a different characteristics of the concept. The proposed method\noutperforms the state-of-the-art studies on the task of learning low-level\nconcepts, and it is competitive in learning higher level concepts as well. It\nis capable to work at large scale with no supervision through exploiting the\navailable sources.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 14:51:00 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Golge", "Eren", ""], ["Duygulu", "Pinar", ""]]}, {"id": "1312.4400", "submitter": "Min Lin", "authors": "Min Lin, Qiang Chen, Shuicheng Yan", "title": "Network In Network", "comments": "10 pages, 4 figures, for iclr2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep network structure called \"Network In Network\" (NIN)\nto enhance model discriminability for local patches within the receptive field.\nThe conventional convolutional layer uses linear filters followed by a\nnonlinear activation function to scan the input. Instead, we build micro neural\nnetworks with more complex structures to abstract the data within the receptive\nfield. We instantiate the micro neural network with a multilayer perceptron,\nwhich is a potent function approximator. The feature maps are obtained by\nsliding the micro networks over the input in a similar manner as CNN; they are\nthen fed into the next layer. Deep NIN can be implemented by stacking mutiple\nof the above described structure. With enhanced local modeling via the micro\nnetwork, we are able to utilize global average pooling over feature maps in the\nclassification layer, which is easier to interpret and less prone to\noverfitting than traditional fully connected layers. We demonstrated the\nstate-of-the-art classification performances with NIN on CIFAR-10 and\nCIFAR-100, and reasonable performances on SVHN and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 15:34:13 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 09:30:27 GMT"}, {"version": "v3", "created": "Tue, 4 Mar 2014 05:15:42 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Lin", "Min", ""], ["Chen", "Qiang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1312.4569", "submitter": "Vu Pham", "authors": "Vu Pham, Th\\'eodore Bluche, Christopher Kermorvant, J\\'er\\^ome\n  Louradour", "title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) with Long Short-Term memory cells currently\nhold the best known results in unconstrained handwriting recognition. We show\nthat their performance can be greatly improved using dropout - a recently\nproposed regularization method for deep architectures. While previous works\nshowed that dropout gave superior performance in the context of convolutional\nnetworks, it had never been applied to RNNs. In our approach, dropout is\ncarefully used in the network so that it does not affect the recurrent\nconnections, hence the power of RNNs in modeling sequence is preserved.\nExtensive experiments on a broad range of handwritten databases confirm the\neffectiveness of dropout on deep architectures even when the network mainly\nconsists of recurrent and shared connections.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 10:45:48 GMT"}, {"version": "v2", "created": "Mon, 10 Mar 2014 15:34:55 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Pham", "Vu", ""], ["Bluche", "Th\u00e9odore", ""], ["Kermorvant", "Christopher", ""], ["Louradour", "J\u00e9r\u00f4me", ""]]}, {"id": "1312.4637", "submitter": "Zhen Zhang", "authors": "Zhen Zhang, Qinfeng Shi, Yanning Zhang, Chunhua Shen, Anton van den\n  Hengel", "title": "Constraint Reduction using Marginal Polytope Diagrams for MAP LP\n  Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LP relaxation-based message passing algorithms provide an effective tool for\nMAP inference over Probabilistic Graphical Models. However, different LP\nrelaxations often have different objective functions and variables of differing\ndimensions, which presents a barrier to effective comparison and analysis. In\naddition, the computational complexity of LP relaxation-based methods grows\nquickly with the number of constraints. Reducing the number of constraints\nwithout sacrificing the quality of the solutions is thus desirable.\n  We propose a unified formulation under which existing MAP LP relaxations may\nbe compared and analysed. Furthermore, we propose a new tool called Marginal\nPolytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited\nsuch as node redundancy and edge equivalence. We show that using Marginal\nPolytope Diagrams allows the number of constraints to be reduced without\nloosening the LP relaxations. Then, using Marginal Polytope Diagrams and\nconstraint reduction, we develop three novel message passing algorithms, and\ndemonstrate that two of these show a significant improvement in speed over\nstate-of-art algorithms while delivering a competitive, and sometimes higher,\nquality of solution.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 04:44:04 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 09:30:20 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Zhang", "Zhen", ""], ["Shi", "Qinfeng", ""], ["Zhang", "Yanning", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1312.4659", "submitter": "Alexander Toshev", "authors": "Alexander Toshev, Christian Szegedy", "title": "DeepPose: Human Pose Estimation via Deep Neural Networks", "comments": "IEEE Conference on Computer Vision and Pattern Recognition, 2014", "journal-ref": null, "doi": "10.1109/CVPR.2014.214", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for human pose estimation based on Deep Neural Networks\n(DNNs). The pose estimation is formulated as a DNN-based regression problem\ntowards body joints. We present a cascade of such DNN regressors which results\nin high precision pose estimates. The approach has the advantage of reasoning\nabout pose in a holistic fashion and has a simple but yet powerful formulation\nwhich capitalizes on recent advances in Deep Learning. We present a detailed\nempirical analysis with state-of-art or better performance on four academic\nbenchmarks of diverse real-world images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 06:36:10 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 21:26:18 GMT"}, {"version": "v3", "created": "Wed, 20 Aug 2014 17:42:45 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Toshev", "Alexander", ""], ["Szegedy", "Christian", ""]]}, {"id": "1312.4740", "submitter": "Yalong Bai", "authors": "Yalong Bai, Kuiyuan Yang, Wei Yu, Wei-Ying Ma, Tiejun Zhao", "title": "Learning High-level Image Representation for Image Retrieval via\n  Multi-Task DNN using Clickthrough Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval refers to finding relevant images from an image database for\na query, which is considered difficult for the gap between low-level\nrepresentation of images and high-level representation of queries. Recently\nfurther developed Deep Neural Network sheds light on automatically learning\nhigh-level image representation from raw pixels. In this paper, we proposed a\nmulti-task DNN learned for image retrieval, which contains two parts, i.e.,\nquery-sharing layers for image representation computation and query-specific\nlayers for relevance estimation. The weights of multi-task DNN are learned on\nclickthrough data by Ring Training. Experimental results on both simulated and\nreal dataset show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 12:11:04 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2013 00:47:19 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Bai", "Yalong", ""], ["Yang", "Kuiyuan", ""], ["Yu", "Wei", ""], ["Ma", "Wei-Ying", ""], ["Zhao", "Tiejun", ""]]}, {"id": "1312.4746", "submitter": "Martin Kleinsteuber", "authors": "Claudia Nieuwenhuis, Daniel Cremers, Simon Hawe, Martin Kleinsteuber", "title": "Co-Sparse Textural Similarity for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for segmenting natural images based on texture and\ncolor information, which leverages the co-sparse analysis model for image\nsegmentation within a convex multilabel optimization framework. As a key\ningredient of this method, we introduce a novel textural similarity measure,\nwhich builds upon the co-sparse representation of image patches. We propose a\nBayesian approach to merge textural similarity with information about color and\nlocation. Combined with recently developed convex multilabel optimization\nmethods this leads to an efficient algorithm for both supervised and\nunsupervised segmentation, which is easily parallelized on graphics hardware.\nThe approach provides competitive results in unsupervised segmentation and\noutperforms state-of-the-art interactive segmentation methods on the Graz\nBenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 12:35:19 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Nieuwenhuis", "Claudia", ""], ["Cremers", "Daniel", ""], ["Hawe", "Simon", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1312.4752", "submitter": "Ricardo Martins", "authors": "Ricardo Martins", "title": "BW - Eye Ophthalmologic decision support system based on clinical\n  workflow and data mining techniques-image registration algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blueworks - Medical Expert Diagnosis is developing an application, BWEye, to\nbe used as an ophthalmology consultation decision support system. The\nimplementation of this application involves several different tasks and one of\nthem is the implementation of an ophthalmology images registration algorithm.\nThe work reported in this document is related with the implementation of an\nalgorithm to register images of angiography, colour retinography and redfree\nretinography. The implementations described were developed in the software\nMATLAB.\n  The implemented algorithm is based in the detection of the bifurcation points\n(y-features) of the vascular structures of the retina that usually are visible\nin the referred type of images. There are proposed two approaches to establish\nan initial set of features correspondences. The first approach is based in the\nmaximization of the mutual information of the bifurcation regions of the\nfeatures of images. The second approach is based in the characterization of\neach bifurcation point and in the minimization of the Euclidean distance\nbetween the descriptions of the features of the images in the descriptors\nspace. The final set of the matching features for a pair of images is defined\nthrough the application of the RANSAC algorithm.\n  Although, it was not achieved the implementation of a full functional\nalgorithm, there were made several analysis that can be important to future\nimprovement of the current implementation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 12:41:39 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Martins", "Ricardo", ""]]}, {"id": "1312.4894", "submitter": "Yangqing Jia", "authors": "Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev, Sergey\n  Ioffe", "title": "Deep Convolutional Ranking for Multilabel Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilabel image annotation is one of the most important challenges in\ncomputer vision with many real-world applications. While existing work usually\nuse conventional visual features for multilabel annotation, features based on\nDeep Neural Networks have shown potential to significantly boost performance.\nIn this work, we propose to leverage the advantage of such features and analyze\nkey components that lead to better performances. Specifically, we show that a\nsignificant performance gain could be obtained by combining convolutional\narchitectures with approximate top-$k$ ranking objectives, as thye naturally\nfit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset\noutperforms the conventional visual features by about 10%, obtaining the best\nreported performance in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 19:00:50 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 19:21:13 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Gong", "Yunchao", ""], ["Jia", "Yangqing", ""], ["Leung", "Thomas", ""], ["Toshev", "Alexander", ""], ["Ioffe", "Sergey", ""]]}, {"id": "1312.4967", "submitter": "Stefanie Wuhrer", "authors": "Stefanie Wuhrer, Leonid Pishchulin, Alan Brunton, Chang Shu and Jochen\n  Lang", "title": "Estimation of Human Body Shape and Posture Under Clothing", "comments": "23 pages, 11 figures", "journal-ref": "Computer Vision and Image Understanding, 127, pp. 31-42, 2014", "doi": "10.1016/j.cviu.2014.06.012", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the body shape and posture of a dressed human subject in motion\nrepresented as a sequence of (possibly incomplete) 3D meshes is important for\nvirtual change rooms and security. To solve this problem, statistical shape\nspaces encoding human body shape and posture variations are commonly used to\nconstrain the search space for the shape estimate. In this work, we propose a\nnovel method that uses a posture-invariant shape space to model body shape\nvariation combined with a skeleton-based deformation to model posture\nvariation. Our method can estimate the body shape and posture of both static\nscans and motion sequences of dressed human body scans. In case of motion\nsequences, our method takes advantage of motion cues to solve for a single body\nshape estimate along with a sequence of posture estimates. We apply our\napproach to both static scans and motion sequences and demonstrate that using\nour method, higher fitting accuracy is achieved than when using a variant of\nthe popular SCAPE model as statistical model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 21:01:05 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 13:03:22 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Wuhrer", "Stefanie", ""], ["Pishchulin", "Leonid", ""], ["Brunton", "Alan", ""], ["Shu", "Chang", ""], ["Lang", "Jochen", ""]]}, {"id": "1312.5033", "submitter": "Tomofumi Fujiwara", "authors": "Tomofumi Fujiwara, Tetsushi Kamegawa and Akio Gofuku", "title": "Evaluation of Plane Detection with RANSAC According to Density of 3D\n  Point Clouds", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have implemented a method that detects planar regions from 3D scan data\nusing Random Sample Consensus (RANSAC) algorithm to address the issue of a\ntrade-off between the scanning speed and the point density of 3D scanning.\nHowever, the limitation of the implemented method has not been clear yet. In\nthis paper, we conducted an additional experiment to evaluate the implemented\nmethod by changing its parameter and environments in both high and low point\ndensity data. As a result, the number of detected planes in high point density\ndata was different from that in low point density data with the same parameter\nvalue.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 03:33:14 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Fujiwara", "Tomofumi", ""], ["Kamegawa", "Tetsushi", ""], ["Gofuku", "Akio", ""]]}, {"id": "1312.5045", "submitter": "Anupriya Gogna", "authors": "Anupriya Gogna, Akash Tayal", "title": "Comparative analysis of evolutionary algorithms for image enhancement", "comments": null, "journal-ref": "International Journal of Metaheuristics Volume 2 Issue 1, July\n  2012 Pages 80-100", "doi": "10.1504/IJMHEUR.2012.048219", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms are metaheuristic techniques that derive inspiration\nfrom the natural process of evolution. They can efficiently solve (generate\nacceptable quality of solution in reasonable time) complex optimization\n(NP-Hard) problems. In this paper, automatic image enhancement is considered as\nan optimization problem and three evolutionary algorithms (Genetic Algorithm,\nDifferential Evolution and Self Organizing Migration Algorithm) are employed to\nsearch for an optimum solution. They are used to find an optimum parameter set\nfor an image enhancement transfer function. The aim is to maximize a fitness\ncriterion which is a measure of image contrast and the visibility of details in\nthe enhanced image. The enhancement results obtained using all three\nevolutionary algorithms are compared amongst themselves and also with the\noutput of histogram equalization method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 05:33:27 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Gogna", "Anupriya", ""], ["Tayal", "Akash", ""]]}, {"id": "1312.5047", "submitter": "Onur Ozyesil", "authors": "Onur Ozyesil, Amit Singer, Ronen Basri", "title": "Stable Camera Motion Estimation Using Convex Programming", "comments": "40 pages, 12 figures, 6 tables; notation and some unclear parts\n  updated, some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inverse problem of estimating n locations $t_1, ..., t_n$ (up to\nglobal scale, translation and negation) in $R^d$ from noisy measurements of a\nsubset of the (unsigned) pairwise lines that connect them, that is, from noisy\nmeasurements of $\\pm (t_i - t_j)/\\|t_i - t_j\\|$ for some pairs (i,j) (where the\nsigns are unknown). This problem is at the core of the structure from motion\n(SfM) problem in computer vision, where the $t_i$'s represent camera locations\nin $R^3$. The noiseless version of the problem, with exact line measurements,\nhas been considered previously under the general title of parallel rigidity\ntheory, mainly in order to characterize the conditions for unique realization\nof locations. For noisy pairwise line measurements, current methods tend to\nproduce spurious solutions that are clustered around a few locations. This\nsensitivity of the location estimates is a well-known problem in SfM,\nespecially for large, irregular collections of images.\n  In this paper we introduce a semidefinite programming (SDP) formulation,\nspecially tailored to overcome the clustering phenomenon. We further identify\nthe implications of parallel rigidity theory for the location estimation\nproblem to be well-posed, and prove exact (in the noiseless case) and stable\nlocation recovery results. We also formulate an alternating direction method to\nsolve the resulting semidefinite program, and provide a distributed version of\nour formulation for large numbers of locations. Specifically for the camera\nlocation estimation problem, we formulate a pairwise line estimation method\nbased on robust camera orientation and subspace estimation. Lastly, we\ndemonstrate the utility of our algorithm through experiments on real images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 06:04:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 02:36:29 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2015 22:46:03 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Ozyesil", "Onur", ""], ["Singer", "Amit", ""], ["Basri", "Ronen", ""]]}, {"id": "1312.5242", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Jost Tobias Springenberg and Thomas Brox", "title": "Unsupervised feature learning by augmenting single images", "comments": "ICLR 2014 workshop track submission (7 pages, 4 figures, 1 table)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deep learning is applied to visual object recognition, data augmentation\nis often used to generate additional training data without extra labeling cost.\nIt helps to reduce overfitting and increase the performance of the algorithm.\nIn this paper we investigate if it is possible to use data augmentation as the\nmain component of an unsupervised feature learning architecture. To that end we\nsample a set of random image patches and declare each of them to be a separate\nsingle-image surrogate class. We then extend these trivial one-element classes\nby applying a variety of transformations to the initial 'seed' patches. Finally\nwe train a convolutional neural network to discriminate between these surrogate\nclasses. The feature representation learned by the network can then be used in\nvarious vision tasks. We find that this simple feature learning algorithm is\nsurprisingly successful, achieving competitive classification results on\nseveral popular vision datasets (STL-10, CIFAR-10, Caltech-101).\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 17:44:17 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 18:02:09 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2014 13:07:23 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Springenberg", "Jost Tobias", ""], ["Brox", "Thomas", ""]]}, {"id": "1312.5355", "submitter": "Phillip Verbancsics", "authors": "Phillip Verbancsics and Josh Harguess", "title": "Generative NeuroEvolution for Deep Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  An important goal for the machine learning (ML) community is to create\napproaches that can learn solutions with human-level capability. One domain\nwhere humans have held a significant advantage is visual processing. A\nsignificant approach to addressing this gap has been machine learning\napproaches that are inspired from the natural systems, such as artificial\nneural networks (ANNs), evolutionary computation (EC), and generative and\ndevelopmental systems (GDS). Research into deep learning has demonstrated that\nsuch architectures can achieve performance competitive with humans on some\nvisual tasks; however, these systems have been primarily trained through\nsupervised and unsupervised learning algorithms. Alternatively, research is\nshowing that evolution may have a significant role in the development of visual\nsystems. Thus this paper investigates the role neuro-evolution (NE) can take in\ndeep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting\nTopologies is a NE approach that can effectively learn large neural structures\nby training an indirect encoding that compresses the ANN weight pattern as a\nfunction of geometry. The results show that HyperNEAT struggles with performing\nimage classification by itself, but can be effective in training a feature\nextractor that other ML approaches can learn from. Thus NeuroEvolution combined\nwith other ML methods provides an intriguing area of research that can\nreplicate the processes in nature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 22:14:31 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Verbancsics", "Phillip", ""], ["Harguess", "Josh", ""]]}, {"id": "1312.5402", "submitter": "Andrew Howard", "authors": "Andrew G. Howard", "title": "Some Improvements on Deep Convolutional Neural Network Based Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate multiple techniques to improve upon the current state of the\nart deep convolutional neural network based image classification pipeline. The\ntechiques include adding more image transformations to training data, adding\nmore transformations to generate additional predictions at test time and using\ncomplementary models applied to higher resolution images. This paper summarizes\nour entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our\nsystem achieved a top 5 classification error rate of 13.55% using no external\ndata which is over a 20% relative improvement on the previous year's winner.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 04:23:23 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Howard", "Andrew G.", ""]]}, {"id": "1312.5479", "submitter": "Jonathan Masci", "authors": "Jonathan Masci and Alex M. Bronstein and Michael M. Bronstein and\n  Pablo Sprechmann and Guillermo Sapiro", "title": "Sparse similarity-preserving hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a lot of attention has been devoted to efficient nearest\nneighbor search by means of similarity-preserving hashing. One of the plights\nof existing hashing techniques is the intrinsic trade-off between performance\nand computational complexity: while longer hash codes allow for lower false\npositive rates, it is very difficult to increase the embedding dimensionality\nwithout incurring in very high false negatives rates or prohibiting\ncomputational costs. In this paper, we propose a way to overcome this\nlimitation by enforcing the hash codes to be sparse. Sparse high-dimensional\ncodes enjoy from the low false positive rates typical of long hashes, while\nkeeping the false negative rates similar to those of a shorter dense hashing\nscheme with equal number of degrees of freedom. We use a tailored feed-forward\nneural network for the hashing function. Extensive experimental evaluation\ninvolving visual and multi-modal data shows the benefits of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 11:04:40 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 10:05:07 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2014 20:37:10 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Masci", "Jonathan", ""], ["Bronstein", "Alex M.", ""], ["Bronstein", "Michael M.", ""], ["Sprechmann", "Pablo", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1312.5568", "submitter": "Martin Kleinsteuber", "authors": "Xian Wei, Hao Shen, Martin Kleinsteuber", "title": "An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video representation is an important and challenging task in the computer\nvision community. In this paper, we assume that image frames of a moving scene\ncan be modeled as a Linear Dynamical System. We propose a sparse coding\nframework, named adaptive video dictionary learning (AVDL), to model a video\nadaptively. The developed framework is able to capture the dynamics of a moving\nscene by exploring both sparse properties and the temporal correlations of\nconsecutive video frames. The proposed method is compared with state of the art\nvideo processing methods on several benchmark data sequences, which exhibit\nappearance changes and heavy occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 14:41:29 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Wei", "Xian", ""], ["Shen", "Hao", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1312.5604", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro", "title": "Learning Transformations for Classification Forests", "comments": "arXiv admin note: text overlap with arXiv:1309.2074", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 16:01:41 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 12:24:54 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1312.5697", "submitter": "Andrew Rabinovich", "authors": "Samy Bengio, Jeff Dean, Dumitru Erhan, Eugene Ie, Quoc Le, Andrew\n  Rabinovich, Jonathon Shlens, Yoram Singer", "title": "Using Web Co-occurrence Statistics for Improving Image Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition and localization are important tasks in computer vision.\nThe focus of this work is the incorporation of contextual information in order\nto improve object recognition and localization. For instance, it is natural to\nexpect not to see an elephant to appear in the middle of an ocean. We consider\na simple approach to encapsulate such common sense knowledge using\nco-occurrence statistics from web documents. By merely counting the number of\ntimes nouns (such as elephants, sharks, oceans, etc.) co-occur in web\ndocuments, we obtain a good estimate of expected co-occurrences in visual data.\nWe then cast the problem of combining textual co-occurrence statistics with the\npredictions of image-based classifiers as an optimization problem. The\nresulting optimization problem serves as a surrogate for our inference\nprocedure. Albeit the simplicity of the resulting optimization problem, it is\neffective in improving both recognition and localization accuracy. Concretely,\nwe observe significant improvements in recognition and localization rates for\nboth ImageNet Detection 2012 and Sun 2012 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 18:53:47 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2013 18:12:16 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Bengio", "Samy", ""], ["Dean", "Jeff", ""], ["Erhan", "Dumitru", ""], ["Ie", "Eugene", ""], ["Le", "Quoc", ""], ["Rabinovich", "Andrew", ""], ["Shlens", "Jonathon", ""], ["Singer", "Yoram", ""]]}, {"id": "1312.5783", "submitter": "Yunlong He `", "authors": "Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi", "title": "Unsupervised Feature Learning by Deep Sparse Coding", "comments": "9 pages, submitted to ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new unsupervised feature learning framework,\nnamely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer\narchitecture for visual object recognition tasks. The main innovation of the\nframework is that it connects the sparse-encoders from different layers by a\nsparse-to-dense module. The sparse-to-dense module is a composition of a local\nspatial pooling step and a low-dimensional embedding process, which takes\nadvantage of the spatial smoothness information in the image. As a result, the\nnew method is able to learn several levels of sparse representation of the\nimage which capture features at a variety of abstraction levels and\nsimultaneously preserve the spatial smoothness between the neighboring image\npatches. Combining the feature representations from multiple layers, DeepSC\nachieves the state-of-the-art performance on multiple object recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 00:21:36 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["He", "Yunlong", ""], ["Kavukcuoglu", "Koray", ""], ["Wang", "Yun", ""], ["Szlam", "Arthur", ""], ["Qi", "Yanjun", ""]]}, {"id": "1312.5785", "submitter": "Du Tran", "authors": "Du Tran and Lorenzo Torresani", "title": "EXMOVES: Classifier-based Features for Scalable Action Recognition", "comments": "In Proceedings of the 2nd International Conference on Learning\n  Representations, Banff, Canada, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces EXMOVES, learned exemplar-based features for efficient\nrecognition of actions in videos. The entries in our descriptor are produced by\nevaluating a set of movement classifiers over spatial-temporal volumes of the\ninput sequence. Each movement classifier is a simple exemplar-SVM trained on\nlow-level features, i.e., an SVM learned using a single annotated positive\nspace-time volume and a large number of unannotated videos.\n  Our representation offers two main advantages. First, since our mid-level\nfeatures are learned from individual video exemplars, they require minimal\namount of supervision. Second, we show that simple linear classification models\ntrained on our global video descriptor yield action recognition accuracy\napproaching the state-of-the-art but at orders of magnitude lower cost, since\nat test-time no sliding window is necessary and linear models are efficient to\ntrain and test. This enables scalable action recognition, i.e., efficient\nclassification of a large number of different actions even in large video\ndatabases. We show the generality of our approach by building our mid-level\ndescriptors from two different low-level feature representations. The accuracy\nand efficiency of the approach are demonstrated on several large-scale action\nrecognition benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 00:37:16 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 19:58:48 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 01:55:14 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Tran", "Du", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1312.5845", "submitter": "Takashi Shinozaki", "authors": "Takashi Shinozaki and Yasushi Naruse", "title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained\n  Multilayered Networks", "comments": "This paper has been withdrawn by the author since the review process\n  for the conference to which it was applied ended", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning method for multilayered neural networks which\nuses feedforward supervisory signal and associates classification of a new\ninput with that of pre-trained input. The proposed method effectively uses rich\ninput information in the earlier layer for robust leaning and revising internal\nrepresentation in a multilayer neural network.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:24:48 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 08:59:56 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2014 11:09:07 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2014 19:11:14 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2014 02:11:46 GMT"}, {"version": "v6", "created": "Fri, 9 May 2014 00:50:33 GMT"}, {"version": "v7", "created": "Mon, 16 Feb 2015 09:37:18 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Shinozaki", "Takashi", ""], ["Naruse", "Yasushi", ""]]}, {"id": "1312.5851", "submitter": "Mikael Henaff", "authors": "Michael Mathieu, Mikael Henaff, Yann LeCun", "title": "Fast Training of Convolutional Networks through FFTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are one of the most widely employed architectures in\ncomputer vision and machine learning. In order to leverage their ability to\nlearn complex functions, large amounts of data are required for training.\nTraining a large convolutional network to produce state-of-the-art results can\ntake weeks, even when using modern GPUs. Producing labels using a trained\nnetwork can also be costly when dealing with web-scale datasets. In this work,\nwe present a simple algorithm which accelerates training and inference by a\nsignificant factor, and can yield improvements of over an order of magnitude\ncompared to existing state-of-the-art implementations. This is done by\ncomputing convolutions as pointwise products in the Fourier domain while\nreusing the same transformed feature map many times. The algorithm is\nimplemented on a GPU architecture and addresses a number of related challenges.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:42:21 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 00:28:06 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2014 01:33:21 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 03:20:51 GMT"}, {"version": "v5", "created": "Thu, 6 Mar 2014 23:27:18 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Mathieu", "Michael", ""], ["Henaff", "Mikael", ""], ["LeCun", "Yann", ""]]}, {"id": "1312.5940", "submitter": "Edouard Oyallon", "authors": "Edouard Oyallon, St\\'ephane Mallat, Laurent Sifre", "title": "Generic Deep Networks with Wavelet Scattering", "comments": "Workshop, 3 pages, prepared for ICLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a two-layer wavelet scattering network, for object\nclassification. This scattering transform computes a spatial wavelet transform\non the first layer and a new joint wavelet transform along spatial, angular and\nscale variables in the second layer. Numerical experiments demonstrate that\nthis two layer convolution network, which involves no learning and no max\npooling, performs efficiently on complex image data sets such as CalTech, with\nstructural objects variability and clutter. It opens the possibility to\nsimplify deep neural network learning by initializing the first layers with\nwavelet filters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 13:48:20 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 10:41:49 GMT"}, {"version": "v3", "created": "Mon, 10 Mar 2014 18:44:50 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Oyallon", "Edouard", ""], ["Mallat", "St\u00e9phane", ""], ["Sifre", "Laurent", ""]]}, {"id": "1312.6024", "submitter": "Yusuf Artan", "authors": "Yusuf Artan, Peter Paul", "title": "Occupancy Detection in Vehicles Using Fisher Vector Image Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high volume of traffic on modern roadways, transportation agencies\nhave proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling\n(HOT) lanes to promote car pooling. However, enforcement of the rules of these\nlanes is currently performed by roadside enforcement officers using visual\nobservation. Manual roadside enforcement is known to be inefficient, costly,\npotentially dangerous, and ultimately ineffective. Violation rates up to\n50%-80% have been reported, while manual enforcement rates of less than 10% are\ntypical. Therefore, there is a need for automated vehicle occupancy detection\nto support HOV/HOT lane enforcement. A key component of determining vehicle\noccupancy is to determine whether or not the vehicle's front passenger seat is\noccupied. In this paper, we examine two methods of determining vehicle front\nseat occupancy using a near infrared (NIR) camera system pointed at the\nvehicle's front windshield. The first method examines a state-of-the-art\ndeformable part model (DPM) based face detection system that is robust to\nfacial pose. The second method examines state-of- the-art local aggregation\nbased image classification using bag-of-visual-words (BOW) and Fisher vectors\n(FV). A dataset of 3000 images was collected on a public roadway and is used to\nperform the comparison. From these experiments it is clear that the image\nclassification approach is superior for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:37:46 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Artan", "Yusuf", ""], ["Paul", "Peter", ""]]}, {"id": "1312.6034", "submitter": "Karen Simonyan", "authors": "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman", "title": "Deep Inside Convolutional Networks: Visualising Image Classification\n  Models and Saliency Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013].\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:45:54 GMT"}, {"version": "v2", "created": "Sat, 19 Apr 2014 11:54:52 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Simonyan", "Karen", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1312.6077", "submitter": "Garrison Cottrell", "authors": "Honghao Shan, Garrison Cottrell", "title": "Efficient Visual Coding: From Retina To V2", "comments": "For the ICLR 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system has a hierarchical structure consisting of layers of\nprocessing, such as the retina, V1, V2, etc. Understanding the functional roles\nof these visual processing layers would help to integrate the\npsychophysiological and neurophysiological models into a consistent theory of\nhuman vision, and would also provide insights to computer vision research. One\nclassical theory of the early visual pathway hypothesizes that it serves to\ncapture the statistical structure of the visual inputs by efficiently coding\nthe visual information in its outputs. Until recently, most computational\nmodels following this theory have focused upon explaining the receptive field\nproperties of one or two visual layers. Recent work in deep networks has\neliminated this concern, however, there is till the retinal layer to consider.\nHere we improve on a previously-described hierarchical model Recursive ICA\n(RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA,\nfollowed by a component-wise nonlinearity derived from considerations of the\nvariable distributions expected by ICA. This process is then repeated. In this\nwork, we improve on this model by using a new version of sparse PCA (sPCA),\nwhich results in biologically-plausible receptive fields for both the sPCA and\nICA/sparse coding. When applied to natural image patches, our model learns\nvisual features exhibiting the receptive field properties of retinal ganglion\ncells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complex\ncells, and V2 cells. Our work provides predictions for experimental\nneuroscience studies. For example, our result suggests that a previous\nneurophysiological study improperly discarded some of their recorded neurons;\nwe predict that their discarded neurons capture the shape contour of objects.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 19:09:38 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 01:03:58 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Shan", "Honghao", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1312.6082", "submitter": "Julian Ibarz", "authors": "Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay\n  Shet", "title": "Multi-digit Number Recognition from Street View Imagery using Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing arbitrary multi-character text in unconstrained natural\nphotographs is a hard problem. In this paper, we address an equally hard\nsub-problem in this domain viz. recognizing arbitrary multi-digit numbers from\nStreet View imagery. Traditional approaches to solve this problem typically\nseparate out the localization, segmentation, and recognition steps. In this\npaper we propose a unified approach that integrates these three steps via the\nuse of a deep convolutional neural network that operates directly on the image\npixels. We employ the DistBelief implementation of deep neural networks in\norder to train large, distributed neural networks on high quality images. We\nfind that the performance of this approach increases with the depth of the\nconvolutional network, with the best performance occurring in the deepest\narchitecture we trained, with eleven hidden layers. We evaluate this approach\non the publicly available SVHN dataset and achieve over $96\\%$ accuracy in\nrecognizing complete street numbers. We show that on a per-digit recognition\ntask, we improve upon the state-of-the-art, achieving $97.84\\%$ accuracy. We\nalso evaluate this approach on an even more challenging dataset generated from\nStreet View imagery containing several tens of millions of street number\nannotations and achieve over $90\\%$ accuracy. To further explore the\napplicability of the proposed system to broader text recognition tasks, we\napply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the\nmost secure reverse turing tests that uses distorted text to distinguish humans\nfrom bots. We report a $99.8\\%$ accuracy on the hardest category of reCAPTCHA.\nOur evaluations on both tasks indicate that at specific operating thresholds,\nthe performance of the proposed system is comparable to, and in some cases\nexceeds, that of human operators.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 19:25:44 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 14:29:59 GMT"}, {"version": "v3", "created": "Tue, 11 Mar 2014 22:40:47 GMT"}, {"version": "v4", "created": "Mon, 14 Apr 2014 05:25:54 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Bulatov", "Yaroslav", ""], ["Ibarz", "Julian", ""], ["Arnoud", "Sacha", ""], ["Shet", "Vinay", ""]]}, {"id": "1312.6095", "submitter": "Bojan Pepikj", "authors": "Bojan Pepik, Michael Stark, Peter Gehler, Bernt Schiele", "title": "Multi-View Priors for Learning Detectors from Sparse Viewpoint Data", "comments": "13 pages, 7 figures, 4 tables, International Conference on Learning\n  Representations 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the majority of today's object class models provide only 2D bounding\nboxes, far richer output hypotheses are desirable including viewpoint,\nfine-grained category, and 3D geometry estimate. However, models trained to\nprovide richer output require larger amounts of training data, preferably well\ncovering the relevant aspects such as viewpoint and fine-grained categories. In\nthis paper, we address this issue from the perspective of transfer learning,\nand design an object class model that explicitly leverages correlations between\nvisual features. Specifically, our model represents prior distributions over\npermissible multi-view detectors in a parametric way -- the priors are learned\nonce from training data of a source object class, and can later be used to\nfacilitate the learning of a detector for a target class. As we show in our\nexperiments, this transfer is not only beneficial for detectors based on\nbasic-level category representations, but also enables the robust learning of\ndetectors that represent classes at finer levels of granularity, where training\ndata is typically even scarcer and more unbalanced. As a result, we report\nlargely improved performance in simultaneous 2D object localization and\nviewpoint estimation on a recent dataset of challenging street scenes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:12:07 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 10:39:35 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Pepik", "Bojan", ""], ["Stark", "Michael", ""], ["Gehler", "Peter", ""], ["Schiele", "Bernt", ""]]}, {"id": "1312.6110", "submitter": "Yichuan Tang", "authors": "Yichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov", "title": "Learning Generative Models with Visual Attention", "comments": "In the proceedings of Neural Information Processing Systems, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention has long been proposed by psychologists as important for\neffectively dealing with the enormous sensory stimulus available in the\nneocortex. Inspired by the visual attention models in computational\nneuroscience and the need of object-centric data for generative models, we\ndescribe for generative learning framework using attentional mechanisms.\nAttentional mechanisms can propagate signals from region of interest in a scene\nto an aligned canonical representation, where generative modeling takes place.\nBy ignoring background clutter, generative models can concentrate their\nresources on the object of interest. Our model is a proper graphical model\nwhere the 2D Similarity transformation is a part of the top-down process. A\nConvNet is employed to provide good initializations during posterior inference\nwhich is based on Hamiltonian Monte Carlo. Upon learning images of faces, our\nmodel can robustly attend to face regions of novel test subjects. More\nimportantly, our model can learn generative models of new faces from a novel\ndataset of large images where the face locations are not known.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:50:43 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 16:49:43 GMT"}, {"version": "v3", "created": "Sat, 21 Feb 2015 22:21:15 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Tang", "Yichuan", ""], ["Srivastava", "Nitish", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1312.6120", "submitter": "Andrew Saxe", "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear\n  neural networks", "comments": "Submission to ICLR2014. Revised based on reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.CV cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread practical success of deep learning methods, our\ntheoretical understanding of the dynamics of learning in deep neural networks\nremains quite sparse. We attempt to bridge the gap between the theory and\npractice of deep learning by systematically analyzing learning dynamics for the\nrestricted case of deep linear neural networks. Despite the linearity of their\ninput-output map, such networks have nonlinear gradient descent dynamics on\nweights that change with the addition of each new hidden layer. We show that\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid\ntransitions to lower error solutions, and faster convergence from greedy\nunsupervised pretraining initial conditions than from random initial\nconditions. We provide an analytical description of these phenomena by finding\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\nanalysis also reveals the surprising finding that as the depth of a network\napproaches infinity, learning speed can nevertheless remain finite: for a\nspecial class of initial conditions on the weights, very deep networks incur\nonly a finite, depth independent, delay in learning speed relative to shallow\nnetworks. We show that, under certain conditions on the training data,\nunsupervised pretraining can find this special class of initial conditions,\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\nclass of random orthogonal initial conditions on weights that, like\nunsupervised pre-training, enjoys depth independent learning times. We further\nshow that these initial conditions also lead to faithful propagation of\ngradients even in deep nonlinear networks, as long as they operate in a special\nregime known as the edge of chaos.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:24:00 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 20:39:04 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2014 17:26:57 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Saxe", "Andrew M.", ""], ["McClelland", "James L.", ""], ["Ganguli", "Surya", ""]]}, {"id": "1312.6150", "submitter": "Sudipta Roy", "authors": "Sudipta Roy, Sanjay Nag, Indra Kanta Maitra, Samir Kumar Bandyopadhyay", "title": "A Review on Automated Brain Tumor Detection and Segmentation from MRI of\n  Brain", "comments": "30 figures. arXiv admin note: text overlap with arXiv:1205.6572 by\n  other authors", "journal-ref": "International Journal of Advanced Research in Computer Science and\n  Software Engineering, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor segmentation from magnetic resonance imaging (MRI) data is an important\nbut time consuming manual task performed by medical experts. Automating this\nprocess is a challenging task because of the high diversity in the appearance\nof tumor tissues among different patients and in many cases similarity with the\nnormal tissues. MRI is an advanced medical imaging technique providing rich\ninformation about the human soft-tissue anatomy. There are different brain\ntumor detection and segmentation methods to detect and segment a brain tumor\nfrom MRI images. These detection and segmentation approaches are reviewed with\nan importance placed on enlightening the advantages and drawbacks of these\nmethods for brain tumor detection and segmentation. The use of MRI image\ndetection and segmentation in different procedures are also described. Here a\nbrief review of different segmentation for detection of brain tumor from MRI of\nbrain has been discussed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 17:56:11 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Roy", "Sudipta", ""], ["Nag", "Sanjay", ""], ["Maitra", "Indra Kanta", ""], ["Bandyopadhyay", "Samir Kumar", ""]]}, {"id": "1312.6158", "submitter": "Mohammad Pezeshki", "authors": "Mohammad Ali Keyvanrad, Mohammad Pezeshki, and Mohammad Ali\n  Homayounpour", "title": "Deep Belief Networks for Image Denoising", "comments": "ICLR 2014 Conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Belief Networks which are hierarchical generative models are effective\ntools for feature representation and extraction. Furthermore, DBNs can be used\nin numerous aspects of Machine Learning such as image denoising. In this paper,\nwe propose a novel method for image denoising which relies on the DBNs' ability\nin feature representation. This work is based upon learning of the noise\nbehavior. Generally, features which are extracted using DBNs are presented as\nthe values of the last layer nodes. We train a DBN a way that the network\ntotally distinguishes between nodes presenting noise and nodes presenting image\ncontent in the last later of DBN, i.e. the nodes in the last layer of trained\nDBN are divided into two distinct groups of nodes. After detecting the nodes\nwhich are presenting the noise, we are able to make the noise nodes inactive\nand reconstruct a noiseless image. In section 4 we explore the results of\napplying this method on the MNIST dataset of handwritten digits which is\ncorrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in\naverage mean square error (MSE) was achieved when the proposed method was used\nfor the reconstruction of the noisy images.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:56:38 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 17:04:35 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Keyvanrad", "Mohammad Ali", ""], ["Pezeshki", "Mohammad", ""], ["Homayounpour", "Mohammad Ali", ""]]}, {"id": "1312.6159", "submitter": "John Bogovic", "authors": "John A. Bogovic, Gary B. Huang, Viren Jain", "title": "Learned versus Hand-Designed Feature Representations for 3d\n  Agglomeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For image recognition and labeling tasks, recent results suggest that machine\nlearning methods that rely on manually specified feature representations may be\noutperformed by methods that automatically derive feature representations based\non the data. Yet for problems that involve analysis of 3d objects, such as mesh\nsegmentation, shape retrieval, or neuron fragment agglomeration, there remains\na strong reliance on hand-designed feature descriptors. In this paper, we\nevaluate a large set of hand-designed 3d feature descriptors alongside features\nlearned from the raw data using both end-to-end and unsupervised learning\ntechniques, in the context of agglomeration of 3d neuron fragments. By\ncombining unsupervised learning techniques with a novel dynamic pooling scheme,\nwe show how pure learning-based methods are for the first time competitive with\nhand-designed 3d shape descriptors. We investigate data augmentation strategies\nfor dramatically increasing the size of the training set, and show how\ncombining both learned and hand-designed features leads to the highest\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:59:03 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Bogovic", "John A.", ""], ["Huang", "Gary B.", ""], ["Jain", "Viren", ""]]}, {"id": "1312.6171", "submitter": "Daniel Silver Dr.", "authors": "Ti Wang and Daniel L. Silver", "title": "Learning Paired-associate Images with An Unsupervised Deep Learning\n  Architecture", "comments": "9 pages, for ICLR-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised multi-modal learning system that learns\nassociative representation from two input modalities, or channels, such that\ninput on one channel will correctly generate the associated response at the\nother and vice versa. In this way, the system develops a kind of supervised\nclassification model meant to simulate aspects of human associative memory. The\nsystem uses a deep learning architecture (DLA) composed of two input/output\nchannels formed from stacked Restricted Boltzmann Machines (RBM) and an\nassociative memory network that combines the two channels. The DLA is trained\non pairs of MNIST handwritten digit images to develop hierarchical features and\nassociative representations that are able to reconstruct one image given its\npaired-associate. Experiments show that the multi-modal learning system\ngenerates models that are as accurate as back-propagation networks but with the\nadvantage of a bi-directional network and unsupervised learning from either\npaired or non-paired training examples.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 23:07:25 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 23:19:26 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Wang", "Ti", ""], ["Silver", "Daniel L.", ""]]}, {"id": "1312.6186", "submitter": "Tom Paine", "authors": "Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang", "title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network\n  Training", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to train large-scale neural networks has resulted in\nstate-of-the-art performance in many areas of computer vision. These results\nhave largely come from computational break throughs of two forms: model\nparallelism, e.g. GPU accelerated training, which has seen quick adoption in\ncomputer vision circles, and data parallelism, e.g. A-SGD, whose large scale\nhas been used mostly in industry. We report early experiments with a system\nthat makes use of both model parallelism and data parallelism, we call GPU\nA-SGD. We show using GPU A-SGD it is possible to speed up training of large\nconvolutional neural networks useful for computer vision. We believe GPU A-SGD\nwill make it possible to train larger networks on larger training sets in a\nreasonable amount of time.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:56:56 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Paine", "Thomas", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""], ["Lin", "Zhe", ""], ["Huang", "Thomas", ""]]}, {"id": "1312.6199", "submitter": "Joan Bruna", "authors": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\n  Dumitru Erhan, Ian Goodfellow, Rob Fergus", "title": "Intriguing properties of neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 03:36:08 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2014 04:37:34 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2014 17:40:08 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2014 16:33:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Szegedy", "Christian", ""], ["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""], ["Bruna", "Joan", ""], ["Erhan", "Dumitru", ""], ["Goodfellow", "Ian", ""], ["Fergus", "Rob", ""]]}, {"id": "1312.6203", "submitter": "Joan Bruna", "authors": "Joan Bruna, Wojciech Zaremba, Arthur Szlam and Yann LeCun", "title": "Spectral Networks and Locally Connected Networks on Graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are extremely efficient architectures in image\nand audio recognition tasks, thanks to their ability to exploit the local\ntranslational invariance of signal classes over their domain. In this paper we\nconsider possible generalizations of CNNs to signals defined on more general\ndomains without the action of a translation group. In particular, we propose\ntwo constructions, one based upon a hierarchical clustering of the domain, and\nanother based on the spectrum of the graph Laplacian. We show through\nexperiments that for low-dimensional graphs it is possible to learn\nconvolutional layers with a number of parameters independent of the input size,\nresulting in efficient deep architectures.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 04:25:53 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 23:23:06 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 16:27:09 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Bruna", "Joan", ""], ["Zaremba", "Wojciech", ""], ["Szlam", "Arthur", ""], ["LeCun", "Yann", ""]]}, {"id": "1312.6204", "submitter": "Judy Hoffman", "authors": "Judy Hoffman, Eric Tzeng, Jeff Donahue, Yangqing Jia, Kate Saenko,\n  Trevor Darrell", "title": "One-Shot Adaptation of Supervised Deep Convolutional Models", "comments": null, "journal-ref": "ICLR Workshop 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset bias remains a significant barrier towards solving real world\ncomputer vision tasks. Though deep convolutional networks have proven to be a\ncompetitive approach for image classification, a question remains: have these\nmodels have solved the dataset bias problem? In general, training or\nfine-tuning a state-of-the-art deep model on a new domain requires a\nsignificant amount of data, which for many applications is simply not\navailable. Transfer of models directly to new domains without adaptation has\nhistorically led to poor recognition performance. In this paper, we pose the\nfollowing question: is a single image dataset, much larger than previously\nexplored for adaptation, comprehensive enough to learn general deep models that\nmay be effectively applied to new image domains? In other words, are deep CNNs\ntrained on large amounts of labeled data as susceptible to dataset bias as\nprevious methods have been shown to be? We show that a generic supervised deep\nCNN model trained on a large dataset reduces, but does not remove, dataset\nbias. Furthermore, we propose several methods for adaptation with deep models\nthat are able to operate with little (one example per category) or no labeled\ndomain specific data. Our experiments show that adaptation of deep models on\nbenchmark visual domain adaptation datasets can provide a significant\nperformance boost.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 04:32:51 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 02:57:42 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Hoffman", "Judy", ""], ["Tzeng", "Eric", ""], ["Donahue", "Jeff", ""], ["Jia", "Yangqing", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1312.6208", "submitter": "Gang Liu", "authors": "Gang Liu, Ting-Zhu Huang, Jun Liu, Xiao-Guang Lv", "title": "Total variation with overlapping group sparsity for image deblurring\n  under impulse noise", "comments": "22 pages, 57 figures, submitted", "journal-ref": "PLOS ONE 2015 10(4): e0122562", "doi": "10.1371/journal.pone.0122562", "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation (TV) regularization method is an effective method for\nimage deblurring in preserving edges. However, the TV based solutions usually\nhave some staircase effects. In this paper, in order to alleviate the staircase\neffect, we propose a new model for restoring blurred images with impulse noise.\nThe model consists of an $\\ell_1$-fidelity term and a TV with overlapping group\nsparsity (OGS) regularization term. Moreover, we impose a box constraint to the\nproposed model for getting more accurate solutions. An efficient and effective\nalgorithm is proposed to solve the model under the framework of the alternating\ndirection method of multipliers (ADMM). We use an inner loop which is nested\ninside the majorization minimization (MM) iteration for the subproblem of the\nproposed method. Compared with other methods, numerical results illustrate that\nthe proposed method, can significantly improve the restoration quality, both in\navoiding staircase effects and in terms of peak signal-to-noise ratio (PSNR)\nand relative error (ReE).\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 05:40:27 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Liu", "Gang", ""], ["Huang", "Ting-Zhu", ""], ["Liu", "Jun", ""], ["Lv", "Xiao-Guang", ""]]}, {"id": "1312.6219", "submitter": "Kasturika B Ray", "authors": "Kasturika B. Ray", "title": "Extracting Region of Interest for Palm Print Authentication", "comments": "8 pages,4 figures, 1 table, 1 photo of author and 1 photo of\n  co-author, 3 paper has published (2011, 2011, 2012)", "journal-ref": "IJASCSE Journal, Volume 2, Issue 6, December2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometrics authentication is an effective method for automatically\nrecognizing individuals. The authentication consists of an enrollment phase and\nan identification or verification phase. In the stages of enrollment known\n(training) samples after the pre-processing stage are used for suitable feature\nextraction to generate the template database. In the verification stage, the\ntest sample is similarly pre processed and subjected to feature extraction\nmodules, and then it is matched with the training feature templates to decide\nwhether it is a genuine or not. This paper presents use of a region of interest\n(ROI) for palm print technology. First some of the existing methods for palm\nprint identification have been introduced. Then focus has been given on\nextraction of a suitable smaller region from the acquired palm print to improve\nthe identification method accuracy. Several existing work in the topic of\nregion extraction have been examined. Subsequently, a simple and original\nmethod has then proposed for locating the ROI that can be effectively used for\npalm print analysis. The ROI extracted using this new technique is suitable for\ndifferent types of processing as it creates a rectangular or square area around\nthe center of activity represented by the lines, wrinkles and ridges of the\npalm print. The effectiveness of the ROI approach has been tested by\nintegrating it with a texture based identification / authentication system\nproposed earlier. The improvement has been shown by comparing the\nidentification accuracy rate before and after the ROI pre-processing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 07:55:08 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Ray", "Kasturika B.", ""]]}, {"id": "1312.6229", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob\n  Fergus, Yann LeCun", "title": "OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integrated framework for using Convolutional Networks for\nclassification, localization and detection. We show how a multiscale and\nsliding window approach can be efficiently implemented within a ConvNet. We\nalso introduce a novel deep learning approach to localization by learning to\npredict object boundaries. Bounding boxes are then accumulated rather than\nsuppressed in order to increase detection confidence. We show that different\ntasks can be learned simultaneously using a single shared network. This\nintegrated framework is the winner of the localization task of the ImageNet\nLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very\ncompetitive results for the detection and classifications tasks. In\npost-competition work, we establish a new state of the art for the detection\ntask. Finally, we release a feature extractor from our best model called\nOverFeat.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 09:52:33 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 20:16:54 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 02:16:09 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2014 03:38:17 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Sermanet", "Pierre", ""], ["Eigen", "David", ""], ["Zhang", "Xiang", ""], ["Mathieu", "Michael", ""], ["Fergus", "Rob", ""], ["LeCun", "Yann", ""]]}, {"id": "1312.6370", "submitter": "Deepak Nayak Ranjan", "authors": "Jahangir Mohammed and Deepak Ranjan Nayak", "title": "An Efficient Edge Detection Technique by Two Dimensional Rectangular\n  Cellular Automata", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new pattern of two dimensional cellular automata linear\nrules that are used for efficient edge detection of an image. Since cellular\nautomata is inherently parallel in nature, it has produced desired output\nwithin a unit time interval. We have observed four linear rules among 512 total\nlinear rules of a rectangular cellular automata in adiabatic or reflexive\nboundary condition that produces an optimal result. These four rules are\ndirectly applied once to the images and produced edge detected output. We\ncompare our results with the existing edge detection algorithms and found that\nour results shows better edge detection with an enhancement of edges.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 12:08:20 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Mohammed", "Jahangir", ""], ["Nayak", "Deepak Ranjan", ""]]}, {"id": "1312.6410", "submitter": "H.R.  Chennamma", "authors": "H.R. Chennamma and Xiaohui Yuan", "title": "A Survey on Eye-Gaze Tracking Techniques", "comments": "6 pages, Journal", "journal-ref": "Indian Journal of Computer Science and Engineering, ISSN :\n  0976-5166, Vol. 4, No. 5, Oct-Nov 2013, pp. 388-393", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of eye-movement is being employed in Human Computer Interaction (HCI)\nresearch. Eye - gaze tracking is one of the most challenging problems in the\narea of computer vision. The goal of this paper is to present a review of\nlatest research in this continued growth of remote eye-gaze tracking. This\noverview includes the basic definitions and terminologies, recent advances in\nthe field and finally the need of future development in the field.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 18:04:54 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Chennamma", "H. R.", ""], ["Yuan", "Xiaohui", ""]]}, {"id": "1312.6430", "submitter": "Kota Hara", "authors": "Kota Hara and Rama Chellappa", "title": "Growing Regression Forests by Classification: Applications to Object\n  Pose Estimation", "comments": "Paper accepted by ECCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel node splitting method for regression trees\nand incorporate it into the regression forest framework. Unlike traditional\nbinary splitting, where the splitting rule is selected from a predefined set of\nbinary splitting rules via trial-and-error, the proposed node splitting method\nfirst finds clusters of the training data which at least locally minimize the\nempirical loss without considering the input space. Then splitting rules which\npreserve the found clusters as much as possible are determined by casting the\nproblem into a classification problem. Consequently, our new node splitting\nmethod enjoys more freedom in choosing the splitting rules, resulting in more\nefficient tree structures. In addition to the Euclidean target space, we\npresent a variant which can naturally deal with a circular target space by the\nproper use of circular statistics. We apply the regression forest employing our\nnode splitting to head pose estimation (Euclidean target space) and car\ndirection estimation (circular target space) and demonstrate that the proposed\nmethod significantly outperforms state-of-the-art methods (38.5% and 22.5%\nerror reduction respectively).\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 22:10:42 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 02:51:13 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Hara", "Kota", ""], ["Chellappa", "Rama", ""]]}, {"id": "1312.6506", "submitter": "Prateek  Singhal", "authors": "Prateek Singhal, Aditya Deshpande, N Dinesh Reddy and K Madhava\n  Krishna", "title": "Top Down Approach to Multiple Plane Detection", "comments": "6 pages, 22 figures, ICPR conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting multiple planes in images is a challenging problem, but one with\nmany applications. Recent work such as J-Linkage and Ordered Residual Kernels\nhave focussed on developing a domain independent approach to detect multiple\nstructures. These multiple structure detection methods are then used for\nestimating multiple homographies given feature matches between two images.\nFeatures participating in the multiple homographies detected, provide us the\nmultiple scene planes. We show that these methods provide locally optimal\nresults and fail to merge detected planar patches to the true scene planes.\nThese methods use only residues obtained on applying homography of one plane to\nanother as cue for merging. In this paper, we develop additional cues such as\nlocal consistency of planes, local normals, texture etc. to perform better\nclassification and merging . We formulate the classification as an MRF problem\nand use TRWS message passing algorithm to solve non metric energy terms and\ncomplex sparse graph structure. We show results on challenging dataset common\nin robotics navigation scenarios where our method shows accuracy of more than\n85 percent on average while being close or same as the actual number of scene\nplanes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 10:09:12 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2013 04:35:01 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Singhal", "Prateek", ""], ["Deshpande", "Aditya", ""], ["Reddy", "N Dinesh", ""], ["Krishna", "K Madhava", ""]]}, {"id": "1312.6594", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold and Ludovic Denoyer and Nicolas Thome and\n  Matthieu Cord and Patrick Gallinari", "title": "Sequentially Generated Instance-Dependent Image Representations for\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new framework for image classification that\nadaptively generates spatial representations. Our strategy is based on a\nsequential process that learns to explore the different regions of any image in\norder to infer its category. In particular, the choice of regions is specific\nto each image, directed by the actual content of previously selected\nregions.The capacity of the system to handle incomplete image information as\nwell as its adaptive region selection allow the system to perform well in\nbudgeted classification tasks by exploiting a dynamicly generated\nrepresentation of each image. We demonstrate the system's abilities in a series\nof image-based exploration and classification tasks that highlight its learned\nexploration and inference abilities.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:36:40 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 14:44:42 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 17:07:21 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Denoyer", "Ludovic", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1312.6599", "submitter": "Shatrughan Modi", "authors": "Shatrughan Modi and Dr. Seema Bawa", "title": "Image Processing based Systems and Techniques for the Recognition of\n  Ancient and Modern Coins", "comments": "5 pages, 1 table, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 47(10):1-5, June\n  2012", "doi": "10.5120/7221-0041", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coins are frequently used in everyday life at various places like in banks,\ngrocery stores, supermarkets, automated weighing machines, vending machines\netc. So, there is a basic need to automate the counting and sorting of coins.\nFor this machines need to recognize the coins very fast and accurately, as\nfurther transaction processing depends on this recognition. Three types of\nsystems are available in the market: Mechanical method based systems,\nElectromagnetic method based systems and Image processing based systems. This\npaper presents an overview of available systems and techniques based on image\nprocessing to recognize ancient and modern coins.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:00:02 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Modi", "Shatrughan", ""], ["Bawa", "Dr. Seema", ""]]}, {"id": "1312.6615", "submitter": "Shatrughan Modi", "authors": "Shatrughan Modi and Dr. Seema Bawa", "title": "Automated Coin Recognition System using ANN", "comments": "6 pages, 11 figures, 1 table, Published with International Journal of\n  Computer Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 26(4):13-18, July\n  2011", "doi": "10.5120/3093-4244", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coins are integral part of our day to day life. We use coins everywhere like\ngrocery store, banks, buses, trains etc. So it becomes a basic need that coins\ncan be sorted and counted automatically. For this it is necessary that coins\ncan be recognized automatically. In this paper we have developed an ANN\n(Artificial Neural Network) based Automated Coin Recognition System for the\nrecognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotation\ninvariance. We have taken images from both sides of coin. So this system is\ncapable of recognizing coins from both sides. Features are extracted from\nimages using techniques of Hough Transformation, Pattern Averaging etc. Then,\nthe extracted features are passed as input to a trained Neural Network. 97.74%\nrecognition rate has been achieved during the experiments i.e. only 2.26% miss\nrecognition, which is quite encouraging.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:40:08 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Modi", "Shatrughan", ""], ["Bawa", "Dr. Seema", ""]]}, {"id": "1312.6782", "submitter": "Avinash Bhute", "authors": "Avinash N Bhute, B. B. Meshram", "title": "IVSS Integration of Color Feature Extraction Techniques for Intelligent\n  Video Search Systems", "comments": "5 pages, 9 figures. 2012 4th International Conference on Electronics\n  Computer Technology - ICECT 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As large amount of visual Information is available on web in form of images,\ngraphics, animations and videos, so it is important in internet era to have an\neffective video search system. As there are number of video search engine\n(blinkx, Videosurf, Google, YouTube, etc.) which search for relevant videos\nbased on user keyword or term, But very less commercial video search engine are\navailable which search videos based on visual image/clip/video. In this paper\nwe are recommending a system that will search for relevant video using color\nfeature of video in response of user Query.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 09:28:08 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bhute", "Avinash N", ""], ["Meshram", "B. B.", ""]]}, {"id": "1312.6813", "submitter": "Gang Liu", "authors": "Gang Liu, Ting-Zhu Huang, Xiao-Guang Lv, Jun Liu", "title": "New explicit thresholding/shrinkage formulas for one class of\n  regularization problems with overlapping group sparsity and their\n  applications", "comments": "22 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least-square regression problems or inverse problems have been widely\nstudied in many fields such as compressive sensing, signal processing, and\nimage processing. To solve this kind of ill-posed problems, a regularization\nterm (i.e., regularizer) should be introduced, under the assumption that the\nsolutions have some specific properties, such as sparsity and group sparsity.\nWidely used regularizers include the $\\ell_1$ norm, total variation (TV)\nsemi-norm, and so on.\n  Recently, a new regularization term with overlapping group sparsity has been\nconsidered. Majorization minimization iteration method or variable duplication\nmethods are often applied to solve them. However, there have been no direct\nmethods for solve the relevant problems because of the difficulty of\noverlapping. In this paper, we proposed new explicit shrinkage formulas for one\nclass of these relevant problems, whose regularization terms have translation\ninvariant overlapping groups. Moreover, we apply our results in TV deblurring\nand denoising with overlapping group sparsity. We use alternating direction\nmethod of multipliers to iterate solve it. Numerical results also verify the\nvalidity and effectiveness of our new explicit shrinkage formulas.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 13:31:45 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2013 07:22:53 GMT"}, {"version": "v3", "created": "Fri, 9 May 2014 05:36:44 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Liu", "Gang", ""], ["Huang", "Ting-Zhu", ""], ["Lv", "Xiao-Guang", ""], ["Liu", "Jun", ""]]}, {"id": "1312.6826", "submitter": "Leizer Teran", "authors": "Leizer Teran, Philippos Mordohai", "title": "3D Interest Point Detection via Discriminative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of detecting the interest points in 3D meshes has typically been\nhandled by geometric methods. These methods, while greatly describing human\npreference, can be ill-equipped for handling the variety and subjectivity in\nhuman responses. Different tasks have different requirements for interest point\ndetection; some tasks may necessitate high precision while other tasks may\nrequire high recall. Sometimes points with high curvature may be desirable,\nwhile in other cases high curvature may be an indication of noise. Geometric\nmethods lack the required flexibility to adapt to such changes. As a\nconsequence, interest point detection seems to be well suited for machine\nlearning methods that can be trained to match the criteria applied on the\nannotated training data. In this paper, we formulate interest point detection\nas a supervised binary classification problem using a random forest as our\nclassifier. Among other challenges, we are faced with an imbalanced learning\nproblem due to the substantial difference in the priors between interest and\nnon-interest points. We address this by re-sampling the training set. We\nvalidate the accuracy of our method and compare our results to those of five\nstate of the art methods on a new, standard benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 14:35:15 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Teran", "Leizer", ""], ["Mordohai", "Philippos", ""]]}, {"id": "1312.6834", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "P. Kiran Sree, I. Ramesh Babu", "title": "Face Detection from still and Video Images using Unsupervised Cellular\n  Automata with K means clustering algorithm", "comments": "ICGST-GVIP Journal, ISSN: 1687-398X, Volume 8, Issue 2, July 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition problem rely upon the features inherent in the pattern of\nimages. Face detection and recognition is one of the challenging research areas\nin the field of computer vision. In this paper, we present a method to identify\nskin pixels from still and video images using skin color. Face regions are\nidentified from this skin pixel region. Facial features such as eyes, nose and\nmouth are then located. Faces are recognized from color images using an RBF\nbased neural network. Unsupervised Cellular Automata with K means clustering\nalgorithm is used to locate different facial elements. Orientation is corrected\nby using eyes. Parameters like inter eye distance, nose length, mouth position,\nDiscrete Cosine Transform (DCT) coefficients etc. are computed and used for a\nRadial Basis Function (RBF) based neural network. This approach reliably works\nfor face sequence with orientation in head, expressions etc.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 15:43:52 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Sree", "P. Kiran", ""], ["Babu", "I. Ramesh", ""]]}, {"id": "1312.6849", "submitter": "Zoran Cvetkovic", "authors": "Matthew Ager and Zoran Cvetkovic and Peter Sollich", "title": "Speech Recognition Front End Without Information Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech representation and modelling in high-dimensional spaces of acoustic\nwaveforms, or a linear transformation thereof, is investigated with the aim of\nimproving the robustness of automatic speech recognition to additive noise. The\nmotivation behind this approach is twofold: (i) the information in acoustic\nwaveforms that is usually removed in the process of extracting low-dimensional\nfeatures might aid robust recognition by virtue of structured redundancy\nanalogous to channel coding, (ii) linear feature domains allow for exact noise\nadaptation, as opposed to representations that involve non-linear processing\nwhich makes noise adaptation challenging. Thus, we develop a generative\nframework for phoneme modelling in high-dimensional linear feature domains, and\nuse it in phoneme classification and recognition tasks. Results show that\nclassification and recognition in this framework perform better than analogous\nPLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional\nand MFCC features at the likelihood level performs uniformly better than either\nof the individual representations across all noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 16:36:16 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 09:17:46 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ager", "Matthew", ""], ["Cvetkovic", "Zoran", ""], ["Sollich", "Peter", ""]]}, {"id": "1312.6885", "submitter": "Brody Huval", "authors": "Brody Huval, Adam Coates, Andrew Ng", "title": "Deep learning for class-generic object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of deep neural networks for the novel task of class\ngeneric object detection. We show that neural networks originally designed for\nimage recognition can be trained to detect objects within images, regardless of\ntheir class, including objects for which no bounding box labels have been\nprovided. In addition, we show that bounding box labels yield a 1% performance\nincrease on the ImageNet recognition challenge.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 20:38:18 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Huval", "Brody", ""], ["Coates", "Adam", ""], ["Ng", "Andrew", ""]]}, {"id": "1312.6965", "submitter": "Faicel Chamroukhi", "authors": "Dorra Trabelsi, Samer Mohammed, Faicel Chamroukhi, Latifa Oukhellou,\n  Yacine Amirat", "title": "An Unsupervised Approach for Automatic Activity Recognition based on\n  Hidden Markov Model Regression", "comments": null, "journal-ref": "IEEE Transactions on Automation Science and Engineering, Volume:\n  10, Issue: 3, July 2013, Pages: 829-835", "doi": "10.1109/TASE.2013.2256349", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using supervised machine learning approaches to recognize human activities\nfrom on-body wearable accelerometers generally requires a large amount of\nlabelled data. When ground truth information is not available, too expensive,\ntime consuming or difficult to collect, one has to rely on unsupervised\napproaches. This paper presents a new unsupervised approach for human activity\nrecognition from raw acceleration data measured using inertial wearable\nsensors. The proposed method is based upon joint segmentation of\nmultidimensional time series using a Hidden Markov Model (HMM) in a multiple\nregression context. The model is learned in an unsupervised framework using the\nExpectation-Maximization (EM) algorithm where no activity labels are needed.\nThe proposed method takes into account the sequential appearance of the data.\nIt is therefore adapted for the temporal acceleration data to accurately detect\nthe activities. It allows both segmentation and classification of the human\nactivities. Experimental results are provided to demonstrate the efficiency of\nthe proposed approach with respect to standard supervised and unsupervised\nclassification approaches\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:03:12 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Trabelsi", "Dorra", ""], ["Mohammed", "Samer", ""], ["Chamroukhi", "Faicel", ""], ["Oukhellou", "Latifa", ""], ["Amirat", "Yacine", ""]]}, {"id": "1312.7085", "submitter": "Peng Lu", "authors": "Peng Lu, Xujun Peng, Xinshan Zhu, Xiaojie Wang", "title": "Finding More Relevance: Propagating Similarity on Markov Random Field\n  for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To effectively retrieve objects from large corpus with high accuracy is a\nchallenge task. In this paper, we propose a method that propagates visual\nfeature level similarities on a Markov random field (MRF) to obtain a high\nlevel correspondence in image space for image pairs. The proposed\ncorrespondence between image pair reflects not only the similarity of low-level\nvisual features but also the relations built through other images in the\ndatabase and it can be easily integrated into the existing\nbag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate\nour method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. The\nexperiment results show that the proposed method significantly improves the\nretrieval accuracy on three datasets and exceeds the current state-of-the-art\nretrieval performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 10:55:14 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Lu", "Peng", ""], ["Peng", "Xujun", ""], ["Zhu", "Xinshan", ""], ["Wang", "Xiaojie", ""]]}, {"id": "1312.7167", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Vikas Sindhwani", "title": "Near-separable Non-negative Matrix Factorization with $\\ell_1$- and\n  Bregman Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a family of tractable NMF algorithms have been proposed under the\nassumption that the data matrix satisfies a separability condition Donoho &\nStodden (2003); Arora et al. (2012). Geometrically, this condition reformulates\nthe NMF problem as that of finding the extreme rays of the conical hull of a\nfinite set of vectors. In this paper, we develop several extensions of the\nconical hull procedures of Kumar et al. (2013) for robust ($\\ell_1$)\napproximations and Bregman divergences. Our methods inherit all the advantages\nof Kumar et al. (2013) including scalability and noise-tolerance. We show that\non foreground-background separation problems in computer vision, robust\nnear-separable NMFs match the performance of Robust PCA, considered state of\nthe art on these problems, with an order of magnitude faster training time. We\nalso demonstrate applications in exemplar selection settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 01:10:00 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1312.7219", "submitter": "Patrizio Frosini", "authors": "Patrizio Frosini and Grzegorz Jablonski", "title": "Combining persistent homology and invariance groups for shape comparison", "comments": "33 pages, 12 figures, 1 table; corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications concerning the comparison of data expressed by\n$\\mathbb{R}^m$-valued functions defined on a topological space $X$, the\ninvariance with respect to a given group $G$ of self-homeomorphisms of $X$ is\nrequired. While persistent homology is quite efficient in the topological and\nqualitative comparison of this kind of data when the invariance group $G$ is\nthe group $\\mathrm{Homeo}(X)$ of all self-homeomorphisms of $X$, this theory is\nnot tailored to manage the case in which $G$ is a proper subgroup of\n$\\mathrm{Homeo}(X)$, and its invariance appears too general for several tasks.\nThis paper proposes a way to adapt persistent homology in order to get\ninvariance just with respect to a given group of self-homeomorphisms of $X$.\nThe main idea consists in a dual approach, based on considering the set of all\n$G$-invariant non-expanding operators defined on the space of the admissible\nfiltering functions on $X$. Some theoretical results concerning this approach\nare proven and two experiments are presented. An experiment illustrates the\napplication of the proposed technique to compare 1D-signals, when the\ninvariance is expressed by the group of affinities, the group of\norientation-preserving affinities, the group of isometries, the group of\ntranslations and the identity group. Another experiment shows how our technique\ncan be used for image comparison.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 09:09:36 GMT"}, {"version": "v2", "created": "Sat, 20 Sep 2014 10:52:41 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 09:03:17 GMT"}, {"version": "v4", "created": "Thu, 28 Jan 2016 10:13:00 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Frosini", "Patrizio", ""], ["Jablonski", "Grzegorz", ""]]}, {"id": "1312.7302", "submitter": "Arjun Jain", "authors": "Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor,\n  Christoph Bregler", "title": "Learning Human Pose Estimation Features with Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "NYU-TR-2013-CS0999", "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new architecture for human pose estimation using a\nmulti- layer convolutional network architecture and a modified learning\ntechnique that learns low-level features and higher-level weak spatial models.\nUnconstrained human pose estimation is one of the hardest problems in computer\nvision, and our new architecture and learning schema shows significant\nimprovement over the current state-of-the-art results. The main contribution of\nthis paper is showing, for the first time, that a specific variation of deep\nlearning is able to outperform all existing traditional architectures on this\ntask. The paper also discusses several lessons learned while researching\nalternatives, most notably, that it is possible to learn strong low-level\nfeature detectors on features that might even just cover a few pixels in the\nimage. Higher-level spatial models improve somewhat the overall result, but to\na much lesser extent then expected. Many researchers previously argued that the\nkinematic structure and top-down information is crucial for this domain, but\nwith our purely bottom up, and weak spatial model, we could improve other more\ncomplicated architectures that currently produce the best results. This mirrors\nwhat many other researchers, like those in the speech recognition, object\nrecognition, and other domains have experienced.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 17:41:13 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 04:29:34 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 20:56:34 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 16:22:38 GMT"}, {"version": "v5", "created": "Tue, 25 Feb 2014 05:32:32 GMT"}, {"version": "v6", "created": "Wed, 23 Apr 2014 19:23:46 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Jain", "Arjun", ""], ["Tompson", "Jonathan", ""], ["Andriluka", "Mykhaylo", ""], ["Taylor", "Graham W.", ""], ["Bregler", "Christoph", ""]]}, {"id": "1312.7335", "submitter": "Balazs Kegl", "authors": "Bal\\'azs K\\'egl", "title": "Correlation-based construction of neighborhood and edge features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an abstract notion of low-level edge detector filters, we\npropose a simple method of unsupervised feature construction based on pairwise\nstatistics of features. In the first step, we construct neighborhoods of\nfeatures by regrouping features that correlate. Then we use these subsets as\nfilters to produce new neighborhood features. Next, we connect neighborhood\nfeatures that correlate, and construct edge features by subtracting the\ncorrelated neighborhood features of each other. To validate the usefulness of\nthe constructed features, we ran AdaBoost.MH on four multi-class classification\nproblems. Our most significant result is a test error of 0.94% on MNIST with an\nalgorithm which is essentially free of any image-specific priors. On CIFAR-10\nour method is suboptimal compared to today's best deep learning techniques,\nnevertheless, we show that the proposed method outperforms not only boosting on\nthe raw pixels, but also boosting on Haar filters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 19:36:51 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 23:17:39 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1312.7345", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi, Quan Wen, Sae Hwang, Hitoshi Iyatomi, Gerald Schaefer", "title": "Lesion Border Detection in Dermoscopy Images Using Ensembles of\n  Thresholding Methods", "comments": "8 pages, 3 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1009.1362", "journal-ref": "Skin Research and Technology 19 (2013) e252--e258", "doi": "10.1111/j.1600-0846.2012.00636.x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dermoscopy is one of the major imaging modalities used in the diagnosis of\nmelanoma and other pigmented skin lesions. Due to the difficulty and\nsubjectivity of human interpretation, automated analysis of dermoscopy images\nhas become an important research area. Border detection is often the first step\nin this analysis. In many cases, the lesion can be roughly separated from the\nbackground skin using a thresholding method applied to the blue channel.\nHowever, no single thresholding method appears to be robust enough to\nsuccessfully handle the wide variety of dermoscopy images encountered in\nclinical practice. In this paper, we present an automated method for detecting\nlesion borders in dermoscopy images using ensembles of thresholding methods.\nExperiments on a difficult set of 90 images demonstrate that the proposed\nmethod is robust, fast, and accurate when compared to nine state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 14:55:19 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Celebi", "M. Emre", ""], ["Wen", "Quan", ""], ["Hwang", "Sae", ""], ["Iyatomi", "Hitoshi", ""], ["Schaefer", "Gerald", ""]]}, {"id": "1312.7366", "submitter": "Stanley Chan", "authors": "Stanley H. Chan, Todd Zickler, and Yue M. Lu", "title": "Monte Carlo non local means: Random sampling for large-scale image\n  filtering", "comments": "submitted for publication", "journal-ref": null, "doi": "10.1109/TIP.2014.2327813", "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized version of the non-local means (NLM) algorithm for\nlarge-scale image filtering. The new algorithm, called Monte Carlo non-local\nmeans (MCNLM), speeds up the classical NLM by computing a small subset of image\npatch distances, which are randomly selected according to a designed sampling\npattern. We make two contributions. First, we analyze the performance of the\nMCNLM algorithm and show that, for large images or large external image\ndatabases, the random outcomes of MCNLM are tightly concentrated around the\ndeterministic full NLM result. In particular, our error probability bounds show\nthat, at any given sampling ratio, the probability for MCNLM to have a large\ndeviation from the original NLM solution decays exponentially as the size of\nthe image or database grows. Second, we derive explicit formulas for optimal\nsampling patterns that minimize the error probability bound by exploiting\npartial knowledge of the pairwise similarity weights. Numerical experiments\nshow that MCNLM is competitive with other state-of-the-art fast NLM algorithms\nfor single-image denoising. When applied to denoising images using an external\ndatabase containing ten billion patches, MCNLM returns a randomized solution\nthat is within 0.2 dB of the full NLM solution while reducing the runtime by\nthree orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 23:31:42 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 19:41:16 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 19:35:11 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chan", "Stanley H.", ""], ["Zickler", "Todd", ""], ["Lu", "Yue M.", ""]]}, {"id": "1312.7414", "submitter": "Kiana Hajebi", "authors": "Kiana Hajebi and Hong Zhang", "title": "Stopping Rules for Bag-of-Words Image Search and Its Application in\n  Appearance-Based Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique to improve the search efficiency of the bag-of-words\n(BoW) method for image retrieval. We introduce a notion of difficulty for the\nimage matching problems and propose methods that reduce the amount of\ncomputations required for the feature vector-quantization task in BoW by\nexploiting the fact that easier queries need less computational resources.\nMeasuring the difficulty of a query and stopping the search accordingly is\nformulated as a stopping problem. We introduce stopping rules that terminate\nthe image search depending on the difficulty of each query, thereby\nsignificantly reducing the computational cost. Our experimental results show\nthe effectiveness of our approach when it is applied to appearance-based\nlocalization problem.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 09:53:02 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Hajebi", "Kiana", ""], ["Zhang", "Hong", ""]]}, {"id": "1312.7446", "submitter": "Sheng Huang", "authors": "Sheng Huang and Dan Yang and Haopeng Zhang and Luwen Huangfu and\n  Xiaohong Zhang", "title": "Shape Primitive Histogram: A Novel Low-Level Face Representation for\n  Face Recognition", "comments": "second version, two columns and 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We further exploit the representational power of Haar wavelet and present a\nnovel low-level face representation named Shape Primitives Histogram (SPH) for\nface recognition. Since human faces exist abundant shape features, we address\nthe face representation issue from the perspective of the shape feature\nextraction. In our approach, we divide faces into a number of tiny shape\nfragments and reduce these shape fragments to several uniform atomic shape\npatterns called Shape Primitives. A convolution with Haar Wavelet templates is\napplied to each shape fragment to identify its belonging shape primitive. After\nthat, we do a histogram statistic of shape primitives in each spatial local\nimage patch for incorporating the spatial information. Finally, each face is\nrepresented as a feature vector via concatenating all the local histograms of\nshape primitives. Four popular face databases, namely ORL, AR, Yale-B and LFW-a\ndatabases, are employed to evaluate SPH and experimentally study the choices of\nthe parameters. Extensive experimental results demonstrate that the proposed\napproach outperform the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 16:09:59 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2014 00:36:10 GMT"}, {"version": "v3", "created": "Tue, 22 Jul 2014 02:37:56 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Huang", "Sheng", ""], ["Yang", "Dan", ""], ["Zhang", "Haopeng", ""], ["Huangfu", "Luwen", ""], ["Zhang", "Xiaohong", ""]]}, {"id": "1312.7463", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran and Shrikanth S.\n  Narayanan", "title": "Generalized Ambiguity Decomposition for Understanding Ensemble Diversity", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity or complementarity of experts in ensemble pattern recognition and\ninformation processing systems is widely-observed by researchers to be crucial\nfor achieving performance improvement upon fusion. Understanding this link\nbetween ensemble diversity and fusion performance is thus an important research\nquestion. However, prior works have theoretically characterized ensemble\ndiversity and have linked it with ensemble performance in very restricted\nsettings. We present a generalized ambiguity decomposition (GAD) theorem as a\nbroad framework for answering these questions. The GAD theorem applies to a\ngeneric convex ensemble of experts for any arbitrary twice-differentiable loss\nfunction. It shows that the ensemble performance approximately decomposes into\na difference of the average expert performance and the diversity of the\nensemble. It thus provides a theoretical explanation for the\nempirically-observed benefit of fusing outputs from diverse classifiers and\nregressors. It also provides a loss function-dependent, ensemble-dependent, and\ndata-dependent definition of diversity. We present extensions of this\ndecomposition to common regression and classification loss functions, and\nreport a simulation-based analysis of the diversity term and the accuracy of\nthe decomposition. We finally present experiments on standard pattern\nrecognition data sets which indicate the accuracy of the decomposition for\nreal-world classification and regression problems.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 19:18:44 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Narayanan", "Shrikanth S.", ""]]}, {"id": "1312.7469", "submitter": "Sheng Huang", "authors": "Sheng Huang and Dan Yang and Dong Yang and Ahmed Elgammal", "title": "Collaborative Discriminant Locality Preserving Projections With its\n  Application to Face Recognition", "comments": "second version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a novel Discriminant Locality Preserving Projections (DLPP)\nalgorithm named Collaborative Discriminant Locality Preserving Projection\n(CDLPP). In our algorithm, the discriminating power of DLPP are further\nexploited from two aspects. On the one hand, the global optimum of class\nscattering is guaranteed via using the between-class scatter matrix to replace\nthe original denominator of DLPP. On the other hand, motivated by collaborative\nrepresentation, an $L_2$-norm constraint is imposed to the projections to\ndiscover the collaborations of dimensions in the sample space. We apply our\nalgorithm to face recognition. Three popular face databases, namely AR, ORL and\nLFW-A, are employed for evaluating the performance of CDLPP. Extensive\nexperimental results demonstrate that CDLPP significantly improves the\ndiscriminating power of DLPP and outperforms the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 20:12:17 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2014 20:43:10 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Huang", "Sheng", ""], ["Yang", "Dan", ""], ["Yang", "Dong", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1312.7511", "submitter": "Shraddha Shinde", "authors": "Shraddha S. Shinde and Prof. Anagha P. Khedkar", "title": "A Novel Scheme for Generating Secure Face Templates Using BDA", "comments": "07 pages,IJASCSE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In identity management system, frequently used biometric recognition system\nneeds awareness towards issue of protecting biometric template as far as more\nreliable solution is apprehensive. In sight of this biometric template\nprotection algorithm should gratify the basic requirements viz. security,\ndiscriminability and cancelability. As no single template protection method is\ncapable of satisfying these requirements, a novel scheme for face template\ngeneration and protection is proposed. The novel scheme is proposed to provide\nsecurity and accuracy in new user enrolment and authentication process. This\nnovel scheme takes advantage of both the hybrid approach and the binary\ndiscriminant analysis algorithm. This algorithm is designed on the basis of\nrandom projection, binary discriminant analysis and fuzzy commitment scheme.\nPublicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other\ndatasets are used for evaluation. The proposed novel scheme enhances the\ndiscriminability and recognition accuracy in terms of matching score of the\nface images for each stage and provides high security against potential attacks\nnamely brute force and smart attacks. In this paper, we discuss results viz.\naverages matching score, computation time and security for hybrid approach and\nnovel approach.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 09:31:01 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Shinde", "Shraddha S.", ""], ["Khedkar", "Prof. Anagha P.", ""]]}, {"id": "1312.7523", "submitter": "Luca Bortolussi", "authors": "Ezio Bartocci and Luca Bortolussi and Guido Sanguinetti", "title": "Learning Temporal Logical Properties Discriminating ECG models of\n  Cardiac Arrhytmias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to learn the formulae characterising the emergent\nbehaviour of a dynamical system from system observations. At a high level, the\napproach starts by devising a statistical dynamical model of the system which\noptimally fits the observations. We then propose general optimisation\nstrategies for selecting high support formulae (under the learnt model of the\nsystem) either within a discrete set of formulae of bounded complexity, or a\nparametric family of formulae. We illustrate and apply the methodology on an\nin-depth case study of characterising cardiac malfunction from\nelectro-cardiogram data, where our approach enables us to quantitatively\ndetermine the diagnostic power of a formula in discriminating between different\ncardiac conditions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 11:44:26 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Bartocci", "Ezio", ""], ["Bortolussi", "Luca", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1312.7557", "submitter": "Sevin Samadi", "authors": "Saeid Fazli, Sevin Samadi", "title": "A Novel Retinal Vessel Segmentation Based On Histogram Transformation\n  Using 2-D Morlet Wavelet and Supervised Classification", "comments": "International Journal of Advanced Studies in Computer Science and\n  Engineering (IJASCSE)December 2013. arXiv admin note: text overlap with\n  arXiv:cs/0510001 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appearance and structure of blood vessels in retinal images have an\nimportant role in diagnosis of diseases. This paper proposes a method for\nautomatic retinal vessel segmentation. In this work, a novel preprocessing\nbased on local histogram equalization is used to enhance the original image\nthen pixels are classified as vessel and non-vessel using a classifier. For\nthis classification, special feature vectors are organized based on responses\nto Morlet wavelet. Morlet wavelet is a continues transform which has the\nability to filter existing noises after preprocessing. Bayesian classifier is\nused and Gaussian mixture model (GMM) is its likelihood function. The\nprobability distributions are approximated according to training set of manual\nthat has been segmented by a specialist. After this, morphological transforms\nare used in different directions to make the existing discontinuities uniform\non the DRIVE database, it achieves the accuracy about 0.9571 which shows that\nit is an accurate method among the available ones for retinal vessel\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 16:58:36 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Fazli", "Saeid", ""], ["Samadi", "Sevin", ""]]}, {"id": "1312.7560", "submitter": "Amiraj Dhawan", "authors": "Amiraj Dhawan, Vipul Honrao", "title": "Implementation of Hand Detection based Techniques for Human Computer\n  Interaction", "comments": null, "journal-ref": "International Journal of Computer Applications, Volume 72 No 17,\n  June 2013", "doi": "10.5120/12632-9151 10.5120/12632-9151 10.5120/12632-9151", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer industry is developing at a fast pace. With this development\nalmost all of the fields under computers have advanced in the past couple of\ndecades. But the same technology is being used for human computer interaction\nthat was used in 1970s. Even today the same type of keyboard and mouse is used\nfor interacting with computer systems. With the recent boom in the mobile\nsegment touchscreens have become popular for interaction with cell phones. But\nthese touchscreens are rarely used on traditional systems. This paper tries to\nintroduce methods for human computer interaction using the users hand which can\nbe used both on traditional computer platforms as well as cell phones. The\nmethods explain how the users detected hand can be used as input for\napplications and also explain applications that can take advantage of this type\nof interaction mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 17:29:01 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Dhawan", "Amiraj", ""], ["Honrao", "Vipul", ""]]}, {"id": "1312.7570", "submitter": "Stefan Mathe", "authors": "Stefan Mathe, Cristian Sminchisescu", "title": "Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for\n  Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems based on bag-of-words models from image features collected at maxima\nof sparse interest point operators have been used successfully for both\ncomputer visual object and action recognition tasks. While the sparse,\ninterest-point based approach to recognition is not inconsistent with visual\nprocessing in biological systems that operate in `saccade and fixate' regimes,\nthe methodology and emphasis in the human and the computer vision communities\nremains sharply distinct. Here, we make three contributions aiming to bridge\nthis gap. First, we complement existing state-of-the art large scale dynamic\ncomputer vision annotated datasets like Hollywood-2 and UCF Sports with human\neye movements collected under the ecological constraints of the visual action\nrecognition task. To our knowledge these are the first large human eye tracking\ndatasets to be collected and made publicly available for video,\nvision.imar.ro/eyetracking (497,107 frames, each viewed by 16 subjects), unique\nin terms of their (a) large scale and computer vision relevance, (b) dynamic,\nvideo stimuli, (c) task control, as opposed to free-viewing. Second, we\nintroduce novel sequential consistency and alignment measures, which underline\nthe remarkable stability of patterns of visual search among subjects. Third, we\nleverage the significant amount of collected data in order to pursue studies\nand build automatic, end-to-end trainable computer vision systems based on\nhuman eye movements. Our studies not only shed light on the differences between\ncomputer vision spatio-temporal interest point image sampling strategies and\nthe human fixations, as well as their impact for visual recognition\nperformance, but also demonstrate that human fixations can be accurately\npredicted, and when used in an end-to-end automatic system, leveraging some of\nthe advanced computer vision practice, can lead to state of the art results.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 18:49:04 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Mathe", "Stefan", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1312.7573", "submitter": "Parisa Nadirkhanlou", "authors": "Saeid Fazli, Parisa Nadirkhanlou", "title": "A Novel Method for Automatic Segmentation of Brain Tumors in MRI Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain tumor segmentation on MRI images is a very difficult and important\ntask which is used in surgical and medical planning and assessments. If experts\ndo the segmentation manually with their own medical knowledge, it will be\ntime-consuming. Therefore, researchers propose methods and systems which can do\nthe segmentation automatically and without any interference. In this article,\nan unsupervised automatic method for brain tumor segmentation on MRI images is\npresented. In this method, at first in the pre-processing level, the extra\nparts which are outside the skull and don't have any helpful information are\nremoved and then anisotropic diffusion filter with 8-connected neighborhood is\napplied to the MRI images to remove noise. By applying the fast bounding\nbox(FBB) algorithm, the tumor area is displayed on the MRI image with a\nbounding box and the central part is selected as sample points for training of\na One Class SVM classifier. A database is also provided by the Zanjan MRI\nCenter. The MRI images are related to 10 patients who have brain tumor. 100\nT2-weighted MRI images are used in this study. Experimental results show the\nhigh precision and dependability of the proposed algorithm. The results are\nalso highly helpful for specialists and radiologists to easily estimate the\nsize and position of a tumor.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 19:12:52 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Fazli", "Saeid", ""], ["Nadirkhanlou", "Parisa", ""]]}, {"id": "1312.7710", "submitter": "Martin Storath", "authors": "Andreas Weinmann, Laurent Demaret, Martin Storath", "title": "Total variation regularization for manifold-valued data", "comments": null, "journal-ref": null, "doi": "10.1137/130951075", "report-no": null, "categories": "math.OC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider total variation minimization for manifold valued data. We propose\na cyclic proximal point algorithm and a parallel proximal point algorithm to\nminimize TV functionals with $\\ell^p$-type data terms in the manifold case.\nThese algorithms are based on iterative geodesic averaging which makes them\neasily applicable to a large class of data manifolds. As an application, we\nconsider denoising images which take their values in a manifold. We apply our\nalgorithms to diffusion tensor images, interferometric SAR images as well as\nsphere and cylinder valued images. For the class of Cartan-Hadamard manifolds\n(which includes the data space in diffusion tensor imaging) we show the\nconvergence of the proposed TV minimizing algorithms to a global minimizer.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 13:31:21 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Weinmann", "Andreas", ""], ["Demaret", "Laurent", ""], ["Storath", "Martin", ""]]}, {"id": "1312.7715", "submitter": "Dan Banica", "authors": "Dan Banica, Cristian Sminchisescu", "title": "Constrained Parametric Proposals and Pooling Methods for Semantic\n  Segmentation in RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of semantic segmentation based on RGB-D data, with\nemphasis on analyzing cluttered indoor scenes containing many instances from\nmany visual categories. Our approach is based on a parametric figure-ground\nintensity and depth-constrained proposal process that generates spatial layout\nhypotheses at multiple locations and scales in the image followed by a\nsequential inference algorithm that integrates the proposals into a complete\nscene estimate. Our contributions can be summarized as proposing the following:\n(1) a generalization of parametric max flow figure-ground proposal methodology\nto take advantage of intensity and depth information, in order to\nsystematically and efficiently generate the breakpoints of an underlying\nspatial model in polynomial time, (2) new region description methods based on\nsecond-order pooling over multiple features constructed using both intensity\nand depth channels, (3) an inference procedure that can resolve conflicts in\noverlapping spatial partitions, and handles scenes with a large number of\nobjects category instances, of very different scales, (4) extensive evaluation\nof the impact of depth, as well as the effectiveness of a large number of\ndescriptors, both pre-designed and automatically obtained using deep learning,\nin a difficult RGB-D semantic segmentation problem with 92 classes. We report\nstate of the art results in the challenging NYU Depth v2 dataset, extended for\nRMRC 2013 Indoor Segmentation Challenge, where currently the proposed model\nranks first, with an average score of 24.61% and a number of 39 classes won.\nMoreover, we show that by combining second-order and deep learning features,\nover 15% relative accuracy improvements can be additionally achieved. In a\nscene classification benchmark, our methodology further improves the state of\nthe art by 24%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 13:44:53 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 16:17:50 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Banica", "Dan", ""], ["Sminchisescu", "Cristian", ""]]}]